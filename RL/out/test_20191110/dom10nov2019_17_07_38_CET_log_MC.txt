Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.187s, episode steps: 100, steps per second: 534, episode reward: -18.863, mean reward: -0.189 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.914, 10.370], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.065s, episode steps: 100, steps per second: 1544, episode reward: -18.529, mean reward: -0.185 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.892, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.070s, episode steps: 100, steps per second: 1426, episode reward: -15.462, mean reward: -0.155 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.629, 10.297], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.076s, episode steps: 100, steps per second: 1317, episode reward: -17.544, mean reward: -0.175 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.225, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.074s, episode steps: 100, steps per second: 1351, episode reward: -15.096, mean reward: -0.151 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.280, 10.299], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.277s, episode steps: 100, steps per second: 78, episode reward: -20.121, mean reward: -0.201 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.605, 10.153], loss: 0.061094, mae: 0.226071, mean_q: 0.066115
   700/100000: episode: 7, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -18.613, mean reward: -0.186 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.654, 10.262], loss: 0.013057, mae: 0.109435, mean_q: -0.109082
   800/100000: episode: 8, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.987, mean reward: -0.180 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.509, 10.198], loss: 0.009892, mae: 0.092657, mean_q: -0.199074
   900/100000: episode: 9, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -12.033, mean reward: -0.120 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.726, 10.098], loss: 0.009037, mae: 0.088121, mean_q: -0.257551
  1000/100000: episode: 10, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -15.677, mean reward: -0.157 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.566, 10.098], loss: 0.008010, mae: 0.085670, mean_q: -0.293547
  1100/100000: episode: 11, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: -18.966, mean reward: -0.190 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.609, 10.098], loss: 0.007944, mae: 0.084726, mean_q: -0.325393
  1200/100000: episode: 12, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -19.095, mean reward: -0.191 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.521, 10.268], loss: 0.007596, mae: 0.082414, mean_q: -0.321380
  1300/100000: episode: 13, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -14.778, mean reward: -0.148 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.584, 10.208], loss: 0.007712, mae: 0.082639, mean_q: -0.365153
  1400/100000: episode: 14, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -19.026, mean reward: -0.190 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.553, 10.106], loss: 0.006329, mae: 0.075334, mean_q: -0.336763
  1500/100000: episode: 15, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -18.193, mean reward: -0.182 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.058, 10.098], loss: 0.006496, mae: 0.076673, mean_q: -0.377212
  1600/100000: episode: 16, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -15.263, mean reward: -0.153 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.758, 10.098], loss: 0.006338, mae: 0.074916, mean_q: -0.338287
  1700/100000: episode: 17, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: -17.660, mean reward: -0.177 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.062, 10.179], loss: 0.006768, mae: 0.077893, mean_q: -0.317074
  1800/100000: episode: 18, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -19.096, mean reward: -0.191 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.543, 10.317], loss: 0.005151, mae: 0.069145, mean_q: -0.331693
  1900/100000: episode: 19, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -18.074, mean reward: -0.181 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.968, 10.098], loss: 0.005536, mae: 0.070711, mean_q: -0.325456
  2000/100000: episode: 20, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -12.619, mean reward: -0.126 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.940, 10.406], loss: 0.005401, mae: 0.070477, mean_q: -0.330325
  2100/100000: episode: 21, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -20.122, mean reward: -0.201 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.446, 10.098], loss: 0.005510, mae: 0.072104, mean_q: -0.368906
  2200/100000: episode: 22, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.044, mean reward: -0.160 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.196, 10.098], loss: 0.005367, mae: 0.069742, mean_q: -0.350684
  2300/100000: episode: 23, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.479, mean reward: -0.155 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.747, 10.135], loss: 0.005127, mae: 0.069615, mean_q: -0.330241
  2400/100000: episode: 24, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -17.301, mean reward: -0.173 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.863, 10.105], loss: 0.005749, mae: 0.072663, mean_q: -0.324407
  2500/100000: episode: 25, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -18.600, mean reward: -0.186 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.271, 10.202], loss: 0.004914, mae: 0.067983, mean_q: -0.328484
  2600/100000: episode: 26, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -13.102, mean reward: -0.131 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.620, 10.155], loss: 0.005384, mae: 0.071745, mean_q: -0.319857
  2700/100000: episode: 27, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -15.975, mean reward: -0.160 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.083, 10.146], loss: 0.004952, mae: 0.068793, mean_q: -0.301010
  2800/100000: episode: 28, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -15.966, mean reward: -0.160 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.699, 10.272], loss: 0.004875, mae: 0.067423, mean_q: -0.327403
  2900/100000: episode: 29, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -9.991, mean reward: -0.100 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.489, 10.098], loss: 0.005194, mae: 0.071137, mean_q: -0.340172
  3000/100000: episode: 30, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -14.459, mean reward: -0.145 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.742, 10.098], loss: 0.005302, mae: 0.070185, mean_q: -0.321386
  3100/100000: episode: 31, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -14.679, mean reward: -0.147 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.867, 10.314], loss: 0.005358, mae: 0.073866, mean_q: -0.298633
  3200/100000: episode: 32, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.322, mean reward: -0.173 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.945, 10.117], loss: 0.004984, mae: 0.069932, mean_q: -0.294893
  3300/100000: episode: 33, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.672, mean reward: -0.177 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.562, 10.177], loss: 0.004952, mae: 0.068908, mean_q: -0.308092
  3400/100000: episode: 34, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.511, mean reward: -0.175 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.785, 10.139], loss: 0.005421, mae: 0.071858, mean_q: -0.341222
  3500/100000: episode: 35, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -19.264, mean reward: -0.193 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.232, 10.327], loss: 0.005036, mae: 0.070818, mean_q: -0.317437
  3600/100000: episode: 36, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -14.593, mean reward: -0.146 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.289, 10.274], loss: 0.005978, mae: 0.076755, mean_q: -0.309748
  3700/100000: episode: 37, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.972, mean reward: -0.170 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.285, 10.206], loss: 0.004243, mae: 0.066981, mean_q: -0.337019
  3800/100000: episode: 38, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.285, mean reward: -0.173 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.780, 10.098], loss: 0.004379, mae: 0.065817, mean_q: -0.309508
  3900/100000: episode: 39, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -16.414, mean reward: -0.164 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.953, 10.184], loss: 0.004778, mae: 0.067938, mean_q: -0.321838
  4000/100000: episode: 40, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.700, mean reward: -0.167 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.261, 10.340], loss: 0.004378, mae: 0.066844, mean_q: -0.298756
  4100/100000: episode: 41, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.289, mean reward: -0.163 [-1.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.551, 10.266], loss: 0.005124, mae: 0.070670, mean_q: -0.306853
  4200/100000: episode: 42, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -17.604, mean reward: -0.176 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.960, 10.266], loss: 0.004693, mae: 0.067503, mean_q: -0.306245
  4300/100000: episode: 43, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -16.867, mean reward: -0.169 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.167, 10.098], loss: 0.004254, mae: 0.064782, mean_q: -0.337678
  4400/100000: episode: 44, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -19.575, mean reward: -0.196 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.159, 10.179], loss: 0.005154, mae: 0.069474, mean_q: -0.344293
  4500/100000: episode: 45, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -11.226, mean reward: -0.112 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.761, 10.287], loss: 0.004445, mae: 0.066022, mean_q: -0.355189
  4600/100000: episode: 46, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.169, mean reward: -0.182 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.502, 10.190], loss: 0.004532, mae: 0.066513, mean_q: -0.360153
  4700/100000: episode: 47, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.260, mean reward: -0.173 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.421, 10.098], loss: 0.005271, mae: 0.070395, mean_q: -0.319679
  4800/100000: episode: 48, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -18.397, mean reward: -0.184 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.849, 10.256], loss: 0.005113, mae: 0.071088, mean_q: -0.318285
  4900/100000: episode: 49, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -17.149, mean reward: -0.171 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.937, 10.182], loss: 0.004621, mae: 0.068997, mean_q: -0.328031
  5000/100000: episode: 50, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -14.021, mean reward: -0.140 [-1.000, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.516, 10.098], loss: 0.004234, mae: 0.065337, mean_q: -0.311352
  5100/100000: episode: 51, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.262, mean reward: -0.173 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.676, 10.098], loss: 0.004396, mae: 0.066033, mean_q: -0.292928
  5200/100000: episode: 52, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -13.573, mean reward: -0.136 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.504, 10.098], loss: 0.005063, mae: 0.070865, mean_q: -0.325095
  5300/100000: episode: 53, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -15.572, mean reward: -0.156 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.962, 10.240], loss: 0.004588, mae: 0.068100, mean_q: -0.261318
  5400/100000: episode: 54, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -17.811, mean reward: -0.178 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.930, 10.098], loss: 0.003747, mae: 0.061760, mean_q: -0.326451
  5500/100000: episode: 55, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -18.233, mean reward: -0.182 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.522, 10.098], loss: 0.004347, mae: 0.066524, mean_q: -0.341448
  5600/100000: episode: 56, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.596, mean reward: -0.166 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.332, 10.345], loss: 0.004159, mae: 0.065297, mean_q: -0.302963
  5700/100000: episode: 57, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.795, mean reward: -0.198 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.510, 10.163], loss: 0.003813, mae: 0.061796, mean_q: -0.309119
  5800/100000: episode: 58, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -18.135, mean reward: -0.181 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.046, 10.159], loss: 0.003920, mae: 0.063328, mean_q: -0.303719
  5900/100000: episode: 59, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -20.651, mean reward: -0.207 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.816, 10.098], loss: 0.004245, mae: 0.065137, mean_q: -0.327041
  6000/100000: episode: 60, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.314, mean reward: -0.163 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.311, 10.098], loss: 0.004776, mae: 0.068469, mean_q: -0.364234
  6100/100000: episode: 61, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -18.343, mean reward: -0.183 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.810, 10.201], loss: 0.004167, mae: 0.066469, mean_q: -0.284804
  6200/100000: episode: 62, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.206, mean reward: -0.182 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.879, 10.098], loss: 0.003957, mae: 0.064102, mean_q: -0.307319
  6300/100000: episode: 63, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -18.055, mean reward: -0.181 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.328, 10.146], loss: 0.003317, mae: 0.058046, mean_q: -0.352987
  6400/100000: episode: 64, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.976, mean reward: -0.180 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.779, 10.169], loss: 0.004400, mae: 0.069582, mean_q: -0.302074
  6500/100000: episode: 65, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -14.859, mean reward: -0.149 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.288, 10.326], loss: 0.003825, mae: 0.064114, mean_q: -0.337044
  6600/100000: episode: 66, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.613, mean reward: -0.186 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.572, 10.133], loss: 0.004287, mae: 0.064689, mean_q: -0.335126
  6700/100000: episode: 67, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -17.817, mean reward: -0.178 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.044, 10.129], loss: 0.005643, mae: 0.073986, mean_q: -0.323453
  6800/100000: episode: 68, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -16.477, mean reward: -0.165 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.135, 10.098], loss: 0.004237, mae: 0.066431, mean_q: -0.300663
  6900/100000: episode: 69, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -20.037, mean reward: -0.200 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.562, 10.119], loss: 0.003658, mae: 0.062329, mean_q: -0.272696
  7000/100000: episode: 70, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -17.677, mean reward: -0.177 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.407, 10.098], loss: 0.003392, mae: 0.059717, mean_q: -0.327372
  7100/100000: episode: 71, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -16.280, mean reward: -0.163 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.785, 10.342], loss: 0.003678, mae: 0.062755, mean_q: -0.320391
  7200/100000: episode: 72, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -13.724, mean reward: -0.137 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.237, 10.147], loss: 0.003449, mae: 0.060920, mean_q: -0.296407
  7300/100000: episode: 73, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -17.436, mean reward: -0.174 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.600, 10.098], loss: 0.003217, mae: 0.058287, mean_q: -0.312485
  7400/100000: episode: 74, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -19.052, mean reward: -0.191 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.393, 10.173], loss: 0.003826, mae: 0.059952, mean_q: -0.344247
  7500/100000: episode: 75, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.006, mean reward: -0.150 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.444, 10.099], loss: 0.004620, mae: 0.069978, mean_q: -0.333248
  7600/100000: episode: 76, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -11.128, mean reward: -0.111 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.339, 10.248], loss: 0.004574, mae: 0.067755, mean_q: -0.320411
  7700/100000: episode: 77, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -18.872, mean reward: -0.189 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.447, 10.098], loss: 0.004183, mae: 0.065509, mean_q: -0.301474
  7800/100000: episode: 78, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -16.721, mean reward: -0.167 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.442, 10.155], loss: 0.004179, mae: 0.065712, mean_q: -0.314402
  7900/100000: episode: 79, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.495, mean reward: -0.185 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.217, 10.210], loss: 0.004429, mae: 0.067407, mean_q: -0.311768
  8000/100000: episode: 80, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -16.629, mean reward: -0.166 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.451, 10.127], loss: 0.003599, mae: 0.061198, mean_q: -0.328095
  8100/100000: episode: 81, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -14.877, mean reward: -0.149 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.909, 10.267], loss: 0.003762, mae: 0.061886, mean_q: -0.309069
  8200/100000: episode: 82, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -13.374, mean reward: -0.134 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.084, 10.098], loss: 0.003576, mae: 0.060836, mean_q: -0.315933
  8300/100000: episode: 83, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -19.447, mean reward: -0.194 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.039, 10.152], loss: 0.003395, mae: 0.059820, mean_q: -0.312812
  8400/100000: episode: 84, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -20.754, mean reward: -0.208 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.737, 10.200], loss: 0.003936, mae: 0.063438, mean_q: -0.303438
  8500/100000: episode: 85, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -17.655, mean reward: -0.177 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.704, 10.098], loss: 0.004462, mae: 0.066657, mean_q: -0.309170
  8600/100000: episode: 86, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.573, mean reward: -0.166 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.213, 10.286], loss: 0.003303, mae: 0.058685, mean_q: -0.335573
  8700/100000: episode: 87, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -18.422, mean reward: -0.184 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.569, 10.098], loss: 0.003561, mae: 0.059767, mean_q: -0.337424
  8800/100000: episode: 88, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -15.052, mean reward: -0.151 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.748, 10.349], loss: 0.003448, mae: 0.060207, mean_q: -0.331518
  8900/100000: episode: 89, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.830, mean reward: -0.158 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.955, 10.212], loss: 0.003268, mae: 0.057793, mean_q: -0.293616
  9000/100000: episode: 90, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -20.143, mean reward: -0.201 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.306, 10.154], loss: 0.004457, mae: 0.066141, mean_q: -0.328254
  9100/100000: episode: 91, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -19.101, mean reward: -0.191 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.636, 10.098], loss: 0.003549, mae: 0.061157, mean_q: -0.322028
  9200/100000: episode: 92, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -17.513, mean reward: -0.175 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.310, 10.098], loss: 0.003410, mae: 0.060091, mean_q: -0.326208
  9300/100000: episode: 93, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -17.044, mean reward: -0.170 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.802, 10.169], loss: 0.003280, mae: 0.058091, mean_q: -0.348649
  9400/100000: episode: 94, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -18.371, mean reward: -0.184 [-1.000, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.615, 10.098], loss: 0.003316, mae: 0.057952, mean_q: -0.335460
  9500/100000: episode: 95, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -16.747, mean reward: -0.167 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.440, 10.141], loss: 0.003872, mae: 0.062664, mean_q: -0.298997
  9600/100000: episode: 96, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -20.082, mean reward: -0.201 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.889, 10.208], loss: 0.003177, mae: 0.057520, mean_q: -0.341687
  9700/100000: episode: 97, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -16.483, mean reward: -0.165 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.009, 10.187], loss: 0.003125, mae: 0.057087, mean_q: -0.317900
  9800/100000: episode: 98, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -20.516, mean reward: -0.205 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.603, 10.098], loss: 0.003566, mae: 0.062185, mean_q: -0.295834
  9900/100000: episode: 99, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -14.796, mean reward: -0.148 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.288, 10.098], loss: 0.003366, mae: 0.058723, mean_q: -0.295202
 10000/100000: episode: 100, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -19.338, mean reward: -0.193 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.525, 10.193], loss: 0.003869, mae: 0.064758, mean_q: -0.344434
 10100/100000: episode: 101, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -20.137, mean reward: -0.201 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.176, 10.156], loss: 0.003002, mae: 0.056234, mean_q: -0.366440
 10200/100000: episode: 102, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.389, mean reward: -0.174 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.626, 10.288], loss: 0.003069, mae: 0.055675, mean_q: -0.351448
 10300/100000: episode: 103, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -18.428, mean reward: -0.184 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.063, 10.098], loss: 0.003534, mae: 0.060690, mean_q: -0.339150
 10400/100000: episode: 104, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -13.467, mean reward: -0.135 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.284, 10.098], loss: 0.004447, mae: 0.065450, mean_q: -0.320783
 10500/100000: episode: 105, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -16.091, mean reward: -0.161 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.378, 10.098], loss: 0.003813, mae: 0.064205, mean_q: -0.355607
 10600/100000: episode: 106, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -17.712, mean reward: -0.177 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.072, 10.098], loss: 0.003241, mae: 0.059197, mean_q: -0.324977
 10700/100000: episode: 107, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -15.973, mean reward: -0.160 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.750, 10.098], loss: 0.003567, mae: 0.062893, mean_q: -0.324276
 10800/100000: episode: 108, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -19.010, mean reward: -0.190 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.744, 10.228], loss: 0.003115, mae: 0.056692, mean_q: -0.347105
 10900/100000: episode: 109, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -12.261, mean reward: -0.123 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.232, 10.098], loss: 0.003256, mae: 0.059161, mean_q: -0.334098
 11000/100000: episode: 110, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -18.261, mean reward: -0.183 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.206, 10.098], loss: 0.003986, mae: 0.065725, mean_q: -0.315591
 11100/100000: episode: 111, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.792, mean reward: -0.168 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.718, 10.362], loss: 0.005326, mae: 0.073006, mean_q: -0.320089
 11200/100000: episode: 112, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -10.102, mean reward: -0.101 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.922, 10.098], loss: 0.003383, mae: 0.060148, mean_q: -0.316993
 11300/100000: episode: 113, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -15.481, mean reward: -0.155 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.826, 10.466], loss: 0.003942, mae: 0.064201, mean_q: -0.341894
 11400/100000: episode: 114, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.479, mean reward: -0.175 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.853, 10.098], loss: 0.003853, mae: 0.065227, mean_q: -0.295467
 11500/100000: episode: 115, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -15.906, mean reward: -0.159 [-1.000, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.875, 10.098], loss: 0.003647, mae: 0.061064, mean_q: -0.315197
 11600/100000: episode: 116, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -13.117, mean reward: -0.131 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.737, 10.098], loss: 0.003562, mae: 0.062320, mean_q: -0.277442
 11700/100000: episode: 117, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.234, mean reward: -0.172 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.153, 10.277], loss: 0.003341, mae: 0.058766, mean_q: -0.314818
 11800/100000: episode: 118, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: -18.334, mean reward: -0.183 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.557, 10.098], loss: 0.003544, mae: 0.060151, mean_q: -0.323910
 11900/100000: episode: 119, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.364, mean reward: -0.194 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.396, 10.232], loss: 0.003550, mae: 0.061119, mean_q: -0.298136
 12000/100000: episode: 120, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -20.740, mean reward: -0.207 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.361, 10.134], loss: 0.003607, mae: 0.061570, mean_q: -0.296348
 12100/100000: episode: 121, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -13.155, mean reward: -0.132 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.824, 10.155], loss: 0.003500, mae: 0.060737, mean_q: -0.345462
 12200/100000: episode: 122, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.127, mean reward: -0.161 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.407, 10.247], loss: 0.002953, mae: 0.055223, mean_q: -0.371631
 12300/100000: episode: 123, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -19.312, mean reward: -0.193 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.876, 10.128], loss: 0.003948, mae: 0.066178, mean_q: -0.323065
 12400/100000: episode: 124, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -14.303, mean reward: -0.143 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.533, 10.417], loss: 0.003296, mae: 0.058565, mean_q: -0.305991
 12500/100000: episode: 125, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -14.350, mean reward: -0.144 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.852, 10.194], loss: 0.003394, mae: 0.059309, mean_q: -0.315163
 12600/100000: episode: 126, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -13.618, mean reward: -0.136 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.753, 10.098], loss: 0.003519, mae: 0.061456, mean_q: -0.345750
 12700/100000: episode: 127, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -19.180, mean reward: -0.192 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.804, 10.098], loss: 0.003398, mae: 0.059075, mean_q: -0.337273
 12800/100000: episode: 128, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.837, mean reward: -0.158 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.821, 10.195], loss: 0.006161, mae: 0.074834, mean_q: -0.329565
 12900/100000: episode: 129, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -16.731, mean reward: -0.167 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.229, 10.209], loss: 0.003548, mae: 0.062219, mean_q: -0.307652
 13000/100000: episode: 130, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -19.883, mean reward: -0.199 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.700, 10.098], loss: 0.004440, mae: 0.065377, mean_q: -0.330913
 13100/100000: episode: 131, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -15.011, mean reward: -0.150 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.446, 10.098], loss: 0.004040, mae: 0.064791, mean_q: -0.323038
 13200/100000: episode: 132, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.439, mean reward: -0.184 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.368, 10.150], loss: 0.003989, mae: 0.065221, mean_q: -0.316455
 13300/100000: episode: 133, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.539, mean reward: -0.185 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.068, 10.138], loss: 0.003842, mae: 0.064980, mean_q: -0.309473
 13400/100000: episode: 134, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -17.785, mean reward: -0.178 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.621, 10.308], loss: 0.003676, mae: 0.061577, mean_q: -0.321337
 13500/100000: episode: 135, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -18.702, mean reward: -0.187 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.457, 10.098], loss: 0.003202, mae: 0.056876, mean_q: -0.330075
 13600/100000: episode: 136, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -18.709, mean reward: -0.187 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.602, 10.098], loss: 0.003467, mae: 0.059901, mean_q: -0.320481
 13700/100000: episode: 137, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -17.809, mean reward: -0.178 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.549, 10.165], loss: 0.003053, mae: 0.056507, mean_q: -0.344118
 13800/100000: episode: 138, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.003, mean reward: -0.180 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.871, 10.322], loss: 0.003300, mae: 0.058745, mean_q: -0.264063
 13900/100000: episode: 139, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.941, mean reward: -0.189 [-1.000, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.079, 10.098], loss: 0.003178, mae: 0.057371, mean_q: -0.329973
 14000/100000: episode: 140, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -14.388, mean reward: -0.144 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.107, 10.115], loss: 0.003702, mae: 0.060707, mean_q: -0.336246
 14100/100000: episode: 141, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -11.812, mean reward: -0.118 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.891, 10.379], loss: 0.003342, mae: 0.058960, mean_q: -0.324581
 14200/100000: episode: 142, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.533, mean reward: -0.185 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.730, 10.180], loss: 0.003159, mae: 0.056986, mean_q: -0.335210
 14300/100000: episode: 143, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -20.125, mean reward: -0.201 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.674, 10.380], loss: 0.003607, mae: 0.061315, mean_q: -0.317105
 14400/100000: episode: 144, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.638, mean reward: -0.186 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.716, 10.276], loss: 0.003342, mae: 0.059059, mean_q: -0.329208
 14500/100000: episode: 145, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.022, mean reward: -0.170 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.667, 10.098], loss: 0.004100, mae: 0.066504, mean_q: -0.342479
 14600/100000: episode: 146, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -18.506, mean reward: -0.185 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.928, 10.352], loss: 0.003442, mae: 0.059399, mean_q: -0.323855
 14700/100000: episode: 147, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.982, mean reward: -0.170 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.782, 10.189], loss: 0.004691, mae: 0.067725, mean_q: -0.322170
 14800/100000: episode: 148, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -19.321, mean reward: -0.193 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.810, 10.175], loss: 0.004073, mae: 0.064628, mean_q: -0.317296
 14900/100000: episode: 149, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.680, mean reward: -0.187 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.824, 10.098], loss: 0.003601, mae: 0.062518, mean_q: -0.319522
 15000/100000: episode: 150, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -16.647, mean reward: -0.166 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.767, 10.233], loss: 0.003216, mae: 0.057498, mean_q: -0.323584
 15100/100000: episode: 151, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -20.262, mean reward: -0.203 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.783, 10.098], loss: 0.003316, mae: 0.057758, mean_q: -0.328616
 15200/100000: episode: 152, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.062, mean reward: -0.181 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.889, 10.125], loss: 0.003014, mae: 0.055423, mean_q: -0.372759
 15300/100000: episode: 153, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -9.848, mean reward: -0.098 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.680, 10.482], loss: 0.004129, mae: 0.063797, mean_q: -0.347254
 15400/100000: episode: 154, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -17.266, mean reward: -0.173 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.860, 10.098], loss: 0.003191, mae: 0.056433, mean_q: -0.329059
 15500/100000: episode: 155, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -13.896, mean reward: -0.139 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.127, 10.098], loss: 0.004297, mae: 0.065394, mean_q: -0.304977
 15600/100000: episode: 156, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -15.822, mean reward: -0.158 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.642, 10.098], loss: 0.003415, mae: 0.059926, mean_q: -0.272576
 15700/100000: episode: 157, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -14.632, mean reward: -0.146 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.689, 10.098], loss: 0.003308, mae: 0.057986, mean_q: -0.305513
 15800/100000: episode: 158, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -19.250, mean reward: -0.193 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.665, 10.098], loss: 0.003486, mae: 0.059990, mean_q: -0.314529
 15900/100000: episode: 159, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.261, mean reward: -0.173 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.242, 10.129], loss: 0.003461, mae: 0.059145, mean_q: -0.303741
 16000/100000: episode: 160, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.126, mean reward: -0.171 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.246, 10.275], loss: 0.003222, mae: 0.057153, mean_q: -0.353200
 16100/100000: episode: 161, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -14.957, mean reward: -0.150 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.529, 10.297], loss: 0.003054, mae: 0.055524, mean_q: -0.315514
 16200/100000: episode: 162, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -15.296, mean reward: -0.153 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.365, 10.098], loss: 0.003385, mae: 0.059056, mean_q: -0.298238
 16300/100000: episode: 163, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.159, mean reward: -0.172 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.762, 10.098], loss: 0.003187, mae: 0.057340, mean_q: -0.313525
 16400/100000: episode: 164, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -20.960, mean reward: -0.210 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.290, 10.144], loss: 0.003111, mae: 0.056635, mean_q: -0.361127
 16500/100000: episode: 165, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -19.253, mean reward: -0.193 [-1.000, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.660, 10.098], loss: 0.002953, mae: 0.054989, mean_q: -0.324311
 16600/100000: episode: 166, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -15.185, mean reward: -0.152 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.705, 10.098], loss: 0.003136, mae: 0.056541, mean_q: -0.300843
 16700/100000: episode: 167, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -14.436, mean reward: -0.144 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.203, 10.110], loss: 0.003215, mae: 0.056801, mean_q: -0.331210
 16800/100000: episode: 168, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -19.482, mean reward: -0.195 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.751, 10.098], loss: 0.003066, mae: 0.055467, mean_q: -0.322870
 16900/100000: episode: 169, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.932, mean reward: -0.179 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.685, 10.347], loss: 0.003010, mae: 0.055798, mean_q: -0.370457
 17000/100000: episode: 170, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.831, mean reward: -0.168 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.963, 10.284], loss: 0.002972, mae: 0.055824, mean_q: -0.312085
 17100/100000: episode: 171, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -18.174, mean reward: -0.182 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.807, 10.171], loss: 0.003686, mae: 0.062214, mean_q: -0.295384
 17200/100000: episode: 172, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -19.032, mean reward: -0.190 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.185, 10.121], loss: 0.003194, mae: 0.056928, mean_q: -0.293050
 17300/100000: episode: 173, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -19.623, mean reward: -0.196 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.093, 10.098], loss: 0.003177, mae: 0.056609, mean_q: -0.312500
 17400/100000: episode: 174, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -19.647, mean reward: -0.196 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.708, 10.098], loss: 0.006807, mae: 0.075929, mean_q: -0.339565
 17500/100000: episode: 175, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -19.208, mean reward: -0.192 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.152, 10.228], loss: 0.003865, mae: 0.065500, mean_q: -0.310686
 17600/100000: episode: 176, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.881, mean reward: -0.189 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.669, 10.324], loss: 0.003096, mae: 0.057743, mean_q: -0.298378
 17700/100000: episode: 177, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -17.224, mean reward: -0.172 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.596, 10.098], loss: 0.002860, mae: 0.054297, mean_q: -0.324564
 17800/100000: episode: 178, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -19.356, mean reward: -0.194 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.384, 10.098], loss: 0.003036, mae: 0.057549, mean_q: -0.333170
 17900/100000: episode: 179, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -20.380, mean reward: -0.204 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.254, 10.098], loss: 0.002784, mae: 0.053803, mean_q: -0.331151
 18000/100000: episode: 180, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.954, mean reward: -0.170 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.719, 10.185], loss: 0.002926, mae: 0.054733, mean_q: -0.339117
 18100/100000: episode: 181, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -6.149, mean reward: -0.061 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.658, 10.098], loss: 0.002829, mae: 0.052291, mean_q: -0.376030
 18200/100000: episode: 182, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.803, mean reward: -0.178 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.664, 10.220], loss: 0.002816, mae: 0.053578, mean_q: -0.322516
 18300/100000: episode: 183, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -15.156, mean reward: -0.152 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.036, 10.098], loss: 0.002955, mae: 0.055798, mean_q: -0.330241
 18400/100000: episode: 184, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.902, mean reward: -0.179 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.262, 10.125], loss: 0.002878, mae: 0.053774, mean_q: -0.341914
 18500/100000: episode: 185, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -16.317, mean reward: -0.163 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.031, 10.199], loss: 0.002774, mae: 0.053064, mean_q: -0.326301
 18600/100000: episode: 186, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -18.981, mean reward: -0.190 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.341, 10.170], loss: 0.003160, mae: 0.055231, mean_q: -0.345148
 18700/100000: episode: 187, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.132, mean reward: -0.191 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.854, 10.098], loss: 0.002795, mae: 0.051548, mean_q: -0.350454
 18800/100000: episode: 188, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -18.471, mean reward: -0.185 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.618, 10.296], loss: 0.003138, mae: 0.057924, mean_q: -0.334426
 18900/100000: episode: 189, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.520, mean reward: -0.165 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.040, 10.345], loss: 0.003150, mae: 0.057481, mean_q: -0.298752
 19000/100000: episode: 190, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.246, mean reward: -0.152 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.855, 10.120], loss: 0.002966, mae: 0.054557, mean_q: -0.318521
 19100/100000: episode: 191, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.339, mean reward: -0.173 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.930, 10.139], loss: 0.002761, mae: 0.053779, mean_q: -0.316940
 19200/100000: episode: 192, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.729, mean reward: -0.157 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.085, 10.245], loss: 0.002832, mae: 0.054049, mean_q: -0.305054
 19300/100000: episode: 193, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -14.236, mean reward: -0.142 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.679, 10.259], loss: 0.002874, mae: 0.054769, mean_q: -0.322923
 19400/100000: episode: 194, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -17.977, mean reward: -0.180 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.170, 10.098], loss: 0.002808, mae: 0.052890, mean_q: -0.342049
 19500/100000: episode: 195, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -14.810, mean reward: -0.148 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.518, 10.427], loss: 0.002885, mae: 0.054252, mean_q: -0.305177
 19600/100000: episode: 196, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.482, mean reward: -0.185 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.403, 10.223], loss: 0.003154, mae: 0.055968, mean_q: -0.319159
 19700/100000: episode: 197, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -8.464, mean reward: -0.085 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.810, 10.098], loss: 0.005557, mae: 0.067377, mean_q: -0.359266
 19800/100000: episode: 198, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -19.338, mean reward: -0.193 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.307, 10.227], loss: 0.003052, mae: 0.055843, mean_q: -0.334180
 19900/100000: episode: 199, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -18.301, mean reward: -0.183 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.344, 10.218], loss: 0.003123, mae: 0.056861, mean_q: -0.355250
 20000/100000: episode: 200, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -18.284, mean reward: -0.183 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.598, 10.098], loss: 0.003196, mae: 0.057820, mean_q: -0.334638
 20100/100000: episode: 201, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: -20.969, mean reward: -0.210 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.664, 10.131], loss: 0.002804, mae: 0.053458, mean_q: -0.311297
 20200/100000: episode: 202, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -14.681, mean reward: -0.147 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.777, 10.098], loss: 0.002896, mae: 0.053559, mean_q: -0.311709
 20300/100000: episode: 203, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.213, mean reward: -0.172 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.814, 10.098], loss: 0.002787, mae: 0.054446, mean_q: -0.313766
 20400/100000: episode: 204, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -15.048, mean reward: -0.150 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.005, 10.098], loss: 0.002790, mae: 0.053155, mean_q: -0.334802
 20500/100000: episode: 205, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: -16.359, mean reward: -0.164 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.153, 10.098], loss: 0.003026, mae: 0.056708, mean_q: -0.300517
 20600/100000: episode: 206, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.524, mean reward: -0.185 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.812, 10.098], loss: 0.005397, mae: 0.070776, mean_q: -0.306998
 20700/100000: episode: 207, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -12.490, mean reward: -0.125 [-1.000, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.608, 10.098], loss: 0.003203, mae: 0.058761, mean_q: -0.311524
 20800/100000: episode: 208, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -17.577, mean reward: -0.176 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.485, 10.098], loss: 0.003464, mae: 0.060744, mean_q: -0.340164
 20900/100000: episode: 209, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -19.567, mean reward: -0.196 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.406, 10.106], loss: 0.003347, mae: 0.058440, mean_q: -0.316027
 21000/100000: episode: 210, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.703, mean reward: -0.197 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.732, 10.098], loss: 0.003160, mae: 0.056278, mean_q: -0.309274
 21100/100000: episode: 211, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.194, mean reward: -0.172 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.700, 10.279], loss: 0.002797, mae: 0.053127, mean_q: -0.319576
 21200/100000: episode: 212, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -13.111, mean reward: -0.131 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.552, 10.098], loss: 0.002928, mae: 0.054906, mean_q: -0.331686
 21300/100000: episode: 213, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -13.728, mean reward: -0.137 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.711, 10.484], loss: 0.002687, mae: 0.051533, mean_q: -0.379102
 21400/100000: episode: 214, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.722, mean reward: -0.177 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.474, 10.189], loss: 0.002928, mae: 0.053654, mean_q: -0.319517
 21500/100000: episode: 215, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -14.988, mean reward: -0.150 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.930, 10.098], loss: 0.003013, mae: 0.054874, mean_q: -0.318696
 21600/100000: episode: 216, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.481, mean reward: -0.165 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.757, 10.098], loss: 0.002818, mae: 0.052513, mean_q: -0.347805
 21700/100000: episode: 217, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -15.347, mean reward: -0.153 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.713, 10.098], loss: 0.002885, mae: 0.053427, mean_q: -0.330577
 21800/100000: episode: 218, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -16.877, mean reward: -0.169 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.088, 10.181], loss: 0.003056, mae: 0.056495, mean_q: -0.311467
 21900/100000: episode: 219, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -16.166, mean reward: -0.162 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.811, 10.098], loss: 0.002629, mae: 0.051064, mean_q: -0.330692
 22000/100000: episode: 220, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -16.997, mean reward: -0.170 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.970, 10.098], loss: 0.002438, mae: 0.050006, mean_q: -0.307522
 22100/100000: episode: 221, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -13.132, mean reward: -0.131 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.320, 10.370], loss: 0.002767, mae: 0.052081, mean_q: -0.343114
 22200/100000: episode: 222, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -17.612, mean reward: -0.176 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.425, 10.111], loss: 0.002802, mae: 0.053517, mean_q: -0.328942
 22300/100000: episode: 223, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -16.633, mean reward: -0.166 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.882, 10.155], loss: 0.002774, mae: 0.052476, mean_q: -0.320889
 22400/100000: episode: 224, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -16.436, mean reward: -0.164 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.906, 10.098], loss: 0.002960, mae: 0.055192, mean_q: -0.284811
 22500/100000: episode: 225, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -18.077, mean reward: -0.181 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.120, 10.168], loss: 0.003998, mae: 0.062685, mean_q: -0.296432
 22600/100000: episode: 226, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -10.555, mean reward: -0.106 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.036, 10.323], loss: 0.002880, mae: 0.055276, mean_q: -0.290840
 22700/100000: episode: 227, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.799, mean reward: -0.178 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.727, 10.098], loss: 0.002737, mae: 0.052548, mean_q: -0.314435
 22800/100000: episode: 228, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -19.357, mean reward: -0.194 [-1.000, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.580, 10.098], loss: 0.002916, mae: 0.055115, mean_q: -0.295193
 22900/100000: episode: 229, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -19.085, mean reward: -0.191 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.188, 10.098], loss: 0.002748, mae: 0.052346, mean_q: -0.350289
 23000/100000: episode: 230, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.846, mean reward: -0.188 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.977, 10.456], loss: 0.002878, mae: 0.054742, mean_q: -0.332191
 23100/100000: episode: 231, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -16.370, mean reward: -0.164 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.632, 10.176], loss: 0.002907, mae: 0.055447, mean_q: -0.270952
 23200/100000: episode: 232, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -17.328, mean reward: -0.173 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.595, 10.098], loss: 0.002988, mae: 0.054544, mean_q: -0.335650
 23300/100000: episode: 233, duration: 0.647s, episode steps: 100, steps per second: 155, episode reward: -13.698, mean reward: -0.137 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.900, 10.280], loss: 0.002673, mae: 0.051797, mean_q: -0.310722
 23400/100000: episode: 234, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: -15.261, mean reward: -0.153 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.793, 10.110], loss: 0.002689, mae: 0.052822, mean_q: -0.316283
 23500/100000: episode: 235, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: -16.382, mean reward: -0.164 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.397, 10.378], loss: 0.002739, mae: 0.053480, mean_q: -0.288391
 23600/100000: episode: 236, duration: 0.727s, episode steps: 100, steps per second: 138, episode reward: -15.315, mean reward: -0.153 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.370, 10.098], loss: 0.002727, mae: 0.053473, mean_q: -0.307823
 23700/100000: episode: 237, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -16.505, mean reward: -0.165 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.202, 10.098], loss: 0.002666, mae: 0.052570, mean_q: -0.316583
 23800/100000: episode: 238, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -13.099, mean reward: -0.131 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.619, 10.098], loss: 0.002619, mae: 0.051425, mean_q: -0.349667
 23900/100000: episode: 239, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -16.114, mean reward: -0.161 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.912, 10.098], loss: 0.002784, mae: 0.053078, mean_q: -0.304309
 24000/100000: episode: 240, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -17.961, mean reward: -0.180 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.426, 10.163], loss: 0.002733, mae: 0.053164, mean_q: -0.303450
 24100/100000: episode: 241, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -10.187, mean reward: -0.102 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.765, 10.098], loss: 0.002792, mae: 0.053050, mean_q: -0.330613
 24200/100000: episode: 242, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -16.889, mean reward: -0.169 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.209, 10.142], loss: 0.002952, mae: 0.053870, mean_q: -0.299331
 24300/100000: episode: 243, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: -18.577, mean reward: -0.186 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.111, 10.219], loss: 0.006992, mae: 0.075009, mean_q: -0.318784
 24400/100000: episode: 244, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: -17.290, mean reward: -0.173 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.186, 10.358], loss: 0.002635, mae: 0.051388, mean_q: -0.334697
 24500/100000: episode: 245, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -18.482, mean reward: -0.185 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.168, 10.150], loss: 0.002716, mae: 0.052321, mean_q: -0.324607
 24600/100000: episode: 246, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -13.819, mean reward: -0.138 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.516, 10.394], loss: 0.002739, mae: 0.052218, mean_q: -0.310065
 24700/100000: episode: 247, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -15.612, mean reward: -0.156 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.108, 10.118], loss: 0.002563, mae: 0.050335, mean_q: -0.312370
 24800/100000: episode: 248, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -20.049, mean reward: -0.200 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.301, 10.154], loss: 0.002633, mae: 0.051301, mean_q: -0.315393
 24900/100000: episode: 249, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -19.241, mean reward: -0.192 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.594, 10.098], loss: 0.002671, mae: 0.052267, mean_q: -0.289132
 25000/100000: episode: 250, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -19.764, mean reward: -0.198 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.675, 10.145], loss: 0.002667, mae: 0.052677, mean_q: -0.313464
 25100/100000: episode: 251, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -17.124, mean reward: -0.171 [-1.000, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.630, 10.098], loss: 0.002581, mae: 0.050946, mean_q: -0.333260
 25200/100000: episode: 252, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.450, mean reward: -0.174 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.682, 10.107], loss: 0.002522, mae: 0.050776, mean_q: -0.299906
 25300/100000: episode: 253, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -19.653, mean reward: -0.197 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.459, 10.098], loss: 0.002590, mae: 0.051144, mean_q: -0.280515
 25400/100000: episode: 254, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -9.230, mean reward: -0.092 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.204, 10.578], loss: 0.002733, mae: 0.052235, mean_q: -0.290128
 25500/100000: episode: 255, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -15.104, mean reward: -0.151 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.018, 10.098], loss: 0.002640, mae: 0.050963, mean_q: -0.308534
 25600/100000: episode: 256, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: -19.159, mean reward: -0.192 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.678, 10.151], loss: 0.002657, mae: 0.051315, mean_q: -0.313736
 25700/100000: episode: 257, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -18.497, mean reward: -0.185 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.720, 10.098], loss: 0.002756, mae: 0.052588, mean_q: -0.300018
 25800/100000: episode: 258, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -19.531, mean reward: -0.195 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.077, 10.148], loss: 0.002525, mae: 0.050882, mean_q: -0.281470
 25900/100000: episode: 259, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -13.108, mean reward: -0.131 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.983, 10.242], loss: 0.002546, mae: 0.050145, mean_q: -0.328694
 26000/100000: episode: 260, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.690, mean reward: -0.197 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.630, 10.155], loss: 0.004500, mae: 0.062253, mean_q: -0.302926
 26100/100000: episode: 261, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -20.089, mean reward: -0.201 [-1.000, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.752, 10.114], loss: 0.003505, mae: 0.060651, mean_q: -0.314932
 26200/100000: episode: 262, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -17.360, mean reward: -0.174 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.448, 10.098], loss: 0.002569, mae: 0.051226, mean_q: -0.324789
 26300/100000: episode: 263, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -15.756, mean reward: -0.158 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.092, 10.098], loss: 0.002533, mae: 0.050860, mean_q: -0.292435
 26400/100000: episode: 264, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -20.093, mean reward: -0.201 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.703, 10.098], loss: 0.002555, mae: 0.050570, mean_q: -0.312241
 26500/100000: episode: 265, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -17.501, mean reward: -0.175 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.935, 10.098], loss: 0.005034, mae: 0.063695, mean_q: -0.347873
 26600/100000: episode: 266, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.432, mean reward: -0.174 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.249, 10.208], loss: 0.002427, mae: 0.049355, mean_q: -0.335738
 26700/100000: episode: 267, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -16.742, mean reward: -0.167 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.524, 10.200], loss: 0.002451, mae: 0.048889, mean_q: -0.294925
 26800/100000: episode: 268, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.497, mean reward: -0.185 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.072, 10.232], loss: 0.002468, mae: 0.048623, mean_q: -0.350368
 26900/100000: episode: 269, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -9.437, mean reward: -0.094 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.716, 10.488], loss: 0.002471, mae: 0.049110, mean_q: -0.304596
 27000/100000: episode: 270, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.917, mean reward: -0.169 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.270, 10.363], loss: 0.002447, mae: 0.049462, mean_q: -0.347017
 27100/100000: episode: 271, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -19.834, mean reward: -0.198 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.799, 10.098], loss: 0.004230, mae: 0.060460, mean_q: -0.305189
 27200/100000: episode: 272, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -14.078, mean reward: -0.141 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.397, 10.206], loss: 0.004500, mae: 0.063784, mean_q: -0.303350
 27300/100000: episode: 273, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -16.809, mean reward: -0.168 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.379, 10.098], loss: 0.002401, mae: 0.048697, mean_q: -0.367826
 27400/100000: episode: 274, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -12.032, mean reward: -0.120 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.301, 10.604], loss: 0.002639, mae: 0.051400, mean_q: -0.304733
 27500/100000: episode: 275, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -13.490, mean reward: -0.135 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.259, 10.117], loss: 0.002769, mae: 0.053011, mean_q: -0.311713
 27600/100000: episode: 276, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -16.281, mean reward: -0.163 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.544, 10.098], loss: 0.002470, mae: 0.049389, mean_q: -0.341699
 27700/100000: episode: 277, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.279, mean reward: -0.173 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.774, 10.098], loss: 0.002495, mae: 0.049456, mean_q: -0.316993
 27800/100000: episode: 278, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -18.673, mean reward: -0.187 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.372, 10.112], loss: 0.002562, mae: 0.050537, mean_q: -0.304472
 27900/100000: episode: 279, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -18.288, mean reward: -0.183 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.086, 10.098], loss: 0.002418, mae: 0.048878, mean_q: -0.320183
 28000/100000: episode: 280, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.531, mean reward: -0.195 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.378, 10.160], loss: 0.002725, mae: 0.052190, mean_q: -0.303529
 28100/100000: episode: 281, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -15.190, mean reward: -0.152 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.626, 10.098], loss: 0.002724, mae: 0.052773, mean_q: -0.329141
 28200/100000: episode: 282, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.767, mean reward: -0.168 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.775, 10.098], loss: 0.002393, mae: 0.049113, mean_q: -0.293331
 28300/100000: episode: 283, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.353, mean reward: -0.154 [-1.000, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.984, 10.098], loss: 0.002548, mae: 0.050338, mean_q: -0.296043
 28400/100000: episode: 284, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.537, mean reward: -0.185 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.888, 10.098], loss: 0.002804, mae: 0.052072, mean_q: -0.315239
 28500/100000: episode: 285, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -16.846, mean reward: -0.168 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.043, 10.203], loss: 0.002801, mae: 0.052673, mean_q: -0.318975
 28600/100000: episode: 286, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: -15.662, mean reward: -0.157 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.409, 10.212], loss: 0.002489, mae: 0.049642, mean_q: -0.322646
 28700/100000: episode: 287, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -19.252, mean reward: -0.193 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.847, 10.098], loss: 0.002511, mae: 0.050858, mean_q: -0.321626
 28800/100000: episode: 288, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -14.744, mean reward: -0.147 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.377, 10.098], loss: 0.002626, mae: 0.050385, mean_q: -0.350744
 28900/100000: episode: 289, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -18.151, mean reward: -0.182 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.702, 10.106], loss: 0.002523, mae: 0.049951, mean_q: -0.334274
 29000/100000: episode: 290, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -20.872, mean reward: -0.209 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.848, 10.135], loss: 0.002452, mae: 0.049077, mean_q: -0.312206
 29100/100000: episode: 291, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.390, mean reward: -0.164 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.801, 10.098], loss: 0.002434, mae: 0.049643, mean_q: -0.290432
 29200/100000: episode: 292, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -15.792, mean reward: -0.158 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.724, 10.435], loss: 0.002498, mae: 0.050536, mean_q: -0.322254
 29300/100000: episode: 293, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -19.718, mean reward: -0.197 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.476, 10.134], loss: 0.002529, mae: 0.050646, mean_q: -0.342143
 29400/100000: episode: 294, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.175, mean reward: -0.152 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.288, 10.123], loss: 0.002500, mae: 0.050209, mean_q: -0.341571
 29500/100000: episode: 295, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.314, mean reward: -0.183 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.085, 10.098], loss: 0.002754, mae: 0.052297, mean_q: -0.334770
 29600/100000: episode: 296, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -9.350, mean reward: -0.093 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.037, 10.098], loss: 0.002648, mae: 0.050947, mean_q: -0.311617
 29700/100000: episode: 297, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -18.465, mean reward: -0.185 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.569, 10.153], loss: 0.002544, mae: 0.049597, mean_q: -0.326983
 29800/100000: episode: 298, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -11.904, mean reward: -0.119 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.177, 10.504], loss: 0.002386, mae: 0.048048, mean_q: -0.340254
 29900/100000: episode: 299, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -12.615, mean reward: -0.126 [-1.000, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.641, 10.098], loss: 0.002956, mae: 0.053541, mean_q: -0.340812
 30000/100000: episode: 300, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -10.708, mean reward: -0.107 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.736, 10.194], loss: 0.005357, mae: 0.066773, mean_q: -0.298259
 30100/100000: episode: 301, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -17.938, mean reward: -0.179 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.463, 10.375], loss: 0.002626, mae: 0.051291, mean_q: -0.298262
 30200/100000: episode: 302, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -11.031, mean reward: -0.110 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-0.864, 10.098], loss: 0.002649, mae: 0.050761, mean_q: -0.324376
 30300/100000: episode: 303, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -16.768, mean reward: -0.168 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.557, 10.288], loss: 0.002606, mae: 0.051333, mean_q: -0.302413
 30400/100000: episode: 304, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.244, mean reward: -0.152 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.058, 10.173], loss: 0.002694, mae: 0.051071, mean_q: -0.286510
 30500/100000: episode: 305, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -13.036, mean reward: -0.130 [-1.000, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.445, 10.157], loss: 0.002634, mae: 0.049986, mean_q: -0.327678
 30600/100000: episode: 306, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -15.278, mean reward: -0.153 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.485, 10.236], loss: 0.002604, mae: 0.049506, mean_q: -0.342160
 30700/100000: episode: 307, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -19.980, mean reward: -0.200 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.653, 10.098], loss: 0.002584, mae: 0.050233, mean_q: -0.301181
 30800/100000: episode: 308, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -17.321, mean reward: -0.173 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.671, 10.186], loss: 0.002785, mae: 0.053097, mean_q: -0.310745
 30900/100000: episode: 309, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -20.590, mean reward: -0.206 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.583, 10.161], loss: 0.002784, mae: 0.051570, mean_q: -0.333280
 31000/100000: episode: 310, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.191, mean reward: -0.162 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.472, 10.261], loss: 0.002766, mae: 0.052432, mean_q: -0.306772
 31100/100000: episode: 311, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -13.046, mean reward: -0.130 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.444, 10.098], loss: 0.002494, mae: 0.048729, mean_q: -0.343445
 31200/100000: episode: 312, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.943, mean reward: -0.169 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.400, 10.098], loss: 0.002665, mae: 0.051091, mean_q: -0.335717
 31300/100000: episode: 313, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -18.842, mean reward: -0.188 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.549, 10.098], loss: 0.002548, mae: 0.050094, mean_q: -0.297780
 31400/100000: episode: 314, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -13.333, mean reward: -0.133 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.432, 10.098], loss: 0.002594, mae: 0.050939, mean_q: -0.336530
 31500/100000: episode: 315, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -16.973, mean reward: -0.170 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.472, 10.324], loss: 0.002632, mae: 0.051082, mean_q: -0.308544
 31600/100000: episode: 316, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -19.078, mean reward: -0.191 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.974, 10.098], loss: 0.002870, mae: 0.054791, mean_q: -0.317833
 31700/100000: episode: 317, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -14.910, mean reward: -0.149 [-1.000, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.613, 10.098], loss: 0.002789, mae: 0.053016, mean_q: -0.261442
 31800/100000: episode: 318, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -13.460, mean reward: -0.135 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.582, 10.273], loss: 0.002757, mae: 0.052439, mean_q: -0.296790
 31900/100000: episode: 319, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -16.052, mean reward: -0.161 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.440, 10.140], loss: 0.002783, mae: 0.053476, mean_q: -0.323647
 32000/100000: episode: 320, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -18.644, mean reward: -0.186 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.356, 10.126], loss: 0.002767, mae: 0.051821, mean_q: -0.293885
 32100/100000: episode: 321, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -19.620, mean reward: -0.196 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.907, 10.150], loss: 0.005734, mae: 0.070086, mean_q: -0.306464
 32200/100000: episode: 322, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -19.767, mean reward: -0.198 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.313, 10.098], loss: 0.003950, mae: 0.065273, mean_q: -0.279661
 32300/100000: episode: 323, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -20.414, mean reward: -0.204 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.411, 10.098], loss: 0.003052, mae: 0.056535, mean_q: -0.288524
 32400/100000: episode: 324, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -19.703, mean reward: -0.197 [-1.000, 0.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.906, 10.250], loss: 0.002887, mae: 0.054438, mean_q: -0.274996
 32500/100000: episode: 325, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -16.437, mean reward: -0.164 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.239, 10.098], loss: 0.002781, mae: 0.052815, mean_q: -0.320053
 32600/100000: episode: 326, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -19.118, mean reward: -0.191 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.657, 10.098], loss: 0.003014, mae: 0.055731, mean_q: -0.288155
 32700/100000: episode: 327, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -15.794, mean reward: -0.158 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.945, 10.098], loss: 0.002717, mae: 0.052774, mean_q: -0.287389
 32800/100000: episode: 328, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.326, mean reward: -0.183 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.965, 10.098], loss: 0.003467, mae: 0.058829, mean_q: -0.299302
 32900/100000: episode: 329, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -17.732, mean reward: -0.177 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.669, 10.171], loss: 0.002650, mae: 0.051160, mean_q: -0.300938
 33000/100000: episode: 330, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: -14.378, mean reward: -0.144 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.706, 10.240], loss: 0.002664, mae: 0.051219, mean_q: -0.283070
 33100/100000: episode: 331, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -20.377, mean reward: -0.204 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.154, 10.128], loss: 0.002615, mae: 0.050451, mean_q: -0.292706
 33200/100000: episode: 332, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.996, mean reward: -0.180 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.286, 10.195], loss: 0.003104, mae: 0.055647, mean_q: -0.323405
 33300/100000: episode: 333, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.609, mean reward: -0.186 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.528, 10.098], loss: 0.002687, mae: 0.050891, mean_q: -0.326532
 33400/100000: episode: 334, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -19.746, mean reward: -0.197 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.885, 10.098], loss: 0.002563, mae: 0.050661, mean_q: -0.308382
 33500/100000: episode: 335, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -21.420, mean reward: -0.214 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.605, 10.191], loss: 0.002605, mae: 0.051440, mean_q: -0.333433
 33600/100000: episode: 336, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -19.715, mean reward: -0.197 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.705, 10.098], loss: 0.002833, mae: 0.053471, mean_q: -0.326519
 33700/100000: episode: 337, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -15.504, mean reward: -0.155 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.075, 10.331], loss: 0.002530, mae: 0.050695, mean_q: -0.321196
 33800/100000: episode: 338, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.794, mean reward: -0.188 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.624, 10.253], loss: 0.002593, mae: 0.050336, mean_q: -0.344309
 33900/100000: episode: 339, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.690, mean reward: -0.167 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.297, 10.190], loss: 0.002561, mae: 0.050212, mean_q: -0.352934
 34000/100000: episode: 340, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.219, mean reward: -0.162 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.028, 10.098], loss: 0.002756, mae: 0.052318, mean_q: -0.293093
 34100/100000: episode: 341, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -19.792, mean reward: -0.198 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.867, 10.211], loss: 0.002967, mae: 0.054510, mean_q: -0.278478
 34200/100000: episode: 342, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -16.662, mean reward: -0.167 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.326, 10.144], loss: 0.002734, mae: 0.051915, mean_q: -0.320613
 34300/100000: episode: 343, duration: 0.593s, episode steps: 100, steps per second: 168, episode reward: -16.487, mean reward: -0.165 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.203, 10.239], loss: 0.002993, mae: 0.054792, mean_q: -0.298904
 34400/100000: episode: 344, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.997, mean reward: -0.160 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.854, 10.258], loss: 0.002863, mae: 0.054909, mean_q: -0.326871
 34500/100000: episode: 345, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.257, mean reward: -0.173 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.662, 10.131], loss: 0.002652, mae: 0.051868, mean_q: -0.352512
 34600/100000: episode: 346, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.880, mean reward: -0.189 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.428, 10.098], loss: 0.002802, mae: 0.052339, mean_q: -0.333156
 34700/100000: episode: 347, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -20.352, mean reward: -0.204 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.121, 10.099], loss: 0.002695, mae: 0.051690, mean_q: -0.297466
 34800/100000: episode: 348, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: -19.062, mean reward: -0.191 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.444, 10.098], loss: 0.002504, mae: 0.050266, mean_q: -0.318736
 34900/100000: episode: 349, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -19.899, mean reward: -0.199 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.055, 10.286], loss: 0.004306, mae: 0.065415, mean_q: -0.344349
 35000/100000: episode: 350, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -16.650, mean reward: -0.167 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.069, 10.098], loss: 0.003212, mae: 0.058151, mean_q: -0.345291
 35100/100000: episode: 351, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -10.642, mean reward: -0.106 [-1.000, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.702, 10.471], loss: 0.002703, mae: 0.052181, mean_q: -0.320398
 35200/100000: episode: 352, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -18.464, mean reward: -0.185 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.477, 10.221], loss: 0.002685, mae: 0.051364, mean_q: -0.341085
 35300/100000: episode: 353, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -14.488, mean reward: -0.145 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.733, 10.098], loss: 0.002622, mae: 0.051626, mean_q: -0.319305
 35400/100000: episode: 354, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: -18.571, mean reward: -0.186 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.288, 10.117], loss: 0.002800, mae: 0.052142, mean_q: -0.305368
 35500/100000: episode: 355, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -17.306, mean reward: -0.173 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.876, 10.098], loss: 0.002676, mae: 0.050681, mean_q: -0.334362
 35600/100000: episode: 356, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -18.500, mean reward: -0.185 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.066, 10.098], loss: 0.002470, mae: 0.048990, mean_q: -0.329360
 35700/100000: episode: 357, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.582, mean reward: -0.186 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.895, 10.409], loss: 0.002614, mae: 0.050441, mean_q: -0.308548
 35800/100000: episode: 358, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.735, mean reward: -0.187 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.421, 10.153], loss: 0.002647, mae: 0.051249, mean_q: -0.311898
 35900/100000: episode: 359, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -12.854, mean reward: -0.129 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.070, 10.261], loss: 0.002557, mae: 0.049882, mean_q: -0.336621
 36000/100000: episode: 360, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -18.889, mean reward: -0.189 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.905, 10.145], loss: 0.002727, mae: 0.052142, mean_q: -0.310071
 36100/100000: episode: 361, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.654, mean reward: -0.177 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.021, 10.130], loss: 0.002555, mae: 0.049720, mean_q: -0.307171
 36200/100000: episode: 362, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.275, mean reward: -0.143 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.479, 10.328], loss: 0.002723, mae: 0.051479, mean_q: -0.336334
 36300/100000: episode: 363, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -13.764, mean reward: -0.138 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.255, 10.358], loss: 0.002647, mae: 0.050837, mean_q: -0.339664
 36400/100000: episode: 364, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -12.477, mean reward: -0.125 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.405, 10.154], loss: 0.002604, mae: 0.051029, mean_q: -0.344049
 36500/100000: episode: 365, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -15.304, mean reward: -0.153 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.115, 10.098], loss: 0.002731, mae: 0.051818, mean_q: -0.311394
 36600/100000: episode: 366, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.240, mean reward: -0.162 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.591, 10.098], loss: 0.002778, mae: 0.052629, mean_q: -0.302873
 36700/100000: episode: 367, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -11.094, mean reward: -0.111 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.894, 10.500], loss: 0.002954, mae: 0.054582, mean_q: -0.305195
 36800/100000: episode: 368, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -13.352, mean reward: -0.134 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.447, 10.098], loss: 0.002756, mae: 0.052845, mean_q: -0.313675
 36900/100000: episode: 369, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -18.029, mean reward: -0.180 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.460, 10.098], loss: 0.002916, mae: 0.053211, mean_q: -0.344496
 37000/100000: episode: 370, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -18.555, mean reward: -0.186 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.182, 10.098], loss: 0.002754, mae: 0.052579, mean_q: -0.314416
 37100/100000: episode: 371, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.863, mean reward: -0.189 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.909, 10.098], loss: 0.002634, mae: 0.051411, mean_q: -0.312027
 37200/100000: episode: 372, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -19.089, mean reward: -0.191 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.258, 10.186], loss: 0.003056, mae: 0.056647, mean_q: -0.306220
 37300/100000: episode: 373, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -14.726, mean reward: -0.147 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.691, 10.113], loss: 0.008378, mae: 0.075798, mean_q: -0.338972
 37400/100000: episode: 374, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.705, mean reward: -0.167 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.299, 10.098], loss: 0.004665, mae: 0.064120, mean_q: -0.312998
 37500/100000: episode: 375, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -17.140, mean reward: -0.171 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.363, 10.218], loss: 0.002916, mae: 0.054805, mean_q: -0.316038
 37600/100000: episode: 376, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -16.127, mean reward: -0.161 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.897, 10.098], loss: 0.002811, mae: 0.053069, mean_q: -0.330417
 37700/100000: episode: 377, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -19.747, mean reward: -0.197 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.005, 10.193], loss: 0.003603, mae: 0.057693, mean_q: -0.334369
 37800/100000: episode: 378, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -16.423, mean reward: -0.164 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.867, 10.218], loss: 0.002655, mae: 0.051224, mean_q: -0.342435
 37900/100000: episode: 379, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -17.383, mean reward: -0.174 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.480, 10.112], loss: 0.002716, mae: 0.051993, mean_q: -0.350945
 38000/100000: episode: 380, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -19.819, mean reward: -0.198 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.211, 10.131], loss: 0.002690, mae: 0.052412, mean_q: -0.338163
 38100/100000: episode: 381, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -11.731, mean reward: -0.117 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.046, 10.210], loss: 0.002588, mae: 0.049608, mean_q: -0.329716
 38200/100000: episode: 382, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -15.909, mean reward: -0.159 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.648, 10.288], loss: 0.002837, mae: 0.053130, mean_q: -0.296332
 38300/100000: episode: 383, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -18.210, mean reward: -0.182 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.014, 10.309], loss: 0.002467, mae: 0.048580, mean_q: -0.350721
 38400/100000: episode: 384, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.759, mean reward: -0.178 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.697, 10.251], loss: 0.002495, mae: 0.050328, mean_q: -0.297754
 38500/100000: episode: 385, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -16.564, mean reward: -0.166 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.632, 10.098], loss: 0.002703, mae: 0.052410, mean_q: -0.305770
 38600/100000: episode: 386, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.125, mean reward: -0.171 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.686, 10.098], loss: 0.003102, mae: 0.056002, mean_q: -0.315523
 38700/100000: episode: 387, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.449, mean reward: -0.174 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.896, 10.098], loss: 0.002679, mae: 0.052191, mean_q: -0.306502
 38800/100000: episode: 388, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -18.912, mean reward: -0.189 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.124, 10.098], loss: 0.002798, mae: 0.053283, mean_q: -0.323716
 38900/100000: episode: 389, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -16.295, mean reward: -0.163 [-1.000, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.463, 10.226], loss: 0.002773, mae: 0.052432, mean_q: -0.283460
 39000/100000: episode: 390, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -15.488, mean reward: -0.155 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.840, 10.436], loss: 0.002569, mae: 0.049691, mean_q: -0.335488
 39100/100000: episode: 391, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -14.059, mean reward: -0.141 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.384, 10.098], loss: 0.002956, mae: 0.054057, mean_q: -0.295205
 39200/100000: episode: 392, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: -19.882, mean reward: -0.199 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.533, 10.098], loss: 0.002575, mae: 0.049622, mean_q: -0.332812
 39300/100000: episode: 393, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.171, mean reward: -0.162 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.098, 10.098], loss: 0.002876, mae: 0.053134, mean_q: -0.299262
 39400/100000: episode: 394, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -19.682, mean reward: -0.197 [-1.000, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.494, 10.230], loss: 0.003483, mae: 0.058475, mean_q: -0.305644
 39500/100000: episode: 395, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: -17.146, mean reward: -0.171 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.458, 10.126], loss: 0.002646, mae: 0.051606, mean_q: -0.308885
 39600/100000: episode: 396, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: -18.719, mean reward: -0.187 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.684, 10.190], loss: 0.002606, mae: 0.050550, mean_q: -0.295190
 39700/100000: episode: 397, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.359, mean reward: -0.184 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.468, 10.145], loss: 0.002891, mae: 0.053572, mean_q: -0.325420
 39800/100000: episode: 398, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -13.805, mean reward: -0.138 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.260, 10.098], loss: 0.002859, mae: 0.053849, mean_q: -0.290104
 39900/100000: episode: 399, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -17.384, mean reward: -0.174 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.868, 10.133], loss: 0.002597, mae: 0.049754, mean_q: -0.354338
 40000/100000: episode: 400, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -16.548, mean reward: -0.165 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.256, 10.276], loss: 0.002825, mae: 0.052479, mean_q: -0.303360
 40100/100000: episode: 401, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -13.442, mean reward: -0.134 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.653, 10.103], loss: 0.002610, mae: 0.049759, mean_q: -0.315787
 40200/100000: episode: 402, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -18.317, mean reward: -0.183 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.713, 10.098], loss: 0.002951, mae: 0.054960, mean_q: -0.305887
 40300/100000: episode: 403, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -19.625, mean reward: -0.196 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.927, 10.098], loss: 0.004153, mae: 0.061928, mean_q: -0.316113
 40400/100000: episode: 404, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -15.523, mean reward: -0.155 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.171, 10.381], loss: 0.003434, mae: 0.060250, mean_q: -0.313154
 40500/100000: episode: 405, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -6.431, mean reward: -0.064 [-1.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.093, 10.412], loss: 0.002692, mae: 0.052865, mean_q: -0.292541
 40600/100000: episode: 406, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.148, mean reward: -0.181 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.324, 10.098], loss: 0.002608, mae: 0.051209, mean_q: -0.299042
 40700/100000: episode: 407, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -15.765, mean reward: -0.158 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.382, 10.103], loss: 0.002534, mae: 0.050735, mean_q: -0.280672
 40800/100000: episode: 408, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -19.011, mean reward: -0.190 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.851, 10.098], loss: 0.002474, mae: 0.048734, mean_q: -0.323917
 40900/100000: episode: 409, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -14.416, mean reward: -0.144 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.904, 10.101], loss: 0.002940, mae: 0.053840, mean_q: -0.318621
 41000/100000: episode: 410, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.563, mean reward: -0.186 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.434, 10.098], loss: 0.002431, mae: 0.049162, mean_q: -0.308870
 41100/100000: episode: 411, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -14.984, mean reward: -0.150 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.175, 10.098], loss: 0.002618, mae: 0.051093, mean_q: -0.322511
 41200/100000: episode: 412, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -15.030, mean reward: -0.150 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.039, 10.098], loss: 0.002888, mae: 0.055007, mean_q: -0.293986
 41300/100000: episode: 413, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -20.000, mean reward: -0.200 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.606, 10.098], loss: 0.002767, mae: 0.051502, mean_q: -0.336856
 41400/100000: episode: 414, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -15.590, mean reward: -0.156 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.820, 10.098], loss: 0.002444, mae: 0.049020, mean_q: -0.334699
 41500/100000: episode: 415, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -15.647, mean reward: -0.156 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.749, 10.169], loss: 0.002444, mae: 0.049473, mean_q: -0.288263
 41600/100000: episode: 416, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -20.025, mean reward: -0.200 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.555, 10.098], loss: 0.003552, mae: 0.054409, mean_q: -0.318699
 41700/100000: episode: 417, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.628, mean reward: -0.186 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.514, 10.124], loss: 0.007299, mae: 0.074826, mean_q: -0.326930
 41800/100000: episode: 418, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -19.165, mean reward: -0.192 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.721, 10.098], loss: 0.003253, mae: 0.058438, mean_q: -0.323448
 41900/100000: episode: 419, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -19.175, mean reward: -0.192 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.448, 10.157], loss: 0.003485, mae: 0.057831, mean_q: -0.349595
 42000/100000: episode: 420, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -19.282, mean reward: -0.193 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.986, 10.414], loss: 0.002696, mae: 0.052590, mean_q: -0.313667
 42100/100000: episode: 421, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -17.603, mean reward: -0.176 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.905, 10.098], loss: 0.002734, mae: 0.051669, mean_q: -0.332715
 42200/100000: episode: 422, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.495, mean reward: -0.135 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.526, 10.098], loss: 0.002669, mae: 0.051497, mean_q: -0.328321
 42300/100000: episode: 423, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.223, mean reward: -0.182 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.699, 10.098], loss: 0.002775, mae: 0.052518, mean_q: -0.334644
 42400/100000: episode: 424, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.621, mean reward: -0.166 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.170, 10.293], loss: 0.002584, mae: 0.049890, mean_q: -0.314018
 42500/100000: episode: 425, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.085, mean reward: -0.121 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.478, 10.098], loss: 0.002303, mae: 0.047962, mean_q: -0.331683
 42600/100000: episode: 426, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -17.669, mean reward: -0.177 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.659, 10.098], loss: 0.002595, mae: 0.050687, mean_q: -0.307094
 42700/100000: episode: 427, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -19.098, mean reward: -0.191 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.214, 10.124], loss: 0.002553, mae: 0.049966, mean_q: -0.327200
 42800/100000: episode: 428, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.176, mean reward: -0.182 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.854, 10.142], loss: 0.002737, mae: 0.051466, mean_q: -0.327563
 42900/100000: episode: 429, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -17.608, mean reward: -0.176 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.688, 10.294], loss: 0.002824, mae: 0.052289, mean_q: -0.296198
 43000/100000: episode: 430, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -12.556, mean reward: -0.126 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.688, 10.274], loss: 0.002647, mae: 0.050699, mean_q: -0.335215
 43100/100000: episode: 431, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.324, mean reward: -0.173 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.293, 10.098], loss: 0.002727, mae: 0.051324, mean_q: -0.296968
 43200/100000: episode: 432, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -15.803, mean reward: -0.158 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.303, 10.190], loss: 0.002725, mae: 0.052459, mean_q: -0.341379
 43300/100000: episode: 433, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -17.025, mean reward: -0.170 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.538, 10.098], loss: 0.002730, mae: 0.052417, mean_q: -0.292480
 43400/100000: episode: 434, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -16.893, mean reward: -0.169 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.979, 10.098], loss: 0.002777, mae: 0.053142, mean_q: -0.303243
 43500/100000: episode: 435, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -12.297, mean reward: -0.123 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.711, 10.276], loss: 0.002830, mae: 0.052031, mean_q: -0.307763
 43600/100000: episode: 436, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -18.734, mean reward: -0.187 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.568, 10.273], loss: 0.002764, mae: 0.052962, mean_q: -0.312691
 43700/100000: episode: 437, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -19.025, mean reward: -0.190 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.907, 10.223], loss: 0.002738, mae: 0.052924, mean_q: -0.307397
 43800/100000: episode: 438, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.806, mean reward: -0.188 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.912, 10.098], loss: 0.002966, mae: 0.055882, mean_q: -0.313984
 43900/100000: episode: 439, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -16.798, mean reward: -0.168 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.952, 10.098], loss: 0.002628, mae: 0.051302, mean_q: -0.325788
 44000/100000: episode: 440, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.101, mean reward: -0.171 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.610, 10.098], loss: 0.002538, mae: 0.049609, mean_q: -0.339458
 44100/100000: episode: 441, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.904, mean reward: -0.179 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.106, 10.142], loss: 0.004746, mae: 0.066426, mean_q: -0.294474
 44200/100000: episode: 442, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -19.911, mean reward: -0.199 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.695, 10.203], loss: 0.002823, mae: 0.055119, mean_q: -0.315741
 44300/100000: episode: 443, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -18.700, mean reward: -0.187 [-1.000, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.614, 10.217], loss: 0.002885, mae: 0.053152, mean_q: -0.328555
 44400/100000: episode: 444, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -20.112, mean reward: -0.201 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.345, 10.098], loss: 0.002623, mae: 0.050962, mean_q: -0.304354
 44500/100000: episode: 445, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.986, mean reward: -0.180 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.733, 10.098], loss: 0.002443, mae: 0.048530, mean_q: -0.332211
 44600/100000: episode: 446, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.673, mean reward: -0.157 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.889, 10.100], loss: 0.002440, mae: 0.049031, mean_q: -0.308738
 44700/100000: episode: 447, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -17.850, mean reward: -0.179 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.309, 10.098], loss: 0.002699, mae: 0.051471, mean_q: -0.342891
 44800/100000: episode: 448, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.813, mean reward: -0.188 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.064, 10.098], loss: 0.002515, mae: 0.049474, mean_q: -0.324081
 44900/100000: episode: 449, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.535, mean reward: -0.165 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.014, 10.213], loss: 0.002553, mae: 0.049948, mean_q: -0.346689
 45000/100000: episode: 450, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -20.926, mean reward: -0.209 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.262, 10.168], loss: 0.002714, mae: 0.053179, mean_q: -0.300266
 45100/100000: episode: 451, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.966, mean reward: -0.190 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.100, 10.098], loss: 0.002435, mae: 0.048342, mean_q: -0.367187
 45200/100000: episode: 452, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.623, mean reward: -0.186 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.841, 10.136], loss: 0.002521, mae: 0.048765, mean_q: -0.328201
 45300/100000: episode: 453, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -16.676, mean reward: -0.167 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.197, 10.121], loss: 0.002446, mae: 0.048710, mean_q: -0.355219
 45400/100000: episode: 454, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -12.069, mean reward: -0.121 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.383, 10.443], loss: 0.003053, mae: 0.056759, mean_q: -0.349395
 45500/100000: episode: 455, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -14.876, mean reward: -0.149 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.926, 10.284], loss: 0.002442, mae: 0.048395, mean_q: -0.325708
 45600/100000: episode: 456, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -12.086, mean reward: -0.121 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.263, 10.131], loss: 0.002575, mae: 0.050182, mean_q: -0.300589
 45700/100000: episode: 457, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -16.267, mean reward: -0.163 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.805, 10.308], loss: 0.002584, mae: 0.050058, mean_q: -0.316417
 45800/100000: episode: 458, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -13.841, mean reward: -0.138 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.521, 10.261], loss: 0.002666, mae: 0.051894, mean_q: -0.311451
 45900/100000: episode: 459, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.353, mean reward: -0.174 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.776, 10.098], loss: 0.002464, mae: 0.049839, mean_q: -0.325156
 46000/100000: episode: 460, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -15.388, mean reward: -0.154 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.949, 10.159], loss: 0.002508, mae: 0.049406, mean_q: -0.334191
 46100/100000: episode: 461, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -15.974, mean reward: -0.160 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.047, 10.126], loss: 0.002517, mae: 0.048355, mean_q: -0.353488
 46200/100000: episode: 462, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -13.027, mean reward: -0.130 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.084, 10.098], loss: 0.002506, mae: 0.049226, mean_q: -0.329410
 46300/100000: episode: 463, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -15.766, mean reward: -0.158 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.551, 10.098], loss: 0.002520, mae: 0.049743, mean_q: -0.330249
 46400/100000: episode: 464, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -15.837, mean reward: -0.158 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.507, 10.098], loss: 0.002663, mae: 0.050866, mean_q: -0.337293
 46500/100000: episode: 465, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -15.647, mean reward: -0.156 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.418, 10.098], loss: 0.002647, mae: 0.050450, mean_q: -0.333487
 46600/100000: episode: 466, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -13.757, mean reward: -0.138 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.632, 10.098], loss: 0.002625, mae: 0.050004, mean_q: -0.330790
 46700/100000: episode: 467, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -12.653, mean reward: -0.127 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.972, 10.098], loss: 0.002807, mae: 0.052974, mean_q: -0.295415
 46800/100000: episode: 468, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -10.465, mean reward: -0.105 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.887, 10.098], loss: 0.002563, mae: 0.050547, mean_q: -0.294467
 46900/100000: episode: 469, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -18.440, mean reward: -0.184 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.839, 10.325], loss: 0.002591, mae: 0.049830, mean_q: -0.272007
 47000/100000: episode: 470, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.506, mean reward: -0.185 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.300, 10.098], loss: 0.002499, mae: 0.049978, mean_q: -0.310524
 47100/100000: episode: 471, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -19.617, mean reward: -0.196 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.830, 10.221], loss: 0.002485, mae: 0.049632, mean_q: -0.308380
 47200/100000: episode: 472, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -13.888, mean reward: -0.139 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.424, 10.325], loss: 0.003327, mae: 0.059139, mean_q: -0.304400
 47300/100000: episode: 473, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -20.275, mean reward: -0.203 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.817, 10.186], loss: 0.002421, mae: 0.050482, mean_q: -0.329671
 47400/100000: episode: 474, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.984, mean reward: -0.170 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.593, 10.144], loss: 0.003128, mae: 0.056661, mean_q: -0.284943
 47500/100000: episode: 475, duration: 0.608s, episode steps: 100, steps per second: 165, episode reward: -15.208, mean reward: -0.152 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.237, 10.098], loss: 0.002302, mae: 0.047694, mean_q: -0.346202
 47600/100000: episode: 476, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -18.379, mean reward: -0.184 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.239, 10.098], loss: 0.002448, mae: 0.048790, mean_q: -0.335211
 47700/100000: episode: 477, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -14.714, mean reward: -0.147 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.614, 10.455], loss: 0.002330, mae: 0.047247, mean_q: -0.338411
 47800/100000: episode: 478, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.379, mean reward: -0.174 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.227, 10.190], loss: 0.002406, mae: 0.048003, mean_q: -0.316718
 47900/100000: episode: 479, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -8.940, mean reward: -0.089 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.438, 10.322], loss: 0.002527, mae: 0.049779, mean_q: -0.294936
 48000/100000: episode: 480, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -19.391, mean reward: -0.194 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.193, 10.156], loss: 0.002479, mae: 0.049244, mean_q: -0.315937
 48100/100000: episode: 481, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.628, mean reward: -0.186 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.069, 10.098], loss: 0.002662, mae: 0.051579, mean_q: -0.302104
 48200/100000: episode: 482, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -19.987, mean reward: -0.200 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.857, 10.210], loss: 0.002661, mae: 0.051904, mean_q: -0.283389
 48300/100000: episode: 483, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -18.725, mean reward: -0.187 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.118, 10.098], loss: 0.002386, mae: 0.048506, mean_q: -0.316013
 48400/100000: episode: 484, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -14.349, mean reward: -0.143 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.588, 10.098], loss: 0.002348, mae: 0.048197, mean_q: -0.313094
 48500/100000: episode: 485, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -10.500, mean reward: -0.105 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.477, 10.360], loss: 0.002570, mae: 0.051356, mean_q: -0.287597
 48600/100000: episode: 486, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -19.277, mean reward: -0.193 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.745, 10.152], loss: 0.003332, mae: 0.053887, mean_q: -0.294095
 48700/100000: episode: 487, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.825, mean reward: -0.188 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.579, 10.098], loss: 0.002574, mae: 0.051475, mean_q: -0.313093
 48800/100000: episode: 488, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -9.881, mean reward: -0.099 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.564, 10.322], loss: 0.002520, mae: 0.049465, mean_q: -0.318357
 48900/100000: episode: 489, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -14.002, mean reward: -0.140 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.888, 10.134], loss: 0.002493, mae: 0.049706, mean_q: -0.293315
 49000/100000: episode: 490, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.704, mean reward: -0.187 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.247, 10.098], loss: 0.002116, mae: 0.045388, mean_q: -0.312448
 49100/100000: episode: 491, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -16.848, mean reward: -0.168 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.673, 10.098], loss: 0.002236, mae: 0.048165, mean_q: -0.330704
 49200/100000: episode: 492, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.463, mean reward: -0.175 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.972, 10.127], loss: 0.002469, mae: 0.049922, mean_q: -0.296432
 49300/100000: episode: 493, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.724, mean reward: -0.177 [-1.000, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.198, 10.298], loss: 0.002274, mae: 0.047533, mean_q: -0.326293
 49400/100000: episode: 494, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -18.488, mean reward: -0.185 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.818, 10.098], loss: 0.002460, mae: 0.049047, mean_q: -0.328007
 49500/100000: episode: 495, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -16.309, mean reward: -0.163 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.595, 10.098], loss: 0.002358, mae: 0.047907, mean_q: -0.317336
 49600/100000: episode: 496, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -19.052, mean reward: -0.191 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.961, 10.206], loss: 0.004530, mae: 0.065982, mean_q: -0.287966
 49700/100000: episode: 497, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -14.649, mean reward: -0.146 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.438, 10.267], loss: 0.002819, mae: 0.054510, mean_q: -0.295328
 49800/100000: episode: 498, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -19.094, mean reward: -0.191 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.252, 10.098], loss: 0.002488, mae: 0.050383, mean_q: -0.287711
 49900/100000: episode: 499, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -15.987, mean reward: -0.160 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.602, 10.098], loss: 0.002499, mae: 0.050254, mean_q: -0.305425
 50000/100000: episode: 500, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -18.940, mean reward: -0.189 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.318, 10.098], loss: 0.002527, mae: 0.049198, mean_q: -0.304268
 50100/100000: episode: 501, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -18.772, mean reward: -0.188 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.941, 10.194], loss: 0.002508, mae: 0.049591, mean_q: -0.293655
 50200/100000: episode: 502, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -14.504, mean reward: -0.145 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.069, 10.098], loss: 0.002395, mae: 0.048082, mean_q: -0.305285
 50300/100000: episode: 503, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -19.403, mean reward: -0.194 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.600, 10.180], loss: 0.002480, mae: 0.050278, mean_q: -0.295705
 50400/100000: episode: 504, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.712, mean reward: -0.157 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.119, 10.098], loss: 0.002479, mae: 0.049551, mean_q: -0.283312
 50500/100000: episode: 505, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -16.253, mean reward: -0.163 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.768, 10.155], loss: 0.002550, mae: 0.050375, mean_q: -0.304463
 50600/100000: episode: 506, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -18.197, mean reward: -0.182 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.168, 10.098], loss: 0.002381, mae: 0.048564, mean_q: -0.272139
 50700/100000: episode: 507, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -18.594, mean reward: -0.186 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.187, 10.098], loss: 0.002477, mae: 0.048428, mean_q: -0.329258
 50800/100000: episode: 508, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -17.327, mean reward: -0.173 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.821, 10.182], loss: 0.002386, mae: 0.049194, mean_q: -0.299997
 50900/100000: episode: 509, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -17.020, mean reward: -0.170 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.996, 10.098], loss: 0.002589, mae: 0.051876, mean_q: -0.326784
 51000/100000: episode: 510, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -14.369, mean reward: -0.144 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.112, 10.098], loss: 0.003268, mae: 0.057887, mean_q: -0.315191
 51100/100000: episode: 511, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -20.862, mean reward: -0.209 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.695, 10.168], loss: 0.002566, mae: 0.049824, mean_q: -0.347077
 51200/100000: episode: 512, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.123, mean reward: -0.171 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.265, 10.182], loss: 0.002561, mae: 0.050464, mean_q: -0.288277
 51300/100000: episode: 513, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.539, mean reward: -0.185 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.874, 10.098], loss: 0.002529, mae: 0.050811, mean_q: -0.287603
 51400/100000: episode: 514, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.402, mean reward: -0.174 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.355, 10.098], loss: 0.002417, mae: 0.048275, mean_q: -0.298109
 51500/100000: episode: 515, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -18.247, mean reward: -0.182 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.811, 10.228], loss: 0.002442, mae: 0.048207, mean_q: -0.338352
 51600/100000: episode: 516, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -14.825, mean reward: -0.148 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.021, 10.098], loss: 0.002471, mae: 0.048445, mean_q: -0.305995
 51700/100000: episode: 517, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.162, mean reward: -0.172 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.477, 10.102], loss: 0.002497, mae: 0.049291, mean_q: -0.320349
 51800/100000: episode: 518, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.402, mean reward: -0.174 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.338, 10.098], loss: 0.003242, mae: 0.056124, mean_q: -0.333858
 51900/100000: episode: 519, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -17.412, mean reward: -0.174 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.576, 10.098], loss: 0.004124, mae: 0.063731, mean_q: -0.312293
 52000/100000: episode: 520, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -19.344, mean reward: -0.193 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.895, 10.098], loss: 0.002532, mae: 0.051221, mean_q: -0.353982
 52100/100000: episode: 521, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.155, mean reward: -0.172 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.677, 10.098], loss: 0.002416, mae: 0.048621, mean_q: -0.325928
 52200/100000: episode: 522, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -17.372, mean reward: -0.174 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.504, 10.261], loss: 0.002427, mae: 0.048885, mean_q: -0.334266
 52300/100000: episode: 523, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -16.042, mean reward: -0.160 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.081, 10.098], loss: 0.002289, mae: 0.047553, mean_q: -0.324413
 52400/100000: episode: 524, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.490, mean reward: -0.185 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.648, 10.098], loss: 0.002548, mae: 0.048546, mean_q: -0.343990
 52500/100000: episode: 525, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -14.150, mean reward: -0.142 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.317, 10.125], loss: 0.002470, mae: 0.048339, mean_q: -0.322044
 52600/100000: episode: 526, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -17.007, mean reward: -0.170 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.759, 10.098], loss: 0.002301, mae: 0.047887, mean_q: -0.338528
 52700/100000: episode: 527, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -16.705, mean reward: -0.167 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.397, 10.168], loss: 0.002549, mae: 0.049723, mean_q: -0.333631
 52800/100000: episode: 528, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -14.817, mean reward: -0.148 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.493, 10.277], loss: 0.002372, mae: 0.046827, mean_q: -0.343640
 52900/100000: episode: 529, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -14.696, mean reward: -0.147 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.860, 10.235], loss: 0.003020, mae: 0.054936, mean_q: -0.336618
 53000/100000: episode: 530, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.152, mean reward: -0.172 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.283, 10.187], loss: 0.002651, mae: 0.051469, mean_q: -0.299155
 53100/100000: episode: 531, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -18.226, mean reward: -0.182 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.225, 10.209], loss: 0.002558, mae: 0.049448, mean_q: -0.301631
 53200/100000: episode: 532, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.606, mean reward: -0.166 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.542, 10.187], loss: 0.002740, mae: 0.053462, mean_q: -0.312027
 53300/100000: episode: 533, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.545, mean reward: -0.175 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.235, 10.167], loss: 0.002467, mae: 0.049202, mean_q: -0.305832
 53400/100000: episode: 534, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -19.062, mean reward: -0.191 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.589, 10.098], loss: 0.002343, mae: 0.048020, mean_q: -0.299883
 53500/100000: episode: 535, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.591, mean reward: -0.156 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.039, 10.269], loss: 0.002342, mae: 0.047336, mean_q: -0.339467
 53600/100000: episode: 536, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -9.322, mean reward: -0.093 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.859, 10.470], loss: 0.002419, mae: 0.047557, mean_q: -0.315317
 53700/100000: episode: 537, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -15.405, mean reward: -0.154 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.228, 10.098], loss: 0.002453, mae: 0.048697, mean_q: -0.293182
 53800/100000: episode: 538, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -13.988, mean reward: -0.140 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.706, 10.098], loss: 0.002629, mae: 0.049982, mean_q: -0.300013
 53900/100000: episode: 539, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.316, mean reward: -0.153 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.763, 10.217], loss: 0.002505, mae: 0.048796, mean_q: -0.334691
 54000/100000: episode: 540, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -15.551, mean reward: -0.156 [-1.000, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.682, 10.098], loss: 0.002510, mae: 0.049376, mean_q: -0.290394
 54100/100000: episode: 541, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -10.915, mean reward: -0.109 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.975, 10.254], loss: 0.002406, mae: 0.048246, mean_q: -0.331190
 54200/100000: episode: 542, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -17.273, mean reward: -0.173 [-1.000, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.110, 10.153], loss: 0.002413, mae: 0.048298, mean_q: -0.321530
 54300/100000: episode: 543, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -20.056, mean reward: -0.201 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.498, 10.098], loss: 0.002390, mae: 0.047992, mean_q: -0.298155
 54400/100000: episode: 544, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -12.857, mean reward: -0.129 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.325, 10.180], loss: 0.002400, mae: 0.048296, mean_q: -0.298100
 54500/100000: episode: 545, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.792, mean reward: -0.188 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.467, 10.098], loss: 0.002517, mae: 0.048499, mean_q: -0.299208
 54600/100000: episode: 546, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.921, mean reward: -0.189 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.231, 10.116], loss: 0.002469, mae: 0.048716, mean_q: -0.320340
 54700/100000: episode: 547, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.953, mean reward: -0.170 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.611, 10.098], loss: 0.002544, mae: 0.049981, mean_q: -0.284103
 54800/100000: episode: 548, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -17.969, mean reward: -0.180 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.316, 10.125], loss: 0.003048, mae: 0.054797, mean_q: -0.324205
 54900/100000: episode: 549, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.647, mean reward: -0.176 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.648, 10.098], loss: 0.003090, mae: 0.052101, mean_q: -0.298132
 55000/100000: episode: 550, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -14.429, mean reward: -0.144 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.898, 10.098], loss: 0.002652, mae: 0.050823, mean_q: -0.300871
 55100/100000: episode: 551, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -19.894, mean reward: -0.199 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.718, 10.136], loss: 0.002490, mae: 0.049555, mean_q: -0.281705
 55200/100000: episode: 552, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -13.693, mean reward: -0.137 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.793, 10.098], loss: 0.004198, mae: 0.056633, mean_q: -0.301953
 55300/100000: episode: 553, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -16.201, mean reward: -0.162 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.290, 10.184], loss: 0.005507, mae: 0.067938, mean_q: -0.308840
 55400/100000: episode: 554, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -17.383, mean reward: -0.174 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.573, 10.160], loss: 0.002603, mae: 0.052297, mean_q: -0.320727
 55500/100000: episode: 555, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -18.906, mean reward: -0.189 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.636, 10.098], loss: 0.002726, mae: 0.052273, mean_q: -0.311086
 55600/100000: episode: 556, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -17.640, mean reward: -0.176 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.745, 10.098], loss: 0.002596, mae: 0.050271, mean_q: -0.303616
 55700/100000: episode: 557, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -19.416, mean reward: -0.194 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.511, 10.098], loss: 0.002399, mae: 0.047826, mean_q: -0.300803
 55800/100000: episode: 558, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -14.517, mean reward: -0.145 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.729, 10.277], loss: 0.002464, mae: 0.048420, mean_q: -0.318737
 55900/100000: episode: 559, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.637, mean reward: -0.166 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.034, 10.191], loss: 0.002353, mae: 0.047884, mean_q: -0.320046
 56000/100000: episode: 560, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -16.603, mean reward: -0.166 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.920, 10.098], loss: 0.002562, mae: 0.049332, mean_q: -0.327085
 56100/100000: episode: 561, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -17.945, mean reward: -0.179 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.347, 10.135], loss: 0.002325, mae: 0.046877, mean_q: -0.357899
 56200/100000: episode: 562, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -11.121, mean reward: -0.111 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.607, 10.098], loss: 0.002528, mae: 0.049542, mean_q: -0.300433
 56300/100000: episode: 563, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -7.431, mean reward: -0.074 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.218, 10.098], loss: 0.002517, mae: 0.049651, mean_q: -0.314704
 56400/100000: episode: 564, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -14.656, mean reward: -0.147 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.168, 10.098], loss: 0.002557, mae: 0.049982, mean_q: -0.266907
 56500/100000: episode: 565, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -16.731, mean reward: -0.167 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.970, 10.098], loss: 0.002407, mae: 0.048315, mean_q: -0.322764
 56600/100000: episode: 566, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.411, mean reward: -0.184 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.333, 10.098], loss: 0.002424, mae: 0.048810, mean_q: -0.285576
 56700/100000: episode: 567, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -13.758, mean reward: -0.138 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.931, 10.210], loss: 0.002656, mae: 0.050475, mean_q: -0.304802
 56800/100000: episode: 568, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -17.471, mean reward: -0.175 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.571, 10.098], loss: 0.002670, mae: 0.050778, mean_q: -0.288095
 56900/100000: episode: 569, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -16.504, mean reward: -0.165 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.591, 10.316], loss: 0.002368, mae: 0.047496, mean_q: -0.282137
 57000/100000: episode: 570, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -17.109, mean reward: -0.171 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.029, 10.098], loss: 0.004036, mae: 0.058899, mean_q: -0.309259
 57100/100000: episode: 571, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.586, mean reward: -0.176 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.645, 10.197], loss: 0.005343, mae: 0.065182, mean_q: -0.305249
 57200/100000: episode: 572, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -14.755, mean reward: -0.148 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.195, 10.125], loss: 0.002925, mae: 0.054632, mean_q: -0.287953
 57300/100000: episode: 573, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.246, mean reward: -0.182 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.934, 10.114], loss: 0.002580, mae: 0.050780, mean_q: -0.289226
 57400/100000: episode: 574, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -18.731, mean reward: -0.187 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.697, 10.211], loss: 0.002438, mae: 0.048499, mean_q: -0.303835
 57500/100000: episode: 575, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.195, mean reward: -0.142 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.386, 10.098], loss: 0.002518, mae: 0.049105, mean_q: -0.351505
 57600/100000: episode: 576, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -17.562, mean reward: -0.176 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.796, 10.227], loss: 0.002570, mae: 0.050172, mean_q: -0.301181
 57700/100000: episode: 577, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.734, mean reward: -0.157 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.706, 10.455], loss: 0.002496, mae: 0.049229, mean_q: -0.323603
 57800/100000: episode: 578, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -14.161, mean reward: -0.142 [-1.000, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.843, 10.246], loss: 0.002578, mae: 0.049178, mean_q: -0.330899
 57900/100000: episode: 579, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -14.276, mean reward: -0.143 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.763, 10.360], loss: 0.002578, mae: 0.049768, mean_q: -0.331546
 58000/100000: episode: 580, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -7.609, mean reward: -0.076 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.018, 10.454], loss: 0.002662, mae: 0.051146, mean_q: -0.267456
 58100/100000: episode: 581, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -18.566, mean reward: -0.186 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.830, 10.136], loss: 0.002499, mae: 0.048665, mean_q: -0.320948
 58200/100000: episode: 582, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.759, mean reward: -0.188 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.434, 10.344], loss: 0.002534, mae: 0.048777, mean_q: -0.313831
 58300/100000: episode: 583, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.635, mean reward: -0.156 [-1.000, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.033, 10.098], loss: 0.002528, mae: 0.048521, mean_q: -0.335373
 58400/100000: episode: 584, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.411, mean reward: -0.174 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.384, 10.190], loss: 0.002730, mae: 0.051975, mean_q: -0.308759
 58500/100000: episode: 585, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -4.427, mean reward: -0.044 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.798, 10.098], loss: 0.002539, mae: 0.049269, mean_q: -0.316971
 58600/100000: episode: 586, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -17.208, mean reward: -0.172 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.992, 10.222], loss: 0.002549, mae: 0.049420, mean_q: -0.305440
 58700/100000: episode: 587, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.932, mean reward: -0.169 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.254, 10.098], loss: 0.002673, mae: 0.050676, mean_q: -0.309214
 58800/100000: episode: 588, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.745, mean reward: -0.157 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.107, 10.355], loss: 0.002614, mae: 0.050300, mean_q: -0.269436
 58900/100000: episode: 589, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -14.992, mean reward: -0.150 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.871, 10.098], loss: 0.002667, mae: 0.050405, mean_q: -0.304670
 59000/100000: episode: 590, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -19.407, mean reward: -0.194 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.610, 10.098], loss: 0.002659, mae: 0.050402, mean_q: -0.304104
 59100/100000: episode: 591, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.891, mean reward: -0.149 [-1.000, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.505, 10.098], loss: 0.002689, mae: 0.051067, mean_q: -0.304104
 59200/100000: episode: 592, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -13.042, mean reward: -0.130 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.842, 10.098], loss: 0.003320, mae: 0.056523, mean_q: -0.309898
 59300/100000: episode: 593, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -13.109, mean reward: -0.131 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.835, 10.316], loss: 0.003158, mae: 0.057087, mean_q: -0.270085
 59400/100000: episode: 594, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.002, mean reward: -0.160 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.500, 10.098], loss: 0.002539, mae: 0.049290, mean_q: -0.284654
 59500/100000: episode: 595, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.809, mean reward: -0.188 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.667, 10.119], loss: 0.002686, mae: 0.050612, mean_q: -0.335911
 59600/100000: episode: 596, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -9.994, mean reward: -0.100 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.298, 10.397], loss: 0.002596, mae: 0.049981, mean_q: -0.289597
 59700/100000: episode: 597, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -11.046, mean reward: -0.110 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.252, 10.098], loss: 0.002458, mae: 0.049446, mean_q: -0.281126
 59800/100000: episode: 598, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -13.983, mean reward: -0.140 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.879, 10.406], loss: 0.002729, mae: 0.052565, mean_q: -0.332270
 59900/100000: episode: 599, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.898, mean reward: -0.179 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.395, 10.121], loss: 0.003012, mae: 0.055434, mean_q: -0.287770
 60000/100000: episode: 600, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -17.222, mean reward: -0.172 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.715, 10.098], loss: 0.002783, mae: 0.050748, mean_q: -0.294519
 60100/100000: episode: 601, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -18.867, mean reward: -0.189 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.326, 10.402], loss: 0.002471, mae: 0.049026, mean_q: -0.334712
 60200/100000: episode: 602, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -19.368, mean reward: -0.194 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.439, 10.098], loss: 0.002788, mae: 0.052235, mean_q: -0.267904
 60300/100000: episode: 603, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -18.649, mean reward: -0.186 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.470, 10.117], loss: 0.002871, mae: 0.052970, mean_q: -0.291786
 60400/100000: episode: 604, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -20.554, mean reward: -0.206 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.055, 10.177], loss: 0.002713, mae: 0.051115, mean_q: -0.307999
 60500/100000: episode: 605, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -18.954, mean reward: -0.190 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.906, 10.098], loss: 0.002704, mae: 0.051878, mean_q: -0.301157
 60600/100000: episode: 606, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -14.658, mean reward: -0.147 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.642, 10.098], loss: 0.002530, mae: 0.049662, mean_q: -0.318554
 60700/100000: episode: 607, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.647, mean reward: -0.176 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.781, 10.152], loss: 0.002414, mae: 0.048210, mean_q: -0.297684
 60800/100000: episode: 608, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.955, mean reward: -0.180 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.534, 10.098], loss: 0.002505, mae: 0.048790, mean_q: -0.311013
 60900/100000: episode: 609, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -10.894, mean reward: -0.109 [-1.000, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.194, 10.195], loss: 0.002636, mae: 0.049050, mean_q: -0.306223
 61000/100000: episode: 610, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -13.679, mean reward: -0.137 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.934, 10.098], loss: 0.002563, mae: 0.049371, mean_q: -0.304132
 61100/100000: episode: 611, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -19.955, mean reward: -0.200 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.483, 10.139], loss: 0.002604, mae: 0.050729, mean_q: -0.291332
 61200/100000: episode: 612, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.098, mean reward: -0.181 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.577, 10.313], loss: 0.002877, mae: 0.052705, mean_q: -0.299950
 61300/100000: episode: 613, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.907, mean reward: -0.169 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.057, 10.112], loss: 0.002979, mae: 0.055194, mean_q: -0.310678
 61400/100000: episode: 614, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.074, mean reward: -0.191 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.271, 10.138], loss: 0.002735, mae: 0.051859, mean_q: -0.322310
 61500/100000: episode: 615, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.817, mean reward: -0.188 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.751, 10.098], loss: 0.002588, mae: 0.050227, mean_q: -0.317420
 61600/100000: episode: 616, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -13.568, mean reward: -0.136 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.176, 10.293], loss: 0.002692, mae: 0.050831, mean_q: -0.274716
 61700/100000: episode: 617, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.271, mean reward: -0.173 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.537, 10.098], loss: 0.002569, mae: 0.049292, mean_q: -0.319733
 61800/100000: episode: 618, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.361, mean reward: -0.184 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.573, 10.322], loss: 0.002542, mae: 0.049055, mean_q: -0.335968
 61900/100000: episode: 619, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -19.717, mean reward: -0.197 [-1.000, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.710, 10.098], loss: 0.002689, mae: 0.049695, mean_q: -0.321523
 62000/100000: episode: 620, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -17.947, mean reward: -0.179 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.320, 10.187], loss: 0.002758, mae: 0.050774, mean_q: -0.308738
 62100/100000: episode: 621, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -12.036, mean reward: -0.120 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.315, 10.146], loss: 0.003579, mae: 0.056309, mean_q: -0.273371
 62200/100000: episode: 622, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.026, mean reward: -0.180 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.123, 10.135], loss: 0.006221, mae: 0.071049, mean_q: -0.313656
 62300/100000: episode: 623, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -17.981, mean reward: -0.180 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.383, 10.213], loss: 0.002688, mae: 0.052784, mean_q: -0.287089
 62400/100000: episode: 624, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -11.715, mean reward: -0.117 [-1.000, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.493, 10.098], loss: 0.002810, mae: 0.054757, mean_q: -0.277974
 62500/100000: episode: 625, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.085, mean reward: -0.191 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.012, 10.114], loss: 0.002680, mae: 0.051210, mean_q: -0.341933
 62600/100000: episode: 626, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.889, mean reward: -0.159 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.975, 10.164], loss: 0.002652, mae: 0.050722, mean_q: -0.310111
 62700/100000: episode: 627, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -20.074, mean reward: -0.201 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.053, 10.307], loss: 0.002553, mae: 0.049620, mean_q: -0.305460
 62800/100000: episode: 628, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -16.558, mean reward: -0.166 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.876, 10.098], loss: 0.002595, mae: 0.050008, mean_q: -0.309740
 62900/100000: episode: 629, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.743, mean reward: -0.177 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.637, 10.259], loss: 0.002533, mae: 0.049033, mean_q: -0.308935
 63000/100000: episode: 630, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.015, mean reward: -0.190 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.592, 10.098], loss: 0.002426, mae: 0.048081, mean_q: -0.325492
 63100/100000: episode: 631, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -13.916, mean reward: -0.139 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.442, 10.158], loss: 0.002281, mae: 0.047117, mean_q: -0.283041
 63200/100000: episode: 632, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -17.202, mean reward: -0.172 [-1.000, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.932, 10.171], loss: 0.002757, mae: 0.051032, mean_q: -0.311637
 63300/100000: episode: 633, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -13.868, mean reward: -0.139 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.875, 10.506], loss: 0.002408, mae: 0.048488, mean_q: -0.332451
 63400/100000: episode: 634, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -13.000, mean reward: -0.130 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.294, 10.098], loss: 0.002555, mae: 0.049689, mean_q: -0.310638
 63500/100000: episode: 635, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.553, mean reward: -0.186 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.808, 10.098], loss: 0.002720, mae: 0.051363, mean_q: -0.325645
 63600/100000: episode: 636, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -15.817, mean reward: -0.158 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.566, 10.098], loss: 0.002581, mae: 0.050849, mean_q: -0.304586
 63700/100000: episode: 637, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -18.694, mean reward: -0.187 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.865, 10.155], loss: 0.002590, mae: 0.051569, mean_q: -0.285801
 63800/100000: episode: 638, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -16.699, mean reward: -0.167 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.588, 10.169], loss: 0.002385, mae: 0.047868, mean_q: -0.347589
 63900/100000: episode: 639, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.137, mean reward: -0.191 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.208, 10.236], loss: 0.002697, mae: 0.051532, mean_q: -0.308033
 64000/100000: episode: 640, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.337, mean reward: -0.173 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.609, 10.200], loss: 0.002779, mae: 0.051658, mean_q: -0.270451
 64100/100000: episode: 641, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: -10.926, mean reward: -0.109 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.020, 10.442], loss: 0.002455, mae: 0.049123, mean_q: -0.318645
 64200/100000: episode: 642, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.145, mean reward: -0.181 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.320, 10.098], loss: 0.002624, mae: 0.050459, mean_q: -0.313899
 64300/100000: episode: 643, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -20.161, mean reward: -0.202 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.797, 10.132], loss: 0.002610, mae: 0.050640, mean_q: -0.316768
 64400/100000: episode: 644, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: -19.324, mean reward: -0.193 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.054, 10.098], loss: 0.003844, mae: 0.055621, mean_q: -0.323955
 64500/100000: episode: 645, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: -14.557, mean reward: -0.146 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.003, 10.124], loss: 0.003503, mae: 0.058600, mean_q: -0.334964
 64600/100000: episode: 646, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.701, mean reward: -0.167 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.925, 10.194], loss: 0.002782, mae: 0.052195, mean_q: -0.308667
 64700/100000: episode: 647, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.670, mean reward: -0.147 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.560, 10.098], loss: 0.002751, mae: 0.051601, mean_q: -0.343310
 64800/100000: episode: 648, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -18.167, mean reward: -0.182 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.908, 10.103], loss: 0.002656, mae: 0.050959, mean_q: -0.322609
 64900/100000: episode: 649, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -11.277, mean reward: -0.113 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.678, 10.282], loss: 0.002529, mae: 0.049068, mean_q: -0.310136
 65000/100000: episode: 650, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.345, mean reward: -0.163 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.907, 10.098], loss: 0.002387, mae: 0.047676, mean_q: -0.348248
 65100/100000: episode: 651, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.602, mean reward: -0.186 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.447, 10.137], loss: 0.003327, mae: 0.056967, mean_q: -0.297683
 65200/100000: episode: 652, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.623, mean reward: -0.176 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.200, 10.103], loss: 0.002592, mae: 0.050594, mean_q: -0.310321
 65300/100000: episode: 653, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -20.243, mean reward: -0.202 [-1.000, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.232, 10.098], loss: 0.002404, mae: 0.047513, mean_q: -0.357044
 65400/100000: episode: 654, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.118, mean reward: -0.181 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.440, 10.236], loss: 0.002696, mae: 0.050533, mean_q: -0.329069
 65500/100000: episode: 655, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -20.917, mean reward: -0.209 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.535, 10.199], loss: 0.002525, mae: 0.048660, mean_q: -0.324477
 65600/100000: episode: 656, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -16.250, mean reward: -0.162 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.220, 10.098], loss: 0.002678, mae: 0.050853, mean_q: -0.296283
 65700/100000: episode: 657, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.316, mean reward: -0.183 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.514, 10.098], loss: 0.002728, mae: 0.051044, mean_q: -0.306711
 65800/100000: episode: 658, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.024, mean reward: -0.180 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.565, 10.098], loss: 0.002889, mae: 0.052636, mean_q: -0.286917
 65900/100000: episode: 659, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -18.977, mean reward: -0.190 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.866, 10.262], loss: 0.002585, mae: 0.051072, mean_q: -0.303200
 66000/100000: episode: 660, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.492, mean reward: -0.175 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.599, 10.220], loss: 0.002661, mae: 0.049887, mean_q: -0.325127
 66100/100000: episode: 661, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.626, mean reward: -0.186 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.667, 10.168], loss: 0.002568, mae: 0.049439, mean_q: -0.332247
 66200/100000: episode: 662, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -13.935, mean reward: -0.139 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.467, 10.098], loss: 0.002433, mae: 0.048253, mean_q: -0.342760
 66300/100000: episode: 663, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -12.458, mean reward: -0.125 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.440, 10.098], loss: 0.002548, mae: 0.049231, mean_q: -0.322639
 66400/100000: episode: 664, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -15.992, mean reward: -0.160 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.519, 10.210], loss: 0.002571, mae: 0.049081, mean_q: -0.342357
 66500/100000: episode: 665, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.445, mean reward: -0.174 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.866, 10.098], loss: 0.004544, mae: 0.060169, mean_q: -0.324211
 66600/100000: episode: 666, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.329, mean reward: -0.193 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.777, 10.142], loss: 0.002855, mae: 0.055363, mean_q: -0.320119
 66700/100000: episode: 667, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.142, mean reward: -0.171 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.552, 10.098], loss: 0.002604, mae: 0.049780, mean_q: -0.327682
 66800/100000: episode: 668, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.475, mean reward: -0.175 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.814, 10.176], loss: 0.002614, mae: 0.049786, mean_q: -0.326122
 66900/100000: episode: 669, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -18.370, mean reward: -0.184 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.320, 10.098], loss: 0.002570, mae: 0.049326, mean_q: -0.304851
 67000/100000: episode: 670, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: -19.682, mean reward: -0.197 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.998, 10.190], loss: 0.002391, mae: 0.047359, mean_q: -0.326443
 67100/100000: episode: 671, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -15.827, mean reward: -0.158 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.382, 10.244], loss: 0.002477, mae: 0.049023, mean_q: -0.339073
 67200/100000: episode: 672, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -15.796, mean reward: -0.158 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.937, 10.098], loss: 0.002546, mae: 0.049747, mean_q: -0.318799
 67300/100000: episode: 673, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -13.342, mean reward: -0.133 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.367, 10.394], loss: 0.002476, mae: 0.048437, mean_q: -0.329439
 67400/100000: episode: 674, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.462, mean reward: -0.145 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.999, 10.331], loss: 0.002544, mae: 0.048460, mean_q: -0.336004
 67500/100000: episode: 675, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.198, mean reward: -0.172 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.887, 10.250], loss: 0.002340, mae: 0.046852, mean_q: -0.339446
 67600/100000: episode: 676, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -15.492, mean reward: -0.155 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.369, 10.387], loss: 0.002441, mae: 0.047945, mean_q: -0.330513
 67700/100000: episode: 677, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.518, mean reward: -0.165 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.490, 10.416], loss: 0.002538, mae: 0.049401, mean_q: -0.306534
 67800/100000: episode: 678, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: -15.478, mean reward: -0.155 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.800, 10.142], loss: 0.002658, mae: 0.050375, mean_q: -0.315085
 67900/100000: episode: 679, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: -15.836, mean reward: -0.158 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.513, 10.098], loss: 0.002391, mae: 0.047651, mean_q: -0.315638
 68000/100000: episode: 680, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.032, mean reward: -0.160 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.555, 10.197], loss: 0.002409, mae: 0.047789, mean_q: -0.315619
 68100/100000: episode: 681, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -14.061, mean reward: -0.141 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.300, 10.222], loss: 0.002465, mae: 0.047526, mean_q: -0.339202
 68200/100000: episode: 682, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -18.478, mean reward: -0.185 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.857, 10.098], loss: 0.002501, mae: 0.048242, mean_q: -0.330621
 68300/100000: episode: 683, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.986, mean reward: -0.150 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.761, 10.107], loss: 0.002382, mae: 0.047466, mean_q: -0.318495
 68400/100000: episode: 684, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -16.534, mean reward: -0.165 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.087, 10.165], loss: 0.002322, mae: 0.048070, mean_q: -0.320925
 68500/100000: episode: 685, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.171, mean reward: -0.182 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.720, 10.098], loss: 0.002403, mae: 0.049750, mean_q: -0.324404
 68600/100000: episode: 686, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -17.347, mean reward: -0.173 [-1.000, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.686, 10.245], loss: 0.002309, mae: 0.047202, mean_q: -0.342220
 68700/100000: episode: 687, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.847, mean reward: -0.168 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.654, 10.098], loss: 0.002413, mae: 0.048572, mean_q: -0.297878
 68800/100000: episode: 688, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.484, mean reward: -0.185 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.202, 10.098], loss: 0.002520, mae: 0.050059, mean_q: -0.298148
 68900/100000: episode: 689, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.301, mean reward: -0.193 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.848, 10.098], loss: 0.005396, mae: 0.068294, mean_q: -0.290527
 69000/100000: episode: 690, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -12.204, mean reward: -0.122 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.579, 10.465], loss: 0.002621, mae: 0.051670, mean_q: -0.310853
 69100/100000: episode: 691, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.383, mean reward: -0.174 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.504, 10.098], loss: 0.002528, mae: 0.049712, mean_q: -0.303431
 69200/100000: episode: 692, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.367, mean reward: -0.164 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.990, 10.293], loss: 0.002224, mae: 0.046185, mean_q: -0.344164
 69300/100000: episode: 693, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -18.682, mean reward: -0.187 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.511, 10.159], loss: 0.002388, mae: 0.048087, mean_q: -0.325728
 69400/100000: episode: 694, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -19.172, mean reward: -0.192 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.375, 10.098], loss: 0.002755, mae: 0.050609, mean_q: -0.303232
 69500/100000: episode: 695, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -17.111, mean reward: -0.171 [-1.000, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.232, 10.098], loss: 0.002677, mae: 0.050699, mean_q: -0.301241
 69600/100000: episode: 696, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -19.087, mean reward: -0.191 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.439, 10.219], loss: 0.003664, mae: 0.056818, mean_q: -0.311781
 69700/100000: episode: 697, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -9.839, mean reward: -0.098 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.011, 10.521], loss: 0.002443, mae: 0.049974, mean_q: -0.311844
 69800/100000: episode: 698, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -19.122, mean reward: -0.191 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.654, 10.212], loss: 0.002572, mae: 0.050575, mean_q: -0.294670
 69900/100000: episode: 699, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -19.681, mean reward: -0.197 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.032, 10.224], loss: 0.002478, mae: 0.048642, mean_q: -0.318543
 70000/100000: episode: 700, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.371, mean reward: -0.184 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.735, 10.098], loss: 0.002644, mae: 0.050837, mean_q: -0.315684
 70100/100000: episode: 701, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -11.649, mean reward: -0.116 [-1.000, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.101, 10.098], loss: 0.002388, mae: 0.048280, mean_q: -0.300607
 70200/100000: episode: 702, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.196, mean reward: -0.152 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.362, 10.098], loss: 0.002356, mae: 0.047205, mean_q: -0.335904
 70300/100000: episode: 703, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -19.555, mean reward: -0.196 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.843, 10.098], loss: 0.002393, mae: 0.047269, mean_q: -0.341099
 70400/100000: episode: 704, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -14.604, mean reward: -0.146 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.402, 10.133], loss: 0.002480, mae: 0.047664, mean_q: -0.330311
 70500/100000: episode: 705, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.695, mean reward: -0.177 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.470, 10.098], loss: 0.002625, mae: 0.050866, mean_q: -0.315196
 70600/100000: episode: 706, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -20.241, mean reward: -0.202 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.538, 10.309], loss: 0.003407, mae: 0.058517, mean_q: -0.334966
 70700/100000: episode: 707, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -13.153, mean reward: -0.132 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.770, 10.098], loss: 0.002667, mae: 0.050393, mean_q: -0.335196
 70800/100000: episode: 708, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -19.890, mean reward: -0.199 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.467, 10.098], loss: 0.002505, mae: 0.048627, mean_q: -0.320466
 70900/100000: episode: 709, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.979, mean reward: -0.180 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.171, 10.269], loss: 0.002763, mae: 0.050730, mean_q: -0.305210
 71000/100000: episode: 710, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -16.997, mean reward: -0.170 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.524, 10.294], loss: 0.003562, mae: 0.054178, mean_q: -0.302430
 71100/100000: episode: 711, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -18.791, mean reward: -0.188 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.059, 10.098], loss: 0.003527, mae: 0.058578, mean_q: -0.284460
 71200/100000: episode: 712, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -17.974, mean reward: -0.180 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.796, 10.098], loss: 0.002526, mae: 0.049372, mean_q: -0.338392
 71300/100000: episode: 713, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -13.155, mean reward: -0.132 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.842, 10.098], loss: 0.002534, mae: 0.048561, mean_q: -0.354769
 71400/100000: episode: 714, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -18.807, mean reward: -0.188 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.036, 10.298], loss: 0.007393, mae: 0.070348, mean_q: -0.317900
 71500/100000: episode: 715, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -14.217, mean reward: -0.142 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.674, 10.381], loss: 0.002709, mae: 0.052404, mean_q: -0.313583
 71600/100000: episode: 716, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: -17.292, mean reward: -0.173 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.133, 10.098], loss: 0.002566, mae: 0.048907, mean_q: -0.340000
 71700/100000: episode: 717, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.990, mean reward: -0.170 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.615, 10.098], loss: 0.002656, mae: 0.051042, mean_q: -0.328347
 71800/100000: episode: 718, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -19.161, mean reward: -0.192 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.362, 10.098], loss: 0.002719, mae: 0.051019, mean_q: -0.323167
 71900/100000: episode: 719, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -15.662, mean reward: -0.157 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.230, 10.098], loss: 0.002568, mae: 0.049528, mean_q: -0.314403
 72000/100000: episode: 720, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -16.663, mean reward: -0.167 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.376, 10.098], loss: 0.002594, mae: 0.049191, mean_q: -0.328384
 72100/100000: episode: 721, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -17.482, mean reward: -0.175 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.063, 10.098], loss: 0.002605, mae: 0.049873, mean_q: -0.325090
 72200/100000: episode: 722, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -10.672, mean reward: -0.107 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.511, 10.098], loss: 0.002642, mae: 0.050002, mean_q: -0.271193
 72300/100000: episode: 723, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -17.737, mean reward: -0.177 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.836, 10.098], loss: 0.002610, mae: 0.049648, mean_q: -0.291339
 72400/100000: episode: 724, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.024, mean reward: -0.180 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.859, 10.098], loss: 0.002550, mae: 0.049408, mean_q: -0.341060
 72500/100000: episode: 725, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.799, mean reward: -0.188 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.911, 10.280], loss: 0.002640, mae: 0.049924, mean_q: -0.325196
 72600/100000: episode: 726, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -15.129, mean reward: -0.151 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.574, 10.098], loss: 0.002723, mae: 0.051473, mean_q: -0.323439
 72700/100000: episode: 727, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -13.925, mean reward: -0.139 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.576, 10.160], loss: 0.002569, mae: 0.049187, mean_q: -0.306407
 72800/100000: episode: 728, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -16.336, mean reward: -0.163 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.502, 10.098], loss: 0.002687, mae: 0.050927, mean_q: -0.293093
 72900/100000: episode: 729, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -12.059, mean reward: -0.121 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.159, 10.098], loss: 0.002528, mae: 0.048845, mean_q: -0.326806
 73000/100000: episode: 730, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -18.798, mean reward: -0.188 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.877, 10.098], loss: 0.002714, mae: 0.050187, mean_q: -0.328656
 73100/100000: episode: 731, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: -15.956, mean reward: -0.160 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.047, 10.313], loss: 0.002450, mae: 0.048569, mean_q: -0.286423
 73200/100000: episode: 732, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: -18.563, mean reward: -0.186 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.454, 10.211], loss: 0.004785, mae: 0.063954, mean_q: -0.319950
 73300/100000: episode: 733, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -18.707, mean reward: -0.187 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.037, 10.215], loss: 0.002894, mae: 0.054242, mean_q: -0.332297
 73400/100000: episode: 734, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -14.242, mean reward: -0.142 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.996, 10.098], loss: 0.002707, mae: 0.050761, mean_q: -0.306054
 73500/100000: episode: 735, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -18.522, mean reward: -0.185 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.022, 10.202], loss: 0.002623, mae: 0.049774, mean_q: -0.300702
 73600/100000: episode: 736, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -15.295, mean reward: -0.153 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.249, 10.356], loss: 0.002528, mae: 0.048877, mean_q: -0.302214
 73700/100000: episode: 737, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -19.065, mean reward: -0.191 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.569, 10.098], loss: 0.002601, mae: 0.050073, mean_q: -0.293292
 73800/100000: episode: 738, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -17.200, mean reward: -0.172 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.704, 10.376], loss: 0.002453, mae: 0.047433, mean_q: -0.330505
 73900/100000: episode: 739, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: -13.961, mean reward: -0.140 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.485, 10.098], loss: 0.002601, mae: 0.049337, mean_q: -0.326541
 74000/100000: episode: 740, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -15.070, mean reward: -0.151 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.568, 10.462], loss: 0.002597, mae: 0.048593, mean_q: -0.293290
 74100/100000: episode: 741, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -17.512, mean reward: -0.175 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.595, 10.275], loss: 0.002536, mae: 0.049210, mean_q: -0.317450
 74200/100000: episode: 742, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: -18.655, mean reward: -0.187 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.659, 10.098], loss: 0.002556, mae: 0.048684, mean_q: -0.302256
 74300/100000: episode: 743, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: -17.830, mean reward: -0.178 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.684, 10.155], loss: 0.002702, mae: 0.051041, mean_q: -0.299514
 74400/100000: episode: 744, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.506, mean reward: -0.185 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.355, 10.098], loss: 0.002494, mae: 0.047965, mean_q: -0.344939
 74500/100000: episode: 745, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -18.868, mean reward: -0.189 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.104, 10.208], loss: 0.002388, mae: 0.047212, mean_q: -0.336551
 74600/100000: episode: 746, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: -17.131, mean reward: -0.171 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.527, 10.132], loss: 0.002398, mae: 0.047662, mean_q: -0.314803
 74700/100000: episode: 747, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -14.933, mean reward: -0.149 [-1.000, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.768, 10.162], loss: 0.002423, mae: 0.047382, mean_q: -0.342106
 74800/100000: episode: 748, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -16.303, mean reward: -0.163 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.919, 10.098], loss: 0.002389, mae: 0.047696, mean_q: -0.308743
 74900/100000: episode: 749, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -19.212, mean reward: -0.192 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.315, 10.098], loss: 0.002454, mae: 0.048000, mean_q: -0.313870
 75000/100000: episode: 750, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.651, mean reward: -0.177 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.070, 10.255], loss: 0.002658, mae: 0.049102, mean_q: -0.336486
 75100/100000: episode: 751, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -15.857, mean reward: -0.159 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.717, 10.233], loss: 0.003209, mae: 0.057448, mean_q: -0.330056
 75200/100000: episode: 752, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -19.633, mean reward: -0.196 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.155, 10.196], loss: 0.002527, mae: 0.049419, mean_q: -0.346876
 75300/100000: episode: 753, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -17.613, mean reward: -0.176 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.646, 10.098], loss: 0.002493, mae: 0.048654, mean_q: -0.285664
 75400/100000: episode: 754, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -14.058, mean reward: -0.141 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.275, 10.098], loss: 0.002594, mae: 0.049156, mean_q: -0.316700
 75500/100000: episode: 755, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.960, mean reward: -0.170 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.772, 10.098], loss: 0.002433, mae: 0.047725, mean_q: -0.337095
 75600/100000: episode: 756, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.139, mean reward: -0.171 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.434, 10.098], loss: 0.002314, mae: 0.047268, mean_q: -0.311717
 75700/100000: episode: 757, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -17.974, mean reward: -0.180 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.462, 10.098], loss: 0.002360, mae: 0.047260, mean_q: -0.296628
 75800/100000: episode: 758, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: -13.962, mean reward: -0.140 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.738, 10.332], loss: 0.002271, mae: 0.046953, mean_q: -0.297197
 75900/100000: episode: 759, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -14.128, mean reward: -0.141 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.269, 10.098], loss: 0.002501, mae: 0.048334, mean_q: -0.310889
 76000/100000: episode: 760, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -15.190, mean reward: -0.152 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.760, 10.098], loss: 0.002164, mae: 0.044983, mean_q: -0.328724
 76100/100000: episode: 761, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -15.141, mean reward: -0.151 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.554, 10.098], loss: 0.002300, mae: 0.047095, mean_q: -0.308869
 76200/100000: episode: 762, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -18.346, mean reward: -0.183 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.701, 10.228], loss: 0.002393, mae: 0.047694, mean_q: -0.310313
 76300/100000: episode: 763, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -17.246, mean reward: -0.172 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.739, 10.166], loss: 0.002313, mae: 0.046614, mean_q: -0.323791
 76400/100000: episode: 764, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -16.863, mean reward: -0.169 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.520, 10.103], loss: 0.002206, mae: 0.045109, mean_q: -0.352941
 76500/100000: episode: 765, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.167, mean reward: -0.162 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.621, 10.258], loss: 0.002473, mae: 0.048753, mean_q: -0.301663
 76600/100000: episode: 766, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -15.334, mean reward: -0.153 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.560, 10.215], loss: 0.002444, mae: 0.048815, mean_q: -0.277586
 76700/100000: episode: 767, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.296, mean reward: -0.173 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.934, 10.098], loss: 0.002266, mae: 0.046695, mean_q: -0.298422
 76800/100000: episode: 768, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.422, mean reward: -0.164 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.949, 10.098], loss: 0.002355, mae: 0.047474, mean_q: -0.290334
 76900/100000: episode: 769, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -19.194, mean reward: -0.192 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.247, 10.392], loss: 0.004968, mae: 0.064260, mean_q: -0.321017
 77000/100000: episode: 770, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.878, mean reward: -0.169 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.574, 10.158], loss: 0.002866, mae: 0.054189, mean_q: -0.306691
 77100/100000: episode: 771, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.163, mean reward: -0.192 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.769, 10.098], loss: 0.002414, mae: 0.047073, mean_q: -0.325504
 77200/100000: episode: 772, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -20.409, mean reward: -0.204 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.396, 10.176], loss: 0.002433, mae: 0.046964, mean_q: -0.311710
 77300/100000: episode: 773, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -15.857, mean reward: -0.159 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.169, 10.098], loss: 0.002386, mae: 0.047614, mean_q: -0.313111
 77400/100000: episode: 774, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -19.549, mean reward: -0.195 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.518, 10.194], loss: 0.002434, mae: 0.047067, mean_q: -0.349316
 77500/100000: episode: 775, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.970, mean reward: -0.190 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.801, 10.127], loss: 0.002463, mae: 0.048310, mean_q: -0.319719
 77600/100000: episode: 776, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.193, mean reward: -0.182 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.913, 10.098], loss: 0.002259, mae: 0.046589, mean_q: -0.312509
 77700/100000: episode: 777, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -18.938, mean reward: -0.189 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.104, 10.207], loss: 0.002429, mae: 0.047626, mean_q: -0.306387
 77800/100000: episode: 778, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.026, mean reward: -0.190 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.028, 10.098], loss: 0.002307, mae: 0.047518, mean_q: -0.304373
 77900/100000: episode: 779, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -12.474, mean reward: -0.125 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.947, 10.396], loss: 0.002289, mae: 0.046047, mean_q: -0.317515
 78000/100000: episode: 780, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -18.387, mean reward: -0.184 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.198, 10.214], loss: 0.002377, mae: 0.047265, mean_q: -0.321922
 78100/100000: episode: 781, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: -8.259, mean reward: -0.083 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.683, 10.236], loss: 0.002443, mae: 0.047170, mean_q: -0.349077
 78200/100000: episode: 782, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -16.009, mean reward: -0.160 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.395, 10.310], loss: 0.002348, mae: 0.047317, mean_q: -0.314830
 78300/100000: episode: 783, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -13.343, mean reward: -0.133 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.100, 10.098], loss: 0.002494, mae: 0.048016, mean_q: -0.318729
 78400/100000: episode: 784, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -18.206, mean reward: -0.182 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.509, 10.098], loss: 0.002524, mae: 0.047776, mean_q: -0.335030
 78500/100000: episode: 785, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -18.733, mean reward: -0.187 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.611, 10.098], loss: 0.002327, mae: 0.047442, mean_q: -0.311615
 78600/100000: episode: 786, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -17.253, mean reward: -0.173 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.849, 10.098], loss: 0.002345, mae: 0.046470, mean_q: -0.331116
 78700/100000: episode: 787, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: -19.345, mean reward: -0.193 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.876, 10.269], loss: 0.002391, mae: 0.047570, mean_q: -0.357298
 78800/100000: episode: 788, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.227, mean reward: -0.162 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.408, 10.098], loss: 0.002553, mae: 0.048861, mean_q: -0.327431
 78900/100000: episode: 789, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -16.655, mean reward: -0.167 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.031, 10.098], loss: 0.002581, mae: 0.049347, mean_q: -0.318169
 79000/100000: episode: 790, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -6.490, mean reward: -0.065 [-1.000, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.013, 10.098], loss: 0.002625, mae: 0.050285, mean_q: -0.300552
 79100/100000: episode: 791, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -13.640, mean reward: -0.136 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.716, 10.098], loss: 0.002333, mae: 0.047090, mean_q: -0.317478
 79200/100000: episode: 792, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -17.127, mean reward: -0.171 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.545, 10.212], loss: 0.002512, mae: 0.048539, mean_q: -0.307987
 79300/100000: episode: 793, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.710, mean reward: -0.177 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.103, 10.098], loss: 0.002390, mae: 0.048227, mean_q: -0.309577
 79400/100000: episode: 794, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -11.158, mean reward: -0.112 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.809, 10.345], loss: 0.003066, mae: 0.054895, mean_q: -0.289136
 79500/100000: episode: 795, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.384, mean reward: -0.174 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.558, 10.170], loss: 0.002413, mae: 0.048474, mean_q: -0.315842
 79600/100000: episode: 796, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -18.524, mean reward: -0.185 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.139, 10.098], loss: 0.002391, mae: 0.047919, mean_q: -0.283052
 79700/100000: episode: 797, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -17.268, mean reward: -0.173 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.756, 10.198], loss: 0.002263, mae: 0.046947, mean_q: -0.315525
 79800/100000: episode: 798, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -19.686, mean reward: -0.197 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.622, 10.110], loss: 0.002262, mae: 0.046775, mean_q: -0.320153
 79900/100000: episode: 799, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.100, mean reward: -0.171 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.523, 10.098], loss: 0.002283, mae: 0.046602, mean_q: -0.326820
 80000/100000: episode: 800, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: -18.030, mean reward: -0.180 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.011, 10.154], loss: 0.002472, mae: 0.047787, mean_q: -0.316866
 80100/100000: episode: 801, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -16.139, mean reward: -0.161 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.394, 10.098], loss: 0.002171, mae: 0.045952, mean_q: -0.311666
 80200/100000: episode: 802, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.916, mean reward: -0.169 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.534, 10.416], loss: 0.002384, mae: 0.048096, mean_q: -0.295094
 80300/100000: episode: 803, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -13.458, mean reward: -0.135 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.580, 10.098], loss: 0.002487, mae: 0.048479, mean_q: -0.308274
 80400/100000: episode: 804, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: -17.864, mean reward: -0.179 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.638, 10.098], loss: 0.002269, mae: 0.046348, mean_q: -0.312473
 80500/100000: episode: 805, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -13.865, mean reward: -0.139 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.882, 10.098], loss: 0.002628, mae: 0.050872, mean_q: -0.304843
 80600/100000: episode: 806, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.844, mean reward: -0.178 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.673, 10.122], loss: 0.003381, mae: 0.058707, mean_q: -0.292284
 80700/100000: episode: 807, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.201, mean reward: -0.172 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.080, 10.098], loss: 0.002507, mae: 0.049584, mean_q: -0.278076
 80800/100000: episode: 808, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.344, mean reward: -0.173 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.735, 10.189], loss: 0.002322, mae: 0.046889, mean_q: -0.311028
 80900/100000: episode: 809, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -17.641, mean reward: -0.176 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.711, 10.098], loss: 0.002550, mae: 0.049027, mean_q: -0.330157
 81000/100000: episode: 810, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: -13.871, mean reward: -0.139 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.037, 10.098], loss: 0.002515, mae: 0.048172, mean_q: -0.311485
 81100/100000: episode: 811, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -15.174, mean reward: -0.152 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.829, 10.098], loss: 0.002328, mae: 0.047439, mean_q: -0.296771
 81200/100000: episode: 812, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -18.339, mean reward: -0.183 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.849, 10.330], loss: 0.002552, mae: 0.049521, mean_q: -0.279211
 81300/100000: episode: 813, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.239, mean reward: -0.182 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.401, 10.098], loss: 0.002459, mae: 0.048348, mean_q: -0.307710
 81400/100000: episode: 814, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -14.813, mean reward: -0.148 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.092, 10.239], loss: 0.002444, mae: 0.048297, mean_q: -0.312365
 81500/100000: episode: 815, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.619, mean reward: -0.186 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.675, 10.098], loss: 0.002655, mae: 0.050885, mean_q: -0.285485
 81600/100000: episode: 816, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.131, mean reward: -0.191 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.412, 10.098], loss: 0.002514, mae: 0.048411, mean_q: -0.307730
 81700/100000: episode: 817, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -18.138, mean reward: -0.181 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.538, 10.253], loss: 0.002453, mae: 0.047705, mean_q: -0.320364
 81800/100000: episode: 818, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.276, mean reward: -0.193 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.088, 10.098], loss: 0.002370, mae: 0.046934, mean_q: -0.350186
 81900/100000: episode: 819, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -17.345, mean reward: -0.173 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.389, 10.098], loss: 0.003107, mae: 0.053180, mean_q: -0.335052
 82000/100000: episode: 820, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -15.412, mean reward: -0.154 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.278, 10.098], loss: 0.002595, mae: 0.050940, mean_q: -0.334811
 82100/100000: episode: 821, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -17.447, mean reward: -0.174 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.306, 10.161], loss: 0.002576, mae: 0.050113, mean_q: -0.288460
 82200/100000: episode: 822, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -11.628, mean reward: -0.116 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.590, 10.098], loss: 0.002533, mae: 0.049209, mean_q: -0.308714
 82300/100000: episode: 823, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: -8.874, mean reward: -0.089 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.597, 10.098], loss: 0.002379, mae: 0.048321, mean_q: -0.296427
 82400/100000: episode: 824, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -13.397, mean reward: -0.134 [-1.000, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.721, 10.479], loss: 0.002316, mae: 0.046693, mean_q: -0.311214
 82500/100000: episode: 825, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -18.280, mean reward: -0.183 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.267, 10.195], loss: 0.002411, mae: 0.047434, mean_q: -0.358163
 82600/100000: episode: 826, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.819, mean reward: -0.168 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.774, 10.098], loss: 0.002576, mae: 0.049517, mean_q: -0.316218
 82700/100000: episode: 827, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -17.956, mean reward: -0.180 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.035, 10.226], loss: 0.002637, mae: 0.050492, mean_q: -0.304758
 82800/100000: episode: 828, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.394, mean reward: -0.194 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.982, 10.098], loss: 0.002545, mae: 0.049349, mean_q: -0.325997
 82900/100000: episode: 829, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -18.332, mean reward: -0.183 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.038, 10.136], loss: 0.002445, mae: 0.048283, mean_q: -0.304096
 83000/100000: episode: 830, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.657, mean reward: -0.167 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.760, 10.335], loss: 0.002597, mae: 0.050227, mean_q: -0.315885
 83100/100000: episode: 831, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.166, mean reward: -0.172 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.238, 10.199], loss: 0.002595, mae: 0.050608, mean_q: -0.275261
 83200/100000: episode: 832, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.483, mean reward: -0.175 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.104, 10.234], loss: 0.002495, mae: 0.049688, mean_q: -0.299489
 83300/100000: episode: 833, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -18.585, mean reward: -0.186 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.270, 10.239], loss: 0.002086, mae: 0.044630, mean_q: -0.360629
 83400/100000: episode: 834, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.477, mean reward: -0.165 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.681, 10.098], loss: 0.004763, mae: 0.062800, mean_q: -0.327090
 83500/100000: episode: 835, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -18.675, mean reward: -0.187 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.849, 10.098], loss: 0.002328, mae: 0.048630, mean_q: -0.314006
 83600/100000: episode: 836, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.310, mean reward: -0.193 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.191, 10.293], loss: 0.002721, mae: 0.050928, mean_q: -0.308574
 83700/100000: episode: 837, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.774, mean reward: -0.168 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.938, 10.262], loss: 0.002525, mae: 0.048967, mean_q: -0.318895
 83800/100000: episode: 838, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -15.370, mean reward: -0.154 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.572, 10.098], loss: 0.002418, mae: 0.048013, mean_q: -0.332459
 83900/100000: episode: 839, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -19.363, mean reward: -0.194 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.978, 10.098], loss: 0.002311, mae: 0.046317, mean_q: -0.319720
 84000/100000: episode: 840, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.536, mean reward: -0.185 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.733, 10.125], loss: 0.002391, mae: 0.046947, mean_q: -0.371162
 84100/100000: episode: 841, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -15.521, mean reward: -0.155 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.390, 10.098], loss: 0.002294, mae: 0.046637, mean_q: -0.314087
 84200/100000: episode: 842, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -16.349, mean reward: -0.163 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.426, 10.098], loss: 0.002486, mae: 0.048755, mean_q: -0.305439
 84300/100000: episode: 843, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -14.341, mean reward: -0.143 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.704, 10.296], loss: 0.002340, mae: 0.047113, mean_q: -0.312275
 84400/100000: episode: 844, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.974, mean reward: -0.180 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.592, 10.098], loss: 0.002617, mae: 0.049462, mean_q: -0.318374
 84500/100000: episode: 845, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -9.704, mean reward: -0.097 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.363, 10.098], loss: 0.002508, mae: 0.049285, mean_q: -0.315684
 84600/100000: episode: 846, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -15.463, mean reward: -0.155 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.171, 10.207], loss: 0.002399, mae: 0.047579, mean_q: -0.332755
 84700/100000: episode: 847, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -15.456, mean reward: -0.155 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.668, 10.098], loss: 0.002507, mae: 0.049980, mean_q: -0.303976
 84800/100000: episode: 848, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -15.370, mean reward: -0.154 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.222, 10.098], loss: 0.002335, mae: 0.046257, mean_q: -0.350089
 84900/100000: episode: 849, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -18.837, mean reward: -0.188 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.090, 10.203], loss: 0.002529, mae: 0.048050, mean_q: -0.307931
 85000/100000: episode: 850, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: -18.709, mean reward: -0.187 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.387, 10.154], loss: 0.002584, mae: 0.049286, mean_q: -0.309713
 85100/100000: episode: 851, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: -14.032, mean reward: -0.140 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.597, 10.098], loss: 0.002289, mae: 0.046600, mean_q: -0.309502
 85200/100000: episode: 852, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.115, mean reward: -0.161 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.709, 10.124], loss: 0.002473, mae: 0.048672, mean_q: -0.302859
 85300/100000: episode: 853, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -18.411, mean reward: -0.184 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.928, 10.205], loss: 0.002433, mae: 0.048002, mean_q: -0.338100
 85400/100000: episode: 854, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -13.725, mean reward: -0.137 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.551, 10.098], loss: 0.002419, mae: 0.048026, mean_q: -0.324957
 85500/100000: episode: 855, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -17.736, mean reward: -0.177 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.389, 10.098], loss: 0.002377, mae: 0.047748, mean_q: -0.310537
 85600/100000: episode: 856, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -18.211, mean reward: -0.182 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.538, 10.265], loss: 0.002451, mae: 0.047754, mean_q: -0.323517
 85700/100000: episode: 857, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: -16.605, mean reward: -0.166 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.466, 10.259], loss: 0.002213, mae: 0.046157, mean_q: -0.337345
 85800/100000: episode: 858, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -18.204, mean reward: -0.182 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.229, 10.098], loss: 0.002513, mae: 0.048558, mean_q: -0.300357
 85900/100000: episode: 859, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -16.199, mean reward: -0.162 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.357, 10.098], loss: 0.002361, mae: 0.047243, mean_q: -0.323124
 86000/100000: episode: 860, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.637, mean reward: -0.196 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.386, 10.116], loss: 0.002307, mae: 0.047129, mean_q: -0.313918
 86100/100000: episode: 861, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.232, mean reward: -0.182 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.833, 10.098], loss: 0.002124, mae: 0.044652, mean_q: -0.313933
 86200/100000: episode: 862, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -19.966, mean reward: -0.200 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.410, 10.299], loss: 0.002777, mae: 0.051776, mean_q: -0.318184
 86300/100000: episode: 863, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.786, mean reward: -0.168 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.313, 10.098], loss: 0.002344, mae: 0.046797, mean_q: -0.326242
 86400/100000: episode: 864, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.280, mean reward: -0.173 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.877, 10.156], loss: 0.002228, mae: 0.046009, mean_q: -0.313711
 86500/100000: episode: 865, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -16.547, mean reward: -0.165 [-1.000, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.662, 10.098], loss: 0.002267, mae: 0.046027, mean_q: -0.337556
 86600/100000: episode: 866, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.103, mean reward: -0.191 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.501, 10.141], loss: 0.002276, mae: 0.045860, mean_q: -0.347065
 86700/100000: episode: 867, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -18.317, mean reward: -0.183 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.617, 10.219], loss: 0.002532, mae: 0.049129, mean_q: -0.298534
 86800/100000: episode: 868, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -13.623, mean reward: -0.136 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.343, 10.098], loss: 0.002375, mae: 0.046683, mean_q: -0.343335
 86900/100000: episode: 869, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -15.309, mean reward: -0.153 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.038, 10.217], loss: 0.002380, mae: 0.047579, mean_q: -0.303423
 87000/100000: episode: 870, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.284, mean reward: -0.183 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.298, 10.098], loss: 0.002502, mae: 0.048303, mean_q: -0.307251
 87100/100000: episode: 871, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.236, mean reward: -0.182 [-1.000, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.046, 10.098], loss: 0.002672, mae: 0.049838, mean_q: -0.300104
 87200/100000: episode: 872, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -13.623, mean reward: -0.136 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.317, 10.098], loss: 0.002569, mae: 0.049003, mean_q: -0.290788
 87300/100000: episode: 873, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -18.373, mean reward: -0.184 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.819, 10.120], loss: 0.002269, mae: 0.045491, mean_q: -0.320630
 87400/100000: episode: 874, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -20.316, mean reward: -0.203 [-1.000, 0.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.575, 10.231], loss: 0.002417, mae: 0.047406, mean_q: -0.358706
 87500/100000: episode: 875, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.395, mean reward: -0.174 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.517, 10.119], loss: 0.002595, mae: 0.048031, mean_q: -0.343846
 87600/100000: episode: 876, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -13.384, mean reward: -0.134 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.067, 10.098], loss: 0.002450, mae: 0.050635, mean_q: -0.322943
 87700/100000: episode: 877, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -16.377, mean reward: -0.164 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.855, 10.158], loss: 0.002376, mae: 0.046502, mean_q: -0.295107
 87800/100000: episode: 878, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -15.963, mean reward: -0.160 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.721, 10.098], loss: 0.002278, mae: 0.046792, mean_q: -0.325547
 87900/100000: episode: 879, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -12.749, mean reward: -0.127 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.301, 10.404], loss: 0.002339, mae: 0.046431, mean_q: -0.297066
 88000/100000: episode: 880, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.181, mean reward: -0.162 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.681, 10.098], loss: 0.002580, mae: 0.048632, mean_q: -0.325650
 88100/100000: episode: 881, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -17.849, mean reward: -0.178 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.254, 10.098], loss: 0.002454, mae: 0.047779, mean_q: -0.298515
 88200/100000: episode: 882, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.740, mean reward: -0.187 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.690, 10.145], loss: 0.002516, mae: 0.049392, mean_q: -0.285249
 88300/100000: episode: 883, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -14.562, mean reward: -0.146 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.136, 10.180], loss: 0.002508, mae: 0.048967, mean_q: -0.289152
 88400/100000: episode: 884, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.010, mean reward: -0.170 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.923, 10.223], loss: 0.002454, mae: 0.048223, mean_q: -0.296764
 88500/100000: episode: 885, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.619, mean reward: -0.176 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.397, 10.098], loss: 0.002588, mae: 0.049194, mean_q: -0.317648
 88600/100000: episode: 886, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -14.561, mean reward: -0.146 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.579, 10.282], loss: 0.003463, mae: 0.054377, mean_q: -0.304246
 88700/100000: episode: 887, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -15.055, mean reward: -0.151 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.605, 10.168], loss: 0.002379, mae: 0.047487, mean_q: -0.308001
 88800/100000: episode: 888, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -20.706, mean reward: -0.207 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.716, 10.151], loss: 0.002461, mae: 0.047864, mean_q: -0.326455
 88900/100000: episode: 889, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -16.038, mean reward: -0.160 [-1.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.024, 10.098], loss: 0.002496, mae: 0.047725, mean_q: -0.315044
 89000/100000: episode: 890, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -16.860, mean reward: -0.169 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.346, 10.185], loss: 0.002506, mae: 0.048659, mean_q: -0.299787
 89100/100000: episode: 891, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -16.580, mean reward: -0.166 [-1.000, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.430, 10.098], loss: 0.002481, mae: 0.047944, mean_q: -0.320949
 89200/100000: episode: 892, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -18.072, mean reward: -0.181 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.578, 10.234], loss: 0.002758, mae: 0.050360, mean_q: -0.323518
 89300/100000: episode: 893, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -18.188, mean reward: -0.182 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.117, 10.181], loss: 0.002495, mae: 0.048251, mean_q: -0.323624
 89400/100000: episode: 894, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -9.885, mean reward: -0.099 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.991, 10.098], loss: 0.002388, mae: 0.047678, mean_q: -0.336397
 89500/100000: episode: 895, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -17.084, mean reward: -0.171 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.265, 10.098], loss: 0.002666, mae: 0.050045, mean_q: -0.325539
 89600/100000: episode: 896, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -16.609, mean reward: -0.166 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.630, 10.221], loss: 0.002608, mae: 0.051328, mean_q: -0.318362
 89700/100000: episode: 897, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -16.770, mean reward: -0.168 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.479, 10.298], loss: 0.002603, mae: 0.050991, mean_q: -0.331072
 89800/100000: episode: 898, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.049, mean reward: -0.170 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.546, 10.137], loss: 0.002586, mae: 0.049038, mean_q: -0.304422
 89900/100000: episode: 899, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -18.306, mean reward: -0.183 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.873, 10.141], loss: 0.002316, mae: 0.045743, mean_q: -0.339720
 90000/100000: episode: 900, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: -18.291, mean reward: -0.183 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.695, 10.098], loss: 0.002403, mae: 0.047121, mean_q: -0.342561
 90100/100000: episode: 901, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.031, mean reward: -0.170 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.655, 10.209], loss: 0.002643, mae: 0.049234, mean_q: -0.316162
 90200/100000: episode: 902, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.479, mean reward: -0.195 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.953, 10.141], loss: 0.002509, mae: 0.048237, mean_q: -0.345883
 90300/100000: episode: 903, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -13.277, mean reward: -0.133 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.236, 10.305], loss: 0.002542, mae: 0.049186, mean_q: -0.318113
 90400/100000: episode: 904, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -17.777, mean reward: -0.178 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.376, 10.149], loss: 0.002712, mae: 0.050117, mean_q: -0.326081
 90500/100000: episode: 905, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -13.445, mean reward: -0.134 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.389, 10.411], loss: 0.002626, mae: 0.049363, mean_q: -0.324462
 90600/100000: episode: 906, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.055, mean reward: -0.181 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.132, 10.317], loss: 0.002635, mae: 0.050130, mean_q: -0.314921
 90700/100000: episode: 907, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.829, mean reward: -0.148 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.099, 10.098], loss: 0.002507, mae: 0.048803, mean_q: -0.303424
 90800/100000: episode: 908, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -17.685, mean reward: -0.177 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.054, 10.330], loss: 0.002452, mae: 0.048083, mean_q: -0.301229
 90900/100000: episode: 909, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -17.411, mean reward: -0.174 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.596, 10.183], loss: 0.002374, mae: 0.047809, mean_q: -0.314198
 91000/100000: episode: 910, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.685, mean reward: -0.187 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.392, 10.141], loss: 0.002430, mae: 0.048083, mean_q: -0.301863
 91100/100000: episode: 911, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -15.356, mean reward: -0.154 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.133, 10.188], loss: 0.002349, mae: 0.047415, mean_q: -0.302237
 91200/100000: episode: 912, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -19.996, mean reward: -0.200 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.970, 10.151], loss: 0.002562, mae: 0.049539, mean_q: -0.355703
 91300/100000: episode: 913, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -20.134, mean reward: -0.201 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.843, 10.098], loss: 0.002516, mae: 0.048602, mean_q: -0.307531
 91400/100000: episode: 914, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -20.109, mean reward: -0.201 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.034, 10.098], loss: 0.002631, mae: 0.051021, mean_q: -0.303711
 91500/100000: episode: 915, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -10.070, mean reward: -0.101 [-1.000, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.853, 10.098], loss: 0.002445, mae: 0.048094, mean_q: -0.336246
 91600/100000: episode: 916, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -13.108, mean reward: -0.131 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.827, 10.098], loss: 0.002593, mae: 0.050473, mean_q: -0.314004
 91700/100000: episode: 917, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -18.765, mean reward: -0.188 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.957, 10.100], loss: 0.003490, mae: 0.056847, mean_q: -0.300591
 91800/100000: episode: 918, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -19.804, mean reward: -0.198 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.422, 10.116], loss: 0.002393, mae: 0.047676, mean_q: -0.319774
 91900/100000: episode: 919, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.420, mean reward: -0.184 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.323, 10.098], loss: 0.002640, mae: 0.049923, mean_q: -0.309163
 92000/100000: episode: 920, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -15.540, mean reward: -0.155 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.818, 10.098], loss: 0.002453, mae: 0.048402, mean_q: -0.294820
 92100/100000: episode: 921, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -13.468, mean reward: -0.135 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.988, 10.098], loss: 0.002535, mae: 0.049181, mean_q: -0.315841
 92200/100000: episode: 922, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -15.852, mean reward: -0.159 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.685, 10.334], loss: 0.002407, mae: 0.047822, mean_q: -0.321123
 92300/100000: episode: 923, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.521, mean reward: -0.165 [-1.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.239, 10.313], loss: 0.002236, mae: 0.046145, mean_q: -0.351440
 92400/100000: episode: 924, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -18.586, mean reward: -0.186 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.231, 10.101], loss: 0.002567, mae: 0.048583, mean_q: -0.312622
 92500/100000: episode: 925, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.396, mean reward: -0.184 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.151, 10.227], loss: 0.002519, mae: 0.050060, mean_q: -0.283156
 92600/100000: episode: 926, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -13.614, mean reward: -0.136 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.923, 10.098], loss: 0.003750, mae: 0.057294, mean_q: -0.304283
 92700/100000: episode: 927, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -14.840, mean reward: -0.148 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.093, 10.098], loss: 0.004499, mae: 0.064492, mean_q: -0.323879
 92800/100000: episode: 928, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -18.818, mean reward: -0.188 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.611, 10.321], loss: 0.002546, mae: 0.049877, mean_q: -0.316535
 92900/100000: episode: 929, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.974, mean reward: -0.190 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.898, 10.098], loss: 0.002381, mae: 0.047167, mean_q: -0.308593
 93000/100000: episode: 930, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.303, mean reward: -0.183 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.207, 10.412], loss: 0.002254, mae: 0.045393, mean_q: -0.335759
 93100/100000: episode: 931, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -15.830, mean reward: -0.158 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.809, 10.361], loss: 0.002235, mae: 0.045390, mean_q: -0.353770
 93200/100000: episode: 932, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -17.628, mean reward: -0.176 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.676, 10.098], loss: 0.002353, mae: 0.046399, mean_q: -0.316458
 93300/100000: episode: 933, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -12.439, mean reward: -0.124 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.522, 10.554], loss: 0.002545, mae: 0.048733, mean_q: -0.329512
 93400/100000: episode: 934, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.071, mean reward: -0.181 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.515, 10.126], loss: 0.002502, mae: 0.048194, mean_q: -0.310724
 93500/100000: episode: 935, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -19.916, mean reward: -0.199 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.049, 10.098], loss: 0.002473, mae: 0.048207, mean_q: -0.299638
 93600/100000: episode: 936, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.403, mean reward: -0.184 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.017, 10.156], loss: 0.002650, mae: 0.049650, mean_q: -0.314732
 93700/100000: episode: 937, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -18.811, mean reward: -0.188 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.839, 10.155], loss: 0.002372, mae: 0.047191, mean_q: -0.308449
 93800/100000: episode: 938, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -19.915, mean reward: -0.199 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.837, 10.200], loss: 0.002474, mae: 0.048257, mean_q: -0.322844
 93900/100000: episode: 939, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -19.778, mean reward: -0.198 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.034, 10.151], loss: 0.002491, mae: 0.047684, mean_q: -0.311023
 94000/100000: episode: 940, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.498, mean reward: -0.145 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.325, 10.304], loss: 0.002471, mae: 0.047178, mean_q: -0.328398
 94100/100000: episode: 941, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: -19.542, mean reward: -0.195 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.211, 10.098], loss: 0.002442, mae: 0.047682, mean_q: -0.295332
 94200/100000: episode: 942, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.723, mean reward: -0.187 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.595, 10.104], loss: 0.002487, mae: 0.047625, mean_q: -0.328395
 94300/100000: episode: 943, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -16.402, mean reward: -0.164 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.219, 10.335], loss: 0.002489, mae: 0.047977, mean_q: -0.305382
 94400/100000: episode: 944, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.858, mean reward: -0.189 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.281, 10.098], loss: 0.002427, mae: 0.047084, mean_q: -0.317233
 94500/100000: episode: 945, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -13.981, mean reward: -0.140 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.208, 10.167], loss: 0.002331, mae: 0.047152, mean_q: -0.321909
 94600/100000: episode: 946, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.578, mean reward: -0.146 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.816, 10.098], loss: 0.002454, mae: 0.047512, mean_q: -0.342673
 94700/100000: episode: 947, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.314, mean reward: -0.183 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.469, 10.098], loss: 0.002322, mae: 0.046337, mean_q: -0.301324
 94800/100000: episode: 948, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -20.848, mean reward: -0.208 [-1.000, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.955, 10.198], loss: 0.002390, mae: 0.047448, mean_q: -0.322506
 94900/100000: episode: 949, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -12.838, mean reward: -0.128 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.080, 10.126], loss: 0.002293, mae: 0.046415, mean_q: -0.335937
 95000/100000: episode: 950, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.297, mean reward: -0.173 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.364, 10.098], loss: 0.002290, mae: 0.046212, mean_q: -0.318455
 95100/100000: episode: 951, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.376, mean reward: -0.174 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.980, 10.111], loss: 0.002393, mae: 0.048559, mean_q: -0.298603
 95200/100000: episode: 952, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -15.960, mean reward: -0.160 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.776, 10.262], loss: 0.002636, mae: 0.050899, mean_q: -0.286949
 95300/100000: episode: 953, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.487, mean reward: -0.175 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.922, 10.098], loss: 0.002556, mae: 0.049266, mean_q: -0.332449
 95400/100000: episode: 954, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -15.187, mean reward: -0.152 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.105, 10.098], loss: 0.002576, mae: 0.048917, mean_q: -0.328313
 95500/100000: episode: 955, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.224, mean reward: -0.172 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.529, 10.098], loss: 0.002488, mae: 0.048207, mean_q: -0.321071
 95600/100000: episode: 956, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -19.234, mean reward: -0.192 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.747, 10.300], loss: 0.002338, mae: 0.046184, mean_q: -0.319712
 95700/100000: episode: 957, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.025, mean reward: -0.180 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.246, 10.098], loss: 0.002347, mae: 0.046576, mean_q: -0.336228
 95800/100000: episode: 958, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -15.538, mean reward: -0.155 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.087, 10.136], loss: 0.002390, mae: 0.047393, mean_q: -0.328378
 95900/100000: episode: 959, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -13.074, mean reward: -0.131 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.571, 10.521], loss: 0.002500, mae: 0.048298, mean_q: -0.299989
 96000/100000: episode: 960, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -14.997, mean reward: -0.150 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.074, 10.098], loss: 0.002381, mae: 0.047190, mean_q: -0.310722
 96100/100000: episode: 961, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -15.792, mean reward: -0.158 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.883, 10.349], loss: 0.002355, mae: 0.047120, mean_q: -0.336481
 96200/100000: episode: 962, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.484, mean reward: -0.195 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.992, 10.098], loss: 0.002545, mae: 0.048034, mean_q: -0.324213
 96300/100000: episode: 963, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.743, mean reward: -0.187 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.536, 10.150], loss: 0.005048, mae: 0.060911, mean_q: -0.306277
 96400/100000: episode: 964, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -17.469, mean reward: -0.175 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.341, 10.098], loss: 0.003085, mae: 0.055098, mean_q: -0.307361
 96500/100000: episode: 965, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -17.613, mean reward: -0.176 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.551, 10.098], loss: 0.002549, mae: 0.048866, mean_q: -0.318156
 96600/100000: episode: 966, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -17.826, mean reward: -0.178 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.885, 10.098], loss: 0.002499, mae: 0.048141, mean_q: -0.327646
 96700/100000: episode: 967, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -16.877, mean reward: -0.169 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.457, 10.098], loss: 0.002523, mae: 0.048487, mean_q: -0.298385
 96800/100000: episode: 968, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -20.189, mean reward: -0.202 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.585, 10.098], loss: 0.002361, mae: 0.046591, mean_q: -0.344613
 96900/100000: episode: 969, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -18.124, mean reward: -0.181 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.975, 10.178], loss: 0.002304, mae: 0.046234, mean_q: -0.311444
 97000/100000: episode: 970, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.227, mean reward: -0.162 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.486, 10.098], loss: 0.002447, mae: 0.047368, mean_q: -0.320011
 97100/100000: episode: 971, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.887, mean reward: -0.169 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.751, 10.128], loss: 0.002543, mae: 0.049260, mean_q: -0.322898
 97200/100000: episode: 972, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -14.476, mean reward: -0.145 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.544, 10.277], loss: 0.002370, mae: 0.047460, mean_q: -0.316774
 97300/100000: episode: 973, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -17.867, mean reward: -0.179 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.097, 10.166], loss: 0.002286, mae: 0.046247, mean_q: -0.333094
 97400/100000: episode: 974, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -13.030, mean reward: -0.130 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-0.923, 10.098], loss: 0.002345, mae: 0.046481, mean_q: -0.343340
 97500/100000: episode: 975, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -17.723, mean reward: -0.177 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.925, 10.241], loss: 0.002397, mae: 0.047323, mean_q: -0.325805
 97600/100000: episode: 976, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.425, mean reward: -0.144 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.551, 10.292], loss: 0.002223, mae: 0.045469, mean_q: -0.336836
 97700/100000: episode: 977, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -11.783, mean reward: -0.118 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.470, 10.426], loss: 0.002415, mae: 0.047860, mean_q: -0.335901
 97800/100000: episode: 978, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.249, mean reward: -0.142 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.329, 10.098], loss: 0.002124, mae: 0.045012, mean_q: -0.337717
 97900/100000: episode: 979, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -13.646, mean reward: -0.136 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.210, 10.098], loss: 0.002246, mae: 0.046851, mean_q: -0.315445
 98000/100000: episode: 980, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.029, mean reward: -0.160 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.835, 10.098], loss: 0.002354, mae: 0.046621, mean_q: -0.324484
 98100/100000: episode: 981, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: -14.064, mean reward: -0.141 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.303, 10.098], loss: 0.002460, mae: 0.048779, mean_q: -0.339019
 98200/100000: episode: 982, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.759, mean reward: -0.178 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.934, 10.269], loss: 0.002282, mae: 0.046656, mean_q: -0.331328
 98300/100000: episode: 983, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -15.248, mean reward: -0.152 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.636, 10.098], loss: 0.002268, mae: 0.046211, mean_q: -0.335066
 98400/100000: episode: 984, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.939, mean reward: -0.179 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.528, 10.098], loss: 0.002422, mae: 0.047491, mean_q: -0.328260
 98500/100000: episode: 985, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.891, mean reward: -0.179 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.022, 10.124], loss: 0.002364, mae: 0.047366, mean_q: -0.301265
 98600/100000: episode: 986, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -9.782, mean reward: -0.098 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.969, 10.098], loss: 0.002380, mae: 0.047736, mean_q: -0.295423
 98700/100000: episode: 987, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -18.187, mean reward: -0.182 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.666, 10.098], loss: 0.002326, mae: 0.048429, mean_q: -0.293955
 98800/100000: episode: 988, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -15.200, mean reward: -0.152 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.903, 10.244], loss: 0.002286, mae: 0.046469, mean_q: -0.306384
 98900/100000: episode: 989, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.613, mean reward: -0.186 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.245, 10.168], loss: 0.002253, mae: 0.046877, mean_q: -0.301589
 99000/100000: episode: 990, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -16.322, mean reward: -0.163 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.675, 10.127], loss: 0.005029, mae: 0.060519, mean_q: -0.283174
 99100/100000: episode: 991, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -16.408, mean reward: -0.164 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.168, 10.307], loss: 0.004023, mae: 0.059992, mean_q: -0.316415
 99200/100000: episode: 992, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -16.759, mean reward: -0.168 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.423, 10.218], loss: 0.002449, mae: 0.049039, mean_q: -0.305689
 99300/100000: episode: 993, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.200, mean reward: -0.172 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.899, 10.170], loss: 0.002200, mae: 0.046128, mean_q: -0.288819
 99400/100000: episode: 994, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -16.684, mean reward: -0.167 [-1.000, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.594, 10.098], loss: 0.002479, mae: 0.048647, mean_q: -0.301799
 99500/100000: episode: 995, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.650, mean reward: -0.197 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.756, 10.125], loss: 0.002324, mae: 0.046206, mean_q: -0.313231
 99600/100000: episode: 996, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -18.672, mean reward: -0.187 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.570, 10.098], loss: 0.002382, mae: 0.047886, mean_q: -0.300613
 99700/100000: episode: 997, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -17.378, mean reward: -0.174 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.588, 10.098], loss: 0.002269, mae: 0.045895, mean_q: -0.334905
 99800/100000: episode: 998, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -14.285, mean reward: -0.143 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.312, 10.104], loss: 0.002310, mae: 0.046613, mean_q: -0.325441
 99900/100000: episode: 999, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -10.859, mean reward: -0.109 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.766, 10.297], loss: 0.002348, mae: 0.047849, mean_q: -0.275282
 100000/100000: episode: 1000, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -16.441, mean reward: -0.164 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.702, 10.297], loss: 0.002398, mae: 0.047056, mean_q: -0.288011
done, took 565.390 seconds
[Info] End Uniform Random Simulation. Falsification occurred 0 times.
