Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.174s, episode steps: 100, steps per second: 573, episode reward: -18.827, mean reward: -0.188 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.167, 10.164], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.070s, episode steps: 100, steps per second: 1438, episode reward: -15.198, mean reward: -0.152 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.743, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.071s, episode steps: 100, steps per second: 1416, episode reward: -19.049, mean reward: -0.190 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.746, 10.215], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.064s, episode steps: 100, steps per second: 1564, episode reward: -14.281, mean reward: -0.143 [-1.000, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.397, 10.124], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.063s, episode steps: 100, steps per second: 1588, episode reward: -17.132, mean reward: -0.171 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-2.471, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.230s, episode steps: 100, steps per second: 81, episode reward: -18.681, mean reward: -0.187 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.532, 10.183], loss: 0.070268, mae: 0.264448, mean_q: -0.421678
   700/100000: episode: 7, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -12.784, mean reward: -0.128 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.099, 10.098], loss: 0.019681, mae: 0.138726, mean_q: -0.328545
   800/100000: episode: 8, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -15.452, mean reward: -0.155 [-1.000, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.776, 10.103], loss: 0.015624, mae: 0.123839, mean_q: -0.291282
   900/100000: episode: 9, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.209, mean reward: -0.172 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.076, 10.150], loss: 0.013650, mae: 0.111560, mean_q: -0.325186
  1000/100000: episode: 10, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -15.869, mean reward: -0.159 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.719, 10.098], loss: 0.012603, mae: 0.107757, mean_q: -0.279981
  1100/100000: episode: 11, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -14.074, mean reward: -0.141 [-1.000, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.021, 10.337], loss: 0.011412, mae: 0.099835, mean_q: -0.310525
  1200/100000: episode: 12, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.936, mean reward: -0.159 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.686, 10.098], loss: 0.010218, mae: 0.097132, mean_q: -0.297513
  1300/100000: episode: 13, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.219, mean reward: -0.172 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.889, 10.117], loss: 0.011125, mae: 0.097824, mean_q: -0.318532
  1400/100000: episode: 14, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -9.351, mean reward: -0.094 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.979, 10.098], loss: 0.010858, mae: 0.098834, mean_q: -0.302174
  1500/100000: episode: 15, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.502, mean reward: -0.195 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.842, 10.228], loss: 0.009636, mae: 0.093998, mean_q: -0.308793
  1600/100000: episode: 16, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -6.964, mean reward: -0.070 [-1.000, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.532, 10.098], loss: 0.010121, mae: 0.098166, mean_q: -0.321408
  1700/100000: episode: 17, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -17.650, mean reward: -0.177 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.500, 10.098], loss: 0.010481, mae: 0.098797, mean_q: -0.315693
  1800/100000: episode: 18, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.496, mean reward: -0.195 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.954, 10.098], loss: 0.011488, mae: 0.104268, mean_q: -0.322932
  1900/100000: episode: 19, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.566, mean reward: -0.176 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.804, 10.098], loss: 0.009317, mae: 0.091560, mean_q: -0.304668
  2000/100000: episode: 20, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -16.311, mean reward: -0.163 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.341, 10.189], loss: 0.008786, mae: 0.091883, mean_q: -0.302466
  2100/100000: episode: 21, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.765, mean reward: -0.198 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.454, 10.098], loss: 0.008502, mae: 0.088754, mean_q: -0.326408
  2200/100000: episode: 22, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -12.900, mean reward: -0.129 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.990, 10.446], loss: 0.009235, mae: 0.092284, mean_q: -0.304736
  2300/100000: episode: 23, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -20.490, mean reward: -0.205 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.753, 10.182], loss: 0.008275, mae: 0.090361, mean_q: -0.306805
  2400/100000: episode: 24, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -20.276, mean reward: -0.203 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.036, 10.098], loss: 0.008142, mae: 0.089425, mean_q: -0.329635
  2500/100000: episode: 25, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.016, mean reward: -0.160 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.804, 10.098], loss: 0.007854, mae: 0.088885, mean_q: -0.297960
  2600/100000: episode: 26, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.815, mean reward: -0.168 [-1.000, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.556, 10.098], loss: 0.008613, mae: 0.091016, mean_q: -0.345883
  2700/100000: episode: 27, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.807, mean reward: -0.198 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.278, 10.098], loss: 0.007849, mae: 0.087511, mean_q: -0.338446
  2800/100000: episode: 28, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -11.345, mean reward: -0.113 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.410, 10.144], loss: 0.006686, mae: 0.081944, mean_q: -0.327253
  2900/100000: episode: 29, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.286, mean reward: -0.153 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.635, 10.098], loss: 0.008211, mae: 0.087495, mean_q: -0.319798
  3000/100000: episode: 30, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.404, mean reward: -0.174 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.138, 10.098], loss: 0.007872, mae: 0.087306, mean_q: -0.326513
  3100/100000: episode: 31, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -12.273, mean reward: -0.123 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.337, 10.098], loss: 0.008439, mae: 0.090513, mean_q: -0.306118
  3200/100000: episode: 32, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -14.586, mean reward: -0.146 [-1.000, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.425, 10.098], loss: 0.007170, mae: 0.084626, mean_q: -0.279379
  3300/100000: episode: 33, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.008, mean reward: -0.160 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.290, 10.198], loss: 0.006000, mae: 0.077161, mean_q: -0.282390
  3400/100000: episode: 34, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -9.765, mean reward: -0.098 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.448, 10.098], loss: 0.007491, mae: 0.084248, mean_q: -0.316642
  3500/100000: episode: 35, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.832, mean reward: -0.188 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.685, 10.141], loss: 0.007573, mae: 0.083709, mean_q: -0.334298
  3600/100000: episode: 36, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.444, mean reward: -0.194 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.834, 10.114], loss: 0.007679, mae: 0.086758, mean_q: -0.297463
  3700/100000: episode: 37, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -20.605, mean reward: -0.206 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.291, 10.220], loss: 0.006661, mae: 0.079886, mean_q: -0.316593
  3800/100000: episode: 38, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -15.498, mean reward: -0.155 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.023, 10.098], loss: 0.006555, mae: 0.081837, mean_q: -0.301433
  3900/100000: episode: 39, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -13.301, mean reward: -0.133 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.617, 10.421], loss: 0.006615, mae: 0.080400, mean_q: -0.306874
  4000/100000: episode: 40, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -20.255, mean reward: -0.203 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.842, 10.162], loss: 0.007245, mae: 0.086006, mean_q: -0.278668
  4100/100000: episode: 41, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.808, mean reward: -0.198 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.012, 10.176], loss: 0.007179, mae: 0.082029, mean_q: -0.308303
  4200/100000: episode: 42, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.511, mean reward: -0.195 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.963, 10.098], loss: 0.005907, mae: 0.076812, mean_q: -0.333290
  4300/100000: episode: 43, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -20.446, mean reward: -0.204 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.708, 10.098], loss: 0.007282, mae: 0.083714, mean_q: -0.319039
  4400/100000: episode: 44, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.133, mean reward: -0.181 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.715, 10.147], loss: 0.006199, mae: 0.081003, mean_q: -0.321163
  4500/100000: episode: 45, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.552, mean reward: -0.176 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.089, 10.143], loss: 0.006017, mae: 0.078523, mean_q: -0.298665
  4600/100000: episode: 46, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -14.921, mean reward: -0.149 [-1.000, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.561, 10.201], loss: 0.005925, mae: 0.076688, mean_q: -0.297709
  4700/100000: episode: 47, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.549, mean reward: -0.185 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.211, 10.098], loss: 0.006356, mae: 0.078717, mean_q: -0.297274
  4800/100000: episode: 48, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.925, mean reward: -0.189 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.380, 10.220], loss: 0.005681, mae: 0.075573, mean_q: -0.300552
  4900/100000: episode: 49, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -14.382, mean reward: -0.144 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.710, 10.098], loss: 0.005385, mae: 0.072282, mean_q: -0.310064
  5000/100000: episode: 50, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -12.180, mean reward: -0.122 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.843, 10.409], loss: 0.006458, mae: 0.080268, mean_q: -0.322065
  5100/100000: episode: 51, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -18.427, mean reward: -0.184 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.762, 10.176], loss: 0.005562, mae: 0.074759, mean_q: -0.294205
  5200/100000: episode: 52, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -19.398, mean reward: -0.194 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.396, 10.098], loss: 0.005555, mae: 0.076080, mean_q: -0.324493
  5300/100000: episode: 53, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -17.806, mean reward: -0.178 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.624, 10.098], loss: 0.005799, mae: 0.075609, mean_q: -0.346047
  5400/100000: episode: 54, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -18.004, mean reward: -0.180 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.672, 10.098], loss: 0.005280, mae: 0.073088, mean_q: -0.284539
  5500/100000: episode: 55, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -20.750, mean reward: -0.208 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.844, 10.176], loss: 0.005427, mae: 0.072455, mean_q: -0.325686
  5600/100000: episode: 56, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -15.179, mean reward: -0.152 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.175, 10.098], loss: 0.005329, mae: 0.072828, mean_q: -0.312103
  5700/100000: episode: 57, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -19.092, mean reward: -0.191 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.714, 10.333], loss: 0.005064, mae: 0.072441, mean_q: -0.311750
  5800/100000: episode: 58, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -13.537, mean reward: -0.135 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.245, 10.098], loss: 0.005732, mae: 0.075887, mean_q: -0.354067
  5900/100000: episode: 59, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.740, mean reward: -0.197 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.206, 10.130], loss: 0.005240, mae: 0.073534, mean_q: -0.301093
  6000/100000: episode: 60, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.660, mean reward: -0.157 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.032, 10.098], loss: 0.005707, mae: 0.075903, mean_q: -0.338785
  6100/100000: episode: 61, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.136, mean reward: -0.191 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.022, 10.344], loss: 0.005561, mae: 0.074301, mean_q: -0.329944
  6200/100000: episode: 62, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -11.596, mean reward: -0.116 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.662, 10.311], loss: 0.005393, mae: 0.077423, mean_q: -0.333972
  6300/100000: episode: 63, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.960, mean reward: -0.190 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.189, 10.140], loss: 0.005392, mae: 0.073500, mean_q: -0.328181
  6400/100000: episode: 64, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.718, mean reward: -0.167 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.514, 10.151], loss: 0.005914, mae: 0.079993, mean_q: -0.325561
  6500/100000: episode: 65, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.782, mean reward: -0.198 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.718, 10.214], loss: 0.004885, mae: 0.071035, mean_q: -0.325363
  6600/100000: episode: 66, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -14.290, mean reward: -0.143 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.677, 10.098], loss: 0.005239, mae: 0.071904, mean_q: -0.320667
  6700/100000: episode: 67, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.791, mean reward: -0.178 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.721, 10.292], loss: 0.004467, mae: 0.069503, mean_q: -0.374294
  6800/100000: episode: 68, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.475, mean reward: -0.155 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.761, 10.098], loss: 0.005123, mae: 0.073785, mean_q: -0.332647
  6900/100000: episode: 69, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.787, mean reward: -0.168 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.396, 10.176], loss: 0.005781, mae: 0.077969, mean_q: -0.332806
  7000/100000: episode: 70, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -20.605, mean reward: -0.206 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.620, 10.177], loss: 0.005163, mae: 0.073880, mean_q: -0.308628
  7100/100000: episode: 71, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.505, mean reward: -0.175 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.854, 10.244], loss: 0.004599, mae: 0.071305, mean_q: -0.303372
  7200/100000: episode: 72, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -11.413, mean reward: -0.114 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.127, 10.098], loss: 0.004803, mae: 0.073189, mean_q: -0.323467
  7300/100000: episode: 73, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.324, mean reward: -0.183 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.536, 10.117], loss: 0.004774, mae: 0.071501, mean_q: -0.324985
  7400/100000: episode: 74, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.555, mean reward: -0.166 [-1.000, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.249, 10.098], loss: 0.004530, mae: 0.070913, mean_q: -0.307125
  7500/100000: episode: 75, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -16.317, mean reward: -0.163 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.341, 10.098], loss: 0.004465, mae: 0.070085, mean_q: -0.330088
  7600/100000: episode: 76, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.807, mean reward: -0.168 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.716, 10.297], loss: 0.004376, mae: 0.069785, mean_q: -0.328669
  7700/100000: episode: 77, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -15.154, mean reward: -0.152 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.178, 10.342], loss: 0.004745, mae: 0.073114, mean_q: -0.306375
  7800/100000: episode: 78, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.769, mean reward: -0.178 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.924, 10.265], loss: 0.004894, mae: 0.072691, mean_q: -0.307742
  7900/100000: episode: 79, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -11.692, mean reward: -0.117 [-1.000, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.255, 10.435], loss: 0.008132, mae: 0.086179, mean_q: -0.328357
  8000/100000: episode: 80, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -12.980, mean reward: -0.130 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.779, 10.281], loss: 0.004650, mae: 0.070084, mean_q: -0.333066
  8100/100000: episode: 81, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -19.053, mean reward: -0.191 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.692, 10.181], loss: 0.004404, mae: 0.068212, mean_q: -0.320571
  8200/100000: episode: 82, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -16.070, mean reward: -0.161 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.000, 10.131], loss: 0.004412, mae: 0.070452, mean_q: -0.293353
  8300/100000: episode: 83, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -15.221, mean reward: -0.152 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.629, 10.196], loss: 0.004103, mae: 0.066480, mean_q: -0.310481
  8400/100000: episode: 84, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -16.572, mean reward: -0.166 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.074, 10.098], loss: 0.004167, mae: 0.065557, mean_q: -0.316001
  8500/100000: episode: 85, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -11.666, mean reward: -0.117 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.473, 10.098], loss: 0.004145, mae: 0.065574, mean_q: -0.315209
  8600/100000: episode: 86, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -11.976, mean reward: -0.120 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.289, 10.413], loss: 0.004115, mae: 0.066296, mean_q: -0.310280
  8700/100000: episode: 87, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -12.345, mean reward: -0.123 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.246, 10.098], loss: 0.004639, mae: 0.070839, mean_q: -0.283055
  8800/100000: episode: 88, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.123, mean reward: -0.191 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.178, 10.110], loss: 0.004269, mae: 0.068393, mean_q: -0.320143
  8900/100000: episode: 89, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.220, mean reward: -0.152 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.405, 10.098], loss: 0.004675, mae: 0.071955, mean_q: -0.357753
  9000/100000: episode: 90, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.465, mean reward: -0.185 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.587, 10.177], loss: 0.004216, mae: 0.068395, mean_q: -0.320186
  9100/100000: episode: 91, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.564, mean reward: -0.196 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.382, 10.098], loss: 0.004532, mae: 0.071483, mean_q: -0.314625
  9200/100000: episode: 92, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.081, mean reward: -0.161 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.656, 10.098], loss: 0.004142, mae: 0.067186, mean_q: -0.310438
  9300/100000: episode: 93, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.369, mean reward: -0.164 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.835, 10.142], loss: 0.004314, mae: 0.068495, mean_q: -0.341647
  9400/100000: episode: 94, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.298, mean reward: -0.173 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.840, 10.098], loss: 0.003725, mae: 0.063985, mean_q: -0.322673
  9500/100000: episode: 95, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.631, mean reward: -0.166 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.202, 10.432], loss: 0.003890, mae: 0.064930, mean_q: -0.312607
  9600/100000: episode: 96, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.263, mean reward: -0.183 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.738, 10.098], loss: 0.003788, mae: 0.064674, mean_q: -0.282767
  9700/100000: episode: 97, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.050, mean reward: -0.191 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.953, 10.224], loss: 0.004183, mae: 0.068400, mean_q: -0.315621
  9800/100000: episode: 98, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -17.796, mean reward: -0.178 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.292, 10.098], loss: 0.005665, mae: 0.078155, mean_q: -0.312330
  9900/100000: episode: 99, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -19.375, mean reward: -0.194 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.646, 10.098], loss: 0.003863, mae: 0.066899, mean_q: -0.298984
 10000/100000: episode: 100, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: -16.700, mean reward: -0.167 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.970, 10.134], loss: 0.004180, mae: 0.068577, mean_q: -0.325107
 10100/100000: episode: 101, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -20.736, mean reward: -0.207 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.790, 10.120], loss: 0.003734, mae: 0.064628, mean_q: -0.314361
 10200/100000: episode: 102, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -18.183, mean reward: -0.182 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.238, 10.098], loss: 0.004395, mae: 0.070361, mean_q: -0.298443
 10300/100000: episode: 103, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: -18.208, mean reward: -0.182 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.882, 10.098], loss: 0.004034, mae: 0.067711, mean_q: -0.302803
 10400/100000: episode: 104, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.097, mean reward: -0.181 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.862, 10.098], loss: 0.003861, mae: 0.065638, mean_q: -0.315979
 10500/100000: episode: 105, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.413, mean reward: -0.184 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.953, 10.169], loss: 0.004234, mae: 0.070199, mean_q: -0.284396
 10600/100000: episode: 106, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -15.326, mean reward: -0.153 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.234, 10.175], loss: 0.004011, mae: 0.065954, mean_q: -0.328458
 10700/100000: episode: 107, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -19.051, mean reward: -0.191 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.257, 10.215], loss: 0.003801, mae: 0.067577, mean_q: -0.322820
 10800/100000: episode: 108, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -17.071, mean reward: -0.171 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.956, 10.259], loss: 0.003870, mae: 0.066653, mean_q: -0.331372
 10900/100000: episode: 109, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: -14.374, mean reward: -0.144 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.142, 10.305], loss: 0.003917, mae: 0.066242, mean_q: -0.312992
 11000/100000: episode: 110, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.931, mean reward: -0.169 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.134, 10.312], loss: 0.003870, mae: 0.066099, mean_q: -0.307455
 11100/100000: episode: 111, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.104, mean reward: -0.181 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.505, 10.165], loss: 0.003510, mae: 0.061244, mean_q: -0.336118
 11200/100000: episode: 112, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.831, mean reward: -0.198 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.732, 10.299], loss: 0.003964, mae: 0.065686, mean_q: -0.322041
 11300/100000: episode: 113, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.147, mean reward: -0.161 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.434, 10.098], loss: 0.003554, mae: 0.062761, mean_q: -0.293536
 11400/100000: episode: 114, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.926, mean reward: -0.169 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.855, 10.383], loss: 0.003828, mae: 0.065546, mean_q: -0.308167
 11500/100000: episode: 115, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.725, mean reward: -0.177 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.210, 10.098], loss: 0.003231, mae: 0.059459, mean_q: -0.313738
 11600/100000: episode: 116, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.285, mean reward: -0.163 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.723, 10.179], loss: 0.003985, mae: 0.065263, mean_q: -0.315380
 11700/100000: episode: 117, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.517, mean reward: -0.185 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.367, 10.098], loss: 0.004708, mae: 0.071329, mean_q: -0.308628
 11800/100000: episode: 118, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.525, mean reward: -0.175 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.844, 10.176], loss: 0.003342, mae: 0.061044, mean_q: -0.319302
 11900/100000: episode: 119, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -17.337, mean reward: -0.173 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.913, 10.098], loss: 0.004014, mae: 0.067695, mean_q: -0.325263
 12000/100000: episode: 120, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.532, mean reward: -0.175 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.658, 10.098], loss: 0.003077, mae: 0.057960, mean_q: -0.321118
 12100/100000: episode: 121, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: -11.744, mean reward: -0.117 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.264, 10.098], loss: 0.003561, mae: 0.063531, mean_q: -0.324149
 12200/100000: episode: 122, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -11.536, mean reward: -0.115 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.938, 10.098], loss: 0.003571, mae: 0.062031, mean_q: -0.299187
 12300/100000: episode: 123, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -18.670, mean reward: -0.187 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.921, 10.101], loss: 0.003740, mae: 0.064754, mean_q: -0.311540
 12400/100000: episode: 124, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -17.891, mean reward: -0.179 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.604, 10.120], loss: 0.003303, mae: 0.060869, mean_q: -0.308403
 12500/100000: episode: 125, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.846, mean reward: -0.168 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.303, 10.098], loss: 0.002941, mae: 0.056105, mean_q: -0.332659
 12600/100000: episode: 126, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -16.564, mean reward: -0.166 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.185, 10.185], loss: 0.003285, mae: 0.059919, mean_q: -0.316396
 12700/100000: episode: 127, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -16.375, mean reward: -0.164 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.404, 10.098], loss: 0.003072, mae: 0.058198, mean_q: -0.307643
 12800/100000: episode: 128, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.301, mean reward: -0.173 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.874, 10.098], loss: 0.003336, mae: 0.061059, mean_q: -0.290169
 12900/100000: episode: 129, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.351, mean reward: -0.174 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.644, 10.098], loss: 0.004132, mae: 0.065335, mean_q: -0.292519
 13000/100000: episode: 130, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.085, mean reward: -0.191 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.138, 10.098], loss: 0.006010, mae: 0.075964, mean_q: -0.280854
 13100/100000: episode: 131, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.799, mean reward: -0.168 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.778, 10.098], loss: 0.003303, mae: 0.060465, mean_q: -0.344332
 13200/100000: episode: 132, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -12.417, mean reward: -0.124 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.801, 10.148], loss: 0.003129, mae: 0.058520, mean_q: -0.336731
 13300/100000: episode: 133, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.032, mean reward: -0.170 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.493, 10.452], loss: 0.003161, mae: 0.058740, mean_q: -0.291965
 13400/100000: episode: 134, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -14.222, mean reward: -0.142 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.501, 10.098], loss: 0.003048, mae: 0.057872, mean_q: -0.332127
 13500/100000: episode: 135, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.229, mean reward: -0.182 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.103, 10.139], loss: 0.003557, mae: 0.061359, mean_q: -0.341304
 13600/100000: episode: 136, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.065, mean reward: -0.171 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.767, 10.098], loss: 0.003776, mae: 0.066304, mean_q: -0.338284
 13700/100000: episode: 137, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.249, mean reward: -0.192 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.681, 10.122], loss: 0.002972, mae: 0.056103, mean_q: -0.329217
 13800/100000: episode: 138, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.740, mean reward: -0.157 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.533, 10.229], loss: 0.004493, mae: 0.068190, mean_q: -0.303408
 13900/100000: episode: 139, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.526, mean reward: -0.155 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.939, 10.451], loss: 0.003046, mae: 0.057498, mean_q: -0.290965
 14000/100000: episode: 140, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.095, mean reward: -0.161 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.910, 10.098], loss: 0.002869, mae: 0.056204, mean_q: -0.328221
 14100/100000: episode: 141, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.765, mean reward: -0.148 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.345, 10.440], loss: 0.003079, mae: 0.057144, mean_q: -0.320559
 14200/100000: episode: 142, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -20.591, mean reward: -0.206 [-1.000, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.659, 10.098], loss: 0.002906, mae: 0.056435, mean_q: -0.341546
 14300/100000: episode: 143, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.292, mean reward: -0.173 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.313, 10.150], loss: 0.003123, mae: 0.057517, mean_q: -0.324901
 14400/100000: episode: 144, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.343, mean reward: -0.163 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.385, 10.098], loss: 0.003101, mae: 0.058051, mean_q: -0.306973
 14500/100000: episode: 145, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.949, mean reward: -0.169 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.762, 10.098], loss: 0.003126, mae: 0.058504, mean_q: -0.349816
 14600/100000: episode: 146, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.701, mean reward: -0.147 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.734, 10.098], loss: 0.003128, mae: 0.058982, mean_q: -0.309571
 14700/100000: episode: 147, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.259, mean reward: -0.123 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.325, 10.098], loss: 0.002809, mae: 0.054615, mean_q: -0.338049
 14800/100000: episode: 148, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -21.357, mean reward: -0.214 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.650, 10.098], loss: 0.002784, mae: 0.054077, mean_q: -0.322970
 14900/100000: episode: 149, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.231, mean reward: -0.182 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.584, 10.098], loss: 0.003126, mae: 0.057981, mean_q: -0.318577
[Info] 100-TH LEVEL FOUND: 0.4297267198562622, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.412s, episode steps: 100, steps per second: 23, episode reward: -19.567, mean reward: -0.196 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.632, 10.098], loss: 0.004078, mae: 0.065886, mean_q: -0.297371
 15020/100000: episode: 151, duration: 0.120s, episode steps: 20, steps per second: 166, episode reward: 3.553, mean reward: 0.178 [0.062, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.343, 10.194], loss: 0.003338, mae: 0.062481, mean_q: -0.275223
 15033/100000: episode: 152, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 3.466, mean reward: 0.267 [0.176, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.441, 10.360], loss: 0.003669, mae: 0.066950, mean_q: -0.297665
 15059/100000: episode: 153, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 5.508, mean reward: 0.212 [0.126, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.374, 10.239], loss: 0.003271, mae: 0.062233, mean_q: -0.334540
 15072/100000: episode: 154, duration: 0.065s, episode steps: 13, steps per second: 198, episode reward: 3.318, mean reward: 0.255 [0.161, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.047, 10.319], loss: 0.002769, mae: 0.055389, mean_q: -0.222504
 15085/100000: episode: 155, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 2.166, mean reward: 0.167 [0.123, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.256], loss: 0.002947, mae: 0.055285, mean_q: -0.413331
 15114/100000: episode: 156, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 5.823, mean reward: 0.201 [0.104, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-1.211, 10.294], loss: 0.003185, mae: 0.060501, mean_q: -0.340884
 15134/100000: episode: 157, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 4.920, mean reward: 0.246 [0.155, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.521, 10.338], loss: 0.002744, mae: 0.055042, mean_q: -0.349787
 15171/100000: episode: 158, duration: 0.197s, episode steps: 37, steps per second: 187, episode reward: 12.310, mean reward: 0.333 [0.101, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.809, 10.614], loss: 0.002783, mae: 0.053235, mean_q: -0.371529
 15205/100000: episode: 159, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 10.535, mean reward: 0.310 [0.126, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-1.401, 10.274], loss: 0.003199, mae: 0.059108, mean_q: -0.260021
 15237/100000: episode: 160, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 10.179, mean reward: 0.318 [0.185, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.154, 10.368], loss: 0.002782, mae: 0.053085, mean_q: -0.342205
 15266/100000: episode: 161, duration: 0.156s, episode steps: 29, steps per second: 185, episode reward: 7.820, mean reward: 0.270 [0.082, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.312, 10.456], loss: 0.002946, mae: 0.054212, mean_q: -0.264414
 15286/100000: episode: 162, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 3.426, mean reward: 0.171 [0.089, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.050, 10.188], loss: 0.002695, mae: 0.051261, mean_q: -0.338761
 15299/100000: episode: 163, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 2.628, mean reward: 0.202 [0.162, 0.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.296], loss: 0.002879, mae: 0.056461, mean_q: -0.136024
 15331/100000: episode: 164, duration: 0.157s, episode steps: 32, steps per second: 204, episode reward: 7.628, mean reward: 0.238 [0.073, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.524, 10.208], loss: 0.003348, mae: 0.060860, mean_q: -0.268324
 15368/100000: episode: 165, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 13.432, mean reward: 0.363 [0.183, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.484, 10.548], loss: 0.004209, mae: 0.068369, mean_q: -0.289156
 15400/100000: episode: 166, duration: 0.176s, episode steps: 32, steps per second: 181, episode reward: 10.748, mean reward: 0.336 [0.237, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.747, 10.448], loss: 0.002962, mae: 0.056896, mean_q: -0.257538
 15429/100000: episode: 167, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 7.159, mean reward: 0.247 [0.135, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.051, 10.252], loss: 0.003040, mae: 0.056523, mean_q: -0.247092
 15442/100000: episode: 168, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 5.021, mean reward: 0.386 [0.245, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.448, 10.562], loss: 0.002454, mae: 0.050569, mean_q: -0.309358
 15476/100000: episode: 169, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 14.319, mean reward: 0.421 [0.147, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.103, 10.540], loss: 0.003150, mae: 0.057802, mean_q: -0.242638
 15499/100000: episode: 170, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 4.514, mean reward: 0.196 [0.097, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.181, 10.318], loss: 0.002706, mae: 0.054635, mean_q: -0.225677
 15512/100000: episode: 171, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 4.859, mean reward: 0.374 [0.296, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.894, 10.553], loss: 0.003655, mae: 0.063739, mean_q: -0.241236
 15541/100000: episode: 172, duration: 0.159s, episode steps: 29, steps per second: 183, episode reward: 9.561, mean reward: 0.330 [0.203, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.611, 10.446], loss: 0.002822, mae: 0.054513, mean_q: -0.212554
 15573/100000: episode: 173, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 6.725, mean reward: 0.210 [0.102, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.195, 10.176], loss: 0.003180, mae: 0.058184, mean_q: -0.222300
 15599/100000: episode: 174, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 5.568, mean reward: 0.214 [0.024, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.201, 10.133], loss: 0.002932, mae: 0.055746, mean_q: -0.157589
 15636/100000: episode: 175, duration: 0.184s, episode steps: 37, steps per second: 201, episode reward: 8.279, mean reward: 0.224 [0.060, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.365, 10.184], loss: 0.003059, mae: 0.056925, mean_q: -0.255675
 15656/100000: episode: 176, duration: 0.099s, episode steps: 20, steps per second: 203, episode reward: 5.336, mean reward: 0.267 [0.155, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.889, 10.264], loss: 0.003573, mae: 0.062116, mean_q: -0.107780
 15676/100000: episode: 177, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 4.843, mean reward: 0.242 [0.168, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.304], loss: 0.003462, mae: 0.061378, mean_q: -0.146278
 15702/100000: episode: 178, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 5.431, mean reward: 0.209 [0.113, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.123, 10.273], loss: 0.002839, mae: 0.055332, mean_q: -0.233407
 15725/100000: episode: 179, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 4.924, mean reward: 0.214 [0.140, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.252, 10.301], loss: 0.003250, mae: 0.059356, mean_q: -0.200565
 15754/100000: episode: 180, duration: 0.149s, episode steps: 29, steps per second: 194, episode reward: 6.355, mean reward: 0.219 [0.122, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.588, 10.230], loss: 0.002922, mae: 0.054640, mean_q: -0.196040
 15767/100000: episode: 181, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 5.094, mean reward: 0.392 [0.271, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.336, 10.519], loss: 0.003151, mae: 0.055879, mean_q: -0.220088
 15780/100000: episode: 182, duration: 0.074s, episode steps: 13, steps per second: 177, episode reward: 3.064, mean reward: 0.236 [0.148, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.316], loss: 0.004927, mae: 0.068809, mean_q: -0.107718
 15814/100000: episode: 183, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 10.778, mean reward: 0.317 [0.181, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.035, 10.309], loss: 0.003802, mae: 0.065650, mean_q: -0.160384
 15843/100000: episode: 184, duration: 0.168s, episode steps: 29, steps per second: 172, episode reward: 5.916, mean reward: 0.204 [0.117, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.235, 10.206], loss: 0.004853, mae: 0.069582, mean_q: -0.149561
 15863/100000: episode: 185, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 4.507, mean reward: 0.225 [0.100, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.357, 10.299], loss: 0.004570, mae: 0.075466, mean_q: -0.171545
 15876/100000: episode: 186, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 3.285, mean reward: 0.253 [0.145, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.339], loss: 0.003163, mae: 0.062945, mean_q: -0.136427
 15896/100000: episode: 187, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 5.253, mean reward: 0.263 [0.178, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.973, 10.311], loss: 0.003058, mae: 0.058980, mean_q: -0.157563
 15933/100000: episode: 188, duration: 0.184s, episode steps: 37, steps per second: 201, episode reward: 12.542, mean reward: 0.339 [0.222, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.352, 10.453], loss: 0.002989, mae: 0.056346, mean_q: -0.151117
 15970/100000: episode: 189, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 11.206, mean reward: 0.303 [0.189, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.490, 10.342], loss: 0.003290, mae: 0.059658, mean_q: -0.126689
 16007/100000: episode: 190, duration: 0.203s, episode steps: 37, steps per second: 182, episode reward: 10.088, mean reward: 0.273 [0.156, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.035, 10.320], loss: 0.008048, mae: 0.077396, mean_q: -0.136689
 16020/100000: episode: 191, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 4.116, mean reward: 0.317 [0.241, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.461], loss: 0.007667, mae: 0.088827, mean_q: -0.072942
 16046/100000: episode: 192, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 7.806, mean reward: 0.300 [0.165, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.171, 10.468], loss: 0.004833, mae: 0.066985, mean_q: -0.153108
 16080/100000: episode: 193, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 9.205, mean reward: 0.271 [0.073, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.523, 10.168], loss: 0.004661, mae: 0.070784, mean_q: -0.142170
 16093/100000: episode: 194, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 4.328, mean reward: 0.333 [0.270, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.421], loss: 0.002562, mae: 0.056398, mean_q: -0.099309
 16127/100000: episode: 195, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 8.008, mean reward: 0.236 [0.117, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.659, 10.460], loss: 0.002873, mae: 0.056382, mean_q: -0.127977
 16164/100000: episode: 196, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 12.662, mean reward: 0.342 [0.233, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-1.394, 10.514], loss: 0.003145, mae: 0.058455, mean_q: -0.074701
 16174/100000: episode: 197, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 1.478, mean reward: 0.148 [0.091, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.183], loss: 0.003001, mae: 0.057484, mean_q: -0.056630
 16194/100000: episode: 198, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 4.324, mean reward: 0.216 [0.041, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.170, 10.100], loss: 0.002967, mae: 0.057307, mean_q: -0.096491
 16207/100000: episode: 199, duration: 0.069s, episode steps: 13, steps per second: 190, episode reward: 4.182, mean reward: 0.322 [0.196, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.401], loss: 0.003302, mae: 0.060182, mean_q: -0.176728
 16217/100000: episode: 200, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 3.341, mean reward: 0.334 [0.289, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.426], loss: 0.003487, mae: 0.061400, mean_q: -0.080812
 16251/100000: episode: 201, duration: 0.183s, episode steps: 34, steps per second: 185, episode reward: 7.766, mean reward: 0.228 [0.078, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.280, 10.318], loss: 0.003267, mae: 0.059110, mean_q: -0.041778
 16264/100000: episode: 202, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 3.609, mean reward: 0.278 [0.214, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.374], loss: 0.003009, mae: 0.056800, mean_q: -0.084721
 16277/100000: episode: 203, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 3.433, mean reward: 0.264 [0.203, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.596, 10.337], loss: 0.002557, mae: 0.053492, mean_q: -0.059115
 16290/100000: episode: 204, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 5.347, mean reward: 0.411 [0.305, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.468, 10.542], loss: 0.002958, mae: 0.055539, mean_q: -0.086310
 16300/100000: episode: 205, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 3.530, mean reward: 0.353 [0.251, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.350], loss: 0.002885, mae: 0.057122, mean_q: -0.071064
 16310/100000: episode: 206, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 3.261, mean reward: 0.326 [0.235, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.428, 10.537], loss: 0.002963, mae: 0.057956, mean_q: -0.077525
 16347/100000: episode: 207, duration: 0.209s, episode steps: 37, steps per second: 177, episode reward: 5.715, mean reward: 0.154 [0.024, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.214, 10.100], loss: 0.002715, mae: 0.054299, mean_q: -0.088975
 16370/100000: episode: 208, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 5.080, mean reward: 0.221 [0.122, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.337], loss: 0.003085, mae: 0.056529, mean_q: -0.136524
 16390/100000: episode: 209, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 5.142, mean reward: 0.257 [0.185, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.041, 10.366], loss: 0.003749, mae: 0.063527, mean_q: -0.080024
 16424/100000: episode: 210, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 11.754, mean reward: 0.346 [0.166, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.446, 10.460], loss: 0.002811, mae: 0.055820, mean_q: -0.040160
 16437/100000: episode: 211, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 3.219, mean reward: 0.248 [0.148, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.106, 10.398], loss: 0.002882, mae: 0.056155, mean_q: -0.062973
 16450/100000: episode: 212, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 4.070, mean reward: 0.313 [0.244, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.371, 10.432], loss: 0.003591, mae: 0.061249, mean_q: -0.095096
 16482/100000: episode: 213, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: 8.564, mean reward: 0.268 [0.168, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.160, 10.333], loss: 0.003533, mae: 0.060926, mean_q: -0.111833
 16492/100000: episode: 214, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 2.889, mean reward: 0.289 [0.223, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.437], loss: 0.002466, mae: 0.051088, mean_q: -0.189460
 16529/100000: episode: 215, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 12.003, mean reward: 0.324 [0.227, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.325, 10.304], loss: 0.002867, mae: 0.056591, mean_q: -0.054149
 16566/100000: episode: 216, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 12.643, mean reward: 0.342 [0.213, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.757, 10.348], loss: 0.002933, mae: 0.057851, mean_q: -0.075266
 16576/100000: episode: 217, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 2.018, mean reward: 0.202 [0.130, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.265], loss: 0.003937, mae: 0.067110, mean_q: 0.106426
 16599/100000: episode: 218, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 3.369, mean reward: 0.146 [0.066, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.907, 10.152], loss: 0.003337, mae: 0.059682, mean_q: -0.007424
 16633/100000: episode: 219, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 11.252, mean reward: 0.331 [0.162, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.398, 10.404], loss: 0.003434, mae: 0.061618, mean_q: -0.042450
 16667/100000: episode: 220, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 10.890, mean reward: 0.320 [0.190, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.742, 10.394], loss: 0.002996, mae: 0.057874, mean_q: -0.040368
 16693/100000: episode: 221, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 4.750, mean reward: 0.183 [0.097, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.168], loss: 0.002800, mae: 0.055922, mean_q: -0.035661
 16727/100000: episode: 222, duration: 0.192s, episode steps: 34, steps per second: 177, episode reward: 9.860, mean reward: 0.290 [0.137, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.035, 10.292], loss: 0.003221, mae: 0.062839, mean_q: 0.034077
 16759/100000: episode: 223, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 8.359, mean reward: 0.261 [0.063, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.187, 10.100], loss: 0.003283, mae: 0.062358, mean_q: -0.025166
 16782/100000: episode: 224, duration: 0.142s, episode steps: 23, steps per second: 162, episode reward: 4.620, mean reward: 0.201 [0.107, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-1.253, 10.298], loss: 0.003142, mae: 0.058679, mean_q: -0.005325
 16795/100000: episode: 225, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 3.766, mean reward: 0.290 [0.198, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.664, 10.332], loss: 0.002814, mae: 0.054105, mean_q: -0.068671
 16808/100000: episode: 226, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 3.029, mean reward: 0.233 [0.142, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.065, 10.368], loss: 0.003261, mae: 0.061103, mean_q: 0.011621
 16821/100000: episode: 227, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 2.943, mean reward: 0.226 [0.108, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.869, 10.340], loss: 0.003155, mae: 0.058614, mean_q: -0.000084
 16855/100000: episode: 228, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 7.371, mean reward: 0.217 [0.060, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.279, 10.103], loss: 0.003136, mae: 0.060240, mean_q: 0.012908
 16884/100000: episode: 229, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 6.653, mean reward: 0.229 [0.021, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.062, 10.198], loss: 0.003043, mae: 0.057932, mean_q: 0.026080
 16894/100000: episode: 230, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 2.365, mean reward: 0.236 [0.191, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.343], loss: 0.003497, mae: 0.062448, mean_q: -0.018333
 16907/100000: episode: 231, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 3.532, mean reward: 0.272 [0.140, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.302, 10.285], loss: 0.003294, mae: 0.062169, mean_q: 0.077810
 16933/100000: episode: 232, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 5.824, mean reward: 0.224 [0.060, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.988, 10.210], loss: 0.003287, mae: 0.061290, mean_q: 0.034824
 16965/100000: episode: 233, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 9.743, mean reward: 0.304 [0.157, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.035, 10.514], loss: 0.003389, mae: 0.062704, mean_q: 0.030057
 16978/100000: episode: 234, duration: 0.072s, episode steps: 13, steps per second: 179, episode reward: 5.501, mean reward: 0.423 [0.304, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.559], loss: 0.002942, mae: 0.058952, mean_q: 0.056070
 17004/100000: episode: 235, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 8.187, mean reward: 0.315 [0.093, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.593, 10.185], loss: 0.002638, mae: 0.055268, mean_q: 0.027851
 17038/100000: episode: 236, duration: 0.204s, episode steps: 34, steps per second: 167, episode reward: 9.045, mean reward: 0.266 [0.155, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.304, 10.333], loss: 0.002893, mae: 0.056850, mean_q: 0.035671
 17061/100000: episode: 237, duration: 0.127s, episode steps: 23, steps per second: 180, episode reward: 5.764, mean reward: 0.251 [0.132, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.141, 10.488], loss: 0.003255, mae: 0.061079, mean_q: 0.042955
 17093/100000: episode: 238, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 6.516, mean reward: 0.204 [0.026, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.035, 10.152], loss: 0.003045, mae: 0.059898, mean_q: 0.046078
 17103/100000: episode: 239, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 3.497, mean reward: 0.350 [0.260, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.354, 10.497], loss: 0.002970, mae: 0.056647, mean_q: 0.033101
[Info] 200-TH LEVEL FOUND: 0.7564354538917542, Considering 10/90 traces
 17140/100000: episode: 240, duration: 4.089s, episode steps: 37, steps per second: 9, episode reward: 9.918, mean reward: 0.268 [0.144, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.435, 10.434], loss: 0.003256, mae: 0.060941, mean_q: 0.043020
 17163/100000: episode: 241, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 9.142, mean reward: 0.397 [0.247, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.185, 10.484], loss: 0.003118, mae: 0.059569, mean_q: 0.101251
 17181/100000: episode: 242, duration: 0.104s, episode steps: 18, steps per second: 174, episode reward: 9.052, mean reward: 0.503 [0.378, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-1.577, 10.519], loss: 0.002666, mae: 0.055267, mean_q: 0.042571
 17207/100000: episode: 243, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 10.545, mean reward: 0.406 [0.228, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.522, 10.419], loss: 0.002963, mae: 0.057793, mean_q: 0.063495
 17225/100000: episode: 244, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 8.028, mean reward: 0.446 [0.368, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.506], loss: 0.002922, mae: 0.057825, mean_q: 0.098439
 17243/100000: episode: 245, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 6.040, mean reward: 0.336 [0.274, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.355], loss: 0.003466, mae: 0.062692, mean_q: 0.115043
 17274/100000: episode: 246, duration: 0.157s, episode steps: 31, steps per second: 197, episode reward: 12.765, mean reward: 0.412 [0.342, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.296, 10.380], loss: 0.003644, mae: 0.063688, mean_q: 0.079920
 17297/100000: episode: 247, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 7.893, mean reward: 0.343 [0.280, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.582, 10.371], loss: 0.002861, mae: 0.057949, mean_q: 0.139260
 17322/100000: episode: 248, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 7.775, mean reward: 0.311 [0.195, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.834, 10.464], loss: 0.003626, mae: 0.063919, mean_q: 0.132581
 17345/100000: episode: 249, duration: 0.130s, episode steps: 23, steps per second: 176, episode reward: 9.427, mean reward: 0.410 [0.224, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.389, 10.451], loss: 0.003226, mae: 0.060139, mean_q: 0.088940
 17363/100000: episode: 250, duration: 0.113s, episode steps: 18, steps per second: 159, episode reward: 7.710, mean reward: 0.428 [0.347, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.492], loss: 0.002601, mae: 0.052530, mean_q: 0.010976
 17386/100000: episode: 251, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 6.981, mean reward: 0.304 [0.168, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.257], loss: 0.003371, mae: 0.060802, mean_q: 0.113287
 17410/100000: episode: 252, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 9.342, mean reward: 0.389 [0.251, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.428], loss: 0.002631, mae: 0.054295, mean_q: 0.132549
 17431/100000: episode: 253, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 7.211, mean reward: 0.343 [0.259, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.429], loss: 0.003083, mae: 0.058866, mean_q: 0.057849
 17456/100000: episode: 254, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 8.270, mean reward: 0.331 [0.141, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.531, 10.262], loss: 0.003392, mae: 0.063271, mean_q: 0.135154
 17479/100000: episode: 255, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 8.093, mean reward: 0.352 [0.248, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.082, 10.454], loss: 0.003411, mae: 0.061069, mean_q: 0.126787
 17502/100000: episode: 256, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 8.672, mean reward: 0.377 [0.287, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.412], loss: 0.003260, mae: 0.061128, mean_q: 0.126131
 17533/100000: episode: 257, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 11.475, mean reward: 0.370 [0.235, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.127, 10.380], loss: 0.003044, mae: 0.058920, mean_q: 0.137910
 17564/100000: episode: 258, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 11.725, mean reward: 0.378 [0.178, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.823, 10.298], loss: 0.003263, mae: 0.060762, mean_q: 0.144145
 17595/100000: episode: 259, duration: 0.180s, episode steps: 31, steps per second: 172, episode reward: 11.137, mean reward: 0.359 [0.213, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.337], loss: 0.003217, mae: 0.059639, mean_q: 0.156996
 17622/100000: episode: 260, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 8.724, mean reward: 0.323 [0.223, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.304, 10.463], loss: 0.003397, mae: 0.064523, mean_q: 0.155756
 17648/100000: episode: 261, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 7.064, mean reward: 0.272 [0.161, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.798, 10.367], loss: 0.003155, mae: 0.061756, mean_q: 0.154572
 17666/100000: episode: 262, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 6.606, mean reward: 0.367 [0.314, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.278, 10.416], loss: 0.002926, mae: 0.056471, mean_q: 0.184037
 17691/100000: episode: 263, duration: 0.147s, episode steps: 25, steps per second: 171, episode reward: 7.454, mean reward: 0.298 [0.147, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.972, 10.378], loss: 0.003569, mae: 0.063383, mean_q: 0.196486
 17715/100000: episode: 264, duration: 0.125s, episode steps: 24, steps per second: 191, episode reward: 9.606, mean reward: 0.400 [0.315, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.093, 10.466], loss: 0.003251, mae: 0.063646, mean_q: 0.157186
 17741/100000: episode: 265, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 10.139, mean reward: 0.390 [0.300, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.256, 10.493], loss: 0.003347, mae: 0.063482, mean_q: 0.189509
 17764/100000: episode: 266, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 8.512, mean reward: 0.370 [0.258, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.057, 10.424], loss: 0.002972, mae: 0.058527, mean_q: 0.190076
 17789/100000: episode: 267, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 10.864, mean reward: 0.435 [0.319, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.377, 10.521], loss: 0.002912, mae: 0.058270, mean_q: 0.200267
 17814/100000: episode: 268, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 6.890, mean reward: 0.276 [0.150, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.231, 10.287], loss: 0.003323, mae: 0.059992, mean_q: 0.142034
 17837/100000: episode: 269, duration: 0.128s, episode steps: 23, steps per second: 179, episode reward: 6.781, mean reward: 0.295 [0.211, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.261], loss: 0.004009, mae: 0.067847, mean_q: 0.226409
 17858/100000: episode: 270, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 8.131, mean reward: 0.387 [0.287, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.360, 10.545], loss: 0.002942, mae: 0.057777, mean_q: 0.195678
 17885/100000: episode: 271, duration: 0.140s, episode steps: 27, steps per second: 192, episode reward: 9.060, mean reward: 0.336 [0.228, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.183, 10.388], loss: 0.002959, mae: 0.057961, mean_q: 0.215473
 17912/100000: episode: 272, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 9.970, mean reward: 0.369 [0.289, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-1.365, 10.449], loss: 0.003378, mae: 0.061554, mean_q: 0.192869
 17930/100000: episode: 273, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 6.517, mean reward: 0.362 [0.292, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.224, 10.402], loss: 0.002973, mae: 0.058762, mean_q: 0.207517
 17953/100000: episode: 274, duration: 0.130s, episode steps: 23, steps per second: 178, episode reward: 9.543, mean reward: 0.415 [0.261, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.696, 10.559], loss: 0.005959, mae: 0.074653, mean_q: 0.234554
 17971/100000: episode: 275, duration: 0.115s, episode steps: 18, steps per second: 157, episode reward: 7.879, mean reward: 0.438 [0.292, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.567], loss: 0.004134, mae: 0.069388, mean_q: 0.254388
 18002/100000: episode: 276, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 12.619, mean reward: 0.407 [0.175, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.035, 10.350], loss: 0.004165, mae: 0.071678, mean_q: 0.276781
 18028/100000: episode: 277, duration: 0.143s, episode steps: 26, steps per second: 181, episode reward: 11.788, mean reward: 0.453 [0.272, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.604], loss: 0.003916, mae: 0.066873, mean_q: 0.258631
 18046/100000: episode: 278, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 5.055, mean reward: 0.281 [0.070, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.130, 10.198], loss: 0.003341, mae: 0.062458, mean_q: 0.238947
 18077/100000: episode: 279, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 13.182, mean reward: 0.425 [0.347, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.178, 10.461], loss: 0.003263, mae: 0.062633, mean_q: 0.262254
 18103/100000: episode: 280, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 11.031, mean reward: 0.424 [0.304, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.519], loss: 0.003226, mae: 0.061053, mean_q: 0.260084
 18121/100000: episode: 281, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 7.842, mean reward: 0.436 [0.355, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-1.203, 10.483], loss: 0.003485, mae: 0.065513, mean_q: 0.228229
 18148/100000: episode: 282, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 12.348, mean reward: 0.457 [0.303, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.332, 10.550], loss: 0.003391, mae: 0.064235, mean_q: 0.250877
 18166/100000: episode: 283, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 7.232, mean reward: 0.402 [0.255, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.174, 10.394], loss: 0.002464, mae: 0.053983, mean_q: 0.242532
 18192/100000: episode: 284, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 8.324, mean reward: 0.320 [0.229, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.019, 10.408], loss: 0.003151, mae: 0.060436, mean_q: 0.303996
 18218/100000: episode: 285, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 11.947, mean reward: 0.460 [0.365, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.229, 10.375], loss: 0.003520, mae: 0.063650, mean_q: 0.246381
 18243/100000: episode: 286, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 6.054, mean reward: 0.242 [0.099, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.059, 10.290], loss: 0.003224, mae: 0.061750, mean_q: 0.276501
 18267/100000: episode: 287, duration: 0.139s, episode steps: 24, steps per second: 172, episode reward: 8.537, mean reward: 0.356 [0.184, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.314, 10.349], loss: 0.003486, mae: 0.064477, mean_q: 0.278670
 18285/100000: episode: 288, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 6.694, mean reward: 0.372 [0.302, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.433], loss: 0.003202, mae: 0.060833, mean_q: 0.297832
 18308/100000: episode: 289, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 9.615, mean reward: 0.418 [0.295, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.823, 10.494], loss: 0.004667, mae: 0.069337, mean_q: 0.244626
 18332/100000: episode: 290, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 4.756, mean reward: 0.198 [0.054, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.445, 10.194], loss: 0.006006, mae: 0.080313, mean_q: 0.336245
 18356/100000: episode: 291, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 9.076, mean reward: 0.378 [0.256, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.703, 10.503], loss: 0.003776, mae: 0.067525, mean_q: 0.292916
 18379/100000: episode: 292, duration: 0.139s, episode steps: 23, steps per second: 166, episode reward: 8.414, mean reward: 0.366 [0.256, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.068, 10.513], loss: 0.003251, mae: 0.061639, mean_q: 0.282347
 18410/100000: episode: 293, duration: 0.186s, episode steps: 31, steps per second: 166, episode reward: 11.299, mean reward: 0.364 [0.268, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.550, 10.469], loss: 0.003744, mae: 0.066985, mean_q: 0.309252
 18434/100000: episode: 294, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 9.247, mean reward: 0.385 [0.307, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.172, 10.437], loss: 0.002970, mae: 0.059149, mean_q: 0.336849
 18457/100000: episode: 295, duration: 0.146s, episode steps: 23, steps per second: 158, episode reward: 7.812, mean reward: 0.340 [0.270, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.116, 10.392], loss: 0.003393, mae: 0.063214, mean_q: 0.343260
 18475/100000: episode: 296, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 7.607, mean reward: 0.423 [0.296, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.521], loss: 0.004002, mae: 0.065308, mean_q: 0.374165
 18506/100000: episode: 297, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 11.828, mean reward: 0.382 [0.257, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.668, 10.394], loss: 0.004649, mae: 0.069269, mean_q: 0.318509
 18529/100000: episode: 298, duration: 0.121s, episode steps: 23, steps per second: 189, episode reward: 8.944, mean reward: 0.389 [0.321, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.359, 10.407], loss: 0.003024, mae: 0.061661, mean_q: 0.336236
 18550/100000: episode: 299, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 6.465, mean reward: 0.308 [0.129, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.344, 10.266], loss: 0.003169, mae: 0.062407, mean_q: 0.330321
 18573/100000: episode: 300, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 11.296, mean reward: 0.491 [0.286, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.542, 10.479], loss: 0.003094, mae: 0.060195, mean_q: 0.334729
 18594/100000: episode: 301, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 8.252, mean reward: 0.393 [0.323, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.873, 10.491], loss: 0.003204, mae: 0.060101, mean_q: 0.331078
 18617/100000: episode: 302, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 8.180, mean reward: 0.356 [0.266, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.483], loss: 0.002969, mae: 0.059220, mean_q: 0.400471
 18641/100000: episode: 303, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 9.170, mean reward: 0.382 [0.263, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.608, 10.537], loss: 0.003150, mae: 0.060115, mean_q: 0.365811
 18662/100000: episode: 304, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 7.609, mean reward: 0.362 [0.227, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.622, 10.422], loss: 0.003231, mae: 0.061149, mean_q: 0.371478
 18685/100000: episode: 305, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 9.131, mean reward: 0.397 [0.284, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.212, 10.456], loss: 0.002926, mae: 0.058585, mean_q: 0.434651
 18703/100000: episode: 306, duration: 0.109s, episode steps: 18, steps per second: 166, episode reward: 8.825, mean reward: 0.490 [0.325, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.423, 10.608], loss: 0.002741, mae: 0.057722, mean_q: 0.382127
 18724/100000: episode: 307, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 6.446, mean reward: 0.307 [0.219, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.441, 10.365], loss: 0.002932, mae: 0.059713, mean_q: 0.394706
 18751/100000: episode: 308, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 10.691, mean reward: 0.396 [0.286, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.741, 10.515], loss: 0.003202, mae: 0.061736, mean_q: 0.403482
 18774/100000: episode: 309, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 7.973, mean reward: 0.347 [0.200, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.262], loss: 0.002920, mae: 0.058704, mean_q: 0.441234
 18800/100000: episode: 310, duration: 0.130s, episode steps: 26, steps per second: 199, episode reward: 10.138, mean reward: 0.390 [0.276, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.727, 10.647], loss: 0.003155, mae: 0.061069, mean_q: 0.423562
 18821/100000: episode: 311, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 7.628, mean reward: 0.363 [0.242, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.043, 10.412], loss: 0.003512, mae: 0.065342, mean_q: 0.446875
 18846/100000: episode: 312, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 7.789, mean reward: 0.312 [0.223, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.365], loss: 0.003063, mae: 0.061289, mean_q: 0.419044
 18869/100000: episode: 313, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 8.363, mean reward: 0.364 [0.298, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.432], loss: 0.003349, mae: 0.063590, mean_q: 0.441136
 18887/100000: episode: 314, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 8.958, mean reward: 0.498 [0.370, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.620, 10.422], loss: 0.006257, mae: 0.076705, mean_q: 0.429859
 18905/100000: episode: 315, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 8.546, mean reward: 0.475 [0.368, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-1.355, 10.512], loss: 0.005555, mae: 0.068713, mean_q: 0.455005
 18932/100000: episode: 316, duration: 0.143s, episode steps: 27, steps per second: 188, episode reward: 11.554, mean reward: 0.428 [0.251, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.459], loss: 0.003278, mae: 0.063380, mean_q: 0.393574
 18950/100000: episode: 317, duration: 0.089s, episode steps: 18, steps per second: 201, episode reward: 6.948, mean reward: 0.386 [0.263, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.444], loss: 0.004989, mae: 0.069696, mean_q: 0.450232
 18974/100000: episode: 318, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 7.897, mean reward: 0.329 [0.180, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.381, 10.392], loss: 0.003304, mae: 0.062533, mean_q: 0.439256
 18995/100000: episode: 319, duration: 0.126s, episode steps: 21, steps per second: 166, episode reward: 6.121, mean reward: 0.291 [0.114, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.752, 10.229], loss: 0.003110, mae: 0.061107, mean_q: 0.442762
 19018/100000: episode: 320, duration: 0.123s, episode steps: 23, steps per second: 188, episode reward: 8.456, mean reward: 0.368 [0.254, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.376], loss: 0.003498, mae: 0.062819, mean_q: 0.508566
 19036/100000: episode: 321, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 8.410, mean reward: 0.467 [0.394, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.597], loss: 0.003969, mae: 0.068218, mean_q: 0.481488
 19063/100000: episode: 322, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 9.141, mean reward: 0.339 [0.189, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.316], loss: 0.004882, mae: 0.069496, mean_q: 0.442625
 19081/100000: episode: 323, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 6.585, mean reward: 0.366 [0.276, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.254, 10.385], loss: 0.003453, mae: 0.066104, mean_q: 0.490648
 19104/100000: episode: 324, duration: 0.123s, episode steps: 23, steps per second: 188, episode reward: 7.626, mean reward: 0.332 [0.284, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.652, 10.385], loss: 0.003667, mae: 0.067680, mean_q: 0.494158
 19135/100000: episode: 325, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 13.214, mean reward: 0.426 [0.319, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.066, 10.520], loss: 0.003531, mae: 0.064657, mean_q: 0.453152
 19166/100000: episode: 326, duration: 0.165s, episode steps: 31, steps per second: 187, episode reward: 15.042, mean reward: 0.485 [0.363, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.303, 10.494], loss: 0.004330, mae: 0.067394, mean_q: 0.470605
 19184/100000: episode: 327, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 7.442, mean reward: 0.413 [0.339, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.336, 10.401], loss: 0.003229, mae: 0.064222, mean_q: 0.512640
 19210/100000: episode: 328, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 8.735, mean reward: 0.336 [0.265, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.854, 10.476], loss: 0.003587, mae: 0.065121, mean_q: 0.497619
 19237/100000: episode: 329, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 11.962, mean reward: 0.443 [0.359, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.578], loss: 0.003563, mae: 0.065533, mean_q: 0.530449
[Info] 300-TH LEVEL FOUND: 0.9626166820526123, Considering 10/90 traces
 19255/100000: episode: 330, duration: 4.007s, episode steps: 18, steps per second: 4, episode reward: 6.274, mean reward: 0.349 [0.231, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.336], loss: 0.003328, mae: 0.061661, mean_q: 0.525789
 19270/100000: episode: 331, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 7.770, mean reward: 0.518 [0.468, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.815, 10.498], loss: 0.003581, mae: 0.065547, mean_q: 0.515188
 19289/100000: episode: 332, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 9.421, mean reward: 0.496 [0.432, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.280, 10.512], loss: 0.003229, mae: 0.063428, mean_q: 0.510042
 19299/100000: episode: 333, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 4.837, mean reward: 0.484 [0.436, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.539], loss: 0.004101, mae: 0.073179, mean_q: 0.567903
 19318/100000: episode: 334, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 8.737, mean reward: 0.460 [0.383, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.525], loss: 0.003511, mae: 0.062934, mean_q: 0.519897
 19331/100000: episode: 335, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 6.371, mean reward: 0.490 [0.413, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.523], loss: 0.003223, mae: 0.063259, mean_q: 0.499438
 19351/100000: episode: 336, duration: 0.120s, episode steps: 20, steps per second: 166, episode reward: 9.108, mean reward: 0.455 [0.328, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.523], loss: 0.003063, mae: 0.061186, mean_q: 0.561590
 19361/100000: episode: 337, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 4.723, mean reward: 0.472 [0.432, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.552], loss: 0.004059, mae: 0.070014, mean_q: 0.559672
 19374/100000: episode: 338, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 5.774, mean reward: 0.444 [0.407, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.556], loss: 0.003640, mae: 0.066912, mean_q: 0.545394
 19386/100000: episode: 339, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 5.259, mean reward: 0.438 [0.316, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.461], loss: 0.002687, mae: 0.057307, mean_q: 0.501434
 19398/100000: episode: 340, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 5.055, mean reward: 0.421 [0.319, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.447, 10.416], loss: 0.003168, mae: 0.062669, mean_q: 0.514183
 19408/100000: episode: 341, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 4.631, mean reward: 0.463 [0.384, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.517], loss: 0.002932, mae: 0.060267, mean_q: 0.529374
 19413/100000: episode: 342, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 2.285, mean reward: 0.457 [0.432, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.464], loss: 0.004053, mae: 0.073601, mean_q: 0.511288
 19423/100000: episode: 343, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 4.661, mean reward: 0.466 [0.427, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.562], loss: 0.003045, mae: 0.060831, mean_q: 0.533543
 19435/100000: episode: 344, duration: 0.061s, episode steps: 12, steps per second: 195, episode reward: 5.226, mean reward: 0.436 [0.330, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.571, 10.491], loss: 0.003243, mae: 0.063968, mean_q: 0.541020
 19440/100000: episode: 345, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 2.466, mean reward: 0.493 [0.417, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.670], loss: 0.002462, mae: 0.053324, mean_q: 0.564987
 19453/100000: episode: 346, duration: 0.074s, episode steps: 13, steps per second: 177, episode reward: 7.381, mean reward: 0.568 [0.472, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.877, 10.591], loss: 0.003742, mae: 0.067911, mean_q: 0.577574
 19471/100000: episode: 347, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 9.077, mean reward: 0.504 [0.408, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.725, 10.734], loss: 0.003428, mae: 0.062990, mean_q: 0.552386
 19481/100000: episode: 348, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 4.540, mean reward: 0.454 [0.404, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.432, 10.505], loss: 0.003103, mae: 0.061052, mean_q: 0.553384
 19486/100000: episode: 349, duration: 0.039s, episode steps: 5, steps per second: 127, episode reward: 2.355, mean reward: 0.471 [0.399, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.587], loss: 0.004189, mae: 0.070773, mean_q: 0.580919
 19498/100000: episode: 350, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 4.823, mean reward: 0.402 [0.288, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.822, 10.419], loss: 0.003574, mae: 0.064635, mean_q: 0.590486
 19510/100000: episode: 351, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 6.550, mean reward: 0.546 [0.477, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.669], loss: 0.004099, mae: 0.068328, mean_q: 0.594915
 19515/100000: episode: 352, duration: 0.032s, episode steps: 5, steps per second: 154, episode reward: 2.552, mean reward: 0.510 [0.444, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.411], loss: 0.003132, mae: 0.061077, mean_q: 0.545333
 19528/100000: episode: 353, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 5.411, mean reward: 0.416 [0.361, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.529], loss: 0.003122, mae: 0.062444, mean_q: 0.603348
 19541/100000: episode: 354, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 5.481, mean reward: 0.422 [0.345, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.560, 10.507], loss: 0.003254, mae: 0.062214, mean_q: 0.579628
 19554/100000: episode: 355, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 5.908, mean reward: 0.454 [0.352, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.486], loss: 0.002691, mae: 0.058113, mean_q: 0.588275
 19559/100000: episode: 356, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 2.574, mean reward: 0.515 [0.506, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.535], loss: 0.002350, mae: 0.054302, mean_q: 0.587053
 19571/100000: episode: 357, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 5.637, mean reward: 0.470 [0.323, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.082, 10.473], loss: 0.002907, mae: 0.060116, mean_q: 0.602402
 19583/100000: episode: 358, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 5.818, mean reward: 0.485 [0.422, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.444, 10.561], loss: 0.003157, mae: 0.062734, mean_q: 0.573013
 19596/100000: episode: 359, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 5.889, mean reward: 0.453 [0.412, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.248, 10.556], loss: 0.003929, mae: 0.068142, mean_q: 0.596260
 19609/100000: episode: 360, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 7.000, mean reward: 0.538 [0.360, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.487, 10.518], loss: 0.003175, mae: 0.063416, mean_q: 0.607863
 19627/100000: episode: 361, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 6.111, mean reward: 0.340 [0.245, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.436], loss: 0.003008, mae: 0.060578, mean_q: 0.609320
 19646/100000: episode: 362, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 9.337, mean reward: 0.491 [0.415, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.666, 10.535], loss: 0.003321, mae: 0.064874, mean_q: 0.630231
 19651/100000: episode: 363, duration: 0.028s, episode steps: 5, steps per second: 175, episode reward: 2.514, mean reward: 0.503 [0.477, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.587], loss: 0.003241, mae: 0.061501, mean_q: 0.632223
 19664/100000: episode: 364, duration: 0.079s, episode steps: 13, steps per second: 166, episode reward: 6.801, mean reward: 0.523 [0.458, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.231, 10.585], loss: 0.003634, mae: 0.063991, mean_q: 0.611940
 19682/100000: episode: 365, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 6.542, mean reward: 0.363 [0.280, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.430], loss: 0.003612, mae: 0.067046, mean_q: 0.605207
 19700/100000: episode: 366, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 6.673, mean reward: 0.371 [0.288, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.444, 10.451], loss: 0.003491, mae: 0.064113, mean_q: 0.621393
 19710/100000: episode: 367, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 5.786, mean reward: 0.579 [0.507, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.327, 10.681], loss: 0.002915, mae: 0.059005, mean_q: 0.613155
 19722/100000: episode: 368, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 5.630, mean reward: 0.469 [0.347, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.527, 10.514], loss: 0.003130, mae: 0.061835, mean_q: 0.628729
 19735/100000: episode: 369, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 7.407, mean reward: 0.570 [0.518, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-1.296, 10.616], loss: 0.003670, mae: 0.066272, mean_q: 0.642770
 19748/100000: episode: 370, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 6.174, mean reward: 0.475 [0.394, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-1.579, 10.530], loss: 0.002822, mae: 0.057833, mean_q: 0.648003
 19758/100000: episode: 371, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 5.188, mean reward: 0.519 [0.490, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.580], loss: 0.003204, mae: 0.061406, mean_q: 0.639428
 19771/100000: episode: 372, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 7.050, mean reward: 0.542 [0.494, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.361, 10.594], loss: 0.003238, mae: 0.062789, mean_q: 0.647055
 19776/100000: episode: 373, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 2.500, mean reward: 0.500 [0.465, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.464], loss: 0.003855, mae: 0.071216, mean_q: 0.631779
 19789/100000: episode: 374, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 6.568, mean reward: 0.505 [0.425, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.433, 10.490], loss: 0.003446, mae: 0.062275, mean_q: 0.648836
 19802/100000: episode: 375, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 6.478, mean reward: 0.498 [0.408, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.669], loss: 0.003089, mae: 0.060734, mean_q: 0.624834
 19815/100000: episode: 376, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 7.927, mean reward: 0.610 [0.559, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.087, 10.623], loss: 0.003444, mae: 0.064620, mean_q: 0.632302
 19827/100000: episode: 377, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 4.997, mean reward: 0.416 [0.343, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-1.086, 10.486], loss: 0.002270, mae: 0.052835, mean_q: 0.656132
 19846/100000: episode: 378, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 7.640, mean reward: 0.402 [0.257, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.412], loss: 0.003340, mae: 0.063491, mean_q: 0.664700
 19859/100000: episode: 379, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 5.852, mean reward: 0.450 [0.386, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.554], loss: 0.003140, mae: 0.061961, mean_q: 0.669818
 19864/100000: episode: 380, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 2.302, mean reward: 0.460 [0.435, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.570], loss: 0.003066, mae: 0.060867, mean_q: 0.654869
 19876/100000: episode: 381, duration: 0.078s, episode steps: 12, steps per second: 155, episode reward: 5.617, mean reward: 0.468 [0.366, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.456], loss: 0.004835, mae: 0.077515, mean_q: 0.665511
 19889/100000: episode: 382, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 5.511, mean reward: 0.424 [0.270, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.367, 10.489], loss: 0.003736, mae: 0.065643, mean_q: 0.646293
 19908/100000: episode: 383, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 9.022, mean reward: 0.475 [0.383, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.362, 10.403], loss: 0.003104, mae: 0.062193, mean_q: 0.634570
 19918/100000: episode: 384, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 5.427, mean reward: 0.543 [0.497, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.633], loss: 0.002645, mae: 0.056576, mean_q: 0.664973
 19937/100000: episode: 385, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 10.807, mean reward: 0.569 [0.457, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.586], loss: 0.003767, mae: 0.067311, mean_q: 0.652601
 19955/100000: episode: 386, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 8.211, mean reward: 0.456 [0.367, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.572], loss: 0.004101, mae: 0.069049, mean_q: 0.661979
 19965/100000: episode: 387, duration: 0.064s, episode steps: 10, steps per second: 155, episode reward: 4.982, mean reward: 0.498 [0.455, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.618], loss: 0.003517, mae: 0.063346, mean_q: 0.664726
 19984/100000: episode: 388, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 9.307, mean reward: 0.490 [0.387, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.496], loss: 0.003608, mae: 0.066106, mean_q: 0.662673
 19989/100000: episode: 389, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 2.422, mean reward: 0.484 [0.437, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.463], loss: 0.003850, mae: 0.073010, mean_q: 0.682287
 20009/100000: episode: 390, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 6.988, mean reward: 0.349 [0.282, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.451], loss: 0.003480, mae: 0.063675, mean_q: 0.674115
 20028/100000: episode: 391, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 7.950, mean reward: 0.418 [0.271, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.440], loss: 0.003465, mae: 0.063269, mean_q: 0.673458
 20040/100000: episode: 392, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 5.626, mean reward: 0.469 [0.355, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.072, 10.538], loss: 0.003189, mae: 0.061400, mean_q: 0.673078
 20059/100000: episode: 393, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 9.947, mean reward: 0.524 [0.410, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.721, 10.598], loss: 0.003562, mae: 0.064401, mean_q: 0.656572
 20069/100000: episode: 394, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 4.722, mean reward: 0.472 [0.444, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.567], loss: 0.003302, mae: 0.062101, mean_q: 0.668255
 20089/100000: episode: 395, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 10.114, mean reward: 0.506 [0.419, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.441, 10.646], loss: 0.003675, mae: 0.066132, mean_q: 0.657769
 20102/100000: episode: 396, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 6.373, mean reward: 0.490 [0.434, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.576], loss: 0.004734, mae: 0.075543, mean_q: 0.661352
 20120/100000: episode: 397, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 6.432, mean reward: 0.357 [0.287, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.400], loss: 0.003183, mae: 0.061341, mean_q: 0.682658
 20140/100000: episode: 398, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 8.154, mean reward: 0.408 [0.368, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.357, 10.527], loss: 0.003719, mae: 0.066281, mean_q: 0.663475
 20152/100000: episode: 399, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 6.018, mean reward: 0.502 [0.435, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.345, 10.523], loss: 0.003609, mae: 0.066964, mean_q: 0.676434
 20165/100000: episode: 400, duration: 0.085s, episode steps: 13, steps per second: 154, episode reward: 6.846, mean reward: 0.527 [0.448, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-1.054, 10.569], loss: 0.003814, mae: 0.065488, mean_q: 0.676917
 20184/100000: episode: 401, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 7.381, mean reward: 0.388 [0.335, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.449], loss: 0.003529, mae: 0.064925, mean_q: 0.669299
 20199/100000: episode: 402, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 5.409, mean reward: 0.361 [0.179, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.159, 10.394], loss: 0.003320, mae: 0.062782, mean_q: 0.672086
 20211/100000: episode: 403, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 5.010, mean reward: 0.417 [0.307, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.468], loss: 0.003033, mae: 0.059442, mean_q: 0.657656
 20216/100000: episode: 404, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 2.433, mean reward: 0.487 [0.416, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.589], loss: 0.003728, mae: 0.061570, mean_q: 0.681443
 20236/100000: episode: 405, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 9.484, mean reward: 0.474 [0.363, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.200, 10.664], loss: 0.003389, mae: 0.064989, mean_q: 0.682017
 20246/100000: episode: 406, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 5.060, mean reward: 0.506 [0.430, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.382, 10.429], loss: 0.003793, mae: 0.064351, mean_q: 0.679200
 20251/100000: episode: 407, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 2.561, mean reward: 0.512 [0.465, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.228, 10.617], loss: 0.002809, mae: 0.059567, mean_q: 0.687809
 20263/100000: episode: 408, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 5.146, mean reward: 0.429 [0.317, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.153, 10.528], loss: 0.003235, mae: 0.062767, mean_q: 0.686493
 20276/100000: episode: 409, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 6.968, mean reward: 0.536 [0.493, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.070, 10.590], loss: 0.002996, mae: 0.059558, mean_q: 0.687517
 20288/100000: episode: 410, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 5.713, mean reward: 0.476 [0.418, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-1.286, 10.621], loss: 0.003847, mae: 0.067124, mean_q: 0.682439
 20293/100000: episode: 411, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 2.505, mean reward: 0.501 [0.469, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.586], loss: 0.002606, mae: 0.059157, mean_q: 0.702519
 20308/100000: episode: 412, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 6.167, mean reward: 0.411 [0.265, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.682, 10.438], loss: 0.003355, mae: 0.063353, mean_q: 0.664193
 20320/100000: episode: 413, duration: 0.061s, episode steps: 12, steps per second: 195, episode reward: 5.463, mean reward: 0.455 [0.353, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.048, 10.429], loss: 0.002883, mae: 0.060800, mean_q: 0.691955
 20332/100000: episode: 414, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 5.454, mean reward: 0.454 [0.416, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-1.796, 10.573], loss: 0.003357, mae: 0.063510, mean_q: 0.696728
 20337/100000: episode: 415, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 2.669, mean reward: 0.534 [0.511, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.577], loss: 0.003012, mae: 0.061856, mean_q: 0.688847
 20349/100000: episode: 416, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 5.727, mean reward: 0.477 [0.424, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.578], loss: 0.003542, mae: 0.064984, mean_q: 0.656945
 20364/100000: episode: 417, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 7.058, mean reward: 0.471 [0.421, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.476], loss: 0.003616, mae: 0.064110, mean_q: 0.685198
 20377/100000: episode: 418, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 5.759, mean reward: 0.443 [0.384, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.127, 10.551], loss: 0.003419, mae: 0.063619, mean_q: 0.696529
 20397/100000: episode: 419, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 9.309, mean reward: 0.465 [0.420, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.288, 10.553], loss: 0.003032, mae: 0.060312, mean_q: 0.690710
[Info] 400-TH LEVEL FOUND: 1.0353329181671143, Considering 10/90 traces
 20410/100000: episode: 420, duration: 4.059s, episode steps: 13, steps per second: 3, episode reward: 5.786, mean reward: 0.445 [0.397, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.451], loss: 0.003409, mae: 0.065332, mean_q: 0.684105
 20420/100000: episode: 421, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 6.494, mean reward: 0.649 [0.586, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.204, 10.760], loss: 0.002849, mae: 0.058511, mean_q: 0.670243
 20429/100000: episode: 422, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 4.105, mean reward: 0.456 [0.412, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-1.059, 10.470], loss: 0.003766, mae: 0.067476, mean_q: 0.678601
 20443/100000: episode: 423, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 7.003, mean reward: 0.500 [0.440, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.529], loss: 0.003404, mae: 0.062959, mean_q: 0.710369
 20452/100000: episode: 424, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 5.294, mean reward: 0.588 [0.537, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.635], loss: 0.004118, mae: 0.068032, mean_q: 0.694627
 20463/100000: episode: 425, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 5.183, mean reward: 0.471 [0.403, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.486], loss: 0.003386, mae: 0.062879, mean_q: 0.683047
 20471/100000: episode: 426, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 3.767, mean reward: 0.471 [0.432, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.525], loss: 0.003490, mae: 0.062791, mean_q: 0.698057
 20484/100000: episode: 427, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 6.740, mean reward: 0.518 [0.479, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.540, 10.615], loss: 0.004611, mae: 0.074083, mean_q: 0.705200
 20493/100000: episode: 428, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 5.110, mean reward: 0.568 [0.507, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.711], loss: 0.003719, mae: 0.066678, mean_q: 0.710051
 20501/100000: episode: 429, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 5.195, mean reward: 0.649 [0.588, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.762], loss: 0.003327, mae: 0.062292, mean_q: 0.687604
 20510/100000: episode: 430, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 4.737, mean reward: 0.526 [0.471, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.533], loss: 0.003035, mae: 0.058621, mean_q: 0.704613
 20524/100000: episode: 431, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 5.953, mean reward: 0.425 [0.290, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.426], loss: 0.003108, mae: 0.060611, mean_q: 0.714114
 20533/100000: episode: 432, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 6.003, mean reward: 0.667 [0.638, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.461, 10.645], loss: 0.003167, mae: 0.062511, mean_q: 0.720382
[Info] FALSIFICATION!
 20537/100000: episode: 433, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 11.697, mean reward: 2.924 [0.509, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.010, 10.791], loss: 0.002873, mae: 0.057405, mean_q: 0.683407
 20637/100000: episode: 434, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.596, mean reward: -0.196 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.682, 10.146], loss: 0.031745, mae: 0.085751, mean_q: 0.686861
 20737/100000: episode: 435, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.512, mean reward: -0.185 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.849, 10.120], loss: 0.058626, mae: 0.103729, mean_q: 0.644495
 20837/100000: episode: 436, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.170, mean reward: -0.162 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.461, 10.098], loss: 0.004121, mae: 0.067997, mean_q: 0.645522
 20937/100000: episode: 437, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.540, mean reward: -0.155 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.310, 10.098], loss: 0.018099, mae: 0.076323, mean_q: 0.656438
 21037/100000: episode: 438, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.492, mean reward: -0.195 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.986, 10.129], loss: 0.017443, mae: 0.075615, mean_q: 0.635979
 21137/100000: episode: 439, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -11.848, mean reward: -0.118 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.905, 10.422], loss: 0.003457, mae: 0.063823, mean_q: 0.598408
 21237/100000: episode: 440, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.118, mean reward: -0.151 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.165, 10.104], loss: 0.016513, mae: 0.067382, mean_q: 0.588955
 21337/100000: episode: 441, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.335, mean reward: -0.173 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.029, 10.098], loss: 0.004305, mae: 0.071110, mean_q: 0.568136
 21437/100000: episode: 442, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.369, mean reward: -0.184 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.570, 10.136], loss: 0.003571, mae: 0.063943, mean_q: 0.550805
 21537/100000: episode: 443, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.326, mean reward: -0.193 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.955, 10.155], loss: 0.016720, mae: 0.069705, mean_q: 0.529336
 21637/100000: episode: 444, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -8.323, mean reward: -0.083 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.898, 10.320], loss: 0.004013, mae: 0.065965, mean_q: 0.518566
 21737/100000: episode: 445, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -20.363, mean reward: -0.204 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.870, 10.098], loss: 0.016835, mae: 0.071131, mean_q: 0.500789
 21837/100000: episode: 446, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.257, mean reward: -0.153 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.784, 10.134], loss: 0.016876, mae: 0.071382, mean_q: 0.485482
 21937/100000: episode: 447, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.763, mean reward: -0.198 [-1.000, 0.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.077, 10.180], loss: 0.017418, mae: 0.074496, mean_q: 0.471957
 22037/100000: episode: 448, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.471, mean reward: -0.145 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.840, 10.137], loss: 0.030224, mae: 0.079224, mean_q: 0.450707
 22137/100000: episode: 449, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -19.579, mean reward: -0.196 [-1.000, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.640, 10.098], loss: 0.017579, mae: 0.074856, mean_q: 0.421132
 22237/100000: episode: 450, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.806, mean reward: -0.178 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.511, 10.284], loss: 0.032104, mae: 0.092024, mean_q: 0.400341
 22337/100000: episode: 451, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -11.889, mean reward: -0.119 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.081, 10.233], loss: 0.017510, mae: 0.075248, mean_q: 0.395655
 22437/100000: episode: 452, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.268, mean reward: -0.183 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.385, 10.347], loss: 0.042884, mae: 0.082276, mean_q: 0.381937
 22537/100000: episode: 453, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.534, mean reward: -0.135 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.484, 10.142], loss: 0.030004, mae: 0.075274, mean_q: 0.347580
 22637/100000: episode: 454, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.417, mean reward: -0.164 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.284, 10.098], loss: 0.003483, mae: 0.062027, mean_q: 0.338372
 22737/100000: episode: 455, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.365, mean reward: -0.164 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.501, 10.098], loss: 0.003676, mae: 0.063713, mean_q: 0.310872
 22837/100000: episode: 456, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.358, mean reward: -0.184 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.628, 10.112], loss: 0.016656, mae: 0.067656, mean_q: 0.265898
 22937/100000: episode: 457, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.127, mean reward: -0.181 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.928, 10.140], loss: 0.003895, mae: 0.066613, mean_q: 0.265989
 23037/100000: episode: 458, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.664, mean reward: -0.157 [-1.000, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.560, 10.116], loss: 0.016572, mae: 0.067563, mean_q: 0.229554
 23137/100000: episode: 459, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -16.332, mean reward: -0.163 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.985, 10.108], loss: 0.003465, mae: 0.061533, mean_q: 0.209411
 23237/100000: episode: 460, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -14.643, mean reward: -0.146 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.258, 10.098], loss: 0.016821, mae: 0.070029, mean_q: 0.190319
 23337/100000: episode: 461, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.780, mean reward: -0.188 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.842, 10.180], loss: 0.017079, mae: 0.072323, mean_q: 0.167652
 23437/100000: episode: 462, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.703, mean reward: -0.157 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.630, 10.098], loss: 0.016317, mae: 0.066313, mean_q: 0.146915
 23537/100000: episode: 463, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -13.452, mean reward: -0.135 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.011, 10.160], loss: 0.003267, mae: 0.059715, mean_q: 0.125081
 23637/100000: episode: 464, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.802, mean reward: -0.168 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.611, 10.271], loss: 0.030659, mae: 0.083120, mean_q: 0.130485
 23737/100000: episode: 465, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.921, mean reward: -0.189 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.122, 10.215], loss: 0.003831, mae: 0.066501, mean_q: 0.087381
 23837/100000: episode: 466, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -5.682, mean reward: -0.057 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.813, 10.098], loss: 0.029051, mae: 0.070546, mean_q: 0.086210
 23937/100000: episode: 467, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.533, mean reward: -0.175 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.786, 10.126], loss: 0.017531, mae: 0.074651, mean_q: 0.038266
 24037/100000: episode: 468, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.813, mean reward: -0.168 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.664, 10.115], loss: 0.003725, mae: 0.064354, mean_q: 0.034626
 24137/100000: episode: 469, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -17.792, mean reward: -0.178 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.666, 10.212], loss: 0.016683, mae: 0.068400, mean_q: -0.008662
 24237/100000: episode: 470, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -14.666, mean reward: -0.147 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.843, 10.098], loss: 0.016150, mae: 0.064438, mean_q: -0.025660
 24337/100000: episode: 471, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.678, mean reward: -0.197 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.174, 10.128], loss: 0.017955, mae: 0.077777, mean_q: -0.066186
 24437/100000: episode: 472, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.423, mean reward: -0.164 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.739, 10.274], loss: 0.003695, mae: 0.063057, mean_q: -0.061880
 24537/100000: episode: 473, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -13.368, mean reward: -0.134 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.769, 10.248], loss: 0.004228, mae: 0.068902, mean_q: -0.116111
 24637/100000: episode: 474, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.178, mean reward: -0.182 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.912, 10.098], loss: 0.016792, mae: 0.068676, mean_q: -0.122924
 24737/100000: episode: 475, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -13.351, mean reward: -0.134 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.264, 10.098], loss: 0.003191, mae: 0.058446, mean_q: -0.143392
 24837/100000: episode: 476, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.792, mean reward: -0.178 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.323, 10.098], loss: 0.003577, mae: 0.061766, mean_q: -0.147138
 24937/100000: episode: 477, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.766, mean reward: -0.168 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.861, 10.098], loss: 0.003258, mae: 0.058703, mean_q: -0.195515
 25037/100000: episode: 478, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.991, mean reward: -0.190 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.892, 10.098], loss: 0.016715, mae: 0.067421, mean_q: -0.234843
 25137/100000: episode: 479, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.379, mean reward: -0.154 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.060, 10.098], loss: 0.003661, mae: 0.064928, mean_q: -0.212935
 25237/100000: episode: 480, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.662, mean reward: -0.167 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.498, 10.380], loss: 0.003291, mae: 0.060235, mean_q: -0.259920
 25337/100000: episode: 481, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -13.328, mean reward: -0.133 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.282, 10.098], loss: 0.003190, mae: 0.058319, mean_q: -0.238213
 25437/100000: episode: 482, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.970, mean reward: -0.170 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.760, 10.098], loss: 0.016854, mae: 0.067230, mean_q: -0.275319
 25537/100000: episode: 483, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.841, mean reward: -0.188 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.247, 10.228], loss: 0.003184, mae: 0.058886, mean_q: -0.286684
 25637/100000: episode: 484, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -12.456, mean reward: -0.125 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.100, 10.324], loss: 0.003100, mae: 0.059835, mean_q: -0.318090
 25737/100000: episode: 485, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.243, mean reward: -0.172 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.839, 10.098], loss: 0.003058, mae: 0.057722, mean_q: -0.295380
 25837/100000: episode: 486, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.119, mean reward: -0.141 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.851, 10.098], loss: 0.002893, mae: 0.055894, mean_q: -0.314867
 25937/100000: episode: 487, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.926, mean reward: -0.169 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.025, 10.174], loss: 0.002946, mae: 0.055830, mean_q: -0.309125
 26037/100000: episode: 488, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -15.878, mean reward: -0.159 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.723, 10.395], loss: 0.002992, mae: 0.057248, mean_q: -0.287902
 26137/100000: episode: 489, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.367, mean reward: -0.174 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.790, 10.098], loss: 0.003107, mae: 0.057010, mean_q: -0.312514
 26237/100000: episode: 490, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -16.203, mean reward: -0.162 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.853, 10.230], loss: 0.002900, mae: 0.055948, mean_q: -0.305986
 26337/100000: episode: 491, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -19.106, mean reward: -0.191 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.234, 10.191], loss: 0.003019, mae: 0.056642, mean_q: -0.333209
 26437/100000: episode: 492, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.571, mean reward: -0.196 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.582, 10.152], loss: 0.002997, mae: 0.057081, mean_q: -0.311781
 26537/100000: episode: 493, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.352, mean reward: -0.174 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.634, 10.098], loss: 0.002975, mae: 0.055753, mean_q: -0.266013
 26637/100000: episode: 494, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.812, mean reward: -0.158 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.836, 10.178], loss: 0.003090, mae: 0.057587, mean_q: -0.300999
 26737/100000: episode: 495, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -20.504, mean reward: -0.205 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.606, 10.335], loss: 0.002894, mae: 0.054849, mean_q: -0.284607
 26837/100000: episode: 496, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.660, mean reward: -0.207 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.415, 10.098], loss: 0.002600, mae: 0.053004, mean_q: -0.310186
 26937/100000: episode: 497, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.101, mean reward: -0.181 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.038, 10.226], loss: 0.002793, mae: 0.055307, mean_q: -0.302685
 27037/100000: episode: 498, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.071, mean reward: -0.171 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.595, 10.209], loss: 0.002856, mae: 0.056037, mean_q: -0.286811
 27137/100000: episode: 499, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.558, mean reward: -0.196 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.947, 10.098], loss: 0.003222, mae: 0.059786, mean_q: -0.299324
 27237/100000: episode: 500, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.566, mean reward: -0.186 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.222, 10.301], loss: 0.003706, mae: 0.062892, mean_q: -0.329112
 27337/100000: episode: 501, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.708, mean reward: -0.147 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.311, 10.098], loss: 0.002741, mae: 0.053220, mean_q: -0.348394
 27437/100000: episode: 502, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.122, mean reward: -0.161 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.755, 10.098], loss: 0.004329, mae: 0.064905, mean_q: -0.313368
 27537/100000: episode: 503, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.223, mean reward: -0.182 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.485, 10.098], loss: 0.002517, mae: 0.051276, mean_q: -0.315886
 27637/100000: episode: 504, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.264, mean reward: -0.183 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.493, 10.098], loss: 0.002871, mae: 0.055194, mean_q: -0.320566
 27737/100000: episode: 505, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.973, mean reward: -0.160 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.178, 10.098], loss: 0.002924, mae: 0.056141, mean_q: -0.344843
 27837/100000: episode: 506, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.526, mean reward: -0.155 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.795, 10.174], loss: 0.002412, mae: 0.049964, mean_q: -0.310977
 27937/100000: episode: 507, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.402, mean reward: -0.174 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.079, 10.098], loss: 0.002633, mae: 0.052963, mean_q: -0.329654
 28037/100000: episode: 508, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -14.709, mean reward: -0.147 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.695, 10.098], loss: 0.002640, mae: 0.052694, mean_q: -0.310999
 28137/100000: episode: 509, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.469, mean reward: -0.185 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.237, 10.320], loss: 0.002642, mae: 0.053155, mean_q: -0.351375
 28237/100000: episode: 510, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.094, mean reward: -0.191 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.392, 10.128], loss: 0.002695, mae: 0.053640, mean_q: -0.314765
 28337/100000: episode: 511, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.381, mean reward: -0.184 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.890, 10.177], loss: 0.002535, mae: 0.051327, mean_q: -0.304260
 28437/100000: episode: 512, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.256, mean reward: -0.143 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.536, 10.412], loss: 0.002499, mae: 0.051052, mean_q: -0.345517
 28537/100000: episode: 513, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.456, mean reward: -0.155 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.070, 10.098], loss: 0.002622, mae: 0.052605, mean_q: -0.325068
 28637/100000: episode: 514, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -11.343, mean reward: -0.113 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.421, 10.098], loss: 0.002577, mae: 0.051645, mean_q: -0.337727
 28737/100000: episode: 515, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.906, mean reward: -0.179 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.927, 10.147], loss: 0.002559, mae: 0.052489, mean_q: -0.292721
 28837/100000: episode: 516, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -8.315, mean reward: -0.083 [-1.000, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.576, 10.547], loss: 0.002489, mae: 0.051431, mean_q: -0.350134
 28937/100000: episode: 517, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.039, mean reward: -0.150 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.216, 10.100], loss: 0.003241, mae: 0.060083, mean_q: -0.344923
 29037/100000: episode: 518, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.162, mean reward: -0.182 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.440, 10.100], loss: 0.002733, mae: 0.054856, mean_q: -0.306492
 29137/100000: episode: 519, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.539, mean reward: -0.165 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.647, 10.098], loss: 0.003063, mae: 0.058130, mean_q: -0.312453
 29237/100000: episode: 520, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -20.006, mean reward: -0.200 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.648, 10.098], loss: 0.002552, mae: 0.051928, mean_q: -0.297805
 29337/100000: episode: 521, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.317, mean reward: -0.173 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.928, 10.321], loss: 0.002513, mae: 0.051015, mean_q: -0.315659
 29437/100000: episode: 522, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -13.656, mean reward: -0.137 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.129, 10.098], loss: 0.002921, mae: 0.056826, mean_q: -0.326493
 29537/100000: episode: 523, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.383, mean reward: -0.194 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.646, 10.098], loss: 0.002651, mae: 0.052514, mean_q: -0.337653
 29637/100000: episode: 524, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -14.469, mean reward: -0.145 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.137, 10.098], loss: 0.003244, mae: 0.058564, mean_q: -0.282476
 29737/100000: episode: 525, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.933, mean reward: -0.199 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.277, 10.105], loss: 0.003697, mae: 0.060602, mean_q: -0.336014
 29837/100000: episode: 526, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -9.859, mean reward: -0.099 [-1.000, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.646, 10.098], loss: 0.002819, mae: 0.053792, mean_q: -0.316140
 29937/100000: episode: 527, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -19.150, mean reward: -0.191 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.081, 10.098], loss: 0.002725, mae: 0.052609, mean_q: -0.344483
 30037/100000: episode: 528, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.231, mean reward: -0.192 [-1.000, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.189, 10.098], loss: 0.002424, mae: 0.050788, mean_q: -0.303585
 30137/100000: episode: 529, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -15.228, mean reward: -0.152 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.121, 10.098], loss: 0.002758, mae: 0.053258, mean_q: -0.310305
 30237/100000: episode: 530, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.860, mean reward: -0.169 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.750, 10.105], loss: 0.002673, mae: 0.052413, mean_q: -0.314438
 30337/100000: episode: 531, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.702, mean reward: -0.167 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.751, 10.343], loss: 0.002803, mae: 0.054170, mean_q: -0.340087
 30437/100000: episode: 532, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.942, mean reward: -0.179 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.961, 10.098], loss: 0.002522, mae: 0.051561, mean_q: -0.314243
[Info] 100-TH LEVEL FOUND: 0.6052687168121338, Considering 10/90 traces
 30537/100000: episode: 533, duration: 4.420s, episode steps: 100, steps per second: 23, episode reward: -14.549, mean reward: -0.145 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.528, 10.098], loss: 0.002959, mae: 0.055205, mean_q: -0.324962
 30545/100000: episode: 534, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 2.635, mean reward: 0.329 [0.241, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.277, 10.100], loss: 0.002424, mae: 0.053655, mean_q: -0.379028
 30569/100000: episode: 535, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 7.025, mean reward: 0.293 [0.156, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.092, 10.100], loss: 0.002539, mae: 0.055216, mean_q: -0.377120
 30599/100000: episode: 536, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 8.101, mean reward: 0.270 [0.167, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.943, 10.295], loss: 0.002262, mae: 0.050885, mean_q: -0.311831
 30642/100000: episode: 537, duration: 0.242s, episode steps: 43, steps per second: 177, episode reward: 16.064, mean reward: 0.374 [0.208, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.401, 10.359], loss: 0.002647, mae: 0.053405, mean_q: -0.292929
 30668/100000: episode: 538, duration: 0.155s, episode steps: 26, steps per second: 167, episode reward: 5.588, mean reward: 0.215 [0.038, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.214, 10.110], loss: 0.002679, mae: 0.052192, mean_q: -0.316187
 30676/100000: episode: 539, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 1.870, mean reward: 0.234 [0.143, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.519, 10.100], loss: 0.003079, mae: 0.056454, mean_q: -0.254645
 30699/100000: episode: 540, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 8.215, mean reward: 0.357 [0.229, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.312, 10.100], loss: 0.004338, mae: 0.068760, mean_q: -0.258337
 30723/100000: episode: 541, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 8.169, mean reward: 0.340 [0.250, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.326, 10.100], loss: 0.003125, mae: 0.059408, mean_q: -0.297213
 30754/100000: episode: 542, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 10.740, mean reward: 0.346 [0.165, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.297, 10.255], loss: 0.002546, mae: 0.052921, mean_q: -0.305545
 30780/100000: episode: 543, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 7.141, mean reward: 0.275 [0.070, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.194, 10.100], loss: 0.002791, mae: 0.054396, mean_q: -0.275689
 30823/100000: episode: 544, duration: 0.228s, episode steps: 43, steps per second: 188, episode reward: 13.238, mean reward: 0.308 [0.204, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.855, 10.355], loss: 0.003091, mae: 0.056884, mean_q: -0.252257
 30853/100000: episode: 545, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 6.672, mean reward: 0.222 [0.072, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.288, 10.255], loss: 0.003028, mae: 0.056345, mean_q: -0.232515
 30879/100000: episode: 546, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 8.446, mean reward: 0.325 [0.189, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.630, 10.100], loss: 0.002716, mae: 0.052403, mean_q: -0.280669
 30909/100000: episode: 547, duration: 0.151s, episode steps: 30, steps per second: 198, episode reward: 9.277, mean reward: 0.309 [0.227, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.271, 10.390], loss: 0.002462, mae: 0.052010, mean_q: -0.252040
 30940/100000: episode: 548, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 7.565, mean reward: 0.244 [0.023, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.114, 10.210], loss: 0.002931, mae: 0.054816, mean_q: -0.262095
 30963/100000: episode: 549, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 6.429, mean reward: 0.280 [0.117, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.918, 10.100], loss: 0.002674, mae: 0.053640, mean_q: -0.209363
 30986/100000: episode: 550, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 6.063, mean reward: 0.264 [0.152, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.193, 10.100], loss: 0.003052, mae: 0.055483, mean_q: -0.268761
 31009/100000: episode: 551, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 6.119, mean reward: 0.266 [0.189, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.133, 10.100], loss: 0.003001, mae: 0.056236, mean_q: -0.233384
 31032/100000: episode: 552, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 6.138, mean reward: 0.267 [0.184, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.793, 10.100], loss: 0.002539, mae: 0.052097, mean_q: -0.271503
 31055/100000: episode: 553, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 6.537, mean reward: 0.284 [0.235, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.849, 10.100], loss: 0.002469, mae: 0.050399, mean_q: -0.205570
 31063/100000: episode: 554, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 2.500, mean reward: 0.312 [0.263, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-1.031, 10.100], loss: 0.002460, mae: 0.047981, mean_q: -0.185509
 31106/100000: episode: 555, duration: 0.219s, episode steps: 43, steps per second: 197, episode reward: 12.682, mean reward: 0.295 [0.212, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.162, 10.346], loss: 0.002691, mae: 0.053834, mean_q: -0.182029
 31129/100000: episode: 556, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 7.245, mean reward: 0.315 [0.251, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.429, 10.380], loss: 0.002392, mae: 0.049023, mean_q: -0.214746
 31154/100000: episode: 557, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 7.268, mean reward: 0.291 [0.223, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.647, 10.100], loss: 0.002880, mae: 0.054692, mean_q: -0.212542
 31177/100000: episode: 558, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 5.627, mean reward: 0.245 [0.099, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.696, 10.235], loss: 0.003017, mae: 0.055873, mean_q: -0.188958
 31208/100000: episode: 559, duration: 0.183s, episode steps: 31, steps per second: 170, episode reward: 8.533, mean reward: 0.275 [0.012, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.072, 10.155], loss: 0.002667, mae: 0.051324, mean_q: -0.184420
 31233/100000: episode: 560, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 5.748, mean reward: 0.230 [0.062, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.035, 10.100], loss: 0.002265, mae: 0.048847, mean_q: -0.194711
 31256/100000: episode: 561, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 5.200, mean reward: 0.226 [0.137, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.210, 10.100], loss: 0.002556, mae: 0.051300, mean_q: -0.216718
 31286/100000: episode: 562, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 11.413, mean reward: 0.380 [0.267, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.532, 10.384], loss: 0.002610, mae: 0.053570, mean_q: -0.227885
 31309/100000: episode: 563, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 4.284, mean reward: 0.186 [0.032, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.331, 10.100], loss: 0.002968, mae: 0.056448, mean_q: -0.180639
 31339/100000: episode: 564, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 12.887, mean reward: 0.430 [0.249, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.583, 10.667], loss: 0.004018, mae: 0.065407, mean_q: -0.166203
 31365/100000: episode: 565, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 8.690, mean reward: 0.334 [0.229, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.941, 10.100], loss: 0.005377, mae: 0.070554, mean_q: -0.150039
 31388/100000: episode: 566, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 7.441, mean reward: 0.324 [0.120, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.152, 10.100], loss: 0.004567, mae: 0.073780, mean_q: -0.183080
 31418/100000: episode: 567, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 8.231, mean reward: 0.274 [0.210, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.097, 10.421], loss: 0.003854, mae: 0.064843, mean_q: -0.169330
 31449/100000: episode: 568, duration: 0.155s, episode steps: 31, steps per second: 200, episode reward: 10.004, mean reward: 0.323 [0.198, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.313, 10.339], loss: 0.004305, mae: 0.068045, mean_q: -0.180222
 31480/100000: episode: 569, duration: 0.160s, episode steps: 31, steps per second: 193, episode reward: 8.450, mean reward: 0.273 [0.094, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.731, 10.194], loss: 0.005575, mae: 0.080653, mean_q: -0.098730
 31503/100000: episode: 570, duration: 0.140s, episode steps: 23, steps per second: 165, episode reward: 6.762, mean reward: 0.294 [0.195, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.199, 10.100], loss: 0.002790, mae: 0.058940, mean_q: -0.113848
 31546/100000: episode: 571, duration: 0.238s, episode steps: 43, steps per second: 181, episode reward: 11.826, mean reward: 0.275 [0.123, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.788, 10.284], loss: 0.002886, mae: 0.056532, mean_q: -0.135790
 31589/100000: episode: 572, duration: 0.229s, episode steps: 43, steps per second: 188, episode reward: 7.681, mean reward: 0.179 [0.019, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.429, 10.156], loss: 0.002915, mae: 0.055352, mean_q: -0.132372
 31619/100000: episode: 573, duration: 0.160s, episode steps: 30, steps per second: 188, episode reward: 6.689, mean reward: 0.223 [0.087, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.160, 10.192], loss: 0.002771, mae: 0.054490, mean_q: -0.060054
 31644/100000: episode: 574, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 7.909, mean reward: 0.316 [0.228, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.307, 10.100], loss: 0.002721, mae: 0.055209, mean_q: -0.082628
 31674/100000: episode: 575, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 10.154, mean reward: 0.338 [0.209, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.276, 10.368], loss: 0.003042, mae: 0.057011, mean_q: -0.113682
 31705/100000: episode: 576, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 10.645, mean reward: 0.343 [0.220, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.226, 10.364], loss: 0.002856, mae: 0.054512, mean_q: -0.110365
 31728/100000: episode: 577, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 8.814, mean reward: 0.383 [0.245, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.454, 10.100], loss: 0.002874, mae: 0.054950, mean_q: -0.166943
 31751/100000: episode: 578, duration: 0.118s, episode steps: 23, steps per second: 196, episode reward: 6.085, mean reward: 0.265 [0.117, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.599, 10.372], loss: 0.002699, mae: 0.053975, mean_q: -0.115291
 31759/100000: episode: 579, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 2.281, mean reward: 0.285 [0.219, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.263, 10.100], loss: 0.002582, mae: 0.051365, mean_q: -0.157084
 31782/100000: episode: 580, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 8.560, mean reward: 0.372 [0.214, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.836, 10.315], loss: 0.003056, mae: 0.055656, mean_q: -0.033887
 31805/100000: episode: 581, duration: 0.139s, episode steps: 23, steps per second: 166, episode reward: 7.185, mean reward: 0.312 [0.226, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.422], loss: 0.002631, mae: 0.053221, mean_q: -0.064588
 31830/100000: episode: 582, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 6.695, mean reward: 0.268 [0.145, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.289, 10.100], loss: 0.002544, mae: 0.052172, mean_q: -0.068226
 31853/100000: episode: 583, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 12.608, mean reward: 0.548 [0.379, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.637], loss: 0.002540, mae: 0.051927, mean_q: -0.088604
 31877/100000: episode: 584, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 9.146, mean reward: 0.381 [0.270, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.917, 10.100], loss: 0.002830, mae: 0.054746, mean_q: -0.066531
 31900/100000: episode: 585, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 5.893, mean reward: 0.256 [0.179, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.965, 10.100], loss: 0.002985, mae: 0.055977, mean_q: -0.064144
 31923/100000: episode: 586, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 7.288, mean reward: 0.317 [0.153, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.452, 10.100], loss: 0.002998, mae: 0.056527, mean_q: -0.074628
 31949/100000: episode: 587, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 10.275, mean reward: 0.395 [0.244, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.774, 10.100], loss: 0.002694, mae: 0.052971, mean_q: -0.048119
 31973/100000: episode: 588, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 7.598, mean reward: 0.317 [0.161, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.265, 10.100], loss: 0.002825, mae: 0.055141, mean_q: -0.038199
 31997/100000: episode: 589, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 8.637, mean reward: 0.360 [0.147, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.114, 10.100], loss: 0.002548, mae: 0.051589, mean_q: -0.057992
 32028/100000: episode: 590, duration: 0.185s, episode steps: 31, steps per second: 168, episode reward: 6.473, mean reward: 0.209 [0.065, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.572, 10.140], loss: 0.002871, mae: 0.054694, mean_q: -0.004428
 32052/100000: episode: 591, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 6.474, mean reward: 0.270 [0.116, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.201, 10.100], loss: 0.002914, mae: 0.054393, mean_q: -0.019449
 32075/100000: episode: 592, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 6.885, mean reward: 0.299 [0.173, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.499, 10.100], loss: 0.002516, mae: 0.051665, mean_q: -0.045074
 32083/100000: episode: 593, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 2.012, mean reward: 0.251 [0.206, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.267, 10.100], loss: 0.002813, mae: 0.053739, mean_q: -0.121753
 32109/100000: episode: 594, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 10.080, mean reward: 0.388 [0.283, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.262, 10.100], loss: 0.002832, mae: 0.054481, mean_q: -0.044670
 32132/100000: episode: 595, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 4.958, mean reward: 0.216 [0.032, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.301, 10.100], loss: 0.002362, mae: 0.050907, mean_q: -0.062457
 32158/100000: episode: 596, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 9.929, mean reward: 0.382 [0.245, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.649, 10.100], loss: 0.002640, mae: 0.052138, mean_q: -0.016952
 32183/100000: episode: 597, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 9.383, mean reward: 0.375 [0.278, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.211, 10.100], loss: 0.003053, mae: 0.057032, mean_q: 0.037282
 32206/100000: episode: 598, duration: 0.125s, episode steps: 23, steps per second: 185, episode reward: 6.005, mean reward: 0.261 [0.188, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-1.061, 10.100], loss: 0.003046, mae: 0.057458, mean_q: 0.019081
 32229/100000: episode: 599, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 6.967, mean reward: 0.303 [0.209, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.268, 10.100], loss: 0.003308, mae: 0.061024, mean_q: 0.067809
 32252/100000: episode: 600, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 5.640, mean reward: 0.245 [0.123, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.180, 10.100], loss: 0.003126, mae: 0.058413, mean_q: -0.030413
 32283/100000: episode: 601, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 10.864, mean reward: 0.350 [0.236, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.035, 10.444], loss: 0.002828, mae: 0.054877, mean_q: 0.025854
 32291/100000: episode: 602, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 2.223, mean reward: 0.278 [0.233, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.286, 10.100], loss: 0.003229, mae: 0.059355, mean_q: 0.128188
 32314/100000: episode: 603, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 7.075, mean reward: 0.308 [0.104, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.415, 10.204], loss: 0.002713, mae: 0.053236, mean_q: -0.026140
 32339/100000: episode: 604, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 9.673, mean reward: 0.387 [0.202, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.158, 10.100], loss: 0.002929, mae: 0.056489, mean_q: 0.002511
 32364/100000: episode: 605, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 9.202, mean reward: 0.368 [0.153, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.130, 10.100], loss: 0.002695, mae: 0.054500, mean_q: 0.037608
 32395/100000: episode: 606, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 8.289, mean reward: 0.267 [0.129, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.292, 10.283], loss: 0.003351, mae: 0.060438, mean_q: 0.015022
 32418/100000: episode: 607, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 4.865, mean reward: 0.212 [0.060, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.425, 10.100], loss: 0.002531, mae: 0.051187, mean_q: -0.009409
 32461/100000: episode: 608, duration: 0.227s, episode steps: 43, steps per second: 190, episode reward: 10.050, mean reward: 0.234 [0.009, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.202, 10.118], loss: 0.002748, mae: 0.054699, mean_q: 0.063493
 32484/100000: episode: 609, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 5.988, mean reward: 0.260 [0.193, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.801, 10.100], loss: 0.004653, mae: 0.070330, mean_q: 0.020972
 32507/100000: episode: 610, duration: 0.131s, episode steps: 23, steps per second: 175, episode reward: 7.164, mean reward: 0.311 [0.238, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.198, 10.100], loss: 0.002573, mae: 0.054827, mean_q: 0.016909
 32515/100000: episode: 611, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 2.437, mean reward: 0.305 [0.241, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.307, 10.100], loss: 0.003139, mae: 0.062011, mean_q: 0.131999
 32538/100000: episode: 612, duration: 0.128s, episode steps: 23, steps per second: 179, episode reward: 4.936, mean reward: 0.215 [0.037, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.529, 10.160], loss: 0.003250, mae: 0.059348, mean_q: 0.032193
 32562/100000: episode: 613, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 7.880, mean reward: 0.328 [0.213, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.448, 10.100], loss: 0.002972, mae: 0.056511, mean_q: 0.063888
 32585/100000: episode: 614, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 5.915, mean reward: 0.257 [0.184, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.385, 10.332], loss: 0.002484, mae: 0.052072, mean_q: 0.066712
 32610/100000: episode: 615, duration: 0.149s, episode steps: 25, steps per second: 168, episode reward: 8.372, mean reward: 0.335 [0.235, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.528, 10.100], loss: 0.002806, mae: 0.055553, mean_q: 0.084584
 32640/100000: episode: 616, duration: 0.159s, episode steps: 30, steps per second: 188, episode reward: 8.951, mean reward: 0.298 [0.202, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.134, 10.371], loss: 0.002901, mae: 0.057862, mean_q: 0.125307
 32671/100000: episode: 617, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 10.056, mean reward: 0.324 [0.183, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.035, 10.474], loss: 0.002795, mae: 0.055757, mean_q: 0.097011
 32696/100000: episode: 618, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 4.995, mean reward: 0.200 [0.064, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.610, 10.100], loss: 0.003099, mae: 0.059476, mean_q: 0.087977
 32722/100000: episode: 619, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 10.239, mean reward: 0.394 [0.154, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.855, 10.100], loss: 0.002890, mae: 0.055273, mean_q: 0.067783
 32748/100000: episode: 620, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 9.982, mean reward: 0.384 [0.331, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.315, 10.100], loss: 0.002841, mae: 0.056146, mean_q: 0.122648
 32756/100000: episode: 621, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 1.740, mean reward: 0.218 [0.146, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.352, 10.100], loss: 0.002681, mae: 0.052779, mean_q: 0.068670
 32781/100000: episode: 622, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 6.984, mean reward: 0.279 [0.172, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.503, 10.100], loss: 0.002338, mae: 0.050565, mean_q: 0.096451
[Info] 200-TH LEVEL FOUND: 0.7882043719291687, Considering 10/90 traces
 32806/100000: episode: 623, duration: 4.031s, episode steps: 25, steps per second: 6, episode reward: 8.168, mean reward: 0.327 [0.237, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.269, 10.100], loss: 0.002497, mae: 0.052389, mean_q: 0.101231
 32833/100000: episode: 624, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 11.477, mean reward: 0.425 [0.336, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.387, 10.604], loss: 0.002597, mae: 0.053529, mean_q: 0.156018
 32857/100000: episode: 625, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 8.870, mean reward: 0.370 [0.265, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.435, 10.401], loss: 0.002923, mae: 0.056892, mean_q: 0.102939
 32878/100000: episode: 626, duration: 0.138s, episode steps: 21, steps per second: 152, episode reward: 6.798, mean reward: 0.324 [0.256, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.202, 10.418], loss: 0.002865, mae: 0.055572, mean_q: 0.096500
 32902/100000: episode: 627, duration: 0.127s, episode steps: 24, steps per second: 190, episode reward: 6.327, mean reward: 0.264 [0.167, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.959, 10.269], loss: 0.002868, mae: 0.056943, mean_q: 0.137586
 32923/100000: episode: 628, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 5.596, mean reward: 0.266 [0.154, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.165, 10.267], loss: 0.002842, mae: 0.056872, mean_q: 0.127039
 32962/100000: episode: 629, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 8.901, mean reward: 0.228 [0.042, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.216, 10.100], loss: 0.002422, mae: 0.052418, mean_q: 0.124717
 32989/100000: episode: 630, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 9.207, mean reward: 0.341 [0.198, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.683, 10.312], loss: 0.002867, mae: 0.057550, mean_q: 0.179525
 33013/100000: episode: 631, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 7.448, mean reward: 0.310 [0.188, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.497, 10.326], loss: 0.002372, mae: 0.051237, mean_q: 0.142950
 33040/100000: episode: 632, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 7.846, mean reward: 0.291 [0.088, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.200, 10.240], loss: 0.002812, mae: 0.055974, mean_q: 0.120442
 33079/100000: episode: 633, duration: 0.208s, episode steps: 39, steps per second: 188, episode reward: 9.327, mean reward: 0.239 [0.036, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.621, 10.230], loss: 0.002932, mae: 0.056728, mean_q: 0.145913
 33100/100000: episode: 634, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 7.146, mean reward: 0.340 [0.217, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.413], loss: 0.003037, mae: 0.057063, mean_q: 0.191418
 33127/100000: episode: 635, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 8.104, mean reward: 0.300 [0.157, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.024, 10.271], loss: 0.002671, mae: 0.054918, mean_q: 0.144822
 33166/100000: episode: 636, duration: 0.213s, episode steps: 39, steps per second: 183, episode reward: 14.390, mean reward: 0.369 [0.266, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.794, 10.369], loss: 0.002755, mae: 0.056035, mean_q: 0.191354
 33206/100000: episode: 637, duration: 0.225s, episode steps: 40, steps per second: 178, episode reward: 11.849, mean reward: 0.296 [0.178, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.116, 10.389], loss: 0.002793, mae: 0.056745, mean_q: 0.155086
 33225/100000: episode: 638, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 7.484, mean reward: 0.394 [0.221, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.760, 10.413], loss: 0.002742, mae: 0.056095, mean_q: 0.170578
 33252/100000: episode: 639, duration: 0.152s, episode steps: 27, steps per second: 177, episode reward: 7.666, mean reward: 0.284 [0.056, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.369, 10.110], loss: 0.002811, mae: 0.055235, mean_q: 0.214642
 33273/100000: episode: 640, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 9.305, mean reward: 0.443 [0.247, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.469], loss: 0.002745, mae: 0.056719, mean_q: 0.115863
 33300/100000: episode: 641, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 10.099, mean reward: 0.374 [0.262, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.475], loss: 0.002439, mae: 0.051174, mean_q: 0.201049
 33321/100000: episode: 642, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 5.127, mean reward: 0.244 [0.144, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.292], loss: 0.002977, mae: 0.058184, mean_q: 0.277728
 33348/100000: episode: 643, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 10.686, mean reward: 0.396 [0.301, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.409, 10.482], loss: 0.003275, mae: 0.061324, mean_q: 0.246533
 33372/100000: episode: 644, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 6.385, mean reward: 0.266 [0.040, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.850, 10.189], loss: 0.002819, mae: 0.056920, mean_q: 0.193030
 33393/100000: episode: 645, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 7.611, mean reward: 0.362 [0.245, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.379, 10.327], loss: 0.002725, mae: 0.055818, mean_q: 0.274310
 33417/100000: episode: 646, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 8.584, mean reward: 0.358 [0.255, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.018, 10.373], loss: 0.002395, mae: 0.051864, mean_q: 0.211293
 33441/100000: episode: 647, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 7.953, mean reward: 0.331 [0.259, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.385], loss: 0.003137, mae: 0.058925, mean_q: 0.276074
 33465/100000: episode: 648, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 7.863, mean reward: 0.328 [0.195, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.350, 10.316], loss: 0.002528, mae: 0.054452, mean_q: 0.223647
 33504/100000: episode: 649, duration: 0.210s, episode steps: 39, steps per second: 186, episode reward: 12.713, mean reward: 0.326 [0.206, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.367, 10.329], loss: 0.002430, mae: 0.054083, mean_q: 0.211294
 33544/100000: episode: 650, duration: 0.222s, episode steps: 40, steps per second: 180, episode reward: 14.389, mean reward: 0.360 [0.057, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.244, 10.196], loss: 0.002832, mae: 0.057687, mean_q: 0.298653
 33571/100000: episode: 651, duration: 0.144s, episode steps: 27, steps per second: 188, episode reward: 8.590, mean reward: 0.318 [0.138, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.105, 10.300], loss: 0.003258, mae: 0.061424, mean_q: 0.278117
 33592/100000: episode: 652, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 7.197, mean reward: 0.343 [0.223, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.530, 10.358], loss: 0.002958, mae: 0.057144, mean_q: 0.264072
 33631/100000: episode: 653, duration: 0.208s, episode steps: 39, steps per second: 187, episode reward: 15.219, mean reward: 0.390 [0.183, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.691, 10.303], loss: 0.002590, mae: 0.053735, mean_q: 0.267419
 33670/100000: episode: 654, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 8.859, mean reward: 0.227 [0.032, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.579, 10.111], loss: 0.002683, mae: 0.055066, mean_q: 0.251443
 33689/100000: episode: 655, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 6.526, mean reward: 0.343 [0.173, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.327, 10.313], loss: 0.002725, mae: 0.056686, mean_q: 0.257274
 33710/100000: episode: 656, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 11.281, mean reward: 0.537 [0.400, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.824, 10.663], loss: 0.002576, mae: 0.053613, mean_q: 0.267949
 33750/100000: episode: 657, duration: 0.218s, episode steps: 40, steps per second: 184, episode reward: 13.537, mean reward: 0.338 [0.191, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.938, 10.351], loss: 0.002853, mae: 0.056375, mean_q: 0.228347
 33774/100000: episode: 658, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 9.804, mean reward: 0.408 [0.295, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-1.162, 10.465], loss: 0.002783, mae: 0.056847, mean_q: 0.244897
 33801/100000: episode: 659, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 12.691, mean reward: 0.470 [0.335, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.132, 10.720], loss: 0.002989, mae: 0.060100, mean_q: 0.285706
 33825/100000: episode: 660, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 8.226, mean reward: 0.343 [0.197, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.150, 10.283], loss: 0.002922, mae: 0.058509, mean_q: 0.287671
 33865/100000: episode: 661, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 11.883, mean reward: 0.297 [0.167, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.949, 10.365], loss: 0.002481, mae: 0.053149, mean_q: 0.301618
 33884/100000: episode: 662, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 6.999, mean reward: 0.368 [0.299, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.389, 10.422], loss: 0.002554, mae: 0.054799, mean_q: 0.278925
 33908/100000: episode: 663, duration: 0.148s, episode steps: 24, steps per second: 162, episode reward: 7.073, mean reward: 0.295 [0.230, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.509, 10.405], loss: 0.002599, mae: 0.055716, mean_q: 0.335358
 33929/100000: episode: 664, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 7.834, mean reward: 0.373 [0.261, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.319], loss: 0.002719, mae: 0.054542, mean_q: 0.282862
 33950/100000: episode: 665, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 7.055, mean reward: 0.336 [0.157, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.258, 10.265], loss: 0.002708, mae: 0.056402, mean_q: 0.317961
 33974/100000: episode: 666, duration: 0.146s, episode steps: 24, steps per second: 165, episode reward: 10.872, mean reward: 0.453 [0.345, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.269, 10.535], loss: 0.002986, mae: 0.059010, mean_q: 0.364862
 33993/100000: episode: 667, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 7.275, mean reward: 0.383 [0.212, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.037, 10.488], loss: 0.002674, mae: 0.057289, mean_q: 0.341215
 34017/100000: episode: 668, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 9.052, mean reward: 0.377 [0.285, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.333, 10.543], loss: 0.002930, mae: 0.058161, mean_q: 0.217282
 34038/100000: episode: 669, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 9.186, mean reward: 0.437 [0.344, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.470], loss: 0.002664, mae: 0.056495, mean_q: 0.350588
 34062/100000: episode: 670, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 8.032, mean reward: 0.335 [0.105, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.139, 10.325], loss: 0.002482, mae: 0.053918, mean_q: 0.407668
 34089/100000: episode: 671, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 8.091, mean reward: 0.300 [0.116, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.662, 10.289], loss: 0.003077, mae: 0.059859, mean_q: 0.390046
 34116/100000: episode: 672, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 7.505, mean reward: 0.278 [0.155, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.141, 10.324], loss: 0.002618, mae: 0.054715, mean_q: 0.369202
 34140/100000: episode: 673, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 10.665, mean reward: 0.444 [0.331, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.966, 10.522], loss: 0.002564, mae: 0.054598, mean_q: 0.354727
 34164/100000: episode: 674, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 8.378, mean reward: 0.349 [0.284, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.439, 10.450], loss: 0.002741, mae: 0.054179, mean_q: 0.378265
 34203/100000: episode: 675, duration: 0.214s, episode steps: 39, steps per second: 183, episode reward: 14.211, mean reward: 0.364 [0.056, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.116, 10.210], loss: 0.002725, mae: 0.056377, mean_q: 0.399062
 34222/100000: episode: 676, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 6.234, mean reward: 0.328 [0.242, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.407, 10.436], loss: 0.002672, mae: 0.056085, mean_q: 0.384747
 34249/100000: episode: 677, duration: 0.145s, episode steps: 27, steps per second: 187, episode reward: 9.140, mean reward: 0.339 [0.194, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.792, 10.371], loss: 0.002511, mae: 0.055098, mean_q: 0.385486
 34270/100000: episode: 678, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 9.227, mean reward: 0.439 [0.334, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-1.133, 10.446], loss: 0.002444, mae: 0.054120, mean_q: 0.411495
 34291/100000: episode: 679, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 6.530, mean reward: 0.311 [0.236, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.438], loss: 0.002782, mae: 0.056310, mean_q: 0.414851
 34315/100000: episode: 680, duration: 0.127s, episode steps: 24, steps per second: 190, episode reward: 9.173, mean reward: 0.382 [0.249, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.120, 10.324], loss: 0.002711, mae: 0.055956, mean_q: 0.381425
 34355/100000: episode: 681, duration: 0.220s, episode steps: 40, steps per second: 181, episode reward: 14.320, mean reward: 0.358 [0.265, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.111, 10.422], loss: 0.002554, mae: 0.054969, mean_q: 0.424252
 34379/100000: episode: 682, duration: 0.127s, episode steps: 24, steps per second: 188, episode reward: 6.985, mean reward: 0.291 [0.196, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.431, 10.378], loss: 0.002790, mae: 0.057969, mean_q: 0.419127
 34403/100000: episode: 683, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 8.704, mean reward: 0.363 [0.306, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.579, 10.468], loss: 0.002909, mae: 0.057787, mean_q: 0.387865
 34442/100000: episode: 684, duration: 0.220s, episode steps: 39, steps per second: 177, episode reward: 9.504, mean reward: 0.244 [0.090, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.566, 10.208], loss: 0.002711, mae: 0.056937, mean_q: 0.420115
 34466/100000: episode: 685, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 6.914, mean reward: 0.288 [0.224, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.203, 10.355], loss: 0.002742, mae: 0.057053, mean_q: 0.424948
 34487/100000: episode: 686, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 5.457, mean reward: 0.260 [0.142, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.285], loss: 0.002806, mae: 0.057058, mean_q: 0.410256
 34527/100000: episode: 687, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 12.713, mean reward: 0.318 [0.218, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.278, 10.358], loss: 0.002688, mae: 0.056345, mean_q: 0.436697
 34551/100000: episode: 688, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 7.635, mean reward: 0.318 [0.244, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.061, 10.381], loss: 0.002497, mae: 0.054325, mean_q: 0.429827
 34591/100000: episode: 689, duration: 0.210s, episode steps: 40, steps per second: 190, episode reward: 15.483, mean reward: 0.387 [0.245, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-1.163, 10.342], loss: 0.002445, mae: 0.053983, mean_q: 0.457618
 34615/100000: episode: 690, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 8.531, mean reward: 0.355 [0.240, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.161, 10.314], loss: 0.002630, mae: 0.055272, mean_q: 0.492158
 34639/100000: episode: 691, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 7.264, mean reward: 0.303 [0.182, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.345], loss: 0.002687, mae: 0.056345, mean_q: 0.457955
 34679/100000: episode: 692, duration: 0.215s, episode steps: 40, steps per second: 186, episode reward: 13.406, mean reward: 0.335 [0.106, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.127, 10.323], loss: 0.002428, mae: 0.053705, mean_q: 0.469199
 34719/100000: episode: 693, duration: 0.215s, episode steps: 40, steps per second: 186, episode reward: 11.760, mean reward: 0.294 [0.137, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.521, 10.241], loss: 0.002554, mae: 0.055083, mean_q: 0.498398
 34746/100000: episode: 694, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 9.614, mean reward: 0.356 [0.268, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.958, 10.371], loss: 0.002362, mae: 0.053800, mean_q: 0.497527
 34767/100000: episode: 695, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 8.412, mean reward: 0.401 [0.260, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.378], loss: 0.002892, mae: 0.058205, mean_q: 0.501165
 34806/100000: episode: 696, duration: 0.229s, episode steps: 39, steps per second: 171, episode reward: 13.584, mean reward: 0.348 [0.040, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.821, 10.173], loss: 0.002424, mae: 0.054140, mean_q: 0.530881
 34827/100000: episode: 697, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 10.599, mean reward: 0.505 [0.441, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.502, 10.493], loss: 0.002611, mae: 0.056116, mean_q: 0.478993
 34851/100000: episode: 698, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 9.424, mean reward: 0.393 [0.248, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.412], loss: 0.002751, mae: 0.058399, mean_q: 0.512501
 34878/100000: episode: 699, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 11.987, mean reward: 0.444 [0.318, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.721, 10.578], loss: 0.002380, mae: 0.054175, mean_q: 0.541214
 34899/100000: episode: 700, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 7.923, mean reward: 0.377 [0.264, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.454], loss: 0.003117, mae: 0.060805, mean_q: 0.499564
 34923/100000: episode: 701, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 6.809, mean reward: 0.284 [0.115, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.369, 10.230], loss: 0.002679, mae: 0.057815, mean_q: 0.533090
 34950/100000: episode: 702, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 10.095, mean reward: 0.374 [0.233, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.192, 10.490], loss: 0.002610, mae: 0.056090, mean_q: 0.518871
 34977/100000: episode: 703, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 10.371, mean reward: 0.384 [0.265, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.813, 10.419], loss: 0.003176, mae: 0.062640, mean_q: 0.553266
 35004/100000: episode: 704, duration: 0.149s, episode steps: 27, steps per second: 182, episode reward: 6.866, mean reward: 0.254 [0.141, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.926, 10.289], loss: 0.002606, mae: 0.057198, mean_q: 0.574281
 35031/100000: episode: 705, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 11.026, mean reward: 0.408 [0.242, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.487, 10.362], loss: 0.002545, mae: 0.055151, mean_q: 0.522667
 35055/100000: episode: 706, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 7.001, mean reward: 0.292 [0.092, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.186], loss: 0.002447, mae: 0.054399, mean_q: 0.545275
 35079/100000: episode: 707, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 7.319, mean reward: 0.305 [0.130, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.150, 10.272], loss: 0.002901, mae: 0.059724, mean_q: 0.566809
 35118/100000: episode: 708, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 9.662, mean reward: 0.248 [0.019, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.078, 10.220], loss: 0.002843, mae: 0.059827, mean_q: 0.585305
 35142/100000: episode: 709, duration: 0.143s, episode steps: 24, steps per second: 167, episode reward: 7.520, mean reward: 0.313 [0.192, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.279, 10.304], loss: 0.002841, mae: 0.059070, mean_q: 0.546549
 35182/100000: episode: 710, duration: 0.216s, episode steps: 40, steps per second: 185, episode reward: 14.020, mean reward: 0.350 [0.269, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-1.512, 10.401], loss: 0.002742, mae: 0.058167, mean_q: 0.592811
 35206/100000: episode: 711, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 6.863, mean reward: 0.286 [0.142, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.903, 10.264], loss: 0.002865, mae: 0.059873, mean_q: 0.592331
 35245/100000: episode: 712, duration: 0.220s, episode steps: 39, steps per second: 177, episode reward: 9.147, mean reward: 0.235 [0.054, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-1.025, 10.221], loss: 0.002912, mae: 0.059649, mean_q: 0.599722
[Info] 300-TH LEVEL FOUND: 0.9810832142829895, Considering 10/90 traces
 35269/100000: episode: 713, duration: 4.074s, episode steps: 24, steps per second: 6, episode reward: 6.952, mean reward: 0.290 [0.203, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.459, 10.281], loss: 0.002575, mae: 0.056063, mean_q: 0.589841
 35292/100000: episode: 714, duration: 0.131s, episode steps: 23, steps per second: 175, episode reward: 11.242, mean reward: 0.489 [0.348, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.540], loss: 0.002391, mae: 0.053995, mean_q: 0.614230
 35321/100000: episode: 715, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 10.182, mean reward: 0.351 [0.195, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.541, 10.376], loss: 0.002771, mae: 0.059142, mean_q: 0.611688
 35340/100000: episode: 716, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 8.775, mean reward: 0.462 [0.362, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.610], loss: 0.002561, mae: 0.057767, mean_q: 0.617635
 35356/100000: episode: 717, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 7.598, mean reward: 0.475 [0.327, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.408, 10.531], loss: 0.002497, mae: 0.056642, mean_q: 0.610091
 35372/100000: episode: 718, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 5.984, mean reward: 0.374 [0.257, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.489], loss: 0.002724, mae: 0.058768, mean_q: 0.611038
 35394/100000: episode: 719, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 9.784, mean reward: 0.445 [0.302, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.640, 10.338], loss: 0.002761, mae: 0.058887, mean_q: 0.606522
 35424/100000: episode: 720, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 14.147, mean reward: 0.472 [0.340, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.783, 10.518], loss: 0.002783, mae: 0.058584, mean_q: 0.621891
 35440/100000: episode: 721, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 8.336, mean reward: 0.521 [0.451, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.528], loss: 0.002608, mae: 0.056610, mean_q: 0.610458
 35456/100000: episode: 722, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 7.281, mean reward: 0.455 [0.388, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.226, 10.470], loss: 0.002612, mae: 0.056368, mean_q: 0.632611
 35479/100000: episode: 723, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 11.284, mean reward: 0.491 [0.409, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.573], loss: 0.002420, mae: 0.053997, mean_q: 0.621475
 35508/100000: episode: 724, duration: 0.154s, episode steps: 29, steps per second: 189, episode reward: 12.770, mean reward: 0.440 [0.287, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.209, 10.449], loss: 0.002991, mae: 0.060515, mean_q: 0.623622
 35531/100000: episode: 725, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 7.517, mean reward: 0.327 [0.171, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.372], loss: 0.002797, mae: 0.058609, mean_q: 0.634932
 35560/100000: episode: 726, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 10.314, mean reward: 0.356 [0.220, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-1.147, 10.313], loss: 0.002720, mae: 0.058396, mean_q: 0.629638
 35576/100000: episode: 727, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 7.127, mean reward: 0.445 [0.330, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.425, 10.514], loss: 0.002450, mae: 0.054931, mean_q: 0.622249
 35592/100000: episode: 728, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 7.328, mean reward: 0.458 [0.384, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.520, 10.546], loss: 0.002875, mae: 0.059348, mean_q: 0.612195
 35622/100000: episode: 729, duration: 0.152s, episode steps: 30, steps per second: 197, episode reward: 15.304, mean reward: 0.510 [0.404, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.844, 10.506], loss: 0.002589, mae: 0.056498, mean_q: 0.627490
 35638/100000: episode: 730, duration: 0.105s, episode steps: 16, steps per second: 152, episode reward: 7.311, mean reward: 0.457 [0.260, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.556, 10.435], loss: 0.002360, mae: 0.052894, mean_q: 0.630025
 35654/100000: episode: 731, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 6.817, mean reward: 0.426 [0.346, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.428], loss: 0.002919, mae: 0.059402, mean_q: 0.633670
 35673/100000: episode: 732, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 9.719, mean reward: 0.512 [0.428, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.603], loss: 0.002717, mae: 0.056755, mean_q: 0.628389
 35702/100000: episode: 733, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 13.071, mean reward: 0.451 [0.224, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.425], loss: 0.002627, mae: 0.057218, mean_q: 0.628075
 35718/100000: episode: 734, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 5.517, mean reward: 0.345 [0.269, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.134, 10.401], loss: 0.002361, mae: 0.052852, mean_q: 0.621214
 35737/100000: episode: 735, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 8.305, mean reward: 0.437 [0.344, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.405, 10.486], loss: 0.002440, mae: 0.055874, mean_q: 0.631972
 35753/100000: episode: 736, duration: 0.100s, episode steps: 16, steps per second: 160, episode reward: 6.772, mean reward: 0.423 [0.274, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.348, 10.470], loss: 0.002498, mae: 0.056272, mean_q: 0.632052
 35776/100000: episode: 737, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 11.210, mean reward: 0.487 [0.408, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.582], loss: 0.002911, mae: 0.059720, mean_q: 0.626438
 35792/100000: episode: 738, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 6.598, mean reward: 0.412 [0.327, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-1.119, 10.497], loss: 0.002948, mae: 0.061796, mean_q: 0.640656
 35808/100000: episode: 739, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 7.972, mean reward: 0.498 [0.411, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-1.095, 10.471], loss: 0.002653, mae: 0.056591, mean_q: 0.646818
 35831/100000: episode: 740, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 9.026, mean reward: 0.392 [0.240, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.385], loss: 0.002762, mae: 0.056966, mean_q: 0.618563
 35850/100000: episode: 741, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 10.595, mean reward: 0.558 [0.423, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.094, 10.512], loss: 0.002381, mae: 0.054369, mean_q: 0.648180
 35866/100000: episode: 742, duration: 0.094s, episode steps: 16, steps per second: 169, episode reward: 8.354, mean reward: 0.522 [0.390, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-1.099, 10.533], loss: 0.002713, mae: 0.057180, mean_q: 0.636775
 35882/100000: episode: 743, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 7.005, mean reward: 0.438 [0.324, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.434], loss: 0.002804, mae: 0.059425, mean_q: 0.630109
 35911/100000: episode: 744, duration: 0.180s, episode steps: 29, steps per second: 161, episode reward: 13.549, mean reward: 0.467 [0.398, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.600, 10.567], loss: 0.002508, mae: 0.055673, mean_q: 0.640253
 35941/100000: episode: 745, duration: 0.171s, episode steps: 30, steps per second: 175, episode reward: 10.464, mean reward: 0.349 [0.212, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.035, 10.324], loss: 0.002646, mae: 0.056778, mean_q: 0.639743
 35957/100000: episode: 746, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 7.463, mean reward: 0.466 [0.370, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.043, 10.514], loss: 0.002044, mae: 0.049322, mean_q: 0.659096
 35973/100000: episode: 747, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 7.246, mean reward: 0.453 [0.357, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.056, 10.482], loss: 0.002553, mae: 0.055512, mean_q: 0.651293
 36003/100000: episode: 748, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 13.305, mean reward: 0.443 [0.364, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.175, 10.518], loss: 0.002828, mae: 0.059412, mean_q: 0.650491
 36019/100000: episode: 749, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 7.024, mean reward: 0.439 [0.277, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.428], loss: 0.002635, mae: 0.058733, mean_q: 0.642398
 36038/100000: episode: 750, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 8.599, mean reward: 0.453 [0.365, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.079, 10.584], loss: 0.002571, mae: 0.056328, mean_q: 0.651691
 36060/100000: episode: 751, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 9.058, mean reward: 0.412 [0.244, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.143, 10.308], loss: 0.002693, mae: 0.057731, mean_q: 0.637833
 36076/100000: episode: 752, duration: 0.090s, episode steps: 16, steps per second: 179, episode reward: 5.892, mean reward: 0.368 [0.228, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.437], loss: 0.002459, mae: 0.055490, mean_q: 0.663925
 36092/100000: episode: 753, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 5.988, mean reward: 0.374 [0.206, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.570, 10.410], loss: 0.002447, mae: 0.055265, mean_q: 0.668157
 36122/100000: episode: 754, duration: 0.178s, episode steps: 30, steps per second: 169, episode reward: 11.428, mean reward: 0.381 [0.278, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.489, 10.500], loss: 0.002874, mae: 0.058450, mean_q: 0.646457
 36144/100000: episode: 755, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 9.530, mean reward: 0.433 [0.280, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.280, 10.471], loss: 0.002968, mae: 0.060966, mean_q: 0.658377
 36160/100000: episode: 756, duration: 0.092s, episode steps: 16, steps per second: 175, episode reward: 6.601, mean reward: 0.413 [0.342, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.509], loss: 0.002736, mae: 0.059253, mean_q: 0.661477
 36179/100000: episode: 757, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 8.118, mean reward: 0.427 [0.335, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.852, 10.436], loss: 0.002784, mae: 0.058632, mean_q: 0.664242
 36198/100000: episode: 758, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 9.360, mean reward: 0.493 [0.380, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.171, 10.467], loss: 0.002644, mae: 0.059193, mean_q: 0.660453
 36217/100000: episode: 759, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 10.635, mean reward: 0.560 [0.452, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.687, 10.542], loss: 0.002604, mae: 0.057573, mean_q: 0.663768
 36246/100000: episode: 760, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 10.788, mean reward: 0.372 [0.227, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.109, 10.367], loss: 0.002704, mae: 0.057660, mean_q: 0.660171
 36265/100000: episode: 761, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 10.138, mean reward: 0.534 [0.487, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.684, 10.620], loss: 0.002263, mae: 0.053406, mean_q: 0.654523
 36287/100000: episode: 762, duration: 0.125s, episode steps: 22, steps per second: 177, episode reward: 8.643, mean reward: 0.393 [0.234, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.451], loss: 0.003121, mae: 0.062944, mean_q: 0.665741
 36310/100000: episode: 763, duration: 0.121s, episode steps: 23, steps per second: 189, episode reward: 10.677, mean reward: 0.464 [0.407, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.333, 10.459], loss: 0.002734, mae: 0.059143, mean_q: 0.665017
 36329/100000: episode: 764, duration: 0.127s, episode steps: 19, steps per second: 150, episode reward: 10.873, mean reward: 0.572 [0.489, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.090, 10.655], loss: 0.002531, mae: 0.055131, mean_q: 0.649129
 36359/100000: episode: 765, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 9.730, mean reward: 0.324 [0.180, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.954, 10.330], loss: 0.002889, mae: 0.059959, mean_q: 0.667140
 36382/100000: episode: 766, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 10.370, mean reward: 0.451 [0.371, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.615], loss: 0.002773, mae: 0.058626, mean_q: 0.669659
 36398/100000: episode: 767, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 6.541, mean reward: 0.409 [0.340, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.554, 10.424], loss: 0.002716, mae: 0.057543, mean_q: 0.672299
 36428/100000: episode: 768, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 15.952, mean reward: 0.532 [0.395, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.409, 10.565], loss: 0.002289, mae: 0.054312, mean_q: 0.679460
 36447/100000: episode: 769, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 10.201, mean reward: 0.537 [0.465, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.516, 10.577], loss: 0.002993, mae: 0.060889, mean_q: 0.687368
 36463/100000: episode: 770, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 7.129, mean reward: 0.446 [0.356, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.403, 10.479], loss: 0.003057, mae: 0.062247, mean_q: 0.675821
 36479/100000: episode: 771, duration: 0.080s, episode steps: 16, steps per second: 201, episode reward: 7.084, mean reward: 0.443 [0.369, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.375, 10.487], loss: 0.002614, mae: 0.056795, mean_q: 0.662826
 36502/100000: episode: 772, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 8.582, mean reward: 0.373 [0.224, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.701, 10.439], loss: 0.002310, mae: 0.054026, mean_q: 0.669112
 36524/100000: episode: 773, duration: 0.110s, episode steps: 22, steps per second: 199, episode reward: 10.979, mean reward: 0.499 [0.417, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.002, 10.473], loss: 0.002635, mae: 0.057204, mean_q: 0.683871
 36547/100000: episode: 774, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 10.348, mean reward: 0.450 [0.393, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.538, 10.544], loss: 0.002392, mae: 0.055078, mean_q: 0.680111
 36563/100000: episode: 775, duration: 0.089s, episode steps: 16, steps per second: 181, episode reward: 8.427, mean reward: 0.527 [0.450, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.888, 10.571], loss: 0.002715, mae: 0.058365, mean_q: 0.687475
 36579/100000: episode: 776, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 8.189, mean reward: 0.512 [0.419, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.498, 10.528], loss: 0.002421, mae: 0.054792, mean_q: 0.686401
 36609/100000: episode: 777, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 9.711, mean reward: 0.324 [0.216, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.035, 10.378], loss: 0.003178, mae: 0.062294, mean_q: 0.679622
 36638/100000: episode: 778, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 12.568, mean reward: 0.433 [0.297, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.742, 10.584], loss: 0.002521, mae: 0.055391, mean_q: 0.679518
 36668/100000: episode: 779, duration: 0.169s, episode steps: 30, steps per second: 178, episode reward: 13.528, mean reward: 0.451 [0.321, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.320, 10.423], loss: 0.002807, mae: 0.059251, mean_q: 0.689317
 36687/100000: episode: 780, duration: 0.111s, episode steps: 19, steps per second: 172, episode reward: 8.670, mean reward: 0.456 [0.295, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.394], loss: 0.002561, mae: 0.055433, mean_q: 0.688082
 36717/100000: episode: 781, duration: 0.175s, episode steps: 30, steps per second: 171, episode reward: 12.210, mean reward: 0.407 [0.226, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.405, 10.332], loss: 0.002234, mae: 0.052956, mean_q: 0.701055
 36747/100000: episode: 782, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 11.577, mean reward: 0.386 [0.282, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-1.244, 10.544], loss: 0.002144, mae: 0.051583, mean_q: 0.686672
 36777/100000: episode: 783, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 9.609, mean reward: 0.320 [0.123, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.254, 10.243], loss: 0.003117, mae: 0.061866, mean_q: 0.688644
 36796/100000: episode: 784, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 7.916, mean reward: 0.417 [0.224, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.137, 10.436], loss: 0.002915, mae: 0.060227, mean_q: 0.677691
 36812/100000: episode: 785, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 6.489, mean reward: 0.406 [0.322, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.495, 10.491], loss: 0.002299, mae: 0.055180, mean_q: 0.674712
 36834/100000: episode: 786, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 10.171, mean reward: 0.462 [0.403, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.576, 10.575], loss: 0.002410, mae: 0.054718, mean_q: 0.693619
 36857/100000: episode: 787, duration: 0.131s, episode steps: 23, steps per second: 175, episode reward: 9.363, mean reward: 0.407 [0.280, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.051, 10.391], loss: 0.002424, mae: 0.055398, mean_q: 0.697326
 36873/100000: episode: 788, duration: 0.087s, episode steps: 16, steps per second: 185, episode reward: 8.819, mean reward: 0.551 [0.395, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-1.411, 10.601], loss: 0.002431, mae: 0.055373, mean_q: 0.699925
 36902/100000: episode: 789, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 10.111, mean reward: 0.349 [0.145, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.322], loss: 0.002476, mae: 0.055459, mean_q: 0.697495
 36932/100000: episode: 790, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 15.663, mean reward: 0.522 [0.448, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.551, 10.540], loss: 0.002537, mae: 0.056483, mean_q: 0.701411
 36962/100000: episode: 791, duration: 0.166s, episode steps: 30, steps per second: 181, episode reward: 11.298, mean reward: 0.377 [0.296, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.035, 10.474], loss: 0.002554, mae: 0.056634, mean_q: 0.701828
 36992/100000: episode: 792, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 11.650, mean reward: 0.388 [0.314, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.161, 10.371], loss: 0.002645, mae: 0.058583, mean_q: 0.702008
 37015/100000: episode: 793, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 7.386, mean reward: 0.321 [0.145, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-1.345, 10.262], loss: 0.002306, mae: 0.054650, mean_q: 0.712706
 37031/100000: episode: 794, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 7.970, mean reward: 0.498 [0.434, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.428], loss: 0.002443, mae: 0.054306, mean_q: 0.693735
 37050/100000: episode: 795, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 8.791, mean reward: 0.463 [0.380, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.050, 10.529], loss: 0.002546, mae: 0.056663, mean_q: 0.691819
 37080/100000: episode: 796, duration: 0.173s, episode steps: 30, steps per second: 174, episode reward: 11.847, mean reward: 0.395 [0.296, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.553], loss: 0.002094, mae: 0.050851, mean_q: 0.708336
 37110/100000: episode: 797, duration: 0.169s, episode steps: 30, steps per second: 178, episode reward: 11.355, mean reward: 0.379 [0.291, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.304, 10.475], loss: 0.002586, mae: 0.056896, mean_q: 0.704862
[Info] FALSIFICATION!
 37114/100000: episode: 798, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 11.909, mean reward: 2.977 [0.606, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.170, 9.935], loss: 0.002305, mae: 0.054055, mean_q: 0.704995
 37214/100000: episode: 799, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -16.790, mean reward: -0.168 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.265, 10.165], loss: 0.015933, mae: 0.067440, mean_q: 0.701168
 37314/100000: episode: 800, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -13.999, mean reward: -0.140 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.296, 10.367], loss: 0.028375, mae: 0.071812, mean_q: 0.670477
 37414/100000: episode: 801, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: -17.401, mean reward: -0.174 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.845, 10.213], loss: 0.003617, mae: 0.060081, mean_q: 0.658494
 37514/100000: episode: 802, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: -17.617, mean reward: -0.176 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.935, 10.098], loss: 0.003356, mae: 0.058884, mean_q: 0.641569
 37614/100000: episode: 803, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.779, mean reward: -0.188 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.650, 10.207], loss: 0.016219, mae: 0.064907, mean_q: 0.616057
 37714/100000: episode: 804, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -19.940, mean reward: -0.199 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.805, 10.098], loss: 0.002840, mae: 0.056417, mean_q: 0.592876
 37814/100000: episode: 805, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: -16.833, mean reward: -0.168 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.565, 10.189], loss: 0.003079, mae: 0.057586, mean_q: 0.585716
 37914/100000: episode: 806, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: -14.133, mean reward: -0.141 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.895, 10.292], loss: 0.003900, mae: 0.062600, mean_q: 0.561663
 38014/100000: episode: 807, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: -16.632, mean reward: -0.166 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.430, 10.098], loss: 0.002841, mae: 0.057184, mean_q: 0.540519
 38114/100000: episode: 808, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -19.279, mean reward: -0.193 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.223, 10.117], loss: 0.002956, mae: 0.056194, mean_q: 0.518067
 38214/100000: episode: 809, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: -18.939, mean reward: -0.189 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.952, 10.393], loss: 0.003959, mae: 0.059029, mean_q: 0.515551
 38314/100000: episode: 810, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -20.392, mean reward: -0.204 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.720, 10.098], loss: 0.002706, mae: 0.056276, mean_q: 0.461435
 38414/100000: episode: 811, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: -17.895, mean reward: -0.179 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.926, 10.098], loss: 0.002642, mae: 0.056298, mean_q: 0.475660
 38514/100000: episode: 812, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: -20.583, mean reward: -0.206 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.525, 10.153], loss: 0.002413, mae: 0.053612, mean_q: 0.444853
 38614/100000: episode: 813, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -10.751, mean reward: -0.108 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.532, 10.420], loss: 0.002364, mae: 0.053861, mean_q: 0.423992
 38714/100000: episode: 814, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -19.117, mean reward: -0.191 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.021, 10.098], loss: 0.002373, mae: 0.052752, mean_q: 0.386421
 38814/100000: episode: 815, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.342, mean reward: -0.173 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.921, 10.170], loss: 0.002569, mae: 0.055033, mean_q: 0.391971
 38914/100000: episode: 816, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -15.020, mean reward: -0.150 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.067, 10.427], loss: 0.002512, mae: 0.053929, mean_q: 0.337659
 39014/100000: episode: 817, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -14.896, mean reward: -0.149 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.470, 10.098], loss: 0.015476, mae: 0.061493, mean_q: 0.330174
 39114/100000: episode: 818, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -18.990, mean reward: -0.190 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.584, 10.268], loss: 0.002438, mae: 0.053324, mean_q: 0.315613
 39214/100000: episode: 819, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -20.739, mean reward: -0.207 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.733, 10.098], loss: 0.002343, mae: 0.052031, mean_q: 0.305692
 39314/100000: episode: 820, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -20.174, mean reward: -0.202 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.348, 10.098], loss: 0.015895, mae: 0.064142, mean_q: 0.281459
 39414/100000: episode: 821, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -14.666, mean reward: -0.147 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.187, 10.098], loss: 0.002499, mae: 0.053745, mean_q: 0.256595
 39514/100000: episode: 822, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -16.960, mean reward: -0.170 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.186, 10.098], loss: 0.003241, mae: 0.060314, mean_q: 0.246948
 39614/100000: episode: 823, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -19.127, mean reward: -0.191 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.272, 10.098], loss: 0.002409, mae: 0.052207, mean_q: 0.218304
 39714/100000: episode: 824, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -16.398, mean reward: -0.164 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.431, 10.098], loss: 0.002424, mae: 0.052375, mean_q: 0.224989
 39814/100000: episode: 825, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -19.775, mean reward: -0.198 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.007, 10.124], loss: 0.002665, mae: 0.056027, mean_q: 0.208971
 39914/100000: episode: 826, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.765, mean reward: -0.158 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.559, 10.101], loss: 0.002690, mae: 0.054708, mean_q: 0.178736
 40014/100000: episode: 827, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.566, mean reward: -0.146 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.045, 10.405], loss: 0.002666, mae: 0.055115, mean_q: 0.118674
 40114/100000: episode: 828, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.527, mean reward: -0.185 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.845, 10.098], loss: 0.015775, mae: 0.062795, mean_q: 0.127257
 40214/100000: episode: 829, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.018, mean reward: -0.180 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.123, 10.186], loss: 0.002607, mae: 0.055626, mean_q: 0.094707
 40314/100000: episode: 830, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.565, mean reward: -0.186 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.629, 10.247], loss: 0.015467, mae: 0.059838, mean_q: 0.097506
 40414/100000: episode: 831, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.418, mean reward: -0.184 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.105, 10.098], loss: 0.017274, mae: 0.076604, mean_q: 0.059220
 40514/100000: episode: 832, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.214, mean reward: -0.142 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.847, 10.289], loss: 0.017078, mae: 0.071745, mean_q: 0.041527
 40614/100000: episode: 833, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.881, mean reward: -0.159 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.002, 10.098], loss: 0.002370, mae: 0.051656, mean_q: 0.019994
 40714/100000: episode: 834, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.854, mean reward: -0.169 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.985, 10.098], loss: 0.015361, mae: 0.059115, mean_q: -0.006947
 40814/100000: episode: 835, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.915, mean reward: -0.169 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.810, 10.175], loss: 0.028377, mae: 0.065396, mean_q: -0.011965
 40914/100000: episode: 836, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -11.520, mean reward: -0.115 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-0.642, 10.098], loss: 0.015555, mae: 0.062495, mean_q: -0.056115
 41014/100000: episode: 837, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.977, mean reward: -0.170 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.584, 10.099], loss: 0.002377, mae: 0.050901, mean_q: -0.063928
 41114/100000: episode: 838, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -19.074, mean reward: -0.191 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.372, 10.098], loss: 0.015267, mae: 0.057367, mean_q: -0.124130
 41214/100000: episode: 839, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -13.152, mean reward: -0.132 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.024, 10.293], loss: 0.002335, mae: 0.049175, mean_q: -0.143919
 41314/100000: episode: 840, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.013, mean reward: -0.180 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.815, 10.234], loss: 0.002349, mae: 0.050204, mean_q: -0.116955
 41414/100000: episode: 841, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.138, mean reward: -0.161 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.147, 10.174], loss: 0.002396, mae: 0.049636, mean_q: -0.208842
 41514/100000: episode: 842, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.361, mean reward: -0.174 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.795, 10.117], loss: 0.002408, mae: 0.049080, mean_q: -0.208627
 41614/100000: episode: 843, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.834, mean reward: -0.188 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.608, 10.194], loss: 0.002510, mae: 0.050872, mean_q: -0.206543
 41714/100000: episode: 844, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.441, mean reward: -0.174 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.867, 10.158], loss: 0.002928, mae: 0.056262, mean_q: -0.239430
 41814/100000: episode: 845, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.059, mean reward: -0.181 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.957, 10.168], loss: 0.002383, mae: 0.049680, mean_q: -0.285891
 41914/100000: episode: 846, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.066, mean reward: -0.141 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.466, 10.238], loss: 0.042243, mae: 0.076808, mean_q: -0.287344
 42014/100000: episode: 847, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.251, mean reward: -0.173 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.474, 10.155], loss: 0.002702, mae: 0.052789, mean_q: -0.290135
 42114/100000: episode: 848, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.260, mean reward: -0.173 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.199, 10.219], loss: 0.002436, mae: 0.049814, mean_q: -0.307856
 42214/100000: episode: 849, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.330, mean reward: -0.193 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.033, 10.098], loss: 0.002299, mae: 0.049051, mean_q: -0.317446
 42314/100000: episode: 850, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -16.116, mean reward: -0.161 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.467, 10.259], loss: 0.002492, mae: 0.049682, mean_q: -0.319629
 42414/100000: episode: 851, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.579, mean reward: -0.166 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.539, 10.098], loss: 0.002495, mae: 0.049989, mean_q: -0.324844
 42514/100000: episode: 852, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -11.987, mean reward: -0.120 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.713, 10.098], loss: 0.002527, mae: 0.050511, mean_q: -0.346487
 42614/100000: episode: 853, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.138, mean reward: -0.181 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.025, 10.098], loss: 0.002523, mae: 0.049663, mean_q: -0.319946
 42714/100000: episode: 854, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.149, mean reward: -0.181 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.928, 10.098], loss: 0.002681, mae: 0.052775, mean_q: -0.320836
 42814/100000: episode: 855, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.747, mean reward: -0.157 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.429, 10.148], loss: 0.002536, mae: 0.050547, mean_q: -0.314776
 42914/100000: episode: 856, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.408, mean reward: -0.194 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.484, 10.141], loss: 0.002513, mae: 0.051042, mean_q: -0.332853
 43014/100000: episode: 857, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.248, mean reward: -0.162 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.655, 10.246], loss: 0.002521, mae: 0.050970, mean_q: -0.329338
 43114/100000: episode: 858, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -18.976, mean reward: -0.190 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.304, 10.315], loss: 0.002830, mae: 0.056486, mean_q: -0.309912
 43214/100000: episode: 859, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.839, mean reward: -0.188 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.764, 10.131], loss: 0.002565, mae: 0.050759, mean_q: -0.322057
 43314/100000: episode: 860, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.000, mean reward: -0.180 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.415, 10.326], loss: 0.002406, mae: 0.048907, mean_q: -0.338804
 43414/100000: episode: 861, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.871, mean reward: -0.179 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.598, 10.098], loss: 0.002451, mae: 0.049487, mean_q: -0.320418
 43514/100000: episode: 862, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -20.984, mean reward: -0.210 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.027, 10.246], loss: 0.002535, mae: 0.050353, mean_q: -0.342816
 43614/100000: episode: 863, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -17.305, mean reward: -0.173 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.402, 10.098], loss: 0.002563, mae: 0.050277, mean_q: -0.345885
 43714/100000: episode: 864, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.192, mean reward: -0.162 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.163, 10.253], loss: 0.002517, mae: 0.049772, mean_q: -0.325608
 43814/100000: episode: 865, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -17.206, mean reward: -0.172 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.852, 10.121], loss: 0.002526, mae: 0.050262, mean_q: -0.322791
 43914/100000: episode: 866, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.039, mean reward: -0.190 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.616, 10.098], loss: 0.002692, mae: 0.052187, mean_q: -0.335649
 44014/100000: episode: 867, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.895, mean reward: -0.179 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.415, 10.098], loss: 0.002760, mae: 0.052946, mean_q: -0.343930
 44114/100000: episode: 868, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -15.942, mean reward: -0.159 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.613, 10.253], loss: 0.002687, mae: 0.052804, mean_q: -0.284197
 44214/100000: episode: 869, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.131, mean reward: -0.191 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.886, 10.098], loss: 0.002685, mae: 0.051785, mean_q: -0.307344
 44314/100000: episode: 870, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -8.389, mean reward: -0.084 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.742, 10.471], loss: 0.002757, mae: 0.053014, mean_q: -0.325382
 44414/100000: episode: 871, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -10.772, mean reward: -0.108 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.023, 10.505], loss: 0.004137, mae: 0.064860, mean_q: -0.311263
 44514/100000: episode: 872, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -20.127, mean reward: -0.201 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.736, 10.101], loss: 0.002467, mae: 0.050992, mean_q: -0.305720
 44614/100000: episode: 873, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -12.535, mean reward: -0.125 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.369, 10.329], loss: 0.002734, mae: 0.053087, mean_q: -0.311262
 44714/100000: episode: 874, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.234, mean reward: -0.172 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.340, 10.098], loss: 0.002321, mae: 0.048173, mean_q: -0.325656
 44814/100000: episode: 875, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.803, mean reward: -0.198 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.437, 10.098], loss: 0.002559, mae: 0.051942, mean_q: -0.315789
 44914/100000: episode: 876, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.623, mean reward: -0.176 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.914, 10.098], loss: 0.002664, mae: 0.052057, mean_q: -0.327767
 45014/100000: episode: 877, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.794, mean reward: -0.168 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.442, 10.098], loss: 0.002355, mae: 0.048984, mean_q: -0.345267
 45114/100000: episode: 878, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.125, mean reward: -0.141 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.294, 10.098], loss: 0.002370, mae: 0.049276, mean_q: -0.327817
 45214/100000: episode: 879, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.748, mean reward: -0.197 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.834, 10.133], loss: 0.002449, mae: 0.049515, mean_q: -0.329484
 45314/100000: episode: 880, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -5.344, mean reward: -0.053 [-1.000, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.996, 10.303], loss: 0.002531, mae: 0.050649, mean_q: -0.307872
 45414/100000: episode: 881, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.626, mean reward: -0.156 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.461, 10.315], loss: 0.002350, mae: 0.049691, mean_q: -0.307315
 45514/100000: episode: 882, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.127, mean reward: -0.181 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.399, 10.098], loss: 0.002577, mae: 0.051147, mean_q: -0.304585
 45614/100000: episode: 883, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.230, mean reward: -0.172 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.454, 10.239], loss: 0.002462, mae: 0.050096, mean_q: -0.326825
 45714/100000: episode: 884, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -14.967, mean reward: -0.150 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.893, 10.098], loss: 0.002504, mae: 0.050411, mean_q: -0.311710
 45814/100000: episode: 885, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.090, mean reward: -0.181 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.299, 10.098], loss: 0.002773, mae: 0.053611, mean_q: -0.311362
 45914/100000: episode: 886, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -15.282, mean reward: -0.153 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.344, 10.265], loss: 0.002495, mae: 0.049949, mean_q: -0.351650
 46014/100000: episode: 887, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.317, mean reward: -0.183 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.076, 10.140], loss: 0.002660, mae: 0.052169, mean_q: -0.321222
 46114/100000: episode: 888, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.618, mean reward: -0.126 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.180, 10.433], loss: 0.002551, mae: 0.051065, mean_q: -0.329279
 46214/100000: episode: 889, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.754, mean reward: -0.178 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.154, 10.098], loss: 0.005511, mae: 0.071489, mean_q: -0.305040
 46314/100000: episode: 890, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.107, mean reward: -0.161 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.474, 10.333], loss: 0.002504, mae: 0.050883, mean_q: -0.318698
 46414/100000: episode: 891, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.610, mean reward: -0.166 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.767, 10.237], loss: 0.002655, mae: 0.052323, mean_q: -0.310635
 46514/100000: episode: 892, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.425, mean reward: -0.164 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.761, 10.098], loss: 0.002487, mae: 0.050407, mean_q: -0.321500
 46614/100000: episode: 893, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.990, mean reward: -0.170 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.657, 10.098], loss: 0.002434, mae: 0.050154, mean_q: -0.301256
 46714/100000: episode: 894, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.323, mean reward: -0.183 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.724, 10.176], loss: 0.002514, mae: 0.050846, mean_q: -0.282966
 46814/100000: episode: 895, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.460, mean reward: -0.185 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.797, 10.203], loss: 0.002775, mae: 0.054102, mean_q: -0.301102
 46914/100000: episode: 896, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.189, mean reward: -0.192 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.470, 10.106], loss: 0.002593, mae: 0.051940, mean_q: -0.298367
 47014/100000: episode: 897, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.067, mean reward: -0.191 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.639, 10.190], loss: 0.002652, mae: 0.051596, mean_q: -0.329630
[Info] 100-TH LEVEL FOUND: 0.6477323174476624, Considering 10/90 traces
 47114/100000: episode: 898, duration: 4.451s, episode steps: 100, steps per second: 22, episode reward: -12.955, mean reward: -0.130 [-1.000, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.745, 10.098], loss: 0.002382, mae: 0.049880, mean_q: -0.304144
 47137/100000: episode: 899, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 7.295, mean reward: 0.317 [0.176, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.278, 10.322], loss: 0.002302, mae: 0.048930, mean_q: -0.259609
 47145/100000: episode: 900, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 3.248, mean reward: 0.406 [0.273, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.225, 10.300], loss: 0.001895, mae: 0.044890, mean_q: -0.328287
 47187/100000: episode: 901, duration: 0.226s, episode steps: 42, steps per second: 186, episode reward: 13.847, mean reward: 0.330 [0.230, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.176, 10.477], loss: 0.002593, mae: 0.050644, mean_q: -0.293718
 47201/100000: episode: 902, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 5.107, mean reward: 0.365 [0.283, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.801, 10.482], loss: 0.002378, mae: 0.048440, mean_q: -0.306417
 47234/100000: episode: 903, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 8.718, mean reward: 0.264 [0.058, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.942, 10.557], loss: 0.002425, mae: 0.049673, mean_q: -0.280035
 47258/100000: episode: 904, duration: 0.139s, episode steps: 24, steps per second: 172, episode reward: 7.861, mean reward: 0.328 [0.197, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.764, 10.100], loss: 0.002686, mae: 0.052038, mean_q: -0.299014
 47281/100000: episode: 905, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 6.479, mean reward: 0.282 [0.188, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.049, 10.329], loss: 0.002297, mae: 0.048673, mean_q: -0.271960
 47305/100000: episode: 906, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 11.219, mean reward: 0.467 [0.239, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.405, 10.100], loss: 0.002738, mae: 0.053296, mean_q: -0.274217
 47325/100000: episode: 907, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 8.976, mean reward: 0.449 [0.395, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.278, 10.100], loss: 0.002991, mae: 0.056014, mean_q: -0.259631
 47350/100000: episode: 908, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 8.805, mean reward: 0.352 [0.238, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.080, 10.514], loss: 0.002293, mae: 0.049200, mean_q: -0.300944
 47383/100000: episode: 909, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 7.383, mean reward: 0.224 [0.054, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.219, 10.100], loss: 0.002676, mae: 0.052203, mean_q: -0.252257
 47406/100000: episode: 910, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 8.318, mean reward: 0.362 [0.147, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.512, 10.383], loss: 0.002429, mae: 0.050700, mean_q: -0.287143
 47448/100000: episode: 911, duration: 0.231s, episode steps: 42, steps per second: 182, episode reward: 16.666, mean reward: 0.397 [0.211, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.468, 10.445], loss: 0.002542, mae: 0.051963, mean_q: -0.263363
 47481/100000: episode: 912, duration: 0.188s, episode steps: 33, steps per second: 175, episode reward: 7.746, mean reward: 0.235 [0.091, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.487, 10.205], loss: 0.002853, mae: 0.055876, mean_q: -0.210464
 47506/100000: episode: 913, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 8.233, mean reward: 0.329 [0.190, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.146, 10.280], loss: 0.002877, mae: 0.053857, mean_q: -0.234793
 47530/100000: episode: 914, duration: 0.127s, episode steps: 24, steps per second: 188, episode reward: 8.175, mean reward: 0.341 [0.274, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-1.270, 10.100], loss: 0.002505, mae: 0.051067, mean_q: -0.272303
 47550/100000: episode: 915, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 7.797, mean reward: 0.390 [0.301, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.588, 10.100], loss: 0.002597, mae: 0.052165, mean_q: -0.175183
 47558/100000: episode: 916, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 2.609, mean reward: 0.326 [0.295, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.407], loss: 0.002936, mae: 0.057503, mean_q: -0.198074
 47606/100000: episode: 917, duration: 0.261s, episode steps: 48, steps per second: 184, episode reward: 13.449, mean reward: 0.280 [0.121, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.345, 10.383], loss: 0.002583, mae: 0.052074, mean_q: -0.203549
 47630/100000: episode: 918, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 9.479, mean reward: 0.395 [0.295, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.530, 10.100], loss: 0.002811, mae: 0.055453, mean_q: -0.204389
 47663/100000: episode: 919, duration: 0.177s, episode steps: 33, steps per second: 187, episode reward: 9.007, mean reward: 0.273 [0.183, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.390, 10.385], loss: 0.002394, mae: 0.049503, mean_q: -0.191909
 47705/100000: episode: 920, duration: 0.238s, episode steps: 42, steps per second: 177, episode reward: 10.845, mean reward: 0.258 [0.066, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.434, 10.232], loss: 0.002749, mae: 0.054491, mean_q: -0.202462
 47741/100000: episode: 921, duration: 0.187s, episode steps: 36, steps per second: 192, episode reward: 13.130, mean reward: 0.365 [0.223, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-1.002, 10.368], loss: 0.003119, mae: 0.059508, mean_q: -0.223692
 47755/100000: episode: 922, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 4.716, mean reward: 0.337 [0.196, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.370, 10.384], loss: 0.002782, mae: 0.054272, mean_q: -0.236125
 47779/100000: episode: 923, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 10.794, mean reward: 0.450 [0.249, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.500, 10.100], loss: 0.002614, mae: 0.056116, mean_q: -0.140257
 47815/100000: episode: 924, duration: 0.184s, episode steps: 36, steps per second: 195, episode reward: 11.894, mean reward: 0.330 [0.226, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.975, 10.415], loss: 0.002619, mae: 0.053491, mean_q: -0.191791
 47857/100000: episode: 925, duration: 0.225s, episode steps: 42, steps per second: 186, episode reward: 11.115, mean reward: 0.265 [0.067, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.306, 10.306], loss: 0.002766, mae: 0.053276, mean_q: -0.170576
 47880/100000: episode: 926, duration: 0.123s, episode steps: 23, steps per second: 188, episode reward: 7.217, mean reward: 0.314 [0.269, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.466], loss: 0.002435, mae: 0.049556, mean_q: -0.244587
 47894/100000: episode: 927, duration: 0.095s, episode steps: 14, steps per second: 147, episode reward: 4.791, mean reward: 0.342 [0.277, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.415], loss: 0.002634, mae: 0.053843, mean_q: -0.142552
 47930/100000: episode: 928, duration: 0.187s, episode steps: 36, steps per second: 193, episode reward: 12.467, mean reward: 0.346 [0.208, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.065, 10.530], loss: 0.002804, mae: 0.053831, mean_q: -0.145470
 47938/100000: episode: 929, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 3.205, mean reward: 0.401 [0.351, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.564], loss: 0.002671, mae: 0.052959, mean_q: -0.199729
 47958/100000: episode: 930, duration: 0.127s, episode steps: 20, steps per second: 157, episode reward: 5.961, mean reward: 0.298 [0.199, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.436, 10.100], loss: 0.002584, mae: 0.053024, mean_q: -0.162662
 47994/100000: episode: 931, duration: 0.199s, episode steps: 36, steps per second: 181, episode reward: 12.444, mean reward: 0.346 [0.238, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.912, 10.371], loss: 0.002743, mae: 0.053259, mean_q: -0.183638
 48042/100000: episode: 932, duration: 0.268s, episode steps: 48, steps per second: 179, episode reward: 12.327, mean reward: 0.257 [0.054, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-1.527, 10.157], loss: 0.002758, mae: 0.054663, mean_q: -0.203381
 48056/100000: episode: 933, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 2.862, mean reward: 0.204 [0.045, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.191], loss: 0.002699, mae: 0.052138, mean_q: -0.152998
 48098/100000: episode: 934, duration: 0.229s, episode steps: 42, steps per second: 184, episode reward: 9.939, mean reward: 0.237 [0.015, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-1.018, 10.229], loss: 0.002775, mae: 0.054157, mean_q: -0.136517
 48134/100000: episode: 935, duration: 0.203s, episode steps: 36, steps per second: 177, episode reward: 13.200, mean reward: 0.367 [0.272, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.618, 10.371], loss: 0.002729, mae: 0.054205, mean_q: -0.128944
 48170/100000: episode: 936, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 8.018, mean reward: 0.223 [0.066, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.881, 10.187], loss: 0.002856, mae: 0.055585, mean_q: -0.126414
 48195/100000: episode: 937, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 5.132, mean reward: 0.205 [0.025, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.467, 10.115], loss: 0.002828, mae: 0.054447, mean_q: -0.144080
 48220/100000: episode: 938, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 6.362, mean reward: 0.254 [0.122, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.723, 10.358], loss: 0.002743, mae: 0.054638, mean_q: -0.110159
 48262/100000: episode: 939, duration: 0.222s, episode steps: 42, steps per second: 189, episode reward: 7.791, mean reward: 0.186 [0.004, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.635, 10.120], loss: 0.002636, mae: 0.052611, mean_q: -0.111505
 48298/100000: episode: 940, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 11.652, mean reward: 0.324 [0.162, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.486, 10.301], loss: 0.002542, mae: 0.052497, mean_q: -0.089057
 48321/100000: episode: 941, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 4.566, mean reward: 0.199 [0.044, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.220, 10.129], loss: 0.002957, mae: 0.056151, mean_q: -0.096183
 48329/100000: episode: 942, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 2.724, mean reward: 0.341 [0.246, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.469, 10.348], loss: 0.002987, mae: 0.057184, mean_q: -0.115160
 48377/100000: episode: 943, duration: 0.243s, episode steps: 48, steps per second: 198, episode reward: 13.697, mean reward: 0.285 [0.147, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.772, 10.248], loss: 0.002668, mae: 0.053367, mean_q: -0.089794
 48410/100000: episode: 944, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 5.333, mean reward: 0.162 [0.011, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.671, 10.100], loss: 0.002914, mae: 0.056476, mean_q: -0.082338
 48443/100000: episode: 945, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 11.354, mean reward: 0.344 [0.272, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.338, 10.563], loss: 0.002786, mae: 0.055697, mean_q: -0.057373
 48451/100000: episode: 946, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 3.056, mean reward: 0.382 [0.336, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.484], loss: 0.003119, mae: 0.058089, mean_q: 0.085056
 48499/100000: episode: 947, duration: 0.255s, episode steps: 48, steps per second: 188, episode reward: 12.410, mean reward: 0.259 [0.109, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.384, 10.310], loss: 0.002697, mae: 0.053521, mean_q: -0.032786
 48541/100000: episode: 948, duration: 0.231s, episode steps: 42, steps per second: 182, episode reward: 8.108, mean reward: 0.193 [0.030, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.170, 10.192], loss: 0.002867, mae: 0.055678, mean_q: -0.039106
 48549/100000: episode: 949, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 2.304, mean reward: 0.288 [0.234, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.254], loss: 0.002575, mae: 0.055286, mean_q: -0.040692
 48573/100000: episode: 950, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 9.754, mean reward: 0.406 [0.294, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.097, 10.100], loss: 0.002726, mae: 0.054764, mean_q: 0.020644
 48581/100000: episode: 951, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 2.429, mean reward: 0.304 [0.280, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.363], loss: 0.002834, mae: 0.054297, mean_q: -0.096720
 48623/100000: episode: 952, duration: 0.222s, episode steps: 42, steps per second: 189, episode reward: 9.550, mean reward: 0.227 [0.123, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.180, 10.237], loss: 0.002880, mae: 0.055466, mean_q: -0.051854
 48631/100000: episode: 953, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 2.944, mean reward: 0.368 [0.311, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.456], loss: 0.003369, mae: 0.059761, mean_q: -0.038825
 48673/100000: episode: 954, duration: 0.235s, episode steps: 42, steps per second: 179, episode reward: 14.607, mean reward: 0.348 [0.170, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.161, 10.430], loss: 0.003368, mae: 0.060339, mean_q: -0.046397
 48715/100000: episode: 955, duration: 0.223s, episode steps: 42, steps per second: 188, episode reward: 14.083, mean reward: 0.335 [0.095, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.393, 10.214], loss: 0.002794, mae: 0.055065, mean_q: -0.041754
 48739/100000: episode: 956, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 9.701, mean reward: 0.404 [0.285, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.566, 10.100], loss: 0.003089, mae: 0.058665, mean_q: -0.009497
 48781/100000: episode: 957, duration: 0.234s, episode steps: 42, steps per second: 179, episode reward: 9.950, mean reward: 0.237 [0.052, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.675, 10.100], loss: 0.002846, mae: 0.055456, mean_q: 0.008445
 48829/100000: episode: 958, duration: 0.254s, episode steps: 48, steps per second: 189, episode reward: 14.571, mean reward: 0.304 [0.181, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.896, 10.310], loss: 0.002726, mae: 0.054280, mean_q: -0.010202
 48853/100000: episode: 959, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 10.185, mean reward: 0.424 [0.336, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.510, 10.100], loss: 0.002755, mae: 0.054546, mean_q: 0.009497
 48873/100000: episode: 960, duration: 0.108s, episode steps: 20, steps per second: 184, episode reward: 8.157, mean reward: 0.408 [0.313, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.237, 10.100], loss: 0.002713, mae: 0.056742, mean_q: 0.073274
 48915/100000: episode: 961, duration: 0.225s, episode steps: 42, steps per second: 187, episode reward: 15.149, mean reward: 0.361 [0.216, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-1.450, 10.441], loss: 0.003232, mae: 0.059310, mean_q: 0.039674
 48929/100000: episode: 962, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 4.630, mean reward: 0.331 [0.293, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.543, 10.442], loss: 0.002878, mae: 0.056436, mean_q: -0.006934
 48965/100000: episode: 963, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 13.420, mean reward: 0.373 [0.240, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.132, 10.321], loss: 0.003005, mae: 0.057628, mean_q: 0.000255
 48979/100000: episode: 964, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 2.814, mean reward: 0.201 [0.065, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.156], loss: 0.002596, mae: 0.053248, mean_q: -0.012384
 48987/100000: episode: 965, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 2.887, mean reward: 0.361 [0.295, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.425], loss: 0.002989, mae: 0.056575, mean_q: 0.022095
 49012/100000: episode: 966, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 5.950, mean reward: 0.238 [0.056, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.162], loss: 0.002615, mae: 0.055402, mean_q: 0.068532
 49037/100000: episode: 967, duration: 0.138s, episode steps: 25, steps per second: 182, episode reward: 6.721, mean reward: 0.269 [0.081, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.049, 10.240], loss: 0.002624, mae: 0.053797, mean_q: 0.029454
 49045/100000: episode: 968, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 2.341, mean reward: 0.293 [0.196, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.397], loss: 0.002958, mae: 0.057790, mean_q: 0.150367
 49068/100000: episode: 969, duration: 0.134s, episode steps: 23, steps per second: 172, episode reward: 6.538, mean reward: 0.284 [0.210, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.390], loss: 0.005329, mae: 0.072739, mean_q: 0.076373
 49116/100000: episode: 970, duration: 0.249s, episode steps: 48, steps per second: 193, episode reward: 13.665, mean reward: 0.285 [0.170, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.764, 10.295], loss: 0.006101, mae: 0.072878, mean_q: 0.054338
 49140/100000: episode: 971, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 10.867, mean reward: 0.453 [0.267, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.196, 10.100], loss: 0.003071, mae: 0.060110, mean_q: 0.034627
 49148/100000: episode: 972, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 2.797, mean reward: 0.350 [0.296, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.415], loss: 0.002889, mae: 0.062995, mean_q: 0.064101
 49173/100000: episode: 973, duration: 0.160s, episode steps: 25, steps per second: 156, episode reward: 8.126, mean reward: 0.325 [0.244, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.416], loss: 0.003495, mae: 0.063129, mean_q: 0.152633
 49193/100000: episode: 974, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 8.569, mean reward: 0.428 [0.340, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.366, 10.100], loss: 0.002826, mae: 0.057837, mean_q: 0.174751
 49217/100000: episode: 975, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 8.192, mean reward: 0.341 [0.173, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.401, 10.100], loss: 0.002531, mae: 0.052045, mean_q: 0.039666
 49240/100000: episode: 976, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 3.791, mean reward: 0.165 [0.020, 0.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.140], loss: 0.002370, mae: 0.050653, mean_q: 0.043224
 49264/100000: episode: 977, duration: 0.125s, episode steps: 24, steps per second: 191, episode reward: 8.453, mean reward: 0.352 [0.258, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.310, 10.100], loss: 0.002721, mae: 0.055078, mean_q: 0.122260
 49300/100000: episode: 978, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 13.239, mean reward: 0.368 [0.257, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.776, 10.514], loss: 0.002515, mae: 0.052354, mean_q: 0.074976
 49320/100000: episode: 979, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 7.541, mean reward: 0.377 [0.294, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.154, 10.100], loss: 0.002794, mae: 0.055438, mean_q: 0.088085
 49340/100000: episode: 980, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 8.674, mean reward: 0.434 [0.342, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.345, 10.100], loss: 0.003324, mae: 0.062514, mean_q: 0.134461
 49365/100000: episode: 981, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 6.059, mean reward: 0.242 [0.099, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.305], loss: 0.002742, mae: 0.054407, mean_q: 0.112834
 49379/100000: episode: 982, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 5.406, mean reward: 0.386 [0.271, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.856, 10.577], loss: 0.002828, mae: 0.056848, mean_q: 0.135855
 49404/100000: episode: 983, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 7.113, mean reward: 0.285 [0.076, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.767, 10.268], loss: 0.002828, mae: 0.057968, mean_q: 0.142978
 49427/100000: episode: 984, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 8.225, mean reward: 0.358 [0.223, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-1.392, 10.383], loss: 0.002641, mae: 0.055187, mean_q: 0.136101
 49475/100000: episode: 985, duration: 0.265s, episode steps: 48, steps per second: 181, episode reward: 11.184, mean reward: 0.233 [0.022, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.458, 10.190], loss: 0.002578, mae: 0.054825, mean_q: 0.130132
 49483/100000: episode: 986, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 3.357, mean reward: 0.420 [0.367, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.434], loss: 0.002758, mae: 0.056126, mean_q: 0.043612
 49508/100000: episode: 987, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 7.172, mean reward: 0.287 [0.217, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.080, 10.410], loss: 0.002870, mae: 0.056540, mean_q: 0.063876
[Info] 200-TH LEVEL FOUND: 0.9095515012741089, Considering 10/90 traces
 49533/100000: episode: 988, duration: 4.075s, episode steps: 25, steps per second: 6, episode reward: 7.804, mean reward: 0.312 [0.171, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.570, 10.348], loss: 0.002457, mae: 0.052851, mean_q: 0.145970
 49563/100000: episode: 989, duration: 0.169s, episode steps: 30, steps per second: 177, episode reward: 8.815, mean reward: 0.294 [0.190, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.127, 10.273], loss: 0.002816, mae: 0.055095, mean_q: 0.144859
 49582/100000: episode: 990, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 10.570, mean reward: 0.556 [0.432, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.390, 10.100], loss: 0.002790, mae: 0.058834, mean_q: 0.151482
 49598/100000: episode: 991, duration: 0.089s, episode steps: 16, steps per second: 181, episode reward: 7.619, mean reward: 0.476 [0.327, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.244, 10.100], loss: 0.002722, mae: 0.057658, mean_q: 0.181105
 49608/100000: episode: 992, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 5.163, mean reward: 0.516 [0.440, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.516, 10.100], loss: 0.003085, mae: 0.060528, mean_q: 0.181917
 49627/100000: episode: 993, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 8.174, mean reward: 0.430 [0.358, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.421, 10.100], loss: 0.002568, mae: 0.053931, mean_q: 0.160301
 49641/100000: episode: 994, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 5.745, mean reward: 0.410 [0.337, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.555, 10.100], loss: 0.002486, mae: 0.054842, mean_q: 0.228567
 49667/100000: episode: 995, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 9.276, mean reward: 0.357 [0.254, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.625, 10.463], loss: 0.002764, mae: 0.056254, mean_q: 0.218318
 49675/100000: episode: 996, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 3.979, mean reward: 0.497 [0.452, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.412, 10.100], loss: 0.002336, mae: 0.052273, mean_q: 0.188031
 49693/100000: episode: 997, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 7.505, mean reward: 0.417 [0.364, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.045, 10.100], loss: 0.002485, mae: 0.052357, mean_q: 0.210793
 49719/100000: episode: 998, duration: 0.139s, episode steps: 26, steps per second: 188, episode reward: 12.910, mean reward: 0.497 [0.406, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.491], loss: 0.002933, mae: 0.057360, mean_q: 0.201021
 49729/100000: episode: 999, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 4.794, mean reward: 0.479 [0.460, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.385, 10.100], loss: 0.002941, mae: 0.055958, mean_q: 0.084599
 49759/100000: episode: 1000, duration: 0.161s, episode steps: 30, steps per second: 187, episode reward: 8.313, mean reward: 0.277 [0.033, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.654, 10.100], loss: 0.002966, mae: 0.057994, mean_q: 0.236490
 49785/100000: episode: 1001, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 10.057, mean reward: 0.387 [0.272, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.969, 10.423], loss: 0.002575, mae: 0.054190, mean_q: 0.175835
 49793/100000: episode: 1002, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 4.417, mean reward: 0.552 [0.520, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.433, 10.100], loss: 0.002772, mae: 0.058246, mean_q: 0.285648
 49801/100000: episode: 1003, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 3.295, mean reward: 0.412 [0.364, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.378, 10.100], loss: 0.003246, mae: 0.061641, mean_q: 0.267773
 49831/100000: episode: 1004, duration: 0.171s, episode steps: 30, steps per second: 175, episode reward: 10.343, mean reward: 0.345 [0.158, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.213, 10.293], loss: 0.002805, mae: 0.057102, mean_q: 0.209001
[Info] FALSIFICATION!
 49840/100000: episode: 1005, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 14.430, mean reward: 1.603 [0.476, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.707, 9.988], loss: 0.003072, mae: 0.061883, mean_q: 0.333117
 49940/100000: episode: 1006, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.784, mean reward: -0.168 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.605, 10.124], loss: 0.016451, mae: 0.066225, mean_q: 0.214423
 50040/100000: episode: 1007, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.608, mean reward: -0.166 [-1.000, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.745, 10.337], loss: 0.020576, mae: 0.088005, mean_q: 0.229492
 50140/100000: episode: 1008, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.061, mean reward: -0.161 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.589, 10.098], loss: 0.016962, mae: 0.068308, mean_q: 0.193194
 50240/100000: episode: 1009, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.696, mean reward: -0.187 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.924, 10.432], loss: 0.030434, mae: 0.077532, mean_q: 0.210688
 50340/100000: episode: 1010, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.250, mean reward: -0.182 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.342, 10.098], loss: 0.003116, mae: 0.060118, mean_q: 0.198909
 50440/100000: episode: 1011, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.092, mean reward: -0.151 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.124, 10.098], loss: 0.002973, mae: 0.058549, mean_q: 0.208289
 50540/100000: episode: 1012, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -15.831, mean reward: -0.158 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.433, 10.098], loss: 0.029111, mae: 0.072871, mean_q: 0.211476
 50640/100000: episode: 1013, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.093, mean reward: -0.191 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.297, 10.098], loss: 0.002838, mae: 0.056942, mean_q: 0.215793
 50740/100000: episode: 1014, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.906, mean reward: -0.169 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.167, 10.098], loss: 0.002843, mae: 0.056656, mean_q: 0.198446
 50840/100000: episode: 1015, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -11.504, mean reward: -0.115 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.653, 10.098], loss: 0.002712, mae: 0.056073, mean_q: 0.189694
 50940/100000: episode: 1016, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -18.372, mean reward: -0.184 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.588, 10.098], loss: 0.002770, mae: 0.055877, mean_q: 0.181893
 51040/100000: episode: 1017, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.217, mean reward: -0.182 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.473, 10.284], loss: 0.018727, mae: 0.077867, mean_q: 0.179855
 51140/100000: episode: 1018, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.058, mean reward: -0.171 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.657, 10.222], loss: 0.003362, mae: 0.060157, mean_q: 0.206973
 51240/100000: episode: 1019, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.388, mean reward: -0.174 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.540, 10.098], loss: 0.018773, mae: 0.083877, mean_q: 0.217726
 51340/100000: episode: 1020, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.292, mean reward: -0.143 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.532, 10.317], loss: 0.015092, mae: 0.059219, mean_q: 0.216220
 51440/100000: episode: 1021, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.372, mean reward: -0.194 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.545, 10.183], loss: 0.003376, mae: 0.061727, mean_q: 0.199564
 51540/100000: episode: 1022, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.675, mean reward: -0.167 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.308, 10.098], loss: 0.015855, mae: 0.064749, mean_q: 0.180291
 51640/100000: episode: 1023, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.903, mean reward: -0.159 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.520, 10.100], loss: 0.016134, mae: 0.067284, mean_q: 0.210228
 51740/100000: episode: 1024, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.955, mean reward: -0.180 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.988, 10.176], loss: 0.002786, mae: 0.056737, mean_q: 0.225943
 51840/100000: episode: 1025, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.568, mean reward: -0.176 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.377, 10.352], loss: 0.002790, mae: 0.056420, mean_q: 0.199338
 51940/100000: episode: 1026, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.979, mean reward: -0.170 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.127, 10.354], loss: 0.014786, mae: 0.058284, mean_q: 0.220425
 52040/100000: episode: 1027, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -19.713, mean reward: -0.197 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.148, 10.098], loss: 0.016916, mae: 0.071763, mean_q: 0.176636
 52140/100000: episode: 1028, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -10.462, mean reward: -0.105 [-1.000, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.168, 10.098], loss: 0.002920, mae: 0.057637, mean_q: 0.175464
 52240/100000: episode: 1029, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.577, mean reward: -0.176 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.540, 10.098], loss: 0.003033, mae: 0.058491, mean_q: 0.167279
 52340/100000: episode: 1030, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.966, mean reward: -0.180 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.803, 10.098], loss: 0.016595, mae: 0.069843, mean_q: 0.138518
 52440/100000: episode: 1031, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -20.545, mean reward: -0.205 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.359, 10.191], loss: 0.002746, mae: 0.054919, mean_q: 0.138523
 52540/100000: episode: 1032, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -11.427, mean reward: -0.114 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.911, 10.098], loss: 0.002853, mae: 0.056965, mean_q: 0.103504
 52640/100000: episode: 1033, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.296, mean reward: -0.173 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.563, 10.134], loss: 0.015237, mae: 0.063175, mean_q: 0.074805
 52740/100000: episode: 1034, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -11.705, mean reward: -0.117 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.310, 10.098], loss: 0.016089, mae: 0.067473, mean_q: 0.072264
 52840/100000: episode: 1035, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.407, mean reward: -0.184 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.284, 10.098], loss: 0.002636, mae: 0.053739, mean_q: 0.037348
 52940/100000: episode: 1036, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.203, mean reward: -0.142 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.062, 10.098], loss: 0.039984, mae: 0.081936, mean_q: 0.030918
 53040/100000: episode: 1037, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.768, mean reward: -0.168 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.227, 10.183], loss: 0.039043, mae: 0.085689, mean_q: 0.026393
 53140/100000: episode: 1038, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -16.075, mean reward: -0.161 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.300, 10.098], loss: 0.014563, mae: 0.063613, mean_q: -0.017261
 53240/100000: episode: 1039, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -20.705, mean reward: -0.207 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.747, 10.224], loss: 0.002920, mae: 0.055417, mean_q: -0.038227
 53340/100000: episode: 1040, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.701, mean reward: -0.197 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.066, 10.254], loss: 0.015264, mae: 0.063808, mean_q: -0.023990
 53440/100000: episode: 1041, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.972, mean reward: -0.190 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.556, 10.098], loss: 0.014982, mae: 0.064974, mean_q: -0.047807
 53540/100000: episode: 1042, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.851, mean reward: -0.189 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.017, 10.142], loss: 0.014584, mae: 0.061178, mean_q: -0.078730
 53640/100000: episode: 1043, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.968, mean reward: -0.180 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.370, 10.135], loss: 0.026625, mae: 0.077440, mean_q: -0.117441
 53740/100000: episode: 1044, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.163, mean reward: -0.182 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.503, 10.273], loss: 0.018867, mae: 0.085609, mean_q: -0.134904
 53840/100000: episode: 1045, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.441, mean reward: -0.184 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.208, 10.164], loss: 0.015768, mae: 0.068765, mean_q: -0.134686
 53940/100000: episode: 1046, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.007, mean reward: -0.140 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.431, 10.457], loss: 0.002928, mae: 0.054620, mean_q: -0.132292
 54040/100000: episode: 1047, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -14.765, mean reward: -0.148 [-1.000, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-3.229, 10.214], loss: 0.002807, mae: 0.053248, mean_q: -0.180261
 54140/100000: episode: 1048, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.570, mean reward: -0.186 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.190, 10.118], loss: 0.002668, mae: 0.052062, mean_q: -0.219873
 54240/100000: episode: 1049, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.469, mean reward: -0.175 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.087, 10.132], loss: 0.002520, mae: 0.049804, mean_q: -0.207719
 54340/100000: episode: 1050, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.192, mean reward: -0.172 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.599, 10.098], loss: 0.036278, mae: 0.073677, mean_q: -0.213904
 54440/100000: episode: 1051, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -14.858, mean reward: -0.149 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.914, 10.479], loss: 0.003408, mae: 0.056473, mean_q: -0.235916
 54540/100000: episode: 1052, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -13.058, mean reward: -0.131 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.089, 10.098], loss: 0.002667, mae: 0.051219, mean_q: -0.281549
 54640/100000: episode: 1053, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.254, mean reward: -0.183 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.514, 10.263], loss: 0.002533, mae: 0.050513, mean_q: -0.289163
 54740/100000: episode: 1054, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.408, mean reward: -0.154 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.462, 10.292], loss: 0.014411, mae: 0.059576, mean_q: -0.303105
 54840/100000: episode: 1055, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.763, mean reward: -0.178 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.811, 10.179], loss: 0.013569, mae: 0.062295, mean_q: -0.316169
 54940/100000: episode: 1056, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.255, mean reward: -0.183 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.171, 10.207], loss: 0.002621, mae: 0.051658, mean_q: -0.297818
 55040/100000: episode: 1057, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.323, mean reward: -0.193 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.546, 10.098], loss: 0.002659, mae: 0.051735, mean_q: -0.326591
 55140/100000: episode: 1058, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.926, mean reward: -0.189 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.360, 10.214], loss: 0.002520, mae: 0.049900, mean_q: -0.305824
 55240/100000: episode: 1059, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -20.174, mean reward: -0.202 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.734, 10.098], loss: 0.002445, mae: 0.049426, mean_q: -0.361000
 55340/100000: episode: 1060, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.852, mean reward: -0.179 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.894, 10.098], loss: 0.002681, mae: 0.051998, mean_q: -0.317104
 55440/100000: episode: 1061, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.296, mean reward: -0.173 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.662, 10.157], loss: 0.002436, mae: 0.049621, mean_q: -0.342935
 55540/100000: episode: 1062, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -8.389, mean reward: -0.084 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.639, 10.098], loss: 0.002661, mae: 0.051084, mean_q: -0.337755
 55640/100000: episode: 1063, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.651, mean reward: -0.197 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.452, 10.217], loss: 0.002587, mae: 0.051776, mean_q: -0.267952
 55740/100000: episode: 1064, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -14.693, mean reward: -0.147 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.362, 10.391], loss: 0.002391, mae: 0.048644, mean_q: -0.335346
 55840/100000: episode: 1065, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.230, mean reward: -0.182 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.937, 10.098], loss: 0.002476, mae: 0.050348, mean_q: -0.336633
 55940/100000: episode: 1066, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -16.035, mean reward: -0.160 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.199, 10.098], loss: 0.002324, mae: 0.048630, mean_q: -0.328217
 56040/100000: episode: 1067, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.789, mean reward: -0.198 [-1.000, 0.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.649, 10.098], loss: 0.002431, mae: 0.049691, mean_q: -0.341074
 56140/100000: episode: 1068, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -14.932, mean reward: -0.149 [-1.000, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.924, 10.204], loss: 0.002543, mae: 0.050696, mean_q: -0.297795
 56240/100000: episode: 1069, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.438, mean reward: -0.124 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.021, 10.098], loss: 0.002394, mae: 0.049213, mean_q: -0.301637
 56340/100000: episode: 1070, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.483, mean reward: -0.165 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.476, 10.173], loss: 0.002599, mae: 0.050786, mean_q: -0.311652
 56440/100000: episode: 1071, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.209, mean reward: -0.162 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.043, 10.099], loss: 0.002582, mae: 0.051154, mean_q: -0.306976
 56540/100000: episode: 1072, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.788, mean reward: -0.188 [-1.000, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.560, 10.343], loss: 0.002657, mae: 0.051202, mean_q: -0.299908
 56640/100000: episode: 1073, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.770, mean reward: -0.188 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.957, 10.235], loss: 0.002521, mae: 0.050405, mean_q: -0.322196
 56740/100000: episode: 1074, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.741, mean reward: -0.187 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.899, 10.098], loss: 0.002580, mae: 0.050845, mean_q: -0.350883
 56840/100000: episode: 1075, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.964, mean reward: -0.160 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.315, 10.098], loss: 0.002482, mae: 0.049928, mean_q: -0.330094
 56940/100000: episode: 1076, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -13.298, mean reward: -0.133 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.810, 10.235], loss: 0.002573, mae: 0.051152, mean_q: -0.295866
 57040/100000: episode: 1077, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.941, mean reward: -0.169 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.675, 10.098], loss: 0.002589, mae: 0.050915, mean_q: -0.317515
 57140/100000: episode: 1078, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.279, mean reward: -0.153 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.675, 10.295], loss: 0.002659, mae: 0.051092, mean_q: -0.318732
 57240/100000: episode: 1079, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.040, mean reward: -0.180 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.714, 10.141], loss: 0.002616, mae: 0.051192, mean_q: -0.284041
 57340/100000: episode: 1080, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -20.200, mean reward: -0.202 [-1.000, 0.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.787, 10.133], loss: 0.002515, mae: 0.049472, mean_q: -0.336924
 57440/100000: episode: 1081, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.745, mean reward: -0.157 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.909, 10.373], loss: 0.002538, mae: 0.050232, mean_q: -0.304402
 57540/100000: episode: 1082, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.067, mean reward: -0.191 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.768, 10.223], loss: 0.002375, mae: 0.048373, mean_q: -0.295633
 57640/100000: episode: 1083, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -14.234, mean reward: -0.142 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.911, 10.098], loss: 0.002635, mae: 0.051224, mean_q: -0.313271
 57740/100000: episode: 1084, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.345, mean reward: -0.163 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.595, 10.098], loss: 0.003466, mae: 0.055639, mean_q: -0.351489
 57840/100000: episode: 1085, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.344, mean reward: -0.183 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.656, 10.135], loss: 0.003352, mae: 0.060773, mean_q: -0.330994
 57940/100000: episode: 1086, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.975, mean reward: -0.160 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.472, 10.362], loss: 0.002535, mae: 0.049869, mean_q: -0.325702
 58040/100000: episode: 1087, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -16.021, mean reward: -0.160 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.359, 10.098], loss: 0.002604, mae: 0.050234, mean_q: -0.323221
 58140/100000: episode: 1088, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.700, mean reward: -0.177 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.403, 10.262], loss: 0.002676, mae: 0.050870, mean_q: -0.295506
 58240/100000: episode: 1089, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.745, mean reward: -0.187 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.262, 10.170], loss: 0.002677, mae: 0.051581, mean_q: -0.307570
 58340/100000: episode: 1090, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.877, mean reward: -0.189 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.286, 10.214], loss: 0.002873, mae: 0.052648, mean_q: -0.324449
 58440/100000: episode: 1091, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.733, mean reward: -0.177 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.301, 10.098], loss: 0.002703, mae: 0.051970, mean_q: -0.295383
 58540/100000: episode: 1092, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.000, mean reward: -0.190 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.546, 10.098], loss: 0.002780, mae: 0.052169, mean_q: -0.301452
 58640/100000: episode: 1093, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.644, mean reward: -0.186 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.608, 10.098], loss: 0.002548, mae: 0.049690, mean_q: -0.310129
 58740/100000: episode: 1094, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.177, mean reward: -0.172 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.594, 10.098], loss: 0.002447, mae: 0.048953, mean_q: -0.326672
 58840/100000: episode: 1095, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.623, mean reward: -0.156 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.320, 10.098], loss: 0.002458, mae: 0.048758, mean_q: -0.293175
 58940/100000: episode: 1096, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.458, mean reward: -0.195 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.849, 10.098], loss: 0.002543, mae: 0.049916, mean_q: -0.329201
 59040/100000: episode: 1097, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.501, mean reward: -0.185 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.208, 10.123], loss: 0.002527, mae: 0.049645, mean_q: -0.333637
 59140/100000: episode: 1098, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -15.365, mean reward: -0.154 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.701, 10.098], loss: 0.002491, mae: 0.049544, mean_q: -0.342318
 59240/100000: episode: 1099, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.204, mean reward: -0.192 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.959, 10.098], loss: 0.002597, mae: 0.050390, mean_q: -0.347963
 59340/100000: episode: 1100, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -9.821, mean reward: -0.098 [-1.000, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.784, 10.098], loss: 0.002514, mae: 0.049840, mean_q: -0.320069
 59440/100000: episode: 1101, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -9.631, mean reward: -0.096 [-1.000, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.579, 10.098], loss: 0.002609, mae: 0.050266, mean_q: -0.314044
 59540/100000: episode: 1102, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.315, mean reward: -0.163 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.562, 10.098], loss: 0.002542, mae: 0.049321, mean_q: -0.346931
 59640/100000: episode: 1103, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.195, mean reward: -0.182 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.524, 10.098], loss: 0.002593, mae: 0.050464, mean_q: -0.327756
 59740/100000: episode: 1104, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.322, mean reward: -0.173 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.205, 10.098], loss: 0.002613, mae: 0.051029, mean_q: -0.285892
[Info] 100-TH LEVEL FOUND: 0.6199634075164795, Considering 10/90 traces
 59840/100000: episode: 1105, duration: 4.429s, episode steps: 100, steps per second: 23, episode reward: -16.375, mean reward: -0.164 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.974, 10.275], loss: 0.002439, mae: 0.049083, mean_q: -0.313405
 59883/100000: episode: 1106, duration: 0.230s, episode steps: 43, steps per second: 187, episode reward: 14.218, mean reward: 0.331 [0.204, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.649, 10.100], loss: 0.002522, mae: 0.049770, mean_q: -0.280300
 59926/100000: episode: 1107, duration: 0.237s, episode steps: 43, steps per second: 181, episode reward: 13.076, mean reward: 0.304 [0.130, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.035, 10.100], loss: 0.002534, mae: 0.051589, mean_q: -0.233230
 59948/100000: episode: 1108, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 7.642, mean reward: 0.347 [0.239, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.432, 10.100], loss: 0.002535, mae: 0.049371, mean_q: -0.329685
 59979/100000: episode: 1109, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 12.248, mean reward: 0.395 [0.246, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.471, 10.449], loss: 0.002760, mae: 0.050730, mean_q: -0.309163
 60002/100000: episode: 1110, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 8.495, mean reward: 0.369 [0.296, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.938, 10.100], loss: 0.002857, mae: 0.053710, mean_q: -0.244397
 60025/100000: episode: 1111, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 8.850, mean reward: 0.385 [0.258, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.323, 10.100], loss: 0.002338, mae: 0.049249, mean_q: -0.305999
 60042/100000: episode: 1112, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 5.875, mean reward: 0.346 [0.192, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-1.443, 10.365], loss: 0.002148, mae: 0.046085, mean_q: -0.307792
 60059/100000: episode: 1113, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 4.848, mean reward: 0.285 [0.199, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.411], loss: 0.002634, mae: 0.052235, mean_q: -0.270318
 60078/100000: episode: 1114, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 7.337, mean reward: 0.386 [0.328, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.512], loss: 0.002841, mae: 0.053243, mean_q: -0.234207
 60102/100000: episode: 1115, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 7.830, mean reward: 0.326 [0.232, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.228, 10.303], loss: 0.002552, mae: 0.050425, mean_q: -0.272222
 60125/100000: episode: 1116, duration: 0.123s, episode steps: 23, steps per second: 186, episode reward: 6.904, mean reward: 0.300 [0.235, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.312, 10.100], loss: 0.002443, mae: 0.048481, mean_q: -0.254506
 60149/100000: episode: 1117, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 9.275, mean reward: 0.386 [0.298, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.518], loss: 0.002603, mae: 0.049862, mean_q: -0.244811
 60180/100000: episode: 1118, duration: 0.174s, episode steps: 31, steps per second: 179, episode reward: 11.546, mean reward: 0.372 [0.218, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.490, 10.439], loss: 0.003083, mae: 0.055989, mean_q: -0.252748
 60202/100000: episode: 1119, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 9.504, mean reward: 0.432 [0.348, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.680, 10.100], loss: 0.002511, mae: 0.050910, mean_q: -0.212415
 60219/100000: episode: 1120, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 5.050, mean reward: 0.297 [0.190, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.423], loss: 0.002450, mae: 0.049973, mean_q: -0.226129
 60242/100000: episode: 1121, duration: 0.119s, episode steps: 23, steps per second: 192, episode reward: 8.931, mean reward: 0.388 [0.285, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.368, 10.100], loss: 0.002651, mae: 0.053222, mean_q: -0.171282
 60266/100000: episode: 1122, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 7.211, mean reward: 0.300 [0.163, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.339, 10.305], loss: 0.002877, mae: 0.053746, mean_q: -0.254823
 60288/100000: episode: 1123, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 7.002, mean reward: 0.318 [0.228, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.514, 10.100], loss: 0.002222, mae: 0.048853, mean_q: -0.231453
 60311/100000: episode: 1124, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 8.182, mean reward: 0.356 [0.276, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.368, 10.100], loss: 0.002498, mae: 0.050093, mean_q: -0.273256
 60334/100000: episode: 1125, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 6.415, mean reward: 0.279 [0.184, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.288, 10.100], loss: 0.002533, mae: 0.049407, mean_q: -0.257234
 60356/100000: episode: 1126, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 6.643, mean reward: 0.302 [0.227, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.418, 10.100], loss: 0.002604, mae: 0.050260, mean_q: -0.227878
 60373/100000: episode: 1127, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 6.278, mean reward: 0.369 [0.277, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.581, 10.450], loss: 0.002846, mae: 0.052623, mean_q: -0.135113
 60396/100000: episode: 1128, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 6.422, mean reward: 0.279 [0.165, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.273, 10.100], loss: 0.002445, mae: 0.048851, mean_q: -0.228001
 60419/100000: episode: 1129, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 8.492, mean reward: 0.369 [0.317, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.425, 10.100], loss: 0.002470, mae: 0.049307, mean_q: -0.142618
 60442/100000: episode: 1130, duration: 0.125s, episode steps: 23, steps per second: 185, episode reward: 4.903, mean reward: 0.213 [0.053, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.475, 10.100], loss: 0.002408, mae: 0.050632, mean_q: -0.194166
 60485/100000: episode: 1131, duration: 0.216s, episode steps: 43, steps per second: 199, episode reward: 12.567, mean reward: 0.292 [0.078, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.707, 10.100], loss: 0.002672, mae: 0.054575, mean_q: -0.160179
 60493/100000: episode: 1132, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 3.185, mean reward: 0.398 [0.336, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.552], loss: 0.002524, mae: 0.053099, mean_q: -0.255055
 60524/100000: episode: 1133, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 7.249, mean reward: 0.234 [0.097, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.883, 10.262], loss: 0.002805, mae: 0.055218, mean_q: -0.142094
 60547/100000: episode: 1134, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 6.457, mean reward: 0.281 [0.159, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.244, 10.100], loss: 0.002743, mae: 0.053126, mean_q: -0.175734
 60590/100000: episode: 1135, duration: 0.254s, episode steps: 43, steps per second: 169, episode reward: 8.871, mean reward: 0.206 [0.019, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.367, 10.124], loss: 0.002592, mae: 0.050638, mean_q: -0.187798
 60613/100000: episode: 1136, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 9.065, mean reward: 0.394 [0.290, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.744, 10.100], loss: 0.002686, mae: 0.052894, mean_q: -0.177769
 60656/100000: episode: 1137, duration: 0.250s, episode steps: 43, steps per second: 172, episode reward: 13.341, mean reward: 0.310 [0.132, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.865, 10.100], loss: 0.003204, mae: 0.057185, mean_q: -0.187790
 60678/100000: episode: 1138, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 7.880, mean reward: 0.358 [0.303, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-1.194, 10.100], loss: 0.002957, mae: 0.054815, mean_q: -0.181461
 60695/100000: episode: 1139, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 5.444, mean reward: 0.320 [0.182, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.414, 10.337], loss: 0.002993, mae: 0.055264, mean_q: -0.219539
 60738/100000: episode: 1140, duration: 0.214s, episode steps: 43, steps per second: 201, episode reward: 15.445, mean reward: 0.359 [0.219, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.910, 10.100], loss: 0.002532, mae: 0.050359, mean_q: -0.157807
 60757/100000: episode: 1141, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 4.052, mean reward: 0.213 [0.113, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.090, 10.251], loss: 0.002695, mae: 0.052464, mean_q: -0.149616
 60780/100000: episode: 1142, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 5.887, mean reward: 0.256 [0.172, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.213, 10.100], loss: 0.002627, mae: 0.053005, mean_q: -0.148299
 60802/100000: episode: 1143, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 9.033, mean reward: 0.411 [0.275, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.471, 10.100], loss: 0.002956, mae: 0.053166, mean_q: -0.197797
 60824/100000: episode: 1144, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 6.232, mean reward: 0.283 [0.201, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.337, 10.100], loss: 0.002881, mae: 0.055554, mean_q: -0.152373
 60846/100000: episode: 1145, duration: 0.135s, episode steps: 22, steps per second: 163, episode reward: 7.413, mean reward: 0.337 [0.255, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.250, 10.100], loss: 0.003331, mae: 0.058746, mean_q: -0.092483
 60889/100000: episode: 1146, duration: 0.239s, episode steps: 43, steps per second: 180, episode reward: 14.576, mean reward: 0.339 [0.111, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-1.435, 10.100], loss: 0.013796, mae: 0.087480, mean_q: -0.112597
 60912/100000: episode: 1147, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 6.029, mean reward: 0.262 [0.167, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.542, 10.100], loss: 0.006534, mae: 0.077804, mean_q: -0.138856
 60935/100000: episode: 1148, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 8.332, mean reward: 0.362 [0.269, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.224, 10.100], loss: 0.003892, mae: 0.061663, mean_q: -0.103349
 60958/100000: episode: 1149, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 8.962, mean reward: 0.390 [0.311, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.285, 10.100], loss: 0.003273, mae: 0.060624, mean_q: -0.047920
 60982/100000: episode: 1150, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 7.631, mean reward: 0.318 [0.214, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.814, 10.423], loss: 0.002739, mae: 0.055305, mean_q: -0.104275
 61006/100000: episode: 1151, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 6.608, mean reward: 0.275 [0.197, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.861, 10.343], loss: 0.003146, mae: 0.058549, mean_q: -0.043829
 61029/100000: episode: 1152, duration: 0.131s, episode steps: 23, steps per second: 175, episode reward: 6.814, mean reward: 0.296 [0.212, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.247, 10.100], loss: 0.003902, mae: 0.063776, mean_q: -0.045979
 61053/100000: episode: 1153, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 8.740, mean reward: 0.364 [0.178, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.393, 10.335], loss: 0.003038, mae: 0.058632, mean_q: -0.120986
 61061/100000: episode: 1154, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 3.127, mean reward: 0.391 [0.368, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.388], loss: 0.002945, mae: 0.057503, mean_q: -0.104131
 61078/100000: episode: 1155, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 4.870, mean reward: 0.286 [0.194, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.307, 10.369], loss: 0.002501, mae: 0.053098, mean_q: -0.120046
 61121/100000: episode: 1156, duration: 0.220s, episode steps: 43, steps per second: 196, episode reward: 12.688, mean reward: 0.295 [0.192, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.821, 10.100], loss: 0.002720, mae: 0.053609, mean_q: -0.059942
 61140/100000: episode: 1157, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 5.913, mean reward: 0.311 [0.184, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.850, 10.253], loss: 0.002713, mae: 0.054737, mean_q: -0.043789
 61163/100000: episode: 1158, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 6.480, mean reward: 0.282 [0.199, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.556, 10.100], loss: 0.002370, mae: 0.051927, mean_q: -0.042272
 61186/100000: episode: 1159, duration: 0.125s, episode steps: 23, steps per second: 185, episode reward: 6.387, mean reward: 0.278 [0.126, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.184, 10.100], loss: 0.002315, mae: 0.050347, mean_q: -0.083119
 61194/100000: episode: 1160, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 2.864, mean reward: 0.358 [0.271, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.285, 10.424], loss: 0.003034, mae: 0.057805, mean_q: -0.064771
 61217/100000: episode: 1161, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 7.638, mean reward: 0.332 [0.221, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.557, 10.100], loss: 0.002550, mae: 0.052024, mean_q: -0.153563
 61239/100000: episode: 1162, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 7.403, mean reward: 0.337 [0.249, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.233, 10.100], loss: 0.002972, mae: 0.053948, mean_q: -0.113527
 61258/100000: episode: 1163, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 6.348, mean reward: 0.334 [0.244, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.369, 10.527], loss: 0.002526, mae: 0.051006, mean_q: -0.116140
 61281/100000: episode: 1164, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 7.136, mean reward: 0.310 [0.233, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.582, 10.100], loss: 0.002782, mae: 0.054947, mean_q: -0.043051
 61303/100000: episode: 1165, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 8.188, mean reward: 0.372 [0.260, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.460, 10.100], loss: 0.002747, mae: 0.053684, mean_q: -0.084049
 61325/100000: episode: 1166, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 7.384, mean reward: 0.336 [0.295, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.342, 10.100], loss: 0.002346, mae: 0.049741, mean_q: -0.032696
 61348/100000: episode: 1167, duration: 0.134s, episode steps: 23, steps per second: 171, episode reward: 7.896, mean reward: 0.343 [0.251, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.378, 10.100], loss: 0.002605, mae: 0.053424, mean_q: -0.032870
 61367/100000: episode: 1168, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 4.667, mean reward: 0.246 [0.119, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.374], loss: 0.002654, mae: 0.053344, mean_q: 0.008392
 61375/100000: episode: 1169, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 3.024, mean reward: 0.378 [0.320, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.416], loss: 0.003159, mae: 0.056705, mean_q: 0.003517
 61418/100000: episode: 1170, duration: 0.216s, episode steps: 43, steps per second: 199, episode reward: 12.590, mean reward: 0.293 [0.061, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.794, 10.100], loss: 0.002554, mae: 0.052678, mean_q: -0.015547
 61441/100000: episode: 1171, duration: 0.143s, episode steps: 23, steps per second: 161, episode reward: 5.736, mean reward: 0.249 [0.109, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.918, 10.100], loss: 0.002388, mae: 0.051683, mean_q: -0.002929
 61464/100000: episode: 1172, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 7.586, mean reward: 0.330 [0.212, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.309, 10.100], loss: 0.002530, mae: 0.053734, mean_q: 0.078136
 61483/100000: episode: 1173, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 6.162, mean reward: 0.324 [0.211, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.719, 10.402], loss: 0.002535, mae: 0.052483, mean_q: 0.046194
 61491/100000: episode: 1174, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 2.456, mean reward: 0.307 [0.210, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.255, 10.357], loss: 0.002468, mae: 0.051506, mean_q: -0.060882
 61508/100000: episode: 1175, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 3.836, mean reward: 0.226 [0.134, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.295], loss: 0.002668, mae: 0.053489, mean_q: 0.035854
 61551/100000: episode: 1176, duration: 0.227s, episode steps: 43, steps per second: 189, episode reward: 13.586, mean reward: 0.316 [0.104, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-1.113, 10.100], loss: 0.002419, mae: 0.050320, mean_q: -0.014708
 61570/100000: episode: 1177, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 6.189, mean reward: 0.326 [0.276, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.228, 10.387], loss: 0.002247, mae: 0.049308, mean_q: 0.065142
 61593/100000: episode: 1178, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 7.614, mean reward: 0.331 [0.212, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.630, 10.100], loss: 0.002446, mae: 0.051223, mean_q: 0.053393
 61616/100000: episode: 1179, duration: 0.121s, episode steps: 23, steps per second: 189, episode reward: 6.805, mean reward: 0.296 [0.178, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.347, 10.100], loss: 0.002707, mae: 0.053148, mean_q: 0.011169
 61639/100000: episode: 1180, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 7.551, mean reward: 0.328 [0.257, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.261, 10.100], loss: 0.002287, mae: 0.050006, mean_q: 0.023018
 61670/100000: episode: 1181, duration: 0.182s, episode steps: 31, steps per second: 170, episode reward: 10.621, mean reward: 0.343 [0.208, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.035, 10.477], loss: 0.002687, mae: 0.052815, mean_q: 0.065866
 61713/100000: episode: 1182, duration: 0.245s, episode steps: 43, steps per second: 175, episode reward: 12.656, mean reward: 0.294 [0.109, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.862, 10.100], loss: 0.002598, mae: 0.052263, mean_q: -0.013052
 61756/100000: episode: 1183, duration: 0.216s, episode steps: 43, steps per second: 199, episode reward: 9.621, mean reward: 0.224 [0.060, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.075, 10.100], loss: 0.002563, mae: 0.052163, mean_q: 0.051536
 61799/100000: episode: 1184, duration: 0.234s, episode steps: 43, steps per second: 184, episode reward: 11.185, mean reward: 0.260 [0.100, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-1.244, 10.100], loss: 0.002365, mae: 0.050258, mean_q: 0.055174
 61818/100000: episode: 1185, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 6.593, mean reward: 0.347 [0.216, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.175, 10.356], loss: 0.002654, mae: 0.054363, mean_q: 0.073036
 61841/100000: episode: 1186, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 6.997, mean reward: 0.304 [0.234, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.319, 10.100], loss: 0.002611, mae: 0.053279, mean_q: 0.085685
 61858/100000: episode: 1187, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 3.402, mean reward: 0.200 [0.064, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.078, 10.173], loss: 0.002430, mae: 0.051270, mean_q: 0.015238
 61881/100000: episode: 1188, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 10.392, mean reward: 0.452 [0.310, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.360, 10.100], loss: 0.002878, mae: 0.055977, mean_q: 0.075027
 61903/100000: episode: 1189, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 8.038, mean reward: 0.365 [0.222, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.593, 10.100], loss: 0.002243, mae: 0.049321, mean_q: 0.049915
 61926/100000: episode: 1190, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 9.640, mean reward: 0.419 [0.281, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.508, 10.100], loss: 0.002807, mae: 0.055368, mean_q: 0.102007
 61949/100000: episode: 1191, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 7.031, mean reward: 0.306 [0.152, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.135, 10.100], loss: 0.002621, mae: 0.054566, mean_q: 0.122452
 61972/100000: episode: 1192, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 6.581, mean reward: 0.286 [0.229, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.294, 10.100], loss: 0.002329, mae: 0.050365, mean_q: 0.064569
 61995/100000: episode: 1193, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 8.670, mean reward: 0.377 [0.171, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.230, 10.100], loss: 0.002435, mae: 0.051405, mean_q: 0.061150
 62018/100000: episode: 1194, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 5.787, mean reward: 0.252 [0.096, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.210, 10.100], loss: 0.002748, mae: 0.055064, mean_q: 0.127162
[Info] 200-TH LEVEL FOUND: 0.8071715235710144, Considering 10/90 traces
 62035/100000: episode: 1195, duration: 4.017s, episode steps: 17, steps per second: 4, episode reward: 5.497, mean reward: 0.323 [0.243, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.222, 10.386], loss: 0.002656, mae: 0.054986, mean_q: 0.053512
[Info] FALSIFICATION!
 62051/100000: episode: 1196, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 18.889, mean reward: 1.181 [0.456, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.339, 10.020], loss: 0.003012, mae: 0.059680, mean_q: 0.150917
 62151/100000: episode: 1197, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.060, mean reward: -0.171 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.989, 10.098], loss: 0.002855, mae: 0.056072, mean_q: 0.114266
 62251/100000: episode: 1198, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.086, mean reward: -0.181 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.744, 10.234], loss: 0.002516, mae: 0.052560, mean_q: 0.123213
 62351/100000: episode: 1199, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -15.392, mean reward: -0.154 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.828, 10.098], loss: 0.002460, mae: 0.051916, mean_q: 0.086321
 62451/100000: episode: 1200, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.167, mean reward: -0.172 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.008, 10.098], loss: 0.002668, mae: 0.054434, mean_q: 0.113176
 62551/100000: episode: 1201, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -7.026, mean reward: -0.070 [-1.000, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.708, 10.347], loss: 0.002673, mae: 0.054450, mean_q: 0.121971
 62651/100000: episode: 1202, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.115, mean reward: -0.161 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.780, 10.098], loss: 0.028885, mae: 0.067339, mean_q: 0.106324
 62751/100000: episode: 1203, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.450, mean reward: -0.174 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.588, 10.265], loss: 0.003724, mae: 0.063140, mean_q: 0.081450
 62851/100000: episode: 1204, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.998, mean reward: -0.180 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.671, 10.107], loss: 0.002480, mae: 0.051924, mean_q: 0.079508
 62951/100000: episode: 1205, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -16.333, mean reward: -0.163 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.957, 10.098], loss: 0.002545, mae: 0.053425, mean_q: 0.100713
 63051/100000: episode: 1206, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.067, mean reward: -0.151 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.908, 10.098], loss: 0.002506, mae: 0.052027, mean_q: 0.113593
 63151/100000: episode: 1207, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -12.002, mean reward: -0.120 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.829, 10.196], loss: 0.015893, mae: 0.061630, mean_q: 0.091755
 63251/100000: episode: 1208, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -20.142, mean reward: -0.201 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.476, 10.099], loss: 0.016196, mae: 0.067096, mean_q: 0.141184
 63351/100000: episode: 1209, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -11.671, mean reward: -0.117 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.136, 10.230], loss: 0.029164, mae: 0.073335, mean_q: 0.124291
 63451/100000: episode: 1210, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -13.056, mean reward: -0.131 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.961, 10.098], loss: 0.015525, mae: 0.061785, mean_q: 0.142371
 63551/100000: episode: 1211, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.960, mean reward: -0.170 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.833, 10.098], loss: 0.015747, mae: 0.063863, mean_q: 0.110855
 63651/100000: episode: 1212, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.245, mean reward: -0.172 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.459, 10.098], loss: 0.002602, mae: 0.053325, mean_q: 0.112488
 63751/100000: episode: 1213, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.737, mean reward: -0.167 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.442, 10.098], loss: 0.002507, mae: 0.051830, mean_q: 0.080401
 63851/100000: episode: 1214, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.483, mean reward: -0.155 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.747, 10.098], loss: 0.015598, mae: 0.061303, mean_q: 0.117444
 63951/100000: episode: 1215, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.538, mean reward: -0.175 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.530, 10.214], loss: 0.015391, mae: 0.061467, mean_q: 0.122045
 64051/100000: episode: 1216, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -12.732, mean reward: -0.127 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.819, 10.098], loss: 0.015582, mae: 0.059393, mean_q: 0.141088
 64151/100000: episode: 1217, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.411, mean reward: -0.144 [-1.000, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.276, 10.119], loss: 0.016481, mae: 0.068879, mean_q: 0.104045
 64251/100000: episode: 1218, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.323, mean reward: -0.173 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.541, 10.235], loss: 0.004192, mae: 0.066644, mean_q: 0.098614
 64351/100000: episode: 1219, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.369, mean reward: -0.194 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.907, 10.192], loss: 0.015828, mae: 0.066900, mean_q: 0.120985
 64451/100000: episode: 1220, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -12.288, mean reward: -0.123 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.050, 10.210], loss: 0.040268, mae: 0.085122, mean_q: 0.126825
 64551/100000: episode: 1221, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.810, mean reward: -0.178 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.882, 10.098], loss: 0.002588, mae: 0.053475, mean_q: 0.108087
 64651/100000: episode: 1222, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.991, mean reward: -0.160 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.993, 10.190], loss: 0.027971, mae: 0.076333, mean_q: 0.140563
 64751/100000: episode: 1223, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.487, mean reward: -0.185 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.062, 10.203], loss: 0.003017, mae: 0.057446, mean_q: 0.115976
 64851/100000: episode: 1224, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -20.259, mean reward: -0.203 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.664, 10.196], loss: 0.002422, mae: 0.051685, mean_q: 0.114739
 64951/100000: episode: 1225, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -13.136, mean reward: -0.131 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.655, 10.247], loss: 0.028802, mae: 0.078169, mean_q: 0.079465
 65051/100000: episode: 1226, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.442, mean reward: -0.164 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.874, 10.180], loss: 0.002788, mae: 0.054865, mean_q: 0.060080
 65151/100000: episode: 1227, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.885, mean reward: -0.179 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.878, 10.285], loss: 0.015433, mae: 0.064400, mean_q: 0.050810
 65251/100000: episode: 1228, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.931, mean reward: -0.169 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.672, 10.098], loss: 0.028614, mae: 0.082927, mean_q: 0.033444
 65351/100000: episode: 1229, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -15.173, mean reward: -0.152 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.228, 10.384], loss: 0.002694, mae: 0.054056, mean_q: 0.008855
 65451/100000: episode: 1230, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.955, mean reward: -0.170 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.463, 10.135], loss: 0.013959, mae: 0.054703, mean_q: -0.027517
 65551/100000: episode: 1231, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.729, mean reward: -0.167 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.803, 10.201], loss: 0.003859, mae: 0.063033, mean_q: -0.020622
 65651/100000: episode: 1232, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.360, mean reward: -0.174 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.285, 10.105], loss: 0.002690, mae: 0.053704, mean_q: -0.016662
 65751/100000: episode: 1233, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.000, mean reward: -0.160 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.226, 10.273], loss: 0.002680, mae: 0.052988, mean_q: -0.061733
 65851/100000: episode: 1234, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.593, mean reward: -0.186 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.033, 10.165], loss: 0.002507, mae: 0.051571, mean_q: -0.069725
 65951/100000: episode: 1235, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.998, mean reward: -0.170 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.829, 10.253], loss: 0.015267, mae: 0.065608, mean_q: -0.124130
 66051/100000: episode: 1236, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -12.389, mean reward: -0.124 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.797, 10.098], loss: 0.002690, mae: 0.053725, mean_q: -0.148085
 66151/100000: episode: 1237, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.245, mean reward: -0.172 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.651, 10.098], loss: 0.002670, mae: 0.052737, mean_q: -0.170765
 66251/100000: episode: 1238, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.486, mean reward: -0.185 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.263, 10.098], loss: 0.002686, mae: 0.052419, mean_q: -0.130825
 66351/100000: episode: 1239, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.694, mean reward: -0.167 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.790, 10.098], loss: 0.002663, mae: 0.053210, mean_q: -0.172876
 66451/100000: episode: 1240, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.236, mean reward: -0.172 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.340, 10.098], loss: 0.014254, mae: 0.057022, mean_q: -0.203762
 66551/100000: episode: 1241, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.436, mean reward: -0.194 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.013, 10.276], loss: 0.016227, mae: 0.074788, mean_q: -0.180244
 66651/100000: episode: 1242, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.002, mean reward: -0.200 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.249, 10.182], loss: 0.014028, mae: 0.061522, mean_q: -0.263090
 66751/100000: episode: 1243, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.594, mean reward: -0.176 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.910, 10.223], loss: 0.003062, mae: 0.055744, mean_q: -0.284567
 66851/100000: episode: 1244, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.644, mean reward: -0.166 [-1.000, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.416, 10.100], loss: 0.031312, mae: 0.101087, mean_q: -0.264308
 66951/100000: episode: 1245, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -18.553, mean reward: -0.186 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.027, 10.098], loss: 0.004971, mae: 0.069949, mean_q: -0.309383
 67051/100000: episode: 1246, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.759, mean reward: -0.188 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.637, 10.124], loss: 0.002717, mae: 0.053104, mean_q: -0.319665
 67151/100000: episode: 1247, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -15.843, mean reward: -0.158 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.629, 10.098], loss: 0.002592, mae: 0.050844, mean_q: -0.300602
 67251/100000: episode: 1248, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.951, mean reward: -0.180 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.475, 10.098], loss: 0.002541, mae: 0.050896, mean_q: -0.298623
 67351/100000: episode: 1249, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.028, mean reward: -0.170 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.256, 10.451], loss: 0.002649, mae: 0.051043, mean_q: -0.304364
 67451/100000: episode: 1250, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.068, mean reward: -0.181 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.359, 10.219], loss: 0.002602, mae: 0.051176, mean_q: -0.293014
 67551/100000: episode: 1251, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -9.709, mean reward: -0.097 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.667, 10.098], loss: 0.002721, mae: 0.051631, mean_q: -0.334604
 67651/100000: episode: 1252, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.084, mean reward: -0.141 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.460, 10.286], loss: 0.002471, mae: 0.049244, mean_q: -0.326767
 67751/100000: episode: 1253, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -20.570, mean reward: -0.206 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.489, 10.244], loss: 0.002683, mae: 0.052029, mean_q: -0.309009
 67851/100000: episode: 1254, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.746, mean reward: -0.207 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.927, 10.098], loss: 0.002680, mae: 0.051654, mean_q: -0.309925
 67951/100000: episode: 1255, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.689, mean reward: -0.197 [-1.000, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.106, 10.181], loss: 0.002641, mae: 0.051541, mean_q: -0.304920
 68051/100000: episode: 1256, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.657, mean reward: -0.197 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.654, 10.100], loss: 0.002626, mae: 0.051086, mean_q: -0.299030
 68151/100000: episode: 1257, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.525, mean reward: -0.155 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.281, 10.098], loss: 0.002541, mae: 0.049965, mean_q: -0.323248
 68251/100000: episode: 1258, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.579, mean reward: -0.176 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.173, 10.098], loss: 0.002576, mae: 0.050361, mean_q: -0.321248
 68351/100000: episode: 1259, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.841, mean reward: -0.178 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.601, 10.098], loss: 0.002533, mae: 0.049408, mean_q: -0.329744
 68451/100000: episode: 1260, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.422, mean reward: -0.184 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.506, 10.137], loss: 0.002627, mae: 0.050591, mean_q: -0.300820
 68551/100000: episode: 1261, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.709, mean reward: -0.177 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.053, 10.098], loss: 0.002487, mae: 0.049057, mean_q: -0.336997
 68651/100000: episode: 1262, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.457, mean reward: -0.155 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.099, 10.098], loss: 0.002672, mae: 0.051403, mean_q: -0.315627
 68751/100000: episode: 1263, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.981, mean reward: -0.180 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.184, 10.164], loss: 0.002762, mae: 0.052131, mean_q: -0.319286
 68851/100000: episode: 1264, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -8.641, mean reward: -0.086 [-1.000, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.810, 10.098], loss: 0.002677, mae: 0.051160, mean_q: -0.296105
 68951/100000: episode: 1265, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -17.929, mean reward: -0.179 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.773, 10.098], loss: 0.002855, mae: 0.053209, mean_q: -0.289977
 69051/100000: episode: 1266, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -14.710, mean reward: -0.147 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.983, 10.212], loss: 0.002771, mae: 0.051666, mean_q: -0.324972
 69151/100000: episode: 1267, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.612, mean reward: -0.176 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.962, 10.172], loss: 0.002749, mae: 0.051241, mean_q: -0.335930
 69251/100000: episode: 1268, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.314, mean reward: -0.173 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.850, 10.098], loss: 0.002761, mae: 0.051900, mean_q: -0.336634
 69351/100000: episode: 1269, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.824, mean reward: -0.178 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.827, 10.178], loss: 0.002939, mae: 0.053388, mean_q: -0.331288
 69451/100000: episode: 1270, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -19.240, mean reward: -0.192 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.146, 10.110], loss: 0.002556, mae: 0.049788, mean_q: -0.334546
 69551/100000: episode: 1271, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.380, mean reward: -0.184 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.539, 10.115], loss: 0.002703, mae: 0.051371, mean_q: -0.304653
 69651/100000: episode: 1272, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.629, mean reward: -0.186 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.755, 10.201], loss: 0.002689, mae: 0.051261, mean_q: -0.293482
 69751/100000: episode: 1273, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -19.517, mean reward: -0.195 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.581, 10.146], loss: 0.002666, mae: 0.050898, mean_q: -0.343759
 69851/100000: episode: 1274, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.891, mean reward: -0.179 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.437, 10.129], loss: 0.002487, mae: 0.050080, mean_q: -0.313340
 69951/100000: episode: 1275, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.871, mean reward: -0.199 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.770, 10.098], loss: 0.002737, mae: 0.051685, mean_q: -0.331618
 70051/100000: episode: 1276, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.410, mean reward: -0.144 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.704, 10.156], loss: 0.002932, mae: 0.053280, mean_q: -0.341328
 70151/100000: episode: 1277, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -17.717, mean reward: -0.177 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.482, 10.098], loss: 0.002584, mae: 0.051775, mean_q: -0.341476
 70251/100000: episode: 1278, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.779, mean reward: -0.168 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.859, 10.098], loss: 0.002704, mae: 0.052074, mean_q: -0.303063
 70351/100000: episode: 1279, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.994, mean reward: -0.170 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.612, 10.143], loss: 0.002616, mae: 0.051622, mean_q: -0.336255
 70451/100000: episode: 1280, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.589, mean reward: -0.186 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.089, 10.098], loss: 0.002479, mae: 0.049743, mean_q: -0.340377
 70551/100000: episode: 1281, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -13.360, mean reward: -0.134 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.773, 10.098], loss: 0.002585, mae: 0.050905, mean_q: -0.302894
 70651/100000: episode: 1282, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -18.197, mean reward: -0.182 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.182, 10.098], loss: 0.002582, mae: 0.050289, mean_q: -0.338466
 70751/100000: episode: 1283, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.277, mean reward: -0.143 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.256, 10.549], loss: 0.002593, mae: 0.050657, mean_q: -0.332216
 70851/100000: episode: 1284, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.570, mean reward: -0.176 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.654, 10.107], loss: 0.002676, mae: 0.052239, mean_q: -0.294137
 70951/100000: episode: 1285, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.994, mean reward: -0.190 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.119, 10.295], loss: 0.002572, mae: 0.051603, mean_q: -0.336526
 71051/100000: episode: 1286, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.594, mean reward: -0.146 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.062, 10.375], loss: 0.002540, mae: 0.048863, mean_q: -0.321172
 71151/100000: episode: 1287, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.627, mean reward: -0.186 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.498, 10.200], loss: 0.002574, mae: 0.050195, mean_q: -0.346576
 71251/100000: episode: 1288, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.242, mean reward: -0.162 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.641, 10.098], loss: 0.002647, mae: 0.052537, mean_q: -0.318014
 71351/100000: episode: 1289, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.901, mean reward: -0.199 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.042, 10.118], loss: 0.002535, mae: 0.050236, mean_q: -0.332641
 71451/100000: episode: 1290, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -12.284, mean reward: -0.123 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.825, 10.098], loss: 0.002417, mae: 0.049043, mean_q: -0.309332
 71551/100000: episode: 1291, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.132, mean reward: -0.181 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.899, 10.144], loss: 0.002481, mae: 0.048951, mean_q: -0.323689
 71651/100000: episode: 1292, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -14.223, mean reward: -0.142 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.979, 10.098], loss: 0.002768, mae: 0.052206, mean_q: -0.280082
 71751/100000: episode: 1293, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -20.567, mean reward: -0.206 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.958, 10.101], loss: 0.002645, mae: 0.050057, mean_q: -0.341760
 71851/100000: episode: 1294, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.152, mean reward: -0.182 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.477, 10.223], loss: 0.002531, mae: 0.051172, mean_q: -0.309737
 71951/100000: episode: 1295, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.837, mean reward: -0.168 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.758, 10.286], loss: 0.002669, mae: 0.050913, mean_q: -0.335220
[Info] 100-TH LEVEL FOUND: 0.7009078860282898, Considering 10/90 traces
 72051/100000: episode: 1296, duration: 4.439s, episode steps: 100, steps per second: 23, episode reward: -14.160, mean reward: -0.142 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.600, 10.385], loss: 0.002636, mae: 0.051644, mean_q: -0.318053
 72061/100000: episode: 1297, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 4.064, mean reward: 0.406 [0.335, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.156, 10.100], loss: 0.002932, mae: 0.054721, mean_q: -0.296315
 72079/100000: episode: 1298, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 6.197, mean reward: 0.344 [0.256, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.467, 10.414], loss: 0.002383, mae: 0.049651, mean_q: -0.348374
 72107/100000: episode: 1299, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 12.606, mean reward: 0.450 [0.316, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.613, 10.100], loss: 0.002484, mae: 0.050362, mean_q: -0.287334
 72158/100000: episode: 1300, duration: 0.265s, episode steps: 51, steps per second: 193, episode reward: 11.999, mean reward: 0.235 [0.022, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.525, 10.235], loss: 0.002653, mae: 0.050713, mean_q: -0.332159
 72205/100000: episode: 1301, duration: 0.231s, episode steps: 47, steps per second: 203, episode reward: 11.933, mean reward: 0.254 [0.090, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.745, 10.235], loss: 0.002521, mae: 0.050246, mean_q: -0.277962
 72256/100000: episode: 1302, duration: 0.275s, episode steps: 51, steps per second: 185, episode reward: 11.630, mean reward: 0.228 [0.080, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.417, 10.373], loss: 0.002434, mae: 0.048916, mean_q: -0.338695
 72278/100000: episode: 1303, duration: 0.135s, episode steps: 22, steps per second: 163, episode reward: 6.758, mean reward: 0.307 [0.183, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.298, 10.100], loss: 0.002401, mae: 0.050175, mean_q: -0.228618
 72291/100000: episode: 1304, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 3.631, mean reward: 0.279 [0.194, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.136, 10.100], loss: 0.002959, mae: 0.055368, mean_q: -0.282150
 72301/100000: episode: 1305, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 3.146, mean reward: 0.315 [0.255, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.295, 10.100], loss: 0.002803, mae: 0.054424, mean_q: -0.155559
 72314/100000: episode: 1306, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 3.522, mean reward: 0.271 [0.155, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.261, 10.100], loss: 0.002039, mae: 0.045894, mean_q: -0.313805
 72361/100000: episode: 1307, duration: 0.252s, episode steps: 47, steps per second: 186, episode reward: 16.558, mean reward: 0.352 [0.177, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.899, 10.374], loss: 0.002423, mae: 0.048978, mean_q: -0.289187
 72383/100000: episode: 1308, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 8.544, mean reward: 0.388 [0.265, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.534, 10.100], loss: 0.002959, mae: 0.054111, mean_q: -0.210339
 72401/100000: episode: 1309, duration: 0.100s, episode steps: 18, steps per second: 179, episode reward: 5.652, mean reward: 0.314 [0.236, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.163, 10.378], loss: 0.002424, mae: 0.049841, mean_q: -0.226457
 72419/100000: episode: 1310, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 6.019, mean reward: 0.334 [0.253, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.495], loss: 0.002343, mae: 0.048274, mean_q: -0.311441
 72441/100000: episode: 1311, duration: 0.130s, episode steps: 22, steps per second: 170, episode reward: 10.464, mean reward: 0.476 [0.273, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-1.931, 10.100], loss: 0.002487, mae: 0.051514, mean_q: -0.229096
 72454/100000: episode: 1312, duration: 0.086s, episode steps: 13, steps per second: 152, episode reward: 4.062, mean reward: 0.312 [0.214, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.274, 10.100], loss: 0.002520, mae: 0.049659, mean_q: -0.303451
 72467/100000: episode: 1313, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 3.616, mean reward: 0.278 [0.226, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.288, 10.100], loss: 0.002757, mae: 0.053936, mean_q: -0.281420
 72495/100000: episode: 1314, duration: 0.141s, episode steps: 28, steps per second: 198, episode reward: 8.236, mean reward: 0.294 [0.112, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.394, 10.100], loss: 0.003361, mae: 0.060443, mean_q: -0.244551
 72508/100000: episode: 1315, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 4.683, mean reward: 0.360 [0.296, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.890, 10.100], loss: 0.003755, mae: 0.065259, mean_q: -0.227498
 72538/100000: episode: 1316, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 10.277, mean reward: 0.343 [0.245, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.698, 10.100], loss: 0.002812, mae: 0.056673, mean_q: -0.214749
 72551/100000: episode: 1317, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 5.198, mean reward: 0.400 [0.361, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.382, 10.100], loss: 0.003574, mae: 0.062652, mean_q: -0.206108
 72573/100000: episode: 1318, duration: 0.132s, episode steps: 22, steps per second: 166, episode reward: 6.729, mean reward: 0.306 [0.228, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-1.015, 10.100], loss: 0.002810, mae: 0.055097, mean_q: -0.258427
 72603/100000: episode: 1319, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 5.819, mean reward: 0.194 [0.036, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.856, 10.100], loss: 0.002472, mae: 0.049668, mean_q: -0.241245
 72631/100000: episode: 1320, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 10.850, mean reward: 0.387 [0.328, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.553, 10.100], loss: 0.002425, mae: 0.048236, mean_q: -0.263690
 72644/100000: episode: 1321, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 4.268, mean reward: 0.328 [0.238, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.853, 10.100], loss: 0.002339, mae: 0.048352, mean_q: -0.293345
 72657/100000: episode: 1322, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 4.746, mean reward: 0.365 [0.301, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.428, 10.100], loss: 0.002806, mae: 0.051427, mean_q: -0.228001
 72673/100000: episode: 1323, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 5.553, mean reward: 0.347 [0.212, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.289, 10.100], loss: 0.002872, mae: 0.054026, mean_q: -0.139597
 72686/100000: episode: 1324, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 4.673, mean reward: 0.359 [0.312, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.358, 10.100], loss: 0.002619, mae: 0.052749, mean_q: -0.185656
 72704/100000: episode: 1325, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 8.452, mean reward: 0.470 [0.320, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.614], loss: 0.002341, mae: 0.046881, mean_q: -0.235080
 72751/100000: episode: 1326, duration: 0.251s, episode steps: 47, steps per second: 187, episode reward: 12.509, mean reward: 0.266 [0.110, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.292, 10.432], loss: 0.002539, mae: 0.049766, mean_q: -0.182714
 72767/100000: episode: 1327, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 4.716, mean reward: 0.295 [0.172, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.121, 10.100], loss: 0.002298, mae: 0.049385, mean_q: -0.176412
 72814/100000: episode: 1328, duration: 0.255s, episode steps: 47, steps per second: 184, episode reward: 13.080, mean reward: 0.278 [0.193, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.403, 10.258], loss: 0.002577, mae: 0.051133, mean_q: -0.184890
[Info] FALSIFICATION!
 72819/100000: episode: 1329, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 11.810, mean reward: 2.362 [0.315, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.583 [-0.289, 6.903], loss: 0.002534, mae: 0.048541, mean_q: -0.235177
 72919/100000: episode: 1330, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -21.075, mean reward: -0.211 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.491, 10.098], loss: 0.016692, mae: 0.063787, mean_q: -0.171573
 73019/100000: episode: 1331, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.449, mean reward: -0.154 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.437, 10.324], loss: 0.002787, mae: 0.052606, mean_q: -0.201191
 73119/100000: episode: 1332, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.001, mean reward: -0.150 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.219, 10.098], loss: 0.002791, mae: 0.053116, mean_q: -0.154607
 73219/100000: episode: 1333, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.181, mean reward: -0.182 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.726, 10.098], loss: 0.002696, mae: 0.052706, mean_q: -0.149640
 73319/100000: episode: 1334, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -15.902, mean reward: -0.159 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.752, 10.232], loss: 0.002423, mae: 0.049302, mean_q: -0.202280
 73419/100000: episode: 1335, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.860, mean reward: -0.179 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.146, 10.187], loss: 0.015714, mae: 0.058508, mean_q: -0.149600
 73519/100000: episode: 1336, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.443, mean reward: -0.174 [-1.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.827, 10.321], loss: 0.003305, mae: 0.058014, mean_q: -0.183287
 73619/100000: episode: 1337, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.873, mean reward: -0.179 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.475, 10.098], loss: 0.016416, mae: 0.065249, mean_q: -0.182157
 73719/100000: episode: 1338, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.234, mean reward: -0.162 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.280, 10.256], loss: 0.015757, mae: 0.061042, mean_q: -0.201484
 73819/100000: episode: 1339, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -18.276, mean reward: -0.183 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.138, 10.098], loss: 0.002557, mae: 0.050812, mean_q: -0.172507
 73919/100000: episode: 1340, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -13.881, mean reward: -0.139 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.329, 10.098], loss: 0.015746, mae: 0.059907, mean_q: -0.169544
 74019/100000: episode: 1341, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.792, mean reward: -0.188 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.241, 10.107], loss: 0.002780, mae: 0.053286, mean_q: -0.174699
 74119/100000: episode: 1342, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -15.827, mean reward: -0.158 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.368, 10.098], loss: 0.016413, mae: 0.065990, mean_q: -0.170707
 74219/100000: episode: 1343, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.983, mean reward: -0.180 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.684, 10.098], loss: 0.040106, mae: 0.090082, mean_q: -0.202563
 74319/100000: episode: 1344, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.794, mean reward: -0.148 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.544, 10.098], loss: 0.002577, mae: 0.052531, mean_q: -0.176013
 74419/100000: episode: 1345, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.040, mean reward: -0.190 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.038, 10.187], loss: 0.015753, mae: 0.065019, mean_q: -0.151909
 74519/100000: episode: 1346, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -20.375, mean reward: -0.204 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.620, 10.110], loss: 0.015138, mae: 0.060001, mean_q: -0.179765
 74619/100000: episode: 1347, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.333, mean reward: -0.163 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.536, 10.098], loss: 0.002746, mae: 0.052845, mean_q: -0.181378
 74719/100000: episode: 1348, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -11.171, mean reward: -0.112 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.750, 10.317], loss: 0.040147, mae: 0.084008, mean_q: -0.182315
 74819/100000: episode: 1349, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.632, mean reward: -0.156 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.368, 10.098], loss: 0.003452, mae: 0.060119, mean_q: -0.188978
 74919/100000: episode: 1350, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -12.506, mean reward: -0.125 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.493, 10.098], loss: 0.002859, mae: 0.054233, mean_q: -0.182749
 75019/100000: episode: 1351, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.884, mean reward: -0.189 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.959, 10.183], loss: 0.027803, mae: 0.070386, mean_q: -0.158139
 75119/100000: episode: 1352, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.554, mean reward: -0.176 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.246, 10.098], loss: 0.002669, mae: 0.051625, mean_q: -0.181883
 75219/100000: episode: 1353, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.230, mean reward: -0.162 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.768, 10.098], loss: 0.002601, mae: 0.050567, mean_q: -0.199162
 75319/100000: episode: 1354, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.969, mean reward: -0.160 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.212, 10.463], loss: 0.002563, mae: 0.050268, mean_q: -0.155438
 75419/100000: episode: 1355, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.152, mean reward: -0.142 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.594, 10.101], loss: 0.028991, mae: 0.078851, mean_q: -0.153285
 75519/100000: episode: 1356, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.402, mean reward: -0.154 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.701, 10.250], loss: 0.002564, mae: 0.051449, mean_q: -0.158439
 75619/100000: episode: 1357, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.880, mean reward: -0.169 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.424, 10.098], loss: 0.002631, mae: 0.051480, mean_q: -0.177558
 75719/100000: episode: 1358, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.721, mean reward: -0.157 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.903, 10.142], loss: 0.002650, mae: 0.051200, mean_q: -0.188163
 75819/100000: episode: 1359, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.545, mean reward: -0.195 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.294, 10.098], loss: 0.002593, mae: 0.050595, mean_q: -0.185693
 75919/100000: episode: 1360, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.550, mean reward: -0.176 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.200, 10.240], loss: 0.027722, mae: 0.071341, mean_q: -0.189104
 76019/100000: episode: 1361, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.259, mean reward: -0.163 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.216, 10.273], loss: 0.004230, mae: 0.062365, mean_q: -0.199609
 76119/100000: episode: 1362, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.865, mean reward: -0.189 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.918, 10.183], loss: 0.015426, mae: 0.065450, mean_q: -0.155065
 76219/100000: episode: 1363, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.591, mean reward: -0.156 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.156, 10.098], loss: 0.002462, mae: 0.049957, mean_q: -0.196741
 76319/100000: episode: 1364, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -20.197, mean reward: -0.202 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.850, 10.282], loss: 0.002625, mae: 0.051536, mean_q: -0.183935
 76419/100000: episode: 1365, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.583, mean reward: -0.166 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.695, 10.126], loss: 0.002754, mae: 0.052609, mean_q: -0.157721
 76519/100000: episode: 1366, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.453, mean reward: -0.185 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.009, 10.098], loss: 0.002551, mae: 0.051861, mean_q: -0.165017
 76619/100000: episode: 1367, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -13.975, mean reward: -0.140 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.363, 10.294], loss: 0.002644, mae: 0.052422, mean_q: -0.143308
 76719/100000: episode: 1368, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.957, mean reward: -0.190 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.471, 10.146], loss: 0.002505, mae: 0.050386, mean_q: -0.180922
 76819/100000: episode: 1369, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -7.805, mean reward: -0.078 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.447, 10.404], loss: 0.002428, mae: 0.049368, mean_q: -0.185870
 76919/100000: episode: 1370, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -13.614, mean reward: -0.136 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.845, 10.098], loss: 0.028257, mae: 0.078210, mean_q: -0.152875
 77019/100000: episode: 1371, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.437, mean reward: -0.174 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.122, 10.294], loss: 0.004000, mae: 0.065352, mean_q: -0.189016
 77119/100000: episode: 1372, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -12.881, mean reward: -0.129 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.672, 10.098], loss: 0.002475, mae: 0.049852, mean_q: -0.203363
 77219/100000: episode: 1373, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.192, mean reward: -0.172 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.465, 10.098], loss: 0.015603, mae: 0.066736, mean_q: -0.182889
 77319/100000: episode: 1374, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.986, mean reward: -0.170 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.889, 10.118], loss: 0.002515, mae: 0.049998, mean_q: -0.217024
 77419/100000: episode: 1375, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.228, mean reward: -0.172 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.720, 10.167], loss: 0.014932, mae: 0.059735, mean_q: -0.256921
 77519/100000: episode: 1376, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.800, mean reward: -0.178 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.828, 10.186], loss: 0.002832, mae: 0.052878, mean_q: -0.275642
 77619/100000: episode: 1377, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -14.968, mean reward: -0.150 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.561, 10.333], loss: 0.002440, mae: 0.048929, mean_q: -0.281078
 77719/100000: episode: 1378, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.336, mean reward: -0.183 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.664, 10.098], loss: 0.002394, mae: 0.048082, mean_q: -0.310652
 77819/100000: episode: 1379, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -15.787, mean reward: -0.158 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.396, 10.162], loss: 0.015196, mae: 0.064549, mean_q: -0.321529
 77919/100000: episode: 1380, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.813, mean reward: -0.128 [-1.000, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.703, 10.098], loss: 0.002499, mae: 0.049454, mean_q: -0.330085
 78019/100000: episode: 1381, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.270, mean reward: -0.173 [-1.000, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.416, 10.201], loss: 0.002630, mae: 0.051244, mean_q: -0.309379
 78119/100000: episode: 1382, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.163, mean reward: -0.182 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.802, 10.264], loss: 0.002498, mae: 0.049844, mean_q: -0.372610
 78219/100000: episode: 1383, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.534, mean reward: -0.165 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.368, 10.364], loss: 0.002416, mae: 0.048826, mean_q: -0.317554
 78319/100000: episode: 1384, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.744, mean reward: -0.177 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.381, 10.098], loss: 0.002534, mae: 0.050369, mean_q: -0.284937
 78419/100000: episode: 1385, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.337, mean reward: -0.143 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.701, 10.239], loss: 0.002589, mae: 0.050490, mean_q: -0.308138
 78519/100000: episode: 1386, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.792, mean reward: -0.178 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.746, 10.184], loss: 0.002533, mae: 0.049715, mean_q: -0.326594
 78619/100000: episode: 1387, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.147, mean reward: -0.151 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.907, 10.098], loss: 0.002540, mae: 0.049491, mean_q: -0.324009
 78719/100000: episode: 1388, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.162, mean reward: -0.162 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.075, 10.103], loss: 0.002640, mae: 0.050100, mean_q: -0.319336
 78819/100000: episode: 1389, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.267, mean reward: -0.163 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.960, 10.098], loss: 0.002657, mae: 0.050686, mean_q: -0.283044
 78919/100000: episode: 1390, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -13.731, mean reward: -0.137 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.786, 10.098], loss: 0.002708, mae: 0.052410, mean_q: -0.295779
 79019/100000: episode: 1391, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -15.613, mean reward: -0.156 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.721, 10.300], loss: 0.002343, mae: 0.048035, mean_q: -0.320926
 79119/100000: episode: 1392, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.022, mean reward: -0.160 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.678, 10.213], loss: 0.002536, mae: 0.050567, mean_q: -0.307932
 79219/100000: episode: 1393, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.277, mean reward: -0.133 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.382, 10.406], loss: 0.002358, mae: 0.048328, mean_q: -0.322188
 79319/100000: episode: 1394, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.993, mean reward: -0.170 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.614, 10.141], loss: 0.002670, mae: 0.050274, mean_q: -0.316119
 79419/100000: episode: 1395, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.944, mean reward: -0.159 [-1.000, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.324, 10.098], loss: 0.002498, mae: 0.049740, mean_q: -0.327808
 79519/100000: episode: 1396, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.334, mean reward: -0.193 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.047, 10.258], loss: 0.002647, mae: 0.052925, mean_q: -0.277672
 79619/100000: episode: 1397, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -14.250, mean reward: -0.143 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.938, 10.309], loss: 0.002652, mae: 0.051572, mean_q: -0.267866
 79719/100000: episode: 1398, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.386, mean reward: -0.174 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.332, 10.206], loss: 0.002671, mae: 0.051680, mean_q: -0.301564
 79819/100000: episode: 1399, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.202, mean reward: -0.182 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.386, 10.098], loss: 0.002664, mae: 0.051215, mean_q: -0.303751
 79919/100000: episode: 1400, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -17.981, mean reward: -0.180 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.224, 10.098], loss: 0.002460, mae: 0.049482, mean_q: -0.354176
 80019/100000: episode: 1401, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.811, mean reward: -0.158 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.482, 10.098], loss: 0.002497, mae: 0.049392, mean_q: -0.349385
 80119/100000: episode: 1402, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -17.060, mean reward: -0.171 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.438, 10.098], loss: 0.005132, mae: 0.066795, mean_q: -0.294509
 80219/100000: episode: 1403, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -16.406, mean reward: -0.164 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.817, 10.098], loss: 0.003125, mae: 0.058686, mean_q: -0.297961
 80319/100000: episode: 1404, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.380, mean reward: -0.184 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.053, 10.301], loss: 0.002595, mae: 0.050315, mean_q: -0.316283
 80419/100000: episode: 1405, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.747, mean reward: -0.187 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.472, 10.245], loss: 0.002484, mae: 0.049494, mean_q: -0.318198
 80519/100000: episode: 1406, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.770, mean reward: -0.168 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.510, 10.225], loss: 0.002469, mae: 0.048928, mean_q: -0.303747
 80619/100000: episode: 1407, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -13.505, mean reward: -0.135 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.085, 10.098], loss: 0.002481, mae: 0.048286, mean_q: -0.305398
 80719/100000: episode: 1408, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.393, mean reward: -0.194 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.559, 10.098], loss: 0.002306, mae: 0.047404, mean_q: -0.291352
 80819/100000: episode: 1409, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -10.915, mean reward: -0.109 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.765, 10.098], loss: 0.002329, mae: 0.047902, mean_q: -0.287147
 80919/100000: episode: 1410, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.573, mean reward: -0.196 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.372, 10.387], loss: 0.002435, mae: 0.048100, mean_q: -0.337922
 81019/100000: episode: 1411, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.348, mean reward: -0.173 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.037, 10.210], loss: 0.002560, mae: 0.049427, mean_q: -0.313716
 81119/100000: episode: 1412, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.553, mean reward: -0.176 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.485, 10.162], loss: 0.002381, mae: 0.048156, mean_q: -0.317652
 81219/100000: episode: 1413, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.193, mean reward: -0.152 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.343, 10.265], loss: 0.002472, mae: 0.049519, mean_q: -0.307011
 81319/100000: episode: 1414, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.417, mean reward: -0.174 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.694, 10.122], loss: 0.002408, mae: 0.048362, mean_q: -0.284093
 81419/100000: episode: 1415, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.903, mean reward: -0.189 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.349, 10.267], loss: 0.002438, mae: 0.048281, mean_q: -0.304974
 81519/100000: episode: 1416, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.142, mean reward: -0.181 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.703, 10.098], loss: 0.002244, mae: 0.045786, mean_q: -0.303865
 81619/100000: episode: 1417, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.392, mean reward: -0.184 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.688, 10.098], loss: 0.002367, mae: 0.047729, mean_q: -0.301651
 81719/100000: episode: 1418, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.256, mean reward: -0.173 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.088, 10.098], loss: 0.002284, mae: 0.046295, mean_q: -0.320630
 81819/100000: episode: 1419, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.068, mean reward: -0.171 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.440, 10.214], loss: 0.002499, mae: 0.048532, mean_q: -0.295744
 81919/100000: episode: 1420, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.918, mean reward: -0.169 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.270, 10.362], loss: 0.002569, mae: 0.049737, mean_q: -0.349356
 82019/100000: episode: 1421, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -20.262, mean reward: -0.203 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.471, 10.098], loss: 0.002482, mae: 0.048856, mean_q: -0.326150
 82119/100000: episode: 1422, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.901, mean reward: -0.159 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.514, 10.098], loss: 0.002433, mae: 0.048080, mean_q: -0.286089
 82219/100000: episode: 1423, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -14.591, mean reward: -0.146 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.159, 10.481], loss: 0.002357, mae: 0.048439, mean_q: -0.340257
 82319/100000: episode: 1424, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.699, mean reward: -0.187 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.719, 10.210], loss: 0.002608, mae: 0.050596, mean_q: -0.303570
 82419/100000: episode: 1425, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -10.244, mean reward: -0.102 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.339, 10.415], loss: 0.002485, mae: 0.048818, mean_q: -0.329675
 82519/100000: episode: 1426, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.290, mean reward: -0.193 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.050, 10.098], loss: 0.002309, mae: 0.048010, mean_q: -0.324949
 82619/100000: episode: 1427, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.558, mean reward: -0.176 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.209, 10.098], loss: 0.002471, mae: 0.048465, mean_q: -0.328571
 82719/100000: episode: 1428, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.490, mean reward: -0.185 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.694, 10.098], loss: 0.002414, mae: 0.047771, mean_q: -0.318569
[Info] 100-TH LEVEL FOUND: 0.6370009779930115, Considering 10/90 traces
 82819/100000: episode: 1429, duration: 4.368s, episode steps: 100, steps per second: 23, episode reward: -15.119, mean reward: -0.151 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.510, 10.192], loss: 0.002430, mae: 0.048037, mean_q: -0.313726
 82886/100000: episode: 1430, duration: 0.350s, episode steps: 67, steps per second: 191, episode reward: 10.167, mean reward: 0.152 [0.017, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.752 [-0.759, 10.100], loss: 0.002369, mae: 0.047362, mean_q: -0.305977
 82931/100000: episode: 1431, duration: 0.241s, episode steps: 45, steps per second: 187, episode reward: 12.702, mean reward: 0.282 [0.024, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.639, 10.173], loss: 0.002422, mae: 0.048072, mean_q: -0.306154
 82976/100000: episode: 1432, duration: 0.236s, episode steps: 45, steps per second: 191, episode reward: 11.590, mean reward: 0.258 [0.120, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.176, 10.552], loss: 0.002210, mae: 0.045561, mean_q: -0.352549
 83018/100000: episode: 1433, duration: 0.222s, episode steps: 42, steps per second: 190, episode reward: 12.749, mean reward: 0.304 [0.108, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.193, 10.279], loss: 0.002324, mae: 0.047039, mean_q: -0.325432
 83085/100000: episode: 1434, duration: 0.335s, episode steps: 67, steps per second: 200, episode reward: 14.569, mean reward: 0.217 [0.116, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.761 [-0.776, 10.284], loss: 0.002321, mae: 0.046590, mean_q: -0.301919
 83127/100000: episode: 1435, duration: 0.227s, episode steps: 42, steps per second: 185, episode reward: 13.463, mean reward: 0.321 [0.200, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.390, 10.495], loss: 0.002145, mae: 0.046069, mean_q: -0.293603
 83169/100000: episode: 1436, duration: 0.238s, episode steps: 42, steps per second: 176, episode reward: 14.136, mean reward: 0.337 [0.144, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.205, 10.242], loss: 0.002586, mae: 0.050394, mean_q: -0.302617
 83197/100000: episode: 1437, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 9.946, mean reward: 0.355 [0.112, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-1.779, 10.100], loss: 0.002585, mae: 0.053027, mean_q: -0.277396
 83226/100000: episode: 1438, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 8.752, mean reward: 0.302 [0.159, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.710, 10.253], loss: 0.002784, mae: 0.052582, mean_q: -0.254694
 83255/100000: episode: 1439, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 9.510, mean reward: 0.328 [0.228, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.578, 10.324], loss: 0.002474, mae: 0.049306, mean_q: -0.255460
 83285/100000: episode: 1440, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 11.059, mean reward: 0.369 [0.230, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.289, 10.100], loss: 0.002528, mae: 0.051605, mean_q: -0.180519
 83314/100000: episode: 1441, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 8.652, mean reward: 0.298 [0.128, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.035, 10.555], loss: 0.002433, mae: 0.049955, mean_q: -0.205943
 83325/100000: episode: 1442, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 4.536, mean reward: 0.412 [0.349, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.443, 10.470], loss: 0.002268, mae: 0.046743, mean_q: -0.323925
 83354/100000: episode: 1443, duration: 0.150s, episode steps: 29, steps per second: 194, episode reward: 12.527, mean reward: 0.432 [0.259, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.478, 10.419], loss: 0.002799, mae: 0.053623, mean_q: -0.249412
 83365/100000: episode: 1444, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 4.151, mean reward: 0.377 [0.315, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.489], loss: 0.002363, mae: 0.050911, mean_q: -0.188511
 83394/100000: episode: 1445, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 9.299, mean reward: 0.321 [0.200, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.060, 10.327], loss: 0.002651, mae: 0.052097, mean_q: -0.215183
 83439/100000: episode: 1446, duration: 0.262s, episode steps: 45, steps per second: 172, episode reward: 10.346, mean reward: 0.230 [0.006, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.610, 10.112], loss: 0.002345, mae: 0.047028, mean_q: -0.222780
 83458/100000: episode: 1447, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 4.788, mean reward: 0.252 [0.175, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.314, 10.100], loss: 0.002691, mae: 0.052962, mean_q: -0.121152
 83486/100000: episode: 1448, duration: 0.165s, episode steps: 28, steps per second: 170, episode reward: 8.635, mean reward: 0.308 [0.203, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.685, 10.100], loss: 0.002490, mae: 0.049269, mean_q: -0.168370
 83554/100000: episode: 1449, duration: 0.358s, episode steps: 68, steps per second: 190, episode reward: 15.662, mean reward: 0.230 [0.081, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.747 [-0.398, 10.100], loss: 0.002806, mae: 0.053278, mean_q: -0.174331
 83599/100000: episode: 1450, duration: 0.225s, episode steps: 45, steps per second: 200, episode reward: 8.905, mean reward: 0.198 [0.008, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-1.724, 10.100], loss: 0.002706, mae: 0.052104, mean_q: -0.159618
 83627/100000: episode: 1451, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 12.363, mean reward: 0.442 [0.314, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.359, 10.100], loss: 0.002807, mae: 0.052615, mean_q: -0.147984
 83655/100000: episode: 1452, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 10.957, mean reward: 0.391 [0.269, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.196, 10.100], loss: 0.002668, mae: 0.050669, mean_q: -0.153025
 83666/100000: episode: 1453, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 3.822, mean reward: 0.347 [0.288, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.406], loss: 0.002716, mae: 0.050259, mean_q: -0.191941
 83678/100000: episode: 1454, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 5.924, mean reward: 0.494 [0.371, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.712], loss: 0.002604, mae: 0.052692, mean_q: -0.152554
 83706/100000: episode: 1455, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 10.800, mean reward: 0.386 [0.231, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.113, 10.100], loss: 0.002557, mae: 0.051574, mean_q: -0.105973
 83774/100000: episode: 1456, duration: 0.350s, episode steps: 68, steps per second: 194, episode reward: 25.273, mean reward: 0.372 [0.218, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.738 [-1.165, 10.100], loss: 0.002803, mae: 0.053504, mean_q: -0.173504
 83841/100000: episode: 1457, duration: 0.361s, episode steps: 67, steps per second: 186, episode reward: 12.982, mean reward: 0.194 [0.027, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.758 [-0.715, 10.200], loss: 0.002984, mae: 0.055337, mean_q: -0.110685
 83908/100000: episode: 1458, duration: 0.367s, episode steps: 67, steps per second: 183, episode reward: 11.992, mean reward: 0.179 [0.012, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.771 [-0.304, 10.403], loss: 0.002613, mae: 0.051306, mean_q: -0.121311
 83953/100000: episode: 1459, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 10.316, mean reward: 0.229 [0.013, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.207, 10.146], loss: 0.002495, mae: 0.050822, mean_q: -0.111294
 84021/100000: episode: 1460, duration: 0.391s, episode steps: 68, steps per second: 174, episode reward: 14.877, mean reward: 0.219 [0.022, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.761 [-0.612, 10.100], loss: 0.002507, mae: 0.050792, mean_q: -0.117243
 84088/100000: episode: 1461, duration: 0.367s, episode steps: 67, steps per second: 183, episode reward: 12.666, mean reward: 0.189 [0.033, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.759 [-0.998, 10.100], loss: 0.002805, mae: 0.053749, mean_q: -0.128414
 84116/100000: episode: 1462, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 9.674, mean reward: 0.345 [0.272, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.931, 10.100], loss: 0.002460, mae: 0.051957, mean_q: -0.142281
 84145/100000: episode: 1463, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 6.758, mean reward: 0.233 [0.095, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-1.141, 10.294], loss: 0.003115, mae: 0.055941, mean_q: -0.127841
 84175/100000: episode: 1464, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 8.596, mean reward: 0.287 [0.063, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.054, 10.100], loss: 0.002631, mae: 0.052633, mean_q: -0.100781
 84203/100000: episode: 1465, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 8.166, mean reward: 0.292 [0.153, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.371, 10.100], loss: 0.003203, mae: 0.057495, mean_q: -0.089971
 84245/100000: episode: 1466, duration: 0.243s, episode steps: 42, steps per second: 173, episode reward: 16.255, mean reward: 0.387 [0.244, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.284, 10.349], loss: 0.003318, mae: 0.058658, mean_q: -0.029711
 84313/100000: episode: 1467, duration: 0.364s, episode steps: 68, steps per second: 187, episode reward: 16.889, mean reward: 0.248 [0.091, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.748 [-0.483, 10.100], loss: 0.002696, mae: 0.053768, mean_q: -0.038711
 84358/100000: episode: 1468, duration: 0.237s, episode steps: 45, steps per second: 190, episode reward: 11.012, mean reward: 0.245 [0.118, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.471, 10.395], loss: 0.004501, mae: 0.064870, mean_q: -0.078784
 84388/100000: episode: 1469, duration: 0.160s, episode steps: 30, steps per second: 188, episode reward: 9.653, mean reward: 0.322 [0.164, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.367, 10.100], loss: 0.002704, mae: 0.054279, mean_q: -0.070017
 84416/100000: episode: 1470, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 7.829, mean reward: 0.280 [0.125, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.137, 10.100], loss: 0.003278, mae: 0.058932, mean_q: -0.018886
 84445/100000: episode: 1471, duration: 0.168s, episode steps: 29, steps per second: 172, episode reward: 10.202, mean reward: 0.352 [0.213, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.035, 10.610], loss: 0.002623, mae: 0.052406, mean_q: -0.033710
 84473/100000: episode: 1472, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 6.326, mean reward: 0.226 [0.025, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.762, 10.100], loss: 0.002988, mae: 0.056924, mean_q: -0.070607
 84501/100000: episode: 1473, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 12.624, mean reward: 0.451 [0.311, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.276, 10.100], loss: 0.002811, mae: 0.056609, mean_q: -0.036468
 84513/100000: episode: 1474, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 5.487, mean reward: 0.457 [0.346, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-1.050, 10.520], loss: 0.002576, mae: 0.051035, mean_q: -0.058790
 84555/100000: episode: 1475, duration: 0.223s, episode steps: 42, steps per second: 188, episode reward: 12.575, mean reward: 0.299 [0.123, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.686, 10.274], loss: 0.002831, mae: 0.054751, mean_q: 0.014921
 84622/100000: episode: 1476, duration: 0.353s, episode steps: 67, steps per second: 190, episode reward: 11.418, mean reward: 0.170 [0.005, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.749 [-0.761, 10.100], loss: 0.002767, mae: 0.053577, mean_q: 0.013416
 84664/100000: episode: 1477, duration: 0.213s, episode steps: 42, steps per second: 197, episode reward: 12.535, mean reward: 0.298 [0.143, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.549, 10.338], loss: 0.002740, mae: 0.053748, mean_q: -0.030253
 84693/100000: episode: 1478, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 10.447, mean reward: 0.360 [0.278, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.187, 10.406], loss: 0.002829, mae: 0.054265, mean_q: 0.010399
 84721/100000: episode: 1479, duration: 0.168s, episode steps: 28, steps per second: 167, episode reward: 8.336, mean reward: 0.298 [0.203, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.104, 10.100], loss: 0.002998, mae: 0.057028, mean_q: 0.044617
 84766/100000: episode: 1480, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 12.866, mean reward: 0.286 [0.022, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.513, 10.100], loss: 0.003110, mae: 0.056517, mean_q: 0.030725
 84777/100000: episode: 1481, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 4.862, mean reward: 0.442 [0.360, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.243, 10.498], loss: 0.002549, mae: 0.052732, mean_q: 0.039260
 84822/100000: episode: 1482, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 10.189, mean reward: 0.226 [0.037, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.196, 10.322], loss: 0.002401, mae: 0.050923, mean_q: 0.008665
 84851/100000: episode: 1483, duration: 0.155s, episode steps: 29, steps per second: 188, episode reward: 7.182, mean reward: 0.248 [0.155, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.621, 10.406], loss: 0.003397, mae: 0.061223, mean_q: 0.072366
 84880/100000: episode: 1484, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 10.124, mean reward: 0.349 [0.247, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.613, 10.429], loss: 0.002918, mae: 0.056033, mean_q: 0.064132
 84899/100000: episode: 1485, duration: 0.120s, episode steps: 19, steps per second: 158, episode reward: 5.812, mean reward: 0.306 [0.221, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.751, 10.100], loss: 0.002702, mae: 0.054571, mean_q: 0.007168
 84918/100000: episode: 1486, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 5.201, mean reward: 0.274 [0.189, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.384, 10.100], loss: 0.002821, mae: 0.055355, mean_q: 0.064051
 84985/100000: episode: 1487, duration: 0.352s, episode steps: 67, steps per second: 190, episode reward: 10.707, mean reward: 0.160 [0.017, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.756 [-0.464, 10.100], loss: 0.002977, mae: 0.056260, mean_q: 0.076953
 85052/100000: episode: 1488, duration: 0.327s, episode steps: 67, steps per second: 205, episode reward: 11.754, mean reward: 0.175 [0.006, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.764 [-0.695, 10.100], loss: 0.003015, mae: 0.057097, mean_q: 0.076803
 85071/100000: episode: 1489, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 5.032, mean reward: 0.265 [0.195, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.292, 10.100], loss: 0.002851, mae: 0.055019, mean_q: 0.056526
 85101/100000: episode: 1490, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 9.541, mean reward: 0.318 [0.159, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.425, 10.100], loss: 0.002672, mae: 0.052770, mean_q: 0.085108
 85146/100000: episode: 1491, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 15.283, mean reward: 0.340 [0.210, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.381, 10.353], loss: 0.002858, mae: 0.054721, mean_q: 0.062187
 85158/100000: episode: 1492, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 6.115, mean reward: 0.510 [0.427, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.687, 10.563], loss: 0.003465, mae: 0.060022, mean_q: 0.076910
 85187/100000: episode: 1493, duration: 0.157s, episode steps: 29, steps per second: 184, episode reward: 9.139, mean reward: 0.315 [0.254, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.035, 10.401], loss: 0.003147, mae: 0.059606, mean_q: 0.095572
 85199/100000: episode: 1494, duration: 0.068s, episode steps: 12, steps per second: 178, episode reward: 6.360, mean reward: 0.530 [0.405, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.542], loss: 0.003354, mae: 0.061178, mean_q: 0.164799
 85267/100000: episode: 1495, duration: 0.360s, episode steps: 68, steps per second: 189, episode reward: 17.609, mean reward: 0.259 [0.032, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.755 [-0.419, 10.100], loss: 0.003220, mae: 0.059183, mean_q: 0.125140
 85312/100000: episode: 1496, duration: 0.240s, episode steps: 45, steps per second: 188, episode reward: 11.560, mean reward: 0.257 [0.019, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.751, 10.293], loss: 0.003155, mae: 0.057472, mean_q: 0.096468
 85323/100000: episode: 1497, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 4.513, mean reward: 0.410 [0.362, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.537], loss: 0.003531, mae: 0.060910, mean_q: 0.117255
 85352/100000: episode: 1498, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 9.283, mean reward: 0.320 [0.148, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.844, 10.260], loss: 0.003338, mae: 0.059674, mean_q: 0.125146
 85382/100000: episode: 1499, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 11.411, mean reward: 0.380 [0.191, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-1.083, 10.100], loss: 0.003220, mae: 0.059023, mean_q: 0.089683
 85449/100000: episode: 1500, duration: 0.365s, episode steps: 67, steps per second: 184, episode reward: 10.064, mean reward: 0.150 [0.016, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.759 [-0.559, 10.113], loss: 0.003029, mae: 0.056667, mean_q: 0.143054
 85494/100000: episode: 1501, duration: 0.244s, episode steps: 45, steps per second: 184, episode reward: 13.777, mean reward: 0.306 [0.178, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.483, 10.328], loss: 0.002973, mae: 0.057697, mean_q: 0.146797
 85524/100000: episode: 1502, duration: 0.159s, episode steps: 30, steps per second: 188, episode reward: 8.615, mean reward: 0.287 [0.158, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.331, 10.100], loss: 0.003215, mae: 0.057974, mean_q: 0.129801
 85591/100000: episode: 1503, duration: 0.373s, episode steps: 67, steps per second: 179, episode reward: 16.604, mean reward: 0.248 [0.069, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.762 [-0.238, 10.267], loss: 0.003346, mae: 0.060555, mean_q: 0.159192
 85636/100000: episode: 1504, duration: 0.250s, episode steps: 45, steps per second: 180, episode reward: 11.809, mean reward: 0.262 [0.075, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.185, 10.225], loss: 0.003482, mae: 0.061763, mean_q: 0.189172
 85681/100000: episode: 1505, duration: 0.263s, episode steps: 45, steps per second: 171, episode reward: 13.635, mean reward: 0.303 [0.184, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.211, 10.358], loss: 0.003277, mae: 0.059875, mean_q: 0.198743
 85723/100000: episode: 1506, duration: 0.214s, episode steps: 42, steps per second: 196, episode reward: 10.243, mean reward: 0.244 [0.047, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.568, 10.218], loss: 0.003159, mae: 0.058776, mean_q: 0.206869
 85765/100000: episode: 1507, duration: 0.220s, episode steps: 42, steps per second: 191, episode reward: 15.249, mean reward: 0.363 [0.189, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.218, 10.327], loss: 0.003344, mae: 0.061652, mean_q: 0.225956
 85795/100000: episode: 1508, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 10.503, mean reward: 0.350 [0.277, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.339, 10.100], loss: 0.002848, mae: 0.056604, mean_q: 0.191554
 85807/100000: episode: 1509, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 4.549, mean reward: 0.379 [0.284, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.357, 10.457], loss: 0.003529, mae: 0.063077, mean_q: 0.298629
 85874/100000: episode: 1510, duration: 0.359s, episode steps: 67, steps per second: 187, episode reward: 13.201, mean reward: 0.197 [0.025, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.759 [-0.288, 10.295], loss: 0.003257, mae: 0.060081, mean_q: 0.225420
 85941/100000: episode: 1511, duration: 0.362s, episode steps: 67, steps per second: 185, episode reward: 11.688, mean reward: 0.174 [0.029, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.754 [-0.108, 10.100], loss: 0.003021, mae: 0.058169, mean_q: 0.214155
 85970/100000: episode: 1512, duration: 0.148s, episode steps: 29, steps per second: 195, episode reward: 9.517, mean reward: 0.328 [0.240, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.450], loss: 0.002882, mae: 0.057074, mean_q: 0.274790
 86038/100000: episode: 1513, duration: 0.366s, episode steps: 68, steps per second: 186, episode reward: 11.781, mean reward: 0.173 [0.013, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.763 [-1.016, 10.127], loss: 0.002847, mae: 0.056172, mean_q: 0.228494
 86067/100000: episode: 1514, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 10.384, mean reward: 0.358 [0.182, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.395, 10.344], loss: 0.002863, mae: 0.056263, mean_q: 0.253136
 86086/100000: episode: 1515, duration: 0.118s, episode steps: 19, steps per second: 162, episode reward: 6.083, mean reward: 0.320 [0.189, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.409, 10.100], loss: 0.002935, mae: 0.058765, mean_q: 0.281117
 86128/100000: episode: 1516, duration: 0.221s, episode steps: 42, steps per second: 190, episode reward: 13.741, mean reward: 0.327 [0.166, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.199, 10.415], loss: 0.003151, mae: 0.060502, mean_q: 0.270481
 86170/100000: episode: 1517, duration: 0.235s, episode steps: 42, steps per second: 179, episode reward: 10.150, mean reward: 0.242 [0.016, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.537, 10.140], loss: 0.003035, mae: 0.059396, mean_q: 0.245377
 86198/100000: episode: 1518, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 9.610, mean reward: 0.343 [0.186, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.213, 10.100], loss: 0.003892, mae: 0.066655, mean_q: 0.292672
[Info] 200-TH LEVEL FOUND: 0.9006055593490601, Considering 10/90 traces
 86217/100000: episode: 1519, duration: 4.054s, episode steps: 19, steps per second: 5, episode reward: 5.452, mean reward: 0.287 [0.205, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.377, 10.100], loss: 0.003357, mae: 0.061707, mean_q: 0.269182
 86227/100000: episode: 1520, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 4.734, mean reward: 0.473 [0.411, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.456], loss: 0.003128, mae: 0.058728, mean_q: 0.329639
 86246/100000: episode: 1521, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 7.778, mean reward: 0.409 [0.235, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.058, 10.100], loss: 0.003226, mae: 0.060289, mean_q: 0.287043
 86257/100000: episode: 1522, duration: 0.068s, episode steps: 11, steps per second: 163, episode reward: 4.925, mean reward: 0.448 [0.342, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.536], loss: 0.003126, mae: 0.061906, mean_q: 0.276346
 86267/100000: episode: 1523, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 4.331, mean reward: 0.433 [0.397, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.561], loss: 0.003152, mae: 0.062182, mean_q: 0.273192
 86305/100000: episode: 1524, duration: 0.214s, episode steps: 38, steps per second: 177, episode reward: 14.649, mean reward: 0.386 [0.164, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.765, 10.375], loss: 0.005733, mae: 0.069950, mean_q: 0.269213
 86316/100000: episode: 1525, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 4.864, mean reward: 0.442 [0.396, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.741, 10.546], loss: 0.005064, mae: 0.071196, mean_q: 0.299492
 86327/100000: episode: 1526, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 5.313, mean reward: 0.483 [0.378, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.565], loss: 0.003687, mae: 0.063727, mean_q: 0.327205
 86338/100000: episode: 1527, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 4.111, mean reward: 0.374 [0.257, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.312, 10.339], loss: 0.003321, mae: 0.062506, mean_q: 0.316549
 86353/100000: episode: 1528, duration: 0.094s, episode steps: 15, steps per second: 159, episode reward: 6.987, mean reward: 0.466 [0.392, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.711, 10.100], loss: 0.003952, mae: 0.063009, mean_q: 0.324493
 86364/100000: episode: 1529, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 4.777, mean reward: 0.434 [0.339, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.137, 10.516], loss: 0.003887, mae: 0.066696, mean_q: 0.303883
 86375/100000: episode: 1530, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 3.843, mean reward: 0.349 [0.308, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.441], loss: 0.004570, mae: 0.072181, mean_q: 0.331274
 86394/100000: episode: 1531, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 8.580, mean reward: 0.452 [0.360, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.384, 10.100], loss: 0.003825, mae: 0.066613, mean_q: 0.314548
 86405/100000: episode: 1532, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 5.311, mean reward: 0.483 [0.421, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.099, 10.419], loss: 0.003649, mae: 0.064979, mean_q: 0.353194
 86416/100000: episode: 1533, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 4.824, mean reward: 0.439 [0.346, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.367, 10.596], loss: 0.003954, mae: 0.059785, mean_q: 0.343902
 86427/100000: episode: 1534, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 4.843, mean reward: 0.440 [0.363, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.707, 10.467], loss: 0.003269, mae: 0.062872, mean_q: 0.304966
 86436/100000: episode: 1535, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 4.010, mean reward: 0.446 [0.407, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.466], loss: 0.007038, mae: 0.076191, mean_q: 0.343151
 86447/100000: episode: 1536, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 4.934, mean reward: 0.449 [0.378, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.887, 10.530], loss: 0.006497, mae: 0.073392, mean_q: 0.295025
 86458/100000: episode: 1537, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 4.857, mean reward: 0.442 [0.411, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.552], loss: 0.006978, mae: 0.072516, mean_q: 0.291340
 86469/100000: episode: 1538, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 5.533, mean reward: 0.503 [0.436, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.626], loss: 0.003431, mae: 0.063437, mean_q: 0.307496
 86480/100000: episode: 1539, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 5.444, mean reward: 0.495 [0.377, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.602], loss: 0.003893, mae: 0.061227, mean_q: 0.345744
 86491/100000: episode: 1540, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 4.720, mean reward: 0.429 [0.384, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.138, 10.488], loss: 0.004899, mae: 0.067591, mean_q: 0.313675
 86510/100000: episode: 1541, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 10.079, mean reward: 0.530 [0.458, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.539, 10.100], loss: 0.003990, mae: 0.065801, mean_q: 0.375531
 86548/100000: episode: 1542, duration: 0.234s, episode steps: 38, steps per second: 162, episode reward: 13.160, mean reward: 0.346 [0.200, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.711, 10.377], loss: 0.004676, mae: 0.068393, mean_q: 0.353216
 86559/100000: episode: 1543, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 4.260, mean reward: 0.387 [0.353, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.611, 10.485], loss: 0.003498, mae: 0.062147, mean_q: 0.340111
 86574/100000: episode: 1544, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 6.873, mean reward: 0.458 [0.376, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.501, 10.100], loss: 0.004951, mae: 0.068206, mean_q: 0.352680
 86589/100000: episode: 1545, duration: 0.088s, episode steps: 15, steps per second: 169, episode reward: 6.951, mean reward: 0.463 [0.389, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.379, 10.100], loss: 0.006340, mae: 0.071756, mean_q: 0.358437
 86599/100000: episode: 1546, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 3.671, mean reward: 0.367 [0.245, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.417], loss: 0.003614, mae: 0.064338, mean_q: 0.342145
 86637/100000: episode: 1547, duration: 0.205s, episode steps: 38, steps per second: 185, episode reward: 12.099, mean reward: 0.318 [0.177, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.745, 10.286], loss: 0.004653, mae: 0.064961, mean_q: 0.354142
 86648/100000: episode: 1548, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 5.113, mean reward: 0.465 [0.373, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-1.385, 10.552], loss: 0.003293, mae: 0.063377, mean_q: 0.420375
 86663/100000: episode: 1549, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 7.827, mean reward: 0.522 [0.446, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.310, 10.100], loss: 0.003628, mae: 0.065358, mean_q: 0.419056
 86673/100000: episode: 1550, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 4.884, mean reward: 0.488 [0.425, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.703, 10.589], loss: 0.002939, mae: 0.058298, mean_q: 0.338735
 86692/100000: episode: 1551, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 9.148, mean reward: 0.481 [0.395, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.489, 10.100], loss: 0.003660, mae: 0.064727, mean_q: 0.387734
 86701/100000: episode: 1552, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 4.489, mean reward: 0.499 [0.415, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.579, 10.572], loss: 0.007702, mae: 0.073626, mean_q: 0.365024
 86710/100000: episode: 1553, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 4.466, mean reward: 0.496 [0.454, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.170, 10.539], loss: 0.003806, mae: 0.070003, mean_q: 0.388159
 86721/100000: episode: 1554, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 5.392, mean reward: 0.490 [0.414, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.587], loss: 0.003537, mae: 0.067000, mean_q: 0.391095
 86759/100000: episode: 1555, duration: 0.220s, episode steps: 38, steps per second: 173, episode reward: 14.246, mean reward: 0.375 [0.224, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.096, 10.559], loss: 0.003104, mae: 0.061278, mean_q: 0.399619
 86797/100000: episode: 1556, duration: 0.220s, episode steps: 38, steps per second: 173, episode reward: 7.698, mean reward: 0.203 [0.043, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.200, 10.100], loss: 0.003306, mae: 0.060594, mean_q: 0.395897
 86835/100000: episode: 1557, duration: 0.208s, episode steps: 38, steps per second: 183, episode reward: 14.370, mean reward: 0.378 [0.238, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-1.750, 10.394], loss: 0.003148, mae: 0.060354, mean_q: 0.438636
 86846/100000: episode: 1558, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 5.449, mean reward: 0.495 [0.433, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.545], loss: 0.003137, mae: 0.059991, mean_q: 0.408785
 86857/100000: episode: 1559, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 5.153, mean reward: 0.468 [0.406, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.094, 10.585], loss: 0.002859, mae: 0.058325, mean_q: 0.376350
 86868/100000: episode: 1560, duration: 0.070s, episode steps: 11, steps per second: 156, episode reward: 5.321, mean reward: 0.484 [0.363, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.642], loss: 0.003051, mae: 0.060668, mean_q: 0.455277
 86879/100000: episode: 1561, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 4.576, mean reward: 0.416 [0.326, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.586, 10.463], loss: 0.003655, mae: 0.063973, mean_q: 0.390025
 86890/100000: episode: 1562, duration: 0.077s, episode steps: 11, steps per second: 143, episode reward: 4.703, mean reward: 0.428 [0.312, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.522], loss: 0.003232, mae: 0.062843, mean_q: 0.399962
 86901/100000: episode: 1563, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 4.464, mean reward: 0.406 [0.356, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.604, 10.448], loss: 0.003119, mae: 0.061060, mean_q: 0.434952
 86912/100000: episode: 1564, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 4.381, mean reward: 0.398 [0.361, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.450, 10.525], loss: 0.003249, mae: 0.061477, mean_q: 0.410337
 86950/100000: episode: 1565, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 12.541, mean reward: 0.330 [0.153, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.083, 10.264], loss: 0.003048, mae: 0.060333, mean_q: 0.459244
 86961/100000: episode: 1566, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 5.178, mean reward: 0.471 [0.356, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.639], loss: 0.003433, mae: 0.062131, mean_q: 0.436694
 86972/100000: episode: 1567, duration: 0.075s, episode steps: 11, steps per second: 146, episode reward: 5.067, mean reward: 0.461 [0.404, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.168, 10.579], loss: 0.003182, mae: 0.061543, mean_q: 0.459638
 86983/100000: episode: 1568, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 4.533, mean reward: 0.412 [0.310, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.204, 10.521], loss: 0.003514, mae: 0.065541, mean_q: 0.455490
 86994/100000: episode: 1569, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 5.114, mean reward: 0.465 [0.418, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.635, 10.549], loss: 0.004037, mae: 0.066435, mean_q: 0.468153
 87005/100000: episode: 1570, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 5.609, mean reward: 0.510 [0.436, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.685], loss: 0.003201, mae: 0.062733, mean_q: 0.453901
 87016/100000: episode: 1571, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 5.194, mean reward: 0.472 [0.414, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.605], loss: 0.003728, mae: 0.063718, mean_q: 0.500446
 87031/100000: episode: 1572, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 7.388, mean reward: 0.493 [0.341, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.429, 10.100], loss: 0.003115, mae: 0.060492, mean_q: 0.494748
 87046/100000: episode: 1573, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 7.658, mean reward: 0.511 [0.425, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.376, 10.100], loss: 0.003519, mae: 0.062299, mean_q: 0.399965
 87057/100000: episode: 1574, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 4.022, mean reward: 0.366 [0.211, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.479], loss: 0.003045, mae: 0.061483, mean_q: 0.476601
 87072/100000: episode: 1575, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 8.113, mean reward: 0.541 [0.493, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.237, 10.100], loss: 0.003724, mae: 0.062637, mean_q: 0.480882
 87083/100000: episode: 1576, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 5.037, mean reward: 0.458 [0.406, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.746, 10.569], loss: 0.003086, mae: 0.062259, mean_q: 0.507580
 87094/100000: episode: 1577, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 3.674, mean reward: 0.334 [0.222, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.302], loss: 0.003264, mae: 0.063281, mean_q: 0.402383
 87105/100000: episode: 1578, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 4.173, mean reward: 0.379 [0.279, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-2.997, 10.410], loss: 0.003910, mae: 0.066891, mean_q: 0.551582
 87116/100000: episode: 1579, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 5.036, mean reward: 0.458 [0.407, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.054, 10.597], loss: 0.003809, mae: 0.066933, mean_q: 0.543782
 87131/100000: episode: 1580, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 7.172, mean reward: 0.478 [0.405, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.929, 10.100], loss: 0.002998, mae: 0.061435, mean_q: 0.512498
 87142/100000: episode: 1581, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 5.339, mean reward: 0.485 [0.361, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-1.475, 10.570], loss: 0.003100, mae: 0.061028, mean_q: 0.514917
 87152/100000: episode: 1582, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 4.597, mean reward: 0.460 [0.426, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.287, 10.516], loss: 0.003187, mae: 0.060360, mean_q: 0.456850
 87167/100000: episode: 1583, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 5.613, mean reward: 0.374 [0.256, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.256, 10.100], loss: 0.003309, mae: 0.061764, mean_q: 0.532099
 87177/100000: episode: 1584, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 3.911, mean reward: 0.391 [0.347, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.438], loss: 0.003321, mae: 0.062645, mean_q: 0.551219
 87188/100000: episode: 1585, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 5.114, mean reward: 0.465 [0.367, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.552, 10.461], loss: 0.003170, mae: 0.062681, mean_q: 0.466464
 87198/100000: episode: 1586, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 4.392, mean reward: 0.439 [0.394, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.535], loss: 0.003276, mae: 0.064092, mean_q: 0.527537
 87209/100000: episode: 1587, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 2.992, mean reward: 0.272 [0.187, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.322], loss: 0.003129, mae: 0.061122, mean_q: 0.468811
 87220/100000: episode: 1588, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 4.825, mean reward: 0.439 [0.350, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-1.314, 10.339], loss: 0.003733, mae: 0.068770, mean_q: 0.520442
 87231/100000: episode: 1589, duration: 0.061s, episode steps: 11, steps per second: 182, episode reward: 4.993, mean reward: 0.454 [0.400, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.586], loss: 0.003030, mae: 0.062015, mean_q: 0.535898
 87242/100000: episode: 1590, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 3.437, mean reward: 0.312 [0.290, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.310, 10.395], loss: 0.002917, mae: 0.059315, mean_q: 0.504726
 87253/100000: episode: 1591, duration: 0.072s, episode steps: 11, steps per second: 154, episode reward: 4.947, mean reward: 0.450 [0.332, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.544], loss: 0.003329, mae: 0.062804, mean_q: 0.526398
 87291/100000: episode: 1592, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 12.103, mean reward: 0.319 [0.046, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.839, 10.172], loss: 0.003494, mae: 0.063739, mean_q: 0.505697
 87306/100000: episode: 1593, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 6.017, mean reward: 0.401 [0.324, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.372, 10.100], loss: 0.003063, mae: 0.058472, mean_q: 0.537703
 87317/100000: episode: 1594, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 3.905, mean reward: 0.355 [0.317, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.484], loss: 0.003401, mae: 0.065185, mean_q: 0.526716
 87326/100000: episode: 1595, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 4.200, mean reward: 0.467 [0.399, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.203, 10.517], loss: 0.002797, mae: 0.058683, mean_q: 0.544602
 87341/100000: episode: 1596, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 7.281, mean reward: 0.485 [0.386, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.365, 10.100], loss: 0.003461, mae: 0.063633, mean_q: 0.543159
 87350/100000: episode: 1597, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 3.819, mean reward: 0.424 [0.338, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.518], loss: 0.003182, mae: 0.062622, mean_q: 0.556979
 87361/100000: episode: 1598, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 5.028, mean reward: 0.457 [0.411, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.567], loss: 0.003042, mae: 0.059079, mean_q: 0.548221
 87370/100000: episode: 1599, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 3.540, mean reward: 0.393 [0.371, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.351, 10.473], loss: 0.003463, mae: 0.064345, mean_q: 0.513801
 87381/100000: episode: 1600, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 4.971, mean reward: 0.452 [0.376, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.494], loss: 0.003196, mae: 0.063214, mean_q: 0.503182
 87392/100000: episode: 1601, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 5.048, mean reward: 0.459 [0.417, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.508], loss: 0.003238, mae: 0.061554, mean_q: 0.522662
 87411/100000: episode: 1602, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 9.803, mean reward: 0.516 [0.451, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.610, 10.100], loss: 0.003221, mae: 0.062240, mean_q: 0.565570
 87422/100000: episode: 1603, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 4.086, mean reward: 0.371 [0.264, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.266, 10.450], loss: 0.003274, mae: 0.062518, mean_q: 0.585062
 87433/100000: episode: 1604, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 5.167, mean reward: 0.470 [0.408, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.563], loss: 0.002911, mae: 0.059304, mean_q: 0.551886
[Info] FALSIFICATION!
 87450/100000: episode: 1605, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 19.580, mean reward: 1.152 [0.537, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.457, 10.093], loss: 0.002816, mae: 0.057209, mean_q: 0.559732
 87550/100000: episode: 1606, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -15.858, mean reward: -0.159 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.748, 10.158], loss: 0.003276, mae: 0.063070, mean_q: 0.557561
 87650/100000: episode: 1607, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -14.787, mean reward: -0.148 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.885, 10.098], loss: 0.072525, mae: 0.123952, mean_q: 0.595114
 87750/100000: episode: 1608, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -14.851, mean reward: -0.149 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.955, 10.125], loss: 0.004194, mae: 0.069508, mean_q: 0.555452
 87850/100000: episode: 1609, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.583, mean reward: -0.186 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.225, 10.098], loss: 0.003568, mae: 0.063271, mean_q: 0.547316
 87950/100000: episode: 1610, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.580, mean reward: -0.186 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.950, 10.240], loss: 0.029282, mae: 0.078139, mean_q: 0.521997
 88050/100000: episode: 1611, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.453, mean reward: -0.165 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.681, 10.290], loss: 0.003200, mae: 0.060848, mean_q: 0.485522
 88150/100000: episode: 1612, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.392, mean reward: -0.194 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.681, 10.194], loss: 0.003735, mae: 0.062230, mean_q: 0.485776
 88250/100000: episode: 1613, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.374, mean reward: -0.194 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.674, 10.174], loss: 0.003048, mae: 0.060338, mean_q: 0.452456
 88350/100000: episode: 1614, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -12.359, mean reward: -0.124 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.321, 10.407], loss: 0.016553, mae: 0.069708, mean_q: 0.436635
 88450/100000: episode: 1615, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.749, mean reward: -0.187 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.796, 10.098], loss: 0.016892, mae: 0.074565, mean_q: 0.407396
 88550/100000: episode: 1616, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.854, mean reward: -0.189 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.423, 10.283], loss: 0.028990, mae: 0.074665, mean_q: 0.395603
 88650/100000: episode: 1617, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.233, mean reward: -0.172 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.166, 10.098], loss: 0.016080, mae: 0.069170, mean_q: 0.372093
 88750/100000: episode: 1618, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.667, mean reward: -0.177 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.768, 10.191], loss: 0.002970, mae: 0.058577, mean_q: 0.391097
 88850/100000: episode: 1619, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -19.659, mean reward: -0.197 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.726, 10.138], loss: 0.015618, mae: 0.062434, mean_q: 0.349043
 88950/100000: episode: 1620, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -19.347, mean reward: -0.193 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.588, 10.185], loss: 0.016033, mae: 0.069079, mean_q: 0.324841
 89050/100000: episode: 1621, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -13.936, mean reward: -0.139 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.092, 10.098], loss: 0.002748, mae: 0.056201, mean_q: 0.306374
 89150/100000: episode: 1622, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.958, mean reward: -0.180 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.721, 10.098], loss: 0.002742, mae: 0.055896, mean_q: 0.304442
 89250/100000: episode: 1623, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -18.270, mean reward: -0.183 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.108, 10.098], loss: 0.002691, mae: 0.055116, mean_q: 0.263356
 89350/100000: episode: 1624, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -12.922, mean reward: -0.129 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.491, 10.098], loss: 0.028782, mae: 0.069772, mean_q: 0.256963
 89450/100000: episode: 1625, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.524, mean reward: -0.185 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.317, 10.100], loss: 0.017167, mae: 0.076606, mean_q: 0.235461
 89550/100000: episode: 1626, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -13.036, mean reward: -0.130 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.767, 10.098], loss: 0.016158, mae: 0.068936, mean_q: 0.215920
 89650/100000: episode: 1627, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -14.014, mean reward: -0.140 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.250, 10.270], loss: 0.002767, mae: 0.054640, mean_q: 0.192935
 89750/100000: episode: 1628, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.308, mean reward: -0.183 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.031, 10.256], loss: 0.002829, mae: 0.056048, mean_q: 0.203981
 89850/100000: episode: 1629, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.182, mean reward: -0.152 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.692, 10.282], loss: 0.015282, mae: 0.058710, mean_q: 0.179571
 89950/100000: episode: 1630, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.909, mean reward: -0.189 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.752, 10.333], loss: 0.003636, mae: 0.060644, mean_q: 0.175753
 90050/100000: episode: 1631, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.154, mean reward: -0.172 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.741, 10.098], loss: 0.002618, mae: 0.053059, mean_q: 0.131062
 90150/100000: episode: 1632, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.981, mean reward: -0.180 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.402, 10.098], loss: 0.015484, mae: 0.058571, mean_q: 0.121303
 90250/100000: episode: 1633, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.661, mean reward: -0.177 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.523, 10.149], loss: 0.003004, mae: 0.056331, mean_q: 0.131591
 90350/100000: episode: 1634, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.116, mean reward: -0.191 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.761, 10.098], loss: 0.002645, mae: 0.052630, mean_q: 0.064143
 90450/100000: episode: 1635, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.288, mean reward: -0.193 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.510, 10.098], loss: 0.015979, mae: 0.062932, mean_q: 0.078800
 90550/100000: episode: 1636, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.456, mean reward: -0.175 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.333, 10.098], loss: 0.002629, mae: 0.051993, mean_q: 0.048697
 90650/100000: episode: 1637, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.412, mean reward: -0.194 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.323, 10.098], loss: 0.015954, mae: 0.063416, mean_q: 0.038583
 90750/100000: episode: 1638, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.273, mean reward: -0.193 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.335, 10.098], loss: 0.002586, mae: 0.051797, mean_q: 0.001588
 90850/100000: episode: 1639, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -15.879, mean reward: -0.159 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.731, 10.098], loss: 0.002502, mae: 0.050635, mean_q: -0.022609
 90950/100000: episode: 1640, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -20.681, mean reward: -0.207 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.068, 10.122], loss: 0.028228, mae: 0.067172, mean_q: -0.021070
 91050/100000: episode: 1641, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.497, mean reward: -0.175 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.160, 10.098], loss: 0.015699, mae: 0.063996, mean_q: -0.025561
 91150/100000: episode: 1642, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.374, mean reward: -0.164 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.795, 10.098], loss: 0.002500, mae: 0.050441, mean_q: -0.059861
 91250/100000: episode: 1643, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -17.155, mean reward: -0.172 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.678, 10.098], loss: 0.015502, mae: 0.058503, mean_q: -0.078357
 91350/100000: episode: 1644, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.186, mean reward: -0.192 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.451, 10.116], loss: 0.015982, mae: 0.065925, mean_q: -0.047038
 91450/100000: episode: 1645, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.775, mean reward: -0.128 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.702, 10.098], loss: 0.027770, mae: 0.066287, mean_q: -0.140867
 91550/100000: episode: 1646, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.991, mean reward: -0.180 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.841, 10.098], loss: 0.015203, mae: 0.061835, mean_q: -0.144569
 91650/100000: episode: 1647, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.051, mean reward: -0.181 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.921, 10.339], loss: 0.018189, mae: 0.081174, mean_q: -0.147929
 91750/100000: episode: 1648, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.342, mean reward: -0.193 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.264, 10.098], loss: 0.017391, mae: 0.073700, mean_q: -0.161900
 91850/100000: episode: 1649, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.478, mean reward: -0.195 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.616, 10.258], loss: 0.002602, mae: 0.052716, mean_q: -0.186081
 91950/100000: episode: 1650, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -14.241, mean reward: -0.142 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.011, 10.280], loss: 0.002535, mae: 0.050935, mean_q: -0.173809
 92050/100000: episode: 1651, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.035, mean reward: -0.140 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.932, 10.098], loss: 0.015956, mae: 0.061639, mean_q: -0.225074
 92150/100000: episode: 1652, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -12.548, mean reward: -0.125 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.612, 10.098], loss: 0.002430, mae: 0.049064, mean_q: -0.272083
 92250/100000: episode: 1653, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.406, mean reward: -0.174 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.537, 10.098], loss: 0.002375, mae: 0.048069, mean_q: -0.291661
 92350/100000: episode: 1654, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -10.108, mean reward: -0.101 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.769, 10.098], loss: 0.002459, mae: 0.048803, mean_q: -0.291883
 92450/100000: episode: 1655, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.809, mean reward: -0.168 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.204, 10.224], loss: 0.002403, mae: 0.047932, mean_q: -0.329171
 92550/100000: episode: 1656, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.990, mean reward: -0.150 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.532, 10.098], loss: 0.002323, mae: 0.047419, mean_q: -0.333899
 92650/100000: episode: 1657, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.534, mean reward: -0.185 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.707, 10.098], loss: 0.002489, mae: 0.048133, mean_q: -0.318662
 92750/100000: episode: 1658, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -12.780, mean reward: -0.128 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.262, 10.278], loss: 0.002324, mae: 0.047287, mean_q: -0.326547
 92850/100000: episode: 1659, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.311, mean reward: -0.193 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.560, 10.098], loss: 0.002186, mae: 0.046290, mean_q: -0.341655
 92950/100000: episode: 1660, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -19.153, mean reward: -0.192 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.634, 10.141], loss: 0.002382, mae: 0.048012, mean_q: -0.338252
 93050/100000: episode: 1661, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.096, mean reward: -0.171 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.708, 10.246], loss: 0.002261, mae: 0.046663, mean_q: -0.316811
 93150/100000: episode: 1662, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.362, mean reward: -0.164 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.333, 10.098], loss: 0.002258, mae: 0.045934, mean_q: -0.326206
 93250/100000: episode: 1663, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.336, mean reward: -0.183 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.305, 10.206], loss: 0.002341, mae: 0.047384, mean_q: -0.323478
 93350/100000: episode: 1664, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -11.097, mean reward: -0.111 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.595, 10.098], loss: 0.002344, mae: 0.047336, mean_q: -0.337319
 93450/100000: episode: 1665, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.758, mean reward: -0.198 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.157, 10.098], loss: 0.002364, mae: 0.047737, mean_q: -0.306532
 93550/100000: episode: 1666, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -17.625, mean reward: -0.176 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.667, 10.267], loss: 0.002254, mae: 0.047248, mean_q: -0.323048
 93650/100000: episode: 1667, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.074, mean reward: -0.171 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.424, 10.191], loss: 0.002403, mae: 0.048172, mean_q: -0.301168
 93750/100000: episode: 1668, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.018, mean reward: -0.180 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.748, 10.101], loss: 0.002191, mae: 0.045979, mean_q: -0.329332
 93850/100000: episode: 1669, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.574, mean reward: -0.146 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.826, 10.098], loss: 0.002205, mae: 0.046489, mean_q: -0.308741
 93950/100000: episode: 1670, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -17.872, mean reward: -0.179 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.522, 10.123], loss: 0.002457, mae: 0.048656, mean_q: -0.303339
 94050/100000: episode: 1671, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.113, mean reward: -0.191 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.900, 10.111], loss: 0.002251, mae: 0.047183, mean_q: -0.339375
 94150/100000: episode: 1672, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.308, mean reward: -0.183 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.283, 10.149], loss: 0.002260, mae: 0.046992, mean_q: -0.337320
 94250/100000: episode: 1673, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.880, mean reward: -0.199 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.625, 10.187], loss: 0.002338, mae: 0.046747, mean_q: -0.337016
 94350/100000: episode: 1674, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -19.872, mean reward: -0.199 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.762, 10.130], loss: 0.002394, mae: 0.048996, mean_q: -0.329280
 94450/100000: episode: 1675, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.993, mean reward: -0.170 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.153, 10.267], loss: 0.002401, mae: 0.048323, mean_q: -0.339007
 94550/100000: episode: 1676, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.748, mean reward: -0.157 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.639, 10.098], loss: 0.002415, mae: 0.048831, mean_q: -0.307569
 94650/100000: episode: 1677, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.720, mean reward: -0.177 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.931, 10.098], loss: 0.002473, mae: 0.048989, mean_q: -0.343276
 94750/100000: episode: 1678, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.703, mean reward: -0.177 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.580, 10.114], loss: 0.002272, mae: 0.047398, mean_q: -0.316268
 94850/100000: episode: 1679, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.815, mean reward: -0.198 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.550, 10.098], loss: 0.002450, mae: 0.048742, mean_q: -0.311761
 94950/100000: episode: 1680, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -6.781, mean reward: -0.068 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.216, 10.098], loss: 0.002500, mae: 0.051183, mean_q: -0.354393
 95050/100000: episode: 1681, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.880, mean reward: -0.179 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.805, 10.296], loss: 0.002470, mae: 0.048853, mean_q: -0.322814
 95150/100000: episode: 1682, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -14.938, mean reward: -0.149 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.267, 10.253], loss: 0.002299, mae: 0.047109, mean_q: -0.304624
 95250/100000: episode: 1683, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.467, mean reward: -0.135 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.587, 10.098], loss: 0.002338, mae: 0.047961, mean_q: -0.308133
 95350/100000: episode: 1684, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -13.603, mean reward: -0.136 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.402, 10.098], loss: 0.002427, mae: 0.048685, mean_q: -0.300239
 95450/100000: episode: 1685, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.683, mean reward: -0.157 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.010, 10.098], loss: 0.002335, mae: 0.047841, mean_q: -0.340472
 95550/100000: episode: 1686, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.555, mean reward: -0.176 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.607, 10.098], loss: 0.002358, mae: 0.047247, mean_q: -0.320582
 95650/100000: episode: 1687, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.673, mean reward: -0.177 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.878, 10.098], loss: 0.002358, mae: 0.048272, mean_q: -0.333449
 95750/100000: episode: 1688, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -12.905, mean reward: -0.129 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.615, 10.488], loss: 0.002390, mae: 0.048195, mean_q: -0.309177
 95850/100000: episode: 1689, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.068, mean reward: -0.191 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.302, 10.098], loss: 0.002295, mae: 0.047278, mean_q: -0.302943
 95950/100000: episode: 1690, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.635, mean reward: -0.166 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.402, 10.381], loss: 0.002394, mae: 0.049220, mean_q: -0.301902
 96050/100000: episode: 1691, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.049, mean reward: -0.170 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.970, 10.098], loss: 0.002496, mae: 0.048784, mean_q: -0.322374
 96150/100000: episode: 1692, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -10.576, mean reward: -0.106 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.855, 10.098], loss: 0.002318, mae: 0.047619, mean_q: -0.323000
 96250/100000: episode: 1693, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.711, mean reward: -0.187 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.103, 10.098], loss: 0.002583, mae: 0.050091, mean_q: -0.323291
 96350/100000: episode: 1694, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -19.084, mean reward: -0.191 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.643, 10.200], loss: 0.002498, mae: 0.049572, mean_q: -0.318806
 96450/100000: episode: 1695, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.378, mean reward: -0.174 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.566, 10.098], loss: 0.002530, mae: 0.050219, mean_q: -0.267203
 96550/100000: episode: 1696, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -19.124, mean reward: -0.191 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.262, 10.098], loss: 0.002459, mae: 0.049248, mean_q: -0.279661
 96650/100000: episode: 1697, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -12.040, mean reward: -0.120 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.582, 10.236], loss: 0.002327, mae: 0.048762, mean_q: -0.296838
 96750/100000: episode: 1698, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.664, mean reward: -0.187 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.726, 10.147], loss: 0.002321, mae: 0.048353, mean_q: -0.293496
 96850/100000: episode: 1699, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.575, mean reward: -0.156 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.670, 10.227], loss: 0.002561, mae: 0.052339, mean_q: -0.295788
 96950/100000: episode: 1700, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.091, mean reward: -0.191 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.252, 10.098], loss: 0.002416, mae: 0.048327, mean_q: -0.323697
 97050/100000: episode: 1701, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.086, mean reward: -0.141 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.649, 10.355], loss: 0.002343, mae: 0.047608, mean_q: -0.309014
 97150/100000: episode: 1702, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.460, mean reward: -0.165 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.282, 10.098], loss: 0.002416, mae: 0.048742, mean_q: -0.313919
 97250/100000: episode: 1703, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -19.337, mean reward: -0.193 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.290, 10.158], loss: 0.002519, mae: 0.050350, mean_q: -0.296037
 97350/100000: episode: 1704, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -10.213, mean reward: -0.102 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.998, 10.531], loss: 0.002624, mae: 0.051967, mean_q: -0.271323
[Info] 100-TH LEVEL FOUND: 0.6326233744621277, Considering 10/90 traces
 97450/100000: episode: 1705, duration: 4.402s, episode steps: 100, steps per second: 23, episode reward: -17.057, mean reward: -0.171 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.149, 10.098], loss: 0.002446, mae: 0.049192, mean_q: -0.344862
 97486/100000: episode: 1706, duration: 0.201s, episode steps: 36, steps per second: 179, episode reward: 9.348, mean reward: 0.260 [0.112, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-1.019, 10.294], loss: 0.002417, mae: 0.048964, mean_q: -0.365190
 97532/100000: episode: 1707, duration: 0.251s, episode steps: 46, steps per second: 183, episode reward: 12.670, mean reward: 0.275 [0.144, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.402, 10.100], loss: 0.002457, mae: 0.049889, mean_q: -0.233986
 97552/100000: episode: 1708, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 8.549, mean reward: 0.427 [0.321, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.644, 10.481], loss: 0.002135, mae: 0.046488, mean_q: -0.265750
 97590/100000: episode: 1709, duration: 0.206s, episode steps: 38, steps per second: 184, episode reward: 6.593, mean reward: 0.173 [0.049, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.622, 10.100], loss: 0.002601, mae: 0.050634, mean_q: -0.280785
 97628/100000: episode: 1710, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 10.252, mean reward: 0.270 [0.059, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-1.301, 10.100], loss: 0.002168, mae: 0.046308, mean_q: -0.287219
 97669/100000: episode: 1711, duration: 0.230s, episode steps: 41, steps per second: 178, episode reward: 9.696, mean reward: 0.236 [0.063, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-1.026, 10.100], loss: 0.002399, mae: 0.048956, mean_q: -0.290165
 97705/100000: episode: 1712, duration: 0.208s, episode steps: 36, steps per second: 173, episode reward: 9.565, mean reward: 0.266 [0.061, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.588, 10.254], loss: 0.002750, mae: 0.052554, mean_q: -0.299233
 97725/100000: episode: 1713, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 4.724, mean reward: 0.236 [0.121, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.320], loss: 0.002659, mae: 0.054009, mean_q: -0.222919
 97735/100000: episode: 1714, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 3.376, mean reward: 0.338 [0.298, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.729, 10.100], loss: 0.002372, mae: 0.049837, mean_q: -0.187946
 97745/100000: episode: 1715, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 3.796, mean reward: 0.380 [0.303, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.562, 10.100], loss: 0.002936, mae: 0.055634, mean_q: -0.251924
 97765/100000: episode: 1716, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 6.602, mean reward: 0.330 [0.255, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.375, 10.404], loss: 0.002557, mae: 0.050448, mean_q: -0.260481
 97790/100000: episode: 1717, duration: 0.150s, episode steps: 25, steps per second: 167, episode reward: 8.483, mean reward: 0.339 [0.275, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.465, 10.100], loss: 0.002262, mae: 0.049129, mean_q: -0.250612
 97810/100000: episode: 1718, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 6.782, mean reward: 0.339 [0.285, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.116, 10.464], loss: 0.002490, mae: 0.048533, mean_q: -0.284966
 97846/100000: episode: 1719, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 11.154, mean reward: 0.310 [0.208, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.287, 10.401], loss: 0.003545, mae: 0.061655, mean_q: -0.260969
 97854/100000: episode: 1720, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 3.221, mean reward: 0.403 [0.341, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-2.455, 10.100], loss: 0.008771, mae: 0.072510, mean_q: -0.288953
 97864/100000: episode: 1721, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 3.856, mean reward: 0.386 [0.275, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.248, 10.100], loss: 0.003708, mae: 0.068560, mean_q: -0.168471
 97869/100000: episode: 1722, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 1.707, mean reward: 0.341 [0.250, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.388, 10.100], loss: 0.002231, mae: 0.054017, mean_q: -0.259070
 97915/100000: episode: 1723, duration: 0.257s, episode steps: 46, steps per second: 179, episode reward: 8.406, mean reward: 0.183 [0.016, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.035, 10.100], loss: 0.002798, mae: 0.056315, mean_q: -0.235716
 97923/100000: episode: 1724, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 2.261, mean reward: 0.283 [0.159, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.259, 10.100], loss: 0.003021, mae: 0.058018, mean_q: -0.211307
 97969/100000: episode: 1725, duration: 0.247s, episode steps: 46, steps per second: 186, episode reward: 12.934, mean reward: 0.281 [0.056, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.668, 10.100], loss: 0.002834, mae: 0.054538, mean_q: -0.220587
 98005/100000: episode: 1726, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 6.941, mean reward: 0.193 [0.019, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-1.187, 10.210], loss: 0.002308, mae: 0.048940, mean_q: -0.209484
 98051/100000: episode: 1727, duration: 0.264s, episode steps: 46, steps per second: 174, episode reward: 8.153, mean reward: 0.177 [0.034, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-1.306, 10.100], loss: 0.002495, mae: 0.051421, mean_q: -0.191195
 98092/100000: episode: 1728, duration: 0.224s, episode steps: 41, steps per second: 183, episode reward: 10.143, mean reward: 0.247 [0.082, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.185, 10.100], loss: 0.002471, mae: 0.050589, mean_q: -0.206728
 98100/100000: episode: 1729, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 3.310, mean reward: 0.414 [0.380, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.295, 10.100], loss: 0.002683, mae: 0.051836, mean_q: -0.265505
 98125/100000: episode: 1730, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 11.246, mean reward: 0.450 [0.296, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.357, 10.100], loss: 0.002768, mae: 0.053375, mean_q: -0.192178
 98130/100000: episode: 1731, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 1.842, mean reward: 0.368 [0.321, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.292, 10.100], loss: 0.002592, mae: 0.053389, mean_q: -0.235864
 98166/100000: episode: 1732, duration: 0.193s, episode steps: 36, steps per second: 186, episode reward: 10.575, mean reward: 0.294 [0.171, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.035, 10.269], loss: 0.002474, mae: 0.049821, mean_q: -0.192668
 98174/100000: episode: 1733, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 3.569, mean reward: 0.446 [0.392, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.350, 10.100], loss: 0.002299, mae: 0.047812, mean_q: -0.227332
 98199/100000: episode: 1734, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 11.308, mean reward: 0.452 [0.299, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.381, 10.100], loss: 0.002488, mae: 0.049336, mean_q: -0.210972
 98219/100000: episode: 1735, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 7.000, mean reward: 0.350 [0.251, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.192, 10.441], loss: 0.002518, mae: 0.050632, mean_q: -0.136412
 98224/100000: episode: 1736, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 1.693, mean reward: 0.339 [0.289, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.311, 10.100], loss: 0.001856, mae: 0.043880, mean_q: -0.248382
 98260/100000: episode: 1737, duration: 0.195s, episode steps: 36, steps per second: 185, episode reward: 11.201, mean reward: 0.311 [0.210, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.153, 10.345], loss: 0.002355, mae: 0.049437, mean_q: -0.163541
 98296/100000: episode: 1738, duration: 0.195s, episode steps: 36, steps per second: 185, episode reward: 6.357, mean reward: 0.177 [0.029, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.110, 10.153], loss: 0.002256, mae: 0.047453, mean_q: -0.213003
 98301/100000: episode: 1739, duration: 0.037s, episode steps: 5, steps per second: 135, episode reward: 1.755, mean reward: 0.351 [0.306, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.443, 10.100], loss: 0.002211, mae: 0.047556, mean_q: -0.107148
 98306/100000: episode: 1740, duration: 0.028s, episode steps: 5, steps per second: 182, episode reward: 2.000, mean reward: 0.400 [0.299, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.322, 10.100], loss: 0.002471, mae: 0.049339, mean_q: -0.125772
 98314/100000: episode: 1741, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 3.079, mean reward: 0.385 [0.303, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.406, 10.100], loss: 0.002092, mae: 0.046560, mean_q: -0.193243
 98322/100000: episode: 1742, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 2.515, mean reward: 0.314 [0.272, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.365, 10.100], loss: 0.002568, mae: 0.049859, mean_q: -0.193427
 98363/100000: episode: 1743, duration: 0.212s, episode steps: 41, steps per second: 194, episode reward: 9.207, mean reward: 0.225 [0.131, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-1.530, 10.100], loss: 0.002676, mae: 0.050722, mean_q: -0.186260
 98373/100000: episode: 1744, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 2.532, mean reward: 0.253 [0.152, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.635, 10.100], loss: 0.002223, mae: 0.048097, mean_q: -0.072725
 98409/100000: episode: 1745, duration: 0.187s, episode steps: 36, steps per second: 193, episode reward: 15.705, mean reward: 0.436 [0.246, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.246, 10.506], loss: 0.002620, mae: 0.052252, mean_q: -0.126896
 98414/100000: episode: 1746, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 1.354, mean reward: 0.271 [0.252, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.337, 10.100], loss: 0.002726, mae: 0.053042, mean_q: -0.165703
 98434/100000: episode: 1747, duration: 0.123s, episode steps: 20, steps per second: 163, episode reward: 7.733, mean reward: 0.387 [0.312, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.521], loss: 0.002434, mae: 0.049708, mean_q: -0.171573
 98472/100000: episode: 1748, duration: 0.194s, episode steps: 38, steps per second: 196, episode reward: 9.199, mean reward: 0.242 [0.082, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.290, 10.100], loss: 0.002480, mae: 0.050503, mean_q: -0.133976
 98518/100000: episode: 1749, duration: 0.237s, episode steps: 46, steps per second: 194, episode reward: 10.447, mean reward: 0.227 [0.046, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.051, 10.100], loss: 0.002707, mae: 0.053415, mean_q: -0.122843
 98564/100000: episode: 1750, duration: 0.259s, episode steps: 46, steps per second: 178, episode reward: 8.912, mean reward: 0.194 [0.014, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.572, 10.100], loss: 0.002432, mae: 0.050070, mean_q: -0.136124
 98610/100000: episode: 1751, duration: 0.251s, episode steps: 46, steps per second: 183, episode reward: 12.440, mean reward: 0.270 [0.130, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.291, 10.100], loss: 0.002300, mae: 0.048893, mean_q: -0.101668
 98615/100000: episode: 1752, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 1.783, mean reward: 0.357 [0.309, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.512, 10.100], loss: 0.002487, mae: 0.049838, mean_q: -0.130376
 98640/100000: episode: 1753, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 8.829, mean reward: 0.353 [0.207, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.060, 10.100], loss: 0.002351, mae: 0.050171, mean_q: -0.068748
 98676/100000: episode: 1754, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 11.877, mean reward: 0.330 [0.130, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.652, 10.241], loss: 0.002698, mae: 0.052340, mean_q: -0.090707
 98686/100000: episode: 1755, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 2.771, mean reward: 0.277 [0.233, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.308, 10.100], loss: 0.003154, mae: 0.057091, mean_q: -0.117868
 98706/100000: episode: 1756, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 7.233, mean reward: 0.362 [0.260, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.464], loss: 0.002836, mae: 0.054058, mean_q: -0.179231
 98742/100000: episode: 1757, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 12.790, mean reward: 0.355 [0.176, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.342, 10.347], loss: 0.002705, mae: 0.055318, mean_q: -0.149745
 98752/100000: episode: 1758, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 3.382, mean reward: 0.338 [0.275, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.401, 10.100], loss: 0.002742, mae: 0.054638, mean_q: -0.188672
 98798/100000: episode: 1759, duration: 0.234s, episode steps: 46, steps per second: 197, episode reward: 10.158, mean reward: 0.221 [0.019, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.986, 10.250], loss: 0.002688, mae: 0.053549, mean_q: -0.040527
 98806/100000: episode: 1760, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 2.953, mean reward: 0.369 [0.292, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.394, 10.100], loss: 0.002361, mae: 0.047690, mean_q: -0.148198
 98814/100000: episode: 1761, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 3.937, mean reward: 0.492 [0.432, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.409, 10.100], loss: 0.002919, mae: 0.056893, mean_q: -0.067068
 98834/100000: episode: 1762, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 6.595, mean reward: 0.330 [0.264, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.290, 10.393], loss: 0.002423, mae: 0.050788, mean_q: -0.093260
 98872/100000: episode: 1763, duration: 0.207s, episode steps: 38, steps per second: 184, episode reward: 12.090, mean reward: 0.318 [0.201, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.468, 10.100], loss: 0.002672, mae: 0.053056, mean_q: -0.030103
 98880/100000: episode: 1764, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 2.046, mean reward: 0.256 [0.205, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.214, 10.100], loss: 0.002748, mae: 0.054907, mean_q: -0.037818
 98888/100000: episode: 1765, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 2.443, mean reward: 0.305 [0.271, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.369, 10.100], loss: 0.002882, mae: 0.055803, mean_q: 0.020067
 98924/100000: episode: 1766, duration: 0.204s, episode steps: 36, steps per second: 177, episode reward: 12.752, mean reward: 0.354 [0.224, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.822, 10.442], loss: 0.002295, mae: 0.049662, mean_q: -0.070991
 98934/100000: episode: 1767, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 2.964, mean reward: 0.296 [0.249, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.275, 10.100], loss: 0.002558, mae: 0.053645, mean_q: 0.125253
 98944/100000: episode: 1768, duration: 0.069s, episode steps: 10, steps per second: 145, episode reward: 3.155, mean reward: 0.316 [0.200, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.560, 10.100], loss: 0.002498, mae: 0.051505, mean_q: -0.014655
 98954/100000: episode: 1769, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 3.015, mean reward: 0.302 [0.252, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.991, 10.100], loss: 0.002829, mae: 0.053912, mean_q: -0.141231
 98962/100000: episode: 1770, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 3.222, mean reward: 0.403 [0.368, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.346, 10.100], loss: 0.002824, mae: 0.054086, mean_q: -0.080194
 99000/100000: episode: 1771, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 8.562, mean reward: 0.225 [0.070, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.637, 10.100], loss: 0.002732, mae: 0.054756, mean_q: -0.013345
 99041/100000: episode: 1772, duration: 0.224s, episode steps: 41, steps per second: 183, episode reward: 13.067, mean reward: 0.319 [0.213, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.368, 10.100], loss: 0.002687, mae: 0.053064, mean_q: -0.066664
 99061/100000: episode: 1773, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 8.097, mean reward: 0.405 [0.322, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.280, 10.498], loss: 0.002604, mae: 0.052733, mean_q: -0.037736
 99066/100000: episode: 1774, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 1.523, mean reward: 0.305 [0.280, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.424, 10.100], loss: 0.002486, mae: 0.051493, mean_q: -0.020908
 99074/100000: episode: 1775, duration: 0.054s, episode steps: 8, steps per second: 148, episode reward: 2.409, mean reward: 0.301 [0.255, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-1.459, 10.100], loss: 0.002446, mae: 0.051168, mean_q: -0.037151
 99082/100000: episode: 1776, duration: 0.044s, episode steps: 8, steps per second: 184, episode reward: 2.254, mean reward: 0.282 [0.242, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.306, 10.100], loss: 0.002390, mae: 0.053399, mean_q: 0.050799
 99102/100000: episode: 1777, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 5.851, mean reward: 0.293 [0.159, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.101, 10.326], loss: 0.002217, mae: 0.048538, mean_q: -0.079532
 99138/100000: episode: 1778, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 11.806, mean reward: 0.328 [0.152, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.709, 10.318], loss: 0.002507, mae: 0.051499, mean_q: -0.009405
 99176/100000: episode: 1779, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 13.059, mean reward: 0.344 [0.239, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.457, 10.100], loss: 0.002621, mae: 0.052069, mean_q: 0.039110
 99214/100000: episode: 1780, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 7.973, mean reward: 0.210 [0.089, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.486, 10.100], loss: 0.002511, mae: 0.053081, mean_q: 0.022621
 99252/100000: episode: 1781, duration: 0.227s, episode steps: 38, steps per second: 167, episode reward: 15.283, mean reward: 0.402 [0.249, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-1.194, 10.100], loss: 0.002640, mae: 0.052919, mean_q: 0.019164
 99290/100000: episode: 1782, duration: 0.197s, episode steps: 38, steps per second: 192, episode reward: 13.412, mean reward: 0.353 [0.144, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-1.262, 10.100], loss: 0.002548, mae: 0.052612, mean_q: 0.001633
 99326/100000: episode: 1783, duration: 0.180s, episode steps: 36, steps per second: 200, episode reward: 13.222, mean reward: 0.367 [0.212, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.887, 10.616], loss: 0.002662, mae: 0.053248, mean_q: -0.009427
 99336/100000: episode: 1784, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 2.914, mean reward: 0.291 [0.258, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.259, 10.100], loss: 0.002422, mae: 0.050443, mean_q: -0.019471
 99377/100000: episode: 1785, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 14.410, mean reward: 0.351 [0.220, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.713, 10.100], loss: 0.002561, mae: 0.052206, mean_q: 0.027281
 99397/100000: episode: 1786, duration: 0.117s, episode steps: 20, steps per second: 170, episode reward: 6.601, mean reward: 0.330 [0.254, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.871, 10.396], loss: 0.002581, mae: 0.053971, mean_q: 0.061207
 99443/100000: episode: 1787, duration: 0.239s, episode steps: 46, steps per second: 192, episode reward: 13.107, mean reward: 0.285 [0.067, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-1.011, 10.165], loss: 0.002459, mae: 0.052817, mean_q: 0.066319
 99451/100000: episode: 1788, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 2.613, mean reward: 0.327 [0.246, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.333, 10.100], loss: 0.002530, mae: 0.049534, mean_q: -0.106093
 99456/100000: episode: 1789, duration: 0.033s, episode steps: 5, steps per second: 149, episode reward: 1.389, mean reward: 0.278 [0.223, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.450, 10.100], loss: 0.002517, mae: 0.048739, mean_q: -0.027818
 99502/100000: episode: 1790, duration: 0.249s, episode steps: 46, steps per second: 184, episode reward: 14.455, mean reward: 0.314 [0.026, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-1.013, 10.100], loss: 0.002487, mae: 0.052557, mean_q: 0.033140
 99522/100000: episode: 1791, duration: 0.121s, episode steps: 20, steps per second: 166, episode reward: 6.565, mean reward: 0.328 [0.206, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.082, 10.506], loss: 0.002643, mae: 0.054444, mean_q: 0.101385
 99560/100000: episode: 1792, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 10.795, mean reward: 0.284 [0.171, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.679, 10.100], loss: 0.002914, mae: 0.056001, mean_q: 0.097945
 99568/100000: episode: 1793, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 2.708, mean reward: 0.339 [0.298, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.200, 10.100], loss: 0.002560, mae: 0.054344, mean_q: 0.059128
 99576/100000: episode: 1794, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 2.963, mean reward: 0.370 [0.297, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.912, 10.100], loss: 0.002721, mae: 0.054388, mean_q: 0.008154
[Info] 200-TH LEVEL FOUND: 0.8072898387908936, Considering 10/90 traces
 99584/100000: episode: 1795, duration: 3.997s, episode steps: 8, steps per second: 2, episode reward: 3.021, mean reward: 0.378 [0.331, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.870, 10.100], loss: 0.002845, mae: 0.056765, mean_q: 0.155666
 99617/100000: episode: 1796, duration: 0.181s, episode steps: 33, steps per second: 183, episode reward: 10.464, mean reward: 0.317 [0.195, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.130, 10.100], loss: 0.003156, mae: 0.058612, mean_q: 0.079784
 99641/100000: episode: 1797, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 10.207, mean reward: 0.425 [0.301, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.484, 10.100], loss: 0.002658, mae: 0.054240, mean_q: 0.105166
 99674/100000: episode: 1798, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 12.981, mean reward: 0.393 [0.198, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.398, 10.100], loss: 0.002714, mae: 0.055555, mean_q: 0.086613
 99681/100000: episode: 1799, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 2.784, mean reward: 0.398 [0.353, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.402, 10.100], loss: 0.002886, mae: 0.056973, mean_q: 0.119058
 99714/100000: episode: 1800, duration: 0.191s, episode steps: 33, steps per second: 173, episode reward: 12.712, mean reward: 0.385 [0.263, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.953, 10.100], loss: 0.002579, mae: 0.052358, mean_q: 0.076878
 99723/100000: episode: 1801, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 3.951, mean reward: 0.439 [0.369, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.225, 10.100], loss: 0.002602, mae: 0.053080, mean_q: 0.107031
 99747/100000: episode: 1802, duration: 0.134s, episode steps: 24, steps per second: 178, episode reward: 9.983, mean reward: 0.416 [0.293, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.267, 10.100], loss: 0.002504, mae: 0.051553, mean_q: 0.102873
 99765/100000: episode: 1803, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 7.529, mean reward: 0.418 [0.291, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.471, 10.100], loss: 0.002563, mae: 0.052829, mean_q: 0.117281
 99789/100000: episode: 1804, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 10.314, mean reward: 0.430 [0.287, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.006, 10.100], loss: 0.002389, mae: 0.051603, mean_q: 0.112138
 99796/100000: episode: 1805, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 2.750, mean reward: 0.393 [0.327, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.401, 10.100], loss: 0.002554, mae: 0.052619, mean_q: 0.049131
 99829/100000: episode: 1806, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 12.358, mean reward: 0.374 [0.144, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.274, 10.100], loss: 0.002694, mae: 0.055877, mean_q: 0.130813
 99833/100000: episode: 1807, duration: 0.029s, episode steps: 4, steps per second: 140, episode reward: 1.838, mean reward: 0.460 [0.418, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.394, 10.100], loss: 0.002114, mae: 0.044221, mean_q: -0.067915
 99850/100000: episode: 1808, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 7.828, mean reward: 0.460 [0.353, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.359, 10.531], loss: 0.002551, mae: 0.052530, mean_q: 0.121031
 99859/100000: episode: 1809, duration: 0.061s, episode steps: 9, steps per second: 148, episode reward: 3.220, mean reward: 0.358 [0.182, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.348, 10.100], loss: 0.002847, mae: 0.057300, mean_q: 0.196830
 99883/100000: episode: 1810, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 10.108, mean reward: 0.421 [0.271, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.840, 10.100], loss: 0.002887, mae: 0.056281, mean_q: 0.164235
 99887/100000: episode: 1811, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 1.927, mean reward: 0.482 [0.439, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.421, 10.100], loss: 0.002239, mae: 0.051090, mean_q: 0.104832
 99892/100000: episode: 1812, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 2.005, mean reward: 0.401 [0.351, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.263, 10.100], loss: 0.002616, mae: 0.055520, mean_q: 0.018256
 99897/100000: episode: 1813, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 2.190, mean reward: 0.438 [0.396, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.313, 10.100], loss: 0.002746, mae: 0.057568, mean_q: 0.250311
 99930/100000: episode: 1814, duration: 0.182s, episode steps: 33, steps per second: 182, episode reward: 10.922, mean reward: 0.331 [0.199, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.104, 10.100], loss: 0.002729, mae: 0.054871, mean_q: 0.161288
 99954/100000: episode: 1815, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 8.244, mean reward: 0.344 [0.276, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.404, 10.100], loss: 0.002491, mae: 0.053655, mean_q: 0.154954
 99976/100000: episode: 1816, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 8.560, mean reward: 0.389 [0.309, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.786, 10.448], loss: 0.002670, mae: 0.054723, mean_q: 0.136946
 99998/100000: episode: 1817, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 9.663, mean reward: 0.439 [0.314, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.677, 10.390], loss: 0.002406, mae: 0.051711, mean_q: 0.197824
done, took 600.919 seconds
[Info] End Importance Splitting. Falsification occurred 6 times.
