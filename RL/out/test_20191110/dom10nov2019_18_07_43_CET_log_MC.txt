Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.166s, episode steps: 100, steps per second: 604, episode reward: -19.434, mean reward: -0.194 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.544, 10.205], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.060s, episode steps: 100, steps per second: 1661, episode reward: -18.533, mean reward: -0.185 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.518, 10.119], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.062s, episode steps: 100, steps per second: 1615, episode reward: -19.064, mean reward: -0.191 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.657, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.064s, episode steps: 100, steps per second: 1556, episode reward: -16.870, mean reward: -0.169 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.948, 10.146], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.061s, episode steps: 100, steps per second: 1642, episode reward: -19.786, mean reward: -0.198 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.314, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.287s, episode steps: 100, steps per second: 78, episode reward: -19.140, mean reward: -0.191 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.854, 10.116], loss: 0.029843, mae: 0.163857, mean_q: -0.504384
   700/100000: episode: 7, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -19.788, mean reward: -0.198 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.756, 10.098], loss: 0.010328, mae: 0.094898, mean_q: -0.454539
   800/100000: episode: 8, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.537, mean reward: -0.165 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.345, 10.098], loss: 0.008703, mae: 0.085906, mean_q: -0.421799
   900/100000: episode: 9, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -18.164, mean reward: -0.182 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.753, 10.124], loss: 0.007871, mae: 0.082518, mean_q: -0.403298
  1000/100000: episode: 10, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.810, mean reward: -0.158 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.765, 10.098], loss: 0.008004, mae: 0.082815, mean_q: -0.365272
  1100/100000: episode: 11, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -16.100, mean reward: -0.161 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.793, 10.187], loss: 0.007810, mae: 0.083420, mean_q: -0.382529
  1200/100000: episode: 12, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.501, mean reward: -0.165 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.691, 10.171], loss: 0.007228, mae: 0.080088, mean_q: -0.365366
  1300/100000: episode: 13, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.344, mean reward: -0.163 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.430, 10.265], loss: 0.006412, mae: 0.074623, mean_q: -0.360301
  1400/100000: episode: 14, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -14.116, mean reward: -0.141 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.786, 10.098], loss: 0.006359, mae: 0.076779, mean_q: -0.340270
  1500/100000: episode: 15, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -17.513, mean reward: -0.175 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.157, 10.098], loss: 0.006087, mae: 0.074438, mean_q: -0.385127
  1600/100000: episode: 16, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -14.306, mean reward: -0.143 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.158, 10.342], loss: 0.007349, mae: 0.081528, mean_q: -0.366071
  1700/100000: episode: 17, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -19.222, mean reward: -0.192 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.985, 10.098], loss: 0.006317, mae: 0.074157, mean_q: -0.345597
  1800/100000: episode: 18, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -20.255, mean reward: -0.203 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.058, 10.098], loss: 0.006882, mae: 0.078134, mean_q: -0.346151
  1900/100000: episode: 19, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -18.079, mean reward: -0.181 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.825, 10.214], loss: 0.006485, mae: 0.077730, mean_q: -0.365737
  2000/100000: episode: 20, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -13.420, mean reward: -0.134 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.553, 10.289], loss: 0.005229, mae: 0.070570, mean_q: -0.370133
  2100/100000: episode: 21, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -10.161, mean reward: -0.102 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.857, 10.454], loss: 0.004900, mae: 0.067824, mean_q: -0.333233
  2200/100000: episode: 22, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.860, mean reward: -0.169 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.439, 10.193], loss: 0.006720, mae: 0.077766, mean_q: -0.356496
  2300/100000: episode: 23, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -19.707, mean reward: -0.197 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.116, 10.201], loss: 0.005638, mae: 0.074975, mean_q: -0.327291
  2400/100000: episode: 24, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.933, mean reward: -0.189 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.331, 10.139], loss: 0.005549, mae: 0.072069, mean_q: -0.336882
  2500/100000: episode: 25, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.773, mean reward: -0.168 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.027, 10.273], loss: 0.005962, mae: 0.076328, mean_q: -0.343618
  2600/100000: episode: 26, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.825, mean reward: -0.188 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.667, 10.098], loss: 0.005653, mae: 0.070945, mean_q: -0.361383
  2700/100000: episode: 27, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.447, mean reward: -0.164 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.826, 10.098], loss: 0.006325, mae: 0.075744, mean_q: -0.346921
  2800/100000: episode: 28, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.549, mean reward: -0.165 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.145, 10.098], loss: 0.005915, mae: 0.073460, mean_q: -0.327149
  2900/100000: episode: 29, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -20.481, mean reward: -0.205 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.264, 10.098], loss: 0.005992, mae: 0.074279, mean_q: -0.309663
  3000/100000: episode: 30, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.064, mean reward: -0.171 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.163, 10.098], loss: 0.005781, mae: 0.074224, mean_q: -0.331897
  3100/100000: episode: 31, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.490, mean reward: -0.195 [-1.000, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.613, 10.177], loss: 0.005116, mae: 0.070985, mean_q: -0.348470
  3200/100000: episode: 32, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.060, mean reward: -0.171 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.639, 10.098], loss: 0.005677, mae: 0.071998, mean_q: -0.342872
  3300/100000: episode: 33, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -18.049, mean reward: -0.180 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.550, 10.156], loss: 0.005319, mae: 0.068648, mean_q: -0.371635
  3400/100000: episode: 34, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -14.410, mean reward: -0.144 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.252, 10.098], loss: 0.005942, mae: 0.073195, mean_q: -0.344236
  3500/100000: episode: 35, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -18.395, mean reward: -0.184 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.716, 10.273], loss: 0.005717, mae: 0.073565, mean_q: -0.320522
  3600/100000: episode: 36, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.909, mean reward: -0.179 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.602, 10.250], loss: 0.004810, mae: 0.068643, mean_q: -0.341876
  3700/100000: episode: 37, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -19.349, mean reward: -0.193 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.463, 10.162], loss: 0.004757, mae: 0.066814, mean_q: -0.303360
  3800/100000: episode: 38, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.184, mean reward: -0.152 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.314, 10.098], loss: 0.004730, mae: 0.068190, mean_q: -0.339987
  3900/100000: episode: 39, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.512, mean reward: -0.165 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.935, 10.098], loss: 0.004658, mae: 0.068456, mean_q: -0.357724
  4000/100000: episode: 40, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -19.475, mean reward: -0.195 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.046, 10.098], loss: 0.005119, mae: 0.068597, mean_q: -0.346447
  4100/100000: episode: 41, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.588, mean reward: -0.186 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.253, 10.173], loss: 0.005277, mae: 0.071248, mean_q: -0.355765
  4200/100000: episode: 42, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.851, mean reward: -0.159 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.690, 10.555], loss: 0.004828, mae: 0.068803, mean_q: -0.297919
  4300/100000: episode: 43, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -14.422, mean reward: -0.144 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.618, 10.098], loss: 0.004282, mae: 0.065974, mean_q: -0.296766
  4400/100000: episode: 44, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -19.710, mean reward: -0.197 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.800, 10.098], loss: 0.004951, mae: 0.072371, mean_q: -0.313677
  4500/100000: episode: 45, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.365, mean reward: -0.184 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.669, 10.098], loss: 0.003850, mae: 0.061848, mean_q: -0.342514
  4600/100000: episode: 46, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.825, mean reward: -0.158 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.781, 10.298], loss: 0.004530, mae: 0.066741, mean_q: -0.350052
  4700/100000: episode: 47, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -15.887, mean reward: -0.159 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.764, 10.317], loss: 0.005717, mae: 0.073947, mean_q: -0.336091
  4800/100000: episode: 48, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.371, mean reward: -0.164 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.566, 10.285], loss: 0.005519, mae: 0.075414, mean_q: -0.331729
  4900/100000: episode: 49, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.036, mean reward: -0.180 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.157, 10.098], loss: 0.004445, mae: 0.064289, mean_q: -0.326060
  5000/100000: episode: 50, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -19.186, mean reward: -0.192 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.910, 10.098], loss: 0.005211, mae: 0.072570, mean_q: -0.323598
  5100/100000: episode: 51, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -20.140, mean reward: -0.201 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.749, 10.155], loss: 0.005456, mae: 0.071037, mean_q: -0.332788
  5200/100000: episode: 52, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.096, mean reward: -0.181 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.349, 10.135], loss: 0.005463, mae: 0.070005, mean_q: -0.335707
  5300/100000: episode: 53, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.758, mean reward: -0.188 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.673, 10.098], loss: 0.005411, mae: 0.072043, mean_q: -0.306772
  5400/100000: episode: 54, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.576, mean reward: -0.176 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.747, 10.246], loss: 0.004415, mae: 0.066691, mean_q: -0.334643
  5500/100000: episode: 55, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -14.347, mean reward: -0.143 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.306, 10.287], loss: 0.004321, mae: 0.066897, mean_q: -0.293622
  5600/100000: episode: 56, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.620, mean reward: -0.186 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.809, 10.141], loss: 0.005770, mae: 0.074354, mean_q: -0.303384
  5700/100000: episode: 57, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.304, mean reward: -0.193 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.104, 10.098], loss: 0.004847, mae: 0.068383, mean_q: -0.339839
  5800/100000: episode: 58, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -21.310, mean reward: -0.213 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.946, 10.179], loss: 0.004465, mae: 0.067900, mean_q: -0.316795
  5900/100000: episode: 59, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.805, mean reward: -0.198 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.456, 10.098], loss: 0.004574, mae: 0.067154, mean_q: -0.341521
  6000/100000: episode: 60, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.261, mean reward: -0.183 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.452, 10.098], loss: 0.004947, mae: 0.069639, mean_q: -0.346670
  6100/100000: episode: 61, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.243, mean reward: -0.172 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.955, 10.098], loss: 0.004958, mae: 0.070141, mean_q: -0.313490
  6200/100000: episode: 62, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.442, mean reward: -0.144 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.030, 10.098], loss: 0.004915, mae: 0.070343, mean_q: -0.318843
  6300/100000: episode: 63, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -12.024, mean reward: -0.120 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.100, 10.234], loss: 0.004205, mae: 0.064383, mean_q: -0.332937
  6400/100000: episode: 64, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.866, mean reward: -0.169 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.093, 10.297], loss: 0.005118, mae: 0.072302, mean_q: -0.314042
  6500/100000: episode: 65, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.183, mean reward: -0.182 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.596, 10.098], loss: 0.005243, mae: 0.072265, mean_q: -0.297746
  6600/100000: episode: 66, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -20.129, mean reward: -0.201 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.687, 10.274], loss: 0.004514, mae: 0.066411, mean_q: -0.303721
  6700/100000: episode: 67, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.158, mean reward: -0.172 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.466, 10.098], loss: 0.005012, mae: 0.067935, mean_q: -0.318282
  6800/100000: episode: 68, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -12.531, mean reward: -0.125 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.417, 10.098], loss: 0.005576, mae: 0.076446, mean_q: -0.341003
  6900/100000: episode: 69, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -14.601, mean reward: -0.146 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.508, 10.098], loss: 0.004059, mae: 0.063926, mean_q: -0.348814
  7000/100000: episode: 70, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.064, mean reward: -0.181 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.723, 10.374], loss: 0.004485, mae: 0.068193, mean_q: -0.311200
  7100/100000: episode: 71, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -17.148, mean reward: -0.171 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.849, 10.109], loss: 0.004273, mae: 0.066555, mean_q: -0.326224
  7200/100000: episode: 72, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -11.879, mean reward: -0.119 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.950, 10.098], loss: 0.004538, mae: 0.068778, mean_q: -0.310366
  7300/100000: episode: 73, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.176, mean reward: -0.172 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.388, 10.334], loss: 0.004668, mae: 0.067705, mean_q: -0.324950
  7400/100000: episode: 74, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -12.139, mean reward: -0.121 [-1.000, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.478, 10.314], loss: 0.004712, mae: 0.069793, mean_q: -0.303371
  7500/100000: episode: 75, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.796, mean reward: -0.158 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.002, 10.098], loss: 0.004345, mae: 0.067164, mean_q: -0.314741
  7600/100000: episode: 76, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.288, mean reward: -0.183 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.881, 10.239], loss: 0.004685, mae: 0.067599, mean_q: -0.316665
  7700/100000: episode: 77, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -10.231, mean reward: -0.102 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.929, 10.098], loss: 0.004059, mae: 0.065815, mean_q: -0.323206
  7800/100000: episode: 78, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.882, mean reward: -0.179 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.591, 10.210], loss: 0.004544, mae: 0.069529, mean_q: -0.318862
  7900/100000: episode: 79, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.915, mean reward: -0.179 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.090, 10.098], loss: 0.004398, mae: 0.067612, mean_q: -0.317916
  8000/100000: episode: 80, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.097, mean reward: -0.161 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.319, 10.098], loss: 0.004509, mae: 0.068141, mean_q: -0.285266
  8100/100000: episode: 81, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.428, mean reward: -0.164 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.733, 10.168], loss: 0.004306, mae: 0.069027, mean_q: -0.295578
  8200/100000: episode: 82, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.065, mean reward: -0.161 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.739, 10.235], loss: 0.004430, mae: 0.068494, mean_q: -0.324898
  8300/100000: episode: 83, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.771, mean reward: -0.168 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.791, 10.098], loss: 0.005684, mae: 0.077564, mean_q: -0.303171
  8400/100000: episode: 84, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -11.859, mean reward: -0.119 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.999, 10.098], loss: 0.004394, mae: 0.067969, mean_q: -0.292205
  8500/100000: episode: 85, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.029, mean reward: -0.170 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.146, 10.187], loss: 0.004010, mae: 0.065340, mean_q: -0.335818
  8600/100000: episode: 86, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -12.041, mean reward: -0.120 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.002, 10.098], loss: 0.004624, mae: 0.071230, mean_q: -0.309254
  8700/100000: episode: 87, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.334, mean reward: -0.143 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.246, 10.098], loss: 0.004422, mae: 0.069033, mean_q: -0.315413
  8800/100000: episode: 88, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.812, mean reward: -0.148 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.024, 10.271], loss: 0.003980, mae: 0.063870, mean_q: -0.347147
  8900/100000: episode: 89, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.493, mean reward: -0.195 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.822, 10.098], loss: 0.004179, mae: 0.067326, mean_q: -0.332151
  9000/100000: episode: 90, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.320, mean reward: -0.183 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.599, 10.098], loss: 0.004386, mae: 0.068367, mean_q: -0.330296
  9100/100000: episode: 91, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.762, mean reward: -0.188 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.833, 10.098], loss: 0.004817, mae: 0.072039, mean_q: -0.331770
  9200/100000: episode: 92, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.885, mean reward: -0.159 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.701, 10.098], loss: 0.004265, mae: 0.066986, mean_q: -0.330514
  9300/100000: episode: 93, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.581, mean reward: -0.176 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.883, 10.188], loss: 0.004963, mae: 0.070839, mean_q: -0.302253
  9400/100000: episode: 94, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -13.447, mean reward: -0.134 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.347, 10.396], loss: 0.003832, mae: 0.063214, mean_q: -0.293778
  9500/100000: episode: 95, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -13.485, mean reward: -0.135 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.619, 10.098], loss: 0.003942, mae: 0.065847, mean_q: -0.303936
  9600/100000: episode: 96, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.005, mean reward: -0.170 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.921, 10.098], loss: 0.003536, mae: 0.062152, mean_q: -0.293990
  9700/100000: episode: 97, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -6.057, mean reward: -0.061 [-1.000, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.352, 10.318], loss: 0.004134, mae: 0.067475, mean_q: -0.323900
  9800/100000: episode: 98, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.088, mean reward: -0.181 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.035, 10.098], loss: 0.004408, mae: 0.068001, mean_q: -0.318160
  9900/100000: episode: 99, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -11.987, mean reward: -0.120 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.051, 10.098], loss: 0.004510, mae: 0.069451, mean_q: -0.307712
 10000/100000: episode: 100, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.856, mean reward: -0.159 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.875, 10.199], loss: 0.003853, mae: 0.064094, mean_q: -0.298765
 10100/100000: episode: 101, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.286, mean reward: -0.173 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.412, 10.098], loss: 0.004339, mae: 0.069865, mean_q: -0.273287
 10200/100000: episode: 102, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.524, mean reward: -0.185 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.998, 10.114], loss: 0.003823, mae: 0.064475, mean_q: -0.317574
 10300/100000: episode: 103, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.518, mean reward: -0.175 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.349, 10.248], loss: 0.004039, mae: 0.066818, mean_q: -0.324403
 10400/100000: episode: 104, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.158, mean reward: -0.182 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.836, 10.313], loss: 0.003882, mae: 0.064792, mean_q: -0.298779
 10500/100000: episode: 105, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.380, mean reward: -0.184 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.724, 10.222], loss: 0.004645, mae: 0.070569, mean_q: -0.299079
 10600/100000: episode: 106, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -21.949, mean reward: -0.219 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.880, 10.098], loss: 0.003755, mae: 0.063797, mean_q: -0.298756
 10700/100000: episode: 107, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.272, mean reward: -0.183 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.414, 10.098], loss: 0.003721, mae: 0.063484, mean_q: -0.326669
 10800/100000: episode: 108, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.496, mean reward: -0.185 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.773, 10.131], loss: 0.004345, mae: 0.067353, mean_q: -0.292972
 10900/100000: episode: 109, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.128, mean reward: -0.141 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.388, 10.237], loss: 0.003916, mae: 0.065124, mean_q: -0.308073
 11000/100000: episode: 110, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -12.110, mean reward: -0.121 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.255, 10.238], loss: 0.003845, mae: 0.064336, mean_q: -0.307667
 11100/100000: episode: 111, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.676, mean reward: -0.187 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.682, 10.098], loss: 0.005550, mae: 0.076555, mean_q: -0.301960
 11200/100000: episode: 112, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.754, mean reward: -0.158 [-1.000, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-2.295, 10.098], loss: 0.003850, mae: 0.066648, mean_q: -0.304621
 11300/100000: episode: 113, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.281, mean reward: -0.143 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.857, 10.098], loss: 0.003640, mae: 0.061809, mean_q: -0.301378
 11400/100000: episode: 114, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -10.560, mean reward: -0.106 [-1.000, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.312, 10.460], loss: 0.003512, mae: 0.060470, mean_q: -0.335329
 11500/100000: episode: 115, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.732, mean reward: -0.167 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.671, 10.098], loss: 0.005525, mae: 0.073338, mean_q: -0.306702
 11600/100000: episode: 116, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -9.786, mean reward: -0.098 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.958, 10.098], loss: 0.004014, mae: 0.064343, mean_q: -0.323529
 11700/100000: episode: 117, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.675, mean reward: -0.177 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.879, 10.098], loss: 0.004303, mae: 0.068519, mean_q: -0.284772
 11800/100000: episode: 118, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: -18.152, mean reward: -0.182 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.867, 10.103], loss: 0.003946, mae: 0.065205, mean_q: -0.262586
 11900/100000: episode: 119, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -14.931, mean reward: -0.149 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.836, 10.223], loss: 0.005547, mae: 0.074642, mean_q: -0.301844
 12000/100000: episode: 120, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.463, mean reward: -0.165 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.330, 10.214], loss: 0.004756, mae: 0.069439, mean_q: -0.296353
 12100/100000: episode: 121, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.750, mean reward: -0.177 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.200, 10.142], loss: 0.003992, mae: 0.065172, mean_q: -0.300578
 12200/100000: episode: 122, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.505, mean reward: -0.155 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.066, 10.098], loss: 0.003580, mae: 0.060502, mean_q: -0.282697
 12300/100000: episode: 123, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.792, mean reward: -0.168 [-1.000, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.572, 10.098], loss: 0.003710, mae: 0.062878, mean_q: -0.279641
 12400/100000: episode: 124, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -9.710, mean reward: -0.097 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.678, 10.297], loss: 0.003992, mae: 0.065835, mean_q: -0.283440
 12500/100000: episode: 125, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.847, mean reward: -0.178 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.449, 10.154], loss: 0.003899, mae: 0.064280, mean_q: -0.299487
 12600/100000: episode: 126, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.901, mean reward: -0.189 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.426, 10.098], loss: 0.003756, mae: 0.062774, mean_q: -0.296047
 12700/100000: episode: 127, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -20.427, mean reward: -0.204 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.573, 10.100], loss: 0.003847, mae: 0.064646, mean_q: -0.285973
 12800/100000: episode: 128, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -12.478, mean reward: -0.125 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.359, 10.098], loss: 0.003561, mae: 0.061282, mean_q: -0.308969
 12900/100000: episode: 129, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.039, mean reward: -0.170 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.485, 10.219], loss: 0.003974, mae: 0.064769, mean_q: -0.293143
 13000/100000: episode: 130, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.068, mean reward: -0.191 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.240, 10.295], loss: 0.003834, mae: 0.063947, mean_q: -0.283145
 13100/100000: episode: 131, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.627, mean reward: -0.166 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.482, 10.098], loss: 0.006016, mae: 0.076993, mean_q: -0.300836
 13200/100000: episode: 132, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -15.656, mean reward: -0.157 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.001, 10.279], loss: 0.004066, mae: 0.065479, mean_q: -0.295397
 13300/100000: episode: 133, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.493, mean reward: -0.185 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.850, 10.233], loss: 0.004336, mae: 0.066547, mean_q: -0.314670
 13400/100000: episode: 134, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.384, mean reward: -0.144 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.218, 10.098], loss: 0.004192, mae: 0.066567, mean_q: -0.303696
 13500/100000: episode: 135, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -20.235, mean reward: -0.202 [-1.000, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.841, 10.176], loss: 0.003885, mae: 0.063110, mean_q: -0.304915
 13600/100000: episode: 136, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.719, mean reward: -0.157 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.504, 10.365], loss: 0.004149, mae: 0.067255, mean_q: -0.299783
 13700/100000: episode: 137, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.363, mean reward: -0.164 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.384, 10.098], loss: 0.003960, mae: 0.063017, mean_q: -0.302611
 13800/100000: episode: 138, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -20.504, mean reward: -0.205 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.266, 10.106], loss: 0.003397, mae: 0.059089, mean_q: -0.314480
 13900/100000: episode: 139, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -20.218, mean reward: -0.202 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.538, 10.144], loss: 0.003497, mae: 0.059674, mean_q: -0.285249
 14000/100000: episode: 140, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.176, mean reward: -0.142 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.773, 10.320], loss: 0.003633, mae: 0.061004, mean_q: -0.314160
 14100/100000: episode: 141, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.274, mean reward: -0.163 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.912, 10.098], loss: 0.004271, mae: 0.065448, mean_q: -0.323078
 14200/100000: episode: 142, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.521, mean reward: -0.185 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.689, 10.315], loss: 0.003301, mae: 0.058210, mean_q: -0.308439
 14300/100000: episode: 143, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.271, mean reward: -0.183 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.642, 10.098], loss: 0.003602, mae: 0.061900, mean_q: -0.270086
 14400/100000: episode: 144, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -20.915, mean reward: -0.209 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.575, 10.098], loss: 0.005911, mae: 0.073521, mean_q: -0.277301
 14500/100000: episode: 145, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.465, mean reward: -0.175 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.752, 10.138], loss: 0.004146, mae: 0.063721, mean_q: -0.308478
 14600/100000: episode: 146, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -11.864, mean reward: -0.119 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.806, 10.098], loss: 0.004403, mae: 0.067984, mean_q: -0.313926
 14700/100000: episode: 147, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.218, mean reward: -0.172 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.966, 10.259], loss: 0.004117, mae: 0.065056, mean_q: -0.287332
 14800/100000: episode: 148, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -19.174, mean reward: -0.192 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.787, 10.098], loss: 0.004104, mae: 0.065809, mean_q: -0.286649
 14900/100000: episode: 149, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -15.335, mean reward: -0.153 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.879, 10.098], loss: 0.003886, mae: 0.063237, mean_q: -0.353074
 15000/100000: episode: 150, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.140, mean reward: -0.181 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.112, 10.098], loss: 0.003847, mae: 0.063503, mean_q: -0.318794
 15100/100000: episode: 151, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -19.171, mean reward: -0.192 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.223, 10.098], loss: 0.004473, mae: 0.069003, mean_q: -0.346415
 15200/100000: episode: 152, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -13.243, mean reward: -0.132 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.813, 10.098], loss: 0.003721, mae: 0.061631, mean_q: -0.310728
 15300/100000: episode: 153, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.685, mean reward: -0.147 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.547, 10.098], loss: 0.003115, mae: 0.056840, mean_q: -0.317481
 15400/100000: episode: 154, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.638, mean reward: -0.166 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.135, 10.161], loss: 0.003226, mae: 0.057521, mean_q: -0.288883
 15500/100000: episode: 155, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -13.020, mean reward: -0.130 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.195, 10.136], loss: 0.003606, mae: 0.060831, mean_q: -0.314924
 15600/100000: episode: 156, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.191, mean reward: -0.172 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.398, 10.098], loss: 0.003960, mae: 0.061988, mean_q: -0.330299
 15700/100000: episode: 157, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.939, mean reward: -0.169 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.587, 10.098], loss: 0.004590, mae: 0.067989, mean_q: -0.296389
 15800/100000: episode: 158, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -13.439, mean reward: -0.134 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.667, 10.098], loss: 0.003417, mae: 0.060783, mean_q: -0.298864
 15900/100000: episode: 159, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.502, mean reward: -0.175 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.685, 10.098], loss: 0.003295, mae: 0.057925, mean_q: -0.276548
 16000/100000: episode: 160, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.943, mean reward: -0.169 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.720, 10.303], loss: 0.003486, mae: 0.061351, mean_q: -0.314569
 16100/100000: episode: 161, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -12.954, mean reward: -0.130 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.987, 10.117], loss: 0.003461, mae: 0.061759, mean_q: -0.312160
 16200/100000: episode: 162, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.783, mean reward: -0.188 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.404, 10.291], loss: 0.002929, mae: 0.055602, mean_q: -0.317074
 16300/100000: episode: 163, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.213, mean reward: -0.162 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.907, 10.098], loss: 0.003207, mae: 0.057283, mean_q: -0.291215
 16400/100000: episode: 164, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -20.149, mean reward: -0.201 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.282, 10.098], loss: 0.003205, mae: 0.058050, mean_q: -0.325914
 16500/100000: episode: 165, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.018, mean reward: -0.170 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.608, 10.276], loss: 0.006093, mae: 0.075469, mean_q: -0.318628
 16600/100000: episode: 166, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.269, mean reward: -0.173 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.916, 10.207], loss: 0.003710, mae: 0.061519, mean_q: -0.315381
 16700/100000: episode: 167, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -19.355, mean reward: -0.194 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.820, 10.211], loss: 0.002919, mae: 0.055550, mean_q: -0.307548
 16800/100000: episode: 168, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -5.494, mean reward: -0.055 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.629, 10.416], loss: 0.003119, mae: 0.056930, mean_q: -0.348895
 16900/100000: episode: 169, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.930, mean reward: -0.169 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.172, 10.098], loss: 0.003453, mae: 0.060627, mean_q: -0.335397
 17000/100000: episode: 170, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -19.782, mean reward: -0.198 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.919, 10.098], loss: 0.003204, mae: 0.059853, mean_q: -0.283203
 17100/100000: episode: 171, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.861, mean reward: -0.179 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.691, 10.277], loss: 0.002981, mae: 0.055619, mean_q: -0.350386
 17200/100000: episode: 172, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.157, mean reward: -0.172 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.939, 10.098], loss: 0.002963, mae: 0.055845, mean_q: -0.290261
 17300/100000: episode: 173, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.161, mean reward: -0.172 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.246, 10.187], loss: 0.003040, mae: 0.056880, mean_q: -0.302425
 17400/100000: episode: 174, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.078, mean reward: -0.181 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.600, 10.269], loss: 0.003170, mae: 0.057442, mean_q: -0.296883
 17500/100000: episode: 175, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.377, mean reward: -0.194 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.320, 10.098], loss: 0.003041, mae: 0.056665, mean_q: -0.320370
 17600/100000: episode: 176, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.918, mean reward: -0.179 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.285, 10.098], loss: 0.002813, mae: 0.054407, mean_q: -0.338145
 17700/100000: episode: 177, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -19.720, mean reward: -0.197 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.402, 10.181], loss: 0.003362, mae: 0.059492, mean_q: -0.340266
 17800/100000: episode: 178, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.022, mean reward: -0.180 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.629, 10.308], loss: 0.003104, mae: 0.058870, mean_q: -0.307503
 17900/100000: episode: 179, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.658, mean reward: -0.187 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.339, 10.098], loss: 0.003116, mae: 0.057438, mean_q: -0.332009
 18000/100000: episode: 180, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -9.962, mean reward: -0.100 [-1.000, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.957, 10.098], loss: 0.003122, mae: 0.057739, mean_q: -0.306224
 18100/100000: episode: 181, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.260, mean reward: -0.173 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.853, 10.214], loss: 0.003596, mae: 0.061436, mean_q: -0.288411
 18200/100000: episode: 182, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.303, mean reward: -0.183 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.770, 10.098], loss: 0.002880, mae: 0.054586, mean_q: -0.322569
 18300/100000: episode: 183, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.963, mean reward: -0.180 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.348, 10.098], loss: 0.003109, mae: 0.056982, mean_q: -0.307007
 18400/100000: episode: 184, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.224, mean reward: -0.192 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.469, 10.274], loss: 0.005380, mae: 0.072822, mean_q: -0.326670
 18500/100000: episode: 185, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.055, mean reward: -0.181 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.533, 10.216], loss: 0.003900, mae: 0.065562, mean_q: -0.326197
 18600/100000: episode: 186, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -11.430, mean reward: -0.114 [-1.000, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.949, 10.358], loss: 0.003127, mae: 0.057911, mean_q: -0.310608
 18700/100000: episode: 187, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.175, mean reward: -0.162 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.386, 10.098], loss: 0.003218, mae: 0.057741, mean_q: -0.316508
 18800/100000: episode: 188, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.458, mean reward: -0.175 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.143, 10.098], loss: 0.002914, mae: 0.055160, mean_q: -0.316262
 18900/100000: episode: 189, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.597, mean reward: -0.166 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.821, 10.234], loss: 0.003150, mae: 0.057079, mean_q: -0.332100
 19000/100000: episode: 190, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.568, mean reward: -0.196 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.947, 10.098], loss: 0.003146, mae: 0.056822, mean_q: -0.303427
 19100/100000: episode: 191, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.829, mean reward: -0.188 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.701, 10.197], loss: 0.003186, mae: 0.057657, mean_q: -0.319655
 19200/100000: episode: 192, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -14.196, mean reward: -0.142 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.353, 10.222], loss: 0.003062, mae: 0.056461, mean_q: -0.320917
 19300/100000: episode: 193, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.959, mean reward: -0.170 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.039, 10.098], loss: 0.003104, mae: 0.056776, mean_q: -0.349527
 19400/100000: episode: 194, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.804, mean reward: -0.158 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.718, 10.339], loss: 0.003475, mae: 0.061483, mean_q: -0.302889
 19500/100000: episode: 195, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -17.827, mean reward: -0.178 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.738, 10.098], loss: 0.003970, mae: 0.064805, mean_q: -0.324342
 19600/100000: episode: 196, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -12.489, mean reward: -0.125 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.532, 10.340], loss: 0.004240, mae: 0.067753, mean_q: -0.304494
 19700/100000: episode: 197, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -11.273, mean reward: -0.113 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.234, 10.389], loss: 0.003335, mae: 0.060013, mean_q: -0.328236
 19800/100000: episode: 198, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -15.637, mean reward: -0.156 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.256, 10.160], loss: 0.003140, mae: 0.056938, mean_q: -0.316574
 19900/100000: episode: 199, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.608, mean reward: -0.166 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.705, 10.098], loss: 0.003131, mae: 0.056206, mean_q: -0.303244
 20000/100000: episode: 200, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.260, mean reward: -0.163 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.654, 10.098], loss: 0.003124, mae: 0.056656, mean_q: -0.316012
 20100/100000: episode: 201, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.054, mean reward: -0.171 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.542, 10.098], loss: 0.003388, mae: 0.059084, mean_q: -0.295558
 20200/100000: episode: 202, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -8.010, mean reward: -0.080 [-1.000, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.458, 10.503], loss: 0.004164, mae: 0.064246, mean_q: -0.301756
 20300/100000: episode: 203, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.745, mean reward: -0.167 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.558, 10.098], loss: 0.003129, mae: 0.057394, mean_q: -0.351298
 20400/100000: episode: 204, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.132, mean reward: -0.161 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.489, 10.140], loss: 0.003101, mae: 0.056492, mean_q: -0.296233
 20500/100000: episode: 205, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -15.409, mean reward: -0.154 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.024, 10.385], loss: 0.003143, mae: 0.055869, mean_q: -0.325585
 20600/100000: episode: 206, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -16.625, mean reward: -0.166 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.787, 10.283], loss: 0.002944, mae: 0.054285, mean_q: -0.328487
 20700/100000: episode: 207, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -14.708, mean reward: -0.147 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.570, 10.470], loss: 0.002879, mae: 0.054250, mean_q: -0.319540
 20800/100000: episode: 208, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -13.449, mean reward: -0.134 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.833, 10.098], loss: 0.003126, mae: 0.056929, mean_q: -0.287643
 20900/100000: episode: 209, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.478, mean reward: -0.175 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.818, 10.098], loss: 0.003101, mae: 0.056172, mean_q: -0.289536
 21000/100000: episode: 210, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -18.838, mean reward: -0.188 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.365, 10.098], loss: 0.003083, mae: 0.056275, mean_q: -0.279605
 21100/100000: episode: 211, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -19.079, mean reward: -0.191 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.997, 10.243], loss: 0.003127, mae: 0.056022, mean_q: -0.298277
 21200/100000: episode: 212, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.989, mean reward: -0.160 [-1.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.103, 10.131], loss: 0.003159, mae: 0.058570, mean_q: -0.319962
 21300/100000: episode: 213, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.806, mean reward: -0.188 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.219, 10.166], loss: 0.002950, mae: 0.054110, mean_q: -0.293908
 21400/100000: episode: 214, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -6.222, mean reward: -0.062 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.851, 10.098], loss: 0.002929, mae: 0.054186, mean_q: -0.313478
 21500/100000: episode: 215, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.542, mean reward: -0.165 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.687, 10.098], loss: 0.003116, mae: 0.056517, mean_q: -0.311419
 21600/100000: episode: 216, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.980, mean reward: -0.190 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.023, 10.099], loss: 0.003398, mae: 0.060335, mean_q: -0.285823
 21700/100000: episode: 217, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.040, mean reward: -0.190 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.169, 10.103], loss: 0.004898, mae: 0.067883, mean_q: -0.299917
 21800/100000: episode: 218, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.223, mean reward: -0.172 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.208, 10.235], loss: 0.002894, mae: 0.054665, mean_q: -0.312173
 21900/100000: episode: 219, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -20.110, mean reward: -0.201 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.738, 10.098], loss: 0.002734, mae: 0.053286, mean_q: -0.320290
 22000/100000: episode: 220, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.005, mean reward: -0.200 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.268, 10.098], loss: 0.003105, mae: 0.056044, mean_q: -0.319745
 22100/100000: episode: 221, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.642, mean reward: -0.186 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.240, 10.098], loss: 0.003002, mae: 0.055033, mean_q: -0.296430
 22200/100000: episode: 222, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.314, mean reward: -0.183 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.577, 10.247], loss: 0.002837, mae: 0.054578, mean_q: -0.313465
 22300/100000: episode: 223, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.351, mean reward: -0.194 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.615, 10.120], loss: 0.003025, mae: 0.055118, mean_q: -0.307733
 22400/100000: episode: 224, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -10.708, mean reward: -0.107 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.495, 10.098], loss: 0.003281, mae: 0.055828, mean_q: -0.327005
 22500/100000: episode: 225, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.842, mean reward: -0.158 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.799, 10.350], loss: 0.003112, mae: 0.058035, mean_q: -0.316368
 22600/100000: episode: 226, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.345, mean reward: -0.163 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.088, 10.159], loss: 0.003196, mae: 0.058549, mean_q: -0.282031
 22700/100000: episode: 227, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.261, mean reward: -0.163 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.725, 10.215], loss: 0.002904, mae: 0.054683, mean_q: -0.336374
 22800/100000: episode: 228, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -8.581, mean reward: -0.086 [-1.000, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.798, 10.209], loss: 0.003033, mae: 0.055251, mean_q: -0.315940
 22900/100000: episode: 229, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -16.481, mean reward: -0.165 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.812, 10.098], loss: 0.002951, mae: 0.053835, mean_q: -0.286476
 23000/100000: episode: 230, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -12.448, mean reward: -0.124 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.340, 10.558], loss: 0.004870, mae: 0.069033, mean_q: -0.316780
 23100/100000: episode: 231, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.769, mean reward: -0.148 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.854, 10.098], loss: 0.004521, mae: 0.070957, mean_q: -0.278020
 23200/100000: episode: 232, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -14.281, mean reward: -0.143 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.633, 10.276], loss: 0.005883, mae: 0.071212, mean_q: -0.311857
 23300/100000: episode: 233, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.072, mean reward: -0.171 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.691, 10.105], loss: 0.003941, mae: 0.064088, mean_q: -0.303096
 23400/100000: episode: 234, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.963, mean reward: -0.180 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.219, 10.290], loss: 0.003007, mae: 0.055290, mean_q: -0.344873
 23500/100000: episode: 235, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.755, mean reward: -0.178 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.404, 10.116], loss: 0.003159, mae: 0.056049, mean_q: -0.309386
 23600/100000: episode: 236, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.853, mean reward: -0.129 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.015, 10.194], loss: 0.003321, mae: 0.057888, mean_q: -0.246169
 23700/100000: episode: 237, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.281, mean reward: -0.183 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.760, 10.232], loss: 0.003072, mae: 0.055568, mean_q: -0.292394
 23800/100000: episode: 238, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.700, mean reward: -0.167 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.421, 10.201], loss: 0.003912, mae: 0.063547, mean_q: -0.306017
 23900/100000: episode: 239, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -15.323, mean reward: -0.153 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.245, 10.221], loss: 0.003099, mae: 0.056585, mean_q: -0.268479
 24000/100000: episode: 240, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.943, mean reward: -0.179 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.986, 10.127], loss: 0.002939, mae: 0.053436, mean_q: -0.312999
 24100/100000: episode: 241, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.850, mean reward: -0.179 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.632, 10.098], loss: 0.003001, mae: 0.054670, mean_q: -0.290438
 24200/100000: episode: 242, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.876, mean reward: -0.169 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.521, 10.098], loss: 0.003167, mae: 0.057028, mean_q: -0.291559
 24300/100000: episode: 243, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.146, mean reward: -0.151 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.499, 10.098], loss: 0.002677, mae: 0.051474, mean_q: -0.316792
 24400/100000: episode: 244, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -17.171, mean reward: -0.172 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.648, 10.231], loss: 0.002875, mae: 0.053365, mean_q: -0.302592
 24500/100000: episode: 245, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -12.692, mean reward: -0.127 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.865, 10.098], loss: 0.003090, mae: 0.056349, mean_q: -0.299665
 24600/100000: episode: 246, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -19.436, mean reward: -0.194 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.789, 10.230], loss: 0.002846, mae: 0.053223, mean_q: -0.298554
 24700/100000: episode: 247, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.562, mean reward: -0.176 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.735, 10.098], loss: 0.002889, mae: 0.054391, mean_q: -0.276426
 24800/100000: episode: 248, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -18.387, mean reward: -0.184 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.293, 10.098], loss: 0.002737, mae: 0.052485, mean_q: -0.316206
 24900/100000: episode: 249, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.837, mean reward: -0.198 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.315, 10.098], loss: 0.003997, mae: 0.063183, mean_q: -0.308825
 25000/100000: episode: 250, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -19.207, mean reward: -0.192 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.822, 10.117], loss: 0.004747, mae: 0.064946, mean_q: -0.336376
 25100/100000: episode: 251, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.808, mean reward: -0.178 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.640, 10.098], loss: 0.003836, mae: 0.063828, mean_q: -0.302426
 25200/100000: episode: 252, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.150, mean reward: -0.171 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.630, 10.329], loss: 0.003024, mae: 0.054824, mean_q: -0.320065
 25300/100000: episode: 253, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -18.240, mean reward: -0.182 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.203, 10.282], loss: 0.002848, mae: 0.053186, mean_q: -0.299314
 25400/100000: episode: 254, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.113, mean reward: -0.171 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.228, 10.098], loss: 0.003073, mae: 0.054997, mean_q: -0.298406
 25500/100000: episode: 255, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -14.967, mean reward: -0.150 [-1.000, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-2.353, 10.098], loss: 0.002525, mae: 0.050082, mean_q: -0.331683
 25600/100000: episode: 256, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.625, mean reward: -0.146 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.737, 10.114], loss: 0.002873, mae: 0.053338, mean_q: -0.322944
 25700/100000: episode: 257, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -18.738, mean reward: -0.187 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.459, 10.122], loss: 0.003088, mae: 0.057269, mean_q: -0.299850
 25800/100000: episode: 258, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.738, mean reward: -0.147 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.212, 10.098], loss: 0.002953, mae: 0.055200, mean_q: -0.308288
 25900/100000: episode: 259, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.700, mean reward: -0.187 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.821, 10.098], loss: 0.002773, mae: 0.053777, mean_q: -0.298020
 26000/100000: episode: 260, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -20.574, mean reward: -0.206 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.791, 10.098], loss: 0.002782, mae: 0.053071, mean_q: -0.291300
 26100/100000: episode: 261, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -16.304, mean reward: -0.163 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.412, 10.098], loss: 0.002824, mae: 0.053836, mean_q: -0.289265
 26200/100000: episode: 262, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -14.521, mean reward: -0.145 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.426, 10.333], loss: 0.002912, mae: 0.054840, mean_q: -0.288515
 26300/100000: episode: 263, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.142, mean reward: -0.171 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.560, 10.233], loss: 0.006500, mae: 0.069772, mean_q: -0.311691
 26400/100000: episode: 264, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.337, mean reward: -0.163 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.375, 10.331], loss: 0.002970, mae: 0.056319, mean_q: -0.336360
 26500/100000: episode: 265, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.112, mean reward: -0.161 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.122, 10.126], loss: 0.002798, mae: 0.054045, mean_q: -0.344352
 26600/100000: episode: 266, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -9.460, mean reward: -0.095 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.634, 10.098], loss: 0.002769, mae: 0.053483, mean_q: -0.323734
 26700/100000: episode: 267, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -15.018, mean reward: -0.150 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.859, 10.259], loss: 0.002822, mae: 0.053063, mean_q: -0.307967
 26800/100000: episode: 268, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.827, mean reward: -0.178 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.624, 10.223], loss: 0.002853, mae: 0.054083, mean_q: -0.323553
 26900/100000: episode: 269, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.739, mean reward: -0.167 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.220, 10.266], loss: 0.002842, mae: 0.053784, mean_q: -0.310165
 27000/100000: episode: 270, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.563, mean reward: -0.176 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.357, 10.281], loss: 0.002857, mae: 0.053357, mean_q: -0.301669
 27100/100000: episode: 271, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.710, mean reward: -0.187 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.360, 10.349], loss: 0.002666, mae: 0.052253, mean_q: -0.331900
 27200/100000: episode: 272, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.069, mean reward: -0.171 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.020, 10.098], loss: 0.002706, mae: 0.051785, mean_q: -0.329015
 27300/100000: episode: 273, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.188, mean reward: -0.182 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.502, 10.098], loss: 0.002608, mae: 0.050802, mean_q: -0.298402
 27400/100000: episode: 274, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.657, mean reward: -0.197 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.989, 10.170], loss: 0.003140, mae: 0.057548, mean_q: -0.266573
 27500/100000: episode: 275, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.388, mean reward: -0.184 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.287, 10.098], loss: 0.004128, mae: 0.063953, mean_q: -0.336777
 27600/100000: episode: 276, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -12.604, mean reward: -0.126 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.154, 10.493], loss: 0.004324, mae: 0.064740, mean_q: -0.305522
 27700/100000: episode: 277, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -14.388, mean reward: -0.144 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.849, 10.098], loss: 0.003266, mae: 0.058488, mean_q: -0.296387
 27800/100000: episode: 278, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -10.746, mean reward: -0.107 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.707, 10.098], loss: 0.002875, mae: 0.054466, mean_q: -0.298225
 27900/100000: episode: 279, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -13.459, mean reward: -0.135 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.696, 10.370], loss: 0.002457, mae: 0.049830, mean_q: -0.343263
 28000/100000: episode: 280, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.068, mean reward: -0.151 [-1.000, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.424, 10.098], loss: 0.002691, mae: 0.051924, mean_q: -0.312757
 28100/100000: episode: 281, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.214, mean reward: -0.152 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.555, 10.341], loss: 0.002634, mae: 0.051656, mean_q: -0.303305
 28200/100000: episode: 282, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.635, mean reward: -0.166 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.879, 10.098], loss: 0.003051, mae: 0.056851, mean_q: -0.287381
 28300/100000: episode: 283, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -16.022, mean reward: -0.160 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.781, 10.098], loss: 0.002994, mae: 0.055233, mean_q: -0.299098
 28400/100000: episode: 284, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.427, mean reward: -0.164 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.442, 10.098], loss: 0.002659, mae: 0.051111, mean_q: -0.307617
 28500/100000: episode: 285, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.863, mean reward: -0.189 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.117, 10.204], loss: 0.002646, mae: 0.050891, mean_q: -0.333412
 28600/100000: episode: 286, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.646, mean reward: -0.186 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.343, 10.098], loss: 0.002826, mae: 0.052941, mean_q: -0.260773
 28700/100000: episode: 287, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -16.343, mean reward: -0.163 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.320, 10.098], loss: 0.002950, mae: 0.053872, mean_q: -0.312592
 28800/100000: episode: 288, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.596, mean reward: -0.156 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.840, 10.371], loss: 0.004372, mae: 0.063031, mean_q: -0.315332
 28900/100000: episode: 289, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.501, mean reward: -0.175 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.301, 10.098], loss: 0.005473, mae: 0.068485, mean_q: -0.298979
 29000/100000: episode: 290, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -20.054, mean reward: -0.201 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.558, 10.250], loss: 0.002822, mae: 0.053758, mean_q: -0.312909
 29100/100000: episode: 291, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.850, mean reward: -0.178 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.689, 10.348], loss: 0.002750, mae: 0.054523, mean_q: -0.289739
 29200/100000: episode: 292, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.866, mean reward: -0.189 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.528, 10.098], loss: 0.002776, mae: 0.052923, mean_q: -0.314733
 29300/100000: episode: 293, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -13.242, mean reward: -0.132 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.053, 10.098], loss: 0.002649, mae: 0.053273, mean_q: -0.322523
 29400/100000: episode: 294, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -9.966, mean reward: -0.100 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.331, 10.443], loss: 0.005213, mae: 0.071113, mean_q: -0.294346
 29500/100000: episode: 295, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.095, mean reward: -0.181 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.571, 10.098], loss: 0.002728, mae: 0.053070, mean_q: -0.329353
 29600/100000: episode: 296, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.254, mean reward: -0.173 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.988, 10.098], loss: 0.002781, mae: 0.052990, mean_q: -0.258509
 29700/100000: episode: 297, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.155, mean reward: -0.182 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.692, 10.098], loss: 0.002754, mae: 0.052825, mean_q: -0.289003
 29800/100000: episode: 298, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -19.202, mean reward: -0.192 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.717, 10.098], loss: 0.002703, mae: 0.051945, mean_q: -0.321981
 29900/100000: episode: 299, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -12.770, mean reward: -0.128 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.428, 10.321], loss: 0.002539, mae: 0.050044, mean_q: -0.357421
 30000/100000: episode: 300, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -13.679, mean reward: -0.137 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.302, 10.297], loss: 0.002672, mae: 0.051861, mean_q: -0.296063
 30100/100000: episode: 301, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -14.506, mean reward: -0.145 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.475, 10.202], loss: 0.008961, mae: 0.080155, mean_q: -0.333351
 30200/100000: episode: 302, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -13.592, mean reward: -0.136 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.629, 10.345], loss: 0.002855, mae: 0.054676, mean_q: -0.309859
 30300/100000: episode: 303, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.839, mean reward: -0.158 [-1.000, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.958, 10.212], loss: 0.002804, mae: 0.052973, mean_q: -0.297770
 30400/100000: episode: 304, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.846, mean reward: -0.178 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.537, 10.111], loss: 0.002862, mae: 0.055497, mean_q: -0.301329
 30500/100000: episode: 305, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -16.763, mean reward: -0.168 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.518, 10.129], loss: 0.002691, mae: 0.051488, mean_q: -0.299372
 30600/100000: episode: 306, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.356, mean reward: -0.184 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.537, 10.293], loss: 0.002561, mae: 0.050837, mean_q: -0.323651
 30700/100000: episode: 307, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -17.821, mean reward: -0.178 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.936, 10.137], loss: 0.002787, mae: 0.051040, mean_q: -0.305398
 30800/100000: episode: 308, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.571, mean reward: -0.176 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.926, 10.098], loss: 0.002633, mae: 0.050668, mean_q: -0.322176
 30900/100000: episode: 309, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.124, mean reward: -0.161 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.662, 10.098], loss: 0.002604, mae: 0.050088, mean_q: -0.319276
 31000/100000: episode: 310, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.693, mean reward: -0.147 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.622, 10.098], loss: 0.002331, mae: 0.047720, mean_q: -0.307858
 31100/100000: episode: 311, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -13.648, mean reward: -0.136 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.481, 10.447], loss: 0.002680, mae: 0.051839, mean_q: -0.281665
 31200/100000: episode: 312, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.291, mean reward: -0.193 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.998, 10.208], loss: 0.002666, mae: 0.051144, mean_q: -0.304096
 31300/100000: episode: 313, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -11.793, mean reward: -0.118 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.720, 10.451], loss: 0.002854, mae: 0.053300, mean_q: -0.290691
 31400/100000: episode: 314, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.567, mean reward: -0.196 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.244, 10.099], loss: 0.002636, mae: 0.050868, mean_q: -0.315995
 31500/100000: episode: 315, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -14.195, mean reward: -0.142 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.282, 10.098], loss: 0.002916, mae: 0.053776, mean_q: -0.298686
 31600/100000: episode: 316, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.074, mean reward: -0.191 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.420, 10.098], loss: 0.002906, mae: 0.056372, mean_q: -0.292126
 31700/100000: episode: 317, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -16.866, mean reward: -0.169 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.516, 10.098], loss: 0.002682, mae: 0.050914, mean_q: -0.327556
 31800/100000: episode: 318, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -19.337, mean reward: -0.193 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.534, 10.098], loss: 0.002507, mae: 0.049382, mean_q: -0.325195
 31900/100000: episode: 319, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.132, mean reward: -0.181 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.595, 10.098], loss: 0.002698, mae: 0.051363, mean_q: -0.343377
 32000/100000: episode: 320, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -10.555, mean reward: -0.106 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.370, 10.352], loss: 0.002670, mae: 0.051701, mean_q: -0.315765
 32100/100000: episode: 321, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.273, mean reward: -0.193 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.740, 10.098], loss: 0.002840, mae: 0.052566, mean_q: -0.276750
 32200/100000: episode: 322, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -20.620, mean reward: -0.206 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.547, 10.179], loss: 0.002734, mae: 0.052411, mean_q: -0.316713
 32300/100000: episode: 323, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.014, mean reward: -0.160 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.090, 10.252], loss: 0.003859, mae: 0.058313, mean_q: -0.296626
 32400/100000: episode: 324, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.881, mean reward: -0.179 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.009, 10.098], loss: 0.006750, mae: 0.076311, mean_q: -0.290614
 32500/100000: episode: 325, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -17.789, mean reward: -0.178 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.496, 10.300], loss: 0.004732, mae: 0.067240, mean_q: -0.308625
 32600/100000: episode: 326, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -18.700, mean reward: -0.187 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.295, 10.296], loss: 0.002826, mae: 0.053423, mean_q: -0.322712
 32700/100000: episode: 327, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -17.947, mean reward: -0.179 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.823, 10.098], loss: 0.002678, mae: 0.052382, mean_q: -0.293758
 32800/100000: episode: 328, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.303, mean reward: -0.153 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.734, 10.098], loss: 0.002848, mae: 0.052150, mean_q: -0.303608
 32900/100000: episode: 329, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.840, mean reward: -0.168 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.450, 10.209], loss: 0.002602, mae: 0.050813, mean_q: -0.323437
 33000/100000: episode: 330, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -13.090, mean reward: -0.131 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.605, 10.141], loss: 0.002550, mae: 0.050026, mean_q: -0.306764
 33100/100000: episode: 331, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.920, mean reward: -0.159 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.867, 10.098], loss: 0.002626, mae: 0.050312, mean_q: -0.311489
 33200/100000: episode: 332, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -19.375, mean reward: -0.194 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.774, 10.178], loss: 0.002547, mae: 0.049561, mean_q: -0.340187
 33300/100000: episode: 333, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.742, mean reward: -0.177 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.644, 10.148], loss: 0.002655, mae: 0.051236, mean_q: -0.301473
 33400/100000: episode: 334, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -20.652, mean reward: -0.207 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.369, 10.191], loss: 0.002672, mae: 0.051496, mean_q: -0.304569
 33500/100000: episode: 335, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -20.087, mean reward: -0.201 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.105, 10.150], loss: 0.002540, mae: 0.050188, mean_q: -0.322477
 33600/100000: episode: 336, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.797, mean reward: -0.168 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.524, 10.106], loss: 0.003783, mae: 0.059189, mean_q: -0.333618
 33700/100000: episode: 337, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.066, mean reward: -0.181 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.504, 10.109], loss: 0.002817, mae: 0.056488, mean_q: -0.300607
 33800/100000: episode: 338, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.069, mean reward: -0.171 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.125, 10.280], loss: 0.002604, mae: 0.051385, mean_q: -0.312631
 33900/100000: episode: 339, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -11.772, mean reward: -0.118 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.522, 10.524], loss: 0.002700, mae: 0.052137, mean_q: -0.290555
 34000/100000: episode: 340, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.365, mean reward: -0.184 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.467, 10.098], loss: 0.002550, mae: 0.050582, mean_q: -0.320114
 34100/100000: episode: 341, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.520, mean reward: -0.185 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.766, 10.277], loss: 0.002393, mae: 0.049299, mean_q: -0.316555
 34200/100000: episode: 342, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.499, mean reward: -0.185 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.560, 10.125], loss: 0.002695, mae: 0.051909, mean_q: -0.306910
 34300/100000: episode: 343, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -20.333, mean reward: -0.203 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.807, 10.098], loss: 0.002482, mae: 0.049532, mean_q: -0.316630
 34400/100000: episode: 344, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -18.632, mean reward: -0.186 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.180, 10.211], loss: 0.002546, mae: 0.049937, mean_q: -0.316250
 34500/100000: episode: 345, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.183, mean reward: -0.162 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.823, 10.098], loss: 0.002603, mae: 0.051131, mean_q: -0.299878
 34600/100000: episode: 346, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.200, mean reward: -0.172 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.658, 10.171], loss: 0.002417, mae: 0.048154, mean_q: -0.313064
 34700/100000: episode: 347, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.505, mean reward: -0.165 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.285, 10.247], loss: 0.002668, mae: 0.052147, mean_q: -0.302155
 34800/100000: episode: 348, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.495, mean reward: -0.195 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.783, 10.284], loss: 0.003043, mae: 0.055663, mean_q: -0.286290
 34900/100000: episode: 349, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.802, mean reward: -0.178 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.746, 10.098], loss: 0.008007, mae: 0.078775, mean_q: -0.332745
 35000/100000: episode: 350, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.877, mean reward: -0.189 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.900, 10.108], loss: 0.003579, mae: 0.061082, mean_q: -0.310988
 35100/100000: episode: 351, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.450, mean reward: -0.164 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.260, 10.098], loss: 0.003215, mae: 0.056394, mean_q: -0.345529
 35200/100000: episode: 352, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.066, mean reward: -0.191 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.184, 10.125], loss: 0.004551, mae: 0.062123, mean_q: -0.327098
 35300/100000: episode: 353, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.395, mean reward: -0.184 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.228, 10.162], loss: 0.003108, mae: 0.056147, mean_q: -0.290511
 35400/100000: episode: 354, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.600, mean reward: -0.176 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.519, 10.190], loss: 0.003300, mae: 0.057743, mean_q: -0.314440
 35500/100000: episode: 355, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.275, mean reward: -0.163 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.026, 10.222], loss: 0.003338, mae: 0.057899, mean_q: -0.326373
 35600/100000: episode: 356, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.365, mean reward: -0.164 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.057, 10.098], loss: 0.003303, mae: 0.059391, mean_q: -0.337012
 35700/100000: episode: 357, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.315, mean reward: -0.163 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.733, 10.098], loss: 0.003019, mae: 0.055631, mean_q: -0.323055
 35800/100000: episode: 358, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.215, mean reward: -0.162 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.494, 10.098], loss: 0.002959, mae: 0.055819, mean_q: -0.338509
 35900/100000: episode: 359, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -17.121, mean reward: -0.171 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.411, 10.417], loss: 0.003309, mae: 0.058548, mean_q: -0.348334
 36000/100000: episode: 360, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -13.139, mean reward: -0.131 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.653, 10.098], loss: 0.003115, mae: 0.057554, mean_q: -0.315381
 36100/100000: episode: 361, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -15.781, mean reward: -0.158 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.042, 10.098], loss: 0.003303, mae: 0.058241, mean_q: -0.316448
 36200/100000: episode: 362, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -19.467, mean reward: -0.195 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.328, 10.103], loss: 0.002847, mae: 0.054680, mean_q: -0.319284
 36300/100000: episode: 363, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -19.685, mean reward: -0.197 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.925, 10.098], loss: 0.003056, mae: 0.055219, mean_q: -0.304168
 36400/100000: episode: 364, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.021, mean reward: -0.150 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.956, 10.098], loss: 0.002954, mae: 0.056995, mean_q: -0.300731
 36500/100000: episode: 365, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.491, mean reward: -0.165 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.844, 10.098], loss: 0.002560, mae: 0.050335, mean_q: -0.349167
 36600/100000: episode: 366, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -13.386, mean reward: -0.134 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.727, 10.166], loss: 0.002526, mae: 0.050749, mean_q: -0.331068
 36700/100000: episode: 367, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.335, mean reward: -0.173 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.066, 10.301], loss: 0.002509, mae: 0.049772, mean_q: -0.292209
 36800/100000: episode: 368, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.979, mean reward: -0.200 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.639, 10.098], loss: 0.002504, mae: 0.049985, mean_q: -0.292706
 36900/100000: episode: 369, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.453, mean reward: -0.195 [-1.000, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.839, 10.190], loss: 0.002288, mae: 0.047439, mean_q: -0.344546
 37000/100000: episode: 370, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.423, mean reward: -0.164 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.641, 10.318], loss: 0.002477, mae: 0.049890, mean_q: -0.333445
 37100/100000: episode: 371, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -14.468, mean reward: -0.145 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.508, 10.122], loss: 0.002507, mae: 0.049756, mean_q: -0.313909
 37200/100000: episode: 372, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.097, mean reward: -0.161 [-1.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.650, 10.246], loss: 0.003302, mae: 0.056618, mean_q: -0.329140
 37300/100000: episode: 373, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.791, mean reward: -0.188 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.470, 10.118], loss: 0.002403, mae: 0.048970, mean_q: -0.321327
 37400/100000: episode: 374, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.569, mean reward: -0.166 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.144, 10.098], loss: 0.002609, mae: 0.050670, mean_q: -0.325795
 37500/100000: episode: 375, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.775, mean reward: -0.178 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.445, 10.269], loss: 0.002381, mae: 0.048079, mean_q: -0.289765
 37600/100000: episode: 376, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.848, mean reward: -0.168 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.341, 10.098], loss: 0.002489, mae: 0.048629, mean_q: -0.335442
 37700/100000: episode: 377, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -15.198, mean reward: -0.152 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.311, 10.098], loss: 0.002507, mae: 0.049357, mean_q: -0.331519
 37800/100000: episode: 378, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -17.025, mean reward: -0.170 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.626, 10.098], loss: 0.002492, mae: 0.048913, mean_q: -0.330722
 37900/100000: episode: 379, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.657, mean reward: -0.167 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.904, 10.251], loss: 0.002498, mae: 0.049952, mean_q: -0.330625
 38000/100000: episode: 380, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.854, mean reward: -0.179 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.990, 10.217], loss: 0.002592, mae: 0.049521, mean_q: -0.315071
 38100/100000: episode: 381, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -12.765, mean reward: -0.128 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.767, 10.098], loss: 0.002322, mae: 0.046947, mean_q: -0.376539
 38200/100000: episode: 382, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.250, mean reward: -0.173 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.803, 10.098], loss: 0.002342, mae: 0.047727, mean_q: -0.352403
 38300/100000: episode: 383, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.724, mean reward: -0.157 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.515, 10.237], loss: 0.002519, mae: 0.048736, mean_q: -0.339720
 38400/100000: episode: 384, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.185, mean reward: -0.192 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.459, 10.104], loss: 0.002383, mae: 0.048570, mean_q: -0.311207
 38500/100000: episode: 385, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.137, mean reward: -0.171 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.754, 10.098], loss: 0.002716, mae: 0.050701, mean_q: -0.347960
 38600/100000: episode: 386, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.964, mean reward: -0.200 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.131, 10.228], loss: 0.002459, mae: 0.049043, mean_q: -0.334120
 38700/100000: episode: 387, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -15.729, mean reward: -0.157 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.568, 10.268], loss: 0.002392, mae: 0.048327, mean_q: -0.328134
 38800/100000: episode: 388, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.742, mean reward: -0.167 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.731, 10.098], loss: 0.002224, mae: 0.047014, mean_q: -0.325248
 38900/100000: episode: 389, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.861, mean reward: -0.199 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.820, 10.098], loss: 0.004238, mae: 0.060157, mean_q: -0.345780
 39000/100000: episode: 390, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -16.980, mean reward: -0.170 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.033, 10.098], loss: 0.002608, mae: 0.050813, mean_q: -0.326636
 39100/100000: episode: 391, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -19.308, mean reward: -0.193 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.255, 10.098], loss: 0.002229, mae: 0.047000, mean_q: -0.304596
 39200/100000: episode: 392, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.789, mean reward: -0.188 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.186, 10.252], loss: 0.002389, mae: 0.047306, mean_q: -0.319436
 39300/100000: episode: 393, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.553, mean reward: -0.156 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.953, 10.225], loss: 0.002444, mae: 0.049013, mean_q: -0.290398
 39400/100000: episode: 394, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -6.244, mean reward: -0.062 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.634, 10.098], loss: 0.002444, mae: 0.049048, mean_q: -0.305954
 39500/100000: episode: 395, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.116, mean reward: -0.191 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.309, 10.172], loss: 0.002383, mae: 0.048449, mean_q: -0.298322
 39600/100000: episode: 396, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -11.595, mean reward: -0.116 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.771, 10.098], loss: 0.002229, mae: 0.046992, mean_q: -0.318517
 39700/100000: episode: 397, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.984, mean reward: -0.170 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.969, 10.216], loss: 0.002371, mae: 0.047878, mean_q: -0.302536
 39800/100000: episode: 398, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.354, mean reward: -0.144 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.711, 10.228], loss: 0.002448, mae: 0.049426, mean_q: -0.299001
 39900/100000: episode: 399, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -19.747, mean reward: -0.197 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.411, 10.098], loss: 0.002357, mae: 0.048101, mean_q: -0.300485
 40000/100000: episode: 400, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.422, mean reward: -0.194 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.182, 10.126], loss: 0.002436, mae: 0.048530, mean_q: -0.326857
 40100/100000: episode: 401, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -9.308, mean reward: -0.093 [-1.000, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.728, 10.098], loss: 0.002424, mae: 0.048806, mean_q: -0.322162
 40200/100000: episode: 402, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -14.722, mean reward: -0.147 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.589, 10.098], loss: 0.002480, mae: 0.049549, mean_q: -0.315223
 40300/100000: episode: 403, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.699, mean reward: -0.187 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.650, 10.098], loss: 0.002575, mae: 0.049634, mean_q: -0.341004
 40400/100000: episode: 404, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -14.600, mean reward: -0.146 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.056, 10.098], loss: 0.002447, mae: 0.049616, mean_q: -0.301329
 40500/100000: episode: 405, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.888, mean reward: -0.159 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.759, 10.270], loss: 0.002454, mae: 0.049711, mean_q: -0.319379
 40600/100000: episode: 406, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -15.738, mean reward: -0.157 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.794, 10.217], loss: 0.002405, mae: 0.049059, mean_q: -0.312731
 40700/100000: episode: 407, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.728, mean reward: -0.167 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.494, 10.443], loss: 0.002872, mae: 0.054183, mean_q: -0.290207
 40800/100000: episode: 408, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.732, mean reward: -0.187 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.731, 10.098], loss: 0.002533, mae: 0.049109, mean_q: -0.320667
 40900/100000: episode: 409, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.916, mean reward: -0.169 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.988, 10.219], loss: 0.002639, mae: 0.051325, mean_q: -0.298878
 41000/100000: episode: 410, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -19.449, mean reward: -0.194 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.885, 10.098], loss: 0.005664, mae: 0.067273, mean_q: -0.335460
 41100/100000: episode: 411, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.311, mean reward: -0.193 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.340, 10.288], loss: 0.010638, mae: 0.092612, mean_q: -0.332255
 41200/100000: episode: 412, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.099, mean reward: -0.191 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.833, 10.206], loss: 0.003019, mae: 0.055037, mean_q: -0.329218
 41300/100000: episode: 413, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -6.976, mean reward: -0.070 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.779, 10.098], loss: 0.002852, mae: 0.052962, mean_q: -0.315929
 41400/100000: episode: 414, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.804, mean reward: -0.178 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.996, 10.272], loss: 0.002974, mae: 0.055202, mean_q: -0.326278
 41500/100000: episode: 415, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.927, mean reward: -0.169 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.764, 10.098], loss: 0.002730, mae: 0.053980, mean_q: -0.325963
 41600/100000: episode: 416, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -14.291, mean reward: -0.143 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.059, 10.250], loss: 0.002947, mae: 0.054893, mean_q: -0.309565
 41700/100000: episode: 417, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.135, mean reward: -0.181 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.659, 10.098], loss: 0.003022, mae: 0.055811, mean_q: -0.321555
 41800/100000: episode: 418, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -18.210, mean reward: -0.182 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.862, 10.259], loss: 0.002697, mae: 0.053581, mean_q: -0.333924
 41900/100000: episode: 419, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -11.621, mean reward: -0.116 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.802, 10.098], loss: 0.002628, mae: 0.050935, mean_q: -0.328470
 42000/100000: episode: 420, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: -8.823, mean reward: -0.088 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.737, 10.098], loss: 0.002572, mae: 0.050772, mean_q: -0.323339
 42100/100000: episode: 421, duration: 0.669s, episode steps: 100, steps per second: 150, episode reward: -19.711, mean reward: -0.197 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.768, 10.132], loss: 0.004924, mae: 0.065100, mean_q: -0.303213
 42200/100000: episode: 422, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -12.095, mean reward: -0.121 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-1.352, 10.098], loss: 0.003089, mae: 0.057301, mean_q: -0.335731
 42300/100000: episode: 423, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.622, mean reward: -0.176 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.687, 10.209], loss: 0.002754, mae: 0.054494, mean_q: -0.284328
 42400/100000: episode: 424, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.386, mean reward: -0.194 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.378, 10.138], loss: 0.002882, mae: 0.054226, mean_q: -0.321852
 42500/100000: episode: 425, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -7.299, mean reward: -0.073 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.641, 10.483], loss: 0.002692, mae: 0.051950, mean_q: -0.318947
 42600/100000: episode: 426, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -14.612, mean reward: -0.146 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.336, 10.576], loss: 0.002628, mae: 0.051851, mean_q: -0.316713
 42700/100000: episode: 427, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.044, mean reward: -0.180 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.346, 10.098], loss: 0.002525, mae: 0.051142, mean_q: -0.329512
 42800/100000: episode: 428, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -10.685, mean reward: -0.107 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.694, 10.098], loss: 0.002896, mae: 0.054359, mean_q: -0.311045
 42900/100000: episode: 429, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.665, mean reward: -0.157 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.960, 10.098], loss: 0.002691, mae: 0.052379, mean_q: -0.300917
 43000/100000: episode: 430, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.598, mean reward: -0.176 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.516, 10.098], loss: 0.002555, mae: 0.050928, mean_q: -0.324747
 43100/100000: episode: 431, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.778, mean reward: -0.178 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.575, 10.098], loss: 0.002609, mae: 0.052343, mean_q: -0.309210
 43200/100000: episode: 432, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.384, mean reward: -0.184 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.354, 10.098], loss: 0.002796, mae: 0.054437, mean_q: -0.314157
 43300/100000: episode: 433, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.643, mean reward: -0.186 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.101, 10.311], loss: 0.002497, mae: 0.050686, mean_q: -0.325977
 43400/100000: episode: 434, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.475, mean reward: -0.185 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.163, 10.155], loss: 0.002565, mae: 0.051926, mean_q: -0.313247
 43500/100000: episode: 435, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.642, mean reward: -0.146 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.894, 10.098], loss: 0.005295, mae: 0.069533, mean_q: -0.296600
 43600/100000: episode: 436, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -13.289, mean reward: -0.133 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.114, 10.176], loss: 0.003354, mae: 0.060205, mean_q: -0.278033
 43700/100000: episode: 437, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.477, mean reward: -0.125 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.823, 10.125], loss: 0.002756, mae: 0.054382, mean_q: -0.301110
 43800/100000: episode: 438, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.687, mean reward: -0.197 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.300, 10.098], loss: 0.002646, mae: 0.054753, mean_q: -0.278720
 43900/100000: episode: 439, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.329, mean reward: -0.143 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.945, 10.098], loss: 0.002465, mae: 0.050731, mean_q: -0.295669
 44000/100000: episode: 440, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.817, mean reward: -0.198 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.479, 10.148], loss: 0.002737, mae: 0.053509, mean_q: -0.302016
 44100/100000: episode: 441, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -8.439, mean reward: -0.084 [-1.000, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.633, 10.098], loss: 0.002705, mae: 0.052483, mean_q: -0.298940
 44200/100000: episode: 442, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.095, mean reward: -0.191 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.279, 10.098], loss: 0.003115, mae: 0.057978, mean_q: -0.342996
 44300/100000: episode: 443, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -13.746, mean reward: -0.137 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.536, 10.098], loss: 0.003114, mae: 0.056796, mean_q: -0.302888
 44400/100000: episode: 444, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.472, mean reward: -0.145 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.919, 10.098], loss: 0.002606, mae: 0.052104, mean_q: -0.305899
 44500/100000: episode: 445, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.510, mean reward: -0.175 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.900, 10.098], loss: 0.002772, mae: 0.053149, mean_q: -0.317806
 44600/100000: episode: 446, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.919, mean reward: -0.189 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.549, 10.187], loss: 0.003848, mae: 0.063316, mean_q: -0.278271
 44700/100000: episode: 447, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.037, mean reward: -0.160 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.385, 10.167], loss: 0.003588, mae: 0.062816, mean_q: -0.291775
 44800/100000: episode: 448, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.315, mean reward: -0.173 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.402, 10.098], loss: 0.002625, mae: 0.052441, mean_q: -0.280607
 44900/100000: episode: 449, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.847, mean reward: -0.168 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.723, 10.204], loss: 0.002538, mae: 0.051640, mean_q: -0.337598
 45000/100000: episode: 450, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.636, mean reward: -0.196 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.750, 10.098], loss: 0.002461, mae: 0.051050, mean_q: -0.296703
 45100/100000: episode: 451, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.627, mean reward: -0.186 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.381, 10.190], loss: 0.002742, mae: 0.053873, mean_q: -0.295594
 45200/100000: episode: 452, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.240, mean reward: -0.172 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.008, 10.174], loss: 0.002781, mae: 0.053722, mean_q: -0.306437
 45300/100000: episode: 453, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.808, mean reward: -0.188 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.633, 10.228], loss: 0.002949, mae: 0.055661, mean_q: -0.287141
 45400/100000: episode: 454, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -16.034, mean reward: -0.160 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.825, 10.098], loss: 0.002747, mae: 0.053716, mean_q: -0.282599
 45500/100000: episode: 455, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.492, mean reward: -0.165 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.938, 10.315], loss: 0.002556, mae: 0.052300, mean_q: -0.290634
 45600/100000: episode: 456, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.375, mean reward: -0.174 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.769, 10.322], loss: 0.002720, mae: 0.054194, mean_q: -0.296150
 45700/100000: episode: 457, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.030, mean reward: -0.170 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.760, 10.217], loss: 0.002774, mae: 0.053493, mean_q: -0.271895
 45800/100000: episode: 458, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.606, mean reward: -0.166 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.581, 10.315], loss: 0.002702, mae: 0.052778, mean_q: -0.287420
 45900/100000: episode: 459, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -10.801, mean reward: -0.108 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.722, 10.098], loss: 0.003312, mae: 0.059024, mean_q: -0.358053
 46000/100000: episode: 460, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -14.431, mean reward: -0.144 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.721, 10.438], loss: 0.002901, mae: 0.055860, mean_q: -0.286028
 46100/100000: episode: 461, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.198, mean reward: -0.182 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.960, 10.098], loss: 0.002617, mae: 0.052795, mean_q: -0.281499
 46200/100000: episode: 462, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -13.957, mean reward: -0.140 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.334, 10.098], loss: 0.002608, mae: 0.051942, mean_q: -0.293175
 46300/100000: episode: 463, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.020, mean reward: -0.150 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.194, 10.098], loss: 0.002878, mae: 0.054725, mean_q: -0.275330
 46400/100000: episode: 464, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -14.630, mean reward: -0.146 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.113, 10.098], loss: 0.002721, mae: 0.053606, mean_q: -0.270897
 46500/100000: episode: 465, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -10.200, mean reward: -0.102 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.214, 10.427], loss: 0.002911, mae: 0.054999, mean_q: -0.305671
 46600/100000: episode: 466, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -12.258, mean reward: -0.123 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.374, 10.465], loss: 0.003462, mae: 0.059656, mean_q: -0.303829
 46700/100000: episode: 467, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.668, mean reward: -0.177 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.589, 10.132], loss: 0.002775, mae: 0.053546, mean_q: -0.260909
 46800/100000: episode: 468, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -12.536, mean reward: -0.125 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.686, 10.098], loss: 0.002854, mae: 0.054835, mean_q: -0.274411
 46900/100000: episode: 469, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.848, mean reward: -0.168 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.043, 10.129], loss: 0.002637, mae: 0.051406, mean_q: -0.312397
 47000/100000: episode: 470, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.279, mean reward: -0.183 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.726, 10.165], loss: 0.002683, mae: 0.052159, mean_q: -0.323841
 47100/100000: episode: 471, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.174, mean reward: -0.182 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.135, 10.103], loss: 0.002920, mae: 0.055060, mean_q: -0.293441
 47200/100000: episode: 472, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.380, mean reward: -0.184 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.179, 10.098], loss: 0.002642, mae: 0.052892, mean_q: -0.291765
 47300/100000: episode: 473, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.674, mean reward: -0.167 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.833, 10.120], loss: 0.003033, mae: 0.056946, mean_q: -0.311815
 47400/100000: episode: 474, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -7.151, mean reward: -0.072 [-1.000, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.116, 10.098], loss: 0.002653, mae: 0.052874, mean_q: -0.283149
 47500/100000: episode: 475, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.705, mean reward: -0.197 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.978, 10.098], loss: 0.002684, mae: 0.052544, mean_q: -0.309431
 47600/100000: episode: 476, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.084, mean reward: -0.191 [-1.000, 0.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.461, 10.098], loss: 0.002760, mae: 0.053886, mean_q: -0.289504
 47700/100000: episode: 477, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -19.531, mean reward: -0.195 [-1.000, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.599, 10.186], loss: 0.002763, mae: 0.053439, mean_q: -0.259804
 47800/100000: episode: 478, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.476, mean reward: -0.165 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.452, 10.098], loss: 0.002686, mae: 0.053357, mean_q: -0.306125
 47900/100000: episode: 479, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.184, mean reward: -0.162 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.356, 10.345], loss: 0.002706, mae: 0.052931, mean_q: -0.299170
 48000/100000: episode: 480, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.407, mean reward: -0.174 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.001, 10.216], loss: 0.002611, mae: 0.051773, mean_q: -0.318261
 48100/100000: episode: 481, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.335, mean reward: -0.193 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.804, 10.098], loss: 0.002547, mae: 0.051083, mean_q: -0.316091
 48200/100000: episode: 482, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.103, mean reward: -0.181 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.122, 10.098], loss: 0.002672, mae: 0.052258, mean_q: -0.299942
 48300/100000: episode: 483, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.193, mean reward: -0.172 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.744, 10.217], loss: 0.002661, mae: 0.051915, mean_q: -0.297076
 48400/100000: episode: 484, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.538, mean reward: -0.195 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.085, 10.166], loss: 0.002633, mae: 0.051911, mean_q: -0.311194
 48500/100000: episode: 485, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -12.030, mean reward: -0.120 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.380, 10.098], loss: 0.003382, mae: 0.054147, mean_q: -0.304433
 48600/100000: episode: 486, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.149, mean reward: -0.171 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.180, 10.158], loss: 0.007275, mae: 0.072450, mean_q: -0.344593
 48700/100000: episode: 487, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -12.705, mean reward: -0.127 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.008, 10.405], loss: 0.002746, mae: 0.053118, mean_q: -0.261009
 48800/100000: episode: 488, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.904, mean reward: -0.189 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.408, 10.098], loss: 0.002687, mae: 0.051824, mean_q: -0.323471
 48900/100000: episode: 489, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.577, mean reward: -0.156 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.563, 10.098], loss: 0.002582, mae: 0.050784, mean_q: -0.300653
 49000/100000: episode: 490, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -19.281, mean reward: -0.193 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.309, 10.184], loss: 0.002737, mae: 0.052790, mean_q: -0.302642
 49100/100000: episode: 491, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.936, mean reward: -0.169 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.943, 10.189], loss: 0.002650, mae: 0.051111, mean_q: -0.322497
 49200/100000: episode: 492, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.538, mean reward: -0.185 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.754, 10.098], loss: 0.002501, mae: 0.050581, mean_q: -0.327137
 49300/100000: episode: 493, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.986, mean reward: -0.170 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.240, 10.169], loss: 0.002467, mae: 0.049593, mean_q: -0.319063
 49400/100000: episode: 494, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.430, mean reward: -0.174 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.533, 10.209], loss: 0.002437, mae: 0.048940, mean_q: -0.341631
 49500/100000: episode: 495, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -20.414, mean reward: -0.204 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.199, 10.098], loss: 0.002416, mae: 0.049699, mean_q: -0.316375
 49600/100000: episode: 496, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.531, mean reward: -0.155 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.172, 10.276], loss: 0.002912, mae: 0.054543, mean_q: -0.297301
 49700/100000: episode: 497, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.467, mean reward: -0.155 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.935, 10.308], loss: 0.002805, mae: 0.053149, mean_q: -0.298001
 49800/100000: episode: 498, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.543, mean reward: -0.175 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.247, 10.192], loss: 0.002779, mae: 0.052143, mean_q: -0.306107
 49900/100000: episode: 499, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -15.783, mean reward: -0.158 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.828, 10.342], loss: 0.002721, mae: 0.053338, mean_q: -0.303952
 50000/100000: episode: 500, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.374, mean reward: -0.194 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.701, 10.103], loss: 0.002681, mae: 0.051996, mean_q: -0.320224
 50100/100000: episode: 501, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.837, mean reward: -0.178 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.935, 10.110], loss: 0.002233, mae: 0.047144, mean_q: -0.352201
 50200/100000: episode: 502, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.791, mean reward: -0.168 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.337, 10.098], loss: 0.002570, mae: 0.050447, mean_q: -0.305098
 50300/100000: episode: 503, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.385, mean reward: -0.194 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.334, 10.145], loss: 0.002490, mae: 0.049744, mean_q: -0.304303
 50400/100000: episode: 504, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.938, mean reward: -0.189 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.282, 10.098], loss: 0.002424, mae: 0.049565, mean_q: -0.309930
 50500/100000: episode: 505, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.794, mean reward: -0.188 [-1.000, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.639, 10.098], loss: 0.002588, mae: 0.050820, mean_q: -0.306665
 50600/100000: episode: 506, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.740, mean reward: -0.187 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.888, 10.132], loss: 0.004039, mae: 0.058692, mean_q: -0.317059
 50700/100000: episode: 507, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -8.164, mean reward: -0.082 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.593, 10.336], loss: 0.005629, mae: 0.071789, mean_q: -0.343116
 50800/100000: episode: 508, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -19.458, mean reward: -0.195 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.768, 10.127], loss: 0.002922, mae: 0.054760, mean_q: -0.308424
 50900/100000: episode: 509, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -17.503, mean reward: -0.175 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.679, 10.312], loss: 0.002744, mae: 0.052986, mean_q: -0.300480
 51000/100000: episode: 510, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -19.095, mean reward: -0.191 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.685, 10.098], loss: 0.002522, mae: 0.050614, mean_q: -0.329243
 51100/100000: episode: 511, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.979, mean reward: -0.190 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.783, 10.098], loss: 0.002493, mae: 0.050720, mean_q: -0.329035
 51200/100000: episode: 512, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -13.775, mean reward: -0.138 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.260, 10.216], loss: 0.002438, mae: 0.049491, mean_q: -0.306799
 51300/100000: episode: 513, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.752, mean reward: -0.158 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.952, 10.322], loss: 0.002458, mae: 0.050511, mean_q: -0.294098
 51400/100000: episode: 514, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.873, mean reward: -0.189 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.655, 10.098], loss: 0.002700, mae: 0.053833, mean_q: -0.309850
 51500/100000: episode: 515, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -13.342, mean reward: -0.133 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.698, 10.103], loss: 0.002519, mae: 0.051901, mean_q: -0.296187
 51600/100000: episode: 516, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.671, mean reward: -0.167 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.344, 10.188], loss: 0.003869, mae: 0.060084, mean_q: -0.329712
 51700/100000: episode: 517, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.060, mean reward: -0.181 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.822, 10.098], loss: 0.002461, mae: 0.049870, mean_q: -0.323463
 51800/100000: episode: 518, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.436, mean reward: -0.164 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.860, 10.098], loss: 0.002569, mae: 0.051653, mean_q: -0.335333
 51900/100000: episode: 519, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.870, mean reward: -0.199 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.311, 10.148], loss: 0.002369, mae: 0.048618, mean_q: -0.314961
 52000/100000: episode: 520, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.126, mean reward: -0.161 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.429, 10.098], loss: 0.002392, mae: 0.049440, mean_q: -0.302014
 52100/100000: episode: 521, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.598, mean reward: -0.186 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.558, 10.269], loss: 0.002322, mae: 0.048078, mean_q: -0.296759
 52200/100000: episode: 522, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -12.558, mean reward: -0.126 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.223, 10.098], loss: 0.002475, mae: 0.050038, mean_q: -0.326404
 52300/100000: episode: 523, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.774, mean reward: -0.208 [-1.000, 0.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.699, 10.119], loss: 0.002406, mae: 0.049227, mean_q: -0.322323
 52400/100000: episode: 524, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.656, mean reward: -0.187 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.519, 10.098], loss: 0.002293, mae: 0.048004, mean_q: -0.326433
 52500/100000: episode: 525, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.027, mean reward: -0.180 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.112, 10.098], loss: 0.002352, mae: 0.048343, mean_q: -0.298601
 52600/100000: episode: 526, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.828, mean reward: -0.188 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.454, 10.098], loss: 0.002359, mae: 0.049031, mean_q: -0.357592
 52700/100000: episode: 527, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -13.365, mean reward: -0.134 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.467, 10.372], loss: 0.002293, mae: 0.048693, mean_q: -0.325588
 52800/100000: episode: 528, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.321, mean reward: -0.153 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.660, 10.098], loss: 0.002799, mae: 0.053760, mean_q: -0.332771
 52900/100000: episode: 529, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -11.567, mean reward: -0.116 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.238, 10.098], loss: 0.002349, mae: 0.049254, mean_q: -0.295383
 53000/100000: episode: 530, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -12.835, mean reward: -0.128 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.637, 10.098], loss: 0.002549, mae: 0.050927, mean_q: -0.332955
 53100/100000: episode: 531, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.589, mean reward: -0.176 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.400, 10.320], loss: 0.003886, mae: 0.061254, mean_q: -0.309826
 53200/100000: episode: 532, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -19.543, mean reward: -0.195 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.208, 10.259], loss: 0.002774, mae: 0.051009, mean_q: -0.319946
 53300/100000: episode: 533, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.632, mean reward: -0.146 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.173, 10.168], loss: 0.003382, mae: 0.060086, mean_q: -0.340096
 53400/100000: episode: 534, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -12.495, mean reward: -0.125 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.774, 10.098], loss: 0.002404, mae: 0.050755, mean_q: -0.312344
 53500/100000: episode: 535, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.350, mean reward: -0.174 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.798, 10.125], loss: 0.002279, mae: 0.048121, mean_q: -0.303130
 53600/100000: episode: 536, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.258, mean reward: -0.183 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.679, 10.142], loss: 0.002384, mae: 0.049312, mean_q: -0.323282
 53700/100000: episode: 537, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -17.176, mean reward: -0.172 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.346, 10.098], loss: 0.002285, mae: 0.047723, mean_q: -0.359999
 53800/100000: episode: 538, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -16.810, mean reward: -0.168 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.173, 10.098], loss: 0.002254, mae: 0.047968, mean_q: -0.328552
 53900/100000: episode: 539, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -14.830, mean reward: -0.148 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.675, 10.098], loss: 0.002462, mae: 0.050565, mean_q: -0.333223
 54000/100000: episode: 540, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.350, mean reward: -0.164 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.005, 10.098], loss: 0.002540, mae: 0.049689, mean_q: -0.332976
 54100/100000: episode: 541, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.887, mean reward: -0.179 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.549, 10.098], loss: 0.006788, mae: 0.074181, mean_q: -0.319578
 54200/100000: episode: 542, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.777, mean reward: -0.178 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.113, 10.098], loss: 0.002445, mae: 0.050061, mean_q: -0.316982
 54300/100000: episode: 543, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.171, mean reward: -0.162 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.149, 10.098], loss: 0.002379, mae: 0.048897, mean_q: -0.318332
 54400/100000: episode: 544, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -10.899, mean reward: -0.109 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.832, 10.098], loss: 0.002386, mae: 0.047699, mean_q: -0.344264
 54500/100000: episode: 545, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.939, mean reward: -0.189 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.469, 10.326], loss: 0.002296, mae: 0.047327, mean_q: -0.340206
 54600/100000: episode: 546, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -19.595, mean reward: -0.196 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.816, 10.098], loss: 0.002575, mae: 0.050206, mean_q: -0.305684
 54700/100000: episode: 547, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.157, mean reward: -0.162 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.932, 10.169], loss: 0.002452, mae: 0.049208, mean_q: -0.301977
 54800/100000: episode: 548, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.528, mean reward: -0.185 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.614, 10.156], loss: 0.002381, mae: 0.048167, mean_q: -0.295050
 54900/100000: episode: 549, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.207, mean reward: -0.162 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.362, 10.114], loss: 0.002335, mae: 0.048040, mean_q: -0.338915
 55000/100000: episode: 550, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.390, mean reward: -0.184 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.689, 10.210], loss: 0.002355, mae: 0.048156, mean_q: -0.323561
 55100/100000: episode: 551, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.006, mean reward: -0.150 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.717, 10.098], loss: 0.002301, mae: 0.047901, mean_q: -0.325805
 55200/100000: episode: 552, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -16.839, mean reward: -0.168 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.495, 10.171], loss: 0.002306, mae: 0.048429, mean_q: -0.294623
 55300/100000: episode: 553, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.745, mean reward: -0.187 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.217, 10.098], loss: 0.002261, mae: 0.048194, mean_q: -0.328678
 55400/100000: episode: 554, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -16.382, mean reward: -0.164 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.489, 10.425], loss: 0.002519, mae: 0.049221, mean_q: -0.354239
 55500/100000: episode: 555, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -17.173, mean reward: -0.172 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.690, 10.240], loss: 0.002320, mae: 0.048711, mean_q: -0.280232
 55600/100000: episode: 556, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.098, mean reward: -0.161 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.075, 10.238], loss: 0.002183, mae: 0.047369, mean_q: -0.329416
 55700/100000: episode: 557, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -16.623, mean reward: -0.166 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.651, 10.098], loss: 0.002341, mae: 0.048579, mean_q: -0.311440
 55800/100000: episode: 558, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.541, mean reward: -0.165 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.662, 10.098], loss: 0.003252, mae: 0.056443, mean_q: -0.323077
 55900/100000: episode: 559, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.099, mean reward: -0.181 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.077, 10.149], loss: 0.002484, mae: 0.052398, mean_q: -0.309276
 56000/100000: episode: 560, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -14.832, mean reward: -0.148 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.998, 10.098], loss: 0.002361, mae: 0.049420, mean_q: -0.299176
 56100/100000: episode: 561, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.792, mean reward: -0.168 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.851, 10.098], loss: 0.002247, mae: 0.047366, mean_q: -0.319269
 56200/100000: episode: 562, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -10.766, mean reward: -0.108 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.009, 10.098], loss: 0.002314, mae: 0.047879, mean_q: -0.316807
 56300/100000: episode: 563, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -13.623, mean reward: -0.136 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.425, 10.098], loss: 0.002480, mae: 0.050447, mean_q: -0.339659
 56400/100000: episode: 564, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.222, mean reward: -0.172 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.888, 10.190], loss: 0.003194, mae: 0.056974, mean_q: -0.288267
 56500/100000: episode: 565, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -11.042, mean reward: -0.110 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.979, 10.140], loss: 0.003226, mae: 0.056194, mean_q: -0.274341
 56600/100000: episode: 566, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.877, mean reward: -0.169 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.689, 10.098], loss: 0.002387, mae: 0.048536, mean_q: -0.340319
 56700/100000: episode: 567, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.023, mean reward: -0.170 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.383, 10.098], loss: 0.002379, mae: 0.049347, mean_q: -0.292946
 56800/100000: episode: 568, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.563, mean reward: -0.186 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.356, 10.141], loss: 0.002502, mae: 0.050571, mean_q: -0.319930
 56900/100000: episode: 569, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.629, mean reward: -0.186 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.198, 10.228], loss: 0.003475, mae: 0.057030, mean_q: -0.304537
 57000/100000: episode: 570, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.295, mean reward: -0.163 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.083, 10.157], loss: 0.002234, mae: 0.047722, mean_q: -0.322709
 57100/100000: episode: 571, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.857, mean reward: -0.169 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.977, 10.098], loss: 0.002483, mae: 0.049177, mean_q: -0.319388
 57200/100000: episode: 572, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -12.696, mean reward: -0.127 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.438, 10.098], loss: 0.002537, mae: 0.050554, mean_q: -0.314993
 57300/100000: episode: 573, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.578, mean reward: -0.196 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.409, 10.158], loss: 0.002377, mae: 0.048939, mean_q: -0.281351
 57400/100000: episode: 574, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -16.012, mean reward: -0.160 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.568, 10.324], loss: 0.002472, mae: 0.050161, mean_q: -0.308618
 57500/100000: episode: 575, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -15.182, mean reward: -0.152 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.469, 10.182], loss: 0.002390, mae: 0.049059, mean_q: -0.309955
 57600/100000: episode: 576, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.704, mean reward: -0.187 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.058, 10.098], loss: 0.002233, mae: 0.047520, mean_q: -0.305756
 57700/100000: episode: 577, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.140, mean reward: -0.151 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.047, 10.177], loss: 0.002519, mae: 0.050185, mean_q: -0.308765
 57800/100000: episode: 578, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.089, mean reward: -0.191 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.170, 10.237], loss: 0.002419, mae: 0.049336, mean_q: -0.296809
 57900/100000: episode: 579, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.098, mean reward: -0.181 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.271, 10.213], loss: 0.002406, mae: 0.049056, mean_q: -0.296373
 58000/100000: episode: 580, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.524, mean reward: -0.185 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.515, 10.125], loss: 0.002811, mae: 0.052043, mean_q: -0.326829
 58100/100000: episode: 581, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -20.058, mean reward: -0.201 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.550, 10.098], loss: 0.003615, mae: 0.061344, mean_q: -0.296266
 58200/100000: episode: 582, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.256, mean reward: -0.153 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.771, 10.098], loss: 0.002540, mae: 0.051791, mean_q: -0.304893
 58300/100000: episode: 583, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -14.235, mean reward: -0.142 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.022, 10.161], loss: 0.002622, mae: 0.051535, mean_q: -0.312495
 58400/100000: episode: 584, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.639, mean reward: -0.186 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.518, 10.183], loss: 0.002907, mae: 0.054939, mean_q: -0.334107
 58500/100000: episode: 585, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.130, mean reward: -0.191 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.934, 10.098], loss: 0.002452, mae: 0.050885, mean_q: -0.367079
 58600/100000: episode: 586, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.255, mean reward: -0.183 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.406, 10.129], loss: 0.002370, mae: 0.048997, mean_q: -0.330351
 58700/100000: episode: 587, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -14.436, mean reward: -0.144 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.417, 10.418], loss: 0.002420, mae: 0.048533, mean_q: -0.309938
 58800/100000: episode: 588, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.671, mean reward: -0.187 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.655, 10.098], loss: 0.002460, mae: 0.050031, mean_q: -0.313292
 58900/100000: episode: 589, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.546, mean reward: -0.175 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.661, 10.137], loss: 0.002425, mae: 0.049204, mean_q: -0.300605
 59000/100000: episode: 590, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.352, mean reward: -0.194 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.227, 10.098], loss: 0.002392, mae: 0.048877, mean_q: -0.332013
 59100/100000: episode: 591, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -11.002, mean reward: -0.110 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.751, 10.098], loss: 0.002611, mae: 0.051220, mean_q: -0.305229
 59200/100000: episode: 592, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.801, mean reward: -0.188 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.039, 10.268], loss: 0.002495, mae: 0.051271, mean_q: -0.310304
 59300/100000: episode: 593, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.817, mean reward: -0.178 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.687, 10.180], loss: 0.002402, mae: 0.049079, mean_q: -0.320698
 59400/100000: episode: 594, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.572, mean reward: -0.146 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.439, 10.098], loss: 0.002427, mae: 0.050741, mean_q: -0.298745
 59500/100000: episode: 595, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -14.561, mean reward: -0.146 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.497, 10.282], loss: 0.002272, mae: 0.047507, mean_q: -0.291369
 59600/100000: episode: 596, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -20.378, mean reward: -0.204 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.047, 10.098], loss: 0.003825, mae: 0.062391, mean_q: -0.316518
 59700/100000: episode: 597, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -11.948, mean reward: -0.119 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.597, 10.275], loss: 0.002947, mae: 0.055897, mean_q: -0.322678
 59800/100000: episode: 598, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.340, mean reward: -0.173 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.756, 10.098], loss: 0.002391, mae: 0.049704, mean_q: -0.326816
 59900/100000: episode: 599, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -17.986, mean reward: -0.180 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.510, 10.237], loss: 0.003816, mae: 0.062294, mean_q: -0.336183
 60000/100000: episode: 600, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.296, mean reward: -0.173 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.938, 10.111], loss: 0.002298, mae: 0.048861, mean_q: -0.294361
 60100/100000: episode: 601, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -12.412, mean reward: -0.124 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.491, 10.098], loss: 0.002531, mae: 0.050776, mean_q: -0.279226
 60200/100000: episode: 602, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.183, mean reward: -0.192 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.682, 10.137], loss: 0.002302, mae: 0.047634, mean_q: -0.319700
 60300/100000: episode: 603, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -17.218, mean reward: -0.172 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.578, 10.098], loss: 0.002480, mae: 0.050686, mean_q: -0.241881
 60400/100000: episode: 604, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -13.875, mean reward: -0.139 [-1.000, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.054, 10.098], loss: 0.002439, mae: 0.048890, mean_q: -0.327449
 60500/100000: episode: 605, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -10.667, mean reward: -0.107 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.935, 10.462], loss: 0.002535, mae: 0.050698, mean_q: -0.285712
 60600/100000: episode: 606, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -12.181, mean reward: -0.122 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.678, 10.098], loss: 0.002542, mae: 0.050526, mean_q: -0.324229
 60700/100000: episode: 607, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.765, mean reward: -0.168 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.952, 10.388], loss: 0.002612, mae: 0.050193, mean_q: -0.311044
 60800/100000: episode: 608, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.063, mean reward: -0.191 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.740, 10.243], loss: 0.002545, mae: 0.049995, mean_q: -0.296320
 60900/100000: episode: 609, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.631, mean reward: -0.166 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.369, 10.098], loss: 0.002575, mae: 0.049624, mean_q: -0.317579
 61000/100000: episode: 610, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.589, mean reward: -0.156 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.689, 10.252], loss: 0.002526, mae: 0.049655, mean_q: -0.331133
 61100/100000: episode: 611, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.339, mean reward: -0.143 [-1.000, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.577, 10.098], loss: 0.002846, mae: 0.053000, mean_q: -0.273566
 61200/100000: episode: 612, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.742, mean reward: -0.177 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.765, 10.234], loss: 0.002637, mae: 0.051227, mean_q: -0.294802
 61300/100000: episode: 613, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.114, mean reward: -0.141 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.183, 10.309], loss: 0.003244, mae: 0.056951, mean_q: -0.294826
 61400/100000: episode: 614, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -11.183, mean reward: -0.112 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.882, 10.431], loss: 0.003835, mae: 0.060978, mean_q: -0.348254
 61500/100000: episode: 615, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -14.725, mean reward: -0.147 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.167, 10.098], loss: 0.002818, mae: 0.054521, mean_q: -0.299963
 61600/100000: episode: 616, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -12.055, mean reward: -0.121 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.846, 10.253], loss: 0.002515, mae: 0.051528, mean_q: -0.321091
 61700/100000: episode: 617, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.700, mean reward: -0.177 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.857, 10.098], loss: 0.002557, mae: 0.050953, mean_q: -0.288137
 61800/100000: episode: 618, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.379, mean reward: -0.194 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.155, 10.211], loss: 0.002566, mae: 0.051186, mean_q: -0.300488
 61900/100000: episode: 619, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.088, mean reward: -0.161 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.391, 10.134], loss: 0.002335, mae: 0.047398, mean_q: -0.315871
 62000/100000: episode: 620, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.001, mean reward: -0.170 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.236, 10.098], loss: 0.002379, mae: 0.048943, mean_q: -0.285740
 62100/100000: episode: 621, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.674, mean reward: -0.187 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.144, 10.098], loss: 0.003045, mae: 0.054835, mean_q: -0.293708
 62200/100000: episode: 622, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.433, mean reward: -0.174 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.754, 10.250], loss: 0.003805, mae: 0.061416, mean_q: -0.321147
 62300/100000: episode: 623, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.137, mean reward: -0.171 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.057, 10.260], loss: 0.002765, mae: 0.053297, mean_q: -0.299772
 62400/100000: episode: 624, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.174, mean reward: -0.172 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.524, 10.263], loss: 0.002574, mae: 0.049790, mean_q: -0.346607
 62500/100000: episode: 625, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.696, mean reward: -0.177 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.565, 10.098], loss: 0.002358, mae: 0.048364, mean_q: -0.319719
 62600/100000: episode: 626, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -18.447, mean reward: -0.184 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.410, 10.250], loss: 0.002499, mae: 0.049163, mean_q: -0.312085
 62700/100000: episode: 627, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -19.491, mean reward: -0.195 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.607, 10.241], loss: 0.002728, mae: 0.050612, mean_q: -0.326553
 62800/100000: episode: 628, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.734, mean reward: -0.177 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.294, 10.218], loss: 0.002458, mae: 0.048688, mean_q: -0.325521
 62900/100000: episode: 629, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -13.160, mean reward: -0.132 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.394, 10.105], loss: 0.002437, mae: 0.049012, mean_q: -0.327054
 63000/100000: episode: 630, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -13.406, mean reward: -0.134 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.019, 10.258], loss: 0.002644, mae: 0.050090, mean_q: -0.312871
 63100/100000: episode: 631, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.514, mean reward: -0.145 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.308, 10.098], loss: 0.002537, mae: 0.049885, mean_q: -0.311810
 63200/100000: episode: 632, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.010, mean reward: -0.190 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.245, 10.098], loss: 0.002604, mae: 0.050346, mean_q: -0.303318
 63300/100000: episode: 633, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -12.003, mean reward: -0.120 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.289, 10.098], loss: 0.002501, mae: 0.048926, mean_q: -0.296369
 63400/100000: episode: 634, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -7.696, mean reward: -0.077 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.120, 10.098], loss: 0.002922, mae: 0.054594, mean_q: -0.266593
 63500/100000: episode: 635, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.499, mean reward: -0.185 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.724, 10.098], loss: 0.003148, mae: 0.056285, mean_q: -0.289523
 63600/100000: episode: 636, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -20.720, mean reward: -0.207 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.354, 10.098], loss: 0.002703, mae: 0.053059, mean_q: -0.313302
 63700/100000: episode: 637, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.310, mean reward: -0.173 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.299, 10.098], loss: 0.002631, mae: 0.051892, mean_q: -0.286499
 63800/100000: episode: 638, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.233, mean reward: -0.172 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.831, 10.343], loss: 0.003542, mae: 0.056148, mean_q: -0.274736
 63900/100000: episode: 639, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -10.321, mean reward: -0.103 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.110, 10.098], loss: 0.003142, mae: 0.058765, mean_q: -0.290778
 64000/100000: episode: 640, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.338, mean reward: -0.163 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.155, 10.098], loss: 0.002465, mae: 0.049393, mean_q: -0.318558
 64100/100000: episode: 641, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -19.739, mean reward: -0.197 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.727, 10.098], loss: 0.003003, mae: 0.056756, mean_q: -0.280119
 64200/100000: episode: 642, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.845, mean reward: -0.168 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.983, 10.175], loss: 0.002494, mae: 0.049724, mean_q: -0.317058
 64300/100000: episode: 643, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.406, mean reward: -0.174 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.197, 10.101], loss: 0.002759, mae: 0.052367, mean_q: -0.280473
 64400/100000: episode: 644, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.622, mean reward: -0.186 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.288, 10.098], loss: 0.002722, mae: 0.052503, mean_q: -0.276540
 64500/100000: episode: 645, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -15.973, mean reward: -0.160 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.347, 10.098], loss: 0.003639, mae: 0.061890, mean_q: -0.308697
 64600/100000: episode: 646, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.409, mean reward: -0.164 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.641, 10.410], loss: 0.002626, mae: 0.050069, mean_q: -0.321802
 64700/100000: episode: 647, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.414, mean reward: -0.144 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.433, 10.222], loss: 0.002670, mae: 0.051219, mean_q: -0.280326
 64800/100000: episode: 648, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -9.512, mean reward: -0.095 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.171, 10.329], loss: 0.002584, mae: 0.049995, mean_q: -0.325070
 64900/100000: episode: 649, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.201, mean reward: -0.142 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.536, 10.250], loss: 0.002594, mae: 0.050406, mean_q: -0.313905
 65000/100000: episode: 650, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.524, mean reward: -0.155 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.023, 10.183], loss: 0.002718, mae: 0.052950, mean_q: -0.263134
 65100/100000: episode: 651, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.380, mean reward: -0.164 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.489, 10.098], loss: 0.002915, mae: 0.054544, mean_q: -0.283000
 65200/100000: episode: 652, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.156, mean reward: -0.172 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.668, 10.098], loss: 0.002695, mae: 0.050871, mean_q: -0.309214
 65300/100000: episode: 653, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.534, mean reward: -0.145 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.856, 10.098], loss: 0.002666, mae: 0.051236, mean_q: -0.287091
 65400/100000: episode: 654, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.836, mean reward: -0.168 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.182, 10.098], loss: 0.002460, mae: 0.050427, mean_q: -0.301084
 65500/100000: episode: 655, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.723, mean reward: -0.187 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.518, 10.298], loss: 0.002751, mae: 0.052827, mean_q: -0.289148
 65600/100000: episode: 656, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.238, mean reward: -0.192 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.596, 10.191], loss: 0.002646, mae: 0.051756, mean_q: -0.318672
 65700/100000: episode: 657, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.716, mean reward: -0.167 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.917, 10.252], loss: 0.002579, mae: 0.051539, mean_q: -0.294525
 65800/100000: episode: 658, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -9.691, mean reward: -0.097 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.545, 10.098], loss: 0.002517, mae: 0.050760, mean_q: -0.271056
 65900/100000: episode: 659, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.202, mean reward: -0.182 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.959, 10.203], loss: 0.002691, mae: 0.053315, mean_q: -0.304754
 66000/100000: episode: 660, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.793, mean reward: -0.198 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.745, 10.098], loss: 0.002725, mae: 0.052737, mean_q: -0.306176
 66100/100000: episode: 661, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.402, mean reward: -0.164 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.295, 10.170], loss: 0.002443, mae: 0.049531, mean_q: -0.280403
 66200/100000: episode: 662, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.472, mean reward: -0.145 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.625, 10.305], loss: 0.002554, mae: 0.050408, mean_q: -0.315845
 66300/100000: episode: 663, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -16.770, mean reward: -0.168 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.220, 10.098], loss: 0.002603, mae: 0.052457, mean_q: -0.302768
 66400/100000: episode: 664, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -13.300, mean reward: -0.133 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.051, 10.407], loss: 0.003294, mae: 0.056916, mean_q: -0.300587
 66500/100000: episode: 665, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -12.126, mean reward: -0.121 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.550, 10.098], loss: 0.006341, mae: 0.071045, mean_q: -0.303286
 66600/100000: episode: 666, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.297, mean reward: -0.143 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.649, 10.098], loss: 0.002571, mae: 0.051239, mean_q: -0.304603
 66700/100000: episode: 667, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -13.898, mean reward: -0.139 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.650, 10.098], loss: 0.002574, mae: 0.050769, mean_q: -0.303413
 66800/100000: episode: 668, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -14.287, mean reward: -0.143 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.309, 10.098], loss: 0.002787, mae: 0.053027, mean_q: -0.313763
 66900/100000: episode: 669, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.987, mean reward: -0.170 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.428, 10.178], loss: 0.002816, mae: 0.053746, mean_q: -0.271578
 67000/100000: episode: 670, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.018, mean reward: -0.180 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.276, 10.098], loss: 0.002462, mae: 0.049843, mean_q: -0.311120
 67100/100000: episode: 671, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -15.598, mean reward: -0.156 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.894, 10.325], loss: 0.002480, mae: 0.049889, mean_q: -0.289727
 67200/100000: episode: 672, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.954, mean reward: -0.190 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.205, 10.193], loss: 0.002518, mae: 0.050364, mean_q: -0.309575
 67300/100000: episode: 673, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.234, mean reward: -0.182 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.058, 10.098], loss: 0.002606, mae: 0.051727, mean_q: -0.332031
 67400/100000: episode: 674, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.993, mean reward: -0.180 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.360, 10.205], loss: 0.002518, mae: 0.050754, mean_q: -0.297051
 67500/100000: episode: 675, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -7.072, mean reward: -0.071 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.563, 10.154], loss: 0.002817, mae: 0.055085, mean_q: -0.308155
 67600/100000: episode: 676, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.496, mean reward: -0.155 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.487, 10.098], loss: 0.002881, mae: 0.055190, mean_q: -0.321484
 67700/100000: episode: 677, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.106, mean reward: -0.181 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.585, 10.098], loss: 0.002683, mae: 0.052276, mean_q: -0.321258
 67800/100000: episode: 678, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.159, mean reward: -0.142 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.586, 10.304], loss: 0.002652, mae: 0.050965, mean_q: -0.288989
 67900/100000: episode: 679, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.055, mean reward: -0.191 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.578, 10.098], loss: 0.002760, mae: 0.052736, mean_q: -0.286799
 68000/100000: episode: 680, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.055, mean reward: -0.181 [-1.000, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.791, 10.159], loss: 0.002791, mae: 0.053331, mean_q: -0.280213
 68100/100000: episode: 681, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.518, mean reward: -0.145 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.275, 10.339], loss: 0.002752, mae: 0.052917, mean_q: -0.321622
 68200/100000: episode: 682, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.224, mean reward: -0.192 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.703, 10.098], loss: 0.002716, mae: 0.052926, mean_q: -0.299348
 68300/100000: episode: 683, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.163, mean reward: -0.192 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.358, 10.136], loss: 0.002705, mae: 0.051613, mean_q: -0.335031
 68400/100000: episode: 684, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.480, mean reward: -0.185 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.460, 10.202], loss: 0.002717, mae: 0.052614, mean_q: -0.276525
 68500/100000: episode: 685, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.826, mean reward: -0.178 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.433, 10.098], loss: 0.002776, mae: 0.053634, mean_q: -0.294038
 68600/100000: episode: 686, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.300, mean reward: -0.203 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.621, 10.098], loss: 0.003281, mae: 0.058088, mean_q: -0.302821
 68700/100000: episode: 687, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.332, mean reward: -0.183 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.146, 10.098], loss: 0.002899, mae: 0.054227, mean_q: -0.310380
 68800/100000: episode: 688, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -14.748, mean reward: -0.147 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.686, 10.098], loss: 0.002786, mae: 0.053520, mean_q: -0.295528
 68900/100000: episode: 689, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.957, mean reward: -0.180 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.834, 10.140], loss: 0.002769, mae: 0.052838, mean_q: -0.276530
 69000/100000: episode: 690, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.929, mean reward: -0.179 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.606, 10.157], loss: 0.002860, mae: 0.055315, mean_q: -0.270958
 69100/100000: episode: 691, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.148, mean reward: -0.171 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.900, 10.098], loss: 0.002783, mae: 0.052450, mean_q: -0.297357
 69200/100000: episode: 692, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.119, mean reward: -0.191 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.995, 10.157], loss: 0.002632, mae: 0.051469, mean_q: -0.317234
 69300/100000: episode: 693, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -13.075, mean reward: -0.131 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.125, 10.192], loss: 0.002737, mae: 0.053352, mean_q: -0.311633
 69400/100000: episode: 694, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.983, mean reward: -0.190 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.090, 10.098], loss: 0.003369, mae: 0.058922, mean_q: -0.250903
 69500/100000: episode: 695, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -17.246, mean reward: -0.172 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.660, 10.098], loss: 0.002840, mae: 0.053856, mean_q: -0.301066
 69600/100000: episode: 696, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.809, mean reward: -0.198 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.612, 10.143], loss: 0.002719, mae: 0.052766, mean_q: -0.324006
 69700/100000: episode: 697, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.370, mean reward: -0.174 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.069, 10.131], loss: 0.002885, mae: 0.055288, mean_q: -0.297929
 69800/100000: episode: 698, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.881, mean reward: -0.209 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.982, 10.098], loss: 0.002768, mae: 0.053945, mean_q: -0.340021
 69900/100000: episode: 699, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.986, mean reward: -0.170 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.827, 10.098], loss: 0.002735, mae: 0.052944, mean_q: -0.316365
 70000/100000: episode: 700, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -20.046, mean reward: -0.200 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.387, 10.162], loss: 0.002859, mae: 0.055350, mean_q: -0.292364
 70100/100000: episode: 701, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.250, mean reward: -0.172 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.823, 10.118], loss: 0.002904, mae: 0.054977, mean_q: -0.325008
 70200/100000: episode: 702, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.546, mean reward: -0.165 [-1.000, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.744, 10.098], loss: 0.002611, mae: 0.051769, mean_q: -0.319435
 70300/100000: episode: 703, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.613, mean reward: -0.156 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.883, 10.098], loss: 0.002728, mae: 0.052291, mean_q: -0.321365
 70400/100000: episode: 704, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -11.598, mean reward: -0.116 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.684, 10.268], loss: 0.002650, mae: 0.052473, mean_q: -0.299226
 70500/100000: episode: 705, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.801, mean reward: -0.158 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.879, 10.272], loss: 0.003011, mae: 0.056503, mean_q: -0.315671
 70600/100000: episode: 706, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -11.327, mean reward: -0.113 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.374, 10.098], loss: 0.002764, mae: 0.053416, mean_q: -0.324590
 70700/100000: episode: 707, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.508, mean reward: -0.165 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.200, 10.159], loss: 0.002730, mae: 0.052552, mean_q: -0.315009
 70800/100000: episode: 708, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -20.094, mean reward: -0.201 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.154, 10.341], loss: 0.002645, mae: 0.050787, mean_q: -0.340979
 70900/100000: episode: 709, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.095, mean reward: -0.171 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.259, 10.098], loss: 0.002718, mae: 0.052247, mean_q: -0.308073
 71000/100000: episode: 710, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.758, mean reward: -0.188 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.888, 10.098], loss: 0.002774, mae: 0.052529, mean_q: -0.304479
 71100/100000: episode: 711, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -11.815, mean reward: -0.118 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.020, 10.098], loss: 0.002909, mae: 0.053782, mean_q: -0.323068
 71200/100000: episode: 712, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.886, mean reward: -0.169 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.492, 10.217], loss: 0.002701, mae: 0.051780, mean_q: -0.323782
 71300/100000: episode: 713, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.529, mean reward: -0.165 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.010, 10.098], loss: 0.003249, mae: 0.057440, mean_q: -0.289225
 71400/100000: episode: 714, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.416, mean reward: -0.154 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.757, 10.120], loss: 0.004227, mae: 0.066276, mean_q: -0.322267
 71500/100000: episode: 715, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.302, mean reward: -0.193 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.703, 10.098], loss: 0.002778, mae: 0.051613, mean_q: -0.333866
 71600/100000: episode: 716, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.904, mean reward: -0.159 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.484, 10.098], loss: 0.002724, mae: 0.051940, mean_q: -0.314471
 71700/100000: episode: 717, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -21.226, mean reward: -0.212 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.434, 10.123], loss: 0.002735, mae: 0.051664, mean_q: -0.341865
 71800/100000: episode: 718, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -13.002, mean reward: -0.130 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.165, 10.339], loss: 0.003775, mae: 0.057561, mean_q: -0.325824
 71900/100000: episode: 719, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.820, mean reward: -0.178 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.306, 10.098], loss: 0.003378, mae: 0.057750, mean_q: -0.324142
 72000/100000: episode: 720, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.971, mean reward: -0.180 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.226, 10.098], loss: 0.002983, mae: 0.055468, mean_q: -0.335017
 72100/100000: episode: 721, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.357, mean reward: -0.174 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.366, 10.151], loss: 0.002957, mae: 0.055093, mean_q: -0.278017
 72200/100000: episode: 722, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -16.733, mean reward: -0.167 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.680, 10.145], loss: 0.002965, mae: 0.055686, mean_q: -0.323829
 72300/100000: episode: 723, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.898, mean reward: -0.189 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.270, 10.281], loss: 0.002907, mae: 0.054616, mean_q: -0.321857
 72400/100000: episode: 724, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.050, mean reward: -0.191 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.198, 10.142], loss: 0.002772, mae: 0.051888, mean_q: -0.314199
 72500/100000: episode: 725, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.670, mean reward: -0.177 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.731, 10.209], loss: 0.002785, mae: 0.052095, mean_q: -0.312504
 72600/100000: episode: 726, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -10.145, mean reward: -0.101 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.609, 10.098], loss: 0.002599, mae: 0.051149, mean_q: -0.313107
 72700/100000: episode: 727, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.481, mean reward: -0.165 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.035, 10.098], loss: 0.002589, mae: 0.051042, mean_q: -0.314327
 72800/100000: episode: 728, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -21.110, mean reward: -0.211 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.677, 10.098], loss: 0.002511, mae: 0.049809, mean_q: -0.344042
 72900/100000: episode: 729, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -13.236, mean reward: -0.132 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.375, 10.161], loss: 0.002715, mae: 0.050567, mean_q: -0.330744
 73000/100000: episode: 730, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.466, mean reward: -0.145 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.469, 10.183], loss: 0.002897, mae: 0.054456, mean_q: -0.309973
 73100/100000: episode: 731, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -15.932, mean reward: -0.159 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.278, 10.098], loss: 0.002749, mae: 0.051453, mean_q: -0.310008
 73200/100000: episode: 732, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -20.894, mean reward: -0.209 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.502, 10.158], loss: 0.002749, mae: 0.052433, mean_q: -0.300851
 73300/100000: episode: 733, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -7.832, mean reward: -0.078 [-1.000, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.701, 10.098], loss: 0.002575, mae: 0.050473, mean_q: -0.315920
 73400/100000: episode: 734, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -12.929, mean reward: -0.129 [-1.000, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.040, 10.560], loss: 0.002813, mae: 0.052844, mean_q: -0.295940
 73500/100000: episode: 735, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.861, mean reward: -0.199 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.702, 10.098], loss: 0.003036, mae: 0.054848, mean_q: -0.301117
 73600/100000: episode: 736, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -20.087, mean reward: -0.201 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.288, 10.185], loss: 0.002930, mae: 0.053724, mean_q: -0.325850
 73700/100000: episode: 737, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -13.907, mean reward: -0.139 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.019, 10.244], loss: 0.002814, mae: 0.053609, mean_q: -0.310504
 73800/100000: episode: 738, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.461, mean reward: -0.195 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.344, 10.235], loss: 0.002755, mae: 0.052350, mean_q: -0.344869
 73900/100000: episode: 739, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.444, mean reward: -0.194 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.752, 10.098], loss: 0.003791, mae: 0.061147, mean_q: -0.313045
 74000/100000: episode: 740, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -18.050, mean reward: -0.180 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.183, 10.213], loss: 0.002780, mae: 0.053087, mean_q: -0.307146
 74100/100000: episode: 741, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.792, mean reward: -0.148 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.860, 10.126], loss: 0.002864, mae: 0.052960, mean_q: -0.300584
 74200/100000: episode: 742, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.445, mean reward: -0.134 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.892, 10.098], loss: 0.002740, mae: 0.051915, mean_q: -0.325971
 74300/100000: episode: 743, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.424, mean reward: -0.144 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.938, 10.098], loss: 0.002896, mae: 0.054522, mean_q: -0.340465
 74400/100000: episode: 744, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.071, mean reward: -0.141 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.130, 10.098], loss: 0.002970, mae: 0.055083, mean_q: -0.289476
 74500/100000: episode: 745, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -11.016, mean reward: -0.110 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.632, 10.098], loss: 0.002690, mae: 0.051651, mean_q: -0.288279
 74600/100000: episode: 746, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -20.147, mean reward: -0.201 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.097, 10.098], loss: 0.002677, mae: 0.050691, mean_q: -0.305617
 74700/100000: episode: 747, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.700, mean reward: -0.177 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.886, 10.408], loss: 0.002879, mae: 0.053634, mean_q: -0.301346
 74800/100000: episode: 748, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.300, mean reward: -0.163 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.287, 10.196], loss: 0.002817, mae: 0.052387, mean_q: -0.313701
 74900/100000: episode: 749, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.529, mean reward: -0.175 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.853, 10.231], loss: 0.002744, mae: 0.052336, mean_q: -0.299911
 75000/100000: episode: 750, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.504, mean reward: -0.185 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.002, 10.098], loss: 0.002753, mae: 0.052437, mean_q: -0.298294
 75100/100000: episode: 751, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.685, mean reward: -0.137 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.470, 10.098], loss: 0.002793, mae: 0.051542, mean_q: -0.320982
 75200/100000: episode: 752, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.981, mean reward: -0.180 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.421, 10.098], loss: 0.002728, mae: 0.051435, mean_q: -0.312379
 75300/100000: episode: 753, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.828, mean reward: -0.168 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.562, 10.098], loss: 0.002842, mae: 0.053403, mean_q: -0.299345
 75400/100000: episode: 754, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.524, mean reward: -0.155 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.802, 10.098], loss: 0.004635, mae: 0.066575, mean_q: -0.275325
 75500/100000: episode: 755, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.688, mean reward: -0.167 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.378, 10.098], loss: 0.002921, mae: 0.055968, mean_q: -0.264152
 75600/100000: episode: 756, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.853, mean reward: -0.199 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.710, 10.098], loss: 0.002835, mae: 0.054128, mean_q: -0.324658
 75700/100000: episode: 757, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.324, mean reward: -0.193 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.983, 10.158], loss: 0.002666, mae: 0.052675, mean_q: -0.294516
 75800/100000: episode: 758, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.322, mean reward: -0.163 [-1.000, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.699, 10.098], loss: 0.002720, mae: 0.051833, mean_q: -0.317249
 75900/100000: episode: 759, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.626, mean reward: -0.186 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.561, 10.098], loss: 0.002534, mae: 0.049683, mean_q: -0.342689
 76000/100000: episode: 760, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -11.797, mean reward: -0.118 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.979, 10.098], loss: 0.002677, mae: 0.051645, mean_q: -0.310320
 76100/100000: episode: 761, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.940, mean reward: -0.179 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.519, 10.098], loss: 0.002492, mae: 0.049055, mean_q: -0.341196
 76200/100000: episode: 762, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.222, mean reward: -0.192 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.521, 10.159], loss: 0.002353, mae: 0.047805, mean_q: -0.327324
 76300/100000: episode: 763, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -15.022, mean reward: -0.150 [-1.000, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.670, 10.280], loss: 0.002790, mae: 0.052502, mean_q: -0.316397
 76400/100000: episode: 764, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -20.642, mean reward: -0.206 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.294, 10.225], loss: 0.002626, mae: 0.052525, mean_q: -0.284179
 76500/100000: episode: 765, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.721, mean reward: -0.167 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.866, 10.098], loss: 0.002652, mae: 0.051041, mean_q: -0.313348
 76600/100000: episode: 766, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -12.624, mean reward: -0.126 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.281, 10.355], loss: 0.002571, mae: 0.050476, mean_q: -0.303764
 76700/100000: episode: 767, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.573, mean reward: -0.156 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.163, 10.160], loss: 0.002448, mae: 0.049341, mean_q: -0.307706
 76800/100000: episode: 768, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -13.499, mean reward: -0.135 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.992, 10.112], loss: 0.002604, mae: 0.051362, mean_q: -0.294802
 76900/100000: episode: 769, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -14.162, mean reward: -0.142 [-1.000, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.416, 10.098], loss: 0.003982, mae: 0.061756, mean_q: -0.275997
 77000/100000: episode: 770, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.325, mean reward: -0.173 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.734, 10.371], loss: 0.004263, mae: 0.066081, mean_q: -0.299041
 77100/100000: episode: 771, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -15.516, mean reward: -0.155 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.509, 10.098], loss: 0.002496, mae: 0.050454, mean_q: -0.304314
 77200/100000: episode: 772, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.097, mean reward: -0.191 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.772, 10.178], loss: 0.002625, mae: 0.050931, mean_q: -0.310852
 77300/100000: episode: 773, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.640, mean reward: -0.176 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.013, 10.186], loss: 0.002458, mae: 0.048589, mean_q: -0.303337
 77400/100000: episode: 774, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -19.254, mean reward: -0.193 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.694, 10.127], loss: 0.002684, mae: 0.052712, mean_q: -0.298587
 77500/100000: episode: 775, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.132, mean reward: -0.151 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.596, 10.098], loss: 0.002518, mae: 0.049678, mean_q: -0.340117
 77600/100000: episode: 776, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.754, mean reward: -0.188 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.083, 10.098], loss: 0.002749, mae: 0.052232, mean_q: -0.328480
 77700/100000: episode: 777, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.598, mean reward: -0.176 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.496, 10.098], loss: 0.002733, mae: 0.051951, mean_q: -0.299668
 77800/100000: episode: 778, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -14.196, mean reward: -0.142 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.629, 10.314], loss: 0.002950, mae: 0.054160, mean_q: -0.314262
 77900/100000: episode: 779, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.274, mean reward: -0.163 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.565, 10.213], loss: 0.002565, mae: 0.049317, mean_q: -0.331614
 78000/100000: episode: 780, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -15.176, mean reward: -0.152 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.652, 10.235], loss: 0.002669, mae: 0.052861, mean_q: -0.314547
 78100/100000: episode: 781, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -13.040, mean reward: -0.130 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.444, 10.489], loss: 0.003037, mae: 0.055844, mean_q: -0.276443
 78200/100000: episode: 782, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.731, mean reward: -0.187 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.025, 10.276], loss: 0.002659, mae: 0.050793, mean_q: -0.299586
 78300/100000: episode: 783, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -17.135, mean reward: -0.171 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.134, 10.098], loss: 0.002726, mae: 0.052645, mean_q: -0.309478
 78400/100000: episode: 784, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.960, mean reward: -0.190 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.579, 10.221], loss: 0.002761, mae: 0.051035, mean_q: -0.297779
 78500/100000: episode: 785, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -11.792, mean reward: -0.118 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.953, 10.098], loss: 0.002588, mae: 0.050582, mean_q: -0.298777
 78600/100000: episode: 786, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.783, mean reward: -0.178 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.745, 10.296], loss: 0.004612, mae: 0.066878, mean_q: -0.316686
 78700/100000: episode: 787, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.936, mean reward: -0.169 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.059, 10.098], loss: 0.002638, mae: 0.051983, mean_q: -0.294065
 78800/100000: episode: 788, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.235, mean reward: -0.192 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.848, 10.285], loss: 0.002619, mae: 0.051680, mean_q: -0.311531
 78900/100000: episode: 789, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.557, mean reward: -0.146 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.076, 10.111], loss: 0.002725, mae: 0.052183, mean_q: -0.318725
 79000/100000: episode: 790, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -6.209, mean reward: -0.062 [-1.000, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.770, 10.404], loss: 0.002499, mae: 0.049045, mean_q: -0.336710
 79100/100000: episode: 791, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.629, mean reward: -0.166 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.812, 10.098], loss: 0.002605, mae: 0.052195, mean_q: -0.284461
 79200/100000: episode: 792, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.911, mean reward: -0.189 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.854, 10.098], loss: 0.002551, mae: 0.050426, mean_q: -0.311147
 79300/100000: episode: 793, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -8.058, mean reward: -0.081 [-1.000, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.546, 10.312], loss: 0.002531, mae: 0.050370, mean_q: -0.293762
 79400/100000: episode: 794, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.122, mean reward: -0.171 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.638, 10.111], loss: 0.002477, mae: 0.049456, mean_q: -0.292563
 79500/100000: episode: 795, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -14.670, mean reward: -0.147 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.871, 10.245], loss: 0.002531, mae: 0.049868, mean_q: -0.333505
 79600/100000: episode: 796, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.207, mean reward: -0.192 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.631, 10.203], loss: 0.002582, mae: 0.051401, mean_q: -0.293879
 79700/100000: episode: 797, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.365, mean reward: -0.184 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.682, 10.098], loss: 0.002642, mae: 0.051776, mean_q: -0.281078
 79800/100000: episode: 798, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.318, mean reward: -0.193 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.465, 10.098], loss: 0.002606, mae: 0.050872, mean_q: -0.321492
 79900/100000: episode: 799, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.528, mean reward: -0.185 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.471, 10.098], loss: 0.002682, mae: 0.052172, mean_q: -0.309347
 80000/100000: episode: 800, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.742, mean reward: -0.197 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.316, 10.098], loss: 0.003391, mae: 0.058594, mean_q: -0.328654
 80100/100000: episode: 801, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.716, mean reward: -0.177 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.412, 10.098], loss: 0.002684, mae: 0.051030, mean_q: -0.329606
 80200/100000: episode: 802, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -8.302, mean reward: -0.083 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.081, 10.098], loss: 0.003043, mae: 0.054522, mean_q: -0.335381
 80300/100000: episode: 803, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.220, mean reward: -0.192 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.306, 10.098], loss: 0.002618, mae: 0.052185, mean_q: -0.309479
 80400/100000: episode: 804, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -10.153, mean reward: -0.102 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.753, 10.098], loss: 0.002660, mae: 0.050468, mean_q: -0.300422
 80500/100000: episode: 805, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.934, mean reward: -0.189 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.584, 10.190], loss: 0.003108, mae: 0.056991, mean_q: -0.271784
 80600/100000: episode: 806, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.272, mean reward: -0.173 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.165, 10.123], loss: 0.002770, mae: 0.052167, mean_q: -0.296683
 80700/100000: episode: 807, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.330, mean reward: -0.193 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.761, 10.098], loss: 0.002616, mae: 0.050188, mean_q: -0.307472
 80800/100000: episode: 808, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.662, mean reward: -0.157 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.233, 10.098], loss: 0.002600, mae: 0.049300, mean_q: -0.342637
 80900/100000: episode: 809, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -14.505, mean reward: -0.145 [-1.000, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.966, 10.274], loss: 0.002574, mae: 0.049907, mean_q: -0.291490
 81000/100000: episode: 810, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.889, mean reward: -0.189 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.117, 10.198], loss: 0.002989, mae: 0.055294, mean_q: -0.312061
 81100/100000: episode: 811, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.701, mean reward: -0.187 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.789, 10.148], loss: 0.002860, mae: 0.054831, mean_q: -0.310613
 81200/100000: episode: 812, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.176, mean reward: -0.172 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.794, 10.363], loss: 0.002721, mae: 0.051754, mean_q: -0.304011
 81300/100000: episode: 813, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -12.953, mean reward: -0.130 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.763, 10.486], loss: 0.002663, mae: 0.050781, mean_q: -0.334778
 81400/100000: episode: 814, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.636, mean reward: -0.176 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.534, 10.098], loss: 0.003082, mae: 0.057460, mean_q: -0.296517
 81500/100000: episode: 815, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -13.830, mean reward: -0.138 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.583, 10.098], loss: 0.002854, mae: 0.053873, mean_q: -0.303014
 81600/100000: episode: 816, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.104, mean reward: -0.191 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.015, 10.098], loss: 0.002825, mae: 0.052543, mean_q: -0.316640
 81700/100000: episode: 817, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.366, mean reward: -0.194 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.894, 10.268], loss: 0.002884, mae: 0.053669, mean_q: -0.301114
 81800/100000: episode: 818, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.309, mean reward: -0.163 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.731, 10.102], loss: 0.002798, mae: 0.052357, mean_q: -0.290619
 81900/100000: episode: 819, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.310, mean reward: -0.163 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.261, 10.326], loss: 0.002604, mae: 0.050306, mean_q: -0.309017
 82000/100000: episode: 820, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.530, mean reward: -0.185 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.942, 10.156], loss: 0.002780, mae: 0.051971, mean_q: -0.325387
 82100/100000: episode: 821, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.947, mean reward: -0.169 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.530, 10.291], loss: 0.002698, mae: 0.052472, mean_q: -0.280332
 82200/100000: episode: 822, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.903, mean reward: -0.179 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.504, 10.119], loss: 0.002670, mae: 0.053221, mean_q: -0.318617
 82300/100000: episode: 823, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.821, mean reward: -0.198 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.938, 10.157], loss: 0.002675, mae: 0.051008, mean_q: -0.321077
 82400/100000: episode: 824, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -16.319, mean reward: -0.163 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.540, 10.098], loss: 0.002595, mae: 0.050793, mean_q: -0.311927
 82500/100000: episode: 825, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -15.695, mean reward: -0.157 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-2.073, 10.294], loss: 0.002459, mae: 0.048846, mean_q: -0.331103
 82600/100000: episode: 826, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -16.789, mean reward: -0.168 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.481, 10.106], loss: 0.004091, mae: 0.063542, mean_q: -0.323700
 82700/100000: episode: 827, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.260, mean reward: -0.173 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.494, 10.264], loss: 0.002580, mae: 0.051607, mean_q: -0.323158
 82800/100000: episode: 828, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.341, mean reward: -0.183 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.068, 10.098], loss: 0.002666, mae: 0.051169, mean_q: -0.304477
 82900/100000: episode: 829, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.869, mean reward: -0.199 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.999, 10.194], loss: 0.002843, mae: 0.053217, mean_q: -0.321857
 83000/100000: episode: 830, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -13.324, mean reward: -0.133 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.323, 10.098], loss: 0.002733, mae: 0.054502, mean_q: -0.321039
 83100/100000: episode: 831, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.501, mean reward: -0.165 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.831, 10.359], loss: 0.002465, mae: 0.049018, mean_q: -0.317734
 83200/100000: episode: 832, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.074, mean reward: -0.181 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.464, 10.098], loss: 0.002308, mae: 0.048323, mean_q: -0.303189
 83300/100000: episode: 833, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.582, mean reward: -0.146 [-1.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.624, 10.098], loss: 0.002652, mae: 0.050224, mean_q: -0.308199
 83400/100000: episode: 834, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.614, mean reward: -0.166 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.656, 10.308], loss: 0.002689, mae: 0.051686, mean_q: -0.319259
 83500/100000: episode: 835, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.670, mean reward: -0.187 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.617, 10.291], loss: 0.002474, mae: 0.050258, mean_q: -0.269434
 83600/100000: episode: 836, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.314, mean reward: -0.173 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.350, 10.111], loss: 0.002864, mae: 0.054269, mean_q: -0.324638
 83700/100000: episode: 837, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.309, mean reward: -0.153 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.962, 10.181], loss: 0.003220, mae: 0.056608, mean_q: -0.312926
 83800/100000: episode: 838, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.259, mean reward: -0.163 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.697, 10.100], loss: 0.002588, mae: 0.052469, mean_q: -0.293211
 83900/100000: episode: 839, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -15.508, mean reward: -0.155 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.778, 10.157], loss: 0.002690, mae: 0.052313, mean_q: -0.332113
 84000/100000: episode: 840, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.975, mean reward: -0.180 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.914, 10.098], loss: 0.002724, mae: 0.052460, mean_q: -0.330502
 84100/100000: episode: 841, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.821, mean reward: -0.178 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.635, 10.098], loss: 0.002596, mae: 0.050545, mean_q: -0.342690
 84200/100000: episode: 842, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -14.078, mean reward: -0.141 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.639, 10.299], loss: 0.002471, mae: 0.049052, mean_q: -0.316243
 84300/100000: episode: 843, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.470, mean reward: -0.145 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.084, 10.184], loss: 0.004404, mae: 0.059926, mean_q: -0.323392
 84400/100000: episode: 844, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -20.810, mean reward: -0.208 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.911, 10.098], loss: 0.003342, mae: 0.058303, mean_q: -0.286280
 84500/100000: episode: 845, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.168, mean reward: -0.182 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.995, 10.263], loss: 0.002442, mae: 0.049227, mean_q: -0.308802
 84600/100000: episode: 846, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -19.079, mean reward: -0.191 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.639, 10.325], loss: 0.002415, mae: 0.048594, mean_q: -0.330976
 84700/100000: episode: 847, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -13.290, mean reward: -0.133 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.663, 10.377], loss: 0.002513, mae: 0.049185, mean_q: -0.324252
 84800/100000: episode: 848, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -13.341, mean reward: -0.133 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.435, 10.255], loss: 0.002787, mae: 0.052689, mean_q: -0.297398
 84900/100000: episode: 849, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.976, mean reward: -0.160 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.421, 10.098], loss: 0.003148, mae: 0.058378, mean_q: -0.310038
 85000/100000: episode: 850, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.868, mean reward: -0.169 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.579, 10.098], loss: 0.002788, mae: 0.052615, mean_q: -0.330184
 85100/100000: episode: 851, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -12.322, mean reward: -0.123 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.394, 10.148], loss: 0.002359, mae: 0.047513, mean_q: -0.318470
 85200/100000: episode: 852, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.565, mean reward: -0.186 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.614, 10.098], loss: 0.002782, mae: 0.053890, mean_q: -0.303158
 85300/100000: episode: 853, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.757, mean reward: -0.188 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.018, 10.098], loss: 0.002531, mae: 0.049600, mean_q: -0.323220
 85400/100000: episode: 854, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.428, mean reward: -0.164 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.779, 10.198], loss: 0.002529, mae: 0.050627, mean_q: -0.308468
 85500/100000: episode: 855, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -14.026, mean reward: -0.140 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.396, 10.098], loss: 0.002362, mae: 0.047925, mean_q: -0.282686
 85600/100000: episode: 856, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -10.906, mean reward: -0.109 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.973, 10.114], loss: 0.002482, mae: 0.049609, mean_q: -0.319079
 85700/100000: episode: 857, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -10.375, mean reward: -0.104 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.026, 10.098], loss: 0.002327, mae: 0.048246, mean_q: -0.302844
 85800/100000: episode: 858, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.901, mean reward: -0.149 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.298, 10.098], loss: 0.002367, mae: 0.047757, mean_q: -0.298373
 85900/100000: episode: 859, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.449, mean reward: -0.144 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.811, 10.098], loss: 0.002597, mae: 0.052686, mean_q: -0.312609
 86000/100000: episode: 860, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -20.370, mean reward: -0.204 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.380, 10.098], loss: 0.002658, mae: 0.050869, mean_q: -0.294721
 86100/100000: episode: 861, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -16.309, mean reward: -0.163 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.388, 10.164], loss: 0.002407, mae: 0.048825, mean_q: -0.310299
 86200/100000: episode: 862, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.634, mean reward: -0.196 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.108, 10.148], loss: 0.002402, mae: 0.049225, mean_q: -0.311487
 86300/100000: episode: 863, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -15.983, mean reward: -0.160 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.528, 10.162], loss: 0.002573, mae: 0.052269, mean_q: -0.283798
 86400/100000: episode: 864, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.564, mean reward: -0.176 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.639, 10.098], loss: 0.002352, mae: 0.049195, mean_q: -0.346902
 86500/100000: episode: 865, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.768, mean reward: -0.178 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.198, 10.135], loss: 0.002563, mae: 0.050714, mean_q: -0.287778
 86600/100000: episode: 866, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.701, mean reward: -0.177 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.575, 10.116], loss: 0.003011, mae: 0.057988, mean_q: -0.341440
 86700/100000: episode: 867, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.992, mean reward: -0.190 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.207, 10.098], loss: 0.002522, mae: 0.051367, mean_q: -0.320783
 86800/100000: episode: 868, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.840, mean reward: -0.178 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.684, 10.152], loss: 0.002303, mae: 0.048183, mean_q: -0.350817
 86900/100000: episode: 869, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.750, mean reward: -0.178 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.753, 10.157], loss: 0.002557, mae: 0.052512, mean_q: -0.313521
 87000/100000: episode: 870, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -12.606, mean reward: -0.126 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.426, 10.098], loss: 0.003061, mae: 0.056160, mean_q: -0.338200
 87100/100000: episode: 871, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -17.921, mean reward: -0.179 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.717, 10.098], loss: 0.002420, mae: 0.048584, mean_q: -0.322441
 87200/100000: episode: 872, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -20.524, mean reward: -0.205 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.949, 10.098], loss: 0.002464, mae: 0.049546, mean_q: -0.343320
 87300/100000: episode: 873, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.918, mean reward: -0.149 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.951, 10.101], loss: 0.002515, mae: 0.049235, mean_q: -0.305529
 87400/100000: episode: 874, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.204, mean reward: -0.162 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.981, 10.180], loss: 0.002820, mae: 0.054389, mean_q: -0.305797
 87500/100000: episode: 875, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.396, mean reward: -0.184 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.267, 10.098], loss: 0.002439, mae: 0.049326, mean_q: -0.286590
 87600/100000: episode: 876, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.568, mean reward: -0.186 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.837, 10.111], loss: 0.002477, mae: 0.049689, mean_q: -0.342118
 87700/100000: episode: 877, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -11.095, mean reward: -0.111 [-1.000, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.581, 10.098], loss: 0.002436, mae: 0.049029, mean_q: -0.296049
 87800/100000: episode: 878, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -12.170, mean reward: -0.122 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.779, 10.098], loss: 0.002406, mae: 0.048768, mean_q: -0.336841
 87900/100000: episode: 879, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.332, mean reward: -0.173 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.963, 10.098], loss: 0.002479, mae: 0.050148, mean_q: -0.278527
 88000/100000: episode: 880, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.785, mean reward: -0.148 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.648, 10.098], loss: 0.002356, mae: 0.048235, mean_q: -0.290704
 88100/100000: episode: 881, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.251, mean reward: -0.163 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.710, 10.098], loss: 0.002898, mae: 0.054629, mean_q: -0.303067
 88200/100000: episode: 882, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.643, mean reward: -0.176 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.519, 10.126], loss: 0.002687, mae: 0.054071, mean_q: -0.331906
 88300/100000: episode: 883, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.192, mean reward: -0.172 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.817, 10.186], loss: 0.002335, mae: 0.047470, mean_q: -0.332236
 88400/100000: episode: 884, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.660, mean reward: -0.177 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.619, 10.286], loss: 0.002546, mae: 0.049879, mean_q: -0.322279
 88500/100000: episode: 885, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.742, mean reward: -0.167 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.096, 10.098], loss: 0.003090, mae: 0.057241, mean_q: -0.302493
 88600/100000: episode: 886, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -18.933, mean reward: -0.189 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.725, 10.155], loss: 0.002327, mae: 0.047810, mean_q: -0.290448
 88700/100000: episode: 887, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -12.262, mean reward: -0.123 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.126, 10.230], loss: 0.002550, mae: 0.050884, mean_q: -0.299385
 88800/100000: episode: 888, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.680, mean reward: -0.167 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.810, 10.275], loss: 0.002560, mae: 0.051207, mean_q: -0.300538
 88900/100000: episode: 889, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.625, mean reward: -0.186 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.197, 10.101], loss: 0.002426, mae: 0.049028, mean_q: -0.313441
 89000/100000: episode: 890, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.816, mean reward: -0.178 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.900, 10.210], loss: 0.002907, mae: 0.054300, mean_q: -0.300977
 89100/100000: episode: 891, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -10.205, mean reward: -0.102 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.113, 10.098], loss: 0.002696, mae: 0.052631, mean_q: -0.278959
 89200/100000: episode: 892, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.268, mean reward: -0.193 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.178, 10.098], loss: 0.002500, mae: 0.050077, mean_q: -0.288154
 89300/100000: episode: 893, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -17.298, mean reward: -0.173 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.972, 10.199], loss: 0.002672, mae: 0.052804, mean_q: -0.301717
 89400/100000: episode: 894, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.615, mean reward: -0.146 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.311, 10.362], loss: 0.002992, mae: 0.053752, mean_q: -0.302396
 89500/100000: episode: 895, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.411, mean reward: -0.164 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.702, 10.098], loss: 0.002848, mae: 0.055008, mean_q: -0.313990
 89600/100000: episode: 896, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.267, mean reward: -0.193 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.467, 10.098], loss: 0.002459, mae: 0.049239, mean_q: -0.275369
 89700/100000: episode: 897, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.194, mean reward: -0.162 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.145, 10.167], loss: 0.002523, mae: 0.050534, mean_q: -0.286037
 89800/100000: episode: 898, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.094, mean reward: -0.181 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.410, 10.119], loss: 0.002359, mae: 0.047205, mean_q: -0.287642
 89900/100000: episode: 899, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.084, mean reward: -0.181 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.838, 10.169], loss: 0.002375, mae: 0.048264, mean_q: -0.340051
 90000/100000: episode: 900, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.295, mean reward: -0.163 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.751, 10.231], loss: 0.002384, mae: 0.048313, mean_q: -0.298962
 90100/100000: episode: 901, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.697, mean reward: -0.197 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.862, 10.123], loss: 0.002454, mae: 0.050417, mean_q: -0.326971
 90200/100000: episode: 902, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -13.595, mean reward: -0.136 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.675, 10.303], loss: 0.002517, mae: 0.050706, mean_q: -0.297482
 90300/100000: episode: 903, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.735, mean reward: -0.157 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.229, 10.098], loss: 0.002511, mae: 0.050536, mean_q: -0.321963
 90400/100000: episode: 904, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -18.053, mean reward: -0.181 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.360, 10.152], loss: 0.002360, mae: 0.047591, mean_q: -0.337907
 90500/100000: episode: 905, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.567, mean reward: -0.176 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.938, 10.106], loss: 0.002463, mae: 0.050183, mean_q: -0.312322
 90600/100000: episode: 906, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.356, mean reward: -0.184 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.685, 10.098], loss: 0.002752, mae: 0.052128, mean_q: -0.320379
 90700/100000: episode: 907, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.992, mean reward: -0.190 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.455, 10.131], loss: 0.003617, mae: 0.060406, mean_q: -0.307916
 90800/100000: episode: 908, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.640, mean reward: -0.196 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.996, 10.098], loss: 0.002470, mae: 0.050258, mean_q: -0.292433
 90900/100000: episode: 909, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -15.458, mean reward: -0.155 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.392, 10.098], loss: 0.002602, mae: 0.049883, mean_q: -0.312570
 91000/100000: episode: 910, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.808, mean reward: -0.178 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.258, 10.166], loss: 0.002448, mae: 0.048838, mean_q: -0.338575
 91100/100000: episode: 911, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.133, mean reward: -0.131 [-1.000, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.110, 10.142], loss: 0.002639, mae: 0.051291, mean_q: -0.302845
 91200/100000: episode: 912, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.023, mean reward: -0.180 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.223, 10.146], loss: 0.002379, mae: 0.047765, mean_q: -0.319853
 91300/100000: episode: 913, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.625, mean reward: -0.156 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.158, 10.240], loss: 0.002432, mae: 0.048201, mean_q: -0.304133
 91400/100000: episode: 914, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -16.522, mean reward: -0.165 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.537, 10.331], loss: 0.002589, mae: 0.049774, mean_q: -0.318461
 91500/100000: episode: 915, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.405, mean reward: -0.154 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.562, 10.098], loss: 0.002473, mae: 0.048988, mean_q: -0.342019
 91600/100000: episode: 916, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -14.273, mean reward: -0.143 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.297, 10.235], loss: 0.002638, mae: 0.050357, mean_q: -0.337688
 91700/100000: episode: 917, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.090, mean reward: -0.161 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.264, 10.449], loss: 0.002521, mae: 0.051082, mean_q: -0.320280
 91800/100000: episode: 918, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.347, mean reward: -0.133 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.028, 10.225], loss: 0.002975, mae: 0.055243, mean_q: -0.305917
 91900/100000: episode: 919, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.324, mean reward: -0.183 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.220, 10.098], loss: 0.002458, mae: 0.049327, mean_q: -0.328074
 92000/100000: episode: 920, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -20.361, mean reward: -0.204 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.724, 10.190], loss: 0.002540, mae: 0.049365, mean_q: -0.282249
 92100/100000: episode: 921, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.552, mean reward: -0.176 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.928, 10.134], loss: 0.002258, mae: 0.046176, mean_q: -0.344607
 92200/100000: episode: 922, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.052, mean reward: -0.151 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.529, 10.098], loss: 0.002628, mae: 0.050164, mean_q: -0.298311
 92300/100000: episode: 923, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.035, mean reward: -0.180 [-1.000, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.150, 10.098], loss: 0.002799, mae: 0.052505, mean_q: -0.314860
 92400/100000: episode: 924, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.933, mean reward: -0.189 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.759, 10.098], loss: 0.002789, mae: 0.052132, mean_q: -0.301065
 92500/100000: episode: 925, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.163, mean reward: -0.182 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.760, 10.112], loss: 0.002662, mae: 0.050609, mean_q: -0.304565
 92600/100000: episode: 926, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -14.411, mean reward: -0.144 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.269, 10.098], loss: 0.002918, mae: 0.054040, mean_q: -0.311216
 92700/100000: episode: 927, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.370, mean reward: -0.164 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.688, 10.098], loss: 0.002624, mae: 0.052517, mean_q: -0.313041
 92800/100000: episode: 928, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -15.786, mean reward: -0.158 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.192, 10.098], loss: 0.002561, mae: 0.051291, mean_q: -0.313006
 92900/100000: episode: 929, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.396, mean reward: -0.164 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.401, 10.306], loss: 0.002534, mae: 0.048978, mean_q: -0.333530
 93000/100000: episode: 930, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.514, mean reward: -0.175 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.232, 10.279], loss: 0.002558, mae: 0.049978, mean_q: -0.283853
 93100/100000: episode: 931, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.991, mean reward: -0.200 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.111, 10.132], loss: 0.002491, mae: 0.049089, mean_q: -0.309915
 93200/100000: episode: 932, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.662, mean reward: -0.177 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.083, 10.295], loss: 0.002490, mae: 0.049240, mean_q: -0.309129
 93300/100000: episode: 933, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.118, mean reward: -0.181 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.902, 10.098], loss: 0.002550, mae: 0.050941, mean_q: -0.331669
 93400/100000: episode: 934, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.790, mean reward: -0.188 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.769, 10.098], loss: 0.002604, mae: 0.051327, mean_q: -0.307197
 93500/100000: episode: 935, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.435, mean reward: -0.184 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.719, 10.171], loss: 0.002509, mae: 0.048575, mean_q: -0.326186
 93600/100000: episode: 936, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.582, mean reward: -0.176 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.140, 10.246], loss: 0.002454, mae: 0.049818, mean_q: -0.310215
 93700/100000: episode: 937, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -20.634, mean reward: -0.206 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.634, 10.098], loss: 0.002630, mae: 0.051218, mean_q: -0.288116
 93800/100000: episode: 938, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.324, mean reward: -0.153 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.016, 10.148], loss: 0.002610, mae: 0.052076, mean_q: -0.325349
 93900/100000: episode: 939, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -20.339, mean reward: -0.203 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.399, 10.123], loss: 0.002627, mae: 0.052007, mean_q: -0.327731
 94000/100000: episode: 940, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.648, mean reward: -0.166 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.516, 10.218], loss: 0.002660, mae: 0.052009, mean_q: -0.323578
 94100/100000: episode: 941, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.039, mean reward: -0.170 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.967, 10.105], loss: 0.003799, mae: 0.061421, mean_q: -0.335542
 94200/100000: episode: 942, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.688, mean reward: -0.197 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.623, 10.191], loss: 0.002736, mae: 0.053035, mean_q: -0.340930
 94300/100000: episode: 943, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.212, mean reward: -0.182 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.769, 10.203], loss: 0.002396, mae: 0.048504, mean_q: -0.307372
 94400/100000: episode: 944, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.928, mean reward: -0.189 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.930, 10.170], loss: 0.002306, mae: 0.046995, mean_q: -0.307195
 94500/100000: episode: 945, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.443, mean reward: -0.164 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.921, 10.098], loss: 0.002522, mae: 0.049391, mean_q: -0.314982
 94600/100000: episode: 946, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.329, mean reward: -0.183 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.838, 10.098], loss: 0.002472, mae: 0.048603, mean_q: -0.341101
 94700/100000: episode: 947, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.039, mean reward: -0.180 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.704, 10.098], loss: 0.002516, mae: 0.050660, mean_q: -0.297512
 94800/100000: episode: 948, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.467, mean reward: -0.165 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.386, 10.199], loss: 0.002473, mae: 0.049154, mean_q: -0.338963
 94900/100000: episode: 949, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.553, mean reward: -0.186 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.333, 10.098], loss: 0.002482, mae: 0.048872, mean_q: -0.324627
 95000/100000: episode: 950, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -14.398, mean reward: -0.144 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.873, 10.407], loss: 0.002475, mae: 0.049396, mean_q: -0.367534
 95100/100000: episode: 951, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.011, mean reward: -0.180 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.487, 10.098], loss: 0.002547, mae: 0.049852, mean_q: -0.331027
 95200/100000: episode: 952, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -13.895, mean reward: -0.139 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.723, 10.098], loss: 0.002333, mae: 0.046938, mean_q: -0.360893
 95300/100000: episode: 953, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.105, mean reward: -0.151 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.315, 10.179], loss: 0.002501, mae: 0.050782, mean_q: -0.326037
 95400/100000: episode: 954, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.965, mean reward: -0.160 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.988, 10.098], loss: 0.002578, mae: 0.050509, mean_q: -0.311534
 95500/100000: episode: 955, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.174, mean reward: -0.182 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.697, 10.098], loss: 0.002546, mae: 0.051457, mean_q: -0.285399
 95600/100000: episode: 956, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.218, mean reward: -0.192 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.671, 10.306], loss: 0.002480, mae: 0.049684, mean_q: -0.338354
 95700/100000: episode: 957, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.979, mean reward: -0.190 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.515, 10.186], loss: 0.002606, mae: 0.051849, mean_q: -0.279685
 95800/100000: episode: 958, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.737, mean reward: -0.177 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.488, 10.098], loss: 0.002613, mae: 0.050071, mean_q: -0.306005
 95900/100000: episode: 959, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.234, mean reward: -0.172 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.527, 10.239], loss: 0.002730, mae: 0.052312, mean_q: -0.347317
 96000/100000: episode: 960, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -15.864, mean reward: -0.159 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.412, 10.233], loss: 0.011892, mae: 0.079023, mean_q: -0.355611
 96100/100000: episode: 961, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.955, mean reward: -0.170 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.768, 10.291], loss: 0.002915, mae: 0.055962, mean_q: -0.290214
 96200/100000: episode: 962, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.547, mean reward: -0.185 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.088, 10.220], loss: 0.002746, mae: 0.053104, mean_q: -0.321563
 96300/100000: episode: 963, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -12.782, mean reward: -0.128 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.784, 10.365], loss: 0.002444, mae: 0.049650, mean_q: -0.342943
 96400/100000: episode: 964, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.032, mean reward: -0.160 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.411, 10.239], loss: 0.002456, mae: 0.048256, mean_q: -0.327416
 96500/100000: episode: 965, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.759, mean reward: -0.178 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.021, 10.098], loss: 0.002274, mae: 0.046783, mean_q: -0.327114
 96600/100000: episode: 966, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -14.639, mean reward: -0.146 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.001, 10.098], loss: 0.002403, mae: 0.047842, mean_q: -0.346404
 96700/100000: episode: 967, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.852, mean reward: -0.169 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.882, 10.098], loss: 0.002548, mae: 0.051003, mean_q: -0.317735
 96800/100000: episode: 968, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -15.589, mean reward: -0.156 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.110, 10.226], loss: 0.002230, mae: 0.046666, mean_q: -0.333164
 96900/100000: episode: 969, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.121, mean reward: -0.161 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.023, 10.098], loss: 0.002530, mae: 0.049228, mean_q: -0.337744
 97000/100000: episode: 970, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.840, mean reward: -0.178 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.461, 10.175], loss: 0.002519, mae: 0.048851, mean_q: -0.341330
 97100/100000: episode: 971, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -12.834, mean reward: -0.128 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.077, 10.206], loss: 0.002429, mae: 0.048577, mean_q: -0.354277
 97200/100000: episode: 972, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.567, mean reward: -0.166 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.899, 10.398], loss: 0.002456, mae: 0.049386, mean_q: -0.344615
 97300/100000: episode: 973, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.730, mean reward: -0.177 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.320, 10.284], loss: 0.002463, mae: 0.049348, mean_q: -0.296736
 97400/100000: episode: 974, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.839, mean reward: -0.168 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.675, 10.098], loss: 0.002668, mae: 0.052307, mean_q: -0.325493
 97500/100000: episode: 975, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.575, mean reward: -0.136 [-1.000, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.341, 10.151], loss: 0.002633, mae: 0.051750, mean_q: -0.321250
 97600/100000: episode: 976, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.568, mean reward: -0.176 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.815, 10.098], loss: 0.002559, mae: 0.049963, mean_q: -0.326451
 97700/100000: episode: 977, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.280, mean reward: -0.153 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.548, 10.098], loss: 0.002528, mae: 0.050070, mean_q: -0.306110
 97800/100000: episode: 978, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.279, mean reward: -0.183 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.636, 10.098], loss: 0.002451, mae: 0.049401, mean_q: -0.348393
 97900/100000: episode: 979, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.977, mean reward: -0.190 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.898, 10.208], loss: 0.002630, mae: 0.051521, mean_q: -0.305128
 98000/100000: episode: 980, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.219, mean reward: -0.162 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.658, 10.098], loss: 0.002465, mae: 0.049065, mean_q: -0.304328
 98100/100000: episode: 981, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.136, mean reward: -0.191 [-1.000, 0.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.022, 10.227], loss: 0.002380, mae: 0.048616, mean_q: -0.322190
 98200/100000: episode: 982, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.375, mean reward: -0.184 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.894, 10.216], loss: 0.003267, mae: 0.057292, mean_q: -0.314809
 98300/100000: episode: 983, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.080, mean reward: -0.181 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.172, 10.150], loss: 0.002743, mae: 0.055217, mean_q: -0.332515
 98400/100000: episode: 984, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -19.414, mean reward: -0.194 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.670, 10.098], loss: 0.003769, mae: 0.060817, mean_q: -0.294149
 98500/100000: episode: 985, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.479, mean reward: -0.185 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.448, 10.098], loss: 0.002570, mae: 0.051870, mean_q: -0.323621
 98600/100000: episode: 986, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.944, mean reward: -0.149 [-1.000, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.380, 10.098], loss: 0.002542, mae: 0.049180, mean_q: -0.327420
 98700/100000: episode: 987, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -13.691, mean reward: -0.137 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.540, 10.259], loss: 0.002458, mae: 0.047580, mean_q: -0.337134
 98800/100000: episode: 988, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.536, mean reward: -0.155 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.297, 10.338], loss: 0.002619, mae: 0.051003, mean_q: -0.305115
 98900/100000: episode: 989, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.422, mean reward: -0.164 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.536, 10.211], loss: 0.002599, mae: 0.049630, mean_q: -0.315791
 99000/100000: episode: 990, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.481, mean reward: -0.165 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.088, 10.098], loss: 0.002556, mae: 0.048995, mean_q: -0.335795
 99100/100000: episode: 991, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.855, mean reward: -0.169 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.740, 10.098], loss: 0.002429, mae: 0.048492, mean_q: -0.318166
 99200/100000: episode: 992, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.650, mean reward: -0.166 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.996, 10.098], loss: 0.002442, mae: 0.049066, mean_q: -0.331386
 99300/100000: episode: 993, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.810, mean reward: -0.178 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.776, 10.098], loss: 0.002659, mae: 0.052392, mean_q: -0.297912
 99400/100000: episode: 994, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.622, mean reward: -0.186 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.225, 10.098], loss: 0.002496, mae: 0.049700, mean_q: -0.303477
 99500/100000: episode: 995, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -4.070, mean reward: -0.041 [-1.000, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.349, 10.354], loss: 0.002360, mae: 0.048476, mean_q: -0.311145
 99600/100000: episode: 996, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.781, mean reward: -0.178 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.047, 10.339], loss: 0.002571, mae: 0.050278, mean_q: -0.334959
 99700/100000: episode: 997, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.505, mean reward: -0.145 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.591, 10.298], loss: 0.003185, mae: 0.057263, mean_q: -0.306099
 99800/100000: episode: 998, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.888, mean reward: -0.159 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.098, 10.122], loss: 0.002717, mae: 0.054203, mean_q: -0.264472
 99900/100000: episode: 999, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -13.223, mean reward: -0.132 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.865, 10.376], loss: 0.002470, mae: 0.049650, mean_q: -0.312877
 100000/100000: episode: 1000, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.932, mean reward: -0.159 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.070, 10.098], loss: 0.002708, mae: 0.052386, mean_q: -0.274236
done, took 531.671 seconds
[Info] End Uniform Random Simulation. Falsification occurred 0 times.
