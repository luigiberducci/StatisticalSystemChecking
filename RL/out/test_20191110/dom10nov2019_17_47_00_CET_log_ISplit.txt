Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.176s, episode steps: 100, steps per second: 567, episode reward: -16.585, mean reward: -0.166 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.827, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.065s, episode steps: 100, steps per second: 1536, episode reward: -17.856, mean reward: -0.179 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.589, 10.318], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.069s, episode steps: 100, steps per second: 1457, episode reward: -18.082, mean reward: -0.181 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.691, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.063s, episode steps: 100, steps per second: 1597, episode reward: -15.600, mean reward: -0.156 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.620, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.062s, episode steps: 100, steps per second: 1624, episode reward: -18.752, mean reward: -0.188 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.786, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.271s, episode steps: 100, steps per second: 79, episode reward: -20.137, mean reward: -0.201 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.032, 10.248], loss: 0.042258, mae: 0.185222, mean_q: 0.126481
   700/100000: episode: 7, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -18.295, mean reward: -0.183 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.283, 10.244], loss: 0.012602, mae: 0.107937, mean_q: -0.064614
   800/100000: episode: 8, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -15.815, mean reward: -0.158 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.141, 10.098], loss: 0.010395, mae: 0.093496, mean_q: -0.182555
   900/100000: episode: 9, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.184, mean reward: -0.192 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.921, 10.098], loss: 0.009997, mae: 0.091680, mean_q: -0.239573
  1000/100000: episode: 10, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.648, mean reward: -0.176 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.075, 10.118], loss: 0.007808, mae: 0.082574, mean_q: -0.286486
  1100/100000: episode: 11, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -20.317, mean reward: -0.203 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.806, 10.119], loss: 0.007655, mae: 0.080032, mean_q: -0.331546
  1200/100000: episode: 12, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -12.424, mean reward: -0.124 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.291, 10.323], loss: 0.007562, mae: 0.078864, mean_q: -0.316407
  1300/100000: episode: 13, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.568, mean reward: -0.186 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.649, 10.098], loss: 0.008461, mae: 0.085842, mean_q: -0.314046
  1400/100000: episode: 14, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -15.968, mean reward: -0.160 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.568, 10.141], loss: 0.007245, mae: 0.076665, mean_q: -0.341192
  1500/100000: episode: 15, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.842, mean reward: -0.178 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.955, 10.239], loss: 0.006888, mae: 0.076156, mean_q: -0.348134
  1600/100000: episode: 16, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -9.172, mean reward: -0.092 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.691, 10.432], loss: 0.008156, mae: 0.082685, mean_q: -0.339191
  1700/100000: episode: 17, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.297, mean reward: -0.163 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.695, 10.277], loss: 0.007493, mae: 0.082432, mean_q: -0.329946
  1800/100000: episode: 18, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.781, mean reward: -0.168 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.027, 10.098], loss: 0.006709, mae: 0.078043, mean_q: -0.335875
  1900/100000: episode: 19, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.772, mean reward: -0.178 [-1.000, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.122, 10.149], loss: 0.006679, mae: 0.077033, mean_q: -0.307713
  2000/100000: episode: 20, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -15.499, mean reward: -0.155 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.197, 10.098], loss: 0.006766, mae: 0.077008, mean_q: -0.359670
  2100/100000: episode: 21, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -18.050, mean reward: -0.180 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.630, 10.098], loss: 0.006399, mae: 0.076970, mean_q: -0.321302
  2200/100000: episode: 22, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.321, mean reward: -0.183 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.838, 10.098], loss: 0.005738, mae: 0.073343, mean_q: -0.309613
  2300/100000: episode: 23, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.979, mean reward: -0.160 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.852, 10.098], loss: 0.005243, mae: 0.070182, mean_q: -0.307803
  2400/100000: episode: 24, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -12.435, mean reward: -0.124 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.257, 10.250], loss: 0.005475, mae: 0.071272, mean_q: -0.327249
  2500/100000: episode: 25, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -19.168, mean reward: -0.192 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.697, 10.098], loss: 0.005166, mae: 0.071077, mean_q: -0.352806
  2600/100000: episode: 26, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.223, mean reward: -0.172 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.439, 10.098], loss: 0.006201, mae: 0.074586, mean_q: -0.341020
  2700/100000: episode: 27, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.181, mean reward: -0.182 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.820, 10.142], loss: 0.005580, mae: 0.072123, mean_q: -0.341187
  2800/100000: episode: 28, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.152, mean reward: -0.172 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.295, 10.288], loss: 0.004921, mae: 0.066782, mean_q: -0.370498
  2900/100000: episode: 29, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -13.044, mean reward: -0.130 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.538, 10.098], loss: 0.004795, mae: 0.066550, mean_q: -0.326523
  3000/100000: episode: 30, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -21.162, mean reward: -0.212 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.575, 10.098], loss: 0.005858, mae: 0.072870, mean_q: -0.313477
  3100/100000: episode: 31, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: -16.140, mean reward: -0.161 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.009, 10.098], loss: 0.004782, mae: 0.066586, mean_q: -0.350341
  3200/100000: episode: 32, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -13.794, mean reward: -0.138 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.869, 10.280], loss: 0.006432, mae: 0.076957, mean_q: -0.320033
  3300/100000: episode: 33, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.576, mean reward: -0.176 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.623, 10.098], loss: 0.004947, mae: 0.069583, mean_q: -0.308838
  3400/100000: episode: 34, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.520, mean reward: -0.195 [-1.000, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.964, 10.098], loss: 0.005079, mae: 0.069851, mean_q: -0.377749
  3500/100000: episode: 35, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.159, mean reward: -0.182 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.805, 10.098], loss: 0.005998, mae: 0.073432, mean_q: -0.331269
  3600/100000: episode: 36, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.737, mean reward: -0.157 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.870, 10.098], loss: 0.004885, mae: 0.070215, mean_q: -0.309108
  3700/100000: episode: 37, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.461, mean reward: -0.175 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.915, 10.098], loss: 0.005271, mae: 0.071425, mean_q: -0.316622
  3800/100000: episode: 38, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.735, mean reward: -0.167 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.862, 10.371], loss: 0.005302, mae: 0.072903, mean_q: -0.311473
  3900/100000: episode: 39, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.520, mean reward: -0.185 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.330, 10.098], loss: 0.004929, mae: 0.069190, mean_q: -0.352970
  4000/100000: episode: 40, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -15.736, mean reward: -0.157 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.829, 10.327], loss: 0.004913, mae: 0.068484, mean_q: -0.345766
  4100/100000: episode: 41, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.920, mean reward: -0.179 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.952, 10.098], loss: 0.004583, mae: 0.066646, mean_q: -0.314219
  4200/100000: episode: 42, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -17.573, mean reward: -0.176 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.851, 10.098], loss: 0.004427, mae: 0.065878, mean_q: -0.341888
  4300/100000: episode: 43, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -14.830, mean reward: -0.148 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.894, 10.098], loss: 0.004993, mae: 0.071195, mean_q: -0.309080
  4400/100000: episode: 44, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.231, mean reward: -0.172 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.148, 10.098], loss: 0.005862, mae: 0.074967, mean_q: -0.326665
  4500/100000: episode: 45, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.508, mean reward: -0.175 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.555, 10.115], loss: 0.004209, mae: 0.066780, mean_q: -0.337153
  4600/100000: episode: 46, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -16.190, mean reward: -0.162 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.361, 10.207], loss: 0.005414, mae: 0.073692, mean_q: -0.284022
  4700/100000: episode: 47, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -17.956, mean reward: -0.180 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.177, 10.365], loss: 0.004579, mae: 0.068269, mean_q: -0.333364
  4800/100000: episode: 48, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -13.281, mean reward: -0.133 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.946, 10.098], loss: 0.004624, mae: 0.066441, mean_q: -0.360334
  4900/100000: episode: 49, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.989, mean reward: -0.170 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.373, 10.098], loss: 0.004137, mae: 0.063677, mean_q: -0.322038
  5000/100000: episode: 50, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -13.041, mean reward: -0.130 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.400, 10.098], loss: 0.004916, mae: 0.069533, mean_q: -0.296995
  5100/100000: episode: 51, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -10.818, mean reward: -0.108 [-1.000, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.970, 10.217], loss: 0.004479, mae: 0.067875, mean_q: -0.304965
  5200/100000: episode: 52, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.924, mean reward: -0.179 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.956, 10.098], loss: 0.004601, mae: 0.066479, mean_q: -0.311942
  5300/100000: episode: 53, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.968, mean reward: -0.160 [-1.000, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.652, 10.309], loss: 0.005781, mae: 0.074488, mean_q: -0.315963
  5400/100000: episode: 54, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.146, mean reward: -0.161 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.201, 10.098], loss: 0.004778, mae: 0.067228, mean_q: -0.331640
  5500/100000: episode: 55, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -12.815, mean reward: -0.128 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.842, 10.335], loss: 0.004555, mae: 0.068452, mean_q: -0.323355
  5600/100000: episode: 56, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -15.128, mean reward: -0.151 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.559, 10.104], loss: 0.004783, mae: 0.068672, mean_q: -0.320345
  5700/100000: episode: 57, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.581, mean reward: -0.166 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.837, 10.098], loss: 0.004464, mae: 0.065747, mean_q: -0.312625
  5800/100000: episode: 58, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -20.059, mean reward: -0.201 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.760, 10.098], loss: 0.005327, mae: 0.071216, mean_q: -0.310064
  5900/100000: episode: 59, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -13.346, mean reward: -0.133 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.440, 10.098], loss: 0.005411, mae: 0.074205, mean_q: -0.285397
  6000/100000: episode: 60, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.533, mean reward: -0.165 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.974, 10.098], loss: 0.004247, mae: 0.065472, mean_q: -0.302430
  6100/100000: episode: 61, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -18.053, mean reward: -0.181 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.715, 10.206], loss: 0.005809, mae: 0.075273, mean_q: -0.314651
  6200/100000: episode: 62, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.863, mean reward: -0.159 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.466, 10.105], loss: 0.004663, mae: 0.067601, mean_q: -0.343876
  6300/100000: episode: 63, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.095, mean reward: -0.181 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.384, 10.176], loss: 0.003661, mae: 0.061998, mean_q: -0.298519
  6400/100000: episode: 64, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.434, mean reward: -0.194 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.119, 10.304], loss: 0.004457, mae: 0.065799, mean_q: -0.335701
  6500/100000: episode: 65, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.099, mean reward: -0.171 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.349, 10.271], loss: 0.004349, mae: 0.065253, mean_q: -0.314306
  6600/100000: episode: 66, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.275, mean reward: -0.153 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.606, 10.098], loss: 0.003758, mae: 0.061869, mean_q: -0.326775
  6700/100000: episode: 67, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -14.705, mean reward: -0.147 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.303, 10.098], loss: 0.004535, mae: 0.067604, mean_q: -0.350621
  6800/100000: episode: 68, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -13.710, mean reward: -0.137 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.636, 10.366], loss: 0.004481, mae: 0.066525, mean_q: -0.303571
  6900/100000: episode: 69, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.218, mean reward: -0.162 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.884, 10.098], loss: 0.004131, mae: 0.063972, mean_q: -0.303295
  7000/100000: episode: 70, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -10.533, mean reward: -0.105 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.936, 10.098], loss: 0.004089, mae: 0.062542, mean_q: -0.344857
  7100/100000: episode: 71, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -17.668, mean reward: -0.177 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.966, 10.144], loss: 0.006587, mae: 0.078247, mean_q: -0.297412
  7200/100000: episode: 72, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.854, mean reward: -0.169 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.504, 10.106], loss: 0.004851, mae: 0.070159, mean_q: -0.292063
  7300/100000: episode: 73, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.275, mean reward: -0.163 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.934, 10.219], loss: 0.003994, mae: 0.063841, mean_q: -0.297488
  7400/100000: episode: 74, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -17.186, mean reward: -0.172 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.070, 10.134], loss: 0.004491, mae: 0.066837, mean_q: -0.291952
  7500/100000: episode: 75, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.967, mean reward: -0.180 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.196, 10.098], loss: 0.004151, mae: 0.065483, mean_q: -0.299719
  7600/100000: episode: 76, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.652, mean reward: -0.187 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.755, 10.098], loss: 0.003874, mae: 0.062542, mean_q: -0.305643
  7700/100000: episode: 77, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.345, mean reward: -0.173 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.948, 10.098], loss: 0.004108, mae: 0.064598, mean_q: -0.337627
  7800/100000: episode: 78, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -13.897, mean reward: -0.139 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.095, 10.098], loss: 0.003889, mae: 0.062020, mean_q: -0.317233
  7900/100000: episode: 79, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.459, mean reward: -0.175 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.392, 10.098], loss: 0.003910, mae: 0.062307, mean_q: -0.325347
  8000/100000: episode: 80, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.303, mean reward: -0.163 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.951, 10.283], loss: 0.004282, mae: 0.064196, mean_q: -0.305787
  8100/100000: episode: 81, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.100, mean reward: -0.191 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.594, 10.098], loss: 0.003605, mae: 0.061478, mean_q: -0.311669
  8200/100000: episode: 82, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.426, mean reward: -0.194 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.429, 10.098], loss: 0.004346, mae: 0.066329, mean_q: -0.317557
  8300/100000: episode: 83, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.178, mean reward: -0.172 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.344, 10.098], loss: 0.005180, mae: 0.070706, mean_q: -0.293325
  8400/100000: episode: 84, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -14.849, mean reward: -0.148 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.737, 10.098], loss: 0.003787, mae: 0.061664, mean_q: -0.317653
  8500/100000: episode: 85, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -16.949, mean reward: -0.169 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.816, 10.098], loss: 0.003884, mae: 0.062893, mean_q: -0.293910
  8600/100000: episode: 86, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.431, mean reward: -0.184 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.596, 10.098], loss: 0.003808, mae: 0.061709, mean_q: -0.286371
  8700/100000: episode: 87, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -15.447, mean reward: -0.154 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.259, 10.098], loss: 0.004377, mae: 0.066668, mean_q: -0.284721
  8800/100000: episode: 88, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.509, mean reward: -0.195 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.760, 10.231], loss: 0.004405, mae: 0.067614, mean_q: -0.276409
  8900/100000: episode: 89, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -19.452, mean reward: -0.195 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.787, 10.098], loss: 0.003945, mae: 0.063788, mean_q: -0.307824
  9000/100000: episode: 90, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -13.538, mean reward: -0.135 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.881, 10.106], loss: 0.004146, mae: 0.064527, mean_q: -0.330725
  9100/100000: episode: 91, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.045, mean reward: -0.180 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.052, 10.098], loss: 0.003621, mae: 0.060883, mean_q: -0.330200
  9200/100000: episode: 92, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.479, mean reward: -0.185 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.415, 10.251], loss: 0.003454, mae: 0.059851, mean_q: -0.316347
  9300/100000: episode: 93, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -12.910, mean reward: -0.129 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.263, 10.098], loss: 0.003678, mae: 0.062053, mean_q: -0.299528
  9400/100000: episode: 94, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.518, mean reward: -0.165 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.722, 10.231], loss: 0.004270, mae: 0.065758, mean_q: -0.271082
  9500/100000: episode: 95, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -17.879, mean reward: -0.179 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.635, 10.098], loss: 0.003908, mae: 0.063246, mean_q: -0.310010
  9600/100000: episode: 96, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.935, mean reward: -0.169 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.345, 10.098], loss: 0.004109, mae: 0.064647, mean_q: -0.320986
  9700/100000: episode: 97, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.342, mean reward: -0.173 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.515, 10.098], loss: 0.003487, mae: 0.060335, mean_q: -0.288118
  9800/100000: episode: 98, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.261, mean reward: -0.183 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.562, 10.175], loss: 0.003671, mae: 0.060013, mean_q: -0.354550
  9900/100000: episode: 99, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.362, mean reward: -0.184 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.792, 10.098], loss: 0.003604, mae: 0.061108, mean_q: -0.307739
 10000/100000: episode: 100, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.683, mean reward: -0.167 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.166, 10.170], loss: 0.003249, mae: 0.058437, mean_q: -0.323386
 10100/100000: episode: 101, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -14.531, mean reward: -0.145 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.262, 10.302], loss: 0.003174, mae: 0.057027, mean_q: -0.314501
 10200/100000: episode: 102, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -18.086, mean reward: -0.181 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.561, 10.191], loss: 0.004064, mae: 0.064469, mean_q: -0.303323
 10300/100000: episode: 103, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -12.246, mean reward: -0.122 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.466, 10.098], loss: 0.003346, mae: 0.059031, mean_q: -0.331034
 10400/100000: episode: 104, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -13.226, mean reward: -0.132 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.885, 10.316], loss: 0.003601, mae: 0.061674, mean_q: -0.309109
 10500/100000: episode: 105, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.796, mean reward: -0.168 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.932, 10.211], loss: 0.003157, mae: 0.058897, mean_q: -0.311704
 10600/100000: episode: 106, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -13.204, mean reward: -0.132 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.924, 10.204], loss: 0.003175, mae: 0.057922, mean_q: -0.302749
 10700/100000: episode: 107, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.057, mean reward: -0.181 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.610, 10.124], loss: 0.003059, mae: 0.056059, mean_q: -0.350625
 10800/100000: episode: 108, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -12.903, mean reward: -0.129 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.538, 10.155], loss: 0.003159, mae: 0.058345, mean_q: -0.341767
 10900/100000: episode: 109, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.396, mean reward: -0.194 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.385, 10.334], loss: 0.003633, mae: 0.062744, mean_q: -0.328681
 11000/100000: episode: 110, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -14.451, mean reward: -0.145 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.007, 10.098], loss: 0.003471, mae: 0.062246, mean_q: -0.291631
 11100/100000: episode: 111, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -20.170, mean reward: -0.202 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.323, 10.224], loss: 0.003398, mae: 0.060057, mean_q: -0.307505
 11200/100000: episode: 112, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -17.062, mean reward: -0.171 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.076, 10.144], loss: 0.003287, mae: 0.059234, mean_q: -0.295822
 11300/100000: episode: 113, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.625, mean reward: -0.176 [-1.000, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.366, 10.207], loss: 0.003006, mae: 0.056607, mean_q: -0.293618
 11400/100000: episode: 114, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.518, mean reward: -0.175 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.848, 10.259], loss: 0.003594, mae: 0.061930, mean_q: -0.293149
 11500/100000: episode: 115, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.326, mean reward: -0.183 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.944, 10.148], loss: 0.004002, mae: 0.063431, mean_q: -0.295138
 11600/100000: episode: 116, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -17.678, mean reward: -0.177 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.446, 10.098], loss: 0.003406, mae: 0.059850, mean_q: -0.330599
 11700/100000: episode: 117, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -14.027, mean reward: -0.140 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.436, 10.098], loss: 0.002965, mae: 0.055531, mean_q: -0.286964
 11800/100000: episode: 118, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.218, mean reward: -0.182 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.033, 10.220], loss: 0.003251, mae: 0.058996, mean_q: -0.318664
 11900/100000: episode: 119, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.596, mean reward: -0.196 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.464, 10.108], loss: 0.002911, mae: 0.054466, mean_q: -0.321177
 12000/100000: episode: 120, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -20.006, mean reward: -0.200 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.619, 10.098], loss: 0.004613, mae: 0.065445, mean_q: -0.334190
 12100/100000: episode: 121, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -18.584, mean reward: -0.186 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.172, 10.098], loss: 0.003494, mae: 0.058184, mean_q: -0.345759
 12200/100000: episode: 122, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.316, mean reward: -0.183 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.599, 10.278], loss: 0.003517, mae: 0.060476, mean_q: -0.297432
 12300/100000: episode: 123, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -7.095, mean reward: -0.071 [-1.000, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.410 [-1.285, 10.098], loss: 0.002982, mae: 0.055742, mean_q: -0.333611
 12400/100000: episode: 124, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -20.050, mean reward: -0.201 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.856, 10.098], loss: 0.003914, mae: 0.063940, mean_q: -0.330824
 12500/100000: episode: 125, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.379, mean reward: -0.174 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.040, 10.249], loss: 0.003307, mae: 0.057704, mean_q: -0.326640
 12600/100000: episode: 126, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.045, mean reward: -0.170 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.284, 10.098], loss: 0.003377, mae: 0.059119, mean_q: -0.306279
 12700/100000: episode: 127, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -19.925, mean reward: -0.199 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.494, 10.173], loss: 0.003423, mae: 0.060390, mean_q: -0.289044
 12800/100000: episode: 128, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -13.453, mean reward: -0.135 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.525, 10.098], loss: 0.004758, mae: 0.067933, mean_q: -0.292128
 12900/100000: episode: 129, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.180, mean reward: -0.162 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.261, 10.331], loss: 0.003480, mae: 0.060515, mean_q: -0.323088
 13000/100000: episode: 130, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -16.161, mean reward: -0.162 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.743, 10.098], loss: 0.002842, mae: 0.054007, mean_q: -0.347110
 13100/100000: episode: 131, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.708, mean reward: -0.177 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.889, 10.119], loss: 0.003243, mae: 0.057972, mean_q: -0.349353
 13200/100000: episode: 132, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -15.886, mean reward: -0.159 [-1.000, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.882, 10.098], loss: 0.003447, mae: 0.060327, mean_q: -0.315306
 13300/100000: episode: 133, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.554, mean reward: -0.186 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.333, 10.098], loss: 0.003880, mae: 0.062591, mean_q: -0.312226
 13400/100000: episode: 134, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.352, mean reward: -0.174 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.022, 10.281], loss: 0.004298, mae: 0.067874, mean_q: -0.316175
 13500/100000: episode: 135, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.189, mean reward: -0.182 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.529, 10.098], loss: 0.003219, mae: 0.057355, mean_q: -0.319684
 13600/100000: episode: 136, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -15.592, mean reward: -0.156 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.225, 10.320], loss: 0.003573, mae: 0.060810, mean_q: -0.318593
 13700/100000: episode: 137, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.197, mean reward: -0.182 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.978, 10.180], loss: 0.003167, mae: 0.057615, mean_q: -0.325799
 13800/100000: episode: 138, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -17.628, mean reward: -0.176 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.944, 10.098], loss: 0.003579, mae: 0.061343, mean_q: -0.330516
 13900/100000: episode: 139, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -18.765, mean reward: -0.188 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.780, 10.098], loss: 0.003305, mae: 0.058045, mean_q: -0.327887
 14000/100000: episode: 140, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -21.211, mean reward: -0.212 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.087, 10.098], loss: 0.003083, mae: 0.056881, mean_q: -0.321331
 14100/100000: episode: 141, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -20.081, mean reward: -0.201 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.603, 10.098], loss: 0.003152, mae: 0.057311, mean_q: -0.345850
 14200/100000: episode: 142, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -15.561, mean reward: -0.156 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.750, 10.098], loss: 0.003661, mae: 0.061430, mean_q: -0.316519
 14300/100000: episode: 143, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -7.831, mean reward: -0.078 [-1.000, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.226, 10.605], loss: 0.005035, mae: 0.070826, mean_q: -0.293114
 14400/100000: episode: 144, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.092, mean reward: -0.181 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.053, 10.201], loss: 0.003129, mae: 0.055888, mean_q: -0.333396
 14500/100000: episode: 145, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -16.177, mean reward: -0.162 [-1.000, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.852, 10.098], loss: 0.003625, mae: 0.059451, mean_q: -0.348168
 14600/100000: episode: 146, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.134, mean reward: -0.181 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.834, 10.167], loss: 0.003397, mae: 0.057878, mean_q: -0.339764
 14700/100000: episode: 147, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.869, mean reward: -0.179 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.857, 10.181], loss: 0.008381, mae: 0.083100, mean_q: -0.314341
 14800/100000: episode: 148, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -16.824, mean reward: -0.168 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.366, 10.098], loss: 0.004399, mae: 0.065331, mean_q: -0.309471
 14900/100000: episode: 149, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.561, mean reward: -0.186 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.691, 10.321], loss: 0.003737, mae: 0.060874, mean_q: -0.312013
[Info] 100-TH LEVEL FOUND: 0.5649093389511108, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.531s, episode steps: 100, steps per second: 22, episode reward: -16.213, mean reward: -0.162 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.873, 10.488], loss: 0.004661, mae: 0.068232, mean_q: -0.294090
 15039/100000: episode: 151, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 8.120, mean reward: 0.208 [0.019, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.045, 10.162], loss: 0.003072, mae: 0.056682, mean_q: -0.367506
 15061/100000: episode: 152, duration: 0.133s, episode steps: 22, steps per second: 165, episode reward: 4.377, mean reward: 0.199 [0.115, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.099, 10.100], loss: 0.003181, mae: 0.054681, mean_q: -0.326227
 15100/100000: episode: 153, duration: 0.213s, episode steps: 39, steps per second: 183, episode reward: 6.030, mean reward: 0.155 [0.013, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.897, 10.135], loss: 0.003129, mae: 0.056788, mean_q: -0.354044
 15139/100000: episode: 154, duration: 0.213s, episode steps: 39, steps per second: 183, episode reward: 11.685, mean reward: 0.300 [0.093, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.728, 10.100], loss: 0.003232, mae: 0.057411, mean_q: -0.334321
 15191/100000: episode: 155, duration: 0.288s, episode steps: 52, steps per second: 181, episode reward: 8.141, mean reward: 0.157 [0.041, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.577, 10.100], loss: 0.003413, mae: 0.059096, mean_q: -0.303309
 15243/100000: episode: 156, duration: 0.272s, episode steps: 52, steps per second: 191, episode reward: 7.572, mean reward: 0.146 [0.034, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.318, 10.193], loss: 0.003376, mae: 0.057846, mean_q: -0.308388
 15307/100000: episode: 157, duration: 0.357s, episode steps: 64, steps per second: 179, episode reward: 14.098, mean reward: 0.220 [0.029, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.776 [-0.328, 10.177], loss: 0.003803, mae: 0.063159, mean_q: -0.268681
 15359/100000: episode: 158, duration: 0.285s, episode steps: 52, steps per second: 183, episode reward: 10.167, mean reward: 0.196 [0.018, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.785, 10.268], loss: 0.003308, mae: 0.058502, mean_q: -0.287581
 15392/100000: episode: 159, duration: 0.174s, episode steps: 33, steps per second: 189, episode reward: 9.155, mean reward: 0.277 [0.124, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.052, 10.100], loss: 0.003465, mae: 0.058811, mean_q: -0.276916
 15425/100000: episode: 160, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 9.174, mean reward: 0.278 [0.135, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.250, 10.100], loss: 0.003199, mae: 0.058013, mean_q: -0.273414
 15467/100000: episode: 161, duration: 0.238s, episode steps: 42, steps per second: 176, episode reward: 14.604, mean reward: 0.348 [0.194, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-1.172, 10.100], loss: 0.003640, mae: 0.061920, mean_q: -0.264262
 15514/100000: episode: 162, duration: 0.236s, episode steps: 47, steps per second: 199, episode reward: 8.587, mean reward: 0.183 [0.029, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-1.049, 10.163], loss: 0.003340, mae: 0.060093, mean_q: -0.231502
 15561/100000: episode: 163, duration: 0.272s, episode steps: 47, steps per second: 172, episode reward: 12.941, mean reward: 0.275 [0.107, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.365, 10.290], loss: 0.003536, mae: 0.061648, mean_q: -0.228839
 15586/100000: episode: 164, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 5.234, mean reward: 0.209 [0.087, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.454, 10.125], loss: 0.003768, mae: 0.062083, mean_q: -0.169740
 15611/100000: episode: 165, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 6.679, mean reward: 0.267 [0.155, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-1.357, 10.100], loss: 0.003523, mae: 0.058953, mean_q: -0.276092
 15650/100000: episode: 166, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 12.577, mean reward: 0.322 [0.228, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.678, 10.100], loss: 0.004012, mae: 0.066314, mean_q: -0.228023
 15692/100000: episode: 167, duration: 0.238s, episode steps: 42, steps per second: 177, episode reward: 14.702, mean reward: 0.350 [0.216, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.459, 10.100], loss: 0.003671, mae: 0.061380, mean_q: -0.191238
 15756/100000: episode: 168, duration: 0.372s, episode steps: 64, steps per second: 172, episode reward: 16.702, mean reward: 0.261 [0.115, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.791 [-0.887, 10.452], loss: 0.003504, mae: 0.060649, mean_q: -0.210113
 15778/100000: episode: 169, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 6.289, mean reward: 0.286 [0.168, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.354, 10.100], loss: 0.004451, mae: 0.068897, mean_q: -0.164392
 15817/100000: episode: 170, duration: 0.222s, episode steps: 39, steps per second: 176, episode reward: 12.260, mean reward: 0.314 [0.188, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.255, 10.100], loss: 0.003624, mae: 0.061867, mean_q: -0.187120
 15850/100000: episode: 171, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 8.869, mean reward: 0.269 [0.199, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.476, 10.100], loss: 0.003272, mae: 0.059555, mean_q: -0.198561
 15889/100000: episode: 172, duration: 0.205s, episode steps: 39, steps per second: 190, episode reward: 15.624, mean reward: 0.401 [0.177, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.352, 10.100], loss: 0.003074, mae: 0.056927, mean_q: -0.189111
 15936/100000: episode: 173, duration: 0.247s, episode steps: 47, steps per second: 190, episode reward: 10.109, mean reward: 0.215 [0.039, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.419, 10.100], loss: 0.003068, mae: 0.057269, mean_q: -0.139809
 15978/100000: episode: 174, duration: 0.234s, episode steps: 42, steps per second: 179, episode reward: 14.333, mean reward: 0.341 [0.131, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.360, 10.100], loss: 0.003147, mae: 0.056695, mean_q: -0.177428
 16003/100000: episode: 175, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 5.782, mean reward: 0.231 [0.072, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.330, 10.100], loss: 0.003281, mae: 0.058961, mean_q: -0.098030
 16055/100000: episode: 176, duration: 0.292s, episode steps: 52, steps per second: 178, episode reward: 8.089, mean reward: 0.156 [0.039, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-0.613, 10.100], loss: 0.003556, mae: 0.060421, mean_q: -0.183702
 16107/100000: episode: 177, duration: 0.276s, episode steps: 52, steps per second: 188, episode reward: 12.135, mean reward: 0.233 [0.013, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.648, 10.100], loss: 0.003750, mae: 0.063397, mean_q: -0.146237
 16140/100000: episode: 178, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 9.772, mean reward: 0.296 [0.186, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-1.557, 10.100], loss: 0.003729, mae: 0.063854, mean_q: -0.117760
 16187/100000: episode: 179, duration: 0.259s, episode steps: 47, steps per second: 181, episode reward: 9.476, mean reward: 0.202 [0.047, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.577, 10.135], loss: 0.003611, mae: 0.061898, mean_q: -0.108051
 16239/100000: episode: 180, duration: 0.286s, episode steps: 52, steps per second: 182, episode reward: 17.162, mean reward: 0.330 [0.158, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-1.302, 10.566], loss: 0.003355, mae: 0.059557, mean_q: -0.131512
 16303/100000: episode: 181, duration: 0.345s, episode steps: 64, steps per second: 186, episode reward: 13.651, mean reward: 0.213 [0.012, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.781 [-1.296, 10.139], loss: 0.003492, mae: 0.061395, mean_q: -0.125056
 16350/100000: episode: 182, duration: 0.243s, episode steps: 47, steps per second: 194, episode reward: 16.535, mean reward: 0.352 [0.222, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.230, 10.400], loss: 0.003305, mae: 0.060752, mean_q: -0.077461
 16402/100000: episode: 183, duration: 0.308s, episode steps: 52, steps per second: 169, episode reward: 9.601, mean reward: 0.185 [0.044, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.637, 10.100], loss: 0.003643, mae: 0.063483, mean_q: -0.076299
 16435/100000: episode: 184, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 11.264, mean reward: 0.341 [0.206, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.544, 10.100], loss: 0.003463, mae: 0.062148, mean_q: -0.104515
 16460/100000: episode: 185, duration: 0.145s, episode steps: 25, steps per second: 172, episode reward: 6.770, mean reward: 0.271 [0.154, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.281, 10.100], loss: 0.003220, mae: 0.060664, mean_q: -0.100003
 16512/100000: episode: 186, duration: 0.270s, episode steps: 52, steps per second: 192, episode reward: 12.840, mean reward: 0.247 [0.111, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.282, 10.488], loss: 0.003306, mae: 0.060165, mean_q: -0.063633
 16534/100000: episode: 187, duration: 0.138s, episode steps: 22, steps per second: 159, episode reward: 8.381, mean reward: 0.381 [0.175, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.767, 10.100], loss: 0.003951, mae: 0.065102, mean_q: -0.031574
 16573/100000: episode: 188, duration: 0.198s, episode steps: 39, steps per second: 197, episode reward: 8.919, mean reward: 0.229 [0.065, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.125, 10.100], loss: 0.003853, mae: 0.066507, mean_q: -0.029501
 16606/100000: episode: 189, duration: 0.191s, episode steps: 33, steps per second: 173, episode reward: 9.312, mean reward: 0.282 [0.198, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.793, 10.100], loss: 0.003949, mae: 0.066714, mean_q: -0.016530
 16670/100000: episode: 190, duration: 0.345s, episode steps: 64, steps per second: 185, episode reward: 9.572, mean reward: 0.150 [0.011, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.779 [-0.797, 10.100], loss: 0.003408, mae: 0.060960, mean_q: -0.079229
 16722/100000: episode: 191, duration: 0.305s, episode steps: 52, steps per second: 170, episode reward: 14.737, mean reward: 0.283 [0.093, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.299, 10.301], loss: 0.003489, mae: 0.063551, mean_q: -0.063041
 16774/100000: episode: 192, duration: 0.287s, episode steps: 52, steps per second: 181, episode reward: 14.166, mean reward: 0.272 [0.035, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.553, 10.217], loss: 0.004716, mae: 0.069102, mean_q: -0.021291
 16807/100000: episode: 193, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 7.504, mean reward: 0.227 [0.057, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.856, 10.100], loss: 0.004228, mae: 0.064716, mean_q: -0.089400
 16859/100000: episode: 194, duration: 0.283s, episode steps: 52, steps per second: 184, episode reward: 19.062, mean reward: 0.367 [0.263, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.289, 10.520], loss: 0.005243, mae: 0.075573, mean_q: 0.008983
 16911/100000: episode: 195, duration: 0.272s, episode steps: 52, steps per second: 191, episode reward: 11.667, mean reward: 0.224 [0.017, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.318, 10.225], loss: 0.003777, mae: 0.065785, mean_q: 0.021529
 16944/100000: episode: 196, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 8.074, mean reward: 0.245 [0.091, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.374, 10.100], loss: 0.003360, mae: 0.061749, mean_q: -0.036134
 16966/100000: episode: 197, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 8.331, mean reward: 0.379 [0.306, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.552, 10.100], loss: 0.003746, mae: 0.065141, mean_q: 0.028140
 17005/100000: episode: 198, duration: 0.208s, episode steps: 39, steps per second: 188, episode reward: 9.485, mean reward: 0.243 [0.136, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.456, 10.100], loss: 0.003635, mae: 0.063735, mean_q: -0.014986
 17044/100000: episode: 199, duration: 0.222s, episode steps: 39, steps per second: 176, episode reward: 8.485, mean reward: 0.218 [0.011, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.082, 10.190], loss: 0.003249, mae: 0.059515, mean_q: 0.003651
 17091/100000: episode: 200, duration: 0.266s, episode steps: 47, steps per second: 177, episode reward: 8.279, mean reward: 0.176 [0.046, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.232, 10.194], loss: 0.003169, mae: 0.059035, mean_q: 0.020711
 17133/100000: episode: 201, duration: 0.229s, episode steps: 42, steps per second: 183, episode reward: 8.159, mean reward: 0.194 [0.085, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.775, 10.100], loss: 0.003650, mae: 0.063584, mean_q: 0.023347
 17172/100000: episode: 202, duration: 0.213s, episode steps: 39, steps per second: 183, episode reward: 6.940, mean reward: 0.178 [0.036, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.681, 10.144], loss: 0.003026, mae: 0.059454, mean_q: 0.043695
 17219/100000: episode: 203, duration: 0.259s, episode steps: 47, steps per second: 182, episode reward: 12.636, mean reward: 0.269 [0.149, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.739, 10.270], loss: 0.003642, mae: 0.064483, mean_q: 0.051545
 17266/100000: episode: 204, duration: 0.265s, episode steps: 47, steps per second: 178, episode reward: 15.150, mean reward: 0.322 [0.219, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.226, 10.365], loss: 0.003453, mae: 0.061573, mean_q: 0.041578
 17318/100000: episode: 205, duration: 0.284s, episode steps: 52, steps per second: 183, episode reward: 9.390, mean reward: 0.181 [0.011, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.996, 10.100], loss: 0.003078, mae: 0.057790, mean_q: 0.023093
 17351/100000: episode: 206, duration: 0.176s, episode steps: 33, steps per second: 187, episode reward: 8.516, mean reward: 0.258 [0.003, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.503, 10.100], loss: 0.003346, mae: 0.061178, mean_q: 0.065335
 17398/100000: episode: 207, duration: 0.251s, episode steps: 47, steps per second: 187, episode reward: 10.918, mean reward: 0.232 [0.056, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.307, 10.323], loss: 0.003562, mae: 0.062770, mean_q: 0.080359
 17431/100000: episode: 208, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 9.518, mean reward: 0.288 [0.065, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.973, 10.100], loss: 0.003450, mae: 0.061571, mean_q: 0.075775
 17470/100000: episode: 209, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 9.730, mean reward: 0.249 [0.126, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.425, 10.100], loss: 0.003507, mae: 0.063537, mean_q: 0.106046
 17517/100000: episode: 210, duration: 0.256s, episode steps: 47, steps per second: 183, episode reward: 9.208, mean reward: 0.196 [0.031, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.468, 10.100], loss: 0.003109, mae: 0.060263, mean_q: 0.075048
 17539/100000: episode: 211, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 7.324, mean reward: 0.333 [0.218, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.706, 10.100], loss: 0.003707, mae: 0.064421, mean_q: 0.092456
 17564/100000: episode: 212, duration: 0.157s, episode steps: 25, steps per second: 160, episode reward: 4.505, mean reward: 0.180 [0.037, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.658, 10.151], loss: 0.004497, mae: 0.070947, mean_q: 0.155497
 17606/100000: episode: 213, duration: 0.234s, episode steps: 42, steps per second: 179, episode reward: 12.035, mean reward: 0.287 [0.123, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-1.049, 10.100], loss: 0.004785, mae: 0.070718, mean_q: 0.117930
 17631/100000: episode: 214, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 5.468, mean reward: 0.219 [0.071, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.568, 10.100], loss: 0.003898, mae: 0.064928, mean_q: 0.090629
 17683/100000: episode: 215, duration: 0.301s, episode steps: 52, steps per second: 173, episode reward: 11.359, mean reward: 0.218 [0.030, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-1.243, 10.100], loss: 0.004915, mae: 0.070014, mean_q: 0.136809
 17705/100000: episode: 216, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 5.448, mean reward: 0.248 [0.157, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.202, 10.100], loss: 0.003446, mae: 0.063407, mean_q: 0.094230
 17747/100000: episode: 217, duration: 0.224s, episode steps: 42, steps per second: 187, episode reward: 13.264, mean reward: 0.316 [0.188, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.352, 10.100], loss: 0.003230, mae: 0.060179, mean_q: 0.106225
 17780/100000: episode: 218, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 7.324, mean reward: 0.222 [0.125, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.604, 10.100], loss: 0.003687, mae: 0.065229, mean_q: 0.167537
 17827/100000: episode: 219, duration: 0.256s, episode steps: 47, steps per second: 184, episode reward: 12.195, mean reward: 0.259 [0.052, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.343, 10.242], loss: 0.004273, mae: 0.070798, mean_q: 0.163380
 17860/100000: episode: 220, duration: 0.181s, episode steps: 33, steps per second: 183, episode reward: 9.511, mean reward: 0.288 [0.137, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.464, 10.100], loss: 0.003236, mae: 0.062491, mean_q: 0.162951
 17924/100000: episode: 221, duration: 0.350s, episode steps: 64, steps per second: 183, episode reward: 13.506, mean reward: 0.211 [0.054, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.784 [-0.483, 10.163], loss: 0.003654, mae: 0.064255, mean_q: 0.160013
 17966/100000: episode: 222, duration: 0.244s, episode steps: 42, steps per second: 172, episode reward: 9.083, mean reward: 0.216 [0.122, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.533, 10.100], loss: 0.003653, mae: 0.064906, mean_q: 0.197379
 18008/100000: episode: 223, duration: 0.232s, episode steps: 42, steps per second: 181, episode reward: 11.246, mean reward: 0.268 [0.121, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-1.061, 10.100], loss: 0.003786, mae: 0.065646, mean_q: 0.147739
 18041/100000: episode: 224, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 9.548, mean reward: 0.289 [0.172, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.350, 10.100], loss: 0.003642, mae: 0.065898, mean_q: 0.163226
 18088/100000: episode: 225, duration: 0.259s, episode steps: 47, steps per second: 182, episode reward: 10.082, mean reward: 0.215 [0.088, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-1.642, 10.225], loss: 0.003763, mae: 0.066597, mean_q: 0.200706
 18135/100000: episode: 226, duration: 0.257s, episode steps: 47, steps per second: 183, episode reward: 10.948, mean reward: 0.233 [0.100, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.233, 10.412], loss: 0.003739, mae: 0.065492, mean_q: 0.185789
 18174/100000: episode: 227, duration: 0.208s, episode steps: 39, steps per second: 188, episode reward: 5.711, mean reward: 0.146 [0.003, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.580, 10.131], loss: 0.003474, mae: 0.063815, mean_q: 0.213231
 18221/100000: episode: 228, duration: 0.248s, episode steps: 47, steps per second: 189, episode reward: 11.386, mean reward: 0.242 [0.143, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-1.320, 10.314], loss: 0.003622, mae: 0.064522, mean_q: 0.197027
 18263/100000: episode: 229, duration: 0.231s, episode steps: 42, steps per second: 182, episode reward: 15.471, mean reward: 0.368 [0.183, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.234, 10.100], loss: 0.003870, mae: 0.065927, mean_q: 0.205978
 18296/100000: episode: 230, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 12.898, mean reward: 0.391 [0.235, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.906, 10.100], loss: 0.003263, mae: 0.062330, mean_q: 0.242977
 18321/100000: episode: 231, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 5.735, mean reward: 0.229 [0.166, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.396, 10.100], loss: 0.003375, mae: 0.063121, mean_q: 0.246933
 18363/100000: episode: 232, duration: 0.210s, episode steps: 42, steps per second: 200, episode reward: 12.008, mean reward: 0.286 [0.159, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.127, 10.100], loss: 0.003824, mae: 0.065388, mean_q: 0.232942
 18385/100000: episode: 233, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 5.024, mean reward: 0.228 [0.111, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.245, 10.100], loss: 0.003960, mae: 0.067428, mean_q: 0.223373
 18427/100000: episode: 234, duration: 0.226s, episode steps: 42, steps per second: 185, episode reward: 9.580, mean reward: 0.228 [0.009, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.231, 10.170], loss: 0.003814, mae: 0.067134, mean_q: 0.256084
 18460/100000: episode: 235, duration: 0.196s, episode steps: 33, steps per second: 169, episode reward: 5.098, mean reward: 0.154 [0.028, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.102, 10.213], loss: 0.003590, mae: 0.066047, mean_q: 0.252598
 18507/100000: episode: 236, duration: 0.256s, episode steps: 47, steps per second: 183, episode reward: 7.424, mean reward: 0.158 [0.009, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.416, 10.349], loss: 0.003490, mae: 0.064020, mean_q: 0.239155
 18529/100000: episode: 237, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 6.520, mean reward: 0.296 [0.190, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.349, 10.100], loss: 0.003631, mae: 0.065421, mean_q: 0.266508
 18571/100000: episode: 238, duration: 0.222s, episode steps: 42, steps per second: 189, episode reward: 10.998, mean reward: 0.262 [0.038, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.998, 10.100], loss: 0.003299, mae: 0.061747, mean_q: 0.260375
 18604/100000: episode: 239, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 8.581, mean reward: 0.260 [0.059, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.429, 10.100], loss: 0.003709, mae: 0.066667, mean_q: 0.287307
[Info] 200-TH LEVEL FOUND: 0.7865417003631592, Considering 10/90 traces
 18643/100000: episode: 240, duration: 4.253s, episode steps: 39, steps per second: 9, episode reward: 7.454, mean reward: 0.191 [0.036, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-1.517, 10.131], loss: 0.004367, mae: 0.070588, mean_q: 0.282239
 18665/100000: episode: 241, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 9.404, mean reward: 0.427 [0.289, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.248, 10.100], loss: 0.003699, mae: 0.066230, mean_q: 0.284522
 18706/100000: episode: 242, duration: 0.233s, episode steps: 41, steps per second: 176, episode reward: 15.683, mean reward: 0.383 [0.245, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.574, 10.401], loss: 0.003780, mae: 0.067652, mean_q: 0.299498
 18720/100000: episode: 243, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 5.615, mean reward: 0.401 [0.208, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.148, 10.100], loss: 0.003661, mae: 0.066097, mean_q: 0.331037
 18772/100000: episode: 244, duration: 0.296s, episode steps: 52, steps per second: 176, episode reward: 11.926, mean reward: 0.229 [0.044, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.338, 10.287], loss: 0.003469, mae: 0.063478, mean_q: 0.305174
 18786/100000: episode: 245, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 5.965, mean reward: 0.426 [0.357, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.527, 10.100], loss: 0.004350, mae: 0.066853, mean_q: 0.266923
 18838/100000: episode: 246, duration: 0.288s, episode steps: 52, steps per second: 180, episode reward: 13.087, mean reward: 0.252 [0.124, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.643, 10.265], loss: 0.003238, mae: 0.061916, mean_q: 0.304799
 18900/100000: episode: 247, duration: 0.341s, episode steps: 62, steps per second: 182, episode reward: 14.199, mean reward: 0.229 [0.050, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.800 [-0.807, 10.189], loss: 0.003755, mae: 0.065773, mean_q: 0.323664
 18935/100000: episode: 248, duration: 0.194s, episode steps: 35, steps per second: 181, episode reward: 11.887, mean reward: 0.340 [0.180, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.594, 10.100], loss: 0.003702, mae: 0.066502, mean_q: 0.320874
 18987/100000: episode: 249, duration: 0.288s, episode steps: 52, steps per second: 180, episode reward: 14.677, mean reward: 0.282 [0.031, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.828, 10.187], loss: 0.003782, mae: 0.066441, mean_q: 0.329521
 19021/100000: episode: 250, duration: 0.213s, episode steps: 34, steps per second: 159, episode reward: 15.138, mean reward: 0.445 [0.367, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-1.544, 10.100], loss: 0.003461, mae: 0.064472, mean_q: 0.353571
 19049/100000: episode: 251, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 10.793, mean reward: 0.385 [0.217, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.477, 10.100], loss: 0.003121, mae: 0.060607, mean_q: 0.348067
 19065/100000: episode: 252, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 5.906, mean reward: 0.369 [0.277, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.417, 10.100], loss: 0.003377, mae: 0.062145, mean_q: 0.358286
 19100/100000: episode: 253, duration: 0.204s, episode steps: 35, steps per second: 172, episode reward: 13.328, mean reward: 0.381 [0.285, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.438, 10.100], loss: 0.003248, mae: 0.063236, mean_q: 0.366713
 19135/100000: episode: 254, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 15.010, mean reward: 0.429 [0.306, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.680, 10.100], loss: 0.003370, mae: 0.063761, mean_q: 0.393410
 19197/100000: episode: 255, duration: 0.341s, episode steps: 62, steps per second: 182, episode reward: 14.942, mean reward: 0.241 [0.015, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.794 [-0.528, 10.150], loss: 0.004308, mae: 0.071501, mean_q: 0.363787
 19259/100000: episode: 256, duration: 0.334s, episode steps: 62, steps per second: 185, episode reward: 15.721, mean reward: 0.254 [0.130, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.798 [-0.302, 10.289], loss: 0.003346, mae: 0.063117, mean_q: 0.385150
 19266/100000: episode: 257, duration: 0.041s, episode steps: 7, steps per second: 173, episode reward: 3.568, mean reward: 0.510 [0.481, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.148, 10.558], loss: 0.002658, mae: 0.057060, mean_q: 0.356744
 19273/100000: episode: 258, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 3.159, mean reward: 0.451 [0.416, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.545], loss: 0.003604, mae: 0.064669, mean_q: 0.445960
 19287/100000: episode: 259, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 5.855, mean reward: 0.418 [0.331, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.384, 10.100], loss: 0.003650, mae: 0.066471, mean_q: 0.401715
 19309/100000: episode: 260, duration: 0.137s, episode steps: 22, steps per second: 160, episode reward: 7.714, mean reward: 0.351 [0.245, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.755, 10.100], loss: 0.003809, mae: 0.067195, mean_q: 0.403825
 19331/100000: episode: 261, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 5.797, mean reward: 0.263 [0.174, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.361, 10.100], loss: 0.003618, mae: 0.066416, mean_q: 0.394755
 19353/100000: episode: 262, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 10.709, mean reward: 0.487 [0.408, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.776, 10.100], loss: 0.003567, mae: 0.065252, mean_q: 0.404324
 19394/100000: episode: 263, duration: 0.247s, episode steps: 41, steps per second: 166, episode reward: 10.598, mean reward: 0.258 [0.107, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.150, 10.306], loss: 0.003447, mae: 0.064587, mean_q: 0.396612
 19435/100000: episode: 264, duration: 0.234s, episode steps: 41, steps per second: 176, episode reward: 14.133, mean reward: 0.345 [0.160, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.306, 10.287], loss: 0.004331, mae: 0.068253, mean_q: 0.427539
 19497/100000: episode: 265, duration: 0.333s, episode steps: 62, steps per second: 186, episode reward: 13.146, mean reward: 0.212 [0.019, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.791 [-0.506, 10.115], loss: 0.003979, mae: 0.068565, mean_q: 0.426739
 19559/100000: episode: 266, duration: 0.348s, episode steps: 62, steps per second: 178, episode reward: 14.859, mean reward: 0.240 [0.053, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.788 [-1.068, 10.100], loss: 0.003170, mae: 0.062398, mean_q: 0.446294
 19581/100000: episode: 267, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 10.323, mean reward: 0.469 [0.392, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.400, 10.100], loss: 0.003328, mae: 0.063669, mean_q: 0.435974
 19622/100000: episode: 268, duration: 0.260s, episode steps: 41, steps per second: 158, episode reward: 14.204, mean reward: 0.346 [0.202, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.637, 10.382], loss: 0.003263, mae: 0.062984, mean_q: 0.457702
 19674/100000: episode: 269, duration: 0.300s, episode steps: 52, steps per second: 173, episode reward: 13.201, mean reward: 0.254 [0.092, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.348, 10.295], loss: 0.004080, mae: 0.070396, mean_q: 0.472664
 19690/100000: episode: 270, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 6.856, mean reward: 0.428 [0.333, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.276, 10.100], loss: 0.002986, mae: 0.059394, mean_q: 0.488188
 19752/100000: episode: 271, duration: 0.335s, episode steps: 62, steps per second: 185, episode reward: 19.323, mean reward: 0.312 [0.124, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.780 [-0.933, 10.200], loss: 0.003339, mae: 0.063888, mean_q: 0.487768
 19814/100000: episode: 272, duration: 0.345s, episode steps: 62, steps per second: 180, episode reward: 14.446, mean reward: 0.233 [0.040, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.791 [-1.139, 10.100], loss: 0.003798, mae: 0.068112, mean_q: 0.493096
 19842/100000: episode: 273, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 12.037, mean reward: 0.430 [0.320, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.601, 10.100], loss: 0.004007, mae: 0.071172, mean_q: 0.509161
 19877/100000: episode: 274, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 13.537, mean reward: 0.387 [0.240, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.870, 10.100], loss: 0.003563, mae: 0.067195, mean_q: 0.504751
 19891/100000: episode: 275, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 6.283, mean reward: 0.449 [0.389, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.337, 10.100], loss: 0.003046, mae: 0.061275, mean_q: 0.515690
 19913/100000: episode: 276, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 10.769, mean reward: 0.489 [0.393, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.621, 10.100], loss: 0.003292, mae: 0.064878, mean_q: 0.514968
 19975/100000: episode: 277, duration: 0.334s, episode steps: 62, steps per second: 185, episode reward: 16.797, mean reward: 0.271 [0.058, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.797 [-0.635, 10.171], loss: 0.003310, mae: 0.063810, mean_q: 0.519950
 20009/100000: episode: 278, duration: 0.192s, episode steps: 34, steps per second: 177, episode reward: 12.848, mean reward: 0.378 [0.225, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.928, 10.100], loss: 0.003301, mae: 0.063142, mean_q: 0.501800
[Info] FALSIFICATION!
 20020/100000: episode: 279, duration: 0.078s, episode steps: 11, steps per second: 141, episode reward: 14.508, mean reward: 1.319 [0.365, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.816 [-0.013, 8.729], loss: 0.002930, mae: 0.061641, mean_q: 0.519810
 20120/100000: episode: 280, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.841, mean reward: -0.158 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.444, 10.145], loss: 0.016238, mae: 0.068742, mean_q: 0.516692
 20220/100000: episode: 281, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.053, mean reward: -0.171 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.616, 10.098], loss: 0.030748, mae: 0.091197, mean_q: 0.504988
 20320/100000: episode: 282, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.847, mean reward: -0.188 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.402, 10.298], loss: 0.004066, mae: 0.067867, mean_q: 0.482247
 20420/100000: episode: 283, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -21.986, mean reward: -0.220 [-1.000, 0.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.543, 10.278], loss: 0.003465, mae: 0.063957, mean_q: 0.460082
 20520/100000: episode: 284, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -18.728, mean reward: -0.187 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.110, 10.260], loss: 0.003666, mae: 0.066714, mean_q: 0.446962
 20620/100000: episode: 285, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.859, mean reward: -0.189 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.962, 10.098], loss: 0.017542, mae: 0.073534, mean_q: 0.422585
 20720/100000: episode: 286, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -13.519, mean reward: -0.135 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.198, 10.098], loss: 0.017734, mae: 0.081822, mean_q: 0.408166
 20820/100000: episode: 287, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -21.003, mean reward: -0.210 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.566, 10.098], loss: 0.003788, mae: 0.065121, mean_q: 0.389762
 20920/100000: episode: 288, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -15.495, mean reward: -0.155 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.121, 10.445], loss: 0.015841, mae: 0.067330, mean_q: 0.356067
 21020/100000: episode: 289, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.969, mean reward: -0.170 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.677, 10.098], loss: 0.030096, mae: 0.086656, mean_q: 0.348400
 21120/100000: episode: 290, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -10.815, mean reward: -0.108 [-1.000, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.718, 10.098], loss: 0.003840, mae: 0.067350, mean_q: 0.343851
 21220/100000: episode: 291, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.197, mean reward: -0.182 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.939, 10.098], loss: 0.004795, mae: 0.071369, mean_q: 0.312811
 21320/100000: episode: 292, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.519, mean reward: -0.185 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.339, 10.098], loss: 0.003917, mae: 0.066381, mean_q: 0.312566
 21420/100000: episode: 293, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.235, mean reward: -0.162 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.964, 10.098], loss: 0.004043, mae: 0.066752, mean_q: 0.275129
 21520/100000: episode: 294, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.649, mean reward: -0.176 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.424, 10.205], loss: 0.003790, mae: 0.064114, mean_q: 0.276386
 21620/100000: episode: 295, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.809, mean reward: -0.188 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.027, 10.162], loss: 0.003078, mae: 0.059760, mean_q: 0.271907
 21720/100000: episode: 296, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.584, mean reward: -0.176 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.743, 10.229], loss: 0.003895, mae: 0.065477, mean_q: 0.224803
 21820/100000: episode: 297, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.213, mean reward: -0.172 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.355, 10.181], loss: 0.016780, mae: 0.073316, mean_q: 0.214919
 21920/100000: episode: 298, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.463, mean reward: -0.125 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.358, 10.228], loss: 0.015905, mae: 0.066984, mean_q: 0.201339
 22020/100000: episode: 299, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.137, mean reward: -0.181 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.972, 10.098], loss: 0.016245, mae: 0.070870, mean_q: 0.197016
 22120/100000: episode: 300, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.723, mean reward: -0.187 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.011, 10.180], loss: 0.003953, mae: 0.064473, mean_q: 0.173486
 22220/100000: episode: 301, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -13.376, mean reward: -0.134 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.333, 10.098], loss: 0.016766, mae: 0.074405, mean_q: 0.172594
 22320/100000: episode: 302, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.497, mean reward: -0.175 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.856, 10.111], loss: 0.003523, mae: 0.063863, mean_q: 0.155426
 22420/100000: episode: 303, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.852, mean reward: -0.169 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.398, 10.183], loss: 0.015541, mae: 0.065568, mean_q: 0.147371
 22520/100000: episode: 304, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -17.246, mean reward: -0.172 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.318, 10.182], loss: 0.015776, mae: 0.071417, mean_q: 0.141711
 22620/100000: episode: 305, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -11.083, mean reward: -0.111 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.416, 10.483], loss: 0.016128, mae: 0.069684, mean_q: 0.105613
 22720/100000: episode: 306, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.684, mean reward: -0.167 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.584, 10.382], loss: 0.016474, mae: 0.074001, mean_q: 0.059413
 22820/100000: episode: 307, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -13.194, mean reward: -0.132 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.022, 10.251], loss: 0.004478, mae: 0.068103, mean_q: 0.070500
 22920/100000: episode: 308, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.829, mean reward: -0.158 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.839, 10.098], loss: 0.004543, mae: 0.066207, mean_q: 0.077029
 23020/100000: episode: 309, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.535, mean reward: -0.185 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.324, 10.098], loss: 0.003863, mae: 0.065121, mean_q: 0.030400
 23120/100000: episode: 310, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -13.231, mean reward: -0.132 [-1.000, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.829, 10.098], loss: 0.015700, mae: 0.067153, mean_q: 0.019876
 23220/100000: episode: 311, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -18.662, mean reward: -0.187 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.383, 10.119], loss: 0.003849, mae: 0.064875, mean_q: -0.022646
 23320/100000: episode: 312, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -16.603, mean reward: -0.166 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.740, 10.098], loss: 0.003293, mae: 0.059991, mean_q: 0.007402
 23420/100000: episode: 313, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.939, mean reward: -0.169 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.320, 10.266], loss: 0.016139, mae: 0.070261, mean_q: -0.019862
 23520/100000: episode: 314, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.650, mean reward: -0.176 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.149, 10.098], loss: 0.015746, mae: 0.066840, mean_q: -0.043758
 23620/100000: episode: 315, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.133, mean reward: -0.161 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.980, 10.416], loss: 0.003212, mae: 0.059886, mean_q: -0.056024
 23720/100000: episode: 316, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.825, mean reward: -0.168 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.208, 10.098], loss: 0.004102, mae: 0.063086, mean_q: -0.090032
 23820/100000: episode: 317, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -19.262, mean reward: -0.193 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.672, 10.118], loss: 0.017044, mae: 0.069216, mean_q: -0.085378
 23920/100000: episode: 318, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.930, mean reward: -0.169 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.789, 10.098], loss: 0.004314, mae: 0.065653, mean_q: -0.121950
 24020/100000: episode: 319, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.918, mean reward: -0.179 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.600, 10.152], loss: 0.016346, mae: 0.070105, mean_q: -0.137891
 24120/100000: episode: 320, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -16.352, mean reward: -0.164 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.556, 10.254], loss: 0.003589, mae: 0.061765, mean_q: -0.165549
 24220/100000: episode: 321, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -14.670, mean reward: -0.147 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.751, 10.147], loss: 0.003288, mae: 0.058882, mean_q: -0.193913
 24320/100000: episode: 322, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -15.744, mean reward: -0.157 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.552, 10.098], loss: 0.003312, mae: 0.057989, mean_q: -0.193995
 24420/100000: episode: 323, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -14.353, mean reward: -0.144 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.567, 10.215], loss: 0.015132, mae: 0.061484, mean_q: -0.229334
 24520/100000: episode: 324, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.887, mean reward: -0.169 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.939, 10.098], loss: 0.003170, mae: 0.058099, mean_q: -0.243584
 24620/100000: episode: 325, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -14.886, mean reward: -0.149 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.149, 10.176], loss: 0.002940, mae: 0.055112, mean_q: -0.277240
 24720/100000: episode: 326, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.645, mean reward: -0.176 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.093, 10.341], loss: 0.015901, mae: 0.066766, mean_q: -0.243758
 24820/100000: episode: 327, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.985, mean reward: -0.180 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.631, 10.118], loss: 0.002874, mae: 0.053957, mean_q: -0.301840
 24920/100000: episode: 328, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.691, mean reward: -0.187 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.962, 10.267], loss: 0.002850, mae: 0.054388, mean_q: -0.264708
 25020/100000: episode: 329, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.052, mean reward: -0.191 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.499, 10.098], loss: 0.016459, mae: 0.069614, mean_q: -0.335773
 25120/100000: episode: 330, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -16.240, mean reward: -0.162 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.334, 10.204], loss: 0.002857, mae: 0.053912, mean_q: -0.327555
 25220/100000: episode: 331, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -15.196, mean reward: -0.152 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.535, 10.415], loss: 0.002982, mae: 0.055124, mean_q: -0.318457
 25320/100000: episode: 332, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.217, mean reward: -0.192 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.616, 10.230], loss: 0.002931, mae: 0.054501, mean_q: -0.293732
 25420/100000: episode: 333, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -16.448, mean reward: -0.164 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.669, 10.146], loss: 0.002783, mae: 0.053869, mean_q: -0.302634
 25520/100000: episode: 334, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -19.200, mean reward: -0.192 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.799, 10.164], loss: 0.002880, mae: 0.053529, mean_q: -0.337639
 25620/100000: episode: 335, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.279, mean reward: -0.163 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.534, 10.098], loss: 0.002959, mae: 0.055135, mean_q: -0.291989
 25720/100000: episode: 336, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -20.010, mean reward: -0.200 [-1.000, 0.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.482, 10.098], loss: 0.002793, mae: 0.053555, mean_q: -0.325892
 25820/100000: episode: 337, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.594, mean reward: -0.176 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.728, 10.229], loss: 0.002897, mae: 0.054248, mean_q: -0.322098
 25920/100000: episode: 338, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -16.599, mean reward: -0.166 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.722, 10.285], loss: 0.004041, mae: 0.063303, mean_q: -0.331771
 26020/100000: episode: 339, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.449, mean reward: -0.184 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.693, 10.199], loss: 0.002739, mae: 0.053544, mean_q: -0.307038
 26120/100000: episode: 340, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.554, mean reward: -0.176 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.514, 10.140], loss: 0.002774, mae: 0.052492, mean_q: -0.338678
 26220/100000: episode: 341, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.660, mean reward: -0.187 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.104, 10.174], loss: 0.002658, mae: 0.051453, mean_q: -0.329964
 26320/100000: episode: 342, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -14.034, mean reward: -0.140 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.759, 10.437], loss: 0.002989, mae: 0.054795, mean_q: -0.307547
 26420/100000: episode: 343, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -19.437, mean reward: -0.194 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.298, 10.098], loss: 0.003085, mae: 0.057520, mean_q: -0.313179
 26520/100000: episode: 344, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.468, mean reward: -0.195 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.679, 10.158], loss: 0.002683, mae: 0.052333, mean_q: -0.335905
 26620/100000: episode: 345, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -18.434, mean reward: -0.184 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.606, 10.233], loss: 0.003005, mae: 0.054757, mean_q: -0.312197
 26720/100000: episode: 346, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.429, mean reward: -0.164 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.194, 10.336], loss: 0.002934, mae: 0.055273, mean_q: -0.319817
 26820/100000: episode: 347, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -13.005, mean reward: -0.130 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.838, 10.098], loss: 0.002749, mae: 0.052887, mean_q: -0.341873
 26920/100000: episode: 348, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.919, mean reward: -0.179 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.204, 10.134], loss: 0.002938, mae: 0.053869, mean_q: -0.334403
 27020/100000: episode: 349, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -17.327, mean reward: -0.173 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.311, 10.413], loss: 0.002864, mae: 0.053863, mean_q: -0.325377
 27120/100000: episode: 350, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.237, mean reward: -0.172 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.234, 10.181], loss: 0.003010, mae: 0.055143, mean_q: -0.347538
 27220/100000: episode: 351, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -17.898, mean reward: -0.179 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.268, 10.104], loss: 0.002826, mae: 0.053984, mean_q: -0.325512
 27320/100000: episode: 352, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -17.082, mean reward: -0.171 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.412, 10.187], loss: 0.002894, mae: 0.054117, mean_q: -0.330245
 27420/100000: episode: 353, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.030, mean reward: -0.180 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.057, 10.098], loss: 0.003141, mae: 0.057322, mean_q: -0.309127
 27520/100000: episode: 354, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -20.026, mean reward: -0.200 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.831, 10.278], loss: 0.003016, mae: 0.055710, mean_q: -0.303230
 27620/100000: episode: 355, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.016, mean reward: -0.180 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.156, 10.140], loss: 0.002863, mae: 0.054003, mean_q: -0.311596
 27720/100000: episode: 356, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.266, mean reward: -0.183 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.763, 10.180], loss: 0.002861, mae: 0.053503, mean_q: -0.306380
 27820/100000: episode: 357, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.692, mean reward: -0.187 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.597, 10.098], loss: 0.002664, mae: 0.051470, mean_q: -0.347770
 27920/100000: episode: 358, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.731, mean reward: -0.187 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.251, 10.231], loss: 0.003085, mae: 0.055355, mean_q: -0.322373
 28020/100000: episode: 359, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -17.065, mean reward: -0.171 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.568, 10.195], loss: 0.002890, mae: 0.055104, mean_q: -0.358700
 28120/100000: episode: 360, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -13.975, mean reward: -0.140 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.190, 10.405], loss: 0.002677, mae: 0.052114, mean_q: -0.371119
 28220/100000: episode: 361, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: -18.701, mean reward: -0.187 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.500, 10.315], loss: 0.002856, mae: 0.054209, mean_q: -0.284680
 28320/100000: episode: 362, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -19.220, mean reward: -0.192 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.491, 10.148], loss: 0.004752, mae: 0.064959, mean_q: -0.319246
 28420/100000: episode: 363, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -16.769, mean reward: -0.168 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.958, 10.373], loss: 0.005716, mae: 0.072135, mean_q: -0.340397
 28520/100000: episode: 364, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -16.019, mean reward: -0.160 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.984, 10.383], loss: 0.003166, mae: 0.057221, mean_q: -0.298788
 28620/100000: episode: 365, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -18.634, mean reward: -0.186 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.561, 10.098], loss: 0.002843, mae: 0.053459, mean_q: -0.337208
 28720/100000: episode: 366, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: -13.846, mean reward: -0.138 [-1.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.397, 10.257], loss: 0.002808, mae: 0.053282, mean_q: -0.331074
 28820/100000: episode: 367, duration: 0.899s, episode steps: 100, steps per second: 111, episode reward: -19.140, mean reward: -0.191 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.597, 10.125], loss: 0.002784, mae: 0.052457, mean_q: -0.324495
 28920/100000: episode: 368, duration: 1.143s, episode steps: 100, steps per second: 87, episode reward: -15.478, mean reward: -0.155 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.123, 10.098], loss: 0.002580, mae: 0.050135, mean_q: -0.350964
 29020/100000: episode: 369, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: -17.368, mean reward: -0.174 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.373, 10.354], loss: 0.002474, mae: 0.049832, mean_q: -0.325399
 29120/100000: episode: 370, duration: 0.738s, episode steps: 100, steps per second: 136, episode reward: -19.498, mean reward: -0.195 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.979, 10.098], loss: 0.002818, mae: 0.052048, mean_q: -0.346039
 29220/100000: episode: 371, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: -17.957, mean reward: -0.180 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.156, 10.108], loss: 0.002569, mae: 0.050762, mean_q: -0.313806
 29320/100000: episode: 372, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -11.703, mean reward: -0.117 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.976, 10.300], loss: 0.002726, mae: 0.051546, mean_q: -0.333382
 29420/100000: episode: 373, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -13.970, mean reward: -0.140 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.554, 10.098], loss: 0.002859, mae: 0.053192, mean_q: -0.301960
 29520/100000: episode: 374, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -19.410, mean reward: -0.194 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.313, 10.098], loss: 0.003582, mae: 0.059036, mean_q: -0.292062
 29620/100000: episode: 375, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.358, mean reward: -0.164 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.513, 10.098], loss: 0.003242, mae: 0.057628, mean_q: -0.320651
 29720/100000: episode: 376, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -19.826, mean reward: -0.198 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.649, 10.098], loss: 0.002731, mae: 0.053275, mean_q: -0.324203
 29820/100000: episode: 377, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -13.401, mean reward: -0.134 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.861, 10.337], loss: 0.002959, mae: 0.054201, mean_q: -0.355974
 29920/100000: episode: 378, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -18.275, mean reward: -0.183 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.869, 10.098], loss: 0.003038, mae: 0.055692, mean_q: -0.343468
[Info] 100-TH LEVEL FOUND: 0.6423546075820923, Considering 10/90 traces
 30020/100000: episode: 379, duration: 4.733s, episode steps: 100, steps per second: 21, episode reward: -17.295, mean reward: -0.173 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.941, 10.098], loss: 0.002675, mae: 0.052523, mean_q: -0.328111
 30048/100000: episode: 380, duration: 0.159s, episode steps: 28, steps per second: 177, episode reward: 9.722, mean reward: 0.347 [0.264, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.367, 10.376], loss: 0.002487, mae: 0.049836, mean_q: -0.371265
 30065/100000: episode: 381, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 6.222, mean reward: 0.366 [0.277, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.387], loss: 0.003037, mae: 0.052999, mean_q: -0.304778
 30082/100000: episode: 382, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 6.599, mean reward: 0.388 [0.270, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.353], loss: 0.002711, mae: 0.051902, mean_q: -0.263716
 30111/100000: episode: 383, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 7.632, mean reward: 0.263 [0.060, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.628, 10.100], loss: 0.003751, mae: 0.062286, mean_q: -0.397942
 30139/100000: episode: 384, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 11.048, mean reward: 0.395 [0.291, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.779, 10.510], loss: 0.003387, mae: 0.058792, mean_q: -0.340491
 30168/100000: episode: 385, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 7.148, mean reward: 0.246 [0.098, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.628, 10.100], loss: 0.002904, mae: 0.054353, mean_q: -0.304092
 30184/100000: episode: 386, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 5.219, mean reward: 0.326 [0.252, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.052, 10.100], loss: 0.002807, mae: 0.055718, mean_q: -0.243518
 30197/100000: episode: 387, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 4.495, mean reward: 0.346 [0.268, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.044, 10.398], loss: 0.002441, mae: 0.049320, mean_q: -0.314382
 30210/100000: episode: 388, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 3.555, mean reward: 0.273 [0.191, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.337], loss: 0.002836, mae: 0.052354, mean_q: -0.320688
 30261/100000: episode: 389, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 15.207, mean reward: 0.298 [0.202, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.714, 10.357], loss: 0.002823, mae: 0.052558, mean_q: -0.316054
 30290/100000: episode: 390, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 8.621, mean reward: 0.297 [0.158, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.261, 10.100], loss: 0.002861, mae: 0.052875, mean_q: -0.311242
 30319/100000: episode: 391, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 7.399, mean reward: 0.255 [0.069, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.547, 10.100], loss: 0.002559, mae: 0.051694, mean_q: -0.263970
 30347/100000: episode: 392, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 11.687, mean reward: 0.417 [0.270, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.868, 10.410], loss: 0.002882, mae: 0.052399, mean_q: -0.269689
 30360/100000: episode: 393, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 3.350, mean reward: 0.258 [0.173, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.296], loss: 0.002594, mae: 0.050101, mean_q: -0.292824
 30377/100000: episode: 394, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 5.433, mean reward: 0.320 [0.235, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.412], loss: 0.002662, mae: 0.049676, mean_q: -0.336769
 30393/100000: episode: 395, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 4.025, mean reward: 0.252 [0.167, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.152, 10.100], loss: 0.003095, mae: 0.057592, mean_q: -0.186288
 30409/100000: episode: 396, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 5.388, mean reward: 0.337 [0.173, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.390, 10.100], loss: 0.003123, mae: 0.058877, mean_q: -0.216843
 30448/100000: episode: 397, duration: 0.205s, episode steps: 39, steps per second: 191, episode reward: 12.135, mean reward: 0.311 [0.062, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.322, 10.100], loss: 0.002951, mae: 0.054394, mean_q: -0.236196
 30461/100000: episode: 398, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 4.699, mean reward: 0.361 [0.291, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.103, 10.432], loss: 0.002633, mae: 0.052418, mean_q: -0.259069
 30474/100000: episode: 399, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 5.428, mean reward: 0.418 [0.342, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.073, 10.609], loss: 0.002700, mae: 0.050771, mean_q: -0.225771
 30491/100000: episode: 400, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 3.725, mean reward: 0.219 [0.084, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.182, 10.100], loss: 0.003300, mae: 0.056649, mean_q: -0.254948
 30520/100000: episode: 401, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 7.840, mean reward: 0.270 [0.200, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.204, 10.100], loss: 0.003091, mae: 0.058096, mean_q: -0.180164
 30549/100000: episode: 402, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 9.910, mean reward: 0.342 [0.229, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.370, 10.100], loss: 0.002866, mae: 0.052823, mean_q: -0.246704
 30565/100000: episode: 403, duration: 0.090s, episode steps: 16, steps per second: 179, episode reward: 6.155, mean reward: 0.385 [0.289, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.403, 10.100], loss: 0.002622, mae: 0.051098, mean_q: -0.231762
 30604/100000: episode: 404, duration: 0.208s, episode steps: 39, steps per second: 187, episode reward: 13.718, mean reward: 0.352 [0.282, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.635, 10.100], loss: 0.003469, mae: 0.060334, mean_q: -0.193049
 30621/100000: episode: 405, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 5.705, mean reward: 0.336 [0.232, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.516, 10.444], loss: 0.006173, mae: 0.070814, mean_q: -0.194364
 30672/100000: episode: 406, duration: 0.269s, episode steps: 51, steps per second: 189, episode reward: 15.620, mean reward: 0.306 [0.035, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.451, 10.215], loss: 0.004879, mae: 0.070420, mean_q: -0.213060
 30685/100000: episode: 407, duration: 0.076s, episode steps: 13, steps per second: 170, episode reward: 4.852, mean reward: 0.373 [0.315, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.526, 10.496], loss: 0.002665, mae: 0.053236, mean_q: -0.149040
 30702/100000: episode: 408, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 4.694, mean reward: 0.276 [0.203, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-1.472, 10.100], loss: 0.002678, mae: 0.052931, mean_q: -0.195502
 30715/100000: episode: 409, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 5.966, mean reward: 0.459 [0.309, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.673], loss: 0.003345, mae: 0.058093, mean_q: -0.258735
 30731/100000: episode: 410, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 4.912, mean reward: 0.307 [0.195, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.228, 10.100], loss: 0.003111, mae: 0.054679, mean_q: -0.232412
 30760/100000: episode: 411, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 9.646, mean reward: 0.333 [0.212, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.209, 10.100], loss: 0.003219, mae: 0.058743, mean_q: -0.161174
 30777/100000: episode: 412, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 4.169, mean reward: 0.245 [0.094, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.122, 10.100], loss: 0.003452, mae: 0.060128, mean_q: -0.136942
 30806/100000: episode: 413, duration: 0.174s, episode steps: 29, steps per second: 167, episode reward: 8.989, mean reward: 0.310 [0.198, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.498, 10.100], loss: 0.002623, mae: 0.049824, mean_q: -0.233078
 30823/100000: episode: 414, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 4.889, mean reward: 0.288 [0.199, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.251, 10.100], loss: 0.003087, mae: 0.054744, mean_q: -0.175554
 30852/100000: episode: 415, duration: 0.167s, episode steps: 29, steps per second: 174, episode reward: 8.748, mean reward: 0.302 [0.191, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.282, 10.100], loss: 0.002970, mae: 0.054859, mean_q: -0.176567
 30869/100000: episode: 416, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 4.793, mean reward: 0.282 [0.152, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.102, 10.100], loss: 0.002528, mae: 0.051815, mean_q: -0.138510
 30908/100000: episode: 417, duration: 0.213s, episode steps: 39, steps per second: 183, episode reward: 10.004, mean reward: 0.257 [0.172, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.635, 10.100], loss: 0.002853, mae: 0.055781, mean_q: -0.169866
 30937/100000: episode: 418, duration: 0.173s, episode steps: 29, steps per second: 168, episode reward: 10.723, mean reward: 0.370 [0.226, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.920, 10.100], loss: 0.002723, mae: 0.054843, mean_q: -0.206292
 30966/100000: episode: 419, duration: 0.150s, episode steps: 29, steps per second: 194, episode reward: 6.451, mean reward: 0.222 [0.027, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.225, 10.100], loss: 0.003187, mae: 0.057147, mean_q: -0.137870
 30995/100000: episode: 420, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 5.705, mean reward: 0.197 [0.027, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.125, 10.116], loss: 0.003442, mae: 0.059577, mean_q: -0.144518
 31011/100000: episode: 421, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 3.909, mean reward: 0.244 [0.165, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.549, 10.100], loss: 0.003038, mae: 0.056020, mean_q: -0.136073
 31028/100000: episode: 422, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 5.529, mean reward: 0.325 [0.231, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.540, 10.370], loss: 0.003071, mae: 0.057052, mean_q: -0.121475
 31041/100000: episode: 423, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 4.479, mean reward: 0.345 [0.275, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.428], loss: 0.003166, mae: 0.058366, mean_q: -0.092429
 31080/100000: episode: 424, duration: 0.231s, episode steps: 39, steps per second: 169, episode reward: 9.177, mean reward: 0.235 [0.050, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.355, 10.100], loss: 0.003120, mae: 0.056776, mean_q: -0.083521
 31109/100000: episode: 425, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 9.121, mean reward: 0.315 [0.200, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.239, 10.100], loss: 0.002927, mae: 0.056659, mean_q: -0.094122
 31126/100000: episode: 426, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 5.954, mean reward: 0.350 [0.253, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.337, 10.532], loss: 0.003219, mae: 0.058727, mean_q: -0.105154
 31155/100000: episode: 427, duration: 0.168s, episode steps: 29, steps per second: 173, episode reward: 7.333, mean reward: 0.253 [0.154, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.810, 10.100], loss: 0.002946, mae: 0.056976, mean_q: -0.092236
 31194/100000: episode: 428, duration: 0.208s, episode steps: 39, steps per second: 187, episode reward: 11.716, mean reward: 0.300 [0.132, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.236, 10.100], loss: 0.002771, mae: 0.054115, mean_q: -0.091692
 31245/100000: episode: 429, duration: 0.281s, episode steps: 51, steps per second: 181, episode reward: 8.316, mean reward: 0.163 [0.024, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.351, 10.102], loss: 0.002597, mae: 0.052248, mean_q: -0.084916
 31284/100000: episode: 430, duration: 0.228s, episode steps: 39, steps per second: 171, episode reward: 10.970, mean reward: 0.281 [0.038, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.381, 10.147], loss: 0.002929, mae: 0.054962, mean_q: -0.110497
 31300/100000: episode: 431, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 3.956, mean reward: 0.247 [0.188, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.253, 10.100], loss: 0.002989, mae: 0.055203, mean_q: -0.076471
 31316/100000: episode: 432, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 4.081, mean reward: 0.255 [0.165, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.315, 10.100], loss: 0.002808, mae: 0.053138, mean_q: -0.141822
 31332/100000: episode: 433, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 4.483, mean reward: 0.280 [0.201, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.177, 10.100], loss: 0.003236, mae: 0.057190, mean_q: -0.135687
 31383/100000: episode: 434, duration: 0.269s, episode steps: 51, steps per second: 190, episode reward: 22.468, mean reward: 0.441 [0.235, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-1.318, 10.498], loss: 0.003256, mae: 0.059504, mean_q: -0.090196
 31434/100000: episode: 435, duration: 0.278s, episode steps: 51, steps per second: 184, episode reward: 10.528, mean reward: 0.206 [0.028, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-1.092, 10.100], loss: 0.003378, mae: 0.058050, mean_q: -0.053268
 31451/100000: episode: 436, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 5.959, mean reward: 0.351 [0.274, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.676, 10.100], loss: 0.005430, mae: 0.065876, mean_q: -0.113796
 31464/100000: episode: 437, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 5.229, mean reward: 0.402 [0.315, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.516, 10.615], loss: 0.004320, mae: 0.069225, mean_q: -0.133425
 31481/100000: episode: 438, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 5.593, mean reward: 0.329 [0.187, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.751, 10.100], loss: 0.003480, mae: 0.064330, mean_q: -0.070686
 31520/100000: episode: 439, duration: 0.218s, episode steps: 39, steps per second: 179, episode reward: 9.294, mean reward: 0.238 [0.063, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.870, 10.100], loss: 0.003140, mae: 0.058384, mean_q: -0.064147
 31571/100000: episode: 440, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 19.389, mean reward: 0.380 [0.249, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.444, 10.465], loss: 0.003150, mae: 0.059204, mean_q: -0.033637
 31587/100000: episode: 441, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 5.209, mean reward: 0.326 [0.225, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.798, 10.100], loss: 0.002909, mae: 0.055332, mean_q: -0.064148
 31615/100000: episode: 442, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 8.031, mean reward: 0.287 [0.142, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.147, 10.264], loss: 0.003067, mae: 0.056487, mean_q: -0.006296
 31632/100000: episode: 443, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 3.669, mean reward: 0.216 [0.074, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-1.240, 10.244], loss: 0.003805, mae: 0.063554, mean_q: 0.037991
 31661/100000: episode: 444, duration: 0.152s, episode steps: 29, steps per second: 190, episode reward: 9.190, mean reward: 0.317 [0.200, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.371, 10.100], loss: 0.003199, mae: 0.057245, mean_q: -0.069648
 31677/100000: episode: 445, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 3.461, mean reward: 0.216 [0.104, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.100], loss: 0.002929, mae: 0.055957, mean_q: 0.000399
 31693/100000: episode: 446, duration: 0.133s, episode steps: 16, steps per second: 121, episode reward: 4.949, mean reward: 0.309 [0.225, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.230, 10.100], loss: 0.002741, mae: 0.054447, mean_q: 0.029185
 31721/100000: episode: 447, duration: 0.207s, episode steps: 28, steps per second: 135, episode reward: 9.711, mean reward: 0.347 [0.230, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.532, 10.478], loss: 0.002737, mae: 0.054200, mean_q: -0.004198
 31734/100000: episode: 448, duration: 0.094s, episode steps: 13, steps per second: 138, episode reward: 4.991, mean reward: 0.384 [0.262, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.846, 10.487], loss: 0.002618, mae: 0.051688, mean_q: -0.023044
 31763/100000: episode: 449, duration: 0.337s, episode steps: 29, steps per second: 86, episode reward: 10.040, mean reward: 0.346 [0.250, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.435, 10.100], loss: 0.003013, mae: 0.057706, mean_q: 0.050645
 31780/100000: episode: 450, duration: 0.150s, episode steps: 17, steps per second: 113, episode reward: 4.766, mean reward: 0.280 [0.137, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-1.737, 10.100], loss: 0.002740, mae: 0.054099, mean_q: 0.024257
 31819/100000: episode: 451, duration: 0.300s, episode steps: 39, steps per second: 130, episode reward: 13.305, mean reward: 0.341 [0.191, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.627, 10.100], loss: 0.003020, mae: 0.055493, mean_q: -0.041833
 31835/100000: episode: 452, duration: 0.165s, episode steps: 16, steps per second: 97, episode reward: 4.305, mean reward: 0.269 [0.161, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.739, 10.100], loss: 0.002648, mae: 0.051787, mean_q: -0.016678
 31851/100000: episode: 453, duration: 0.148s, episode steps: 16, steps per second: 108, episode reward: 4.912, mean reward: 0.307 [0.233, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.745, 10.100], loss: 0.003183, mae: 0.056161, mean_q: 0.008092
 31880/100000: episode: 454, duration: 0.212s, episode steps: 29, steps per second: 137, episode reward: 12.962, mean reward: 0.447 [0.367, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.388, 10.100], loss: 0.002733, mae: 0.053944, mean_q: 0.040491
 31909/100000: episode: 455, duration: 0.243s, episode steps: 29, steps per second: 119, episode reward: 10.076, mean reward: 0.347 [0.204, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.608, 10.100], loss: 0.002981, mae: 0.056626, mean_q: -0.013332
 31922/100000: episode: 456, duration: 0.092s, episode steps: 13, steps per second: 141, episode reward: 5.122, mean reward: 0.394 [0.287, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.235, 10.485], loss: 0.002699, mae: 0.054677, mean_q: -0.030312
 31939/100000: episode: 457, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 5.140, mean reward: 0.302 [0.229, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.151, 10.100], loss: 0.003001, mae: 0.057186, mean_q: 0.019563
 31956/100000: episode: 458, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 5.933, mean reward: 0.349 [0.205, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.514], loss: 0.003222, mae: 0.059528, mean_q: 0.129239
 31972/100000: episode: 459, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 4.642, mean reward: 0.290 [0.191, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.468, 10.100], loss: 0.002786, mae: 0.055108, mean_q: 0.033288
 31989/100000: episode: 460, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 6.132, mean reward: 0.361 [0.253, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.409], loss: 0.002669, mae: 0.053675, mean_q: 0.057877
 32017/100000: episode: 461, duration: 0.182s, episode steps: 28, steps per second: 154, episode reward: 12.711, mean reward: 0.454 [0.257, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.184, 10.536], loss: 0.002712, mae: 0.053560, mean_q: 0.051574
 32034/100000: episode: 462, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 6.212, mean reward: 0.365 [0.304, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.464], loss: 0.002626, mae: 0.052252, mean_q: 0.031834
 32050/100000: episode: 463, duration: 0.101s, episode steps: 16, steps per second: 159, episode reward: 4.368, mean reward: 0.273 [0.197, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.161, 10.100], loss: 0.003361, mae: 0.060250, mean_q: 0.065935
 32079/100000: episode: 464, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 8.535, mean reward: 0.294 [0.197, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.224, 10.100], loss: 0.003161, mae: 0.060316, mean_q: 0.108126
 32107/100000: episode: 465, duration: 0.177s, episode steps: 28, steps per second: 158, episode reward: 6.460, mean reward: 0.231 [0.134, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.252, 10.274], loss: 0.003264, mae: 0.058674, mean_q: 0.036457
 32124/100000: episode: 466, duration: 0.118s, episode steps: 17, steps per second: 144, episode reward: 5.121, mean reward: 0.301 [0.184, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.381, 10.100], loss: 0.003415, mae: 0.060753, mean_q: 0.134877
 32140/100000: episode: 467, duration: 0.107s, episode steps: 16, steps per second: 150, episode reward: 6.185, mean reward: 0.387 [0.225, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.274, 10.100], loss: 0.002853, mae: 0.056819, mean_q: 0.040597
 32157/100000: episode: 468, duration: 0.123s, episode steps: 17, steps per second: 139, episode reward: 4.402, mean reward: 0.259 [0.102, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.382, 10.100], loss: 0.003448, mae: 0.060834, mean_q: 0.084831
[Info] 200-TH LEVEL FOUND: 0.8401658535003662, Considering 10/90 traces
 32186/100000: episode: 469, duration: 5.315s, episode steps: 29, steps per second: 5, episode reward: 13.432, mean reward: 0.463 [0.308, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.788, 10.100], loss: 0.002978, mae: 0.056210, mean_q: 0.034532
 32205/100000: episode: 470, duration: 0.094s, episode steps: 19, steps per second: 203, episode reward: 8.150, mean reward: 0.429 [0.332, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.273, 10.100], loss: 0.003279, mae: 0.058945, mean_q: 0.136106
 32224/100000: episode: 471, duration: 0.123s, episode steps: 19, steps per second: 154, episode reward: 9.000, mean reward: 0.474 [0.381, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.535], loss: 0.003218, mae: 0.060080, mean_q: 0.083683
 32257/100000: episode: 472, duration: 0.203s, episode steps: 33, steps per second: 163, episode reward: 13.298, mean reward: 0.403 [0.338, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.738, 10.495], loss: 0.003282, mae: 0.060309, mean_q: 0.052485
 32264/100000: episode: 473, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 3.337, mean reward: 0.477 [0.405, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.586], loss: 0.003346, mae: 0.062344, mean_q: -0.008452
 32286/100000: episode: 474, duration: 0.137s, episode steps: 22, steps per second: 160, episode reward: 9.290, mean reward: 0.422 [0.323, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.514], loss: 0.002895, mae: 0.058172, mean_q: 0.151975
 32306/100000: episode: 475, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 8.239, mean reward: 0.412 [0.279, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.633, 10.321], loss: 0.003450, mae: 0.062936, mean_q: 0.152978
 32325/100000: episode: 476, duration: 0.128s, episode steps: 19, steps per second: 149, episode reward: 9.233, mean reward: 0.486 [0.399, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.465, 10.100], loss: 0.003178, mae: 0.059599, mean_q: 0.138928
 32344/100000: episode: 477, duration: 0.125s, episode steps: 19, steps per second: 152, episode reward: 8.179, mean reward: 0.430 [0.363, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.154, 10.501], loss: 0.003407, mae: 0.059848, mean_q: 0.132762
 32364/100000: episode: 478, duration: 0.120s, episode steps: 20, steps per second: 166, episode reward: 9.336, mean reward: 0.467 [0.377, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.130, 10.481], loss: 0.003527, mae: 0.062169, mean_q: 0.170985
 32397/100000: episode: 479, duration: 0.192s, episode steps: 33, steps per second: 171, episode reward: 12.029, mean reward: 0.365 [0.094, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.701, 10.247], loss: 0.003107, mae: 0.057714, mean_q: 0.096153
 32413/100000: episode: 480, duration: 0.109s, episode steps: 16, steps per second: 147, episode reward: 6.272, mean reward: 0.392 [0.277, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.852, 10.100], loss: 0.003175, mae: 0.057863, mean_q: 0.075969
 32429/100000: episode: 481, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 6.843, mean reward: 0.428 [0.269, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.636, 10.100], loss: 0.002792, mae: 0.054148, mean_q: 0.146226
 32445/100000: episode: 482, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 8.022, mean reward: 0.501 [0.405, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.421, 10.100], loss: 0.002912, mae: 0.056649, mean_q: 0.169227
 32467/100000: episode: 483, duration: 0.140s, episode steps: 22, steps per second: 158, episode reward: 9.912, mean reward: 0.451 [0.389, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.453], loss: 0.003310, mae: 0.059216, mean_q: 0.105640
 32489/100000: episode: 484, duration: 0.130s, episode steps: 22, steps per second: 170, episode reward: 9.113, mean reward: 0.414 [0.333, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.818, 10.483], loss: 0.003056, mae: 0.059066, mean_q: 0.203296
 32511/100000: episode: 485, duration: 0.132s, episode steps: 22, steps per second: 166, episode reward: 7.053, mean reward: 0.321 [0.094, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-1.079, 10.266], loss: 0.003792, mae: 0.063275, mean_q: 0.157080
 32544/100000: episode: 486, duration: 0.227s, episode steps: 33, steps per second: 145, episode reward: 12.512, mean reward: 0.379 [0.158, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.430, 10.345], loss: 0.003238, mae: 0.060509, mean_q: 0.172826
 32563/100000: episode: 487, duration: 0.133s, episode steps: 19, steps per second: 143, episode reward: 8.198, mean reward: 0.431 [0.327, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.172, 10.100], loss: 0.003317, mae: 0.060842, mean_q: 0.212302
 32570/100000: episode: 488, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 3.671, mean reward: 0.524 [0.469, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.526, 10.583], loss: 0.004046, mae: 0.065713, mean_q: 0.193935
 32590/100000: episode: 489, duration: 0.141s, episode steps: 20, steps per second: 142, episode reward: 9.705, mean reward: 0.485 [0.401, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.315, 10.580], loss: 0.004069, mae: 0.064008, mean_q: 0.141046
 32603/100000: episode: 490, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 4.460, mean reward: 0.343 [0.257, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.366, 10.100], loss: 0.003874, mae: 0.067293, mean_q: 0.168274
 32622/100000: episode: 491, duration: 0.136s, episode steps: 19, steps per second: 140, episode reward: 7.551, mean reward: 0.397 [0.258, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.206, 10.100], loss: 0.003623, mae: 0.063679, mean_q: 0.163003
 32641/100000: episode: 492, duration: 0.123s, episode steps: 19, steps per second: 154, episode reward: 8.797, mean reward: 0.463 [0.367, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.353, 10.100], loss: 0.003129, mae: 0.059105, mean_q: 0.128718
 32663/100000: episode: 493, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 8.991, mean reward: 0.409 [0.333, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.566], loss: 0.003549, mae: 0.063068, mean_q: 0.238263
 32683/100000: episode: 494, duration: 0.139s, episode steps: 20, steps per second: 144, episode reward: 10.579, mean reward: 0.529 [0.475, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.435], loss: 0.003017, mae: 0.056277, mean_q: 0.162275
 32696/100000: episode: 495, duration: 0.098s, episode steps: 13, steps per second: 133, episode reward: 4.422, mean reward: 0.340 [0.262, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.284, 10.100], loss: 0.003772, mae: 0.064277, mean_q: 0.235605
 32715/100000: episode: 496, duration: 0.126s, episode steps: 19, steps per second: 151, episode reward: 6.260, mean reward: 0.329 [0.189, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.635, 10.332], loss: 0.002764, mae: 0.055860, mean_q: 0.233047
 32734/100000: episode: 497, duration: 0.128s, episode steps: 19, steps per second: 148, episode reward: 8.389, mean reward: 0.442 [0.182, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.064, 10.100], loss: 0.003684, mae: 0.062570, mean_q: 0.219606
 32753/100000: episode: 498, duration: 0.136s, episode steps: 19, steps per second: 139, episode reward: 6.802, mean reward: 0.358 [0.285, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.512], loss: 0.003178, mae: 0.058587, mean_q: 0.194575
 32772/100000: episode: 499, duration: 0.149s, episode steps: 19, steps per second: 128, episode reward: 7.604, mean reward: 0.400 [0.303, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.362, 10.100], loss: 0.003128, mae: 0.057467, mean_q: 0.147769
 32779/100000: episode: 500, duration: 0.067s, episode steps: 7, steps per second: 104, episode reward: 3.410, mean reward: 0.487 [0.369, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-1.490, 10.685], loss: 0.002604, mae: 0.053433, mean_q: 0.116017
 32801/100000: episode: 501, duration: 0.155s, episode steps: 22, steps per second: 142, episode reward: 7.726, mean reward: 0.351 [0.249, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.282, 10.425], loss: 0.003103, mae: 0.058537, mean_q: 0.203402
 32817/100000: episode: 502, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 8.680, mean reward: 0.543 [0.360, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.336, 10.100], loss: 0.003678, mae: 0.063409, mean_q: 0.217714
 32850/100000: episode: 503, duration: 0.216s, episode steps: 33, steps per second: 153, episode reward: 15.875, mean reward: 0.481 [0.342, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.397, 10.706], loss: 0.003174, mae: 0.058436, mean_q: 0.225780
 32869/100000: episode: 504, duration: 0.152s, episode steps: 19, steps per second: 125, episode reward: 5.324, mean reward: 0.280 [0.151, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.304], loss: 0.002913, mae: 0.055860, mean_q: 0.222299
 32888/100000: episode: 505, duration: 0.130s, episode steps: 19, steps per second: 146, episode reward: 9.119, mean reward: 0.480 [0.358, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.363, 10.100], loss: 0.003546, mae: 0.062708, mean_q: 0.258501
 32907/100000: episode: 506, duration: 0.154s, episode steps: 19, steps per second: 124, episode reward: 7.718, mean reward: 0.406 [0.223, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.826, 10.412], loss: 0.003331, mae: 0.059206, mean_q: 0.227436
 32920/100000: episode: 507, duration: 0.110s, episode steps: 13, steps per second: 118, episode reward: 5.758, mean reward: 0.443 [0.391, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.293, 10.100], loss: 0.003090, mae: 0.058086, mean_q: 0.256578
 32936/100000: episode: 508, duration: 0.191s, episode steps: 16, steps per second: 84, episode reward: 6.475, mean reward: 0.405 [0.334, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.272, 10.100], loss: 0.003313, mae: 0.059760, mean_q: 0.293456
 32958/100000: episode: 509, duration: 0.170s, episode steps: 22, steps per second: 129, episode reward: 8.243, mean reward: 0.375 [0.289, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.546], loss: 0.003714, mae: 0.063609, mean_q: 0.310352
 32974/100000: episode: 510, duration: 0.110s, episode steps: 16, steps per second: 146, episode reward: 6.389, mean reward: 0.399 [0.241, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.351, 10.100], loss: 0.003567, mae: 0.064652, mean_q: 0.244834
 32996/100000: episode: 511, duration: 0.152s, episode steps: 22, steps per second: 144, episode reward: 9.116, mean reward: 0.414 [0.294, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.661, 10.633], loss: 0.003660, mae: 0.062266, mean_q: 0.243892
 33018/100000: episode: 512, duration: 0.140s, episode steps: 22, steps per second: 157, episode reward: 8.678, mean reward: 0.394 [0.314, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.545, 10.505], loss: 0.004376, mae: 0.069679, mean_q: 0.285479
 33031/100000: episode: 513, duration: 0.164s, episode steps: 13, steps per second: 79, episode reward: 5.538, mean reward: 0.426 [0.342, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.329, 10.100], loss: 0.003562, mae: 0.064639, mean_q: 0.288800
 33044/100000: episode: 514, duration: 0.145s, episode steps: 13, steps per second: 90, episode reward: 5.251, mean reward: 0.404 [0.345, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.836, 10.100], loss: 0.002871, mae: 0.057511, mean_q: 0.289069
 33063/100000: episode: 515, duration: 0.178s, episode steps: 19, steps per second: 107, episode reward: 7.933, mean reward: 0.418 [0.302, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.111, 10.371], loss: 0.003387, mae: 0.059457, mean_q: 0.294854
 33082/100000: episode: 516, duration: 0.190s, episode steps: 19, steps per second: 100, episode reward: 9.060, mean reward: 0.477 [0.309, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.285, 10.100], loss: 0.004201, mae: 0.065650, mean_q: 0.336899
 33104/100000: episode: 517, duration: 0.203s, episode steps: 22, steps per second: 108, episode reward: 7.195, mean reward: 0.327 [0.239, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.465], loss: 0.004817, mae: 0.071199, mean_q: 0.315354
 33137/100000: episode: 518, duration: 0.206s, episode steps: 33, steps per second: 160, episode reward: 10.903, mean reward: 0.330 [0.188, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.036, 10.305], loss: 0.003717, mae: 0.064974, mean_q: 0.330808
 33144/100000: episode: 519, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 3.086, mean reward: 0.441 [0.374, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.655, 10.572], loss: 0.002813, mae: 0.055747, mean_q: 0.332809
 33163/100000: episode: 520, duration: 0.127s, episode steps: 19, steps per second: 150, episode reward: 9.836, mean reward: 0.518 [0.341, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.327, 10.100], loss: 0.003326, mae: 0.062257, mean_q: 0.290639
 33183/100000: episode: 521, duration: 0.123s, episode steps: 20, steps per second: 163, episode reward: 9.545, mean reward: 0.477 [0.380, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.358, 10.547], loss: 0.003165, mae: 0.060160, mean_q: 0.312549
 33202/100000: episode: 522, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 8.450, mean reward: 0.445 [0.358, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.488, 10.435], loss: 0.003566, mae: 0.065650, mean_q: 0.354799
 33222/100000: episode: 523, duration: 0.123s, episode steps: 20, steps per second: 162, episode reward: 8.068, mean reward: 0.403 [0.328, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.362, 10.480], loss: 0.003313, mae: 0.062133, mean_q: 0.345821
 33244/100000: episode: 524, duration: 0.164s, episode steps: 22, steps per second: 134, episode reward: 6.964, mean reward: 0.317 [0.221, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.324, 10.487], loss: 0.003736, mae: 0.065798, mean_q: 0.323233
 33257/100000: episode: 525, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 5.589, mean reward: 0.430 [0.410, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.634, 10.100], loss: 0.002864, mae: 0.059347, mean_q: 0.364073
 33277/100000: episode: 526, duration: 0.139s, episode steps: 20, steps per second: 144, episode reward: 6.933, mean reward: 0.347 [0.224, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.343], loss: 0.003414, mae: 0.063351, mean_q: 0.355021
 33284/100000: episode: 527, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 2.968, mean reward: 0.424 [0.330, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.388], loss: 0.003592, mae: 0.063809, mean_q: 0.426378
 33303/100000: episode: 528, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 7.189, mean reward: 0.378 [0.284, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.234, 10.100], loss: 0.004747, mae: 0.066640, mean_q: 0.349978
 33325/100000: episode: 529, duration: 0.194s, episode steps: 22, steps per second: 114, episode reward: 9.791, mean reward: 0.445 [0.371, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.783, 10.486], loss: 0.004307, mae: 0.068557, mean_q: 0.368150
 33347/100000: episode: 530, duration: 0.214s, episode steps: 22, steps per second: 103, episode reward: 8.999, mean reward: 0.409 [0.260, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.607, 10.356], loss: 0.003999, mae: 0.064760, mean_q: 0.341593
 33367/100000: episode: 531, duration: 0.226s, episode steps: 20, steps per second: 89, episode reward: 8.084, mean reward: 0.404 [0.270, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.057, 10.391], loss: 0.003821, mae: 0.063845, mean_q: 0.393770
 33386/100000: episode: 532, duration: 0.243s, episode steps: 19, steps per second: 78, episode reward: 8.211, mean reward: 0.432 [0.358, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.658, 10.100], loss: 0.003925, mae: 0.067097, mean_q: 0.388619
 33408/100000: episode: 533, duration: 0.331s, episode steps: 22, steps per second: 66, episode reward: 6.885, mean reward: 0.313 [0.189, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.765, 10.344], loss: 0.002950, mae: 0.058942, mean_q: 0.362895
 33415/100000: episode: 534, duration: 0.093s, episode steps: 7, steps per second: 76, episode reward: 3.024, mean reward: 0.432 [0.396, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.586], loss: 0.008552, mae: 0.078767, mean_q: 0.390322
 33434/100000: episode: 535, duration: 0.212s, episode steps: 19, steps per second: 90, episode reward: 7.975, mean reward: 0.420 [0.289, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.495], loss: 0.007364, mae: 0.078844, mean_q: 0.404814
 33456/100000: episode: 536, duration: 0.200s, episode steps: 22, steps per second: 110, episode reward: 8.970, mean reward: 0.408 [0.293, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.878, 10.557], loss: 0.005557, mae: 0.070210, mean_q: 0.416239
 33475/100000: episode: 537, duration: 0.148s, episode steps: 19, steps per second: 129, episode reward: 9.175, mean reward: 0.483 [0.423, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.521, 10.544], loss: 0.003825, mae: 0.065089, mean_q: 0.378938
 33482/100000: episode: 538, duration: 0.081s, episode steps: 7, steps per second: 86, episode reward: 3.626, mean reward: 0.518 [0.441, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.531], loss: 0.003137, mae: 0.061549, mean_q: 0.433630
 33495/100000: episode: 539, duration: 0.106s, episode steps: 13, steps per second: 122, episode reward: 5.324, mean reward: 0.410 [0.362, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.325, 10.100], loss: 0.004074, mae: 0.065357, mean_q: 0.337887
 33502/100000: episode: 540, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 3.158, mean reward: 0.451 [0.378, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.229, 10.559], loss: 0.002890, mae: 0.057471, mean_q: 0.444843
 33521/100000: episode: 541, duration: 0.136s, episode steps: 19, steps per second: 140, episode reward: 7.787, mean reward: 0.410 [0.315, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.260, 10.445], loss: 0.003573, mae: 0.063815, mean_q: 0.428665
 33554/100000: episode: 542, duration: 0.213s, episode steps: 33, steps per second: 155, episode reward: 14.656, mean reward: 0.444 [0.325, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.827, 10.472], loss: 0.003986, mae: 0.067918, mean_q: 0.446495
 33587/100000: episode: 543, duration: 0.185s, episode steps: 33, steps per second: 178, episode reward: 12.956, mean reward: 0.393 [0.174, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.137, 10.388], loss: 0.003473, mae: 0.062759, mean_q: 0.436966
 33609/100000: episode: 544, duration: 0.149s, episode steps: 22, steps per second: 147, episode reward: 8.194, mean reward: 0.372 [0.313, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.438], loss: 0.004004, mae: 0.066773, mean_q: 0.431275
 33629/100000: episode: 545, duration: 0.145s, episode steps: 20, steps per second: 138, episode reward: 8.496, mean reward: 0.425 [0.345, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.079, 10.350], loss: 0.003641, mae: 0.064303, mean_q: 0.398133
 33649/100000: episode: 546, duration: 0.146s, episode steps: 20, steps per second: 137, episode reward: 8.895, mean reward: 0.445 [0.201, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.606, 10.367], loss: 0.003757, mae: 0.064508, mean_q: 0.447714
 33671/100000: episode: 547, duration: 0.152s, episode steps: 22, steps per second: 144, episode reward: 8.046, mean reward: 0.366 [0.174, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.470, 10.332], loss: 0.003264, mae: 0.061882, mean_q: 0.402606
 33690/100000: episode: 548, duration: 0.117s, episode steps: 19, steps per second: 163, episode reward: 7.381, mean reward: 0.388 [0.317, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.284, 10.100], loss: 0.003508, mae: 0.063131, mean_q: 0.431165
 33706/100000: episode: 549, duration: 0.106s, episode steps: 16, steps per second: 150, episode reward: 6.551, mean reward: 0.409 [0.361, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.342, 10.100], loss: 0.002987, mae: 0.059290, mean_q: 0.401086
 33739/100000: episode: 550, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 16.023, mean reward: 0.486 [0.388, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.759, 10.597], loss: 0.003430, mae: 0.062347, mean_q: 0.433756
 33758/100000: episode: 551, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 7.433, mean reward: 0.391 [0.265, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.463, 10.100], loss: 0.003343, mae: 0.062264, mean_q: 0.454007
 33778/100000: episode: 552, duration: 0.128s, episode steps: 20, steps per second: 156, episode reward: 7.330, mean reward: 0.366 [0.301, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.154, 10.467], loss: 0.003784, mae: 0.064100, mean_q: 0.460245
 33797/100000: episode: 553, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 9.170, mean reward: 0.483 [0.413, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.430, 10.100], loss: 0.004975, mae: 0.067249, mean_q: 0.485067
 33819/100000: episode: 554, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 9.799, mean reward: 0.445 [0.270, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.451], loss: 0.003450, mae: 0.063694, mean_q: 0.432251
 33839/100000: episode: 555, duration: 0.102s, episode steps: 20, steps per second: 195, episode reward: 9.057, mean reward: 0.453 [0.359, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.350, 10.652], loss: 0.003722, mae: 0.063748, mean_q: 0.432115
 33858/100000: episode: 556, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 9.893, mean reward: 0.521 [0.444, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.301, 10.100], loss: 0.003700, mae: 0.066045, mean_q: 0.462239
 33880/100000: episode: 557, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 9.855, mean reward: 0.448 [0.327, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.449, 10.422], loss: 0.004366, mae: 0.071656, mean_q: 0.478750
 33893/100000: episode: 558, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 5.749, mean reward: 0.442 [0.317, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.359, 10.100], loss: 0.004240, mae: 0.069395, mean_q: 0.449674
[Info] 300-TH LEVEL FOUND: 0.9945720434188843, Considering 10/90 traces
 33900/100000: episode: 559, duration: 4.856s, episode steps: 7, steps per second: 1, episode reward: 3.444, mean reward: 0.492 [0.456, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.619], loss: 0.004317, mae: 0.069979, mean_q: 0.461184
 33917/100000: episode: 560, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 7.851, mean reward: 0.462 [0.373, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.570, 10.100], loss: 0.003218, mae: 0.061564, mean_q: 0.490715
 33943/100000: episode: 561, duration: 0.154s, episode steps: 26, steps per second: 168, episode reward: 12.648, mean reward: 0.486 [0.408, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.528], loss: 0.003500, mae: 0.063781, mean_q: 0.514008
 33957/100000: episode: 562, duration: 0.088s, episode steps: 14, steps per second: 160, episode reward: 7.052, mean reward: 0.504 [0.424, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.244, 10.100], loss: 0.003815, mae: 0.066218, mean_q: 0.512895
 33972/100000: episode: 563, duration: 0.101s, episode steps: 15, steps per second: 149, episode reward: 7.417, mean reward: 0.494 [0.435, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.583], loss: 0.003762, mae: 0.066277, mean_q: 0.562980
 33986/100000: episode: 564, duration: 0.103s, episode steps: 14, steps per second: 136, episode reward: 6.939, mean reward: 0.496 [0.414, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.509, 10.100], loss: 0.003214, mae: 0.061768, mean_q: 0.509687
 34000/100000: episode: 565, duration: 0.090s, episode steps: 14, steps per second: 156, episode reward: 7.134, mean reward: 0.510 [0.442, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.413, 10.100], loss: 0.003980, mae: 0.065541, mean_q: 0.486496
 34014/100000: episode: 566, duration: 0.099s, episode steps: 14, steps per second: 141, episode reward: 6.178, mean reward: 0.441 [0.355, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.263, 10.100], loss: 0.004742, mae: 0.066476, mean_q: 0.544928
 34040/100000: episode: 567, duration: 0.172s, episode steps: 26, steps per second: 152, episode reward: 11.976, mean reward: 0.461 [0.342, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.500], loss: 0.003293, mae: 0.062606, mean_q: 0.510056
 34055/100000: episode: 568, duration: 0.116s, episode steps: 15, steps per second: 129, episode reward: 7.802, mean reward: 0.520 [0.377, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.537], loss: 0.003569, mae: 0.063703, mean_q: 0.530511
 34071/100000: episode: 569, duration: 0.110s, episode steps: 16, steps per second: 145, episode reward: 8.657, mean reward: 0.541 [0.446, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.580, 10.674], loss: 0.003356, mae: 0.063976, mean_q: 0.575756
 34086/100000: episode: 570, duration: 0.098s, episode steps: 15, steps per second: 154, episode reward: 6.305, mean reward: 0.420 [0.358, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.491, 10.100], loss: 0.003451, mae: 0.062833, mean_q: 0.551238
 34101/100000: episode: 571, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 8.287, mean reward: 0.552 [0.495, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.585], loss: 0.003314, mae: 0.063506, mean_q: 0.530867
 34127/100000: episode: 572, duration: 0.162s, episode steps: 26, steps per second: 160, episode reward: 10.111, mean reward: 0.389 [0.171, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.311], loss: 0.003235, mae: 0.062016, mean_q: 0.552134
 34141/100000: episode: 573, duration: 0.101s, episode steps: 14, steps per second: 139, episode reward: 6.680, mean reward: 0.477 [0.424, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.395, 10.100], loss: 0.003491, mae: 0.062764, mean_q: 0.575101
 34157/100000: episode: 574, duration: 0.148s, episode steps: 16, steps per second: 108, episode reward: 7.389, mean reward: 0.462 [0.372, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.609, 10.550], loss: 0.003549, mae: 0.065952, mean_q: 0.541940
 34173/100000: episode: 575, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 7.537, mean reward: 0.471 [0.384, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.837, 10.470], loss: 0.003432, mae: 0.065095, mean_q: 0.563093
 34188/100000: episode: 576, duration: 0.099s, episode steps: 15, steps per second: 151, episode reward: 7.151, mean reward: 0.477 [0.403, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.371, 10.100], loss: 0.003759, mae: 0.067507, mean_q: 0.576773
 34203/100000: episode: 577, duration: 0.099s, episode steps: 15, steps per second: 152, episode reward: 6.374, mean reward: 0.425 [0.341, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.073, 10.100], loss: 0.004079, mae: 0.068170, mean_q: 0.567735
 34218/100000: episode: 578, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 7.926, mean reward: 0.528 [0.502, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.762, 10.606], loss: 0.003946, mae: 0.066295, mean_q: 0.551325
 34234/100000: episode: 579, duration: 0.166s, episode steps: 16, steps per second: 96, episode reward: 7.645, mean reward: 0.478 [0.404, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.122, 10.510], loss: 0.004099, mae: 0.068845, mean_q: 0.630846
 34248/100000: episode: 580, duration: 0.125s, episode steps: 14, steps per second: 112, episode reward: 6.888, mean reward: 0.492 [0.438, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.425, 10.100], loss: 0.003963, mae: 0.067324, mean_q: 0.555694
 34263/100000: episode: 581, duration: 0.203s, episode steps: 15, steps per second: 74, episode reward: 8.068, mean reward: 0.538 [0.436, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.342, 10.431], loss: 0.003694, mae: 0.067182, mean_q: 0.609284
 34280/100000: episode: 582, duration: 0.217s, episode steps: 17, steps per second: 78, episode reward: 9.526, mean reward: 0.560 [0.482, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.519, 10.100], loss: 0.002880, mae: 0.058189, mean_q: 0.595210
 34297/100000: episode: 583, duration: 0.136s, episode steps: 17, steps per second: 125, episode reward: 6.992, mean reward: 0.411 [0.317, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.264, 10.100], loss: 0.004185, mae: 0.067149, mean_q: 0.573843
 34314/100000: episode: 584, duration: 0.168s, episode steps: 17, steps per second: 101, episode reward: 7.975, mean reward: 0.469 [0.418, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.385, 10.100], loss: 0.004260, mae: 0.069796, mean_q: 0.562355
 34329/100000: episode: 585, duration: 0.106s, episode steps: 15, steps per second: 142, episode reward: 7.905, mean reward: 0.527 [0.439, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.406, 10.100], loss: 0.002915, mae: 0.058999, mean_q: 0.578185
 34354/100000: episode: 586, duration: 0.167s, episode steps: 25, steps per second: 150, episode reward: 8.656, mean reward: 0.346 [0.258, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.546, 10.421], loss: 0.003721, mae: 0.065997, mean_q: 0.584471
 34369/100000: episode: 587, duration: 0.094s, episode steps: 15, steps per second: 159, episode reward: 6.211, mean reward: 0.414 [0.311, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.381, 10.100], loss: 0.003647, mae: 0.063949, mean_q: 0.609656
 34386/100000: episode: 588, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 6.958, mean reward: 0.409 [0.318, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.237, 10.100], loss: 0.004007, mae: 0.067422, mean_q: 0.618742
 34401/100000: episode: 589, duration: 0.094s, episode steps: 15, steps per second: 159, episode reward: 7.081, mean reward: 0.472 [0.375, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.607, 10.100], loss: 0.003692, mae: 0.065599, mean_q: 0.634753
 34415/100000: episode: 590, duration: 0.100s, episode steps: 14, steps per second: 140, episode reward: 6.892, mean reward: 0.492 [0.455, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.609, 10.100], loss: 0.003602, mae: 0.065018, mean_q: 0.607680
 34429/100000: episode: 591, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 6.582, mean reward: 0.470 [0.374, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.430, 10.100], loss: 0.003487, mae: 0.063464, mean_q: 0.618251
 34445/100000: episode: 592, duration: 0.100s, episode steps: 16, steps per second: 161, episode reward: 6.160, mean reward: 0.385 [0.200, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.403], loss: 0.003782, mae: 0.065268, mean_q: 0.653623
 34460/100000: episode: 593, duration: 0.097s, episode steps: 15, steps per second: 155, episode reward: 7.151, mean reward: 0.477 [0.400, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.440, 10.100], loss: 0.003174, mae: 0.060175, mean_q: 0.627062
 34476/100000: episode: 594, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 7.131, mean reward: 0.446 [0.331, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.504], loss: 0.004674, mae: 0.065858, mean_q: 0.648436
 34491/100000: episode: 595, duration: 0.096s, episode steps: 15, steps per second: 155, episode reward: 7.433, mean reward: 0.496 [0.387, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.533, 10.527], loss: 0.003945, mae: 0.066855, mean_q: 0.625598
 34516/100000: episode: 596, duration: 0.145s, episode steps: 25, steps per second: 172, episode reward: 9.803, mean reward: 0.392 [0.185, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.455], loss: 0.003685, mae: 0.062741, mean_q: 0.659432
 34531/100000: episode: 597, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 7.632, mean reward: 0.509 [0.420, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.551], loss: 0.003697, mae: 0.064883, mean_q: 0.646236
 34556/100000: episode: 598, duration: 0.157s, episode steps: 25, steps per second: 160, episode reward: 9.755, mean reward: 0.390 [0.205, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.399], loss: 0.004036, mae: 0.068245, mean_q: 0.648108
 34572/100000: episode: 599, duration: 0.107s, episode steps: 16, steps per second: 150, episode reward: 8.524, mean reward: 0.533 [0.460, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.635], loss: 0.003563, mae: 0.063601, mean_q: 0.660001
 34586/100000: episode: 600, duration: 0.099s, episode steps: 14, steps per second: 141, episode reward: 6.025, mean reward: 0.430 [0.396, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.379, 10.100], loss: 0.003658, mae: 0.064656, mean_q: 0.674515
 34611/100000: episode: 601, duration: 0.172s, episode steps: 25, steps per second: 145, episode reward: 11.831, mean reward: 0.473 [0.331, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.285, 10.498], loss: 0.003477, mae: 0.062085, mean_q: 0.683884
 34626/100000: episode: 602, duration: 0.111s, episode steps: 15, steps per second: 135, episode reward: 7.330, mean reward: 0.489 [0.394, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.246, 10.478], loss: 0.004022, mae: 0.066974, mean_q: 0.687802
 34651/100000: episode: 603, duration: 0.155s, episode steps: 25, steps per second: 162, episode reward: 10.270, mean reward: 0.411 [0.302, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.358, 10.476], loss: 0.003707, mae: 0.066587, mean_q: 0.688956
 34667/100000: episode: 604, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 7.955, mean reward: 0.497 [0.370, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.215, 10.456], loss: 0.003722, mae: 0.066338, mean_q: 0.678162
 34683/100000: episode: 605, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 7.403, mean reward: 0.463 [0.245, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.266, 10.384], loss: 0.003532, mae: 0.063482, mean_q: 0.682839
 34698/100000: episode: 606, duration: 0.098s, episode steps: 15, steps per second: 153, episode reward: 7.231, mean reward: 0.482 [0.429, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.646, 10.100], loss: 0.003741, mae: 0.063136, mean_q: 0.685705
 34713/100000: episode: 607, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 6.651, mean reward: 0.443 [0.253, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.624, 10.359], loss: 0.003583, mae: 0.064687, mean_q: 0.714145
 34739/100000: episode: 608, duration: 0.132s, episode steps: 26, steps per second: 198, episode reward: 14.713, mean reward: 0.566 [0.389, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.319, 10.523], loss: 0.004617, mae: 0.073184, mean_q: 0.694738
 34756/100000: episode: 609, duration: 0.087s, episode steps: 17, steps per second: 197, episode reward: 5.767, mean reward: 0.339 [0.203, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.232, 10.100], loss: 0.004244, mae: 0.068731, mean_q: 0.705663
[Info] FALSIFICATION!
 34765/100000: episode: 610, duration: 0.064s, episode steps: 9, steps per second: 140, episode reward: 13.814, mean reward: 1.535 [0.412, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-2.012, 9.404], loss: 0.003695, mae: 0.065395, mean_q: 0.738510
 34865/100000: episode: 611, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.372, mean reward: -0.174 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.440, 10.098], loss: 0.017175, mae: 0.074607, mean_q: 0.702088
 34965/100000: episode: 612, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.352, mean reward: -0.184 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.461, 10.189], loss: 0.016746, mae: 0.070520, mean_q: 0.685319
 35065/100000: episode: 613, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: -17.742, mean reward: -0.177 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.456, 10.240], loss: 0.016891, mae: 0.072760, mean_q: 0.670001
 35165/100000: episode: 614, duration: 0.766s, episode steps: 100, steps per second: 131, episode reward: -17.926, mean reward: -0.179 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.097, 10.273], loss: 0.003577, mae: 0.063369, mean_q: 0.653741
 35265/100000: episode: 615, duration: 0.647s, episode steps: 100, steps per second: 154, episode reward: -16.890, mean reward: -0.169 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.959, 10.098], loss: 0.003802, mae: 0.065059, mean_q: 0.620226
 35365/100000: episode: 616, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: -16.631, mean reward: -0.166 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.206, 10.188], loss: 0.017188, mae: 0.074603, mean_q: 0.607199
 35465/100000: episode: 617, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: -17.177, mean reward: -0.172 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.835, 10.098], loss: 0.003535, mae: 0.062907, mean_q: 0.585148
 35565/100000: episode: 618, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: -12.569, mean reward: -0.126 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.643, 10.098], loss: 0.003477, mae: 0.062549, mean_q: 0.567409
 35665/100000: episode: 619, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.111, mean reward: -0.191 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.611, 10.229], loss: 0.029271, mae: 0.074066, mean_q: 0.546336
 35765/100000: episode: 620, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.531, mean reward: -0.195 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.984, 10.098], loss: 0.016460, mae: 0.067797, mean_q: 0.515433
 35865/100000: episode: 621, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.047, mean reward: -0.190 [-1.000, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.532, 10.098], loss: 0.030506, mae: 0.085475, mean_q: 0.528172
 35965/100000: episode: 622, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -11.391, mean reward: -0.114 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.279, 10.098], loss: 0.004474, mae: 0.067900, mean_q: 0.493573
 36065/100000: episode: 623, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -19.557, mean reward: -0.196 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.760, 10.098], loss: 0.003887, mae: 0.062973, mean_q: 0.477539
 36165/100000: episode: 624, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -12.344, mean reward: -0.123 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.929, 10.098], loss: 0.003437, mae: 0.061403, mean_q: 0.451276
 36265/100000: episode: 625, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -18.735, mean reward: -0.187 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.684, 10.224], loss: 0.003694, mae: 0.063351, mean_q: 0.445212
 36365/100000: episode: 626, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.066, mean reward: -0.171 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.979, 10.212], loss: 0.004477, mae: 0.066862, mean_q: 0.429082
 36465/100000: episode: 627, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -16.668, mean reward: -0.167 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.743, 10.175], loss: 0.003546, mae: 0.061398, mean_q: 0.389944
 36565/100000: episode: 628, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -3.437, mean reward: -0.034 [-1.000, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.038, 10.098], loss: 0.003397, mae: 0.060910, mean_q: 0.402846
 36665/100000: episode: 629, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -14.653, mean reward: -0.147 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.064, 10.368], loss: 0.017022, mae: 0.072366, mean_q: 0.365266
 36765/100000: episode: 630, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.881, mean reward: -0.169 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.904, 10.271], loss: 0.029692, mae: 0.074458, mean_q: 0.345408
 36865/100000: episode: 631, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.084, mean reward: -0.181 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.750, 10.116], loss: 0.042424, mae: 0.085571, mean_q: 0.330944
 36965/100000: episode: 632, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -4.938, mean reward: -0.049 [-1.000, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.806, 10.098], loss: 0.003561, mae: 0.062825, mean_q: 0.291787
 37065/100000: episode: 633, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.313, mean reward: -0.163 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.699, 10.368], loss: 0.003023, mae: 0.057125, mean_q: 0.269113
 37165/100000: episode: 634, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.417, mean reward: -0.154 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.380, 10.271], loss: 0.016826, mae: 0.068423, mean_q: 0.249204
 37265/100000: episode: 635, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: -20.024, mean reward: -0.200 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.368, 10.098], loss: 0.016767, mae: 0.071133, mean_q: 0.257365
 37365/100000: episode: 636, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -17.997, mean reward: -0.180 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.807, 10.098], loss: 0.003313, mae: 0.060363, mean_q: 0.184295
 37465/100000: episode: 637, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.369, mean reward: -0.184 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.030, 10.098], loss: 0.016088, mae: 0.063749, mean_q: 0.193743
 37565/100000: episode: 638, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -13.024, mean reward: -0.130 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.402, 10.392], loss: 0.017055, mae: 0.073461, mean_q: 0.172530
 37665/100000: episode: 639, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -17.450, mean reward: -0.175 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.274, 10.118], loss: 0.002981, mae: 0.056339, mean_q: 0.142423
 37765/100000: episode: 640, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.742, mean reward: -0.187 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.722, 10.098], loss: 0.003449, mae: 0.059746, mean_q: 0.112066
 37865/100000: episode: 641, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -12.544, mean reward: -0.125 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.463, 10.390], loss: 0.003267, mae: 0.058604, mean_q: 0.102063
 37965/100000: episode: 642, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.168, mean reward: -0.182 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.660, 10.098], loss: 0.015722, mae: 0.060787, mean_q: 0.099150
 38065/100000: episode: 643, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -19.368, mean reward: -0.194 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.090, 10.098], loss: 0.053401, mae: 0.080913, mean_q: 0.058285
 38165/100000: episode: 644, duration: 0.656s, episode steps: 100, steps per second: 152, episode reward: -17.490, mean reward: -0.175 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.565, 10.204], loss: 0.016211, mae: 0.069938, mean_q: 0.066294
 38265/100000: episode: 645, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -14.547, mean reward: -0.145 [-1.000, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.998, 10.105], loss: 0.016009, mae: 0.065087, mean_q: 0.044626
 38365/100000: episode: 646, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.792, mean reward: -0.168 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.092, 10.353], loss: 0.028381, mae: 0.071461, mean_q: 0.030038
 38465/100000: episode: 647, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.008, mean reward: -0.180 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.237, 10.410], loss: 0.002855, mae: 0.054649, mean_q: -0.021333
 38565/100000: episode: 648, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.114, mean reward: -0.161 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.816, 10.098], loss: 0.003098, mae: 0.057267, mean_q: -0.049708
 38665/100000: episode: 649, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -20.530, mean reward: -0.205 [-1.000, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.860, 10.123], loss: 0.015952, mae: 0.062695, mean_q: -0.030767
 38765/100000: episode: 650, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.412, mean reward: -0.174 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.539, 10.345], loss: 0.002984, mae: 0.057160, mean_q: -0.094460
 38865/100000: episode: 651, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.552, mean reward: -0.196 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.774, 10.098], loss: 0.002739, mae: 0.053173, mean_q: -0.141125
 38965/100000: episode: 652, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -13.476, mean reward: -0.135 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.777, 10.291], loss: 0.015433, mae: 0.057059, mean_q: -0.162895
 39065/100000: episode: 653, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -15.651, mean reward: -0.157 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.932, 10.098], loss: 0.041365, mae: 0.079262, mean_q: -0.148154
 39165/100000: episode: 654, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -20.833, mean reward: -0.208 [-1.000, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.404, 10.118], loss: 0.016639, mae: 0.069614, mean_q: -0.165333
 39265/100000: episode: 655, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.504, mean reward: -0.185 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.586, 10.098], loss: 0.004274, mae: 0.062852, mean_q: -0.172945
 39365/100000: episode: 656, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.384, mean reward: -0.184 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.394, 10.098], loss: 0.053822, mae: 0.084438, mean_q: -0.207118
 39465/100000: episode: 657, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.509, mean reward: -0.145 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.977, 10.362], loss: 0.027993, mae: 0.069415, mean_q: -0.220008
 39565/100000: episode: 658, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.204, mean reward: -0.192 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.359, 10.247], loss: 0.002740, mae: 0.053722, mean_q: -0.272594
 39665/100000: episode: 659, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.251, mean reward: -0.193 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.368, 10.098], loss: 0.015339, mae: 0.059910, mean_q: -0.284062
 39765/100000: episode: 660, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -14.160, mean reward: -0.142 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.328, 10.098], loss: 0.002808, mae: 0.053007, mean_q: -0.313131
 39865/100000: episode: 661, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -11.787, mean reward: -0.118 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.903, 10.098], loss: 0.002551, mae: 0.051154, mean_q: -0.321864
 39965/100000: episode: 662, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -14.166, mean reward: -0.142 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.818, 10.098], loss: 0.002647, mae: 0.050992, mean_q: -0.276635
 40065/100000: episode: 663, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -8.242, mean reward: -0.082 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.139, 10.098], loss: 0.002782, mae: 0.051877, mean_q: -0.307232
 40165/100000: episode: 664, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.233, mean reward: -0.172 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.474, 10.366], loss: 0.002784, mae: 0.053335, mean_q: -0.282612
 40265/100000: episode: 665, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.850, mean reward: -0.188 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.580, 10.098], loss: 0.002488, mae: 0.049739, mean_q: -0.308311
 40365/100000: episode: 666, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.559, mean reward: -0.186 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.388, 10.102], loss: 0.002602, mae: 0.050952, mean_q: -0.357284
 40465/100000: episode: 667, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.594, mean reward: -0.176 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.811, 10.122], loss: 0.002640, mae: 0.051515, mean_q: -0.313703
 40565/100000: episode: 668, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -19.596, mean reward: -0.196 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.428, 10.242], loss: 0.002639, mae: 0.052059, mean_q: -0.279286
 40665/100000: episode: 669, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.249, mean reward: -0.172 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.936, 10.098], loss: 0.002793, mae: 0.054320, mean_q: -0.297811
 40765/100000: episode: 670, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.647, mean reward: -0.176 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.729, 10.098], loss: 0.002626, mae: 0.052165, mean_q: -0.286106
 40865/100000: episode: 671, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -12.405, mean reward: -0.124 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.794, 10.098], loss: 0.002776, mae: 0.054539, mean_q: -0.315595
 40965/100000: episode: 672, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: -20.710, mean reward: -0.207 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.448, 10.219], loss: 0.002510, mae: 0.050186, mean_q: -0.320379
 41065/100000: episode: 673, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.035, mean reward: -0.180 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.477, 10.194], loss: 0.002546, mae: 0.050588, mean_q: -0.330943
 41165/100000: episode: 674, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.659, mean reward: -0.197 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.830, 10.188], loss: 0.003065, mae: 0.056717, mean_q: -0.315275
 41265/100000: episode: 675, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.487, mean reward: -0.185 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.079, 10.098], loss: 0.002454, mae: 0.050474, mean_q: -0.314420
 41365/100000: episode: 676, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.237, mean reward: -0.182 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.449, 10.098], loss: 0.002570, mae: 0.051379, mean_q: -0.311927
 41465/100000: episode: 677, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -16.824, mean reward: -0.168 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.668, 10.100], loss: 0.004300, mae: 0.065679, mean_q: -0.303517
 41565/100000: episode: 678, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.985, mean reward: -0.190 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.013, 10.124], loss: 0.002496, mae: 0.051289, mean_q: -0.325442
 41665/100000: episode: 679, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -10.223, mean reward: -0.102 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.473, 10.098], loss: 0.002467, mae: 0.050767, mean_q: -0.301967
 41765/100000: episode: 680, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.308, mean reward: -0.153 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.749, 10.098], loss: 0.002537, mae: 0.050854, mean_q: -0.322051
 41865/100000: episode: 681, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -12.695, mean reward: -0.127 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.605, 10.144], loss: 0.002470, mae: 0.049835, mean_q: -0.324088
 41965/100000: episode: 682, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -18.845, mean reward: -0.188 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.838, 10.138], loss: 0.002549, mae: 0.051347, mean_q: -0.321633
 42065/100000: episode: 683, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -19.257, mean reward: -0.193 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.406, 10.206], loss: 0.002433, mae: 0.049815, mean_q: -0.331397
 42165/100000: episode: 684, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.156, mean reward: -0.172 [-1.000, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.789, 10.251], loss: 0.002537, mae: 0.051121, mean_q: -0.335223
 42265/100000: episode: 685, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.828, mean reward: -0.178 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.234, 10.213], loss: 0.002374, mae: 0.049364, mean_q: -0.316737
 42365/100000: episode: 686, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.669, mean reward: -0.187 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.792, 10.166], loss: 0.002343, mae: 0.049382, mean_q: -0.344553
 42465/100000: episode: 687, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -12.862, mean reward: -0.129 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.418, 10.407], loss: 0.002677, mae: 0.052533, mean_q: -0.299405
 42565/100000: episode: 688, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.975, mean reward: -0.170 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.980, 10.272], loss: 0.002655, mae: 0.051268, mean_q: -0.349698
 42665/100000: episode: 689, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.592, mean reward: -0.166 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.006, 10.098], loss: 0.002575, mae: 0.051235, mean_q: -0.288044
 42765/100000: episode: 690, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -19.729, mean reward: -0.197 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.744, 10.226], loss: 0.002341, mae: 0.048405, mean_q: -0.299603
 42865/100000: episode: 691, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -15.927, mean reward: -0.159 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.696, 10.098], loss: 0.002530, mae: 0.050776, mean_q: -0.353254
 42965/100000: episode: 692, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.618, mean reward: -0.166 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.926, 10.205], loss: 0.003574, mae: 0.058303, mean_q: -0.315677
 43065/100000: episode: 693, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.946, mean reward: -0.149 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.776, 10.098], loss: 0.002574, mae: 0.051195, mean_q: -0.305617
 43165/100000: episode: 694, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.549, mean reward: -0.175 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.534, 10.352], loss: 0.002375, mae: 0.048746, mean_q: -0.358713
 43265/100000: episode: 695, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -13.506, mean reward: -0.135 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.451, 10.232], loss: 0.002509, mae: 0.049917, mean_q: -0.327738
 43365/100000: episode: 696, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.886, mean reward: -0.159 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.979, 10.184], loss: 0.002657, mae: 0.051505, mean_q: -0.319860
 43465/100000: episode: 697, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.260, mean reward: -0.183 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.428, 10.131], loss: 0.002554, mae: 0.050878, mean_q: -0.338728
 43565/100000: episode: 698, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.375, mean reward: -0.184 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.536, 10.185], loss: 0.002428, mae: 0.049426, mean_q: -0.292066
 43665/100000: episode: 699, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -10.139, mean reward: -0.101 [-1.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.683, 10.098], loss: 0.002707, mae: 0.053049, mean_q: -0.317679
 43765/100000: episode: 700, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -13.530, mean reward: -0.135 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.020, 10.098], loss: 0.002456, mae: 0.049382, mean_q: -0.315189
 43865/100000: episode: 701, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.937, mean reward: -0.189 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.735, 10.115], loss: 0.002577, mae: 0.050994, mean_q: -0.301768
 43965/100000: episode: 702, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.062, mean reward: -0.191 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.485, 10.098], loss: 0.002612, mae: 0.050807, mean_q: -0.330516
 44065/100000: episode: 703, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.803, mean reward: -0.168 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.452, 10.237], loss: 0.003446, mae: 0.056108, mean_q: -0.320221
 44165/100000: episode: 704, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.480, mean reward: -0.155 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.150, 10.098], loss: 0.002463, mae: 0.049386, mean_q: -0.295760
 44265/100000: episode: 705, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.934, mean reward: -0.169 [-1.000, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.439, 10.198], loss: 0.002521, mae: 0.049892, mean_q: -0.325844
 44365/100000: episode: 706, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -20.865, mean reward: -0.209 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.889, 10.098], loss: 0.002771, mae: 0.051974, mean_q: -0.326728
 44465/100000: episode: 707, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.397, mean reward: -0.194 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.652, 10.098], loss: 0.002593, mae: 0.050213, mean_q: -0.335389
 44565/100000: episode: 708, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.271, mean reward: -0.193 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.674, 10.196], loss: 0.002420, mae: 0.048287, mean_q: -0.319598
 44665/100000: episode: 709, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -19.163, mean reward: -0.192 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.712, 10.123], loss: 0.002353, mae: 0.047880, mean_q: -0.302510
[Info] 100-TH LEVEL FOUND: 0.6568360328674316, Considering 10/90 traces
 44765/100000: episode: 710, duration: 4.488s, episode steps: 100, steps per second: 22, episode reward: -15.001, mean reward: -0.150 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.633, 10.139], loss: 0.002576, mae: 0.050340, mean_q: -0.329234
 44805/100000: episode: 711, duration: 0.234s, episode steps: 40, steps per second: 171, episode reward: 15.010, mean reward: 0.375 [0.217, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.961, 10.100], loss: 0.002464, mae: 0.049693, mean_q: -0.331508
 44853/100000: episode: 712, duration: 0.286s, episode steps: 48, steps per second: 168, episode reward: 9.290, mean reward: 0.194 [0.014, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.799, 10.100], loss: 0.002629, mae: 0.053366, mean_q: -0.297044
 44890/100000: episode: 713, duration: 0.211s, episode steps: 37, steps per second: 176, episode reward: 8.185, mean reward: 0.221 [0.030, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.712, 10.280], loss: 0.002393, mae: 0.050966, mean_q: -0.321271
 44927/100000: episode: 714, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 8.580, mean reward: 0.232 [0.022, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.035, 10.295], loss: 0.002503, mae: 0.051131, mean_q: -0.291350
 44949/100000: episode: 715, duration: 0.105s, episode steps: 22, steps per second: 210, episode reward: 7.723, mean reward: 0.351 [0.271, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.186, 10.437], loss: 0.003153, mae: 0.055752, mean_q: -0.305156
 44989/100000: episode: 716, duration: 0.212s, episode steps: 40, steps per second: 189, episode reward: 15.303, mean reward: 0.383 [0.211, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-1.287, 10.100], loss: 0.002530, mae: 0.049632, mean_q: -0.346511
 45004/100000: episode: 717, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 5.757, mean reward: 0.384 [0.272, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.517], loss: 0.002386, mae: 0.050488, mean_q: -0.259780
 45049/100000: episode: 718, duration: 0.247s, episode steps: 45, steps per second: 182, episode reward: 10.013, mean reward: 0.223 [0.060, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.580, 10.100], loss: 0.002540, mae: 0.050894, mean_q: -0.205716
 45097/100000: episode: 719, duration: 0.269s, episode steps: 48, steps per second: 178, episode reward: 12.407, mean reward: 0.258 [0.114, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.449, 10.100], loss: 0.002662, mae: 0.054802, mean_q: -0.204688
 45126/100000: episode: 720, duration: 0.160s, episode steps: 29, steps per second: 182, episode reward: 9.078, mean reward: 0.313 [0.203, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.994, 10.100], loss: 0.002709, mae: 0.053898, mean_q: -0.258902
 45166/100000: episode: 721, duration: 0.211s, episode steps: 40, steps per second: 189, episode reward: 10.007, mean reward: 0.250 [0.130, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.729, 10.100], loss: 0.004475, mae: 0.066037, mean_q: -0.264287
 45206/100000: episode: 722, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 15.850, mean reward: 0.396 [0.193, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-1.796, 10.100], loss: 0.002982, mae: 0.057127, mean_q: -0.203970
 45246/100000: episode: 723, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 12.364, mean reward: 0.309 [0.209, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.574, 10.100], loss: 0.002820, mae: 0.054838, mean_q: -0.273309
 45272/100000: episode: 724, duration: 0.134s, episode steps: 26, steps per second: 193, episode reward: 8.601, mean reward: 0.331 [0.136, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.812, 10.100], loss: 0.002486, mae: 0.051356, mean_q: -0.244066
 45312/100000: episode: 725, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 11.716, mean reward: 0.293 [0.167, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.162, 10.100], loss: 0.002319, mae: 0.048969, mean_q: -0.259264
 45327/100000: episode: 726, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 5.476, mean reward: 0.365 [0.267, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.060, 10.455], loss: 0.002612, mae: 0.054013, mean_q: -0.278780
 45375/100000: episode: 727, duration: 0.260s, episode steps: 48, steps per second: 184, episode reward: 16.337, mean reward: 0.340 [0.127, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.905, 10.100], loss: 0.002676, mae: 0.052977, mean_q: -0.197789
 45420/100000: episode: 728, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 13.212, mean reward: 0.294 [0.096, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.682, 10.100], loss: 0.002878, mae: 0.055051, mean_q: -0.186890
 45446/100000: episode: 729, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 6.201, mean reward: 0.239 [0.068, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.487, 10.100], loss: 0.002364, mae: 0.049848, mean_q: -0.182739
 45486/100000: episode: 730, duration: 0.198s, episode steps: 40, steps per second: 202, episode reward: 13.425, mean reward: 0.336 [0.240, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-1.006, 10.100], loss: 0.002559, mae: 0.051719, mean_q: -0.194692
 45515/100000: episode: 731, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 11.784, mean reward: 0.406 [0.232, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.593, 10.100], loss: 0.002742, mae: 0.055833, mean_q: -0.142696
 45560/100000: episode: 732, duration: 0.244s, episode steps: 45, steps per second: 184, episode reward: 16.558, mean reward: 0.368 [0.177, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.348, 10.100], loss: 0.002570, mae: 0.052718, mean_q: -0.168262
 45586/100000: episode: 733, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 6.924, mean reward: 0.266 [0.025, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.258, 10.100], loss: 0.002260, mae: 0.047743, mean_q: -0.190430
 45626/100000: episode: 734, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 13.722, mean reward: 0.343 [0.230, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.500, 10.100], loss: 0.002854, mae: 0.053685, mean_q: -0.168381
 45666/100000: episode: 735, duration: 0.227s, episode steps: 40, steps per second: 176, episode reward: 12.318, mean reward: 0.308 [0.091, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.673, 10.100], loss: 0.002853, mae: 0.055225, mean_q: -0.111757
 45710/100000: episode: 736, duration: 0.261s, episode steps: 44, steps per second: 169, episode reward: 15.253, mean reward: 0.347 [0.164, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.294, 10.323], loss: 0.002520, mae: 0.050776, mean_q: -0.163574
 45739/100000: episode: 737, duration: 0.176s, episode steps: 29, steps per second: 165, episode reward: 10.420, mean reward: 0.359 [0.233, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.923, 10.100], loss: 0.002983, mae: 0.055395, mean_q: -0.123494
 45784/100000: episode: 738, duration: 0.274s, episode steps: 45, steps per second: 164, episode reward: 17.025, mean reward: 0.378 [0.099, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.689, 10.100], loss: 0.002798, mae: 0.053675, mean_q: -0.141342
 45810/100000: episode: 739, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 11.789, mean reward: 0.453 [0.373, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.337, 10.100], loss: 0.003102, mae: 0.057359, mean_q: -0.093634
 45854/100000: episode: 740, duration: 0.236s, episode steps: 44, steps per second: 186, episode reward: 16.108, mean reward: 0.366 [0.206, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.695, 10.503], loss: 0.002888, mae: 0.055081, mean_q: -0.056690
 45880/100000: episode: 741, duration: 0.139s, episode steps: 26, steps per second: 186, episode reward: 8.439, mean reward: 0.325 [0.206, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.602, 10.100], loss: 0.003626, mae: 0.061962, mean_q: -0.128891
 45920/100000: episode: 742, duration: 0.222s, episode steps: 40, steps per second: 180, episode reward: 13.788, mean reward: 0.345 [0.250, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.803, 10.100], loss: 0.004975, mae: 0.070827, mean_q: -0.076180
 45949/100000: episode: 743, duration: 0.144s, episode steps: 29, steps per second: 201, episode reward: 8.396, mean reward: 0.290 [0.098, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.047, 10.100], loss: 0.004025, mae: 0.061858, mean_q: -0.090945
 45989/100000: episode: 744, duration: 0.210s, episode steps: 40, steps per second: 191, episode reward: 13.176, mean reward: 0.329 [0.191, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.171, 10.100], loss: 0.003800, mae: 0.062818, mean_q: -0.061877
 46029/100000: episode: 745, duration: 0.207s, episode steps: 40, steps per second: 193, episode reward: 10.629, mean reward: 0.266 [0.074, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.265, 10.100], loss: 0.002828, mae: 0.055940, mean_q: -0.114443
 46073/100000: episode: 746, duration: 0.235s, episode steps: 44, steps per second: 188, episode reward: 14.351, mean reward: 0.326 [0.212, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.501, 10.357], loss: 0.002754, mae: 0.054329, mean_q: -0.014249
 46118/100000: episode: 747, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: 8.681, mean reward: 0.193 [0.022, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.058, 10.100], loss: 0.002759, mae: 0.054847, mean_q: -0.059882
 46140/100000: episode: 748, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 7.585, mean reward: 0.345 [0.266, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.396], loss: 0.003383, mae: 0.059841, mean_q: -0.010460
 46166/100000: episode: 749, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 9.263, mean reward: 0.356 [0.270, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.200, 10.100], loss: 0.002836, mae: 0.054634, mean_q: -0.044517
 46203/100000: episode: 750, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 13.217, mean reward: 0.357 [0.158, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.286, 10.100], loss: 0.002687, mae: 0.053944, mean_q: -0.047497
 46248/100000: episode: 751, duration: 0.242s, episode steps: 45, steps per second: 186, episode reward: 13.770, mean reward: 0.306 [0.112, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.490, 10.100], loss: 0.002524, mae: 0.051319, mean_q: -0.033839
 46277/100000: episode: 752, duration: 0.156s, episode steps: 29, steps per second: 185, episode reward: 10.869, mean reward: 0.375 [0.202, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.499, 10.100], loss: 0.003248, mae: 0.058122, mean_q: -0.015402
 46317/100000: episode: 753, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 13.958, mean reward: 0.349 [0.135, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.966, 10.100], loss: 0.002492, mae: 0.051972, mean_q: -0.061184
 46354/100000: episode: 754, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 9.701, mean reward: 0.262 [0.042, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.045, 10.100], loss: 0.002768, mae: 0.055121, mean_q: 0.028108
 46394/100000: episode: 755, duration: 0.225s, episode steps: 40, steps per second: 178, episode reward: 11.249, mean reward: 0.281 [0.114, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.337, 10.100], loss: 0.002982, mae: 0.056717, mean_q: 0.041013
 46420/100000: episode: 756, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 7.314, mean reward: 0.281 [0.112, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.091, 10.100], loss: 0.003082, mae: 0.057554, mean_q: -0.010197
 46435/100000: episode: 757, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 4.946, mean reward: 0.330 [0.255, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.291, 10.568], loss: 0.002612, mae: 0.052266, mean_q: -0.008862
 46472/100000: episode: 758, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 10.589, mean reward: 0.286 [0.176, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.344, 10.100], loss: 0.002624, mae: 0.053293, mean_q: 0.007483
 46498/100000: episode: 759, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 8.919, mean reward: 0.343 [0.209, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.476, 10.100], loss: 0.002719, mae: 0.054300, mean_q: -0.046020
 46546/100000: episode: 760, duration: 0.267s, episode steps: 48, steps per second: 180, episode reward: 15.368, mean reward: 0.320 [0.180, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.906, 10.100], loss: 0.002760, mae: 0.055758, mean_q: 0.057506
 46583/100000: episode: 761, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 7.892, mean reward: 0.213 [0.015, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.525, 10.112], loss: 0.002963, mae: 0.056626, mean_q: 0.027115
 46628/100000: episode: 762, duration: 0.267s, episode steps: 45, steps per second: 168, episode reward: 8.399, mean reward: 0.187 [0.034, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.213, 10.101], loss: 0.003082, mae: 0.057371, mean_q: 0.043021
 46643/100000: episode: 763, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 4.858, mean reward: 0.324 [0.156, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.407], loss: 0.002647, mae: 0.054516, mean_q: 0.033679
 46658/100000: episode: 764, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 5.698, mean reward: 0.380 [0.313, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.444], loss: 0.003066, mae: 0.056931, mean_q: 0.030321
 46698/100000: episode: 765, duration: 0.243s, episode steps: 40, steps per second: 164, episode reward: 9.888, mean reward: 0.247 [0.100, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.323, 10.100], loss: 0.002941, mae: 0.057473, mean_q: 0.066926
 46720/100000: episode: 766, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 7.919, mean reward: 0.360 [0.285, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.891, 10.441], loss: 0.002888, mae: 0.057109, mean_q: 0.077730
 46765/100000: episode: 767, duration: 0.222s, episode steps: 45, steps per second: 203, episode reward: 17.297, mean reward: 0.384 [0.187, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.716, 10.100], loss: 0.002809, mae: 0.055110, mean_q: 0.033182
 46805/100000: episode: 768, duration: 0.215s, episode steps: 40, steps per second: 186, episode reward: 12.545, mean reward: 0.314 [0.136, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.487, 10.100], loss: 0.002892, mae: 0.055975, mean_q: 0.048740
 46827/100000: episode: 769, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 9.747, mean reward: 0.443 [0.356, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-1.608, 10.465], loss: 0.003058, mae: 0.055289, mean_q: 0.037283
 46867/100000: episode: 770, duration: 0.226s, episode steps: 40, steps per second: 177, episode reward: 15.498, mean reward: 0.387 [0.227, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-1.054, 10.100], loss: 0.002678, mae: 0.054777, mean_q: 0.081781
 46911/100000: episode: 771, duration: 0.231s, episode steps: 44, steps per second: 191, episode reward: 10.930, mean reward: 0.248 [0.037, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-2.005, 10.227], loss: 0.002782, mae: 0.054182, mean_q: 0.104369
 46933/100000: episode: 772, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 6.084, mean reward: 0.277 [0.227, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.274], loss: 0.003160, mae: 0.059334, mean_q: 0.099372
 46948/100000: episode: 773, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 4.550, mean reward: 0.303 [0.213, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.311, 10.364], loss: 0.003247, mae: 0.060238, mean_q: 0.101284
 46993/100000: episode: 774, duration: 0.230s, episode steps: 45, steps per second: 196, episode reward: 13.028, mean reward: 0.290 [0.124, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.236, 10.100], loss: 0.002795, mae: 0.056446, mean_q: 0.088173
 47015/100000: episode: 775, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 7.683, mean reward: 0.349 [0.194, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.265, 10.341], loss: 0.003068, mae: 0.058315, mean_q: 0.156362
 47059/100000: episode: 776, duration: 0.222s, episode steps: 44, steps per second: 198, episode reward: 14.738, mean reward: 0.335 [0.179, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.668, 10.504], loss: 0.003099, mae: 0.059501, mean_q: 0.083188
 47088/100000: episode: 777, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 9.766, mean reward: 0.337 [0.205, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.723, 10.100], loss: 0.003217, mae: 0.059843, mean_q: 0.134027
 47132/100000: episode: 778, duration: 0.231s, episode steps: 44, steps per second: 191, episode reward: 14.839, mean reward: 0.337 [0.177, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-1.358, 10.373], loss: 0.003821, mae: 0.063585, mean_q: 0.141497
 47172/100000: episode: 779, duration: 0.193s, episode steps: 40, steps per second: 207, episode reward: 9.554, mean reward: 0.239 [0.110, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.459, 10.100], loss: 0.002861, mae: 0.057728, mean_q: 0.167025
 47201/100000: episode: 780, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 8.004, mean reward: 0.276 [0.080, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.029, 10.100], loss: 0.003077, mae: 0.058353, mean_q: 0.127198
 47249/100000: episode: 781, duration: 0.238s, episode steps: 48, steps per second: 201, episode reward: 14.622, mean reward: 0.305 [0.170, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.423, 10.100], loss: 0.002779, mae: 0.055174, mean_q: 0.136457
 47297/100000: episode: 782, duration: 0.232s, episode steps: 48, steps per second: 207, episode reward: 16.079, mean reward: 0.335 [0.156, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.796, 10.100], loss: 0.002918, mae: 0.056225, mean_q: 0.134872
 47312/100000: episode: 783, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 5.476, mean reward: 0.365 [0.302, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.362, 10.515], loss: 0.002563, mae: 0.052857, mean_q: 0.107098
 47360/100000: episode: 784, duration: 0.259s, episode steps: 48, steps per second: 186, episode reward: 9.386, mean reward: 0.196 [0.020, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.656, 10.156], loss: 0.002962, mae: 0.058091, mean_q: 0.173003
 47408/100000: episode: 785, duration: 0.250s, episode steps: 48, steps per second: 192, episode reward: 10.767, mean reward: 0.224 [0.016, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.130, 10.153], loss: 0.002820, mae: 0.057287, mean_q: 0.151893
 47448/100000: episode: 786, duration: 0.242s, episode steps: 40, steps per second: 165, episode reward: 8.600, mean reward: 0.215 [0.017, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.627, 10.131], loss: 0.002718, mae: 0.055769, mean_q: 0.196547
 47474/100000: episode: 787, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 10.403, mean reward: 0.400 [0.280, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.401, 10.100], loss: 0.002912, mae: 0.057302, mean_q: 0.201909
 47496/100000: episode: 788, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 9.702, mean reward: 0.441 [0.385, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.282, 10.456], loss: 0.002673, mae: 0.056933, mean_q: 0.185528
 47522/100000: episode: 789, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 8.112, mean reward: 0.312 [0.167, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.196, 10.100], loss: 0.003172, mae: 0.059092, mean_q: 0.208198
 47562/100000: episode: 790, duration: 0.267s, episode steps: 40, steps per second: 150, episode reward: 10.677, mean reward: 0.267 [0.031, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.145, 10.225], loss: 0.003060, mae: 0.058458, mean_q: 0.207997
 47588/100000: episode: 791, duration: 0.166s, episode steps: 26, steps per second: 156, episode reward: 10.505, mean reward: 0.404 [0.296, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.902, 10.100], loss: 0.003079, mae: 0.059328, mean_q: 0.200165
 47633/100000: episode: 792, duration: 0.303s, episode steps: 45, steps per second: 148, episode reward: 9.226, mean reward: 0.205 [0.043, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.703, 10.100], loss: 0.003291, mae: 0.060132, mean_q: 0.188572
 47673/100000: episode: 793, duration: 0.232s, episode steps: 40, steps per second: 172, episode reward: 8.814, mean reward: 0.220 [0.025, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-1.034, 10.294], loss: 0.003225, mae: 0.059638, mean_q: 0.186522
 47717/100000: episode: 794, duration: 0.253s, episode steps: 44, steps per second: 174, episode reward: 11.611, mean reward: 0.264 [0.016, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.562, 10.132], loss: 0.002972, mae: 0.058513, mean_q: 0.252931
 47732/100000: episode: 795, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 3.879, mean reward: 0.259 [0.068, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.933, 10.228], loss: 0.003394, mae: 0.062920, mean_q: 0.226151
 47780/100000: episode: 796, duration: 0.247s, episode steps: 48, steps per second: 195, episode reward: 8.189, mean reward: 0.171 [0.044, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.445, 10.100], loss: 0.002615, mae: 0.054468, mean_q: 0.224560
 47795/100000: episode: 797, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 5.039, mean reward: 0.336 [0.281, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.730, 10.461], loss: 0.002323, mae: 0.052391, mean_q: 0.233543
 47810/100000: episode: 798, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 3.286, mean reward: 0.219 [0.126, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.111, 10.234], loss: 0.002834, mae: 0.057262, mean_q: 0.240764
 47855/100000: episode: 799, duration: 0.243s, episode steps: 45, steps per second: 185, episode reward: 13.181, mean reward: 0.293 [0.156, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.844, 10.100], loss: 0.002770, mae: 0.057127, mean_q: 0.265837
[Info] 200-TH LEVEL FOUND: 0.877755343914032, Considering 10/90 traces
 47884/100000: episode: 800, duration: 4.296s, episode steps: 29, steps per second: 7, episode reward: 10.147, mean reward: 0.350 [0.240, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-1.610, 10.100], loss: 0.002951, mae: 0.056871, mean_q: 0.239410
 47913/100000: episode: 801, duration: 0.169s, episode steps: 29, steps per second: 171, episode reward: 12.747, mean reward: 0.440 [0.358, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.545], loss: 0.003416, mae: 0.062171, mean_q: 0.247278
 47946/100000: episode: 802, duration: 0.191s, episode steps: 33, steps per second: 173, episode reward: 9.752, mean reward: 0.296 [0.053, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.550, 10.100], loss: 0.003039, mae: 0.058375, mean_q: 0.263007
 47975/100000: episode: 803, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 8.058, mean reward: 0.278 [0.138, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.792, 10.100], loss: 0.002690, mae: 0.055064, mean_q: 0.220801
 48009/100000: episode: 804, duration: 0.214s, episode steps: 34, steps per second: 159, episode reward: 12.290, mean reward: 0.361 [0.259, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.750, 10.449], loss: 0.002655, mae: 0.055209, mean_q: 0.263969
 48024/100000: episode: 805, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 6.507, mean reward: 0.434 [0.365, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.601, 10.580], loss: 0.002819, mae: 0.056483, mean_q: 0.297370
 48057/100000: episode: 806, duration: 0.194s, episode steps: 33, steps per second: 170, episode reward: 16.597, mean reward: 0.503 [0.322, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.453, 10.100], loss: 0.002732, mae: 0.056079, mean_q: 0.283081
 48090/100000: episode: 807, duration: 0.209s, episode steps: 33, steps per second: 158, episode reward: 16.139, mean reward: 0.489 [0.405, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.744, 10.100], loss: 0.003209, mae: 0.062926, mean_q: 0.308474
 48123/100000: episode: 808, duration: 0.188s, episode steps: 33, steps per second: 176, episode reward: 15.294, mean reward: 0.463 [0.347, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-1.048, 10.100], loss: 0.002744, mae: 0.055534, mean_q: 0.310680
 48138/100000: episode: 809, duration: 0.092s, episode steps: 15, steps per second: 164, episode reward: 4.692, mean reward: 0.313 [0.203, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.364], loss: 0.002904, mae: 0.054979, mean_q: 0.291019
 48153/100000: episode: 810, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 5.411, mean reward: 0.361 [0.279, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.073, 10.351], loss: 0.002769, mae: 0.056671, mean_q: 0.314430
 48168/100000: episode: 811, duration: 0.073s, episode steps: 15, steps per second: 206, episode reward: 7.516, mean reward: 0.501 [0.431, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.696, 10.625], loss: 0.002767, mae: 0.056766, mean_q: 0.319250
 48201/100000: episode: 812, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 15.080, mean reward: 0.457 [0.348, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.866, 10.100], loss: 0.002843, mae: 0.058130, mean_q: 0.347383
 48229/100000: episode: 813, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 11.150, mean reward: 0.398 [0.284, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.293, 10.100], loss: 0.003017, mae: 0.058880, mean_q: 0.286405
 48244/100000: episode: 814, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 6.598, mean reward: 0.440 [0.363, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.516], loss: 0.002811, mae: 0.057552, mean_q: 0.339453
 48277/100000: episode: 815, duration: 0.184s, episode steps: 33, steps per second: 179, episode reward: 8.736, mean reward: 0.265 [0.016, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.035, 10.221], loss: 0.002846, mae: 0.057158, mean_q: 0.303389
 48306/100000: episode: 816, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 10.464, mean reward: 0.361 [0.268, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.035, 10.441], loss: 0.002898, mae: 0.058876, mean_q: 0.358719
 48340/100000: episode: 817, duration: 0.199s, episode steps: 34, steps per second: 171, episode reward: 13.221, mean reward: 0.389 [0.297, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-1.323, 10.100], loss: 0.002779, mae: 0.057347, mean_q: 0.378385
 48374/100000: episode: 818, duration: 0.205s, episode steps: 34, steps per second: 165, episode reward: 13.927, mean reward: 0.410 [0.146, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.262, 10.244], loss: 0.003166, mae: 0.059242, mean_q: 0.357993
 48408/100000: episode: 819, duration: 0.208s, episode steps: 34, steps per second: 163, episode reward: 14.312, mean reward: 0.421 [0.333, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.989, 10.560], loss: 0.003026, mae: 0.059360, mean_q: 0.374843
 48423/100000: episode: 820, duration: 0.150s, episode steps: 15, steps per second: 100, episode reward: 7.427, mean reward: 0.495 [0.390, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.173, 10.610], loss: 0.003129, mae: 0.062617, mean_q: 0.397715
 48438/100000: episode: 821, duration: 0.105s, episode steps: 15, steps per second: 143, episode reward: 6.509, mean reward: 0.434 [0.308, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.568, 10.431], loss: 0.002512, mae: 0.053840, mean_q: 0.361448
 48471/100000: episode: 822, duration: 0.218s, episode steps: 33, steps per second: 152, episode reward: 15.458, mean reward: 0.468 [0.291, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.618, 10.100], loss: 0.003087, mae: 0.059210, mean_q: 0.379804
 48505/100000: episode: 823, duration: 0.226s, episode steps: 34, steps per second: 150, episode reward: 9.068, mean reward: 0.267 [0.142, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.709, 10.214], loss: 0.002750, mae: 0.057866, mean_q: 0.427182
 48539/100000: episode: 824, duration: 0.222s, episode steps: 34, steps per second: 153, episode reward: 13.429, mean reward: 0.395 [0.301, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.041, 10.521], loss: 0.002676, mae: 0.056365, mean_q: 0.371860
 48568/100000: episode: 825, duration: 0.191s, episode steps: 29, steps per second: 152, episode reward: 6.670, mean reward: 0.230 [0.097, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.085, 10.266], loss: 0.002771, mae: 0.056906, mean_q: 0.410424
 48583/100000: episode: 826, duration: 0.100s, episode steps: 15, steps per second: 151, episode reward: 6.962, mean reward: 0.464 [0.352, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.526], loss: 0.002978, mae: 0.058589, mean_q: 0.422502
 48598/100000: episode: 827, duration: 0.106s, episode steps: 15, steps per second: 142, episode reward: 6.385, mean reward: 0.426 [0.323, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.262, 10.570], loss: 0.002898, mae: 0.059665, mean_q: 0.394648
 48613/100000: episode: 828, duration: 0.107s, episode steps: 15, steps per second: 140, episode reward: 5.612, mean reward: 0.374 [0.296, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.970, 10.416], loss: 0.003126, mae: 0.062141, mean_q: 0.432895
 48642/100000: episode: 829, duration: 0.156s, episode steps: 29, steps per second: 185, episode reward: 9.185, mean reward: 0.317 [0.127, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.809, 10.100], loss: 0.002931, mae: 0.058533, mean_q: 0.457475
 48675/100000: episode: 830, duration: 0.194s, episode steps: 33, steps per second: 170, episode reward: 16.090, mean reward: 0.488 [0.410, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.596, 10.100], loss: 0.002537, mae: 0.054519, mean_q: 0.440853
 48690/100000: episode: 831, duration: 0.096s, episode steps: 15, steps per second: 157, episode reward: 5.646, mean reward: 0.376 [0.294, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.455, 10.419], loss: 0.002809, mae: 0.058017, mean_q: 0.467888
 48724/100000: episode: 832, duration: 0.205s, episode steps: 34, steps per second: 166, episode reward: 11.527, mean reward: 0.339 [0.191, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.380, 10.368], loss: 0.003145, mae: 0.060557, mean_q: 0.442391
 48753/100000: episode: 833, duration: 0.166s, episode steps: 29, steps per second: 175, episode reward: 13.648, mean reward: 0.471 [0.389, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.769, 10.100], loss: 0.003590, mae: 0.061649, mean_q: 0.460239
 48787/100000: episode: 834, duration: 0.193s, episode steps: 34, steps per second: 176, episode reward: 14.520, mean reward: 0.427 [0.321, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.451, 10.502], loss: 0.005461, mae: 0.069670, mean_q: 0.426463
 48821/100000: episode: 835, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 12.647, mean reward: 0.372 [0.073, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.822, 10.100], loss: 0.003771, mae: 0.061017, mean_q: 0.478047
 48855/100000: episode: 836, duration: 0.189s, episode steps: 34, steps per second: 180, episode reward: 16.087, mean reward: 0.473 [0.360, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.899, 10.100], loss: 0.003664, mae: 0.064566, mean_q: 0.452924
 48883/100000: episode: 837, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 11.779, mean reward: 0.421 [0.318, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.542, 10.100], loss: 0.003576, mae: 0.058696, mean_q: 0.477318
 48916/100000: episode: 838, duration: 0.200s, episode steps: 33, steps per second: 165, episode reward: 15.502, mean reward: 0.470 [0.349, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.414, 10.100], loss: 0.003001, mae: 0.058025, mean_q: 0.501658
 48944/100000: episode: 839, duration: 0.159s, episode steps: 28, steps per second: 177, episode reward: 11.180, mean reward: 0.399 [0.133, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.601, 10.100], loss: 0.003188, mae: 0.057695, mean_q: 0.479560
 48973/100000: episode: 840, duration: 0.184s, episode steps: 29, steps per second: 157, episode reward: 11.310, mean reward: 0.390 [0.272, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.028, 10.465], loss: 0.003133, mae: 0.061861, mean_q: 0.523664
 49006/100000: episode: 841, duration: 0.210s, episode steps: 33, steps per second: 157, episode reward: 17.646, mean reward: 0.535 [0.292, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.785, 10.100], loss: 0.002976, mae: 0.059728, mean_q: 0.532316
 49021/100000: episode: 842, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 6.770, mean reward: 0.451 [0.396, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.561], loss: 0.003123, mae: 0.058403, mean_q: 0.500521
 49049/100000: episode: 843, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 10.469, mean reward: 0.374 [0.250, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.713, 10.100], loss: 0.002741, mae: 0.056866, mean_q: 0.486751
 49083/100000: episode: 844, duration: 0.201s, episode steps: 34, steps per second: 170, episode reward: 15.089, mean reward: 0.444 [0.209, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.186, 10.100], loss: 0.002788, mae: 0.058310, mean_q: 0.491274
 49117/100000: episode: 845, duration: 0.215s, episode steps: 34, steps per second: 158, episode reward: 12.329, mean reward: 0.363 [0.082, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.587, 10.100], loss: 0.003258, mae: 0.063444, mean_q: 0.522314
 49151/100000: episode: 846, duration: 0.217s, episode steps: 34, steps per second: 156, episode reward: 9.931, mean reward: 0.292 [0.134, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.035, 10.236], loss: 0.002999, mae: 0.059539, mean_q: 0.524301
[Info] FALSIFICATION!
 49162/100000: episode: 847, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 15.201, mean reward: 1.382 [0.427, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.267, 10.484], loss: 0.003007, mae: 0.060693, mean_q: 0.573042
 49262/100000: episode: 848, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -19.168, mean reward: -0.192 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.531, 10.138], loss: 0.002876, mae: 0.059070, mean_q: 0.550659
 49362/100000: episode: 849, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -12.724, mean reward: -0.127 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.495, 10.409], loss: 0.003159, mae: 0.062243, mean_q: 0.538520
 49462/100000: episode: 850, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: -14.470, mean reward: -0.145 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.630, 10.098], loss: 0.002948, mae: 0.059543, mean_q: 0.547992
 49562/100000: episode: 851, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.115, mean reward: -0.161 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.514, 10.098], loss: 0.002838, mae: 0.058223, mean_q: 0.555415
 49662/100000: episode: 852, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -11.169, mean reward: -0.112 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.569, 10.098], loss: 0.018059, mae: 0.074004, mean_q: 0.541722
 49762/100000: episode: 853, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.265, mean reward: -0.123 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.075, 10.098], loss: 0.003448, mae: 0.061709, mean_q: 0.548839
 49862/100000: episode: 854, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.250, mean reward: -0.173 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.424, 10.259], loss: 0.003555, mae: 0.063649, mean_q: 0.518422
 49962/100000: episode: 855, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -15.184, mean reward: -0.152 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.340, 10.355], loss: 0.016572, mae: 0.069502, mean_q: 0.492711
 50062/100000: episode: 856, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.113, mean reward: -0.181 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.373, 10.098], loss: 0.003393, mae: 0.060065, mean_q: 0.493946
 50162/100000: episode: 857, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.851, mean reward: -0.149 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.316, 10.098], loss: 0.003408, mae: 0.061195, mean_q: 0.453931
 50262/100000: episode: 858, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.061, mean reward: -0.171 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.820, 10.206], loss: 0.030094, mae: 0.074527, mean_q: 0.456022
 50362/100000: episode: 859, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -12.203, mean reward: -0.122 [-1.000, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.361, 10.272], loss: 0.017412, mae: 0.075444, mean_q: 0.450936
 50462/100000: episode: 860, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.538, mean reward: -0.155 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.717, 10.098], loss: 0.030943, mae: 0.081066, mean_q: 0.419859
 50562/100000: episode: 861, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.867, mean reward: -0.179 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.301, 10.122], loss: 0.016492, mae: 0.066998, mean_q: 0.412683
 50662/100000: episode: 862, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.074, mean reward: -0.181 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.902, 10.184], loss: 0.003061, mae: 0.058894, mean_q: 0.369987
 50762/100000: episode: 863, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.884, mean reward: -0.169 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.156, 10.098], loss: 0.029878, mae: 0.077138, mean_q: 0.338539
 50862/100000: episode: 864, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -18.307, mean reward: -0.183 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.857, 10.098], loss: 0.043028, mae: 0.083658, mean_q: 0.356143
 50962/100000: episode: 865, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.017, mean reward: -0.190 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.913, 10.098], loss: 0.016655, mae: 0.069463, mean_q: 0.309743
 51062/100000: episode: 866, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -21.556, mean reward: -0.216 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.066, 10.098], loss: 0.042144, mae: 0.081328, mean_q: 0.325603
 51162/100000: episode: 867, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.637, mean reward: -0.176 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.204, 10.187], loss: 0.029132, mae: 0.077173, mean_q: 0.309822
 51262/100000: episode: 868, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -19.151, mean reward: -0.192 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.181, 10.098], loss: 0.002856, mae: 0.056939, mean_q: 0.257188
 51362/100000: episode: 869, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -15.542, mean reward: -0.155 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.955, 10.098], loss: 0.016134, mae: 0.065435, mean_q: 0.218865
 51462/100000: episode: 870, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.673, mean reward: -0.187 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.191, 10.190], loss: 0.015904, mae: 0.062774, mean_q: 0.240389
 51562/100000: episode: 871, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -12.989, mean reward: -0.130 [-1.000, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.111, 10.098], loss: 0.016056, mae: 0.064439, mean_q: 0.193302
 51662/100000: episode: 872, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -10.542, mean reward: -0.105 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.033, 10.579], loss: 0.016066, mae: 0.062757, mean_q: 0.185460
 51762/100000: episode: 873, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.842, mean reward: -0.188 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.265, 10.098], loss: 0.029613, mae: 0.074974, mean_q: 0.173876
 51862/100000: episode: 874, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -20.326, mean reward: -0.203 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.599, 10.130], loss: 0.004220, mae: 0.064043, mean_q: 0.148923
 51962/100000: episode: 875, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.355, mean reward: -0.174 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.878, 10.098], loss: 0.017511, mae: 0.070838, mean_q: 0.105736
 52062/100000: episode: 876, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -14.991, mean reward: -0.150 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.811, 10.323], loss: 0.003323, mae: 0.059915, mean_q: 0.098491
 52162/100000: episode: 877, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.477, mean reward: -0.155 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.673, 10.175], loss: 0.016391, mae: 0.068518, mean_q: 0.086945
 52262/100000: episode: 878, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.105, mean reward: -0.181 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.582, 10.204], loss: 0.002760, mae: 0.055295, mean_q: 0.079900
 52362/100000: episode: 879, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.622, mean reward: -0.156 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.031, 10.307], loss: 0.002572, mae: 0.053152, mean_q: 0.061151
 52462/100000: episode: 880, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.000, mean reward: -0.190 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.934, 10.154], loss: 0.015581, mae: 0.058769, mean_q: 0.052046
 52562/100000: episode: 881, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.068, mean reward: -0.191 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.899, 10.153], loss: 0.002955, mae: 0.057980, mean_q: 0.014943
 52662/100000: episode: 882, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -16.526, mean reward: -0.165 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.322, 10.098], loss: 0.029101, mae: 0.070004, mean_q: 0.021779
 52762/100000: episode: 883, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -20.744, mean reward: -0.207 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.488, 10.098], loss: 0.003309, mae: 0.060596, mean_q: 0.034016
 52862/100000: episode: 884, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.002, mean reward: -0.180 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.725, 10.098], loss: 0.015912, mae: 0.060803, mean_q: -0.030956
 52962/100000: episode: 885, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -20.291, mean reward: -0.203 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.561, 10.164], loss: 0.002858, mae: 0.055792, mean_q: -0.073119
 53062/100000: episode: 886, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.793, mean reward: -0.188 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.973, 10.098], loss: 0.028711, mae: 0.070524, mean_q: -0.091798
 53162/100000: episode: 887, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -18.345, mean reward: -0.183 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.425, 10.222], loss: 0.020826, mae: 0.089733, mean_q: -0.137476
 53262/100000: episode: 888, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -12.710, mean reward: -0.127 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.975, 10.098], loss: 0.005519, mae: 0.069340, mean_q: -0.139514
 53362/100000: episode: 889, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.160, mean reward: -0.142 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.542, 10.205], loss: 0.003776, mae: 0.059922, mean_q: -0.133388
 53462/100000: episode: 890, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -5.106, mean reward: -0.051 [-1.000, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.502, 10.425], loss: 0.002525, mae: 0.050109, mean_q: -0.180588
 53562/100000: episode: 891, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.395, mean reward: -0.174 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.329, 10.233], loss: 0.002545, mae: 0.050624, mean_q: -0.171612
 53662/100000: episode: 892, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.612, mean reward: -0.196 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.768, 10.134], loss: 0.016112, mae: 0.061135, mean_q: -0.179631
 53762/100000: episode: 893, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -15.419, mean reward: -0.154 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.461, 10.098], loss: 0.003033, mae: 0.056378, mean_q: -0.205321
 53862/100000: episode: 894, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.572, mean reward: -0.176 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.363, 10.161], loss: 0.015812, mae: 0.059463, mean_q: -0.232172
 53962/100000: episode: 895, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.479, mean reward: -0.195 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.663, 10.289], loss: 0.002598, mae: 0.050740, mean_q: -0.283168
 54062/100000: episode: 896, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -20.216, mean reward: -0.202 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.961, 10.098], loss: 0.002758, mae: 0.051950, mean_q: -0.307405
 54162/100000: episode: 897, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -19.344, mean reward: -0.193 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.266, 10.132], loss: 0.016229, mae: 0.060900, mean_q: -0.311318
 54262/100000: episode: 898, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.872, mean reward: -0.159 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.230, 10.305], loss: 0.002649, mae: 0.050272, mean_q: -0.339761
 54362/100000: episode: 899, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.926, mean reward: -0.159 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.665, 10.098], loss: 0.002424, mae: 0.049160, mean_q: -0.344710
 54462/100000: episode: 900, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.943, mean reward: -0.209 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.708, 10.121], loss: 0.002517, mae: 0.049341, mean_q: -0.334050
 54562/100000: episode: 901, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -12.353, mean reward: -0.124 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.639, 10.098], loss: 0.002616, mae: 0.051389, mean_q: -0.277452
 54662/100000: episode: 902, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.657, mean reward: -0.167 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.913, 10.251], loss: 0.002531, mae: 0.049115, mean_q: -0.323067
 54762/100000: episode: 903, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.447, mean reward: -0.174 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.128, 10.237], loss: 0.002459, mae: 0.048839, mean_q: -0.330942
 54862/100000: episode: 904, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -20.128, mean reward: -0.201 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.883, 10.098], loss: 0.002496, mae: 0.050301, mean_q: -0.347455
 54962/100000: episode: 905, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -16.180, mean reward: -0.162 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.693, 10.318], loss: 0.002477, mae: 0.049926, mean_q: -0.315010
 55062/100000: episode: 906, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -19.167, mean reward: -0.192 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.722, 10.233], loss: 0.002355, mae: 0.048228, mean_q: -0.323766
 55162/100000: episode: 907, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -19.221, mean reward: -0.192 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.938, 10.098], loss: 0.002377, mae: 0.047746, mean_q: -0.333560
 55262/100000: episode: 908, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.020, mean reward: -0.120 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.721, 10.098], loss: 0.002445, mae: 0.050404, mean_q: -0.320327
 55362/100000: episode: 909, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: -15.828, mean reward: -0.158 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.003, 10.098], loss: 0.002553, mae: 0.050522, mean_q: -0.316445
 55462/100000: episode: 910, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: -12.150, mean reward: -0.121 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.852, 10.098], loss: 0.002358, mae: 0.048770, mean_q: -0.313762
 55562/100000: episode: 911, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -14.581, mean reward: -0.146 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.262, 10.098], loss: 0.002293, mae: 0.047479, mean_q: -0.318413
 55662/100000: episode: 912, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.178, mean reward: -0.182 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.666, 10.098], loss: 0.002270, mae: 0.046350, mean_q: -0.343850
 55762/100000: episode: 913, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.280, mean reward: -0.183 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.817, 10.098], loss: 0.002293, mae: 0.047993, mean_q: -0.296888
 55862/100000: episode: 914, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.742, mean reward: -0.187 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.948, 10.143], loss: 0.002400, mae: 0.047935, mean_q: -0.352731
 55962/100000: episode: 915, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -15.544, mean reward: -0.155 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.163, 10.216], loss: 0.002602, mae: 0.049865, mean_q: -0.335013
 56062/100000: episode: 916, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.580, mean reward: -0.176 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.272, 10.392], loss: 0.002413, mae: 0.048824, mean_q: -0.309494
 56162/100000: episode: 917, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.372, mean reward: -0.164 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.400, 10.403], loss: 0.002558, mae: 0.049459, mean_q: -0.337251
 56262/100000: episode: 918, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.423, mean reward: -0.184 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.041, 10.170], loss: 0.002658, mae: 0.053026, mean_q: -0.299332
 56362/100000: episode: 919, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.457, mean reward: -0.155 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.421, 10.236], loss: 0.002556, mae: 0.051446, mean_q: -0.310149
 56462/100000: episode: 920, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.507, mean reward: -0.185 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.492, 10.277], loss: 0.002507, mae: 0.049733, mean_q: -0.295955
 56562/100000: episode: 921, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.465, mean reward: -0.195 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.372, 10.140], loss: 0.002485, mae: 0.048713, mean_q: -0.314246
 56662/100000: episode: 922, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.669, mean reward: -0.167 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.725, 10.109], loss: 0.002577, mae: 0.052629, mean_q: -0.328876
 56762/100000: episode: 923, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.059, mean reward: -0.191 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.998, 10.207], loss: 0.002292, mae: 0.048220, mean_q: -0.301029
 56862/100000: episode: 924, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -15.785, mean reward: -0.158 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.428, 10.119], loss: 0.002373, mae: 0.048954, mean_q: -0.321401
 56962/100000: episode: 925, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.637, mean reward: -0.196 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.570, 10.255], loss: 0.002325, mae: 0.048253, mean_q: -0.311870
 57062/100000: episode: 926, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.245, mean reward: -0.172 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.637, 10.098], loss: 0.002404, mae: 0.048534, mean_q: -0.301672
 57162/100000: episode: 927, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.728, mean reward: -0.187 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.887, 10.312], loss: 0.002355, mae: 0.048561, mean_q: -0.307139
 57262/100000: episode: 928, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.173, mean reward: -0.182 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.226, 10.098], loss: 0.002196, mae: 0.046673, mean_q: -0.321426
 57362/100000: episode: 929, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.007, mean reward: -0.170 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.322, 10.265], loss: 0.002574, mae: 0.050702, mean_q: -0.336032
 57462/100000: episode: 930, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -18.153, mean reward: -0.182 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.518, 10.119], loss: 0.002330, mae: 0.047200, mean_q: -0.343528
 57562/100000: episode: 931, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.156, mean reward: -0.172 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.979, 10.158], loss: 0.002238, mae: 0.047169, mean_q: -0.324468
 57662/100000: episode: 932, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -15.127, mean reward: -0.151 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.880, 10.098], loss: 0.003802, mae: 0.055709, mean_q: -0.337793
 57762/100000: episode: 933, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.844, mean reward: -0.188 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.829, 10.119], loss: 0.002603, mae: 0.051723, mean_q: -0.297800
 57862/100000: episode: 934, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -19.991, mean reward: -0.200 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.806, 10.201], loss: 0.002295, mae: 0.048669, mean_q: -0.304305
 57962/100000: episode: 935, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -15.293, mean reward: -0.153 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.948, 10.235], loss: 0.002392, mae: 0.049171, mean_q: -0.321515
 58062/100000: episode: 936, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -20.313, mean reward: -0.203 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.930, 10.098], loss: 0.002269, mae: 0.047586, mean_q: -0.327995
 58162/100000: episode: 937, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -18.239, mean reward: -0.182 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.188, 10.127], loss: 0.002234, mae: 0.047164, mean_q: -0.365773
 58262/100000: episode: 938, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -13.033, mean reward: -0.130 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.400, 10.361], loss: 0.002387, mae: 0.049180, mean_q: -0.339017
 58362/100000: episode: 939, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -12.704, mean reward: -0.127 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.480, 10.511], loss: 0.002428, mae: 0.048970, mean_q: -0.358535
 58462/100000: episode: 940, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.295, mean reward: -0.173 [-1.000, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.272, 10.130], loss: 0.002332, mae: 0.048595, mean_q: -0.309406
 58562/100000: episode: 941, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -20.892, mean reward: -0.209 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.186, 10.241], loss: 0.002378, mae: 0.048281, mean_q: -0.322928
 58662/100000: episode: 942, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.947, mean reward: -0.159 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.124, 10.098], loss: 0.002304, mae: 0.048392, mean_q: -0.291361
 58762/100000: episode: 943, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -10.777, mean reward: -0.108 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.474, 10.098], loss: 0.002607, mae: 0.051304, mean_q: -0.345791
 58862/100000: episode: 944, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.555, mean reward: -0.186 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.766, 10.187], loss: 0.002432, mae: 0.049901, mean_q: -0.303831
 58962/100000: episode: 945, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.732, mean reward: -0.187 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.621, 10.098], loss: 0.002242, mae: 0.046856, mean_q: -0.314243
 59062/100000: episode: 946, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.503, mean reward: -0.155 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.429, 10.134], loss: 0.002334, mae: 0.047390, mean_q: -0.345948
[Info] 100-TH LEVEL FOUND: 0.6493260860443115, Considering 10/90 traces
 59162/100000: episode: 947, duration: 4.692s, episode steps: 100, steps per second: 21, episode reward: -17.059, mean reward: -0.171 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.939, 10.098], loss: 0.002423, mae: 0.048873, mean_q: -0.329224
 59178/100000: episode: 948, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 7.015, mean reward: 0.438 [0.224, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.061, 10.606], loss: 0.002387, mae: 0.047288, mean_q: -0.408625
 59194/100000: episode: 949, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 6.731, mean reward: 0.421 [0.331, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.586, 10.591], loss: 0.002325, mae: 0.049485, mean_q: -0.318443
 59210/100000: episode: 950, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 3.232, mean reward: 0.202 [0.123, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.271], loss: 0.002685, mae: 0.051580, mean_q: -0.259359
 59215/100000: episode: 951, duration: 0.035s, episode steps: 5, steps per second: 144, episode reward: 1.900, mean reward: 0.380 [0.276, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-1.226, 10.369], loss: 0.002257, mae: 0.050414, mean_q: -0.336850
 59231/100000: episode: 952, duration: 0.123s, episode steps: 16, steps per second: 130, episode reward: 5.398, mean reward: 0.337 [0.193, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.373], loss: 0.002898, mae: 0.056093, mean_q: -0.323790
 59236/100000: episode: 953, duration: 0.038s, episode steps: 5, steps per second: 132, episode reward: 1.333, mean reward: 0.267 [0.216, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.289], loss: 0.002810, mae: 0.056532, mean_q: -0.259429
 59271/100000: episode: 954, duration: 0.401s, episode steps: 35, steps per second: 87, episode reward: 10.961, mean reward: 0.313 [0.181, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.693, 10.497], loss: 0.002263, mae: 0.049195, mean_q: -0.283099
 59294/100000: episode: 955, duration: 0.218s, episode steps: 23, steps per second: 105, episode reward: 7.758, mean reward: 0.337 [0.213, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.893, 10.100], loss: 0.002454, mae: 0.048498, mean_q: -0.310976
 59317/100000: episode: 956, duration: 0.182s, episode steps: 23, steps per second: 126, episode reward: 6.373, mean reward: 0.277 [0.193, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.360, 10.100], loss: 0.002163, mae: 0.045341, mean_q: -0.293868
 59352/100000: episode: 957, duration: 0.328s, episode steps: 35, steps per second: 107, episode reward: 10.957, mean reward: 0.313 [0.182, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.035, 10.448], loss: 0.002298, mae: 0.049392, mean_q: -0.278325
 59387/100000: episode: 958, duration: 0.276s, episode steps: 35, steps per second: 127, episode reward: 7.073, mean reward: 0.202 [0.081, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-1.183, 10.253], loss: 0.002426, mae: 0.048554, mean_q: -0.313028
 59422/100000: episode: 959, duration: 0.249s, episode steps: 35, steps per second: 140, episode reward: 12.310, mean reward: 0.352 [0.235, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.175, 10.463], loss: 0.002552, mae: 0.051059, mean_q: -0.269880
 59438/100000: episode: 960, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 5.338, mean reward: 0.334 [0.279, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.069, 10.386], loss: 0.002955, mae: 0.055797, mean_q: -0.232209
 59457/100000: episode: 961, duration: 0.128s, episode steps: 19, steps per second: 148, episode reward: 6.296, mean reward: 0.331 [0.252, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.327, 10.100], loss: 0.002380, mae: 0.049951, mean_q: -0.233769
 59492/100000: episode: 962, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 9.167, mean reward: 0.262 [0.092, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.447, 10.288], loss: 0.002395, mae: 0.048932, mean_q: -0.294718
 59515/100000: episode: 963, duration: 0.148s, episode steps: 23, steps per second: 156, episode reward: 7.248, mean reward: 0.315 [0.213, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.240, 10.100], loss: 0.002593, mae: 0.050088, mean_q: -0.269410
 59520/100000: episode: 964, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 1.460, mean reward: 0.292 [0.264, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.347], loss: 0.002166, mae: 0.046377, mean_q: -0.403997
 59525/100000: episode: 965, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 1.777, mean reward: 0.355 [0.298, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.381], loss: 0.002682, mae: 0.050071, mean_q: -0.184376
 59548/100000: episode: 966, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 8.316, mean reward: 0.362 [0.197, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.420, 10.100], loss: 0.002413, mae: 0.048431, mean_q: -0.287168
 59568/100000: episode: 967, duration: 0.141s, episode steps: 20, steps per second: 142, episode reward: 4.628, mean reward: 0.231 [0.101, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.160, 10.100], loss: 0.002022, mae: 0.044609, mean_q: -0.285159
 59591/100000: episode: 968, duration: 0.137s, episode steps: 23, steps per second: 167, episode reward: 6.538, mean reward: 0.284 [0.174, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.228, 10.100], loss: 0.002486, mae: 0.048978, mean_q: -0.205602
 59631/100000: episode: 969, duration: 0.252s, episode steps: 40, steps per second: 159, episode reward: 9.868, mean reward: 0.247 [0.089, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.697, 10.262], loss: 0.002606, mae: 0.050925, mean_q: -0.244799
 59671/100000: episode: 970, duration: 0.245s, episode steps: 40, steps per second: 163, episode reward: 11.100, mean reward: 0.277 [0.112, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-1.062, 10.181], loss: 0.002500, mae: 0.051283, mean_q: -0.233636
 59688/100000: episode: 971, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 5.598, mean reward: 0.329 [0.174, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.283], loss: 0.002754, mae: 0.053486, mean_q: -0.236428
 59693/100000: episode: 972, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: 1.380, mean reward: 0.276 [0.260, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.343], loss: 0.002942, mae: 0.056936, mean_q: -0.050216
 59709/100000: episode: 973, duration: 0.097s, episode steps: 16, steps per second: 164, episode reward: 5.324, mean reward: 0.333 [0.213, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.061, 10.367], loss: 0.002864, mae: 0.053444, mean_q: -0.172571
 59725/100000: episode: 974, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 4.072, mean reward: 0.255 [0.187, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.974, 10.335], loss: 0.002325, mae: 0.050052, mean_q: -0.202461
 59741/100000: episode: 975, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 4.409, mean reward: 0.276 [0.198, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.384, 10.360], loss: 0.002294, mae: 0.047550, mean_q: -0.201091
 59757/100000: episode: 976, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 6.172, mean reward: 0.386 [0.306, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.420], loss: 0.002384, mae: 0.051314, mean_q: -0.176975
 59797/100000: episode: 977, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 7.304, mean reward: 0.183 [0.018, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.072, 10.100], loss: 0.002506, mae: 0.051503, mean_q: -0.151936
 59817/100000: episode: 978, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 6.920, mean reward: 0.346 [0.163, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.088, 10.100], loss: 0.003034, mae: 0.056069, mean_q: -0.245784
 59837/100000: episode: 979, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 7.666, mean reward: 0.383 [0.319, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.716, 10.100], loss: 0.002452, mae: 0.051273, mean_q: -0.193005
 59856/100000: episode: 980, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 7.499, mean reward: 0.395 [0.327, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.518, 10.100], loss: 0.002535, mae: 0.052863, mean_q: -0.218438
 59875/100000: episode: 981, duration: 0.091s, episode steps: 19, steps per second: 209, episode reward: 4.182, mean reward: 0.220 [0.013, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.314, 10.100], loss: 0.002481, mae: 0.052094, mean_q: -0.204991
 59898/100000: episode: 982, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 7.169, mean reward: 0.312 [0.143, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.512, 10.100], loss: 0.002775, mae: 0.054188, mean_q: -0.191124
 59921/100000: episode: 983, duration: 0.113s, episode steps: 23, steps per second: 204, episode reward: 10.287, mean reward: 0.447 [0.229, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-1.038, 10.100], loss: 0.003073, mae: 0.060643, mean_q: -0.137529
 59961/100000: episode: 984, duration: 0.211s, episode steps: 40, steps per second: 190, episode reward: 13.678, mean reward: 0.342 [0.237, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.688, 10.373], loss: 0.002752, mae: 0.054064, mean_q: -0.150149
 59984/100000: episode: 985, duration: 0.119s, episode steps: 23, steps per second: 194, episode reward: 9.039, mean reward: 0.393 [0.253, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.220, 10.100], loss: 0.002403, mae: 0.048573, mean_q: -0.224153
 60003/100000: episode: 986, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 6.506, mean reward: 0.342 [0.219, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.181, 10.100], loss: 0.002505, mae: 0.050501, mean_q: -0.134050
 60023/100000: episode: 987, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 5.854, mean reward: 0.293 [0.223, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.014, 10.100], loss: 0.002220, mae: 0.046537, mean_q: -0.229365
 60046/100000: episode: 988, duration: 0.119s, episode steps: 23, steps per second: 194, episode reward: 5.429, mean reward: 0.236 [0.144, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.984, 10.100], loss: 0.002635, mae: 0.051837, mean_q: -0.169798
 60062/100000: episode: 989, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 3.581, mean reward: 0.224 [0.118, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.593, 10.244], loss: 0.002788, mae: 0.051743, mean_q: -0.220876
 60082/100000: episode: 990, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 7.390, mean reward: 0.370 [0.258, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.717, 10.100], loss: 0.002569, mae: 0.051186, mean_q: -0.203397
 60087/100000: episode: 991, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 1.655, mean reward: 0.331 [0.203, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.559, 10.366], loss: 0.003248, mae: 0.057488, mean_q: -0.177539
 60103/100000: episode: 992, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 6.959, mean reward: 0.435 [0.297, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.480, 10.550], loss: 0.002607, mae: 0.053661, mean_q: -0.108864
 60126/100000: episode: 993, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 6.592, mean reward: 0.287 [0.200, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.399, 10.100], loss: 0.002929, mae: 0.056380, mean_q: -0.082474
 60143/100000: episode: 994, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 5.868, mean reward: 0.345 [0.244, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.145, 10.384], loss: 0.002429, mae: 0.048669, mean_q: -0.207932
 60160/100000: episode: 995, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 4.585, mean reward: 0.270 [0.184, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.062, 10.295], loss: 0.002675, mae: 0.051499, mean_q: -0.181234
 60177/100000: episode: 996, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 6.451, mean reward: 0.379 [0.326, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.172, 10.446], loss: 0.002896, mae: 0.055206, mean_q: -0.175762
 60217/100000: episode: 997, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 8.216, mean reward: 0.205 [0.009, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.528, 10.263], loss: 0.002574, mae: 0.051610, mean_q: -0.130659
 60233/100000: episode: 998, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 5.531, mean reward: 0.346 [0.266, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.498], loss: 0.002504, mae: 0.051715, mean_q: -0.157497
 60252/100000: episode: 999, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 5.496, mean reward: 0.289 [0.084, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.582, 10.100], loss: 0.002584, mae: 0.053403, mean_q: -0.126684
 60268/100000: episode: 1000, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 3.904, mean reward: 0.244 [0.180, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.291], loss: 0.002735, mae: 0.051635, mean_q: -0.205403
 60284/100000: episode: 1001, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 5.035, mean reward: 0.315 [0.219, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.750, 10.288], loss: 0.003117, mae: 0.055367, mean_q: -0.134352
 60289/100000: episode: 1002, duration: 0.030s, episode steps: 5, steps per second: 164, episode reward: 1.878, mean reward: 0.376 [0.350, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.449], loss: 0.002882, mae: 0.057324, mean_q: 0.007695
 60305/100000: episode: 1003, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 4.919, mean reward: 0.307 [0.198, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.305], loss: 0.002697, mae: 0.053383, mean_q: -0.145581
 60322/100000: episode: 1004, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 4.825, mean reward: 0.284 [0.198, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.331, 10.354], loss: 0.002674, mae: 0.052312, mean_q: -0.092162
 60339/100000: episode: 1005, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 6.066, mean reward: 0.357 [0.292, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.162, 10.489], loss: 0.002596, mae: 0.051199, mean_q: -0.157290
 60374/100000: episode: 1006, duration: 0.196s, episode steps: 35, steps per second: 178, episode reward: 13.669, mean reward: 0.391 [0.201, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.764, 10.343], loss: 0.002536, mae: 0.051292, mean_q: -0.125765
 60379/100000: episode: 1007, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 1.332, mean reward: 0.266 [0.229, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.285], loss: 0.002789, mae: 0.052957, mean_q: -0.182405
 60414/100000: episode: 1008, duration: 0.169s, episode steps: 35, steps per second: 207, episode reward: 7.701, mean reward: 0.220 [0.136, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.060, 10.321], loss: 0.002776, mae: 0.053850, mean_q: -0.098817
 60430/100000: episode: 1009, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 4.711, mean reward: 0.294 [0.202, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.378], loss: 0.002868, mae: 0.057663, mean_q: -0.067788
 60450/100000: episode: 1010, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 5.270, mean reward: 0.264 [0.137, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.154, 10.100], loss: 0.005393, mae: 0.069634, mean_q: -0.043152
 60490/100000: episode: 1011, duration: 0.251s, episode steps: 40, steps per second: 159, episode reward: 8.438, mean reward: 0.211 [0.058, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.754, 10.194], loss: 0.003609, mae: 0.065236, mean_q: -0.059267
 60506/100000: episode: 1012, duration: 0.105s, episode steps: 16, steps per second: 152, episode reward: 6.265, mean reward: 0.392 [0.303, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.512], loss: 0.002807, mae: 0.056048, mean_q: -0.189138
 60541/100000: episode: 1013, duration: 0.226s, episode steps: 35, steps per second: 155, episode reward: 8.348, mean reward: 0.239 [0.047, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.999, 10.142], loss: 0.002803, mae: 0.055585, mean_q: -0.112754
 60581/100000: episode: 1014, duration: 0.271s, episode steps: 40, steps per second: 147, episode reward: 10.551, mean reward: 0.264 [0.078, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.421, 10.204], loss: 0.002770, mae: 0.055246, mean_q: -0.053383
 60621/100000: episode: 1015, duration: 0.239s, episode steps: 40, steps per second: 167, episode reward: 10.823, mean reward: 0.271 [0.062, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.281, 10.137], loss: 0.002939, mae: 0.055875, mean_q: -0.043781
 60637/100000: episode: 1016, duration: 0.105s, episode steps: 16, steps per second: 152, episode reward: 4.696, mean reward: 0.294 [0.196, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.301], loss: 0.002634, mae: 0.051935, mean_q: -0.043478
 60657/100000: episode: 1017, duration: 0.125s, episode steps: 20, steps per second: 160, episode reward: 5.628, mean reward: 0.281 [0.152, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.621, 10.100], loss: 0.003278, mae: 0.060624, mean_q: -0.029549
 60680/100000: episode: 1018, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 8.530, mean reward: 0.371 [0.203, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.347, 10.100], loss: 0.002699, mae: 0.054774, mean_q: -0.041635
 60696/100000: episode: 1019, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 5.569, mean reward: 0.348 [0.211, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.400], loss: 0.002610, mae: 0.051387, mean_q: -0.119505
 60712/100000: episode: 1020, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 6.364, mean reward: 0.398 [0.324, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-1.266, 10.477], loss: 0.002241, mae: 0.050498, mean_q: -0.072124
 60731/100000: episode: 1021, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 5.476, mean reward: 0.288 [0.153, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.500, 10.100], loss: 0.002720, mae: 0.053175, mean_q: -0.074169
 60748/100000: episode: 1022, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 6.632, mean reward: 0.390 [0.310, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.470], loss: 0.002566, mae: 0.052442, mean_q: -0.041161
 60753/100000: episode: 1023, duration: 0.029s, episode steps: 5, steps per second: 170, episode reward: 1.550, mean reward: 0.310 [0.276, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.459], loss: 0.003469, mae: 0.062278, mean_q: -0.112338
 60793/100000: episode: 1024, duration: 0.215s, episode steps: 40, steps per second: 186, episode reward: 10.435, mean reward: 0.261 [0.103, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.808, 10.177], loss: 0.002793, mae: 0.054708, mean_q: -0.021018
 60809/100000: episode: 1025, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 6.347, mean reward: 0.397 [0.347, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.448], loss: 0.003072, mae: 0.057882, mean_q: 0.011386
 60825/100000: episode: 1026, duration: 0.077s, episode steps: 16, steps per second: 208, episode reward: 4.941, mean reward: 0.309 [0.178, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.367], loss: 0.002912, mae: 0.056072, mean_q: 0.026806
 60860/100000: episode: 1027, duration: 0.164s, episode steps: 35, steps per second: 213, episode reward: 10.509, mean reward: 0.300 [0.112, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.791, 10.200], loss: 0.002826, mae: 0.054706, mean_q: -0.018680
 60879/100000: episode: 1028, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 5.399, mean reward: 0.284 [0.161, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.766, 10.100], loss: 0.003133, mae: 0.058912, mean_q: 0.021772
 60914/100000: episode: 1029, duration: 0.181s, episode steps: 35, steps per second: 194, episode reward: 7.624, mean reward: 0.218 [0.006, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.453, 10.162], loss: 0.002775, mae: 0.054664, mean_q: 0.000394
 60933/100000: episode: 1030, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 6.245, mean reward: 0.329 [0.117, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.366, 10.100], loss: 0.002712, mae: 0.054602, mean_q: 0.034060
 60968/100000: episode: 1031, duration: 0.195s, episode steps: 35, steps per second: 179, episode reward: 12.224, mean reward: 0.349 [0.221, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.375, 10.352], loss: 0.003256, mae: 0.059492, mean_q: -0.004067
 60991/100000: episode: 1032, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 6.405, mean reward: 0.278 [0.169, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.249, 10.100], loss: 0.003367, mae: 0.061272, mean_q: 0.036697
 61014/100000: episode: 1033, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 8.103, mean reward: 0.352 [0.253, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.458, 10.100], loss: 0.002671, mae: 0.054074, mean_q: 0.020470
 61031/100000: episode: 1034, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 7.618, mean reward: 0.448 [0.329, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.549], loss: 0.003618, mae: 0.062888, mean_q: 0.108698
 61071/100000: episode: 1035, duration: 0.204s, episode steps: 40, steps per second: 196, episode reward: 12.847, mean reward: 0.321 [0.170, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.509, 10.315], loss: 0.002992, mae: 0.056440, mean_q: -0.000867
 61106/100000: episode: 1036, duration: 0.182s, episode steps: 35, steps per second: 193, episode reward: 9.741, mean reward: 0.278 [0.133, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.397, 10.288], loss: 0.002791, mae: 0.055173, mean_q: -0.004506
[Info] 200-TH LEVEL FOUND: 0.8402624726295471, Considering 10/90 traces
 61126/100000: episode: 1037, duration: 4.215s, episode steps: 20, steps per second: 5, episode reward: 6.886, mean reward: 0.344 [0.239, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.181, 10.100], loss: 0.002907, mae: 0.056753, mean_q: 0.029002
 61137/100000: episode: 1038, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 5.318, mean reward: 0.483 [0.454, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.310, 10.100], loss: 0.002478, mae: 0.052682, mean_q: 0.014987
 61151/100000: episode: 1039, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 4.740, mean reward: 0.339 [0.230, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.665, 10.448], loss: 0.002592, mae: 0.052378, mean_q: 0.057457
 61164/100000: episode: 1040, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 5.140, mean reward: 0.395 [0.350, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.402, 10.481], loss: 0.003024, mae: 0.058947, mean_q: 0.116508
 61177/100000: episode: 1041, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 3.854, mean reward: 0.296 [0.180, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.059, 10.386], loss: 0.002887, mae: 0.057630, mean_q: 0.069230
 61190/100000: episode: 1042, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 5.278, mean reward: 0.406 [0.335, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.671, 10.460], loss: 0.003332, mae: 0.062452, mean_q: 0.151753
 61205/100000: episode: 1043, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 5.884, mean reward: 0.392 [0.262, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.450], loss: 0.002633, mae: 0.054615, mean_q: 0.002857
 61219/100000: episode: 1044, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 5.295, mean reward: 0.378 [0.318, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.184, 10.477], loss: 0.002466, mae: 0.053103, mean_q: 0.113948
 61254/100000: episode: 1045, duration: 0.210s, episode steps: 35, steps per second: 167, episode reward: 15.591, mean reward: 0.445 [0.339, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.669, 10.554], loss: 0.003369, mae: 0.060861, mean_q: 0.051581
 61265/100000: episode: 1046, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 6.084, mean reward: 0.553 [0.497, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.394, 10.100], loss: 0.003544, mae: 0.061601, mean_q: 0.035416
 61276/100000: episode: 1047, duration: 0.069s, episode steps: 11, steps per second: 158, episode reward: 5.281, mean reward: 0.480 [0.457, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.588], loss: 0.003195, mae: 0.058152, mean_q: 0.066253
 61311/100000: episode: 1048, duration: 0.181s, episode steps: 35, steps per second: 194, episode reward: 13.262, mean reward: 0.379 [0.210, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.386, 10.512], loss: 0.002726, mae: 0.054228, mean_q: 0.055553
 61323/100000: episode: 1049, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 4.223, mean reward: 0.352 [0.266, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.501, 10.410], loss: 0.002908, mae: 0.055338, mean_q: 0.070409
 61335/100000: episode: 1050, duration: 0.070s, episode steps: 12, steps per second: 173, episode reward: 4.725, mean reward: 0.394 [0.313, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.519], loss: 0.003132, mae: 0.060100, mean_q: 0.151515
 61348/100000: episode: 1051, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 4.762, mean reward: 0.366 [0.285, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.432], loss: 0.003481, mae: 0.063117, mean_q: 0.133282
 61359/100000: episode: 1052, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 4.352, mean reward: 0.396 [0.313, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.434], loss: 0.002995, mae: 0.058578, mean_q: 0.060602
 61375/100000: episode: 1053, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 6.194, mean reward: 0.387 [0.340, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.424], loss: 0.002995, mae: 0.055892, mean_q: 0.023889
 61391/100000: episode: 1054, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 6.339, mean reward: 0.396 [0.311, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.291, 10.582], loss: 0.002561, mae: 0.052971, mean_q: 0.102140
 61402/100000: episode: 1055, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 4.688, mean reward: 0.426 [0.323, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.185, 10.486], loss: 0.003303, mae: 0.062675, mean_q: 0.163218
 61426/100000: episode: 1056, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 9.960, mean reward: 0.415 [0.281, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.093, 10.613], loss: 0.002543, mae: 0.053146, mean_q: 0.133700
 61440/100000: episode: 1057, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 5.266, mean reward: 0.376 [0.285, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.471, 10.501], loss: 0.003601, mae: 0.061455, mean_q: 0.087231
 61451/100000: episode: 1058, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 4.648, mean reward: 0.423 [0.380, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.463], loss: 0.003067, mae: 0.060665, mean_q: 0.112489
 61466/100000: episode: 1059, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 6.572, mean reward: 0.438 [0.330, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.318, 10.388], loss: 0.003320, mae: 0.061500, mean_q: 0.086669
 61490/100000: episode: 1060, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 8.489, mean reward: 0.354 [0.138, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.323], loss: 0.002933, mae: 0.057052, mean_q: 0.136827
 61502/100000: episode: 1061, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 3.513, mean reward: 0.293 [0.258, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.725, 10.401], loss: 0.003040, mae: 0.056699, mean_q: 0.201280
 61526/100000: episode: 1062, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 10.212, mean reward: 0.425 [0.277, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.640], loss: 0.003128, mae: 0.058121, mean_q: 0.111811
 61540/100000: episode: 1063, duration: 0.104s, episode steps: 14, steps per second: 134, episode reward: 4.831, mean reward: 0.345 [0.193, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.357, 10.324], loss: 0.003000, mae: 0.057902, mean_q: 0.110421
 61552/100000: episode: 1064, duration: 0.070s, episode steps: 12, steps per second: 170, episode reward: 4.828, mean reward: 0.402 [0.345, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.443], loss: 0.003095, mae: 0.058743, mean_q: 0.125281
 61563/100000: episode: 1065, duration: 0.079s, episode steps: 11, steps per second: 139, episode reward: 4.599, mean reward: 0.418 [0.359, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.389, 10.100], loss: 0.002945, mae: 0.057276, mean_q: 0.105321
 61577/100000: episode: 1066, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 5.384, mean reward: 0.385 [0.282, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-1.268, 10.409], loss: 0.003433, mae: 0.060707, mean_q: 0.118582
 61588/100000: episode: 1067, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 4.043, mean reward: 0.368 [0.262, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-1.147, 10.450], loss: 0.002958, mae: 0.060355, mean_q: 0.099243
 61600/100000: episode: 1068, duration: 0.078s, episode steps: 12, steps per second: 153, episode reward: 3.240, mean reward: 0.270 [0.201, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.355], loss: 0.003179, mae: 0.059136, mean_q: 0.189871
 61635/100000: episode: 1069, duration: 0.218s, episode steps: 35, steps per second: 160, episode reward: 15.543, mean reward: 0.444 [0.316, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.513, 10.495], loss: 0.002944, mae: 0.058770, mean_q: 0.178285
 61670/100000: episode: 1070, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 10.055, mean reward: 0.287 [0.119, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.614, 10.252], loss: 0.003436, mae: 0.059943, mean_q: 0.148899
 61684/100000: episode: 1071, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 4.874, mean reward: 0.348 [0.270, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.325, 10.397], loss: 0.003127, mae: 0.058316, mean_q: 0.194144
 61695/100000: episode: 1072, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 3.828, mean reward: 0.348 [0.260, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.443], loss: 0.002692, mae: 0.053438, mean_q: 0.163069
 61708/100000: episode: 1073, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 5.581, mean reward: 0.429 [0.361, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.476, 10.552], loss: 0.002576, mae: 0.052734, mean_q: 0.094671
 61720/100000: episode: 1074, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 4.969, mean reward: 0.414 [0.346, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.465], loss: 0.002977, mae: 0.057740, mean_q: 0.074309
 61744/100000: episode: 1075, duration: 0.123s, episode steps: 24, steps per second: 196, episode reward: 10.958, mean reward: 0.457 [0.371, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.791, 10.598], loss: 0.003085, mae: 0.058770, mean_q: 0.176815
 61759/100000: episode: 1076, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 6.862, mean reward: 0.457 [0.333, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.404], loss: 0.003024, mae: 0.058395, mean_q: 0.155946
 61775/100000: episode: 1077, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 7.951, mean reward: 0.497 [0.420, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.814, 10.559], loss: 0.003149, mae: 0.060427, mean_q: 0.161661
 61791/100000: episode: 1078, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 6.291, mean reward: 0.393 [0.302, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.300, 10.483], loss: 0.003162, mae: 0.058446, mean_q: 0.170735
 61806/100000: episode: 1079, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 7.077, mean reward: 0.472 [0.371, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.155, 10.478], loss: 0.003084, mae: 0.060212, mean_q: 0.220895
 61841/100000: episode: 1080, duration: 0.177s, episode steps: 35, steps per second: 198, episode reward: 16.030, mean reward: 0.458 [0.290, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.425, 10.409], loss: 0.003386, mae: 0.062381, mean_q: 0.236508
 61854/100000: episode: 1081, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 4.311, mean reward: 0.332 [0.256, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.074, 10.428], loss: 0.003530, mae: 0.061082, mean_q: 0.147082
 61869/100000: episode: 1082, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 7.499, mean reward: 0.500 [0.381, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.714, 10.583], loss: 0.003316, mae: 0.061338, mean_q: 0.210998
 61885/100000: episode: 1083, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 7.051, mean reward: 0.441 [0.374, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-1.714, 10.597], loss: 0.003041, mae: 0.059732, mean_q: 0.210791
 61901/100000: episode: 1084, duration: 0.077s, episode steps: 16, steps per second: 208, episode reward: 5.634, mean reward: 0.352 [0.255, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.060, 10.407], loss: 0.002997, mae: 0.057625, mean_q: 0.227187
 61913/100000: episode: 1085, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 4.716, mean reward: 0.393 [0.310, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.971, 10.438], loss: 0.003028, mae: 0.057841, mean_q: 0.167943
 61928/100000: episode: 1086, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 6.792, mean reward: 0.453 [0.358, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.871, 10.610], loss: 0.003230, mae: 0.059935, mean_q: 0.196965
 61939/100000: episode: 1087, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 4.422, mean reward: 0.402 [0.294, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.509], loss: 0.003337, mae: 0.062091, mean_q: 0.297411
 61955/100000: episode: 1088, duration: 0.097s, episode steps: 16, steps per second: 164, episode reward: 5.283, mean reward: 0.330 [0.238, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.620, 10.372], loss: 0.003349, mae: 0.061889, mean_q: 0.238696
 61971/100000: episode: 1089, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 5.085, mean reward: 0.318 [0.147, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.266], loss: 0.003262, mae: 0.061735, mean_q: 0.205214
 61982/100000: episode: 1090, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 4.635, mean reward: 0.421 [0.323, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.239, 10.100], loss: 0.002770, mae: 0.057962, mean_q: 0.202680
 61993/100000: episode: 1091, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 4.704, mean reward: 0.428 [0.358, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.589], loss: 0.003617, mae: 0.063985, mean_q: 0.217241
 62004/100000: episode: 1092, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 4.953, mean reward: 0.450 [0.412, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.572], loss: 0.003376, mae: 0.063256, mean_q: 0.229522
 62016/100000: episode: 1093, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 3.516, mean reward: 0.293 [0.205, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.359], loss: 0.003280, mae: 0.060931, mean_q: 0.243383
 62027/100000: episode: 1094, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 5.720, mean reward: 0.520 [0.454, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.721, 10.100], loss: 0.002800, mae: 0.055617, mean_q: 0.223693
 62051/100000: episode: 1095, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 8.427, mean reward: 0.351 [0.228, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.597, 10.367], loss: 0.003177, mae: 0.059522, mean_q: 0.241594
 62067/100000: episode: 1096, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 6.245, mean reward: 0.390 [0.284, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.452], loss: 0.002970, mae: 0.056212, mean_q: 0.203641
 62081/100000: episode: 1097, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 6.888, mean reward: 0.492 [0.415, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-1.435, 10.527], loss: 0.003299, mae: 0.061404, mean_q: 0.292631
 62097/100000: episode: 1098, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 7.339, mean reward: 0.459 [0.402, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.117, 10.598], loss: 0.002960, mae: 0.057288, mean_q: 0.223394
 62132/100000: episode: 1099, duration: 0.181s, episode steps: 35, steps per second: 194, episode reward: 12.136, mean reward: 0.347 [0.216, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.810, 10.467], loss: 0.002853, mae: 0.056589, mean_q: 0.306572
 62143/100000: episode: 1100, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 5.448, mean reward: 0.495 [0.391, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.643], loss: 0.003402, mae: 0.061975, mean_q: 0.269063
 62158/100000: episode: 1101, duration: 0.081s, episode steps: 15, steps per second: 184, episode reward: 6.024, mean reward: 0.402 [0.245, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.985, 10.370], loss: 0.002683, mae: 0.056905, mean_q: 0.246486
 62173/100000: episode: 1102, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 6.859, mean reward: 0.457 [0.334, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.649, 10.524], loss: 0.003440, mae: 0.062686, mean_q: 0.296245
 62184/100000: episode: 1103, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 5.033, mean reward: 0.458 [0.401, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.490, 10.100], loss: 0.005445, mae: 0.068152, mean_q: 0.295881
 62199/100000: episode: 1104, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 6.305, mean reward: 0.420 [0.362, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.103, 10.530], loss: 0.003320, mae: 0.063157, mean_q: 0.304529
 62214/100000: episode: 1105, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 6.474, mean reward: 0.432 [0.336, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.542, 10.510], loss: 0.002966, mae: 0.058900, mean_q: 0.302112
 62229/100000: episode: 1106, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 6.528, mean reward: 0.435 [0.390, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.439], loss: 0.003163, mae: 0.062040, mean_q: 0.378553
 62253/100000: episode: 1107, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 9.267, mean reward: 0.386 [0.211, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.298, 10.354], loss: 0.003116, mae: 0.059100, mean_q: 0.276234
 62269/100000: episode: 1108, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 7.010, mean reward: 0.438 [0.336, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.539], loss: 0.003013, mae: 0.058613, mean_q: 0.247846
 62280/100000: episode: 1109, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 5.189, mean reward: 0.472 [0.441, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.425, 10.100], loss: 0.003580, mae: 0.064016, mean_q: 0.283587
 62296/100000: episode: 1110, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 5.247, mean reward: 0.328 [0.221, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-1.004, 10.358], loss: 0.003467, mae: 0.063383, mean_q: 0.266976
 62307/100000: episode: 1111, duration: 0.071s, episode steps: 11, steps per second: 154, episode reward: 5.774, mean reward: 0.525 [0.389, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.488, 10.100], loss: 0.003210, mae: 0.060041, mean_q: 0.361362
 62318/100000: episode: 1112, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 5.184, mean reward: 0.471 [0.398, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.469], loss: 0.003316, mae: 0.059810, mean_q: 0.302876
 62353/100000: episode: 1113, duration: 0.182s, episode steps: 35, steps per second: 193, episode reward: 6.593, mean reward: 0.188 [0.022, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.231, 10.162], loss: 0.003433, mae: 0.061894, mean_q: 0.331179
 62367/100000: episode: 1114, duration: 0.072s, episode steps: 14, steps per second: 196, episode reward: 5.561, mean reward: 0.397 [0.309, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-1.118, 10.351], loss: 0.003345, mae: 0.063024, mean_q: 0.351100
 62391/100000: episode: 1115, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 7.845, mean reward: 0.327 [0.230, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.420], loss: 0.003804, mae: 0.064878, mean_q: 0.370798
 62404/100000: episode: 1116, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 5.688, mean reward: 0.438 [0.332, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.470], loss: 0.006127, mae: 0.074671, mean_q: 0.315562
 62439/100000: episode: 1117, duration: 0.207s, episode steps: 35, steps per second: 169, episode reward: 12.330, mean reward: 0.352 [0.174, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.785, 10.364], loss: 0.004195, mae: 0.071164, mean_q: 0.360680
 62450/100000: episode: 1118, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 4.696, mean reward: 0.427 [0.361, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.485, 10.100], loss: 0.003576, mae: 0.061189, mean_q: 0.352308
 62461/100000: episode: 1119, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 4.765, mean reward: 0.433 [0.396, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.495, 10.484], loss: 0.006695, mae: 0.073550, mean_q: 0.249948
 62475/100000: episode: 1120, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 6.365, mean reward: 0.455 [0.378, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.538], loss: 0.007204, mae: 0.077245, mean_q: 0.274253
 62486/100000: episode: 1121, duration: 0.086s, episode steps: 11, steps per second: 127, episode reward: 4.328, mean reward: 0.393 [0.347, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.720, 10.466], loss: 0.006091, mae: 0.073394, mean_q: 0.375258
 62497/100000: episode: 1122, duration: 0.077s, episode steps: 11, steps per second: 143, episode reward: 5.444, mean reward: 0.495 [0.470, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.380, 10.100], loss: 0.003703, mae: 0.066105, mean_q: 0.354422
 62521/100000: episode: 1123, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 10.544, mean reward: 0.439 [0.325, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.957, 10.546], loss: 0.004036, mae: 0.064839, mean_q: 0.375629
 62532/100000: episode: 1124, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 4.876, mean reward: 0.443 [0.391, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.324, 10.501], loss: 0.003836, mae: 0.066755, mean_q: 0.327980
 62547/100000: episode: 1125, duration: 0.099s, episode steps: 15, steps per second: 152, episode reward: 6.565, mean reward: 0.438 [0.350, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.573], loss: 0.004288, mae: 0.066615, mean_q: 0.421089
 62558/100000: episode: 1126, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 4.537, mean reward: 0.412 [0.306, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.820, 10.428], loss: 0.003479, mae: 0.063764, mean_q: 0.370365
[Info] 300-TH LEVEL FOUND: 0.9439021348953247, Considering 10/90 traces
 62571/100000: episode: 1127, duration: 4.739s, episode steps: 13, steps per second: 3, episode reward: 6.527, mean reward: 0.502 [0.418, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.699], loss: 0.005214, mae: 0.068047, mean_q: 0.349134
 62586/100000: episode: 1128, duration: 0.113s, episode steps: 15, steps per second: 133, episode reward: 6.353, mean reward: 0.424 [0.360, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.568], loss: 0.003508, mae: 0.063684, mean_q: 0.377746
 62601/100000: episode: 1129, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 6.104, mean reward: 0.407 [0.351, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.244, 10.460], loss: 0.002954, mae: 0.058084, mean_q: 0.408586
 62610/100000: episode: 1130, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 4.340, mean reward: 0.482 [0.391, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.378, 10.100], loss: 0.002877, mae: 0.058081, mean_q: 0.400114
 62619/100000: episode: 1131, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 4.576, mean reward: 0.508 [0.460, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.159, 10.100], loss: 0.003591, mae: 0.061562, mean_q: 0.395732
 62630/100000: episode: 1132, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 4.839, mean reward: 0.440 [0.377, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.483], loss: 0.003723, mae: 0.065548, mean_q: 0.405682
 62641/100000: episode: 1133, duration: 0.055s, episode steps: 11, steps per second: 198, episode reward: 5.296, mean reward: 0.481 [0.438, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.504, 10.561], loss: 0.004247, mae: 0.070193, mean_q: 0.434864
 62656/100000: episode: 1134, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 6.060, mean reward: 0.404 [0.355, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.480], loss: 0.003850, mae: 0.064782, mean_q: 0.397266
 62666/100000: episode: 1135, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 5.153, mean reward: 0.515 [0.434, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.505, 10.100], loss: 0.005303, mae: 0.070963, mean_q: 0.339997
 62673/100000: episode: 1136, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 3.622, mean reward: 0.517 [0.478, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.371, 10.502], loss: 0.002784, mae: 0.058742, mean_q: 0.384677
 62685/100000: episode: 1137, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 5.903, mean reward: 0.492 [0.340, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.570], loss: 0.003620, mae: 0.065595, mean_q: 0.424772
 62694/100000: episode: 1138, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 4.151, mean reward: 0.461 [0.415, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.529], loss: 0.005551, mae: 0.068250, mean_q: 0.371927
 62705/100000: episode: 1139, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 5.031, mean reward: 0.457 [0.414, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-1.175, 10.492], loss: 0.005883, mae: 0.078148, mean_q: 0.409529
 62715/100000: episode: 1140, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 5.333, mean reward: 0.533 [0.415, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.314, 10.100], loss: 0.003699, mae: 0.066033, mean_q: 0.394356
 62737/100000: episode: 1141, duration: 0.152s, episode steps: 22, steps per second: 145, episode reward: 8.876, mean reward: 0.403 [0.239, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.427], loss: 0.003735, mae: 0.063610, mean_q: 0.438314
[Info] FALSIFICATION!
 62740/100000: episode: 1142, duration: 0.023s, episode steps: 3, steps per second: 132, episode reward: 10.962, mean reward: 3.654 [0.463, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.201, 9.988], loss: 0.003389, mae: 0.061226, mean_q: 0.502772
 62840/100000: episode: 1143, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -20.086, mean reward: -0.201 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.119, 10.166], loss: 0.018174, mae: 0.076795, mean_q: 0.441232
 62940/100000: episode: 1144, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -9.585, mean reward: -0.096 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.111, 10.098], loss: 0.018025, mae: 0.077567, mean_q: 0.414428
 63040/100000: episode: 1145, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.059, mean reward: -0.131 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.845, 10.150], loss: 0.017524, mae: 0.072928, mean_q: 0.442002
 63140/100000: episode: 1146, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.537, mean reward: -0.145 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.455, 10.221], loss: 0.017358, mae: 0.072941, mean_q: 0.425466
 63240/100000: episode: 1147, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.486, mean reward: -0.145 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.571, 10.245], loss: 0.031822, mae: 0.088450, mean_q: 0.439817
 63340/100000: episode: 1148, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -14.391, mean reward: -0.144 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.553, 10.098], loss: 0.003722, mae: 0.064635, mean_q: 0.408385
 63440/100000: episode: 1149, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -10.920, mean reward: -0.109 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.722, 10.098], loss: 0.003791, mae: 0.064915, mean_q: 0.439452
 63540/100000: episode: 1150, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -17.091, mean reward: -0.171 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.046, 10.175], loss: 0.017155, mae: 0.072633, mean_q: 0.438656
 63640/100000: episode: 1151, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -16.428, mean reward: -0.164 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.775, 10.304], loss: 0.017254, mae: 0.071760, mean_q: 0.429511
 63740/100000: episode: 1152, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: -6.331, mean reward: -0.063 [-1.000, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.742, 10.677], loss: 0.016679, mae: 0.068401, mean_q: 0.421657
 63840/100000: episode: 1153, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -20.256, mean reward: -0.203 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.675, 10.288], loss: 0.004301, mae: 0.070073, mean_q: 0.395152
 63940/100000: episode: 1154, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.587, mean reward: -0.196 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.574, 10.118], loss: 0.003992, mae: 0.066478, mean_q: 0.418653
 64040/100000: episode: 1155, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -17.302, mean reward: -0.173 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.092, 10.122], loss: 0.017412, mae: 0.074284, mean_q: 0.396340
 64140/100000: episode: 1156, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -14.559, mean reward: -0.146 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.673, 10.098], loss: 0.030596, mae: 0.083313, mean_q: 0.396318
 64240/100000: episode: 1157, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.264, mean reward: -0.183 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.183, 10.250], loss: 0.003705, mae: 0.063955, mean_q: 0.385324
 64340/100000: episode: 1158, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.256, mean reward: -0.193 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.717, 10.190], loss: 0.017294, mae: 0.073170, mean_q: 0.364299
 64440/100000: episode: 1159, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.230, mean reward: -0.182 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.868, 10.098], loss: 0.017094, mae: 0.071868, mean_q: 0.357608
 64540/100000: episode: 1160, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -14.936, mean reward: -0.149 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.349, 10.098], loss: 0.016914, mae: 0.069672, mean_q: 0.321823
 64640/100000: episode: 1161, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -16.181, mean reward: -0.162 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.489, 10.098], loss: 0.017303, mae: 0.074064, mean_q: 0.315075
 64740/100000: episode: 1162, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.675, mean reward: -0.167 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.588, 10.098], loss: 0.003585, mae: 0.061787, mean_q: 0.281443
 64840/100000: episode: 1163, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.216, mean reward: -0.172 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.056, 10.221], loss: 0.003508, mae: 0.062449, mean_q: 0.273100
 64940/100000: episode: 1164, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.628, mean reward: -0.176 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.849, 10.225], loss: 0.003265, mae: 0.059445, mean_q: 0.231176
 65040/100000: episode: 1165, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.211, mean reward: -0.172 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.768, 10.190], loss: 0.030339, mae: 0.076731, mean_q: 0.245163
 65140/100000: episode: 1166, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -13.140, mean reward: -0.131 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.423, 10.098], loss: 0.003324, mae: 0.060128, mean_q: 0.236378
 65240/100000: episode: 1167, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.636, mean reward: -0.196 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.054, 10.098], loss: 0.003281, mae: 0.059152, mean_q: 0.186592
 65340/100000: episode: 1168, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -10.729, mean reward: -0.107 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.240, 10.098], loss: 0.016292, mae: 0.063433, mean_q: 0.170189
 65440/100000: episode: 1169, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.108, mean reward: -0.161 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.594, 10.098], loss: 0.016585, mae: 0.067733, mean_q: 0.158412
 65540/100000: episode: 1170, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -14.446, mean reward: -0.144 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.872, 10.374], loss: 0.016265, mae: 0.064045, mean_q: 0.156474
 65640/100000: episode: 1171, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -14.883, mean reward: -0.149 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.204, 10.442], loss: 0.016473, mae: 0.066642, mean_q: 0.098373
 65740/100000: episode: 1172, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -19.895, mean reward: -0.199 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.028, 10.232], loss: 0.016984, mae: 0.070622, mean_q: 0.123244
 65840/100000: episode: 1173, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -10.091, mean reward: -0.101 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.900, 10.235], loss: 0.016221, mae: 0.064020, mean_q: 0.115866
 65940/100000: episode: 1174, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.347, mean reward: -0.173 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.716, 10.137], loss: 0.016749, mae: 0.068035, mean_q: 0.105078
 66040/100000: episode: 1175, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.818, mean reward: -0.188 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.271, 10.098], loss: 0.003391, mae: 0.059252, mean_q: 0.077575
 66140/100000: episode: 1176, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.468, mean reward: -0.165 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.439, 10.098], loss: 0.030352, mae: 0.079522, mean_q: 0.056700
 66240/100000: episode: 1177, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.746, mean reward: -0.187 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.136, 10.248], loss: 0.029931, mae: 0.080252, mean_q: -0.007051
 66340/100000: episode: 1178, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.959, mean reward: -0.170 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.423, 10.343], loss: 0.017517, mae: 0.072600, mean_q: 0.005634
 66440/100000: episode: 1179, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -20.439, mean reward: -0.204 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.721, 10.098], loss: 0.003097, mae: 0.056036, mean_q: -0.029093
 66540/100000: episode: 1180, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -12.945, mean reward: -0.129 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.245, 10.098], loss: 0.016562, mae: 0.065385, mean_q: -0.062297
 66640/100000: episode: 1181, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.693, mean reward: -0.147 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.510, 10.098], loss: 0.002822, mae: 0.052472, mean_q: -0.064350
 66740/100000: episode: 1182, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.273, mean reward: -0.163 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.501, 10.098], loss: 0.003036, mae: 0.054189, mean_q: -0.094015
 66840/100000: episode: 1183, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.308, mean reward: -0.193 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.305, 10.098], loss: 0.002897, mae: 0.054316, mean_q: -0.112302
 66940/100000: episode: 1184, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.238, mean reward: -0.192 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.548, 10.255], loss: 0.016253, mae: 0.062253, mean_q: -0.131553
 67040/100000: episode: 1185, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.549, mean reward: -0.145 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.306, 10.214], loss: 0.002728, mae: 0.051638, mean_q: -0.149325
 67140/100000: episode: 1186, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.820, mean reward: -0.158 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.424, 10.168], loss: 0.029845, mae: 0.069886, mean_q: -0.192659
 67240/100000: episode: 1187, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.069, mean reward: -0.191 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.843, 10.098], loss: 0.003064, mae: 0.053352, mean_q: -0.235679
 67340/100000: episode: 1188, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -10.302, mean reward: -0.103 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.249, 10.098], loss: 0.028643, mae: 0.060579, mean_q: -0.236412
 67440/100000: episode: 1189, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -15.819, mean reward: -0.158 [-1.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.357, 10.098], loss: 0.017525, mae: 0.071591, mean_q: -0.212160
 67540/100000: episode: 1190, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -15.579, mean reward: -0.156 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.243, 10.098], loss: 0.015354, mae: 0.057048, mean_q: -0.268234
 67640/100000: episode: 1191, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -16.942, mean reward: -0.169 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.621, 10.186], loss: 0.016717, mae: 0.068845, mean_q: -0.288754
 67740/100000: episode: 1192, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -13.765, mean reward: -0.138 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.023, 10.359], loss: 0.002587, mae: 0.049775, mean_q: -0.313803
 67840/100000: episode: 1193, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.383, mean reward: -0.184 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.350, 10.098], loss: 0.002662, mae: 0.050534, mean_q: -0.303355
 67940/100000: episode: 1194, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.936, mean reward: -0.159 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.814, 10.098], loss: 0.002659, mae: 0.049954, mean_q: -0.307104
 68040/100000: episode: 1195, duration: 0.480s, episode steps: 100, steps per second: 209, episode reward: -16.056, mean reward: -0.161 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.643, 10.109], loss: 0.002717, mae: 0.051993, mean_q: -0.299534
 68140/100000: episode: 1196, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.910, mean reward: -0.179 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.688, 10.098], loss: 0.002545, mae: 0.049922, mean_q: -0.313498
 68240/100000: episode: 1197, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -18.063, mean reward: -0.181 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.753, 10.110], loss: 0.002537, mae: 0.050168, mean_q: -0.294689
 68340/100000: episode: 1198, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.834, mean reward: -0.188 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.759, 10.098], loss: 0.002822, mae: 0.052509, mean_q: -0.302945
 68440/100000: episode: 1199, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -19.471, mean reward: -0.195 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.306, 10.154], loss: 0.002530, mae: 0.049654, mean_q: -0.312746
 68540/100000: episode: 1200, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.587, mean reward: -0.166 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.838, 10.143], loss: 0.002638, mae: 0.050655, mean_q: -0.312171
 68640/100000: episode: 1201, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.442, mean reward: -0.184 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.763, 10.118], loss: 0.002586, mae: 0.049905, mean_q: -0.309854
 68740/100000: episode: 1202, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.931, mean reward: -0.179 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.271, 10.098], loss: 0.002512, mae: 0.049774, mean_q: -0.314294
 68840/100000: episode: 1203, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.450, mean reward: -0.155 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.869, 10.200], loss: 0.002568, mae: 0.049468, mean_q: -0.310954
 68940/100000: episode: 1204, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -20.578, mean reward: -0.206 [-1.000, 0.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.537, 10.098], loss: 0.002429, mae: 0.048822, mean_q: -0.301149
 69040/100000: episode: 1205, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.685, mean reward: -0.167 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.443, 10.377], loss: 0.002564, mae: 0.049351, mean_q: -0.342719
 69140/100000: episode: 1206, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -12.456, mean reward: -0.125 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.755, 10.098], loss: 0.002331, mae: 0.046875, mean_q: -0.352546
 69240/100000: episode: 1207, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.442, mean reward: -0.184 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.771, 10.098], loss: 0.002436, mae: 0.049321, mean_q: -0.305616
 69340/100000: episode: 1208, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -12.138, mean reward: -0.121 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.319, 10.098], loss: 0.002457, mae: 0.048509, mean_q: -0.300967
 69440/100000: episode: 1209, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -18.277, mean reward: -0.183 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.253, 10.170], loss: 0.002481, mae: 0.048991, mean_q: -0.307489
 69540/100000: episode: 1210, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.584, mean reward: -0.186 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.452, 10.098], loss: 0.002599, mae: 0.050873, mean_q: -0.299418
 69640/100000: episode: 1211, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.143, mean reward: -0.181 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.001, 10.198], loss: 0.002465, mae: 0.049168, mean_q: -0.320920
 69740/100000: episode: 1212, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -13.148, mean reward: -0.131 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.608, 10.308], loss: 0.002519, mae: 0.048352, mean_q: -0.325950
 69840/100000: episode: 1213, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.179, mean reward: -0.152 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.867, 10.400], loss: 0.002482, mae: 0.048980, mean_q: -0.318672
 69940/100000: episode: 1214, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -14.797, mean reward: -0.148 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.882, 10.098], loss: 0.002451, mae: 0.049024, mean_q: -0.302289
 70040/100000: episode: 1215, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.142, mean reward: -0.171 [-1.000, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.403, 10.154], loss: 0.002660, mae: 0.050048, mean_q: -0.294704
 70140/100000: episode: 1216, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -12.559, mean reward: -0.126 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.688, 10.098], loss: 0.002485, mae: 0.048807, mean_q: -0.314264
 70240/100000: episode: 1217, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.927, mean reward: -0.189 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.524, 10.255], loss: 0.002562, mae: 0.050273, mean_q: -0.334454
 70340/100000: episode: 1218, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -17.658, mean reward: -0.177 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.612, 10.098], loss: 0.002536, mae: 0.049071, mean_q: -0.315436
 70440/100000: episode: 1219, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -18.417, mean reward: -0.184 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.739, 10.310], loss: 0.003764, mae: 0.056991, mean_q: -0.325858
 70540/100000: episode: 1220, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -5.111, mean reward: -0.051 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.907, 10.495], loss: 0.002544, mae: 0.050753, mean_q: -0.305712
 70640/100000: episode: 1221, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.373, mean reward: -0.164 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.156, 10.147], loss: 0.002401, mae: 0.047795, mean_q: -0.321379
 70740/100000: episode: 1222, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.634, mean reward: -0.166 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.076, 10.098], loss: 0.002525, mae: 0.050117, mean_q: -0.318966
 70840/100000: episode: 1223, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -11.442, mean reward: -0.114 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.095, 10.480], loss: 0.002680, mae: 0.051242, mean_q: -0.308109
 70940/100000: episode: 1224, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -14.932, mean reward: -0.149 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.711, 10.290], loss: 0.002394, mae: 0.049361, mean_q: -0.325778
 71040/100000: episode: 1225, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -11.980, mean reward: -0.120 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.609, 10.098], loss: 0.002624, mae: 0.050466, mean_q: -0.336910
 71140/100000: episode: 1226, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.258, mean reward: -0.163 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.302, 10.099], loss: 0.002800, mae: 0.053075, mean_q: -0.271777
 71240/100000: episode: 1227, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.480, mean reward: -0.195 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.142, 10.186], loss: 0.002665, mae: 0.051200, mean_q: -0.321220
 71340/100000: episode: 1228, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -15.885, mean reward: -0.159 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.154, 10.215], loss: 0.002842, mae: 0.054322, mean_q: -0.282293
 71440/100000: episode: 1229, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.857, mean reward: -0.179 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.314, 10.203], loss: 0.002632, mae: 0.050810, mean_q: -0.278295
 71540/100000: episode: 1230, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.801, mean reward: -0.168 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.276, 10.098], loss: 0.002676, mae: 0.052170, mean_q: -0.283760
 71640/100000: episode: 1231, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.791, mean reward: -0.188 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.296, 10.112], loss: 0.002515, mae: 0.049301, mean_q: -0.333836
 71740/100000: episode: 1232, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.154, mean reward: -0.172 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.970, 10.362], loss: 0.002599, mae: 0.050682, mean_q: -0.324446
 71840/100000: episode: 1233, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.997, mean reward: -0.190 [-1.000, 0.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.866, 10.153], loss: 0.002676, mae: 0.051933, mean_q: -0.301568
 71940/100000: episode: 1234, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.373, mean reward: -0.174 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.933, 10.098], loss: 0.002652, mae: 0.050204, mean_q: -0.328617
 72040/100000: episode: 1235, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.247, mean reward: -0.162 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.687, 10.108], loss: 0.002697, mae: 0.051939, mean_q: -0.278300
 72140/100000: episode: 1236, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.722, mean reward: -0.167 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.673, 10.256], loss: 0.002998, mae: 0.054214, mean_q: -0.293264
 72240/100000: episode: 1237, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -16.492, mean reward: -0.165 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.329, 10.363], loss: 0.004911, mae: 0.067818, mean_q: -0.291542
 72340/100000: episode: 1238, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.496, mean reward: -0.165 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.537, 10.234], loss: 0.002605, mae: 0.051531, mean_q: -0.307677
 72440/100000: episode: 1239, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.394, mean reward: -0.184 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.709, 10.232], loss: 0.002560, mae: 0.049624, mean_q: -0.328362
 72540/100000: episode: 1240, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -20.209, mean reward: -0.202 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.450, 10.114], loss: 0.002818, mae: 0.051992, mean_q: -0.290397
 72640/100000: episode: 1241, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.597, mean reward: -0.166 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.776, 10.098], loss: 0.002543, mae: 0.050201, mean_q: -0.316122
[Info] 100-TH LEVEL FOUND: 0.6951016187667847, Considering 10/90 traces
 72740/100000: episode: 1242, duration: 4.581s, episode steps: 100, steps per second: 22, episode reward: -16.349, mean reward: -0.163 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.902, 10.098], loss: 0.002602, mae: 0.051095, mean_q: -0.301449
 72766/100000: episode: 1243, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 7.599, mean reward: 0.292 [0.222, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.317, 10.100], loss: 0.002835, mae: 0.053427, mean_q: -0.307880
 72804/100000: episode: 1244, duration: 0.187s, episode steps: 38, steps per second: 203, episode reward: 12.657, mean reward: 0.333 [0.235, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.486, 10.436], loss: 0.002467, mae: 0.049275, mean_q: -0.245229
 72822/100000: episode: 1245, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 5.587, mean reward: 0.310 [0.225, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.365, 10.100], loss: 0.002636, mae: 0.052297, mean_q: -0.234552
 72860/100000: episode: 1246, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 9.709, mean reward: 0.256 [0.155, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.927, 10.318], loss: 0.002443, mae: 0.049735, mean_q: -0.329577
 72886/100000: episode: 1247, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 8.835, mean reward: 0.340 [0.096, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.054, 10.100], loss: 0.002929, mae: 0.055259, mean_q: -0.352623
 72902/100000: episode: 1248, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 5.373, mean reward: 0.336 [0.225, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.116, 10.100], loss: 0.002498, mae: 0.052192, mean_q: -0.267120
 72926/100000: episode: 1249, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 6.957, mean reward: 0.290 [0.139, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.649, 10.100], loss: 0.002667, mae: 0.052656, mean_q: -0.231437
 72971/100000: episode: 1250, duration: 0.241s, episode steps: 45, steps per second: 187, episode reward: 13.641, mean reward: 0.303 [0.142, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-1.155, 10.295], loss: 0.002814, mae: 0.052959, mean_q: -0.257541
 72997/100000: episode: 1251, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 9.837, mean reward: 0.378 [0.255, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.326, 10.100], loss: 0.002500, mae: 0.050107, mean_q: -0.225500
 73042/100000: episode: 1252, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 12.842, mean reward: 0.285 [0.116, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.241, 10.377], loss: 0.002634, mae: 0.050858, mean_q: -0.243377
 73080/100000: episode: 1253, duration: 0.189s, episode steps: 38, steps per second: 201, episode reward: 18.334, mean reward: 0.482 [0.335, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.974, 10.582], loss: 0.002546, mae: 0.051250, mean_q: -0.256464
 73118/100000: episode: 1254, duration: 0.282s, episode steps: 38, steps per second: 135, episode reward: 8.972, mean reward: 0.236 [0.035, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.919, 10.139], loss: 0.002655, mae: 0.052688, mean_q: -0.210086
 73134/100000: episode: 1255, duration: 0.138s, episode steps: 16, steps per second: 116, episode reward: 5.523, mean reward: 0.345 [0.283, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.354, 10.100], loss: 0.002582, mae: 0.051778, mean_q: -0.273691
 73158/100000: episode: 1256, duration: 0.197s, episode steps: 24, steps per second: 122, episode reward: 9.130, mean reward: 0.380 [0.313, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.314, 10.100], loss: 0.003172, mae: 0.055732, mean_q: -0.239418
 73196/100000: episode: 1257, duration: 0.229s, episode steps: 38, steps per second: 166, episode reward: 12.100, mean reward: 0.318 [0.090, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.107, 10.285], loss: 0.002548, mae: 0.050199, mean_q: -0.229488
 73211/100000: episode: 1258, duration: 0.134s, episode steps: 15, steps per second: 112, episode reward: 5.767, mean reward: 0.384 [0.271, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.170, 10.100], loss: 0.002508, mae: 0.049637, mean_q: -0.282570
 73226/100000: episode: 1259, duration: 0.097s, episode steps: 15, steps per second: 154, episode reward: 5.058, mean reward: 0.337 [0.241, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.423], loss: 0.003134, mae: 0.055021, mean_q: -0.163265
 73242/100000: episode: 1260, duration: 0.100s, episode steps: 16, steps per second: 161, episode reward: 5.575, mean reward: 0.348 [0.240, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.525, 10.100], loss: 0.002617, mae: 0.050612, mean_q: -0.196141
 73266/100000: episode: 1261, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 9.687, mean reward: 0.404 [0.275, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.962, 10.100], loss: 0.002706, mae: 0.052077, mean_q: -0.228845
 73281/100000: episode: 1262, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 5.319, mean reward: 0.355 [0.273, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.940, 10.100], loss: 0.002681, mae: 0.052207, mean_q: -0.244787
 73319/100000: episode: 1263, duration: 0.222s, episode steps: 38, steps per second: 171, episode reward: 11.307, mean reward: 0.298 [0.121, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.298, 10.286], loss: 0.002586, mae: 0.051548, mean_q: -0.209279
 73357/100000: episode: 1264, duration: 0.300s, episode steps: 38, steps per second: 126, episode reward: 8.496, mean reward: 0.224 [0.093, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.238, 10.241], loss: 0.003009, mae: 0.053970, mean_q: -0.217387
 73373/100000: episode: 1265, duration: 0.233s, episode steps: 16, steps per second: 69, episode reward: 3.537, mean reward: 0.221 [0.121, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.110, 10.100], loss: 0.002622, mae: 0.051447, mean_q: -0.171484
 73389/100000: episode: 1266, duration: 0.134s, episode steps: 16, steps per second: 119, episode reward: 5.518, mean reward: 0.345 [0.268, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.316, 10.100], loss: 0.002884, mae: 0.052733, mean_q: -0.196295
 73404/100000: episode: 1267, duration: 0.127s, episode steps: 15, steps per second: 118, episode reward: 4.922, mean reward: 0.328 [0.216, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.336], loss: 0.002492, mae: 0.049661, mean_q: -0.199545
 73442/100000: episode: 1268, duration: 0.300s, episode steps: 38, steps per second: 126, episode reward: 15.855, mean reward: 0.417 [0.319, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.306, 10.582], loss: 0.002622, mae: 0.051673, mean_q: -0.143058
 73487/100000: episode: 1269, duration: 0.332s, episode steps: 45, steps per second: 136, episode reward: 13.844, mean reward: 0.308 [0.193, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.876, 10.340], loss: 0.002735, mae: 0.053520, mean_q: -0.161047
 73525/100000: episode: 1270, duration: 0.249s, episode steps: 38, steps per second: 153, episode reward: 10.049, mean reward: 0.264 [0.063, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.604, 10.230], loss: 0.002774, mae: 0.053417, mean_q: -0.126019
 73543/100000: episode: 1271, duration: 0.138s, episode steps: 18, steps per second: 130, episode reward: 4.558, mean reward: 0.253 [0.194, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.253, 10.100], loss: 0.002856, mae: 0.056011, mean_q: -0.141150
 73552/100000: episode: 1272, duration: 0.066s, episode steps: 9, steps per second: 136, episode reward: 2.825, mean reward: 0.314 [0.250, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.348, 10.100], loss: 0.002177, mae: 0.048644, mean_q: -0.199884
 73567/100000: episode: 1273, duration: 0.155s, episode steps: 15, steps per second: 97, episode reward: 5.538, mean reward: 0.369 [0.269, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.369, 10.100], loss: 0.002710, mae: 0.052315, mean_q: -0.120176
 73576/100000: episode: 1274, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 2.890, mean reward: 0.321 [0.263, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.276, 10.100], loss: 0.002527, mae: 0.047080, mean_q: -0.228842
 73600/100000: episode: 1275, duration: 0.144s, episode steps: 24, steps per second: 167, episode reward: 6.762, mean reward: 0.282 [0.100, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.205, 10.100], loss: 0.002680, mae: 0.052227, mean_q: -0.206999
 73615/100000: episode: 1276, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 4.768, mean reward: 0.318 [0.243, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.266, 10.100], loss: 0.002954, mae: 0.055886, mean_q: -0.180099
 73641/100000: episode: 1277, duration: 0.134s, episode steps: 26, steps per second: 193, episode reward: 10.235, mean reward: 0.394 [0.294, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.571, 10.100], loss: 0.002537, mae: 0.052191, mean_q: -0.173175
 73656/100000: episode: 1278, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 5.769, mean reward: 0.385 [0.306, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.239, 10.372], loss: 0.002285, mae: 0.049720, mean_q: -0.140726
 73701/100000: episode: 1279, duration: 0.315s, episode steps: 45, steps per second: 143, episode reward: 13.004, mean reward: 0.289 [0.173, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.298, 10.422], loss: 0.002614, mae: 0.053777, mean_q: -0.144250
 73716/100000: episode: 1280, duration: 0.097s, episode steps: 15, steps per second: 155, episode reward: 5.337, mean reward: 0.356 [0.283, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.655, 10.100], loss: 0.002654, mae: 0.050518, mean_q: -0.116537
 73754/100000: episode: 1281, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 15.252, mean reward: 0.401 [0.194, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.133, 10.582], loss: 0.002717, mae: 0.052897, mean_q: -0.117011
 73778/100000: episode: 1282, duration: 0.164s, episode steps: 24, steps per second: 146, episode reward: 8.652, mean reward: 0.361 [0.319, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.372, 10.100], loss: 0.002798, mae: 0.054354, mean_q: -0.073518
 73793/100000: episode: 1283, duration: 0.119s, episode steps: 15, steps per second: 126, episode reward: 7.172, mean reward: 0.478 [0.328, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-1.741, 10.642], loss: 0.002574, mae: 0.052849, mean_q: -0.181604
 73817/100000: episode: 1284, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 9.402, mean reward: 0.392 [0.294, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.154, 10.100], loss: 0.002629, mae: 0.053473, mean_q: -0.049989
 73835/100000: episode: 1285, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 5.948, mean reward: 0.330 [0.182, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.303, 10.100], loss: 0.002822, mae: 0.054376, mean_q: -0.056711
 73880/100000: episode: 1286, duration: 0.268s, episode steps: 45, steps per second: 168, episode reward: 13.517, mean reward: 0.300 [0.203, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-1.348, 10.441], loss: 0.002753, mae: 0.053206, mean_q: -0.131061
 73889/100000: episode: 1287, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 2.509, mean reward: 0.279 [0.195, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.387, 10.100], loss: 0.002918, mae: 0.057396, mean_q: -0.034557
 73904/100000: episode: 1288, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 4.398, mean reward: 0.293 [0.185, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.165, 10.100], loss: 0.003357, mae: 0.061012, mean_q: -0.052269
 73919/100000: episode: 1289, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 5.040, mean reward: 0.336 [0.241, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.334, 10.314], loss: 0.003571, mae: 0.061926, mean_q: -0.120898
 73964/100000: episode: 1290, duration: 0.256s, episode steps: 45, steps per second: 176, episode reward: 20.006, mean reward: 0.445 [0.268, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-1.472, 10.474], loss: 0.003486, mae: 0.061005, mean_q: -0.107492
 74002/100000: episode: 1291, duration: 0.262s, episode steps: 38, steps per second: 145, episode reward: 13.248, mean reward: 0.349 [0.260, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.077, 10.422], loss: 0.002623, mae: 0.053684, mean_q: -0.064464
 74026/100000: episode: 1292, duration: 0.171s, episode steps: 24, steps per second: 140, episode reward: 6.834, mean reward: 0.285 [0.194, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-1.057, 10.100], loss: 0.002705, mae: 0.054150, mean_q: 0.007918
 74041/100000: episode: 1293, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 5.892, mean reward: 0.393 [0.282, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.188, 10.100], loss: 0.002875, mae: 0.054744, mean_q: -0.092441
 74065/100000: episode: 1294, duration: 0.148s, episode steps: 24, steps per second: 162, episode reward: 6.222, mean reward: 0.259 [0.137, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.938, 10.100], loss: 0.002577, mae: 0.053054, mean_q: -0.053449
 74103/100000: episode: 1295, duration: 0.251s, episode steps: 38, steps per second: 151, episode reward: 11.893, mean reward: 0.313 [0.225, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.524, 10.310], loss: 0.002936, mae: 0.054159, mean_q: -0.068303
 74119/100000: episode: 1296, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 6.221, mean reward: 0.389 [0.277, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.017, 10.100], loss: 0.002853, mae: 0.055544, mean_q: 0.043979
 74135/100000: episode: 1297, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 5.196, mean reward: 0.325 [0.280, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.262, 10.100], loss: 0.002626, mae: 0.052138, mean_q: -0.041299
 74173/100000: episode: 1298, duration: 0.221s, episode steps: 38, steps per second: 172, episode reward: 12.883, mean reward: 0.339 [0.177, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.703, 10.308], loss: 0.003041, mae: 0.056029, mean_q: -0.044005
 74197/100000: episode: 1299, duration: 0.132s, episode steps: 24, steps per second: 181, episode reward: 7.112, mean reward: 0.296 [0.183, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.171, 10.100], loss: 0.002714, mae: 0.054002, mean_q: 0.017160
 74235/100000: episode: 1300, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 14.490, mean reward: 0.381 [0.187, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.331, 10.397], loss: 0.002586, mae: 0.051975, mean_q: -0.065401
 74259/100000: episode: 1301, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 7.271, mean reward: 0.303 [0.119, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.215, 10.100], loss: 0.002768, mae: 0.051898, mean_q: -0.029707
 74297/100000: episode: 1302, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 8.874, mean reward: 0.234 [0.023, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-1.016, 10.120], loss: 0.002891, mae: 0.055313, mean_q: -0.001179
 74312/100000: episode: 1303, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 5.818, mean reward: 0.388 [0.294, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.452], loss: 0.003295, mae: 0.060069, mean_q: 0.039763
 74330/100000: episode: 1304, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 8.273, mean reward: 0.460 [0.293, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.388, 10.100], loss: 0.002839, mae: 0.056182, mean_q: 0.061969
 74356/100000: episode: 1305, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 10.575, mean reward: 0.407 [0.253, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.372, 10.100], loss: 0.002508, mae: 0.051619, mean_q: -0.033067
 74380/100000: episode: 1306, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 8.739, mean reward: 0.364 [0.133, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.268, 10.100], loss: 0.002946, mae: 0.054982, mean_q: 0.012400
 74398/100000: episode: 1307, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 5.957, mean reward: 0.331 [0.200, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.768, 10.100], loss: 0.003197, mae: 0.058855, mean_q: -0.001407
 74443/100000: episode: 1308, duration: 0.245s, episode steps: 45, steps per second: 184, episode reward: 10.940, mean reward: 0.243 [0.049, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-1.198, 10.234], loss: 0.002656, mae: 0.052407, mean_q: -0.010880
 74469/100000: episode: 1309, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 11.227, mean reward: 0.432 [0.353, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-1.145, 10.100], loss: 0.002733, mae: 0.053847, mean_q: -0.038059
 74487/100000: episode: 1310, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 5.494, mean reward: 0.305 [0.220, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.165, 10.100], loss: 0.002675, mae: 0.053197, mean_q: 0.031917
 74513/100000: episode: 1311, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 10.481, mean reward: 0.403 [0.271, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.999, 10.100], loss: 0.002963, mae: 0.056363, mean_q: 0.097899
 74558/100000: episode: 1312, duration: 0.262s, episode steps: 45, steps per second: 172, episode reward: 8.379, mean reward: 0.186 [0.031, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.726, 10.215], loss: 0.002924, mae: 0.056076, mean_q: 0.043665
 74584/100000: episode: 1313, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 11.019, mean reward: 0.424 [0.364, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.564, 10.100], loss: 0.003275, mae: 0.059553, mean_q: 0.105416
 74622/100000: episode: 1314, duration: 0.208s, episode steps: 38, steps per second: 182, episode reward: 14.242, mean reward: 0.375 [0.156, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.853, 10.289], loss: 0.002766, mae: 0.054657, mean_q: 0.013510
 74631/100000: episode: 1315, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 3.195, mean reward: 0.355 [0.287, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.401, 10.100], loss: 0.002848, mae: 0.055732, mean_q: 0.005374
 74647/100000: episode: 1316, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 4.989, mean reward: 0.312 [0.154, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.480, 10.100], loss: 0.002475, mae: 0.052883, mean_q: 0.144082
 74662/100000: episode: 1317, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 4.896, mean reward: 0.326 [0.243, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.401], loss: 0.003028, mae: 0.057300, mean_q: 0.108157
 74680/100000: episode: 1318, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 5.468, mean reward: 0.304 [0.217, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.466, 10.100], loss: 0.003048, mae: 0.058333, mean_q: 0.141746
 74718/100000: episode: 1319, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 13.388, mean reward: 0.352 [0.234, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-1.263, 10.363], loss: 0.002915, mae: 0.055995, mean_q: 0.094758
 74727/100000: episode: 1320, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 2.469, mean reward: 0.274 [0.223, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.602, 10.100], loss: 0.003233, mae: 0.058703, mean_q: 0.095468
 74751/100000: episode: 1321, duration: 0.132s, episode steps: 24, steps per second: 181, episode reward: 7.968, mean reward: 0.332 [0.088, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.268, 10.100], loss: 0.002701, mae: 0.055027, mean_q: 0.078235
 74766/100000: episode: 1322, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 4.519, mean reward: 0.301 [0.221, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.377], loss: 0.003362, mae: 0.058665, mean_q: 0.110332
 74804/100000: episode: 1323, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 9.548, mean reward: 0.251 [0.056, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.442, 10.233], loss: 0.003385, mae: 0.061138, mean_q: 0.099972
 74849/100000: episode: 1324, duration: 0.279s, episode steps: 45, steps per second: 161, episode reward: 14.919, mean reward: 0.332 [0.110, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.325, 10.230], loss: 0.002886, mae: 0.057654, mean_q: 0.082056
 74865/100000: episode: 1325, duration: 0.129s, episode steps: 16, steps per second: 125, episode reward: 7.458, mean reward: 0.466 [0.343, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.561, 10.100], loss: 0.003245, mae: 0.060071, mean_q: 0.078241
 74880/100000: episode: 1326, duration: 0.099s, episode steps: 15, steps per second: 152, episode reward: 5.188, mean reward: 0.346 [0.188, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.252, 10.100], loss: 0.003680, mae: 0.064454, mean_q: 0.125896
 74918/100000: episode: 1327, duration: 0.275s, episode steps: 38, steps per second: 138, episode reward: 18.167, mean reward: 0.478 [0.351, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-1.001, 10.493], loss: 0.002891, mae: 0.055714, mean_q: 0.106944
 74956/100000: episode: 1328, duration: 0.233s, episode steps: 38, steps per second: 163, episode reward: 13.299, mean reward: 0.350 [0.251, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.083, 10.341], loss: 0.002927, mae: 0.057173, mean_q: 0.178279
 74971/100000: episode: 1329, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 4.864, mean reward: 0.324 [0.244, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.585, 10.100], loss: 0.002761, mae: 0.055834, mean_q: 0.134942
 74987/100000: episode: 1330, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 6.295, mean reward: 0.393 [0.311, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.286, 10.100], loss: 0.002983, mae: 0.056734, mean_q: 0.192322
 75025/100000: episode: 1331, duration: 0.270s, episode steps: 38, steps per second: 140, episode reward: 12.415, mean reward: 0.327 [0.184, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.737, 10.366], loss: 0.002804, mae: 0.054860, mean_q: 0.108729
[Info] 200-TH LEVEL FOUND: 0.9211030006408691, Considering 10/90 traces
 75041/100000: episode: 1332, duration: 4.572s, episode steps: 16, steps per second: 3, episode reward: 5.142, mean reward: 0.321 [0.223, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.369, 10.100], loss: 0.002656, mae: 0.054103, mean_q: 0.124066
[Info] FALSIFICATION!
 75053/100000: episode: 1333, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 15.701, mean reward: 1.308 [0.341, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.019, 10.137], loss: 0.002951, mae: 0.056745, mean_q: 0.146517
 75153/100000: episode: 1334, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.644, mean reward: -0.166 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.964, 10.098], loss: 0.016039, mae: 0.066133, mean_q: 0.144451
 75253/100000: episode: 1335, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.779, mean reward: -0.188 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.376, 10.132], loss: 0.003064, mae: 0.058335, mean_q: 0.127972
 75353/100000: episode: 1336, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -14.387, mean reward: -0.144 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.943, 10.098], loss: 0.016213, mae: 0.066577, mean_q: 0.124187
 75453/100000: episode: 1337, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.319, mean reward: -0.163 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.184, 10.098], loss: 0.002795, mae: 0.055865, mean_q: 0.153204
 75553/100000: episode: 1338, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -18.727, mean reward: -0.187 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.224, 10.098], loss: 0.029424, mae: 0.074209, mean_q: 0.143322
 75653/100000: episode: 1339, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.104, mean reward: -0.151 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.733, 10.349], loss: 0.003162, mae: 0.059351, mean_q: 0.138219
 75753/100000: episode: 1340, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.602, mean reward: -0.166 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.722, 10.098], loss: 0.003021, mae: 0.058438, mean_q: 0.126382
 75853/100000: episode: 1341, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -19.055, mean reward: -0.191 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.598, 10.236], loss: 0.019304, mae: 0.076897, mean_q: 0.151864
 75953/100000: episode: 1342, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -12.523, mean reward: -0.125 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.700, 10.347], loss: 0.003707, mae: 0.063538, mean_q: 0.113675
 76053/100000: episode: 1343, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.658, mean reward: -0.167 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.948, 10.169], loss: 0.015839, mae: 0.064584, mean_q: 0.120776
 76153/100000: episode: 1344, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.734, mean reward: -0.167 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.920, 10.191], loss: 0.028024, mae: 0.070026, mean_q: 0.164978
 76253/100000: episode: 1345, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.548, mean reward: -0.155 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.772, 10.098], loss: 0.002835, mae: 0.055676, mean_q: 0.131069
 76353/100000: episode: 1346, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.230, mean reward: -0.182 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.624, 10.108], loss: 0.002798, mae: 0.055405, mean_q: 0.112948
 76453/100000: episode: 1347, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.430, mean reward: -0.184 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.782, 10.289], loss: 0.016021, mae: 0.067385, mean_q: 0.134212
 76553/100000: episode: 1348, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.772, mean reward: -0.178 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.513, 10.098], loss: 0.029691, mae: 0.078446, mean_q: 0.147750
 76653/100000: episode: 1349, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.385, mean reward: -0.164 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.991, 10.098], loss: 0.003067, mae: 0.058393, mean_q: 0.155287
 76753/100000: episode: 1350, duration: 0.635s, episode steps: 100, steps per second: 158, episode reward: -11.755, mean reward: -0.118 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.726, 10.098], loss: 0.002902, mae: 0.057026, mean_q: 0.141148
 76853/100000: episode: 1351, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -17.328, mean reward: -0.173 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.307, 10.135], loss: 0.002825, mae: 0.055853, mean_q: 0.125763
 76953/100000: episode: 1352, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.199, mean reward: -0.192 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.991, 10.216], loss: 0.015782, mae: 0.064693, mean_q: 0.132417
 77053/100000: episode: 1353, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -14.113, mean reward: -0.141 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.745, 10.098], loss: 0.028447, mae: 0.070574, mean_q: 0.137793
 77153/100000: episode: 1354, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.622, mean reward: -0.166 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.907, 10.098], loss: 0.002943, mae: 0.057886, mean_q: 0.168117
 77253/100000: episode: 1355, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -19.393, mean reward: -0.194 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.680, 10.098], loss: 0.015695, mae: 0.063437, mean_q: 0.141029
 77353/100000: episode: 1356, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -13.838, mean reward: -0.138 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.128, 10.511], loss: 0.002822, mae: 0.056317, mean_q: 0.147423
 77453/100000: episode: 1357, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.524, mean reward: -0.165 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.207, 10.185], loss: 0.015660, mae: 0.063095, mean_q: 0.144394
 77553/100000: episode: 1358, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -12.091, mean reward: -0.121 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.401, 10.098], loss: 0.002851, mae: 0.056495, mean_q: 0.140710
 77653/100000: episode: 1359, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -20.780, mean reward: -0.208 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.157, 10.262], loss: 0.015758, mae: 0.064786, mean_q: 0.149009
 77753/100000: episode: 1360, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -7.844, mean reward: -0.078 [-1.000, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.393, 10.443], loss: 0.015631, mae: 0.063339, mean_q: 0.101378
 77853/100000: episode: 1361, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.741, mean reward: -0.177 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.104, 10.246], loss: 0.015838, mae: 0.065038, mean_q: 0.104527
 77953/100000: episode: 1362, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -17.098, mean reward: -0.171 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.478, 10.125], loss: 0.003492, mae: 0.060571, mean_q: 0.083470
 78053/100000: episode: 1363, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.329, mean reward: -0.153 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.379, 10.098], loss: 0.002791, mae: 0.055095, mean_q: 0.041463
 78153/100000: episode: 1364, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.231, mean reward: -0.172 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.253, 10.098], loss: 0.016605, mae: 0.069950, mean_q: 0.083723
 78253/100000: episode: 1365, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.758, mean reward: -0.188 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.518, 10.267], loss: 0.002965, mae: 0.057685, mean_q: 0.051453
 78353/100000: episode: 1366, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -19.952, mean reward: -0.200 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.063, 10.099], loss: 0.015625, mae: 0.062079, mean_q: -0.004128
 78453/100000: episode: 1367, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.141, mean reward: -0.131 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.445, 10.167], loss: 0.015444, mae: 0.060784, mean_q: 0.011608
 78553/100000: episode: 1368, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -17.370, mean reward: -0.174 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.222, 10.364], loss: 0.041491, mae: 0.081413, mean_q: -0.014094
 78653/100000: episode: 1369, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: -18.103, mean reward: -0.181 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.094, 10.098], loss: 0.015685, mae: 0.064219, mean_q: -0.029183
 78753/100000: episode: 1370, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.162, mean reward: -0.142 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.449, 10.404], loss: 0.028608, mae: 0.072120, mean_q: -0.050488
 78853/100000: episode: 1371, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -13.525, mean reward: -0.135 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.569, 10.098], loss: 0.002686, mae: 0.053663, mean_q: -0.093539
 78953/100000: episode: 1372, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.167, mean reward: -0.162 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.951, 10.098], loss: 0.015166, mae: 0.057158, mean_q: -0.080512
 79053/100000: episode: 1373, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.155, mean reward: -0.182 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.001, 10.281], loss: 0.015201, mae: 0.058940, mean_q: -0.125076
 79153/100000: episode: 1374, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -16.153, mean reward: -0.162 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.888, 10.121], loss: 0.002525, mae: 0.052180, mean_q: -0.122305
 79253/100000: episode: 1375, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.602, mean reward: -0.166 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.307, 10.098], loss: 0.002544, mae: 0.051239, mean_q: -0.156067
 79353/100000: episode: 1376, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -10.647, mean reward: -0.106 [-1.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.357, 10.199], loss: 0.002451, mae: 0.049861, mean_q: -0.182877
 79453/100000: episode: 1377, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.808, mean reward: -0.198 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.040, 10.098], loss: 0.015239, mae: 0.057792, mean_q: -0.203594
 79553/100000: episode: 1378, duration: 0.623s, episode steps: 100, steps per second: 161, episode reward: -17.072, mean reward: -0.171 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.334, 10.098], loss: 0.002375, mae: 0.048614, mean_q: -0.217477
 79653/100000: episode: 1379, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.564, mean reward: -0.186 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.838, 10.126], loss: 0.015153, mae: 0.056466, mean_q: -0.228264
 79753/100000: episode: 1380, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -17.558, mean reward: -0.176 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.710, 10.380], loss: 0.002652, mae: 0.051998, mean_q: -0.222883
 79853/100000: episode: 1381, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.317, mean reward: -0.153 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.094, 10.098], loss: 0.015068, mae: 0.055671, mean_q: -0.308013
 79953/100000: episode: 1382, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.830, mean reward: -0.178 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.035, 10.098], loss: 0.028319, mae: 0.068349, mean_q: -0.270889
 80053/100000: episode: 1383, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.191, mean reward: -0.162 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.470, 10.216], loss: 0.003930, mae: 0.058372, mean_q: -0.311208
 80153/100000: episode: 1384, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.401, mean reward: -0.154 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.283, 10.098], loss: 0.002591, mae: 0.051397, mean_q: -0.312264
 80253/100000: episode: 1385, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -20.728, mean reward: -0.207 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.347, 10.123], loss: 0.002476, mae: 0.049503, mean_q: -0.282668
 80353/100000: episode: 1386, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -13.765, mean reward: -0.138 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.055, 10.098], loss: 0.002389, mae: 0.048730, mean_q: -0.292555
 80453/100000: episode: 1387, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: -19.172, mean reward: -0.192 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.443, 10.098], loss: 0.002248, mae: 0.047569, mean_q: -0.337256
 80553/100000: episode: 1388, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.424, mean reward: -0.174 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.836, 10.098], loss: 0.002167, mae: 0.045842, mean_q: -0.322566
 80653/100000: episode: 1389, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.143, mean reward: -0.151 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.268, 10.098], loss: 0.002235, mae: 0.046904, mean_q: -0.304394
 80753/100000: episode: 1390, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.680, mean reward: -0.177 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.738, 10.098], loss: 0.002209, mae: 0.046086, mean_q: -0.333255
 80853/100000: episode: 1391, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -16.702, mean reward: -0.167 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.417, 10.158], loss: 0.002305, mae: 0.046958, mean_q: -0.334586
 80953/100000: episode: 1392, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -12.006, mean reward: -0.120 [-1.000, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.470, 10.098], loss: 0.002314, mae: 0.047857, mean_q: -0.303638
 81053/100000: episode: 1393, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.752, mean reward: -0.168 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.468, 10.197], loss: 0.002223, mae: 0.047032, mean_q: -0.290374
 81153/100000: episode: 1394, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -19.112, mean reward: -0.191 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.386, 10.114], loss: 0.002307, mae: 0.047591, mean_q: -0.289078
 81253/100000: episode: 1395, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -11.689, mean reward: -0.117 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.886, 10.472], loss: 0.002330, mae: 0.047518, mean_q: -0.326482
 81353/100000: episode: 1396, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -18.003, mean reward: -0.180 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.579, 10.098], loss: 0.002129, mae: 0.045702, mean_q: -0.316486
 81453/100000: episode: 1397, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.088, mean reward: -0.171 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.555, 10.105], loss: 0.002326, mae: 0.048275, mean_q: -0.295556
 81553/100000: episode: 1398, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.738, mean reward: -0.187 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.612, 10.155], loss: 0.002437, mae: 0.049213, mean_q: -0.271293
 81653/100000: episode: 1399, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.159, mean reward: -0.192 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.815, 10.225], loss: 0.002372, mae: 0.048914, mean_q: -0.311434
 81753/100000: episode: 1400, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.128, mean reward: -0.131 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.821, 10.098], loss: 0.002269, mae: 0.045624, mean_q: -0.319239
 81853/100000: episode: 1401, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -21.355, mean reward: -0.214 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.604, 10.098], loss: 0.002320, mae: 0.048154, mean_q: -0.299485
 81953/100000: episode: 1402, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.365, mean reward: -0.194 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.368, 10.150], loss: 0.002494, mae: 0.049031, mean_q: -0.302417
 82053/100000: episode: 1403, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -13.263, mean reward: -0.133 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.262, 10.287], loss: 0.002796, mae: 0.049909, mean_q: -0.293902
 82153/100000: episode: 1404, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.337, mean reward: -0.153 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.593, 10.314], loss: 0.006408, mae: 0.071323, mean_q: -0.301207
 82253/100000: episode: 1405, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -13.046, mean reward: -0.130 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.158, 10.171], loss: 0.002860, mae: 0.054053, mean_q: -0.314952
 82353/100000: episode: 1406, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -17.793, mean reward: -0.178 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.659, 10.215], loss: 0.002256, mae: 0.046600, mean_q: -0.347399
 82453/100000: episode: 1407, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.199, mean reward: -0.162 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.205, 10.206], loss: 0.002425, mae: 0.049077, mean_q: -0.274725
 82553/100000: episode: 1408, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -17.560, mean reward: -0.176 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.993, 10.098], loss: 0.002237, mae: 0.046086, mean_q: -0.327808
 82653/100000: episode: 1409, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -14.715, mean reward: -0.147 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.595, 10.098], loss: 0.002153, mae: 0.045416, mean_q: -0.307917
 82753/100000: episode: 1410, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.750, mean reward: -0.177 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.592, 10.098], loss: 0.002224, mae: 0.046030, mean_q: -0.321621
 82853/100000: episode: 1411, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.134, mean reward: -0.161 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.010, 10.098], loss: 0.002283, mae: 0.047123, mean_q: -0.296323
 82953/100000: episode: 1412, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.907, mean reward: -0.189 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.991, 10.230], loss: 0.002130, mae: 0.046097, mean_q: -0.315692
 83053/100000: episode: 1413, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.427, mean reward: -0.184 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.869, 10.397], loss: 0.002270, mae: 0.046298, mean_q: -0.317363
 83153/100000: episode: 1414, duration: 0.608s, episode steps: 100, steps per second: 165, episode reward: -18.180, mean reward: -0.182 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.738, 10.098], loss: 0.002343, mae: 0.047601, mean_q: -0.297720
 83253/100000: episode: 1415, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: -19.738, mean reward: -0.197 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.507, 10.128], loss: 0.002319, mae: 0.048048, mean_q: -0.301121
 83353/100000: episode: 1416, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -12.690, mean reward: -0.127 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.021, 10.098], loss: 0.002450, mae: 0.047810, mean_q: -0.321256
 83453/100000: episode: 1417, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -11.566, mean reward: -0.116 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.019, 10.263], loss: 0.002436, mae: 0.048735, mean_q: -0.300372
 83553/100000: episode: 1418, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.223, mean reward: -0.192 [-1.000, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.160, 10.241], loss: 0.002266, mae: 0.047457, mean_q: -0.310134
 83653/100000: episode: 1419, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -12.853, mean reward: -0.129 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.655, 10.476], loss: 0.002424, mae: 0.048793, mean_q: -0.307468
 83753/100000: episode: 1420, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.740, mean reward: -0.157 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.661, 10.161], loss: 0.002461, mae: 0.048293, mean_q: -0.312880
 83853/100000: episode: 1421, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -15.065, mean reward: -0.151 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.437, 10.121], loss: 0.002429, mae: 0.049003, mean_q: -0.296738
 83953/100000: episode: 1422, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -15.315, mean reward: -0.153 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.752, 10.098], loss: 0.002442, mae: 0.049460, mean_q: -0.343398
 84053/100000: episode: 1423, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -17.493, mean reward: -0.175 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.251, 10.098], loss: 0.002240, mae: 0.047043, mean_q: -0.335314
 84153/100000: episode: 1424, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -17.957, mean reward: -0.180 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.559, 10.098], loss: 0.002288, mae: 0.047168, mean_q: -0.332678
 84253/100000: episode: 1425, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -11.836, mean reward: -0.118 [-1.000, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.390, 10.098], loss: 0.002497, mae: 0.049256, mean_q: -0.317113
 84353/100000: episode: 1426, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.818, mean reward: -0.178 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.180, 10.098], loss: 0.002387, mae: 0.048568, mean_q: -0.335178
 84453/100000: episode: 1427, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -20.439, mean reward: -0.204 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.797, 10.098], loss: 0.002603, mae: 0.049779, mean_q: -0.315899
 84553/100000: episode: 1428, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.274, mean reward: -0.183 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.404, 10.098], loss: 0.002431, mae: 0.048454, mean_q: -0.318100
 84653/100000: episode: 1429, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.878, mean reward: -0.159 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.882, 10.098], loss: 0.002540, mae: 0.050084, mean_q: -0.318527
 84753/100000: episode: 1430, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.898, mean reward: -0.149 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.543, 10.131], loss: 0.002485, mae: 0.049559, mean_q: -0.262781
 84853/100000: episode: 1431, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.295, mean reward: -0.163 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.536, 10.150], loss: 0.002791, mae: 0.052980, mean_q: -0.322414
 84953/100000: episode: 1432, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -13.471, mean reward: -0.135 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.981, 10.098], loss: 0.002389, mae: 0.048163, mean_q: -0.353017
[Info] 100-TH LEVEL FOUND: 0.6097791194915771, Considering 10/90 traces
 85053/100000: episode: 1433, duration: 4.697s, episode steps: 100, steps per second: 21, episode reward: -18.817, mean reward: -0.188 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.503, 10.098], loss: 0.002616, mae: 0.050221, mean_q: -0.289710
 85094/100000: episode: 1434, duration: 0.239s, episode steps: 41, steps per second: 171, episode reward: 19.433, mean reward: 0.474 [0.219, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.946, 10.346], loss: 0.002339, mae: 0.048443, mean_q: -0.329137
 85139/100000: episode: 1435, duration: 0.252s, episode steps: 45, steps per second: 178, episode reward: 10.871, mean reward: 0.242 [0.019, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-1.307, 10.212], loss: 0.002443, mae: 0.048698, mean_q: -0.289035
 85180/100000: episode: 1436, duration: 0.260s, episode steps: 41, steps per second: 157, episode reward: 18.622, mean reward: 0.454 [0.280, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.411, 10.475], loss: 0.002514, mae: 0.048555, mean_q: -0.320201
 85215/100000: episode: 1437, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 12.189, mean reward: 0.348 [0.219, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.864, 10.343], loss: 0.002576, mae: 0.048739, mean_q: -0.334114
 85223/100000: episode: 1438, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 3.358, mean reward: 0.420 [0.350, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.528], loss: 0.002348, mae: 0.048066, mean_q: -0.292720
 85254/100000: episode: 1439, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 10.220, mean reward: 0.330 [0.189, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.035, 10.348], loss: 0.002449, mae: 0.048962, mean_q: -0.321376
 85285/100000: episode: 1440, duration: 0.185s, episode steps: 31, steps per second: 168, episode reward: 12.166, mean reward: 0.392 [0.236, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.490, 10.464], loss: 0.002697, mae: 0.050217, mean_q: -0.253279
 85293/100000: episode: 1441, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 2.995, mean reward: 0.374 [0.281, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.474], loss: 0.007970, mae: 0.075930, mean_q: -0.283162
 85301/100000: episode: 1442, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 3.091, mean reward: 0.386 [0.309, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.243, 10.493], loss: 0.007919, mae: 0.069732, mean_q: -0.196982
 85309/100000: episode: 1443, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 3.197, mean reward: 0.400 [0.379, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.482], loss: 0.003848, mae: 0.072531, mean_q: -0.262606
 85340/100000: episode: 1444, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 7.322, mean reward: 0.236 [0.054, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.041, 10.185], loss: 0.003457, mae: 0.060595, mean_q: -0.201674
 85371/100000: episode: 1445, duration: 0.181s, episode steps: 31, steps per second: 171, episode reward: 6.514, mean reward: 0.210 [0.029, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.140, 10.100], loss: 0.002300, mae: 0.048400, mean_q: -0.268076
 85379/100000: episode: 1446, duration: 0.055s, episode steps: 8, steps per second: 146, episode reward: 3.315, mean reward: 0.414 [0.389, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.441], loss: 0.002077, mae: 0.046535, mean_q: -0.295863
 85420/100000: episode: 1447, duration: 0.230s, episode steps: 41, steps per second: 178, episode reward: 13.256, mean reward: 0.323 [0.034, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.512, 10.136], loss: 0.003010, mae: 0.055653, mean_q: -0.248421
 85478/100000: episode: 1448, duration: 0.315s, episode steps: 58, steps per second: 184, episode reward: 10.817, mean reward: 0.187 [0.020, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.832 [-0.456, 10.106], loss: 0.002654, mae: 0.050656, mean_q: -0.231941
 85509/100000: episode: 1449, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 10.969, mean reward: 0.354 [0.195, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.705, 10.521], loss: 0.002724, mae: 0.050385, mean_q: -0.307243
 85517/100000: episode: 1450, duration: 0.053s, episode steps: 8, steps per second: 152, episode reward: 3.431, mean reward: 0.429 [0.381, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.499], loss: 0.002486, mae: 0.051534, mean_q: -0.200448
 85539/100000: episode: 1451, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 7.148, mean reward: 0.325 [0.250, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.312, 10.100], loss: 0.002533, mae: 0.050155, mean_q: -0.214606
 85563/100000: episode: 1452, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 7.412, mean reward: 0.309 [0.090, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.246], loss: 0.002282, mae: 0.048834, mean_q: -0.207104
 85594/100000: episode: 1453, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 7.229, mean reward: 0.233 [0.034, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.035, 10.114], loss: 0.002667, mae: 0.051500, mean_q: -0.218321
 85602/100000: episode: 1454, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 2.909, mean reward: 0.364 [0.304, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.487], loss: 0.003444, mae: 0.058913, mean_q: -0.173365
 85647/100000: episode: 1455, duration: 0.242s, episode steps: 45, steps per second: 186, episode reward: 11.298, mean reward: 0.251 [0.063, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-1.291, 10.100], loss: 0.002655, mae: 0.051756, mean_q: -0.193358
 85655/100000: episode: 1456, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 3.073, mean reward: 0.384 [0.352, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.275, 10.513], loss: 0.002676, mae: 0.050270, mean_q: -0.224296
 85690/100000: episode: 1457, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 8.444, mean reward: 0.241 [0.106, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.379, 10.449], loss: 0.002899, mae: 0.053764, mean_q: -0.155339
 85698/100000: episode: 1458, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 3.373, mean reward: 0.422 [0.381, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.376, 10.535], loss: 0.002808, mae: 0.053466, mean_q: -0.218948
 85725/100000: episode: 1459, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 7.298, mean reward: 0.270 [0.120, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.728, 10.100], loss: 0.002759, mae: 0.052307, mean_q: -0.202948
 85783/100000: episode: 1460, duration: 0.329s, episode steps: 58, steps per second: 176, episode reward: 13.182, mean reward: 0.227 [0.053, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.845 [-0.264, 10.150], loss: 0.002838, mae: 0.053520, mean_q: -0.182778
 85814/100000: episode: 1461, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 5.449, mean reward: 0.176 [0.020, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.433, 10.136], loss: 0.002942, mae: 0.053472, mean_q: -0.170941
 85841/100000: episode: 1462, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 6.985, mean reward: 0.259 [0.157, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.394, 10.100], loss: 0.002824, mae: 0.055162, mean_q: -0.114624
 85882/100000: episode: 1463, duration: 0.212s, episode steps: 41, steps per second: 194, episode reward: 15.829, mean reward: 0.386 [0.234, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.223, 10.372], loss: 0.002978, mae: 0.054519, mean_q: -0.141675
 85909/100000: episode: 1464, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 8.553, mean reward: 0.317 [0.119, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.525, 10.100], loss: 0.002472, mae: 0.050872, mean_q: -0.189544
 85940/100000: episode: 1465, duration: 0.184s, episode steps: 31, steps per second: 169, episode reward: 12.154, mean reward: 0.392 [0.304, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.035, 10.466], loss: 0.002742, mae: 0.053593, mean_q: -0.131236
 85971/100000: episode: 1466, duration: 0.205s, episode steps: 31, steps per second: 151, episode reward: 11.769, mean reward: 0.380 [0.241, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.459, 10.449], loss: 0.002470, mae: 0.050771, mean_q: -0.123566
 86012/100000: episode: 1467, duration: 0.264s, episode steps: 41, steps per second: 155, episode reward: 15.436, mean reward: 0.376 [0.252, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.308, 10.434], loss: 0.002859, mae: 0.053802, mean_q: -0.190271
 86034/100000: episode: 1468, duration: 0.152s, episode steps: 22, steps per second: 145, episode reward: 5.757, mean reward: 0.262 [0.162, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.525, 10.100], loss: 0.002865, mae: 0.055102, mean_q: -0.063852
 86092/100000: episode: 1469, duration: 0.321s, episode steps: 58, steps per second: 181, episode reward: 16.388, mean reward: 0.283 [0.059, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.835 [-0.720, 10.100], loss: 0.002612, mae: 0.051861, mean_q: -0.121780
 86123/100000: episode: 1470, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 10.189, mean reward: 0.329 [0.213, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.993, 10.410], loss: 0.002533, mae: 0.051551, mean_q: -0.134197
 86150/100000: episode: 1471, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 9.357, mean reward: 0.347 [0.117, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.457, 10.100], loss: 0.003037, mae: 0.055189, mean_q: -0.120153
 86185/100000: episode: 1472, duration: 0.212s, episode steps: 35, steps per second: 165, episode reward: 17.027, mean reward: 0.486 [0.324, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-1.074, 10.676], loss: 0.002436, mae: 0.049694, mean_q: -0.170985
 86243/100000: episode: 1473, duration: 0.307s, episode steps: 58, steps per second: 189, episode reward: 10.316, mean reward: 0.178 [0.027, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.838 [-0.233, 10.100], loss: 0.002767, mae: 0.052970, mean_q: -0.130129
 86251/100000: episode: 1474, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 3.109, mean reward: 0.389 [0.318, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.306, 10.484], loss: 0.003085, mae: 0.056742, mean_q: -0.039276
 86275/100000: episode: 1475, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 8.385, mean reward: 0.349 [0.218, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.690, 10.338], loss: 0.002648, mae: 0.051848, mean_q: -0.124611
 86299/100000: episode: 1476, duration: 0.123s, episode steps: 24, steps per second: 194, episode reward: 8.861, mean reward: 0.369 [0.258, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.035, 10.379], loss: 0.002496, mae: 0.051560, mean_q: -0.078667
 86330/100000: episode: 1477, duration: 0.187s, episode steps: 31, steps per second: 165, episode reward: 11.692, mean reward: 0.377 [0.262, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.035, 10.432], loss: 0.002652, mae: 0.051613, mean_q: -0.040581
 86352/100000: episode: 1478, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 5.842, mean reward: 0.266 [0.179, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.246, 10.100], loss: 0.002429, mae: 0.050963, mean_q: -0.094380
 86383/100000: episode: 1479, duration: 0.155s, episode steps: 31, steps per second: 201, episode reward: 9.366, mean reward: 0.302 [0.238, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.248, 10.100], loss: 0.002795, mae: 0.053553, mean_q: -0.085520
 86414/100000: episode: 1480, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 10.054, mean reward: 0.324 [0.175, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.434, 10.100], loss: 0.002455, mae: 0.050486, mean_q: -0.074773
 86422/100000: episode: 1481, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 2.884, mean reward: 0.361 [0.297, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.446], loss: 0.002424, mae: 0.050261, mean_q: -0.102392
 86449/100000: episode: 1482, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 8.403, mean reward: 0.311 [0.160, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.567, 10.100], loss: 0.002665, mae: 0.052233, mean_q: -0.049729
 86507/100000: episode: 1483, duration: 0.297s, episode steps: 58, steps per second: 195, episode reward: 11.734, mean reward: 0.202 [0.010, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-0.983, 10.100], loss: 0.002924, mae: 0.055139, mean_q: -0.052950
 86538/100000: episode: 1484, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 12.044, mean reward: 0.389 [0.250, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.993, 10.364], loss: 0.003039, mae: 0.055561, mean_q: -0.040888
 86569/100000: episode: 1485, duration: 0.181s, episode steps: 31, steps per second: 171, episode reward: 6.342, mean reward: 0.205 [0.071, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.171, 10.100], loss: 0.002731, mae: 0.055226, mean_q: -0.002825
 86610/100000: episode: 1486, duration: 0.229s, episode steps: 41, steps per second: 179, episode reward: 11.733, mean reward: 0.286 [0.053, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.355, 10.100], loss: 0.002588, mae: 0.052002, mean_q: -0.003039
 86651/100000: episode: 1487, duration: 0.247s, episode steps: 41, steps per second: 166, episode reward: 16.012, mean reward: 0.391 [0.164, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.219, 10.329], loss: 0.002875, mae: 0.054638, mean_q: -0.001918
 86675/100000: episode: 1488, duration: 0.145s, episode steps: 24, steps per second: 165, episode reward: 5.777, mean reward: 0.241 [0.151, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.558, 10.268], loss: 0.003136, mae: 0.057146, mean_q: -0.034963
 86710/100000: episode: 1489, duration: 0.194s, episode steps: 35, steps per second: 180, episode reward: 10.872, mean reward: 0.311 [0.145, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-1.110, 10.302], loss: 0.002849, mae: 0.055792, mean_q: 0.072289
 86718/100000: episode: 1490, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 3.165, mean reward: 0.396 [0.301, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.065, 10.533], loss: 0.002597, mae: 0.054403, mean_q: -0.011989
 86776/100000: episode: 1491, duration: 0.312s, episode steps: 58, steps per second: 186, episode reward: 10.984, mean reward: 0.189 [0.042, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.838 [-1.076, 10.100], loss: 0.003023, mae: 0.057961, mean_q: 0.060982
 86807/100000: episode: 1492, duration: 0.185s, episode steps: 31, steps per second: 168, episode reward: 10.483, mean reward: 0.338 [0.226, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.238, 10.100], loss: 0.003064, mae: 0.057469, mean_q: 0.013798
 86848/100000: episode: 1493, duration: 0.260s, episode steps: 41, steps per second: 158, episode reward: 12.296, mean reward: 0.300 [0.111, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.838, 10.273], loss: 0.002710, mae: 0.052682, mean_q: -0.004101
 86856/100000: episode: 1494, duration: 0.054s, episode steps: 8, steps per second: 148, episode reward: 2.569, mean reward: 0.321 [0.155, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.324], loss: 0.002696, mae: 0.055672, mean_q: 0.049689
 86864/100000: episode: 1495, duration: 0.058s, episode steps: 8, steps per second: 137, episode reward: 3.313, mean reward: 0.414 [0.385, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.550], loss: 0.002561, mae: 0.051954, mean_q: -0.002671
 86895/100000: episode: 1496, duration: 0.190s, episode steps: 31, steps per second: 163, episode reward: 10.572, mean reward: 0.341 [0.202, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.035, 10.352], loss: 0.002646, mae: 0.053657, mean_q: 0.075813
 86936/100000: episode: 1497, duration: 0.266s, episode steps: 41, steps per second: 154, episode reward: 14.423, mean reward: 0.352 [0.111, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.565, 10.296], loss: 0.002855, mae: 0.054340, mean_q: 0.035235
 86967/100000: episode: 1498, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 8.374, mean reward: 0.270 [0.165, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-1.170, 10.396], loss: 0.002654, mae: 0.055031, mean_q: 0.062699
 87008/100000: episode: 1499, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 14.249, mean reward: 0.348 [0.183, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.535, 10.427], loss: 0.002722, mae: 0.054689, mean_q: 0.051092
 87035/100000: episode: 1500, duration: 0.149s, episode steps: 27, steps per second: 182, episode reward: 6.499, mean reward: 0.241 [0.139, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.543, 10.100], loss: 0.002984, mae: 0.056191, mean_q: 0.047608
 87057/100000: episode: 1501, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 5.583, mean reward: 0.254 [0.139, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.166, 10.100], loss: 0.002528, mae: 0.051086, mean_q: 0.045384
 87079/100000: episode: 1502, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 6.780, mean reward: 0.308 [0.257, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.270, 10.100], loss: 0.002445, mae: 0.052006, mean_q: 0.099792
 87087/100000: episode: 1503, duration: 0.058s, episode steps: 8, steps per second: 138, episode reward: 3.145, mean reward: 0.393 [0.347, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.480], loss: 0.003131, mae: 0.059427, mean_q: 0.148012
 87128/100000: episode: 1504, duration: 0.233s, episode steps: 41, steps per second: 176, episode reward: 10.989, mean reward: 0.268 [0.042, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.598, 10.182], loss: 0.002725, mae: 0.054291, mean_q: 0.050397
 87159/100000: episode: 1505, duration: 0.172s, episode steps: 31, steps per second: 181, episode reward: 13.110, mean reward: 0.423 [0.314, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.694, 10.491], loss: 0.002800, mae: 0.056182, mean_q: 0.066306
 87183/100000: episode: 1506, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 7.593, mean reward: 0.316 [0.188, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.363, 10.343], loss: 0.003059, mae: 0.055754, mean_q: 0.054826
 87205/100000: episode: 1507, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 5.760, mean reward: 0.262 [0.181, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.225, 10.100], loss: 0.002888, mae: 0.059377, mean_q: 0.145289
 87236/100000: episode: 1508, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 9.987, mean reward: 0.322 [0.185, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.648, 10.100], loss: 0.003036, mae: 0.056944, mean_q: 0.069049
 87267/100000: episode: 1509, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 9.150, mean reward: 0.295 [0.195, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.716, 10.319], loss: 0.003100, mae: 0.058818, mean_q: 0.117265
 87312/100000: episode: 1510, duration: 0.251s, episode steps: 45, steps per second: 179, episode reward: 10.261, mean reward: 0.228 [0.060, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.799, 10.100], loss: 0.003110, mae: 0.059240, mean_q: 0.119439
 87370/100000: episode: 1511, duration: 0.316s, episode steps: 58, steps per second: 183, episode reward: 12.616, mean reward: 0.218 [0.070, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.836 [-0.418, 10.100], loss: 0.002966, mae: 0.057621, mean_q: 0.140260
 87415/100000: episode: 1512, duration: 0.252s, episode steps: 45, steps per second: 178, episode reward: 13.478, mean reward: 0.300 [0.027, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.894, 10.100], loss: 0.002847, mae: 0.056297, mean_q: 0.132697
 87439/100000: episode: 1513, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 6.902, mean reward: 0.288 [0.168, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.312, 10.358], loss: 0.002841, mae: 0.055923, mean_q: 0.100175
 87484/100000: episode: 1514, duration: 0.254s, episode steps: 45, steps per second: 177, episode reward: 10.871, mean reward: 0.242 [0.074, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.242, 10.100], loss: 0.002607, mae: 0.053872, mean_q: 0.137458
 87511/100000: episode: 1515, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 9.652, mean reward: 0.357 [0.157, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.834, 10.100], loss: 0.002804, mae: 0.056386, mean_q: 0.173795
 87569/100000: episode: 1516, duration: 0.335s, episode steps: 58, steps per second: 173, episode reward: 10.230, mean reward: 0.176 [0.006, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-0.726, 10.148], loss: 0.002905, mae: 0.057052, mean_q: 0.180596
 87593/100000: episode: 1517, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 7.015, mean reward: 0.292 [0.084, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.272, 10.283], loss: 0.002779, mae: 0.055655, mean_q: 0.125317
 87624/100000: episode: 1518, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 8.901, mean reward: 0.287 [0.119, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.201, 10.100], loss: 0.002891, mae: 0.056379, mean_q: 0.164329
 87682/100000: episode: 1519, duration: 0.348s, episode steps: 58, steps per second: 166, episode reward: 14.117, mean reward: 0.243 [0.092, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.841 [-0.286, 10.100], loss: 0.002996, mae: 0.057018, mean_q: 0.180396
 87713/100000: episode: 1520, duration: 0.198s, episode steps: 31, steps per second: 157, episode reward: 8.055, mean reward: 0.260 [0.082, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.193, 10.100], loss: 0.003394, mae: 0.062641, mean_q: 0.228105
 87754/100000: episode: 1521, duration: 0.262s, episode steps: 41, steps per second: 156, episode reward: 16.365, mean reward: 0.399 [0.247, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.770, 10.651], loss: 0.003032, mae: 0.058152, mean_q: 0.198416
 87762/100000: episode: 1522, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 3.098, mean reward: 0.387 [0.323, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.754, 10.423], loss: 0.002539, mae: 0.055018, mean_q: 0.243959
[Info] 200-TH LEVEL FOUND: 0.9555172324180603, Considering 10/90 traces
 87807/100000: episode: 1523, duration: 4.355s, episode steps: 45, steps per second: 10, episode reward: 16.454, mean reward: 0.366 [0.284, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.332, 10.100], loss: 0.002959, mae: 0.058677, mean_q: 0.192580
 87846/100000: episode: 1524, duration: 0.241s, episode steps: 39, steps per second: 162, episode reward: 13.638, mean reward: 0.350 [0.076, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.519, 10.196], loss: 0.002966, mae: 0.058553, mean_q: 0.196318
 87884/100000: episode: 1525, duration: 0.266s, episode steps: 38, steps per second: 143, episode reward: 11.328, mean reward: 0.298 [0.092, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.172, 10.223], loss: 0.002832, mae: 0.055616, mean_q: 0.179863
 87924/100000: episode: 1526, duration: 0.252s, episode steps: 40, steps per second: 159, episode reward: 13.438, mean reward: 0.336 [0.013, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.436, 10.126], loss: 0.003257, mae: 0.060521, mean_q: 0.228365
 87964/100000: episode: 1527, duration: 0.274s, episode steps: 40, steps per second: 146, episode reward: 15.588, mean reward: 0.390 [0.196, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.359, 10.408], loss: 0.002951, mae: 0.057843, mean_q: 0.192673
 88004/100000: episode: 1528, duration: 0.223s, episode steps: 40, steps per second: 179, episode reward: 12.016, mean reward: 0.300 [0.142, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.880, 10.436], loss: 0.002851, mae: 0.057463, mean_q: 0.282231
 88043/100000: episode: 1529, duration: 0.221s, episode steps: 39, steps per second: 176, episode reward: 16.575, mean reward: 0.425 [0.357, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.473, 10.550], loss: 0.002927, mae: 0.058373, mean_q: 0.262245
 88083/100000: episode: 1530, duration: 0.214s, episode steps: 40, steps per second: 187, episode reward: 18.218, mean reward: 0.455 [0.306, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.424, 10.579], loss: 0.003288, mae: 0.060980, mean_q: 0.267559
 88118/100000: episode: 1531, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 12.780, mean reward: 0.365 [0.225, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.335, 10.403], loss: 0.002841, mae: 0.056865, mean_q: 0.251492
 88158/100000: episode: 1532, duration: 0.214s, episode steps: 40, steps per second: 187, episode reward: 12.122, mean reward: 0.303 [0.061, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.293, 10.156], loss: 0.002669, mae: 0.056041, mean_q: 0.274634
 88193/100000: episode: 1533, duration: 0.184s, episode steps: 35, steps per second: 190, episode reward: 18.897, mean reward: 0.540 [0.454, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.742, 10.681], loss: 0.007316, mae: 0.070970, mean_q: 0.317630
 88228/100000: episode: 1534, duration: 0.174s, episode steps: 35, steps per second: 201, episode reward: 15.864, mean reward: 0.453 [0.230, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.083, 10.473], loss: 0.005702, mae: 0.071865, mean_q: 0.283272
 88263/100000: episode: 1535, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 16.187, mean reward: 0.462 [0.197, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-2.198, 10.354], loss: 0.004321, mae: 0.066458, mean_q: 0.279969
 88302/100000: episode: 1536, duration: 0.207s, episode steps: 39, steps per second: 189, episode reward: 12.870, mean reward: 0.330 [0.149, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.570, 10.344], loss: 0.003413, mae: 0.063978, mean_q: 0.306255
 88340/100000: episode: 1537, duration: 0.195s, episode steps: 38, steps per second: 195, episode reward: 17.553, mean reward: 0.462 [0.281, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.328, 10.394], loss: 0.003194, mae: 0.061591, mean_q: 0.332458
 88380/100000: episode: 1538, duration: 0.216s, episode steps: 40, steps per second: 185, episode reward: 16.681, mean reward: 0.417 [0.245, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-1.423, 10.476], loss: 0.003071, mae: 0.060029, mean_q: 0.313129
 88420/100000: episode: 1539, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 16.962, mean reward: 0.424 [0.191, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.200, 10.569], loss: 0.002985, mae: 0.058761, mean_q: 0.304721
[Info] FALSIFICATION!
 88428/100000: episode: 1540, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 13.818, mean reward: 1.727 [0.497, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.018, 9.901], loss: 0.003250, mae: 0.062988, mean_q: 0.265086
 88528/100000: episode: 1541, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -7.201, mean reward: -0.072 [-1.000, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.045, 10.098], loss: 0.015469, mae: 0.064572, mean_q: 0.350077
 88628/100000: episode: 1542, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -20.353, mean reward: -0.204 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.560, 10.098], loss: 0.003087, mae: 0.059964, mean_q: 0.324453
 88728/100000: episode: 1543, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -12.463, mean reward: -0.125 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.662, 10.098], loss: 0.015074, mae: 0.061569, mean_q: 0.318120
 88828/100000: episode: 1544, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: -13.544, mean reward: -0.135 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.738, 10.098], loss: 0.003099, mae: 0.060243, mean_q: 0.324267
 88928/100000: episode: 1545, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.962, mean reward: -0.180 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.777, 10.133], loss: 0.002880, mae: 0.057977, mean_q: 0.334423
 89028/100000: episode: 1546, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.640, mean reward: -0.176 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.076, 10.285], loss: 0.002770, mae: 0.056463, mean_q: 0.341985
 89128/100000: episode: 1547, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.839, mean reward: -0.188 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.171, 10.098], loss: 0.015518, mae: 0.065285, mean_q: 0.353419
 89228/100000: episode: 1548, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -14.506, mean reward: -0.145 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.935, 10.246], loss: 0.015405, mae: 0.066319, mean_q: 0.341726
 89328/100000: episode: 1549, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.867, mean reward: -0.159 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.711, 10.098], loss: 0.002786, mae: 0.057076, mean_q: 0.341750
 89428/100000: episode: 1550, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -15.492, mean reward: -0.155 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.763, 10.448], loss: 0.002863, mae: 0.058423, mean_q: 0.335048
 89528/100000: episode: 1551, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.768, mean reward: -0.178 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.714, 10.098], loss: 0.002870, mae: 0.057485, mean_q: 0.329437
 89628/100000: episode: 1552, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: -16.217, mean reward: -0.162 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.151, 10.451], loss: 0.027380, mae: 0.069863, mean_q: 0.327870
 89728/100000: episode: 1553, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -17.719, mean reward: -0.177 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.233, 10.152], loss: 0.002951, mae: 0.059300, mean_q: 0.342713
 89828/100000: episode: 1554, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -20.253, mean reward: -0.203 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.436, 10.098], loss: 0.038682, mae: 0.071599, mean_q: 0.354649
 89928/100000: episode: 1555, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.632, mean reward: -0.146 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.524, 10.098], loss: 0.015554, mae: 0.070339, mean_q: 0.332473
 90028/100000: episode: 1556, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -18.025, mean reward: -0.180 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.632, 10.327], loss: 0.002830, mae: 0.057419, mean_q: 0.307140
 90128/100000: episode: 1557, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -13.702, mean reward: -0.137 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.137, 10.164], loss: 0.002798, mae: 0.057020, mean_q: 0.309999
 90228/100000: episode: 1558, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.413, mean reward: -0.164 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.088, 10.163], loss: 0.014583, mae: 0.060751, mean_q: 0.282721
 90328/100000: episode: 1559, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.028, mean reward: -0.160 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.286, 10.220], loss: 0.003136, mae: 0.060385, mean_q: 0.254410
 90428/100000: episode: 1560, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.270, mean reward: -0.153 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.754, 10.422], loss: 0.015169, mae: 0.066914, mean_q: 0.231236
 90528/100000: episode: 1561, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -15.607, mean reward: -0.156 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.260, 10.192], loss: 0.002707, mae: 0.056720, mean_q: 0.230764
 90628/100000: episode: 1562, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: -19.110, mean reward: -0.191 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.355, 10.098], loss: 0.016626, mae: 0.071308, mean_q: 0.213419
 90728/100000: episode: 1563, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.057, mean reward: -0.181 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.727, 10.098], loss: 0.016515, mae: 0.071882, mean_q: 0.205017
 90828/100000: episode: 1564, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -20.791, mean reward: -0.208 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.497, 10.142], loss: 0.002954, mae: 0.059122, mean_q: 0.216375
 90928/100000: episode: 1565, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.777, mean reward: -0.188 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.656, 10.134], loss: 0.002984, mae: 0.057711, mean_q: 0.131918
 91028/100000: episode: 1566, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.900, mean reward: -0.179 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.014, 10.286], loss: 0.014818, mae: 0.064240, mean_q: 0.152603
 91128/100000: episode: 1567, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.633, mean reward: -0.156 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.745, 10.098], loss: 0.014391, mae: 0.061752, mean_q: 0.107905
 91228/100000: episode: 1568, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -12.338, mean reward: -0.123 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.267, 10.098], loss: 0.002572, mae: 0.053046, mean_q: 0.097045
 91328/100000: episode: 1569, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.291, mean reward: -0.173 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.308, 10.359], loss: 0.002534, mae: 0.052759, mean_q: 0.076419
 91428/100000: episode: 1570, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -17.869, mean reward: -0.179 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.500, 10.215], loss: 0.002463, mae: 0.052690, mean_q: 0.048658
 91528/100000: episode: 1571, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: -19.468, mean reward: -0.195 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.519, 10.184], loss: 0.002457, mae: 0.051921, mean_q: 0.047768
 91628/100000: episode: 1572, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -20.594, mean reward: -0.206 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.048, 10.257], loss: 0.002580, mae: 0.051859, mean_q: -0.006640
 91728/100000: episode: 1573, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -16.724, mean reward: -0.167 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.646, 10.098], loss: 0.026325, mae: 0.064633, mean_q: 0.036497
 91828/100000: episode: 1574, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.178, mean reward: -0.182 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.665, 10.098], loss: 0.014438, mae: 0.060540, mean_q: -0.023609
 91928/100000: episode: 1575, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.436, mean reward: -0.174 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.850, 10.100], loss: 0.025341, mae: 0.064452, mean_q: -0.029680
 92028/100000: episode: 1576, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.983, mean reward: -0.160 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.057, 10.242], loss: 0.002501, mae: 0.051553, mean_q: -0.059085
 92128/100000: episode: 1577, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -13.799, mean reward: -0.138 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.113, 10.098], loss: 0.002413, mae: 0.050818, mean_q: -0.083060
 92228/100000: episode: 1578, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -12.332, mean reward: -0.123 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.134, 10.165], loss: 0.025831, mae: 0.065489, mean_q: -0.078905
 92328/100000: episode: 1579, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -18.096, mean reward: -0.181 [-1.000, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.989, 10.098], loss: 0.014847, mae: 0.065865, mean_q: -0.074836
 92428/100000: episode: 1580, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: -18.602, mean reward: -0.186 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.174, 10.098], loss: 0.003080, mae: 0.055752, mean_q: -0.113455
 92528/100000: episode: 1581, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -19.269, mean reward: -0.193 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.297, 10.127], loss: 0.002664, mae: 0.053138, mean_q: -0.102533
 92628/100000: episode: 1582, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.111, mean reward: -0.161 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.737, 10.308], loss: 0.002462, mae: 0.050333, mean_q: -0.162205
 92728/100000: episode: 1583, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.246, mean reward: -0.162 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.122, 10.098], loss: 0.024875, mae: 0.062350, mean_q: -0.170651
 92828/100000: episode: 1584, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -17.805, mean reward: -0.178 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.785, 10.098], loss: 0.014484, mae: 0.061905, mean_q: -0.169701
 92928/100000: episode: 1585, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -16.332, mean reward: -0.163 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.074, 10.098], loss: 0.002522, mae: 0.050502, mean_q: -0.210016
 93028/100000: episode: 1586, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.111, mean reward: -0.171 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.897, 10.098], loss: 0.002325, mae: 0.048524, mean_q: -0.228466
 93128/100000: episode: 1587, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.185, mean reward: -0.162 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.664, 10.362], loss: 0.002359, mae: 0.047861, mean_q: -0.297664
 93228/100000: episode: 1588, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.722, mean reward: -0.187 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.802, 10.098], loss: 0.015389, mae: 0.065369, mean_q: -0.287919
 93328/100000: episode: 1589, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: -16.096, mean reward: -0.161 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.173, 10.098], loss: 0.025153, mae: 0.063364, mean_q: -0.324307
 93428/100000: episode: 1590, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -18.978, mean reward: -0.190 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.331, 10.101], loss: 0.002331, mae: 0.047194, mean_q: -0.312058
 93528/100000: episode: 1591, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.353, mean reward: -0.164 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.863, 10.098], loss: 0.002308, mae: 0.047133, mean_q: -0.312968
 93628/100000: episode: 1592, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.175, mean reward: -0.192 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.953, 10.104], loss: 0.002352, mae: 0.048085, mean_q: -0.308109
 93728/100000: episode: 1593, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -8.798, mean reward: -0.088 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.791, 10.467], loss: 0.002302, mae: 0.047312, mean_q: -0.330432
 93828/100000: episode: 1594, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -15.236, mean reward: -0.152 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.705, 10.098], loss: 0.002323, mae: 0.048179, mean_q: -0.281279
 93928/100000: episode: 1595, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.125, mean reward: -0.191 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.566, 10.187], loss: 0.002227, mae: 0.046966, mean_q: -0.317420
 94028/100000: episode: 1596, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.365, mean reward: -0.184 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.226, 10.098], loss: 0.002355, mae: 0.047930, mean_q: -0.311167
 94128/100000: episode: 1597, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -16.445, mean reward: -0.164 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.263, 10.265], loss: 0.002304, mae: 0.047044, mean_q: -0.318990
 94228/100000: episode: 1598, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: -16.457, mean reward: -0.165 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.113, 10.098], loss: 0.002251, mae: 0.046596, mean_q: -0.338842
 94328/100000: episode: 1599, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -19.212, mean reward: -0.192 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.474, 10.098], loss: 0.002433, mae: 0.048294, mean_q: -0.323785
 94428/100000: episode: 1600, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.415, mean reward: -0.144 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.106, 10.098], loss: 0.002425, mae: 0.048273, mean_q: -0.327754
 94528/100000: episode: 1601, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.868, mean reward: -0.169 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.262, 10.098], loss: 0.002113, mae: 0.044775, mean_q: -0.331034
 94628/100000: episode: 1602, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -12.232, mean reward: -0.122 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.672, 10.098], loss: 0.002269, mae: 0.046072, mean_q: -0.321085
 94728/100000: episode: 1603, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -13.773, mean reward: -0.138 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.177, 10.098], loss: 0.002375, mae: 0.049149, mean_q: -0.292921
 94828/100000: episode: 1604, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -12.405, mean reward: -0.124 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.162, 10.098], loss: 0.002429, mae: 0.048962, mean_q: -0.271965
 94928/100000: episode: 1605, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.433, mean reward: -0.194 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.008, 10.266], loss: 0.002440, mae: 0.047904, mean_q: -0.321860
 95028/100000: episode: 1606, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -19.923, mean reward: -0.199 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.190, 10.098], loss: 0.002662, mae: 0.051971, mean_q: -0.277594
 95128/100000: episode: 1607, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -17.182, mean reward: -0.172 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.676, 10.098], loss: 0.002484, mae: 0.051160, mean_q: -0.312819
 95228/100000: episode: 1608, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -14.597, mean reward: -0.146 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.062, 10.098], loss: 0.002303, mae: 0.047151, mean_q: -0.324659
 95328/100000: episode: 1609, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -12.404, mean reward: -0.124 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.663, 10.252], loss: 0.002260, mae: 0.046214, mean_q: -0.332890
 95428/100000: episode: 1610, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.105, mean reward: -0.171 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.652, 10.098], loss: 0.002263, mae: 0.046755, mean_q: -0.331402
 95528/100000: episode: 1611, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -19.826, mean reward: -0.198 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.366, 10.098], loss: 0.002329, mae: 0.047299, mean_q: -0.344657
 95628/100000: episode: 1612, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -14.114, mean reward: -0.141 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.648, 10.228], loss: 0.002143, mae: 0.045763, mean_q: -0.331429
 95728/100000: episode: 1613, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.617, mean reward: -0.176 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.783, 10.098], loss: 0.002335, mae: 0.047251, mean_q: -0.326790
 95828/100000: episode: 1614, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.980, mean reward: -0.190 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.540, 10.185], loss: 0.002337, mae: 0.047591, mean_q: -0.307101
 95928/100000: episode: 1615, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -14.629, mean reward: -0.146 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.593, 10.349], loss: 0.002296, mae: 0.047029, mean_q: -0.315498
 96028/100000: episode: 1616, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: -17.677, mean reward: -0.177 [-1.000, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.701, 10.098], loss: 0.002377, mae: 0.047505, mean_q: -0.321482
 96128/100000: episode: 1617, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.661, mean reward: -0.177 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.523, 10.248], loss: 0.002248, mae: 0.046622, mean_q: -0.321139
 96228/100000: episode: 1618, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.966, mean reward: -0.190 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.680, 10.202], loss: 0.002441, mae: 0.047834, mean_q: -0.325062
 96328/100000: episode: 1619, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -9.788, mean reward: -0.098 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-1.945, 10.098], loss: 0.002446, mae: 0.048163, mean_q: -0.308617
 96428/100000: episode: 1620, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.920, mean reward: -0.169 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.937, 10.152], loss: 0.002476, mae: 0.049020, mean_q: -0.325999
 96528/100000: episode: 1621, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.354, mean reward: -0.184 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.121, 10.454], loss: 0.002253, mae: 0.046276, mean_q: -0.322794
 96628/100000: episode: 1622, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: -13.532, mean reward: -0.135 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.606, 10.098], loss: 0.002421, mae: 0.048868, mean_q: -0.281349
 96728/100000: episode: 1623, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.694, mean reward: -0.197 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.847, 10.098], loss: 0.002235, mae: 0.046188, mean_q: -0.289038
 96828/100000: episode: 1624, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -16.287, mean reward: -0.163 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.675, 10.188], loss: 0.002296, mae: 0.047838, mean_q: -0.300050
 96928/100000: episode: 1625, duration: 0.623s, episode steps: 100, steps per second: 161, episode reward: -18.642, mean reward: -0.186 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.726, 10.161], loss: 0.002415, mae: 0.048488, mean_q: -0.314887
 97028/100000: episode: 1626, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -20.067, mean reward: -0.201 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.302, 10.366], loss: 0.002415, mae: 0.048850, mean_q: -0.293539
 97128/100000: episode: 1627, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.475, mean reward: -0.195 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.644, 10.098], loss: 0.002412, mae: 0.049016, mean_q: -0.313348
 97228/100000: episode: 1628, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.616, mean reward: -0.166 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.909, 10.098], loss: 0.002285, mae: 0.047965, mean_q: -0.285629
 97328/100000: episode: 1629, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.269, mean reward: -0.173 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.392, 10.098], loss: 0.002205, mae: 0.046213, mean_q: -0.299947
 97428/100000: episode: 1630, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.334, mean reward: -0.173 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.765, 10.199], loss: 0.002277, mae: 0.047619, mean_q: -0.322351
 97528/100000: episode: 1631, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.648, mean reward: -0.156 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.003, 10.099], loss: 0.002559, mae: 0.050210, mean_q: -0.301082
 97628/100000: episode: 1632, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.685, mean reward: -0.167 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.780, 10.152], loss: 0.004928, mae: 0.060992, mean_q: -0.369792
 97728/100000: episode: 1633, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -5.358, mean reward: -0.054 [-1.000, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.588, 10.617], loss: 0.002828, mae: 0.053732, mean_q: -0.290494
 97828/100000: episode: 1634, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: -13.611, mean reward: -0.136 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.792, 10.323], loss: 0.002530, mae: 0.049405, mean_q: -0.318951
 97928/100000: episode: 1635, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.748, mean reward: -0.177 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.202, 10.098], loss: 0.002322, mae: 0.047405, mean_q: -0.310297
 98028/100000: episode: 1636, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -11.050, mean reward: -0.110 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.302, 10.098], loss: 0.002533, mae: 0.049057, mean_q: -0.276138
 98128/100000: episode: 1637, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.935, mean reward: -0.159 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.738, 10.098], loss: 0.002633, mae: 0.050210, mean_q: -0.278866
 98228/100000: episode: 1638, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.154, mean reward: -0.192 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.479, 10.144], loss: 0.002575, mae: 0.049503, mean_q: -0.323959
 98328/100000: episode: 1639, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -18.123, mean reward: -0.181 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.523, 10.098], loss: 0.002557, mae: 0.050344, mean_q: -0.282640
[Info] 100-TH LEVEL FOUND: 0.7071934938430786, Considering 10/90 traces
 98428/100000: episode: 1640, duration: 4.697s, episode steps: 100, steps per second: 21, episode reward: -19.904, mean reward: -0.199 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.716, 10.106], loss: 0.002690, mae: 0.050207, mean_q: -0.306610
 98460/100000: episode: 1641, duration: 0.187s, episode steps: 32, steps per second: 171, episode reward: 8.623, mean reward: 0.269 [0.052, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.290, 10.156], loss: 0.002479, mae: 0.049346, mean_q: -0.294039
 98492/100000: episode: 1642, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 10.025, mean reward: 0.313 [0.161, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.451, 10.269], loss: 0.002643, mae: 0.049901, mean_q: -0.326431
 98534/100000: episode: 1643, duration: 0.217s, episode steps: 42, steps per second: 194, episode reward: 17.131, mean reward: 0.408 [0.277, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.545, 10.100], loss: 0.002604, mae: 0.049630, mean_q: -0.307898
 98576/100000: episode: 1644, duration: 0.226s, episode steps: 42, steps per second: 186, episode reward: 13.551, mean reward: 0.323 [0.177, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.393, 10.100], loss: 0.002383, mae: 0.047066, mean_q: -0.287531
 98613/100000: episode: 1645, duration: 0.188s, episode steps: 37, steps per second: 197, episode reward: 11.254, mean reward: 0.304 [0.069, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.386, 10.100], loss: 0.002677, mae: 0.051269, mean_q: -0.194207
 98648/100000: episode: 1646, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 9.265, mean reward: 0.265 [0.142, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.933, 10.287], loss: 0.002350, mae: 0.046586, mean_q: -0.268989
 98656/100000: episode: 1647, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 3.093, mean reward: 0.387 [0.358, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.467], loss: 0.002613, mae: 0.048402, mean_q: -0.306589
 98676/100000: episode: 1648, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 5.491, mean reward: 0.275 [0.182, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.908, 10.100], loss: 0.002800, mae: 0.051346, mean_q: -0.286249
 98708/100000: episode: 1649, duration: 0.179s, episode steps: 32, steps per second: 179, episode reward: 11.261, mean reward: 0.352 [0.217, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.035, 10.439], loss: 0.002317, mae: 0.047062, mean_q: -0.288620
 98743/100000: episode: 1650, duration: 0.198s, episode steps: 35, steps per second: 176, episode reward: 12.502, mean reward: 0.357 [0.176, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.396, 10.335], loss: 0.002585, mae: 0.050357, mean_q: -0.231804
 98775/100000: episode: 1651, duration: 0.167s, episode steps: 32, steps per second: 192, episode reward: 7.964, mean reward: 0.249 [0.045, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.144, 10.202], loss: 0.002823, mae: 0.052884, mean_q: -0.196550
 98795/100000: episode: 1652, duration: 0.131s, episode steps: 20, steps per second: 152, episode reward: 6.887, mean reward: 0.344 [0.227, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.555, 10.100], loss: 0.002293, mae: 0.047438, mean_q: -0.263397
 98803/100000: episode: 1653, duration: 0.062s, episode steps: 8, steps per second: 130, episode reward: 3.003, mean reward: 0.375 [0.295, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.538], loss: 0.001950, mae: 0.043237, mean_q: -0.266459
 98818/100000: episode: 1654, duration: 0.097s, episode steps: 15, steps per second: 155, episode reward: 7.037, mean reward: 0.469 [0.366, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.825, 10.100], loss: 0.002478, mae: 0.048104, mean_q: -0.268175
 98826/100000: episode: 1655, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 3.153, mean reward: 0.394 [0.358, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.130, 10.484], loss: 0.002574, mae: 0.047665, mean_q: -0.380415
 98834/100000: episode: 1656, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 3.027, mean reward: 0.378 [0.333, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.531], loss: 0.002678, mae: 0.052187, mean_q: -0.194892
 98866/100000: episode: 1657, duration: 0.206s, episode steps: 32, steps per second: 156, episode reward: 9.446, mean reward: 0.295 [0.150, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.374, 10.293], loss: 0.002477, mae: 0.048717, mean_q: -0.268827
 98881/100000: episode: 1658, duration: 0.099s, episode steps: 15, steps per second: 151, episode reward: 4.137, mean reward: 0.276 [0.073, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.055, 10.100], loss: 0.002900, mae: 0.051882, mean_q: -0.217254
 98916/100000: episode: 1659, duration: 0.242s, episode steps: 35, steps per second: 145, episode reward: 9.958, mean reward: 0.285 [0.090, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.774, 10.205], loss: 0.002412, mae: 0.048313, mean_q: -0.245907
 98942/100000: episode: 1660, duration: 0.156s, episode steps: 26, steps per second: 166, episode reward: 7.193, mean reward: 0.277 [0.062, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.096, 10.100], loss: 0.002426, mae: 0.048852, mean_q: -0.199151
 98968/100000: episode: 1661, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 7.988, mean reward: 0.307 [0.172, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.161, 10.100], loss: 0.002402, mae: 0.046795, mean_q: -0.259718
 98994/100000: episode: 1662, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 6.734, mean reward: 0.259 [0.167, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.321, 10.100], loss: 0.002771, mae: 0.051840, mean_q: -0.215204
 99014/100000: episode: 1663, duration: 0.123s, episode steps: 20, steps per second: 163, episode reward: 6.687, mean reward: 0.334 [0.271, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-2.475, 10.100], loss: 0.002934, mae: 0.054167, mean_q: -0.186165
 99024/100000: episode: 1664, duration: 0.062s, episode steps: 10, steps per second: 163, episode reward: 3.060, mean reward: 0.306 [0.256, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.279, 10.100], loss: 0.002380, mae: 0.048616, mean_q: -0.236278
 99056/100000: episode: 1665, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 8.012, mean reward: 0.250 [0.050, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-1.841, 10.195], loss: 0.002661, mae: 0.051931, mean_q: -0.166493
[Info] FALSIFICATION!
 99092/100000: episode: 1666, duration: 0.202s, episode steps: 36, steps per second: 178, episode reward: 23.453, mean reward: 0.651 [0.231, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.666, 10.020], loss: 0.002402, mae: 0.049074, mean_q: -0.181583
 99192/100000: episode: 1667, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.592, mean reward: -0.166 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.132, 10.098], loss: 0.002588, mae: 0.050543, mean_q: -0.217115
 99292/100000: episode: 1668, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.688, mean reward: -0.197 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.817, 10.098], loss: 0.015979, mae: 0.058610, mean_q: -0.169526
 99392/100000: episode: 1669, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.114, mean reward: -0.151 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.574, 10.098], loss: 0.002815, mae: 0.054113, mean_q: -0.156003
 99492/100000: episode: 1670, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.049, mean reward: -0.180 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.970, 10.098], loss: 0.016372, mae: 0.062297, mean_q: -0.186546
 99592/100000: episode: 1671, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -13.307, mean reward: -0.133 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.411, 10.399], loss: 0.030384, mae: 0.069856, mean_q: -0.185566
 99692/100000: episode: 1672, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -15.755, mean reward: -0.158 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.446, 10.231], loss: 0.016159, mae: 0.060684, mean_q: -0.175928
 99792/100000: episode: 1673, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: -15.323, mean reward: -0.153 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.277, 10.098], loss: 0.028931, mae: 0.068467, mean_q: -0.161728
 99892/100000: episode: 1674, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -18.101, mean reward: -0.181 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.654, 10.098], loss: 0.016077, mae: 0.060772, mean_q: -0.197310
 99992/100000: episode: 1675, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -17.901, mean reward: -0.179 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.692, 10.120], loss: 0.015709, mae: 0.058540, mean_q: -0.179868
done, took 621.118 seconds
[Info] End Importance Splitting. Falsification occurred 7 times.
