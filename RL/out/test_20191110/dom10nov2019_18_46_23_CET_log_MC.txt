Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.172s, episode steps: 100, steps per second: 582, episode reward: -16.737, mean reward: -0.167 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.032, 10.243], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.063s, episode steps: 100, steps per second: 1598, episode reward: -16.883, mean reward: -0.169 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.457, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.060s, episode steps: 100, steps per second: 1666, episode reward: -17.503, mean reward: -0.175 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.294, 10.123], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.060s, episode steps: 100, steps per second: 1667, episode reward: -17.985, mean reward: -0.180 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.646, 10.190], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.060s, episode steps: 100, steps per second: 1673, episode reward: -17.759, mean reward: -0.178 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.299, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.245s, episode steps: 100, steps per second: 80, episode reward: -15.429, mean reward: -0.154 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.412, 10.359], loss: 0.023134, mae: 0.146244, mean_q: -0.291622
   700/100000: episode: 7, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -11.964, mean reward: -0.120 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.158, 10.098], loss: 0.013314, mae: 0.103046, mean_q: -0.300031
   800/100000: episode: 8, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -17.533, mean reward: -0.175 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.200, 10.098], loss: 0.010697, mae: 0.095947, mean_q: -0.298934
   900/100000: episode: 9, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -15.973, mean reward: -0.160 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.602, 10.253], loss: 0.008621, mae: 0.087263, mean_q: -0.302693
  1000/100000: episode: 10, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.753, mean reward: -0.148 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.371, 10.117], loss: 0.008287, mae: 0.083858, mean_q: -0.278311
  1100/100000: episode: 11, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.221, mean reward: -0.192 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.631, 10.298], loss: 0.007250, mae: 0.078294, mean_q: -0.307603
  1200/100000: episode: 12, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -14.293, mean reward: -0.143 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.666, 10.098], loss: 0.008181, mae: 0.084618, mean_q: -0.324548
  1300/100000: episode: 13, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.973, mean reward: -0.150 [-1.000, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.540, 10.098], loss: 0.007902, mae: 0.082865, mean_q: -0.348832
  1400/100000: episode: 14, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.391, mean reward: -0.144 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.253, 10.098], loss: 0.007114, mae: 0.081460, mean_q: -0.330365
  1500/100000: episode: 15, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.026, mean reward: -0.180 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.791, 10.217], loss: 0.007747, mae: 0.084737, mean_q: -0.326750
  1600/100000: episode: 16, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.744, mean reward: -0.177 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.015, 10.236], loss: 0.006177, mae: 0.076896, mean_q: -0.312392
  1700/100000: episode: 17, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.979, mean reward: -0.180 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.510, 10.171], loss: 0.006739, mae: 0.077182, mean_q: -0.316010
  1800/100000: episode: 18, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.317, mean reward: -0.163 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.277, 10.231], loss: 0.006884, mae: 0.078732, mean_q: -0.321182
  1900/100000: episode: 19, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -19.576, mean reward: -0.196 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.993, 10.274], loss: 0.006484, mae: 0.073481, mean_q: -0.349213
  2000/100000: episode: 20, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -20.263, mean reward: -0.203 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.095, 10.165], loss: 0.006220, mae: 0.075514, mean_q: -0.315460
  2100/100000: episode: 21, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.270, mean reward: -0.173 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.152, 10.156], loss: 0.007649, mae: 0.082828, mean_q: -0.340621
  2200/100000: episode: 22, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.049, mean reward: -0.190 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.761, 10.098], loss: 0.005329, mae: 0.070432, mean_q: -0.331397
  2300/100000: episode: 23, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.601, mean reward: -0.146 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.474, 10.128], loss: 0.005640, mae: 0.071413, mean_q: -0.332318
  2400/100000: episode: 24, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.809, mean reward: -0.188 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.341, 10.098], loss: 0.005114, mae: 0.067260, mean_q: -0.335084
  2500/100000: episode: 25, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -12.077, mean reward: -0.121 [-1.000, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.479, 10.098], loss: 0.005059, mae: 0.070077, mean_q: -0.317787
  2600/100000: episode: 26, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -20.435, mean reward: -0.204 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.642, 10.171], loss: 0.005555, mae: 0.072653, mean_q: -0.314968
  2700/100000: episode: 27, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -15.129, mean reward: -0.151 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.702, 10.488], loss: 0.005366, mae: 0.069863, mean_q: -0.310597
  2800/100000: episode: 28, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.012, mean reward: -0.170 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.077, 10.181], loss: 0.006197, mae: 0.075212, mean_q: -0.310347
  2900/100000: episode: 29, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.565, mean reward: -0.176 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.969, 10.173], loss: 0.005944, mae: 0.075900, mean_q: -0.317453
  3000/100000: episode: 30, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.897, mean reward: -0.149 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.762, 10.280], loss: 0.005059, mae: 0.071862, mean_q: -0.328215
  3100/100000: episode: 31, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.943, mean reward: -0.179 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.543, 10.098], loss: 0.005825, mae: 0.073228, mean_q: -0.328531
  3200/100000: episode: 32, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -19.724, mean reward: -0.197 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.174, 10.244], loss: 0.005455, mae: 0.072995, mean_q: -0.322215
  3300/100000: episode: 33, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -9.325, mean reward: -0.093 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.422, 10.098], loss: 0.006483, mae: 0.077310, mean_q: -0.360615
  3400/100000: episode: 34, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -12.031, mean reward: -0.120 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.393, 10.342], loss: 0.006695, mae: 0.078255, mean_q: -0.325026
  3500/100000: episode: 35, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -8.315, mean reward: -0.083 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.167, 10.098], loss: 0.005303, mae: 0.072115, mean_q: -0.338356
  3600/100000: episode: 36, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -14.956, mean reward: -0.150 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.746, 10.205], loss: 0.006094, mae: 0.076038, mean_q: -0.295395
  3700/100000: episode: 37, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.733, mean reward: -0.177 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.328, 10.098], loss: 0.005228, mae: 0.072496, mean_q: -0.344611
  3800/100000: episode: 38, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -2.262, mean reward: -0.023 [-1.000, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.684, 10.098], loss: 0.006145, mae: 0.077960, mean_q: -0.314963
  3900/100000: episode: 39, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.758, mean reward: -0.178 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.924, 10.235], loss: 0.005567, mae: 0.074159, mean_q: -0.331781
  4000/100000: episode: 40, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.184, mean reward: -0.172 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.793, 10.205], loss: 0.005987, mae: 0.077722, mean_q: -0.271638
  4100/100000: episode: 41, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.479, mean reward: -0.175 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.653, 10.217], loss: 0.004843, mae: 0.071973, mean_q: -0.288304
  4200/100000: episode: 42, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.645, mean reward: -0.156 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.907, 10.197], loss: 0.005236, mae: 0.072592, mean_q: -0.316674
  4300/100000: episode: 43, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -15.007, mean reward: -0.150 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.131, 10.282], loss: 0.006155, mae: 0.078235, mean_q: -0.310403
  4400/100000: episode: 44, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.228, mean reward: -0.182 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.060, 10.185], loss: 0.004625, mae: 0.070730, mean_q: -0.310385
  4500/100000: episode: 45, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.045, mean reward: -0.160 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.856, 10.098], loss: 0.005251, mae: 0.070605, mean_q: -0.311201
  4600/100000: episode: 46, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.986, mean reward: -0.180 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.632, 10.098], loss: 0.004792, mae: 0.068233, mean_q: -0.318680
  4700/100000: episode: 47, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.679, mean reward: -0.177 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.377, 10.098], loss: 0.004625, mae: 0.068944, mean_q: -0.309311
  4800/100000: episode: 48, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -18.534, mean reward: -0.185 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.426, 10.119], loss: 0.004644, mae: 0.069344, mean_q: -0.290597
  4900/100000: episode: 49, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -13.665, mean reward: -0.137 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.482, 10.098], loss: 0.004896, mae: 0.070551, mean_q: -0.291520
  5000/100000: episode: 50, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.294, mean reward: -0.193 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.614, 10.098], loss: 0.005069, mae: 0.072021, mean_q: -0.317535
  5100/100000: episode: 51, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.182, mean reward: -0.172 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.695, 10.098], loss: 0.005492, mae: 0.075732, mean_q: -0.289993
  5200/100000: episode: 52, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.652, mean reward: -0.177 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.604, 10.164], loss: 0.005118, mae: 0.071955, mean_q: -0.304798
  5300/100000: episode: 53, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.049, mean reward: -0.160 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.714, 10.403], loss: 0.004320, mae: 0.067421, mean_q: -0.285568
  5400/100000: episode: 54, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -13.237, mean reward: -0.132 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.651, 10.428], loss: 0.004965, mae: 0.072704, mean_q: -0.312552
  5500/100000: episode: 55, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.725, mean reward: -0.147 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.948, 10.098], loss: 0.005618, mae: 0.078716, mean_q: -0.284015
  5600/100000: episode: 56, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -18.208, mean reward: -0.182 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.533, 10.098], loss: 0.004630, mae: 0.071989, mean_q: -0.286527
  5700/100000: episode: 57, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -13.748, mean reward: -0.137 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.493, 10.237], loss: 0.004839, mae: 0.071376, mean_q: -0.305021
  5800/100000: episode: 58, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.405, mean reward: -0.174 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.339, 10.271], loss: 0.004607, mae: 0.071915, mean_q: -0.287834
  5900/100000: episode: 59, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -14.172, mean reward: -0.142 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.371, 10.098], loss: 0.005533, mae: 0.076952, mean_q: -0.317718
  6000/100000: episode: 60, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.345, mean reward: -0.163 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.872, 10.098], loss: 0.003912, mae: 0.065225, mean_q: -0.281594
  6100/100000: episode: 61, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -11.135, mean reward: -0.111 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.803, 10.098], loss: 0.004278, mae: 0.066196, mean_q: -0.296651
  6200/100000: episode: 62, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.633, mean reward: -0.146 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.674, 10.144], loss: 0.004564, mae: 0.068469, mean_q: -0.291616
  6300/100000: episode: 63, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -13.944, mean reward: -0.139 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.631, 10.163], loss: 0.005164, mae: 0.073610, mean_q: -0.305996
  6400/100000: episode: 64, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.342, mean reward: -0.173 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.405, 10.098], loss: 0.004543, mae: 0.070041, mean_q: -0.289900
  6500/100000: episode: 65, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.832, mean reward: -0.158 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.364, 10.098], loss: 0.004543, mae: 0.069687, mean_q: -0.295620
  6600/100000: episode: 66, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.677, mean reward: -0.137 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.612, 10.098], loss: 0.004153, mae: 0.065552, mean_q: -0.314005
  6700/100000: episode: 67, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -12.030, mean reward: -0.120 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.614, 10.098], loss: 0.004434, mae: 0.069987, mean_q: -0.294323
  6800/100000: episode: 68, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.791, mean reward: -0.158 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.841, 10.162], loss: 0.004869, mae: 0.072913, mean_q: -0.286093
  6900/100000: episode: 69, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.673, mean reward: -0.197 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.670, 10.098], loss: 0.004067, mae: 0.067008, mean_q: -0.239491
  7000/100000: episode: 70, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.261, mean reward: -0.193 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.990, 10.100], loss: 0.004494, mae: 0.068424, mean_q: -0.281933
  7100/100000: episode: 71, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.843, mean reward: -0.158 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.223, 10.098], loss: 0.004275, mae: 0.069373, mean_q: -0.269735
  7200/100000: episode: 72, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.103, mean reward: -0.191 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.983, 10.098], loss: 0.004770, mae: 0.072253, mean_q: -0.249889
  7300/100000: episode: 73, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.383, mean reward: -0.184 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.195, 10.098], loss: 0.005624, mae: 0.077917, mean_q: -0.300655
  7400/100000: episode: 74, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.125, mean reward: -0.161 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.102, 10.311], loss: 0.003975, mae: 0.066261, mean_q: -0.273082
  7500/100000: episode: 75, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -19.043, mean reward: -0.190 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.739, 10.098], loss: 0.003879, mae: 0.066506, mean_q: -0.310219
  7600/100000: episode: 76, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -12.636, mean reward: -0.126 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.746, 10.309], loss: 0.003843, mae: 0.064855, mean_q: -0.282270
  7700/100000: episode: 77, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.222, mean reward: -0.152 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.270, 10.098], loss: 0.003657, mae: 0.063809, mean_q: -0.290018
  7800/100000: episode: 78, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -11.459, mean reward: -0.115 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.039, 10.098], loss: 0.005069, mae: 0.071834, mean_q: -0.329961
  7900/100000: episode: 79, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.831, mean reward: -0.188 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.876, 10.098], loss: 0.004153, mae: 0.067112, mean_q: -0.255081
  8000/100000: episode: 80, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -15.494, mean reward: -0.155 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.013, 10.098], loss: 0.004107, mae: 0.066735, mean_q: -0.286434
  8100/100000: episode: 81, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.361, mean reward: -0.184 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.866, 10.098], loss: 0.006958, mae: 0.084968, mean_q: -0.284408
  8200/100000: episode: 82, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.821, mean reward: -0.178 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.036, 10.244], loss: 0.003434, mae: 0.060300, mean_q: -0.277014
  8300/100000: episode: 83, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.964, mean reward: -0.160 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.739, 10.278], loss: 0.003554, mae: 0.061075, mean_q: -0.312539
  8400/100000: episode: 84, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -16.788, mean reward: -0.168 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.690, 10.098], loss: 0.005136, mae: 0.070178, mean_q: -0.311338
  8500/100000: episode: 85, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -15.018, mean reward: -0.150 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.656, 10.295], loss: 0.003612, mae: 0.063409, mean_q: -0.265465
  8600/100000: episode: 86, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -5.697, mean reward: -0.057 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.702, 10.098], loss: 0.003538, mae: 0.061287, mean_q: -0.297132
  8700/100000: episode: 87, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.029, mean reward: -0.160 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.729, 10.525], loss: 0.003351, mae: 0.060510, mean_q: -0.298843
  8800/100000: episode: 88, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -20.494, mean reward: -0.205 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.977, 10.098], loss: 0.003023, mae: 0.056277, mean_q: -0.315562
  8900/100000: episode: 89, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.251, mean reward: -0.183 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.553, 10.198], loss: 0.003453, mae: 0.061275, mean_q: -0.222143
  9000/100000: episode: 90, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -7.084, mean reward: -0.071 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.392, 10.098], loss: 0.003599, mae: 0.062963, mean_q: -0.293887
  9100/100000: episode: 91, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.701, mean reward: -0.187 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.043, 10.230], loss: 0.003669, mae: 0.062340, mean_q: -0.326370
  9200/100000: episode: 92, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.766, mean reward: -0.188 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.432, 10.217], loss: 0.003972, mae: 0.064987, mean_q: -0.264446
  9300/100000: episode: 93, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -20.770, mean reward: -0.208 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.966, 10.126], loss: 0.003274, mae: 0.058978, mean_q: -0.299411
  9400/100000: episode: 94, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.397, mean reward: -0.184 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.751, 10.348], loss: 0.003437, mae: 0.061125, mean_q: -0.311367
  9500/100000: episode: 95, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -14.852, mean reward: -0.149 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.928, 10.098], loss: 0.003334, mae: 0.059529, mean_q: -0.332965
  9600/100000: episode: 96, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.090, mean reward: -0.181 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.449, 10.312], loss: 0.003087, mae: 0.057105, mean_q: -0.322197
  9700/100000: episode: 97, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.790, mean reward: -0.178 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.201, 10.098], loss: 0.003583, mae: 0.061683, mean_q: -0.296781
  9800/100000: episode: 98, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.247, mean reward: -0.142 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.755, 10.098], loss: 0.003692, mae: 0.063661, mean_q: -0.311686
  9900/100000: episode: 99, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -12.375, mean reward: -0.124 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.786, 10.098], loss: 0.003845, mae: 0.065828, mean_q: -0.269292
 10000/100000: episode: 100, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.766, mean reward: -0.178 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.875, 10.098], loss: 0.004371, mae: 0.066524, mean_q: -0.313207
 10100/100000: episode: 101, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.618, mean reward: -0.176 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.312, 10.098], loss: 0.003182, mae: 0.057579, mean_q: -0.338307
 10200/100000: episode: 102, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -19.550, mean reward: -0.196 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.735, 10.098], loss: 0.003350, mae: 0.058980, mean_q: -0.312259
 10300/100000: episode: 103, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.715, mean reward: -0.167 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.331, 10.270], loss: 0.003202, mae: 0.058251, mean_q: -0.281865
 10400/100000: episode: 104, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -15.777, mean reward: -0.158 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.492, 10.110], loss: 0.003154, mae: 0.057124, mean_q: -0.313645
 10500/100000: episode: 105, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -9.599, mean reward: -0.096 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.167, 10.098], loss: 0.003704, mae: 0.063466, mean_q: -0.301707
 10600/100000: episode: 106, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -15.913, mean reward: -0.159 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.035, 10.098], loss: 0.003311, mae: 0.059259, mean_q: -0.329172
 10700/100000: episode: 107, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.879, mean reward: -0.159 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.278, 10.373], loss: 0.003600, mae: 0.063047, mean_q: -0.285764
 10800/100000: episode: 108, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.503, mean reward: -0.195 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.972, 10.322], loss: 0.003172, mae: 0.057786, mean_q: -0.268120
 10900/100000: episode: 109, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -11.346, mean reward: -0.113 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.439, 10.392], loss: 0.003322, mae: 0.059640, mean_q: -0.312003
 11000/100000: episode: 110, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.472, mean reward: -0.185 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.768, 10.098], loss: 0.003443, mae: 0.060234, mean_q: -0.324547
 11100/100000: episode: 111, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.351, mean reward: -0.164 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.535, 10.287], loss: 0.003418, mae: 0.059513, mean_q: -0.309850
 11200/100000: episode: 112, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.742, mean reward: -0.167 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.575, 10.177], loss: 0.003205, mae: 0.058241, mean_q: -0.273000
 11300/100000: episode: 113, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.639, mean reward: -0.176 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.395, 10.213], loss: 0.003401, mae: 0.059478, mean_q: -0.294815
 11400/100000: episode: 114, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -19.611, mean reward: -0.196 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.239, 10.137], loss: 0.003177, mae: 0.058906, mean_q: -0.281250
 11500/100000: episode: 115, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -16.241, mean reward: -0.162 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.539, 10.170], loss: 0.003033, mae: 0.056477, mean_q: -0.308600
 11600/100000: episode: 116, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.669, mean reward: -0.197 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.728, 10.331], loss: 0.003745, mae: 0.062904, mean_q: -0.333103
 11700/100000: episode: 117, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -19.170, mean reward: -0.192 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.802, 10.216], loss: 0.003054, mae: 0.055600, mean_q: -0.310620
 11800/100000: episode: 118, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.277, mean reward: -0.183 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.627, 10.171], loss: 0.003101, mae: 0.057911, mean_q: -0.306368
 11900/100000: episode: 119, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -18.367, mean reward: -0.184 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.536, 10.329], loss: 0.003314, mae: 0.060820, mean_q: -0.267106
 12000/100000: episode: 120, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.442, mean reward: -0.184 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.270, 10.310], loss: 0.003051, mae: 0.056492, mean_q: -0.309391
 12100/100000: episode: 121, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.664, mean reward: -0.157 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.482, 10.098], loss: 0.004841, mae: 0.070069, mean_q: -0.337133
 12200/100000: episode: 122, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.239, mean reward: -0.152 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.116, 10.353], loss: 0.003193, mae: 0.059392, mean_q: -0.307871
 12300/100000: episode: 123, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -19.764, mean reward: -0.198 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.661, 10.237], loss: 0.003290, mae: 0.060134, mean_q: -0.300826
 12400/100000: episode: 124, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.391, mean reward: -0.164 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.872, 10.098], loss: 0.003501, mae: 0.062080, mean_q: -0.295857
 12500/100000: episode: 125, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.398, mean reward: -0.174 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.433, 10.132], loss: 0.005478, mae: 0.072754, mean_q: -0.299964
 12600/100000: episode: 126, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.029, mean reward: -0.150 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.884, 10.427], loss: 0.004206, mae: 0.066748, mean_q: -0.336633
 12700/100000: episode: 127, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.866, mean reward: -0.179 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.854, 10.142], loss: 0.003299, mae: 0.059760, mean_q: -0.285704
 12800/100000: episode: 128, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.546, mean reward: -0.145 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.967, 10.482], loss: 0.003150, mae: 0.059137, mean_q: -0.311167
 12900/100000: episode: 129, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.562, mean reward: -0.176 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.880, 10.098], loss: 0.003107, mae: 0.057276, mean_q: -0.334589
 13000/100000: episode: 130, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.930, mean reward: -0.169 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.921, 10.098], loss: 0.002970, mae: 0.055999, mean_q: -0.324632
 13100/100000: episode: 131, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -12.941, mean reward: -0.129 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.594, 10.098], loss: 0.003118, mae: 0.056405, mean_q: -0.314595
 13200/100000: episode: 132, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -10.151, mean reward: -0.102 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.747, 10.518], loss: 0.003045, mae: 0.056687, mean_q: -0.326645
 13300/100000: episode: 133, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.549, mean reward: -0.155 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.208, 10.108], loss: 0.003366, mae: 0.059393, mean_q: -0.279930
 13400/100000: episode: 134, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.040, mean reward: -0.170 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.576, 10.098], loss: 0.003474, mae: 0.060981, mean_q: -0.291843
 13500/100000: episode: 135, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.571, mean reward: -0.176 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.050, 10.134], loss: 0.003240, mae: 0.059151, mean_q: -0.321211
 13600/100000: episode: 136, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -15.970, mean reward: -0.160 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.631, 10.098], loss: 0.003109, mae: 0.057084, mean_q: -0.303444
 13700/100000: episode: 137, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.323, mean reward: -0.153 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.250, 10.391], loss: 0.002846, mae: 0.054254, mean_q: -0.315832
 13800/100000: episode: 138, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -13.952, mean reward: -0.140 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.579, 10.098], loss: 0.003224, mae: 0.058359, mean_q: -0.294363
 13900/100000: episode: 139, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.405, mean reward: -0.174 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.044, 10.098], loss: 0.003400, mae: 0.060208, mean_q: -0.304045
 14000/100000: episode: 140, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.806, mean reward: -0.168 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.164, 10.259], loss: 0.003050, mae: 0.055619, mean_q: -0.331272
 14100/100000: episode: 141, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.247, mean reward: -0.152 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.599, 10.283], loss: 0.003303, mae: 0.059919, mean_q: -0.340261
 14200/100000: episode: 142, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.511, mean reward: -0.185 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.510, 10.139], loss: 0.003327, mae: 0.059698, mean_q: -0.334959
 14300/100000: episode: 143, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.256, mean reward: -0.183 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.082, 10.098], loss: 0.004837, mae: 0.068829, mean_q: -0.288491
 14400/100000: episode: 144, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.337, mean reward: -0.183 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.396, 10.098], loss: 0.003333, mae: 0.060151, mean_q: -0.318393
 14500/100000: episode: 145, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.252, mean reward: -0.183 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.684, 10.192], loss: 0.003059, mae: 0.056204, mean_q: -0.298096
 14600/100000: episode: 146, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -19.408, mean reward: -0.194 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.504, 10.173], loss: 0.003834, mae: 0.062993, mean_q: -0.344353
 14700/100000: episode: 147, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.809, mean reward: -0.168 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.003, 10.158], loss: 0.007599, mae: 0.080003, mean_q: -0.311858
 14800/100000: episode: 148, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -10.514, mean reward: -0.105 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.472, 10.148], loss: 0.003557, mae: 0.060415, mean_q: -0.305037
 14900/100000: episode: 149, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.140, mean reward: -0.181 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.650, 10.098], loss: 0.003444, mae: 0.059437, mean_q: -0.322049
 15000/100000: episode: 150, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -13.338, mean reward: -0.133 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.565, 10.100], loss: 0.003700, mae: 0.062053, mean_q: -0.285225
 15100/100000: episode: 151, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -12.264, mean reward: -0.123 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.785, 10.265], loss: 0.003119, mae: 0.057020, mean_q: -0.326735
 15200/100000: episode: 152, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.247, mean reward: -0.172 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.582, 10.098], loss: 0.004974, mae: 0.068015, mean_q: -0.324940
 15300/100000: episode: 153, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.198, mean reward: -0.162 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.665, 10.202], loss: 0.004353, mae: 0.066205, mean_q: -0.319085
 15400/100000: episode: 154, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.713, mean reward: -0.167 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.848, 10.129], loss: 0.003653, mae: 0.062308, mean_q: -0.300158
 15500/100000: episode: 155, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.923, mean reward: -0.189 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.871, 10.317], loss: 0.003784, mae: 0.063766, mean_q: -0.297428
 15600/100000: episode: 156, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.513, mean reward: -0.165 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.117, 10.098], loss: 0.003386, mae: 0.060450, mean_q: -0.316947
 15700/100000: episode: 157, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -20.220, mean reward: -0.202 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.767, 10.098], loss: 0.003169, mae: 0.059002, mean_q: -0.297335
 15800/100000: episode: 158, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.233, mean reward: -0.162 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.322, 10.098], loss: 0.003513, mae: 0.062914, mean_q: -0.312899
 15900/100000: episode: 159, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.728, mean reward: -0.177 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.828, 10.350], loss: 0.003122, mae: 0.058657, mean_q: -0.322670
 16000/100000: episode: 160, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.963, mean reward: -0.190 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.945, 10.180], loss: 0.003061, mae: 0.056618, mean_q: -0.315174
 16100/100000: episode: 161, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -15.410, mean reward: -0.154 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.117, 10.098], loss: 0.004432, mae: 0.068373, mean_q: -0.329581
 16200/100000: episode: 162, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.976, mean reward: -0.200 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.932, 10.098], loss: 0.005372, mae: 0.072721, mean_q: -0.321180
 16300/100000: episode: 163, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.923, mean reward: -0.169 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.650, 10.098], loss: 0.003199, mae: 0.058394, mean_q: -0.348326
 16400/100000: episode: 164, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.109, mean reward: -0.181 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.657, 10.098], loss: 0.003927, mae: 0.065005, mean_q: -0.278128
 16500/100000: episode: 165, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -16.363, mean reward: -0.164 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.678, 10.318], loss: 0.003138, mae: 0.057540, mean_q: -0.270977
 16600/100000: episode: 166, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -17.829, mean reward: -0.178 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.142, 10.133], loss: 0.003013, mae: 0.056253, mean_q: -0.326777
 16700/100000: episode: 167, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.625, mean reward: -0.166 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.786, 10.098], loss: 0.003015, mae: 0.056630, mean_q: -0.315957
 16800/100000: episode: 168, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -14.694, mean reward: -0.147 [-1.000, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.228, 10.291], loss: 0.004268, mae: 0.065472, mean_q: -0.317966
 16900/100000: episode: 169, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -13.125, mean reward: -0.131 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.315, 10.232], loss: 0.002777, mae: 0.053556, mean_q: -0.306887
 17000/100000: episode: 170, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.772, mean reward: -0.198 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.525, 10.098], loss: 0.002913, mae: 0.055407, mean_q: -0.310738
 17100/100000: episode: 171, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.087, mean reward: -0.151 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.462, 10.098], loss: 0.003790, mae: 0.062723, mean_q: -0.303237
 17200/100000: episode: 172, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.739, mean reward: -0.197 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.235, 10.205], loss: 0.003325, mae: 0.060011, mean_q: -0.312083
 17300/100000: episode: 173, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.492, mean reward: -0.165 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.012, 10.098], loss: 0.002678, mae: 0.051822, mean_q: -0.313890
 17400/100000: episode: 174, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.667, mean reward: -0.187 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.969, 10.098], loss: 0.002780, mae: 0.054099, mean_q: -0.322082
 17500/100000: episode: 175, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.891, mean reward: -0.179 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.006, 10.098], loss: 0.004429, mae: 0.067252, mean_q: -0.323030
 17600/100000: episode: 176, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.655, mean reward: -0.147 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.576, 10.218], loss: 0.003770, mae: 0.065230, mean_q: -0.285760
 17700/100000: episode: 177, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.122, mean reward: -0.191 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.369, 10.184], loss: 0.002865, mae: 0.054761, mean_q: -0.337373
 17800/100000: episode: 178, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.899, mean reward: -0.149 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.591, 10.266], loss: 0.002940, mae: 0.055058, mean_q: -0.301041
 17900/100000: episode: 179, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.325, mean reward: -0.173 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.850, 10.098], loss: 0.003057, mae: 0.056968, mean_q: -0.302868
 18000/100000: episode: 180, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.363, mean reward: -0.184 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.746, 10.098], loss: 0.003259, mae: 0.059489, mean_q: -0.293049
 18100/100000: episode: 181, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.592, mean reward: -0.166 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.500, 10.100], loss: 0.003030, mae: 0.056190, mean_q: -0.329120
 18200/100000: episode: 182, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.114, mean reward: -0.181 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.198, 10.103], loss: 0.002815, mae: 0.054424, mean_q: -0.310418
 18300/100000: episode: 183, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.962, mean reward: -0.190 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.442, 10.102], loss: 0.003221, mae: 0.057064, mean_q: -0.307059
 18400/100000: episode: 184, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.347, mean reward: -0.153 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.364, 10.132], loss: 0.003339, mae: 0.060256, mean_q: -0.278472
 18500/100000: episode: 185, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.812, mean reward: -0.188 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.863, 10.131], loss: 0.003240, mae: 0.057206, mean_q: -0.337311
 18600/100000: episode: 186, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -14.709, mean reward: -0.147 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.304, 10.098], loss: 0.003117, mae: 0.057397, mean_q: -0.301905
 18700/100000: episode: 187, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.250, mean reward: -0.182 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.808, 10.184], loss: 0.003351, mae: 0.060236, mean_q: -0.328429
 18800/100000: episode: 188, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.158, mean reward: -0.172 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.339, 10.098], loss: 0.007193, mae: 0.075631, mean_q: -0.322057
 18900/100000: episode: 189, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -12.116, mean reward: -0.121 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.367, 10.360], loss: 0.003001, mae: 0.056388, mean_q: -0.334039
 19000/100000: episode: 190, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.041, mean reward: -0.180 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.670, 10.317], loss: 0.003017, mae: 0.057237, mean_q: -0.285298
 19100/100000: episode: 191, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -13.238, mean reward: -0.132 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.308, 10.332], loss: 0.002979, mae: 0.055139, mean_q: -0.308802
 19200/100000: episode: 192, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.392, mean reward: -0.154 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.726, 10.098], loss: 0.002692, mae: 0.053080, mean_q: -0.306995
 19300/100000: episode: 193, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.510, mean reward: -0.125 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.531, 10.373], loss: 0.003025, mae: 0.055401, mean_q: -0.309411
 19400/100000: episode: 194, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -20.926, mean reward: -0.209 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.421, 10.098], loss: 0.003172, mae: 0.057978, mean_q: -0.308763
 19500/100000: episode: 195, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -12.996, mean reward: -0.130 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.004, 10.212], loss: 0.002800, mae: 0.054930, mean_q: -0.312112
 19600/100000: episode: 196, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.930, mean reward: -0.169 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.425, 10.098], loss: 0.003851, mae: 0.061962, mean_q: -0.332926
 19700/100000: episode: 197, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.814, mean reward: -0.168 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.637, 10.233], loss: 0.002886, mae: 0.053832, mean_q: -0.335871
 19800/100000: episode: 198, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -10.687, mean reward: -0.107 [-1.000, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.704, 10.290], loss: 0.005088, mae: 0.068976, mean_q: -0.308638
 19900/100000: episode: 199, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -14.903, mean reward: -0.149 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.434, 10.098], loss: 0.003268, mae: 0.059333, mean_q: -0.292018
 20000/100000: episode: 200, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.163, mean reward: -0.182 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.462, 10.098], loss: 0.003150, mae: 0.057221, mean_q: -0.337614
 20100/100000: episode: 201, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.475, mean reward: -0.195 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.862, 10.098], loss: 0.003191, mae: 0.058994, mean_q: -0.300145
 20200/100000: episode: 202, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.755, mean reward: -0.188 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.477, 10.098], loss: 0.002826, mae: 0.053864, mean_q: -0.291970
 20300/100000: episode: 203, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -9.265, mean reward: -0.093 [-1.000, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.647, 10.550], loss: 0.003480, mae: 0.060960, mean_q: -0.314229
 20400/100000: episode: 204, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.375, mean reward: -0.184 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.111, 10.098], loss: 0.003314, mae: 0.059743, mean_q: -0.295167
 20500/100000: episode: 205, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.767, mean reward: -0.168 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.333, 10.098], loss: 0.003086, mae: 0.057333, mean_q: -0.328621
 20600/100000: episode: 206, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.998, mean reward: -0.180 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.995, 10.098], loss: 0.003106, mae: 0.056491, mean_q: -0.291589
 20700/100000: episode: 207, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.466, mean reward: -0.175 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.258, 10.098], loss: 0.002904, mae: 0.054974, mean_q: -0.307146
 20800/100000: episode: 208, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.035, mean reward: -0.160 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.080, 10.176], loss: 0.003051, mae: 0.056501, mean_q: -0.327239
 20900/100000: episode: 209, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.248, mean reward: -0.182 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.571, 10.290], loss: 0.003483, mae: 0.059400, mean_q: -0.289940
 21000/100000: episode: 210, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -11.383, mean reward: -0.114 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.597, 10.098], loss: 0.003549, mae: 0.062969, mean_q: -0.294521
 21100/100000: episode: 211, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.506, mean reward: -0.195 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.335, 10.280], loss: 0.002978, mae: 0.055187, mean_q: -0.274941
 21200/100000: episode: 212, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.546, mean reward: -0.145 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.971, 10.098], loss: 0.003257, mae: 0.059366, mean_q: -0.351346
 21300/100000: episode: 213, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.838, mean reward: -0.168 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.499, 10.098], loss: 0.003091, mae: 0.057141, mean_q: -0.288733
 21400/100000: episode: 214, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -14.950, mean reward: -0.149 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.566, 10.398], loss: 0.003062, mae: 0.055999, mean_q: -0.296772
 21500/100000: episode: 215, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -20.113, mean reward: -0.201 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.403, 10.207], loss: 0.003218, mae: 0.058492, mean_q: -0.325633
 21600/100000: episode: 216, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.078, mean reward: -0.161 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.422, 10.098], loss: 0.003327, mae: 0.060652, mean_q: -0.332405
 21700/100000: episode: 217, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -10.392, mean reward: -0.104 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.087, 10.098], loss: 0.002839, mae: 0.055436, mean_q: -0.305107
 21800/100000: episode: 218, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.809, mean reward: -0.178 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.562, 10.098], loss: 0.002962, mae: 0.055399, mean_q: -0.285004
 21900/100000: episode: 219, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.163, mean reward: -0.182 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.966, 10.239], loss: 0.003368, mae: 0.059611, mean_q: -0.295479
 22000/100000: episode: 220, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.564, mean reward: -0.186 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.362, 10.146], loss: 0.003529, mae: 0.059786, mean_q: -0.310071
 22100/100000: episode: 221, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -14.186, mean reward: -0.142 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.757, 10.098], loss: 0.003577, mae: 0.062461, mean_q: -0.302341
 22200/100000: episode: 222, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.995, mean reward: -0.180 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.254, 10.150], loss: 0.003058, mae: 0.057587, mean_q: -0.257161
 22300/100000: episode: 223, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.309, mean reward: -0.183 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.680, 10.098], loss: 0.003005, mae: 0.056170, mean_q: -0.319976
 22400/100000: episode: 224, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.164, mean reward: -0.172 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.456, 10.098], loss: 0.003065, mae: 0.058311, mean_q: -0.297804
 22500/100000: episode: 225, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.795, mean reward: -0.178 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.610, 10.338], loss: 0.004341, mae: 0.067405, mean_q: -0.323086
 22600/100000: episode: 226, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -12.682, mean reward: -0.127 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.483, 10.218], loss: 0.003180, mae: 0.057428, mean_q: -0.307136
 22700/100000: episode: 227, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -15.508, mean reward: -0.155 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.503, 10.294], loss: 0.004050, mae: 0.067464, mean_q: -0.317964
 22800/100000: episode: 228, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -15.983, mean reward: -0.160 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.220, 10.271], loss: 0.003300, mae: 0.057758, mean_q: -0.310875
 22900/100000: episode: 229, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.766, mean reward: -0.178 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.936, 10.204], loss: 0.003544, mae: 0.059869, mean_q: -0.330740
 23000/100000: episode: 230, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -10.041, mean reward: -0.100 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.477, 10.460], loss: 0.002903, mae: 0.054946, mean_q: -0.346930
 23100/100000: episode: 231, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -13.538, mean reward: -0.135 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.540, 10.098], loss: 0.003423, mae: 0.061414, mean_q: -0.288070
 23200/100000: episode: 232, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.667, mean reward: -0.167 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.098, 10.135], loss: 0.003397, mae: 0.059436, mean_q: -0.347147
 23300/100000: episode: 233, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.216, mean reward: -0.172 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.881, 10.119], loss: 0.003123, mae: 0.056727, mean_q: -0.286051
 23400/100000: episode: 234, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.578, mean reward: -0.176 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.609, 10.140], loss: 0.003399, mae: 0.059014, mean_q: -0.321999
 23500/100000: episode: 235, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.256, mean reward: -0.193 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.646, 10.228], loss: 0.002996, mae: 0.056080, mean_q: -0.308742
 23600/100000: episode: 236, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -11.345, mean reward: -0.113 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.536, 10.098], loss: 0.003071, mae: 0.055594, mean_q: -0.322079
 23700/100000: episode: 237, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -16.296, mean reward: -0.163 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.202, 10.233], loss: 0.002719, mae: 0.052642, mean_q: -0.321499
 23800/100000: episode: 238, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.081, mean reward: -0.171 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.785, 10.257], loss: 0.003426, mae: 0.060456, mean_q: -0.298370
 23900/100000: episode: 239, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.983, mean reward: -0.150 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.506, 10.098], loss: 0.002929, mae: 0.054923, mean_q: -0.297509
 24000/100000: episode: 240, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.126, mean reward: -0.181 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.126, 10.098], loss: 0.003484, mae: 0.059360, mean_q: -0.294404
 24100/100000: episode: 241, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.621, mean reward: -0.196 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.575, 10.098], loss: 0.003163, mae: 0.058065, mean_q: -0.262699
 24200/100000: episode: 242, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.101, mean reward: -0.191 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.834, 10.289], loss: 0.003212, mae: 0.057399, mean_q: -0.349934
 24300/100000: episode: 243, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.843, mean reward: -0.198 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.391, 10.106], loss: 0.002985, mae: 0.055619, mean_q: -0.284726
 24400/100000: episode: 244, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.483, mean reward: -0.165 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.463, 10.174], loss: 0.002588, mae: 0.051059, mean_q: -0.315846
 24500/100000: episode: 245, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -14.889, mean reward: -0.149 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.325, 10.113], loss: 0.003358, mae: 0.057131, mean_q: -0.311046
 24600/100000: episode: 246, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -18.174, mean reward: -0.182 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.090, 10.098], loss: 0.003116, mae: 0.058110, mean_q: -0.334761
 24700/100000: episode: 247, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.471, mean reward: -0.165 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.554, 10.374], loss: 0.003087, mae: 0.055420, mean_q: -0.326254
 24800/100000: episode: 248, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.577, mean reward: -0.146 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.189, 10.098], loss: 0.002885, mae: 0.053281, mean_q: -0.332202
 24900/100000: episode: 249, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.310, mean reward: -0.163 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.682, 10.173], loss: 0.003073, mae: 0.056995, mean_q: -0.333712
 25000/100000: episode: 250, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.722, mean reward: -0.167 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.436, 10.251], loss: 0.002737, mae: 0.053174, mean_q: -0.320115
 25100/100000: episode: 251, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -12.791, mean reward: -0.128 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.081, 10.404], loss: 0.002791, mae: 0.052823, mean_q: -0.325443
 25200/100000: episode: 252, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -15.495, mean reward: -0.155 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.285, 10.098], loss: 0.003347, mae: 0.058751, mean_q: -0.348815
 25300/100000: episode: 253, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.878, mean reward: -0.169 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.278, 10.210], loss: 0.003315, mae: 0.058753, mean_q: -0.287093
 25400/100000: episode: 254, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -17.406, mean reward: -0.174 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.317, 10.098], loss: 0.007204, mae: 0.080641, mean_q: -0.264051
 25500/100000: episode: 255, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.272, mean reward: -0.163 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.445, 10.098], loss: 0.003804, mae: 0.061956, mean_q: -0.320810
 25600/100000: episode: 256, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.667, mean reward: -0.167 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.584, 10.098], loss: 0.002892, mae: 0.054267, mean_q: -0.324490
 25700/100000: episode: 257, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.641, mean reward: -0.166 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.107, 10.330], loss: 0.002872, mae: 0.053322, mean_q: -0.342159
 25800/100000: episode: 258, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -13.079, mean reward: -0.131 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.655, 10.098], loss: 0.002960, mae: 0.053432, mean_q: -0.311903
 25900/100000: episode: 259, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.116, mean reward: -0.191 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.468, 10.156], loss: 0.003031, mae: 0.054816, mean_q: -0.278360
 26000/100000: episode: 260, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.763, mean reward: -0.188 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.929, 10.098], loss: 0.002849, mae: 0.053016, mean_q: -0.300430
 26100/100000: episode: 261, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -14.716, mean reward: -0.147 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.841, 10.098], loss: 0.002978, mae: 0.054290, mean_q: -0.325612
 26200/100000: episode: 262, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.870, mean reward: -0.169 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.350, 10.098], loss: 0.003531, mae: 0.061157, mean_q: -0.321253
 26300/100000: episode: 263, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -13.137, mean reward: -0.131 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.216, 10.102], loss: 0.003191, mae: 0.056593, mean_q: -0.332167
 26400/100000: episode: 264, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -20.129, mean reward: -0.201 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.652, 10.210], loss: 0.002936, mae: 0.055446, mean_q: -0.308759
 26500/100000: episode: 265, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.487, mean reward: -0.195 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.090, 10.288], loss: 0.003034, mae: 0.055092, mean_q: -0.279118
 26600/100000: episode: 266, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -11.269, mean reward: -0.113 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.134, 10.451], loss: 0.003218, mae: 0.056942, mean_q: -0.319289
 26700/100000: episode: 267, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.866, mean reward: -0.169 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.392, 10.098], loss: 0.003284, mae: 0.058182, mean_q: -0.311797
 26800/100000: episode: 268, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.074, mean reward: -0.171 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.080, 10.152], loss: 0.002792, mae: 0.052227, mean_q: -0.317739
 26900/100000: episode: 269, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.177, mean reward: -0.182 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.122, 10.098], loss: 0.002912, mae: 0.053888, mean_q: -0.344754
 27000/100000: episode: 270, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.248, mean reward: -0.142 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.445, 10.215], loss: 0.003805, mae: 0.062292, mean_q: -0.330052
 27100/100000: episode: 271, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -16.247, mean reward: -0.162 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.347, 10.098], loss: 0.004449, mae: 0.067982, mean_q: -0.303769
 27200/100000: episode: 272, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -20.430, mean reward: -0.204 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.896, 10.174], loss: 0.003052, mae: 0.056061, mean_q: -0.314717
 27300/100000: episode: 273, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.832, mean reward: -0.178 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.433, 10.126], loss: 0.002841, mae: 0.054178, mean_q: -0.278412
 27400/100000: episode: 274, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.468, mean reward: -0.135 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.653, 10.098], loss: 0.002862, mae: 0.055495, mean_q: -0.294656
 27500/100000: episode: 275, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.296, mean reward: -0.163 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.534, 10.182], loss: 0.002824, mae: 0.054244, mean_q: -0.312540
 27600/100000: episode: 276, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.287, mean reward: -0.173 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.484, 10.139], loss: 0.002593, mae: 0.050867, mean_q: -0.338997
 27700/100000: episode: 277, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.139, mean reward: -0.181 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.913, 10.242], loss: 0.003015, mae: 0.055842, mean_q: -0.303081
 27800/100000: episode: 278, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.829, mean reward: -0.198 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.354, 10.101], loss: 0.003397, mae: 0.061057, mean_q: -0.275356
 27900/100000: episode: 279, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.551, mean reward: -0.196 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.911, 10.174], loss: 0.003182, mae: 0.059097, mean_q: -0.306053
 28000/100000: episode: 280, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.102, mean reward: -0.181 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.269, 10.098], loss: 0.002858, mae: 0.054664, mean_q: -0.333184
 28100/100000: episode: 281, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.024, mean reward: -0.160 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.535, 10.364], loss: 0.002640, mae: 0.052927, mean_q: -0.330971
 28200/100000: episode: 282, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.283, mean reward: -0.163 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.837, 10.396], loss: 0.002605, mae: 0.050845, mean_q: -0.322120
 28300/100000: episode: 283, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.346, mean reward: -0.163 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.242, 10.147], loss: 0.002785, mae: 0.053902, mean_q: -0.339849
 28400/100000: episode: 284, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.406, mean reward: -0.164 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.115, 10.098], loss: 0.002675, mae: 0.052006, mean_q: -0.328839
 28500/100000: episode: 285, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.152, mean reward: -0.182 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.961, 10.098], loss: 0.002585, mae: 0.050919, mean_q: -0.319888
 28600/100000: episode: 286, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.816, mean reward: -0.178 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.717, 10.098], loss: 0.002709, mae: 0.052050, mean_q: -0.302694
 28700/100000: episode: 287, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.487, mean reward: -0.195 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.094, 10.098], loss: 0.002929, mae: 0.054565, mean_q: -0.307418
 28800/100000: episode: 288, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.194, mean reward: -0.182 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.576, 10.098], loss: 0.003204, mae: 0.059199, mean_q: -0.314405
 28900/100000: episode: 289, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.969, mean reward: -0.180 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.222, 10.278], loss: 0.003130, mae: 0.058529, mean_q: -0.326767
 29000/100000: episode: 290, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.980, mean reward: -0.180 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.967, 10.164], loss: 0.002886, mae: 0.055636, mean_q: -0.355546
 29100/100000: episode: 291, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -13.839, mean reward: -0.138 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.512, 10.098], loss: 0.005848, mae: 0.072038, mean_q: -0.328575
 29200/100000: episode: 292, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.474, mean reward: -0.165 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.869, 10.098], loss: 0.003663, mae: 0.060421, mean_q: -0.332486
 29300/100000: episode: 293, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -15.139, mean reward: -0.151 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.209, 10.337], loss: 0.003024, mae: 0.056611, mean_q: -0.347176
 29400/100000: episode: 294, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.986, mean reward: -0.190 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.586, 10.098], loss: 0.002701, mae: 0.053597, mean_q: -0.363249
 29500/100000: episode: 295, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.276, mean reward: -0.193 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.190, 10.098], loss: 0.002739, mae: 0.052714, mean_q: -0.297936
 29600/100000: episode: 296, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.666, mean reward: -0.197 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.527, 10.124], loss: 0.002659, mae: 0.051674, mean_q: -0.338814
 29700/100000: episode: 297, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.293, mean reward: -0.173 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.087, 10.326], loss: 0.002920, mae: 0.054594, mean_q: -0.313633
 29800/100000: episode: 298, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.663, mean reward: -0.177 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.689, 10.098], loss: 0.004064, mae: 0.062716, mean_q: -0.308158
 29900/100000: episode: 299, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.696, mean reward: -0.187 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.810, 10.158], loss: 0.003300, mae: 0.060568, mean_q: -0.307431
 30000/100000: episode: 300, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -14.528, mean reward: -0.145 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.749, 10.151], loss: 0.002339, mae: 0.049525, mean_q: -0.314710
 30100/100000: episode: 301, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -17.891, mean reward: -0.179 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.555, 10.168], loss: 0.003024, mae: 0.056012, mean_q: -0.321379
 30200/100000: episode: 302, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.621, mean reward: -0.176 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.321, 10.273], loss: 0.003536, mae: 0.060424, mean_q: -0.313379
 30300/100000: episode: 303, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.590, mean reward: -0.196 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.699, 10.157], loss: 0.003374, mae: 0.060963, mean_q: -0.322903
 30400/100000: episode: 304, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -13.283, mean reward: -0.133 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.691, 10.301], loss: 0.003141, mae: 0.058036, mean_q: -0.304381
 30500/100000: episode: 305, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -16.799, mean reward: -0.168 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.130, 10.227], loss: 0.002676, mae: 0.052841, mean_q: -0.352297
 30600/100000: episode: 306, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.941, mean reward: -0.169 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.257, 10.139], loss: 0.002441, mae: 0.050040, mean_q: -0.317851
 30700/100000: episode: 307, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.512, mean reward: -0.155 [-1.000, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.499, 10.098], loss: 0.002381, mae: 0.049186, mean_q: -0.329748
 30800/100000: episode: 308, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.426, mean reward: -0.174 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.554, 10.098], loss: 0.002646, mae: 0.051562, mean_q: -0.308953
 30900/100000: episode: 309, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -16.382, mean reward: -0.164 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.732, 10.163], loss: 0.002624, mae: 0.052333, mean_q: -0.289974
 31000/100000: episode: 310, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.399, mean reward: -0.184 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.639, 10.248], loss: 0.002654, mae: 0.053206, mean_q: -0.332651
 31100/100000: episode: 311, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.571, mean reward: -0.156 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.192, 10.211], loss: 0.002534, mae: 0.052363, mean_q: -0.377200
 31200/100000: episode: 312, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -10.024, mean reward: -0.100 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.742, 10.517], loss: 0.002631, mae: 0.053500, mean_q: -0.328642
 31300/100000: episode: 313, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.153, mean reward: -0.172 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.797, 10.283], loss: 0.002627, mae: 0.051605, mean_q: -0.322138
 31400/100000: episode: 314, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.149, mean reward: -0.151 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.773, 10.098], loss: 0.003703, mae: 0.059976, mean_q: -0.306365
 31500/100000: episode: 315, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.648, mean reward: -0.176 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.866, 10.104], loss: 0.003214, mae: 0.058485, mean_q: -0.319411
 31600/100000: episode: 316, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.931, mean reward: -0.189 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.904, 10.098], loss: 0.002630, mae: 0.052080, mean_q: -0.305868
 31700/100000: episode: 317, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -14.907, mean reward: -0.149 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.601, 10.098], loss: 0.003718, mae: 0.061027, mean_q: -0.302251
 31800/100000: episode: 318, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -20.423, mean reward: -0.204 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.432, 10.171], loss: 0.004107, mae: 0.066044, mean_q: -0.308894
 31900/100000: episode: 319, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.330, mean reward: -0.183 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.046, 10.349], loss: 0.002529, mae: 0.051371, mean_q: -0.359612
 32000/100000: episode: 320, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.448, mean reward: -0.154 [-1.000, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.199, 10.098], loss: 0.002925, mae: 0.055478, mean_q: -0.297011
 32100/100000: episode: 321, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.445, mean reward: -0.144 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.141, 10.242], loss: 0.002646, mae: 0.052198, mean_q: -0.325500
 32200/100000: episode: 322, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -12.076, mean reward: -0.121 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.224, 10.258], loss: 0.002575, mae: 0.050891, mean_q: -0.313192
 32300/100000: episode: 323, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.395, mean reward: -0.174 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.851, 10.261], loss: 0.002511, mae: 0.050981, mean_q: -0.302066
 32400/100000: episode: 324, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.076, mean reward: -0.181 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.451, 10.151], loss: 0.002695, mae: 0.053215, mean_q: -0.357137
 32500/100000: episode: 325, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -15.545, mean reward: -0.155 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.924, 10.098], loss: 0.002796, mae: 0.053603, mean_q: -0.300065
 32600/100000: episode: 326, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -17.239, mean reward: -0.172 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.804, 10.133], loss: 0.002511, mae: 0.051948, mean_q: -0.287550
 32700/100000: episode: 327, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.510, mean reward: -0.185 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.250, 10.368], loss: 0.003259, mae: 0.058520, mean_q: -0.336690
 32800/100000: episode: 328, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -12.394, mean reward: -0.124 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.704, 10.507], loss: 0.002881, mae: 0.055869, mean_q: -0.323370
 32900/100000: episode: 329, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -14.142, mean reward: -0.141 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.370, 10.098], loss: 0.002507, mae: 0.050647, mean_q: -0.285346
 33000/100000: episode: 330, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.254, mean reward: -0.183 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.282, 10.316], loss: 0.002589, mae: 0.051189, mean_q: -0.327366
 33100/100000: episode: 331, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -16.803, mean reward: -0.168 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.819, 10.122], loss: 0.002737, mae: 0.055166, mean_q: -0.298110
 33200/100000: episode: 332, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.994, mean reward: -0.190 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.112, 10.100], loss: 0.002938, mae: 0.055711, mean_q: -0.338133
 33300/100000: episode: 333, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -19.784, mean reward: -0.198 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.242, 10.098], loss: 0.002920, mae: 0.055588, mean_q: -0.280466
 33400/100000: episode: 334, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -14.414, mean reward: -0.144 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.101, 10.363], loss: 0.002867, mae: 0.055728, mean_q: -0.295695
 33500/100000: episode: 335, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -12.610, mean reward: -0.126 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.038, 10.098], loss: 0.002769, mae: 0.053208, mean_q: -0.308611
 33600/100000: episode: 336, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -18.270, mean reward: -0.183 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.614, 10.098], loss: 0.003157, mae: 0.058265, mean_q: -0.337561
 33700/100000: episode: 337, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.500, mean reward: -0.165 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.608, 10.098], loss: 0.002962, mae: 0.057970, mean_q: -0.279286
 33800/100000: episode: 338, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.402, mean reward: -0.184 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.255, 10.232], loss: 0.002558, mae: 0.052255, mean_q: -0.294808
 33900/100000: episode: 339, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.301, mean reward: -0.153 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.394, 10.098], loss: 0.002613, mae: 0.051870, mean_q: -0.321884
 34000/100000: episode: 340, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.317, mean reward: -0.183 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.053, 10.098], loss: 0.003244, mae: 0.059637, mean_q: -0.293002
 34100/100000: episode: 341, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.902, mean reward: -0.169 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.081, 10.098], loss: 0.002454, mae: 0.049807, mean_q: -0.326990
 34200/100000: episode: 342, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.593, mean reward: -0.146 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.767, 10.098], loss: 0.002624, mae: 0.051840, mean_q: -0.304059
 34300/100000: episode: 343, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.732, mean reward: -0.187 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.784, 10.146], loss: 0.002588, mae: 0.051679, mean_q: -0.301011
 34400/100000: episode: 344, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.464, mean reward: -0.165 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.047, 10.098], loss: 0.002710, mae: 0.052656, mean_q: -0.319876
 34500/100000: episode: 345, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.141, mean reward: -0.171 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.652, 10.142], loss: 0.002604, mae: 0.052308, mean_q: -0.310044
 34600/100000: episode: 346, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.713, mean reward: -0.157 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.195, 10.098], loss: 0.002664, mae: 0.052979, mean_q: -0.321301
 34700/100000: episode: 347, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.405, mean reward: -0.184 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.846, 10.160], loss: 0.002972, mae: 0.055860, mean_q: -0.311945
 34800/100000: episode: 348, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.612, mean reward: -0.176 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.882, 10.311], loss: 0.002796, mae: 0.053830, mean_q: -0.318613
 34900/100000: episode: 349, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.286, mean reward: -0.183 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.500, 10.098], loss: 0.003012, mae: 0.055426, mean_q: -0.308519
 35000/100000: episode: 350, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.575, mean reward: -0.156 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.705, 10.251], loss: 0.002719, mae: 0.053120, mean_q: -0.352714
 35100/100000: episode: 351, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -6.347, mean reward: -0.063 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.200, 10.459], loss: 0.002770, mae: 0.053066, mean_q: -0.351553
 35200/100000: episode: 352, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -15.162, mean reward: -0.152 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.636, 10.098], loss: 0.004382, mae: 0.060707, mean_q: -0.328903
 35300/100000: episode: 353, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.746, mean reward: -0.167 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.516, 10.248], loss: 0.004343, mae: 0.065486, mean_q: -0.278569
 35400/100000: episode: 354, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -11.382, mean reward: -0.114 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.653, 10.231], loss: 0.002888, mae: 0.054577, mean_q: -0.312622
 35500/100000: episode: 355, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.163, mean reward: -0.172 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.886, 10.098], loss: 0.002656, mae: 0.051246, mean_q: -0.297641
 35600/100000: episode: 356, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -19.697, mean reward: -0.197 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.099, 10.129], loss: 0.002658, mae: 0.052501, mean_q: -0.304922
 35700/100000: episode: 357, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -15.063, mean reward: -0.151 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.878, 10.098], loss: 0.002629, mae: 0.051589, mean_q: -0.275709
 35800/100000: episode: 358, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -17.884, mean reward: -0.179 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.567, 10.158], loss: 0.002624, mae: 0.050551, mean_q: -0.306805
 35900/100000: episode: 359, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.587, mean reward: -0.176 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.802, 10.098], loss: 0.002946, mae: 0.054569, mean_q: -0.335033
 36000/100000: episode: 360, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.696, mean reward: -0.187 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.907, 10.098], loss: 0.004459, mae: 0.066567, mean_q: -0.335849
 36100/100000: episode: 361, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.776, mean reward: -0.158 [-1.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.873, 10.098], loss: 0.002984, mae: 0.056747, mean_q: -0.319820
 36200/100000: episode: 362, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.330, mean reward: -0.163 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.290, 10.098], loss: 0.003176, mae: 0.057446, mean_q: -0.308928
 36300/100000: episode: 363, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.744, mean reward: -0.197 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.455, 10.169], loss: 0.002977, mae: 0.055629, mean_q: -0.332154
 36400/100000: episode: 364, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.616, mean reward: -0.186 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.618, 10.098], loss: 0.002921, mae: 0.055033, mean_q: -0.329596
 36500/100000: episode: 365, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.582, mean reward: -0.166 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.593, 10.335], loss: 0.003439, mae: 0.060873, mean_q: -0.320209
 36600/100000: episode: 366, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -15.212, mean reward: -0.152 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.939, 10.166], loss: 0.002595, mae: 0.051408, mean_q: -0.308909
 36700/100000: episode: 367, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.066, mean reward: -0.171 [-1.000, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.180, 10.098], loss: 0.002893, mae: 0.055464, mean_q: -0.296668
 36800/100000: episode: 368, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.656, mean reward: -0.177 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.756, 10.274], loss: 0.002618, mae: 0.052500, mean_q: -0.291796
 36900/100000: episode: 369, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.065, mean reward: -0.181 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.904, 10.098], loss: 0.002816, mae: 0.052686, mean_q: -0.325096
 37000/100000: episode: 370, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.950, mean reward: -0.179 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.828, 10.171], loss: 0.009569, mae: 0.089057, mean_q: -0.305435
 37100/100000: episode: 371, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.547, mean reward: -0.175 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.923, 10.098], loss: 0.003604, mae: 0.060831, mean_q: -0.312033
 37200/100000: episode: 372, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.343, mean reward: -0.153 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.377, 10.254], loss: 0.002961, mae: 0.055877, mean_q: -0.339830
 37300/100000: episode: 373, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -13.422, mean reward: -0.134 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.150, 10.098], loss: 0.002675, mae: 0.051711, mean_q: -0.314176
 37400/100000: episode: 374, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -11.578, mean reward: -0.116 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.776, 10.276], loss: 0.002704, mae: 0.052714, mean_q: -0.312266
 37500/100000: episode: 375, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.559, mean reward: -0.176 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.029, 10.191], loss: 0.003236, mae: 0.058416, mean_q: -0.320034
 37600/100000: episode: 376, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.498, mean reward: -0.155 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.086, 10.098], loss: 0.003043, mae: 0.055778, mean_q: -0.280499
 37700/100000: episode: 377, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.851, mean reward: -0.199 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.283, 10.185], loss: 0.002700, mae: 0.052118, mean_q: -0.325507
 37800/100000: episode: 378, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.122, mean reward: -0.171 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.703, 10.209], loss: 0.002944, mae: 0.055862, mean_q: -0.327326
 37900/100000: episode: 379, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -19.497, mean reward: -0.195 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.460, 10.156], loss: 0.002727, mae: 0.051441, mean_q: -0.338891
 38000/100000: episode: 380, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.628, mean reward: -0.176 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.549, 10.098], loss: 0.002530, mae: 0.048665, mean_q: -0.368078
 38100/100000: episode: 381, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.793, mean reward: -0.148 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.009, 10.271], loss: 0.002956, mae: 0.054660, mean_q: -0.338433
 38200/100000: episode: 382, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.915, mean reward: -0.159 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.782, 10.098], loss: 0.002579, mae: 0.050691, mean_q: -0.296710
 38300/100000: episode: 383, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.348, mean reward: -0.173 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.994, 10.163], loss: 0.003294, mae: 0.056263, mean_q: -0.318273
 38400/100000: episode: 384, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.458, mean reward: -0.175 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.144, 10.098], loss: 0.003603, mae: 0.062526, mean_q: -0.336841
 38500/100000: episode: 385, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.805, mean reward: -0.178 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.450, 10.098], loss: 0.002909, mae: 0.056143, mean_q: -0.303373
 38600/100000: episode: 386, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.582, mean reward: -0.186 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.702, 10.098], loss: 0.002903, mae: 0.054843, mean_q: -0.268000
 38700/100000: episode: 387, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.010, mean reward: -0.160 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.072, 10.157], loss: 0.003752, mae: 0.062782, mean_q: -0.291774
 38800/100000: episode: 388, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.623, mean reward: -0.166 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.282, 10.192], loss: 0.002897, mae: 0.054789, mean_q: -0.339587
 38900/100000: episode: 389, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.492, mean reward: -0.155 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.851, 10.278], loss: 0.002873, mae: 0.054307, mean_q: -0.314507
 39000/100000: episode: 390, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -16.789, mean reward: -0.168 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.124, 10.344], loss: 0.002886, mae: 0.053682, mean_q: -0.316251
 39100/100000: episode: 391, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.232, mean reward: -0.142 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.644, 10.098], loss: 0.003021, mae: 0.055742, mean_q: -0.327955
 39200/100000: episode: 392, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -17.355, mean reward: -0.174 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.750, 10.180], loss: 0.002799, mae: 0.053028, mean_q: -0.301056
 39300/100000: episode: 393, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.604, mean reward: -0.186 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.784, 10.098], loss: 0.002820, mae: 0.053738, mean_q: -0.313945
 39400/100000: episode: 394, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -20.555, mean reward: -0.206 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.094, 10.117], loss: 0.002662, mae: 0.052085, mean_q: -0.328787
 39500/100000: episode: 395, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.535, mean reward: -0.175 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.242, 10.178], loss: 0.002901, mae: 0.055315, mean_q: -0.301238
 39600/100000: episode: 396, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -13.971, mean reward: -0.140 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.782, 10.098], loss: 0.002750, mae: 0.054200, mean_q: -0.288280
 39700/100000: episode: 397, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.900, mean reward: -0.149 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.375, 10.098], loss: 0.002941, mae: 0.054525, mean_q: -0.334446
 39800/100000: episode: 398, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -14.769, mean reward: -0.148 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.740, 10.098], loss: 0.002648, mae: 0.052476, mean_q: -0.320221
 39900/100000: episode: 399, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.294, mean reward: -0.193 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.265, 10.098], loss: 0.003022, mae: 0.056551, mean_q: -0.302438
 40000/100000: episode: 400, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.107, mean reward: -0.181 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.573, 10.196], loss: 0.002584, mae: 0.051085, mean_q: -0.326321
 40100/100000: episode: 401, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.845, mean reward: -0.178 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.229, 10.155], loss: 0.002741, mae: 0.052776, mean_q: -0.298126
 40200/100000: episode: 402, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -7.268, mean reward: -0.073 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.385, 10.257], loss: 0.002678, mae: 0.053791, mean_q: -0.307436
 40300/100000: episode: 403, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.059, mean reward: -0.161 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.974, 10.264], loss: 0.002662, mae: 0.051730, mean_q: -0.324195
 40400/100000: episode: 404, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -14.944, mean reward: -0.149 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.613, 10.098], loss: 0.002873, mae: 0.054696, mean_q: -0.309652
 40500/100000: episode: 405, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.621, mean reward: -0.146 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.967, 10.098], loss: 0.002675, mae: 0.052420, mean_q: -0.325655
 40600/100000: episode: 406, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.349, mean reward: -0.163 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.643, 10.181], loss: 0.003233, mae: 0.057919, mean_q: -0.324269
 40700/100000: episode: 407, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -11.285, mean reward: -0.113 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.538, 10.360], loss: 0.003165, mae: 0.058529, mean_q: -0.307947
 40800/100000: episode: 408, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -8.295, mean reward: -0.083 [-1.000, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.426, 10.098], loss: 0.003146, mae: 0.058809, mean_q: -0.295495
 40900/100000: episode: 409, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -11.843, mean reward: -0.118 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.443, 10.380], loss: 0.002784, mae: 0.053266, mean_q: -0.318905
 41000/100000: episode: 410, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -15.338, mean reward: -0.153 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.020, 10.098], loss: 0.002755, mae: 0.053355, mean_q: -0.320812
 41100/100000: episode: 411, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.189, mean reward: -0.152 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.771, 10.098], loss: 0.002840, mae: 0.053621, mean_q: -0.292793
 41200/100000: episode: 412, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.251, mean reward: -0.183 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.742, 10.098], loss: 0.003480, mae: 0.059847, mean_q: -0.308585
 41300/100000: episode: 413, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.562, mean reward: -0.176 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.518, 10.146], loss: 0.002653, mae: 0.052169, mean_q: -0.296737
 41400/100000: episode: 414, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.288, mean reward: -0.173 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.334, 10.105], loss: 0.002689, mae: 0.053793, mean_q: -0.297166
 41500/100000: episode: 415, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.210, mean reward: -0.182 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.555, 10.098], loss: 0.002899, mae: 0.054821, mean_q: -0.301744
 41600/100000: episode: 416, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.005, mean reward: -0.140 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.369, 10.098], loss: 0.002509, mae: 0.049535, mean_q: -0.322779
 41700/100000: episode: 417, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.752, mean reward: -0.158 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.701, 10.420], loss: 0.002950, mae: 0.055383, mean_q: -0.274310
 41800/100000: episode: 418, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.007, mean reward: -0.170 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.480, 10.098], loss: 0.003018, mae: 0.057196, mean_q: -0.274959
 41900/100000: episode: 419, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.820, mean reward: -0.158 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.891, 10.177], loss: 0.003341, mae: 0.059825, mean_q: -0.286762
 42000/100000: episode: 420, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -18.905, mean reward: -0.189 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.749, 10.268], loss: 0.002715, mae: 0.054694, mean_q: -0.293612
 42100/100000: episode: 421, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.314, mean reward: -0.143 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.942, 10.247], loss: 0.002772, mae: 0.053226, mean_q: -0.297907
 42200/100000: episode: 422, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.261, mean reward: -0.183 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.321, 10.098], loss: 0.003029, mae: 0.053846, mean_q: -0.334765
 42300/100000: episode: 423, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.801, mean reward: -0.178 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.912, 10.098], loss: 0.002568, mae: 0.051582, mean_q: -0.295688
 42400/100000: episode: 424, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -10.997, mean reward: -0.110 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.757, 10.098], loss: 0.002485, mae: 0.050379, mean_q: -0.292231
 42500/100000: episode: 425, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -16.531, mean reward: -0.165 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.811, 10.098], loss: 0.002608, mae: 0.051915, mean_q: -0.285310
 42600/100000: episode: 426, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.835, mean reward: -0.168 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.983, 10.126], loss: 0.004831, mae: 0.066868, mean_q: -0.318184
 42700/100000: episode: 427, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.545, mean reward: -0.185 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.926, 10.098], loss: 0.002541, mae: 0.051979, mean_q: -0.287122
 42800/100000: episode: 428, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.212, mean reward: -0.172 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.421, 10.098], loss: 0.002740, mae: 0.053507, mean_q: -0.290637
 42900/100000: episode: 429, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.844, mean reward: -0.188 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.450, 10.232], loss: 0.002483, mae: 0.050596, mean_q: -0.292679
 43000/100000: episode: 430, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -13.999, mean reward: -0.140 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.252, 10.175], loss: 0.002323, mae: 0.048833, mean_q: -0.290752
 43100/100000: episode: 431, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -10.077, mean reward: -0.101 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.351, 10.265], loss: 0.002469, mae: 0.050938, mean_q: -0.300031
 43200/100000: episode: 432, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.232, mean reward: -0.192 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.552, 10.151], loss: 0.002698, mae: 0.052854, mean_q: -0.258824
 43300/100000: episode: 433, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.490, mean reward: -0.185 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.894, 10.098], loss: 0.003380, mae: 0.057318, mean_q: -0.303361
 43400/100000: episode: 434, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -15.529, mean reward: -0.155 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.877, 10.098], loss: 0.002975, mae: 0.057812, mean_q: -0.273960
 43500/100000: episode: 435, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.620, mean reward: -0.186 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.501, 10.098], loss: 0.002473, mae: 0.050728, mean_q: -0.300185
 43600/100000: episode: 436, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.472, mean reward: -0.165 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.675, 10.119], loss: 0.002429, mae: 0.049822, mean_q: -0.291538
 43700/100000: episode: 437, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.043, mean reward: -0.180 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.535, 10.098], loss: 0.002671, mae: 0.053745, mean_q: -0.259555
 43800/100000: episode: 438, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -20.100, mean reward: -0.201 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.986, 10.098], loss: 0.002422, mae: 0.049444, mean_q: -0.286216
 43900/100000: episode: 439, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.464, mean reward: -0.195 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.500, 10.274], loss: 0.002745, mae: 0.053921, mean_q: -0.307768
 44000/100000: episode: 440, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.279, mean reward: -0.173 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.137, 10.098], loss: 0.002505, mae: 0.050605, mean_q: -0.353238
 44100/100000: episode: 441, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.814, mean reward: -0.198 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.202, 10.136], loss: 0.002884, mae: 0.055664, mean_q: -0.291210
 44200/100000: episode: 442, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.967, mean reward: -0.190 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.385, 10.098], loss: 0.003257, mae: 0.058166, mean_q: -0.329749
 44300/100000: episode: 443, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.616, mean reward: -0.176 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.222, 10.098], loss: 0.003553, mae: 0.058898, mean_q: -0.316146
 44400/100000: episode: 444, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.295, mean reward: -0.153 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.601, 10.098], loss: 0.002584, mae: 0.052371, mean_q: -0.312482
 44500/100000: episode: 445, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.506, mean reward: -0.195 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.179, 10.098], loss: 0.002547, mae: 0.051170, mean_q: -0.324620
 44600/100000: episode: 446, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.612, mean reward: -0.166 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.396, 10.270], loss: 0.002628, mae: 0.051566, mean_q: -0.310611
 44700/100000: episode: 447, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.772, mean reward: -0.178 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.957, 10.098], loss: 0.002499, mae: 0.049352, mean_q: -0.311493
 44800/100000: episode: 448, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -11.417, mean reward: -0.114 [-1.000, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.781, 10.395], loss: 0.002472, mae: 0.049615, mean_q: -0.309615
 44900/100000: episode: 449, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.594, mean reward: -0.196 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.723, 10.098], loss: 0.002611, mae: 0.050980, mean_q: -0.332927
 45000/100000: episode: 450, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.399, mean reward: -0.154 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.554, 10.098], loss: 0.002701, mae: 0.053711, mean_q: -0.323953
 45100/100000: episode: 451, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.730, mean reward: -0.197 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.028, 10.098], loss: 0.002839, mae: 0.053553, mean_q: -0.284828
 45200/100000: episode: 452, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.943, mean reward: -0.169 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.476, 10.098], loss: 0.002731, mae: 0.052577, mean_q: -0.316404
 45300/100000: episode: 453, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.888, mean reward: -0.179 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.570, 10.098], loss: 0.002778, mae: 0.053003, mean_q: -0.304200
 45400/100000: episode: 454, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.882, mean reward: -0.139 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.575, 10.436], loss: 0.003073, mae: 0.057430, mean_q: -0.285444
 45500/100000: episode: 455, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.386, mean reward: -0.194 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.649, 10.098], loss: 0.002895, mae: 0.054447, mean_q: -0.288087
 45600/100000: episode: 456, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -15.130, mean reward: -0.151 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.491, 10.281], loss: 0.002668, mae: 0.052161, mean_q: -0.309119
 45700/100000: episode: 457, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -12.505, mean reward: -0.125 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.882, 10.098], loss: 0.002841, mae: 0.052306, mean_q: -0.303000
 45800/100000: episode: 458, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -18.349, mean reward: -0.183 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.197, 10.244], loss: 0.003021, mae: 0.055619, mean_q: -0.315310
 45900/100000: episode: 459, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -18.211, mean reward: -0.182 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.970, 10.270], loss: 0.002816, mae: 0.052643, mean_q: -0.314572
 46000/100000: episode: 460, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.212, mean reward: -0.182 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.139, 10.210], loss: 0.003291, mae: 0.058554, mean_q: -0.302207
 46100/100000: episode: 461, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.254, mean reward: -0.173 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.790, 10.098], loss: 0.002575, mae: 0.052550, mean_q: -0.323566
 46200/100000: episode: 462, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.404, mean reward: -0.184 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.257, 10.098], loss: 0.002477, mae: 0.048397, mean_q: -0.335650
 46300/100000: episode: 463, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.000, mean reward: -0.150 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.074, 10.098], loss: 0.003170, mae: 0.059094, mean_q: -0.330064
 46400/100000: episode: 464, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -14.317, mean reward: -0.143 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.830, 10.098], loss: 0.002848, mae: 0.054003, mean_q: -0.361601
 46500/100000: episode: 465, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.305, mean reward: -0.163 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.047, 10.267], loss: 0.002781, mae: 0.053092, mean_q: -0.306717
 46600/100000: episode: 466, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.147, mean reward: -0.171 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.567, 10.393], loss: 0.002700, mae: 0.051985, mean_q: -0.329754
 46700/100000: episode: 467, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -19.800, mean reward: -0.198 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.999, 10.098], loss: 0.003739, mae: 0.063127, mean_q: -0.341714
 46800/100000: episode: 468, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -14.452, mean reward: -0.145 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.278, 10.292], loss: 0.002917, mae: 0.055100, mean_q: -0.315258
 46900/100000: episode: 469, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.545, mean reward: -0.145 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.117, 10.315], loss: 0.002541, mae: 0.051070, mean_q: -0.320241
 47000/100000: episode: 470, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -13.750, mean reward: -0.137 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.839, 10.293], loss: 0.003321, mae: 0.059405, mean_q: -0.305711
 47100/100000: episode: 471, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.137, mean reward: -0.171 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.447, 10.256], loss: 0.002712, mae: 0.052749, mean_q: -0.310243
 47200/100000: episode: 472, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.130, mean reward: -0.181 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.128, 10.179], loss: 0.002893, mae: 0.053883, mean_q: -0.309011
 47300/100000: episode: 473, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.379, mean reward: -0.154 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.923, 10.258], loss: 0.002765, mae: 0.053301, mean_q: -0.346876
 47400/100000: episode: 474, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -20.930, mean reward: -0.209 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.890, 10.185], loss: 0.002967, mae: 0.055045, mean_q: -0.299775
 47500/100000: episode: 475, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.987, mean reward: -0.160 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.081, 10.100], loss: 0.002813, mae: 0.054384, mean_q: -0.337483
 47600/100000: episode: 476, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.168, mean reward: -0.192 [-1.000, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.879, 10.201], loss: 0.002653, mae: 0.050761, mean_q: -0.317680
 47700/100000: episode: 477, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.434, mean reward: -0.184 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.338, 10.229], loss: 0.002776, mae: 0.051420, mean_q: -0.334527
 47800/100000: episode: 478, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.579, mean reward: -0.176 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.390, 10.125], loss: 0.003739, mae: 0.060123, mean_q: -0.296603
 47900/100000: episode: 479, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.658, mean reward: -0.157 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.479, 10.349], loss: 0.002885, mae: 0.053712, mean_q: -0.329117
 48000/100000: episode: 480, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.781, mean reward: -0.198 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.355, 10.168], loss: 0.002764, mae: 0.051679, mean_q: -0.325921
 48100/100000: episode: 481, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.955, mean reward: -0.190 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.481, 10.098], loss: 0.003410, mae: 0.059863, mean_q: -0.312334
 48200/100000: episode: 482, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.431, mean reward: -0.194 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.899, 10.098], loss: 0.003338, mae: 0.058174, mean_q: -0.326728
 48300/100000: episode: 483, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -19.039, mean reward: -0.190 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.334, 10.098], loss: 0.003690, mae: 0.061522, mean_q: -0.327108
 48400/100000: episode: 484, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.165, mean reward: -0.182 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.674, 10.133], loss: 0.002905, mae: 0.054281, mean_q: -0.312833
 48500/100000: episode: 485, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -14.643, mean reward: -0.146 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.510, 10.098], loss: 0.003056, mae: 0.054415, mean_q: -0.329782
 48600/100000: episode: 486, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.626, mean reward: -0.166 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.930, 10.267], loss: 0.002848, mae: 0.052856, mean_q: -0.344527
 48700/100000: episode: 487, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.658, mean reward: -0.187 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.080, 10.399], loss: 0.002540, mae: 0.048934, mean_q: -0.287237
 48800/100000: episode: 488, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.314, mean reward: -0.173 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.534, 10.098], loss: 0.002832, mae: 0.053181, mean_q: -0.314540
 48900/100000: episode: 489, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.606, mean reward: -0.176 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.153, 10.160], loss: 0.002752, mae: 0.050659, mean_q: -0.367972
 49000/100000: episode: 490, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -12.368, mean reward: -0.124 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.966, 10.423], loss: 0.002507, mae: 0.049732, mean_q: -0.316199
 49100/100000: episode: 491, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -12.930, mean reward: -0.129 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.834, 10.181], loss: 0.002544, mae: 0.049910, mean_q: -0.319521
 49200/100000: episode: 492, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.502, mean reward: -0.155 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.598, 10.406], loss: 0.002719, mae: 0.051456, mean_q: -0.317993
 49300/100000: episode: 493, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -14.306, mean reward: -0.143 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.965, 10.418], loss: 0.002805, mae: 0.053117, mean_q: -0.299439
 49400/100000: episode: 494, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.601, mean reward: -0.186 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.874, 10.098], loss: 0.006011, mae: 0.072927, mean_q: -0.326565
 49500/100000: episode: 495, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.961, mean reward: -0.190 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.259, 10.098], loss: 0.002879, mae: 0.055261, mean_q: -0.320076
 49600/100000: episode: 496, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.714, mean reward: -0.187 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.009, 10.098], loss: 0.002707, mae: 0.052562, mean_q: -0.288328
 49700/100000: episode: 497, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -13.310, mean reward: -0.133 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.560, 10.392], loss: 0.002613, mae: 0.050342, mean_q: -0.300573
 49800/100000: episode: 498, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.005, mean reward: -0.180 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.260, 10.098], loss: 0.002744, mae: 0.052019, mean_q: -0.344104
 49900/100000: episode: 499, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -18.776, mean reward: -0.188 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.675, 10.168], loss: 0.002502, mae: 0.049321, mean_q: -0.312563
 50000/100000: episode: 500, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.120, mean reward: -0.171 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.328, 10.098], loss: 0.002777, mae: 0.052656, mean_q: -0.304371
 50100/100000: episode: 501, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -19.500, mean reward: -0.195 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.263, 10.164], loss: 0.003035, mae: 0.055349, mean_q: -0.340476
 50200/100000: episode: 502, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.469, mean reward: -0.155 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.539, 10.174], loss: 0.002639, mae: 0.051381, mean_q: -0.326959
 50300/100000: episode: 503, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -9.773, mean reward: -0.098 [-1.000, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.346, 10.098], loss: 0.002682, mae: 0.052087, mean_q: -0.279127
 50400/100000: episode: 504, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -12.438, mean reward: -0.124 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.527, 10.106], loss: 0.002819, mae: 0.053384, mean_q: -0.326018
 50500/100000: episode: 505, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.411, mean reward: -0.174 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.622, 10.098], loss: 0.002533, mae: 0.049789, mean_q: -0.321895
 50600/100000: episode: 506, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.781, mean reward: -0.158 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.712, 10.353], loss: 0.002624, mae: 0.050638, mean_q: -0.298178
 50700/100000: episode: 507, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -19.517, mean reward: -0.195 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.522, 10.157], loss: 0.002853, mae: 0.053318, mean_q: -0.322894
 50800/100000: episode: 508, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -20.257, mean reward: -0.203 [-1.000, 0.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.870, 10.137], loss: 0.002507, mae: 0.048710, mean_q: -0.343878
 50900/100000: episode: 509, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.087, mean reward: -0.171 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.251, 10.198], loss: 0.003239, mae: 0.058917, mean_q: -0.294384
 51000/100000: episode: 510, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.909, mean reward: -0.149 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.357, 10.098], loss: 0.003012, mae: 0.056367, mean_q: -0.292792
 51100/100000: episode: 511, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.736, mean reward: -0.177 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.438, 10.198], loss: 0.002852, mae: 0.054730, mean_q: -0.312787
 51200/100000: episode: 512, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.895, mean reward: -0.189 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.152, 10.098], loss: 0.002748, mae: 0.053438, mean_q: -0.325097
 51300/100000: episode: 513, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.991, mean reward: -0.180 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.748, 10.098], loss: 0.002758, mae: 0.053223, mean_q: -0.308864
 51400/100000: episode: 514, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.777, mean reward: -0.168 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.571, 10.098], loss: 0.002630, mae: 0.051586, mean_q: -0.327567
 51500/100000: episode: 515, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -19.291, mean reward: -0.193 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.223, 10.198], loss: 0.002752, mae: 0.052283, mean_q: -0.324433
 51600/100000: episode: 516, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.826, mean reward: -0.168 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.059, 10.249], loss: 0.002625, mae: 0.051355, mean_q: -0.356562
 51700/100000: episode: 517, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -10.844, mean reward: -0.108 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.633, 10.335], loss: 0.002811, mae: 0.054747, mean_q: -0.317491
 51800/100000: episode: 518, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.591, mean reward: -0.186 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.652, 10.098], loss: 0.002661, mae: 0.051881, mean_q: -0.323257
 51900/100000: episode: 519, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.328, mean reward: -0.133 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.863, 10.254], loss: 0.004051, mae: 0.062776, mean_q: -0.337806
 52000/100000: episode: 520, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.584, mean reward: -0.146 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.937, 10.098], loss: 0.003098, mae: 0.057642, mean_q: -0.324493
 52100/100000: episode: 521, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.278, mean reward: -0.173 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.755, 10.111], loss: 0.002688, mae: 0.052800, mean_q: -0.342693
 52200/100000: episode: 522, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.894, mean reward: -0.199 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.479, 10.098], loss: 0.002781, mae: 0.053112, mean_q: -0.318112
 52300/100000: episode: 523, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.972, mean reward: -0.160 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.715, 10.360], loss: 0.002441, mae: 0.049348, mean_q: -0.330600
 52400/100000: episode: 524, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.605, mean reward: -0.166 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.408, 10.181], loss: 0.002996, mae: 0.057691, mean_q: -0.316489
 52500/100000: episode: 525, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -14.496, mean reward: -0.145 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.472, 10.098], loss: 0.002621, mae: 0.051171, mean_q: -0.312071
 52600/100000: episode: 526, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.075, mean reward: -0.161 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.259, 10.159], loss: 0.002741, mae: 0.052151, mean_q: -0.295831
 52700/100000: episode: 527, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -20.034, mean reward: -0.200 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.673, 10.122], loss: 0.005818, mae: 0.071648, mean_q: -0.319368
 52800/100000: episode: 528, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.529, mean reward: -0.165 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.775, 10.155], loss: 0.002898, mae: 0.054806, mean_q: -0.311404
 52900/100000: episode: 529, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.201, mean reward: -0.162 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.419, 10.230], loss: 0.003012, mae: 0.055285, mean_q: -0.319591
 53000/100000: episode: 530, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.122, mean reward: -0.181 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.947, 10.277], loss: 0.003242, mae: 0.057320, mean_q: -0.336901
 53100/100000: episode: 531, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -11.002, mean reward: -0.110 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.965, 10.098], loss: 0.002754, mae: 0.052210, mean_q: -0.332764
 53200/100000: episode: 532, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -19.292, mean reward: -0.193 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.647, 10.225], loss: 0.002782, mae: 0.053467, mean_q: -0.298722
 53300/100000: episode: 533, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -16.270, mean reward: -0.163 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.710, 10.119], loss: 0.002995, mae: 0.054149, mean_q: -0.316400
 53400/100000: episode: 534, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.031, mean reward: -0.170 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.814, 10.098], loss: 0.003726, mae: 0.061762, mean_q: -0.309860
 53500/100000: episode: 535, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.074, mean reward: -0.181 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.222, 10.226], loss: 0.003015, mae: 0.056472, mean_q: -0.310251
 53600/100000: episode: 536, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -13.308, mean reward: -0.133 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.616, 10.379], loss: 0.003046, mae: 0.055673, mean_q: -0.308018
 53700/100000: episode: 537, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -13.987, mean reward: -0.140 [-1.000, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.735, 10.598], loss: 0.002946, mae: 0.054935, mean_q: -0.289753
 53800/100000: episode: 538, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.299, mean reward: -0.163 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.016, 10.392], loss: 0.002897, mae: 0.054778, mean_q: -0.295134
 53900/100000: episode: 539, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.791, mean reward: -0.188 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.429, 10.223], loss: 0.002853, mae: 0.055036, mean_q: -0.330159
 54000/100000: episode: 540, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.266, mean reward: -0.133 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.022, 10.244], loss: 0.002969, mae: 0.055731, mean_q: -0.310940
 54100/100000: episode: 541, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.563, mean reward: -0.186 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.484, 10.098], loss: 0.002624, mae: 0.051705, mean_q: -0.276477
 54200/100000: episode: 542, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -11.660, mean reward: -0.117 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.665, 10.098], loss: 0.002648, mae: 0.051457, mean_q: -0.299166
 54300/100000: episode: 543, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -12.402, mean reward: -0.124 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.485, 10.392], loss: 0.002781, mae: 0.052006, mean_q: -0.320309
 54400/100000: episode: 544, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -11.403, mean reward: -0.114 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.112, 10.382], loss: 0.002832, mae: 0.052629, mean_q: -0.320032
 54500/100000: episode: 545, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -19.373, mean reward: -0.194 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.907, 10.144], loss: 0.002849, mae: 0.054085, mean_q: -0.313399
 54600/100000: episode: 546, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.161, mean reward: -0.162 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.031, 10.229], loss: 0.003084, mae: 0.055327, mean_q: -0.305145
 54700/100000: episode: 547, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -19.454, mean reward: -0.195 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.920, 10.098], loss: 0.002984, mae: 0.055763, mean_q: -0.294637
 54800/100000: episode: 548, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.627, mean reward: -0.146 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.044, 10.152], loss: 0.003078, mae: 0.056285, mean_q: -0.304028
 54900/100000: episode: 549, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.231, mean reward: -0.172 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.978, 10.098], loss: 0.002625, mae: 0.052160, mean_q: -0.304688
 55000/100000: episode: 550, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.355, mean reward: -0.174 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.144, 10.098], loss: 0.002615, mae: 0.051358, mean_q: -0.337505
 55100/100000: episode: 551, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -13.880, mean reward: -0.139 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.247, 10.123], loss: 0.003435, mae: 0.060297, mean_q: -0.267921
 55200/100000: episode: 552, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -13.452, mean reward: -0.135 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.163, 10.098], loss: 0.002861, mae: 0.053757, mean_q: -0.294375
 55300/100000: episode: 553, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.335, mean reward: -0.183 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.803, 10.200], loss: 0.002893, mae: 0.053483, mean_q: -0.288902
 55400/100000: episode: 554, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.354, mean reward: -0.174 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.544, 10.098], loss: 0.002859, mae: 0.053275, mean_q: -0.308713
 55500/100000: episode: 555, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.478, mean reward: -0.155 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.825, 10.377], loss: 0.002632, mae: 0.051276, mean_q: -0.286713
 55600/100000: episode: 556, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -10.916, mean reward: -0.109 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.547, 10.338], loss: 0.002943, mae: 0.055795, mean_q: -0.313536
 55700/100000: episode: 557, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.360, mean reward: -0.174 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.093, 10.098], loss: 0.003146, mae: 0.057327, mean_q: -0.299697
 55800/100000: episode: 558, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.738, mean reward: -0.167 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.893, 10.098], loss: 0.002761, mae: 0.054300, mean_q: -0.298946
 55900/100000: episode: 559, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -17.357, mean reward: -0.174 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.399, 10.107], loss: 0.002902, mae: 0.054113, mean_q: -0.325287
 56000/100000: episode: 560, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.181, mean reward: -0.192 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.658, 10.110], loss: 0.002802, mae: 0.054058, mean_q: -0.303656
 56100/100000: episode: 561, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.484, mean reward: -0.195 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.572, 10.226], loss: 0.002724, mae: 0.051718, mean_q: -0.313103
 56200/100000: episode: 562, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.466, mean reward: -0.145 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.056, 10.098], loss: 0.002715, mae: 0.052658, mean_q: -0.306556
 56300/100000: episode: 563, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -19.397, mean reward: -0.194 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.769, 10.098], loss: 0.002691, mae: 0.052199, mean_q: -0.304887
 56400/100000: episode: 564, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.192, mean reward: -0.182 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.404, 10.144], loss: 0.002615, mae: 0.050955, mean_q: -0.329925
 56500/100000: episode: 565, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -12.302, mean reward: -0.123 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.393, 10.098], loss: 0.002751, mae: 0.051611, mean_q: -0.272097
 56600/100000: episode: 566, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.555, mean reward: -0.166 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.865, 10.187], loss: 0.005220, mae: 0.070798, mean_q: -0.261682
 56700/100000: episode: 567, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.192, mean reward: -0.172 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.831, 10.098], loss: 0.004109, mae: 0.064128, mean_q: -0.289737
 56800/100000: episode: 568, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.956, mean reward: -0.150 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.281, 10.182], loss: 0.002730, mae: 0.051278, mean_q: -0.326960
 56900/100000: episode: 569, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.802, mean reward: -0.168 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.758, 10.098], loss: 0.002771, mae: 0.053719, mean_q: -0.285833
 57000/100000: episode: 570, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.783, mean reward: -0.178 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.577, 10.098], loss: 0.002716, mae: 0.051980, mean_q: -0.303388
 57100/100000: episode: 571, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.954, mean reward: -0.180 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.572, 10.113], loss: 0.003285, mae: 0.057449, mean_q: -0.317906
 57200/100000: episode: 572, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -4.109, mean reward: -0.041 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.494, 10.372], loss: 0.002598, mae: 0.052097, mean_q: -0.299237
 57300/100000: episode: 573, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.597, mean reward: -0.186 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.645, 10.152], loss: 0.002789, mae: 0.052967, mean_q: -0.304842
 57400/100000: episode: 574, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.510, mean reward: -0.185 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.140, 10.098], loss: 0.002589, mae: 0.050797, mean_q: -0.303907
 57500/100000: episode: 575, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.821, mean reward: -0.168 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.403, 10.162], loss: 0.002727, mae: 0.051833, mean_q: -0.339768
 57600/100000: episode: 576, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.462, mean reward: -0.195 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.137, 10.151], loss: 0.002575, mae: 0.051251, mean_q: -0.320277
 57700/100000: episode: 577, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -20.017, mean reward: -0.200 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.225, 10.117], loss: 0.002789, mae: 0.053846, mean_q: -0.289279
 57800/100000: episode: 578, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.326, mean reward: -0.193 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.735, 10.098], loss: 0.002611, mae: 0.050484, mean_q: -0.319714
 57900/100000: episode: 579, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -19.818, mean reward: -0.198 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.436, 10.134], loss: 0.002686, mae: 0.052971, mean_q: -0.282280
 58000/100000: episode: 580, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.576, mean reward: -0.186 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.746, 10.219], loss: 0.002710, mae: 0.053035, mean_q: -0.299361
 58100/100000: episode: 581, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -16.954, mean reward: -0.170 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.738, 10.098], loss: 0.002606, mae: 0.051442, mean_q: -0.277451
 58200/100000: episode: 582, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -12.757, mean reward: -0.128 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.006, 10.098], loss: 0.002873, mae: 0.053710, mean_q: -0.337458
 58300/100000: episode: 583, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.440, mean reward: -0.184 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.796, 10.098], loss: 0.002621, mae: 0.052069, mean_q: -0.289144
 58400/100000: episode: 584, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -15.082, mean reward: -0.151 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.548, 10.261], loss: 0.003711, mae: 0.061002, mean_q: -0.296987
 58500/100000: episode: 585, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.190, mean reward: -0.182 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.788, 10.330], loss: 0.002995, mae: 0.056101, mean_q: -0.328585
 58600/100000: episode: 586, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.414, mean reward: -0.154 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.891, 10.098], loss: 0.002665, mae: 0.050969, mean_q: -0.283550
 58700/100000: episode: 587, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -10.763, mean reward: -0.108 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.733, 10.282], loss: 0.002765, mae: 0.052962, mean_q: -0.311823
 58800/100000: episode: 588, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.859, mean reward: -0.189 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.030, 10.289], loss: 0.002683, mae: 0.051747, mean_q: -0.322658
 58900/100000: episode: 589, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.507, mean reward: -0.175 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.742, 10.098], loss: 0.002685, mae: 0.050564, mean_q: -0.307815
 59000/100000: episode: 590, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -11.010, mean reward: -0.110 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.331, 10.255], loss: 0.002657, mae: 0.052028, mean_q: -0.321485
 59100/100000: episode: 591, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.860, mean reward: -0.189 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.462, 10.120], loss: 0.002700, mae: 0.052404, mean_q: -0.282354
 59200/100000: episode: 592, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -19.704, mean reward: -0.197 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.094, 10.098], loss: 0.002521, mae: 0.050966, mean_q: -0.291090
 59300/100000: episode: 593, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.228, mean reward: -0.192 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.773, 10.098], loss: 0.002773, mae: 0.051885, mean_q: -0.297583
 59400/100000: episode: 594, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.434, mean reward: -0.174 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.227, 10.145], loss: 0.002842, mae: 0.053097, mean_q: -0.327925
 59500/100000: episode: 595, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -16.724, mean reward: -0.167 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.234, 10.322], loss: 0.002767, mae: 0.052074, mean_q: -0.318630
 59600/100000: episode: 596, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.277, mean reward: -0.173 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.275, 10.123], loss: 0.002626, mae: 0.051562, mean_q: -0.363180
 59700/100000: episode: 597, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -12.276, mean reward: -0.123 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.763, 10.098], loss: 0.002665, mae: 0.051065, mean_q: -0.319292
 59800/100000: episode: 598, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -12.801, mean reward: -0.128 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.620, 10.515], loss: 0.002514, mae: 0.049684, mean_q: -0.345265
 59900/100000: episode: 599, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -17.748, mean reward: -0.177 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.494, 10.260], loss: 0.002674, mae: 0.052243, mean_q: -0.315558
 60000/100000: episode: 600, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -13.774, mean reward: -0.138 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.542, 10.355], loss: 0.002711, mae: 0.052224, mean_q: -0.284904
 60100/100000: episode: 601, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.485, mean reward: -0.175 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.291, 10.098], loss: 0.002816, mae: 0.053325, mean_q: -0.276553
 60200/100000: episode: 602, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.445, mean reward: -0.164 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.226, 10.098], loss: 0.002925, mae: 0.056649, mean_q: -0.309484
 60300/100000: episode: 603, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -13.151, mean reward: -0.132 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.830, 10.182], loss: 0.002634, mae: 0.051699, mean_q: -0.297439
 60400/100000: episode: 604, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.902, mean reward: -0.169 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.681, 10.354], loss: 0.002510, mae: 0.050094, mean_q: -0.284525
 60500/100000: episode: 605, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -15.921, mean reward: -0.159 [-1.000, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.256, 10.114], loss: 0.002669, mae: 0.051946, mean_q: -0.311461
 60600/100000: episode: 606, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.243, mean reward: -0.172 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.314, 10.098], loss: 0.002668, mae: 0.051135, mean_q: -0.306482
 60700/100000: episode: 607, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.393, mean reward: -0.184 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.444, 10.135], loss: 0.002560, mae: 0.050306, mean_q: -0.309135
 60800/100000: episode: 608, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -21.358, mean reward: -0.214 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.845, 10.098], loss: 0.002484, mae: 0.049394, mean_q: -0.276436
 60900/100000: episode: 609, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.207, mean reward: -0.182 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.550, 10.098], loss: 0.002907, mae: 0.055391, mean_q: -0.301977
 61000/100000: episode: 610, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -18.653, mean reward: -0.187 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.487, 10.098], loss: 0.002676, mae: 0.051261, mean_q: -0.313417
 61100/100000: episode: 611, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.617, mean reward: -0.166 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.171, 10.116], loss: 0.002478, mae: 0.051151, mean_q: -0.318668
 61200/100000: episode: 612, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.892, mean reward: -0.149 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.046, 10.586], loss: 0.002626, mae: 0.051087, mean_q: -0.309981
 61300/100000: episode: 613, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.996, mean reward: -0.190 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.615, 10.216], loss: 0.002605, mae: 0.052413, mean_q: -0.292602
 61400/100000: episode: 614, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -15.942, mean reward: -0.159 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.508, 10.173], loss: 0.002389, mae: 0.048170, mean_q: -0.312683
 61500/100000: episode: 615, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.903, mean reward: -0.149 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.574, 10.098], loss: 0.002376, mae: 0.047359, mean_q: -0.331095
 61600/100000: episode: 616, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.137, mean reward: -0.191 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.610, 10.098], loss: 0.002564, mae: 0.050622, mean_q: -0.293882
 61700/100000: episode: 617, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.912, mean reward: -0.179 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.540, 10.098], loss: 0.002451, mae: 0.051250, mean_q: -0.309459
 61800/100000: episode: 618, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.718, mean reward: -0.177 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.108, 10.206], loss: 0.002625, mae: 0.052792, mean_q: -0.367910
 61900/100000: episode: 619, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -15.014, mean reward: -0.150 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.785, 10.098], loss: 0.002522, mae: 0.051889, mean_q: -0.332787
 62000/100000: episode: 620, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.099, mean reward: -0.181 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.547, 10.192], loss: 0.002485, mae: 0.050492, mean_q: -0.333249
 62100/100000: episode: 621, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -20.401, mean reward: -0.204 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.564, 10.098], loss: 0.002555, mae: 0.051004, mean_q: -0.304381
 62200/100000: episode: 622, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -16.667, mean reward: -0.167 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.751, 10.189], loss: 0.002377, mae: 0.048669, mean_q: -0.327943
 62300/100000: episode: 623, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -20.066, mean reward: -0.201 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.253, 10.158], loss: 0.002320, mae: 0.048489, mean_q: -0.316722
 62400/100000: episode: 624, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -20.180, mean reward: -0.202 [-1.000, 0.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.363, 10.249], loss: 0.002374, mae: 0.049614, mean_q: -0.319107
 62500/100000: episode: 625, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -13.348, mean reward: -0.133 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.016, 10.098], loss: 0.002335, mae: 0.048751, mean_q: -0.301324
 62600/100000: episode: 626, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.903, mean reward: -0.139 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.753, 10.280], loss: 0.002500, mae: 0.050033, mean_q: -0.291783
 62700/100000: episode: 627, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -18.501, mean reward: -0.185 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.589, 10.098], loss: 0.002302, mae: 0.047903, mean_q: -0.295018
 62800/100000: episode: 628, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -12.702, mean reward: -0.127 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.503, 10.098], loss: 0.002342, mae: 0.048659, mean_q: -0.300507
 62900/100000: episode: 629, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.288, mean reward: -0.193 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.692, 10.098], loss: 0.002578, mae: 0.051231, mean_q: -0.307949
 63000/100000: episode: 630, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -13.607, mean reward: -0.136 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.481, 10.240], loss: 0.003654, mae: 0.061921, mean_q: -0.319518
 63100/100000: episode: 631, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -10.137, mean reward: -0.101 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.947, 10.098], loss: 0.002266, mae: 0.048020, mean_q: -0.307007
 63200/100000: episode: 632, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -20.819, mean reward: -0.208 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.526, 10.098], loss: 0.002128, mae: 0.046292, mean_q: -0.320998
 63300/100000: episode: 633, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -13.046, mean reward: -0.130 [-1.000, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.985, 10.169], loss: 0.002255, mae: 0.047141, mean_q: -0.280468
 63400/100000: episode: 634, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -12.163, mean reward: -0.122 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.657, 10.420], loss: 0.002364, mae: 0.047931, mean_q: -0.360393
 63500/100000: episode: 635, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.130, mean reward: -0.151 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.153, 10.098], loss: 0.002393, mae: 0.048601, mean_q: -0.340454
 63600/100000: episode: 636, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.438, mean reward: -0.174 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.156, 10.098], loss: 0.002508, mae: 0.050852, mean_q: -0.306435
 63700/100000: episode: 637, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -19.130, mean reward: -0.191 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.855, 10.244], loss: 0.002551, mae: 0.051206, mean_q: -0.284178
 63800/100000: episode: 638, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.723, mean reward: -0.197 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.013, 10.148], loss: 0.002303, mae: 0.047902, mean_q: -0.313367
 63900/100000: episode: 639, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.295, mean reward: -0.173 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.862, 10.174], loss: 0.002417, mae: 0.048493, mean_q: -0.347664
 64000/100000: episode: 640, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -17.023, mean reward: -0.170 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.885, 10.223], loss: 0.002589, mae: 0.052759, mean_q: -0.316603
 64100/100000: episode: 641, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -15.986, mean reward: -0.160 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.582, 10.098], loss: 0.004442, mae: 0.061745, mean_q: -0.309887
 64200/100000: episode: 642, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.620, mean reward: -0.166 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.844, 10.098], loss: 0.003030, mae: 0.056912, mean_q: -0.299227
 64300/100000: episode: 643, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.531, mean reward: -0.185 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.009, 10.222], loss: 0.002264, mae: 0.047402, mean_q: -0.316515
 64400/100000: episode: 644, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.042, mean reward: -0.170 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.595, 10.098], loss: 0.002359, mae: 0.048759, mean_q: -0.332416
 64500/100000: episode: 645, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -13.677, mean reward: -0.137 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.217, 10.098], loss: 0.002352, mae: 0.048231, mean_q: -0.311247
 64600/100000: episode: 646, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.782, mean reward: -0.178 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.757, 10.098], loss: 0.002567, mae: 0.050528, mean_q: -0.326127
 64700/100000: episode: 647, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.443, mean reward: -0.184 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.979, 10.098], loss: 0.002677, mae: 0.051684, mean_q: -0.276020
 64800/100000: episode: 648, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.018, mean reward: -0.150 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.543, 10.115], loss: 0.002400, mae: 0.048380, mean_q: -0.329099
 64900/100000: episode: 649, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -15.427, mean reward: -0.154 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.032, 10.368], loss: 0.002927, mae: 0.055404, mean_q: -0.289428
 65000/100000: episode: 650, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -19.544, mean reward: -0.195 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.060, 10.284], loss: 0.002391, mae: 0.048870, mean_q: -0.330220
 65100/100000: episode: 651, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.112, mean reward: -0.151 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.779, 10.266], loss: 0.002462, mae: 0.049735, mean_q: -0.293088
 65200/100000: episode: 652, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -13.873, mean reward: -0.139 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.367, 10.345], loss: 0.002460, mae: 0.048992, mean_q: -0.347679
 65300/100000: episode: 653, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.842, mean reward: -0.158 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.517, 10.320], loss: 0.002354, mae: 0.047959, mean_q: -0.305610
 65400/100000: episode: 654, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -17.723, mean reward: -0.177 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.505, 10.098], loss: 0.002603, mae: 0.051098, mean_q: -0.308207
 65500/100000: episode: 655, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -14.671, mean reward: -0.147 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.661, 10.098], loss: 0.002415, mae: 0.048920, mean_q: -0.319342
 65600/100000: episode: 656, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.167, mean reward: -0.182 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.912, 10.146], loss: 0.002408, mae: 0.048641, mean_q: -0.343330
 65700/100000: episode: 657, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.836, mean reward: -0.178 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.968, 10.098], loss: 0.002477, mae: 0.050017, mean_q: -0.306519
 65800/100000: episode: 658, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -15.232, mean reward: -0.152 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.273, 10.236], loss: 0.002624, mae: 0.051905, mean_q: -0.298757
 65900/100000: episode: 659, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.091, mean reward: -0.181 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.823, 10.098], loss: 0.002484, mae: 0.048854, mean_q: -0.325239
 66000/100000: episode: 660, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.556, mean reward: -0.166 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.511, 10.098], loss: 0.002417, mae: 0.048664, mean_q: -0.307620
 66100/100000: episode: 661, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.165, mean reward: -0.192 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.326, 10.098], loss: 0.002558, mae: 0.050834, mean_q: -0.335348
 66200/100000: episode: 662, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.959, mean reward: -0.160 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.151, 10.220], loss: 0.002502, mae: 0.049523, mean_q: -0.342620
 66300/100000: episode: 663, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.080, mean reward: -0.181 [-1.000, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.146, 10.098], loss: 0.002921, mae: 0.055582, mean_q: -0.314087
 66400/100000: episode: 664, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.530, mean reward: -0.155 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.272, 10.226], loss: 0.002807, mae: 0.053938, mean_q: -0.329701
 66500/100000: episode: 665, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.394, mean reward: -0.174 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.817, 10.208], loss: 0.002520, mae: 0.050060, mean_q: -0.333892
 66600/100000: episode: 666, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -11.345, mean reward: -0.113 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.669, 10.098], loss: 0.002874, mae: 0.052138, mean_q: -0.323497
 66700/100000: episode: 667, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -20.789, mean reward: -0.208 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.779, 10.098], loss: 0.002757, mae: 0.053815, mean_q: -0.320798
 66800/100000: episode: 668, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -13.132, mean reward: -0.131 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.928, 10.439], loss: 0.003097, mae: 0.055584, mean_q: -0.294226
 66900/100000: episode: 669, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.169, mean reward: -0.182 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.611, 10.200], loss: 0.002770, mae: 0.052499, mean_q: -0.323282
 67000/100000: episode: 670, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.916, mean reward: -0.189 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.309, 10.098], loss: 0.002590, mae: 0.051012, mean_q: -0.308701
 67100/100000: episode: 671, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.851, mean reward: -0.159 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.295, 10.222], loss: 0.002628, mae: 0.050844, mean_q: -0.340087
 67200/100000: episode: 672, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.419, mean reward: -0.174 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.231, 10.280], loss: 0.002710, mae: 0.051805, mean_q: -0.311377
 67300/100000: episode: 673, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.081, mean reward: -0.181 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.940, 10.168], loss: 0.002957, mae: 0.055305, mean_q: -0.350813
 67400/100000: episode: 674, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.936, mean reward: -0.179 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.037, 10.179], loss: 0.002655, mae: 0.051185, mean_q: -0.308830
 67500/100000: episode: 675, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.416, mean reward: -0.194 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.024, 10.100], loss: 0.003459, mae: 0.060562, mean_q: -0.289305
 67600/100000: episode: 676, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.139, mean reward: -0.171 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.036, 10.098], loss: 0.003261, mae: 0.057699, mean_q: -0.319354
 67700/100000: episode: 677, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -13.148, mean reward: -0.131 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.984, 10.098], loss: 0.002842, mae: 0.053419, mean_q: -0.304878
 67800/100000: episode: 678, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -15.785, mean reward: -0.158 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.930, 10.215], loss: 0.002605, mae: 0.050043, mean_q: -0.361587
 67900/100000: episode: 679, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.227, mean reward: -0.172 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.092, 10.098], loss: 0.002645, mae: 0.050715, mean_q: -0.331021
 68000/100000: episode: 680, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.605, mean reward: -0.166 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.433, 10.098], loss: 0.002835, mae: 0.052373, mean_q: -0.313581
 68100/100000: episode: 681, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -14.995, mean reward: -0.150 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.636, 10.098], loss: 0.002804, mae: 0.052318, mean_q: -0.313941
 68200/100000: episode: 682, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -20.150, mean reward: -0.201 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.176, 10.098], loss: 0.002729, mae: 0.050879, mean_q: -0.334630
 68300/100000: episode: 683, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.596, mean reward: -0.166 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.535, 10.098], loss: 0.002779, mae: 0.052578, mean_q: -0.302419
 68400/100000: episode: 684, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.872, mean reward: -0.169 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.520, 10.098], loss: 0.002721, mae: 0.051727, mean_q: -0.326893
 68500/100000: episode: 685, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -13.680, mean reward: -0.137 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.146, 10.098], loss: 0.003001, mae: 0.053640, mean_q: -0.313115
 68600/100000: episode: 686, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.418, mean reward: -0.134 [-1.000, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.918, 10.098], loss: 0.003037, mae: 0.056788, mean_q: -0.296151
 68700/100000: episode: 687, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.957, mean reward: -0.160 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.359, 10.098], loss: 0.002637, mae: 0.050154, mean_q: -0.327831
 68800/100000: episode: 688, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -12.906, mean reward: -0.129 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.224, 10.098], loss: 0.002866, mae: 0.053210, mean_q: -0.306867
 68900/100000: episode: 689, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -11.124, mean reward: -0.111 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.612, 10.308], loss: 0.002826, mae: 0.052611, mean_q: -0.318936
 69000/100000: episode: 690, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.846, mean reward: -0.188 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.634, 10.141], loss: 0.002808, mae: 0.053399, mean_q: -0.293292
 69100/100000: episode: 691, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -19.388, mean reward: -0.194 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.284, 10.098], loss: 0.004726, mae: 0.070507, mean_q: -0.301356
 69200/100000: episode: 692, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.313, mean reward: -0.133 [-1.000, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.223, 10.506], loss: 0.002720, mae: 0.052589, mean_q: -0.324117
 69300/100000: episode: 693, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -20.534, mean reward: -0.205 [-1.000, 0.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.903, 10.198], loss: 0.002608, mae: 0.051973, mean_q: -0.335924
 69400/100000: episode: 694, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.463, mean reward: -0.165 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.499, 10.123], loss: 0.002842, mae: 0.053172, mean_q: -0.297099
 69500/100000: episode: 695, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.351, mean reward: -0.184 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.563, 10.098], loss: 0.002788, mae: 0.051498, mean_q: -0.285216
 69600/100000: episode: 696, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -20.240, mean reward: -0.202 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.754, 10.239], loss: 0.002970, mae: 0.055234, mean_q: -0.313856
 69700/100000: episode: 697, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -19.904, mean reward: -0.199 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.700, 10.151], loss: 0.002874, mae: 0.054499, mean_q: -0.285944
 69800/100000: episode: 698, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.117, mean reward: -0.171 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.143, 10.098], loss: 0.002859, mae: 0.052889, mean_q: -0.296641
 69900/100000: episode: 699, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -13.353, mean reward: -0.134 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.613, 10.327], loss: 0.002548, mae: 0.050785, mean_q: -0.321551
 70000/100000: episode: 700, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.886, mean reward: -0.169 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.193, 10.098], loss: 0.002906, mae: 0.054160, mean_q: -0.295247
 70100/100000: episode: 701, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.832, mean reward: -0.178 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.805, 10.100], loss: 0.002901, mae: 0.053540, mean_q: -0.320291
 70200/100000: episode: 702, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.784, mean reward: -0.198 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.205, 10.098], loss: 0.002670, mae: 0.051314, mean_q: -0.339501
 70300/100000: episode: 703, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.013, mean reward: -0.180 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.041, 10.102], loss: 0.002836, mae: 0.053970, mean_q: -0.339794
 70400/100000: episode: 704, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -14.883, mean reward: -0.149 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.959, 10.098], loss: 0.002908, mae: 0.055564, mean_q: -0.299210
 70500/100000: episode: 705, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.497, mean reward: -0.165 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.882, 10.242], loss: 0.002891, mae: 0.054374, mean_q: -0.326320
 70600/100000: episode: 706, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -20.574, mean reward: -0.206 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.576, 10.201], loss: 0.002813, mae: 0.053333, mean_q: -0.326352
 70700/100000: episode: 707, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.903, mean reward: -0.149 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.205, 10.361], loss: 0.002937, mae: 0.053869, mean_q: -0.320540
 70800/100000: episode: 708, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.873, mean reward: -0.189 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.210, 10.205], loss: 0.003010, mae: 0.054631, mean_q: -0.322796
 70900/100000: episode: 709, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.685, mean reward: -0.147 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.402, 10.333], loss: 0.002976, mae: 0.055126, mean_q: -0.359494
 71000/100000: episode: 710, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -10.888, mean reward: -0.109 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.178, 10.473], loss: 0.002676, mae: 0.052162, mean_q: -0.344665
 71100/100000: episode: 711, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -19.059, mean reward: -0.191 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.916, 10.239], loss: 0.002856, mae: 0.052807, mean_q: -0.304236
 71200/100000: episode: 712, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -17.216, mean reward: -0.172 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.004, 10.098], loss: 0.002635, mae: 0.051803, mean_q: -0.286094
 71300/100000: episode: 713, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.625, mean reward: -0.166 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.553, 10.098], loss: 0.002625, mae: 0.050683, mean_q: -0.335143
 71400/100000: episode: 714, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -10.394, mean reward: -0.104 [-1.000, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.894, 10.563], loss: 0.002725, mae: 0.052624, mean_q: -0.330661
 71500/100000: episode: 715, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.248, mean reward: -0.152 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.061, 10.098], loss: 0.003400, mae: 0.057863, mean_q: -0.329088
 71600/100000: episode: 716, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -16.453, mean reward: -0.165 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.785, 10.098], loss: 0.002959, mae: 0.055134, mean_q: -0.271077
 71700/100000: episode: 717, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -14.001, mean reward: -0.140 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.847, 10.492], loss: 0.002480, mae: 0.048993, mean_q: -0.340056
 71800/100000: episode: 718, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.711, mean reward: -0.167 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.361, 10.257], loss: 0.002726, mae: 0.051206, mean_q: -0.328277
 71900/100000: episode: 719, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.837, mean reward: -0.188 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.922, 10.098], loss: 0.002940, mae: 0.055366, mean_q: -0.296105
 72000/100000: episode: 720, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.954, mean reward: -0.180 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.114, 10.193], loss: 0.002867, mae: 0.052919, mean_q: -0.294667
 72100/100000: episode: 721, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.466, mean reward: -0.185 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.427, 10.158], loss: 0.002703, mae: 0.051181, mean_q: -0.318633
 72200/100000: episode: 722, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.360, mean reward: -0.174 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.688, 10.196], loss: 0.002601, mae: 0.051108, mean_q: -0.307062
 72300/100000: episode: 723, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -17.654, mean reward: -0.177 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.536, 10.197], loss: 0.002737, mae: 0.053042, mean_q: -0.318165
 72400/100000: episode: 724, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -17.889, mean reward: -0.179 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.755, 10.098], loss: 0.002514, mae: 0.050015, mean_q: -0.336827
 72500/100000: episode: 725, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -10.241, mean reward: -0.102 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.441, 10.372], loss: 0.002536, mae: 0.049449, mean_q: -0.326780
 72600/100000: episode: 726, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.469, mean reward: -0.165 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.476, 10.107], loss: 0.002817, mae: 0.052500, mean_q: -0.291449
 72700/100000: episode: 727, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -16.513, mean reward: -0.165 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.472, 10.098], loss: 0.002666, mae: 0.050643, mean_q: -0.331428
 72800/100000: episode: 728, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.002, mean reward: -0.180 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.955, 10.098], loss: 0.002513, mae: 0.049510, mean_q: -0.300574
 72900/100000: episode: 729, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.436, mean reward: -0.184 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.223, 10.244], loss: 0.002750, mae: 0.051752, mean_q: -0.323124
 73000/100000: episode: 730, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.780, mean reward: -0.178 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.463, 10.346], loss: 0.002660, mae: 0.052265, mean_q: -0.298949
 73100/100000: episode: 731, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.070, mean reward: -0.161 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.791, 10.194], loss: 0.002530, mae: 0.049221, mean_q: -0.317791
 73200/100000: episode: 732, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -12.544, mean reward: -0.125 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.254, 10.129], loss: 0.002704, mae: 0.051731, mean_q: -0.307210
 73300/100000: episode: 733, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.493, mean reward: -0.175 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.475, 10.098], loss: 0.002720, mae: 0.051737, mean_q: -0.302993
 73400/100000: episode: 734, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -11.952, mean reward: -0.120 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.367, 10.198], loss: 0.002586, mae: 0.052040, mean_q: -0.290647
 73500/100000: episode: 735, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.698, mean reward: -0.177 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.660, 10.098], loss: 0.003927, mae: 0.059438, mean_q: -0.318753
 73600/100000: episode: 736, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -12.942, mean reward: -0.129 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.266, 10.098], loss: 0.002840, mae: 0.054727, mean_q: -0.286884
 73700/100000: episode: 737, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.433, mean reward: -0.184 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.450, 10.223], loss: 0.002574, mae: 0.050729, mean_q: -0.298712
 73800/100000: episode: 738, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -15.520, mean reward: -0.155 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.681, 10.311], loss: 0.002568, mae: 0.049992, mean_q: -0.306060
 73900/100000: episode: 739, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.753, mean reward: -0.178 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.240, 10.237], loss: 0.002366, mae: 0.049124, mean_q: -0.308598
 74000/100000: episode: 740, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.553, mean reward: -0.196 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.707, 10.098], loss: 0.002613, mae: 0.051209, mean_q: -0.287846
 74100/100000: episode: 741, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.105, mean reward: -0.181 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.934, 10.098], loss: 0.002420, mae: 0.049221, mean_q: -0.304806
 74200/100000: episode: 742, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.399, mean reward: -0.144 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.556, 10.281], loss: 0.002493, mae: 0.049782, mean_q: -0.296927
 74300/100000: episode: 743, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -11.792, mean reward: -0.118 [-1.000, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.305, 10.449], loss: 0.002448, mae: 0.049024, mean_q: -0.309298
 74400/100000: episode: 744, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -13.149, mean reward: -0.131 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.376, 10.098], loss: 0.002493, mae: 0.048837, mean_q: -0.311526
 74500/100000: episode: 745, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.681, mean reward: -0.177 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.661, 10.098], loss: 0.002557, mae: 0.050972, mean_q: -0.328999
 74600/100000: episode: 746, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.620, mean reward: -0.186 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.863, 10.098], loss: 0.002565, mae: 0.050694, mean_q: -0.268675
 74700/100000: episode: 747, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -16.425, mean reward: -0.164 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.172, 10.119], loss: 0.002448, mae: 0.049216, mean_q: -0.314293
 74800/100000: episode: 748, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.843, mean reward: -0.188 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.365, 10.268], loss: 0.002707, mae: 0.052184, mean_q: -0.298742
 74900/100000: episode: 749, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.883, mean reward: -0.189 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.278, 10.106], loss: 0.002517, mae: 0.049621, mean_q: -0.304489
 75000/100000: episode: 750, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.603, mean reward: -0.166 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.075, 10.177], loss: 0.002393, mae: 0.049907, mean_q: -0.293475
 75100/100000: episode: 751, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.449, mean reward: -0.164 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.880, 10.098], loss: 0.002410, mae: 0.048855, mean_q: -0.306807
 75200/100000: episode: 752, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.478, mean reward: -0.145 [-1.000, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.409, 10.098], loss: 0.002789, mae: 0.053245, mean_q: -0.326939
 75300/100000: episode: 753, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.878, mean reward: -0.199 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.228, 10.255], loss: 0.002561, mae: 0.051103, mean_q: -0.300803
 75400/100000: episode: 754, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -12.736, mean reward: -0.127 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.735, 10.356], loss: 0.002744, mae: 0.053356, mean_q: -0.303649
 75500/100000: episode: 755, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.383, mean reward: -0.164 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.204, 10.373], loss: 0.004121, mae: 0.059481, mean_q: -0.316088
 75600/100000: episode: 756, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.856, mean reward: -0.189 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.857, 10.129], loss: 0.002294, mae: 0.047463, mean_q: -0.324658
 75700/100000: episode: 757, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -7.920, mean reward: -0.079 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.307, 10.393], loss: 0.002325, mae: 0.048080, mean_q: -0.319931
 75800/100000: episode: 758, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -11.772, mean reward: -0.118 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.580, 10.149], loss: 0.002462, mae: 0.048212, mean_q: -0.307403
 75900/100000: episode: 759, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -15.765, mean reward: -0.158 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.824, 10.174], loss: 0.002205, mae: 0.045990, mean_q: -0.331230
 76000/100000: episode: 760, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.674, mean reward: -0.177 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.095, 10.126], loss: 0.002373, mae: 0.047740, mean_q: -0.337842
 76100/100000: episode: 761, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.474, mean reward: -0.165 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.531, 10.104], loss: 0.002641, mae: 0.050230, mean_q: -0.319674
 76200/100000: episode: 762, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.027, mean reward: -0.190 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.983, 10.098], loss: 0.002367, mae: 0.048214, mean_q: -0.286552
 76300/100000: episode: 763, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.937, mean reward: -0.189 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.838, 10.098], loss: 0.002610, mae: 0.050111, mean_q: -0.282382
 76400/100000: episode: 764, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -19.453, mean reward: -0.195 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.173, 10.216], loss: 0.002302, mae: 0.047578, mean_q: -0.296150
 76500/100000: episode: 765, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.837, mean reward: -0.198 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.844, 10.233], loss: 0.002602, mae: 0.052057, mean_q: -0.276116
 76600/100000: episode: 766, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.550, mean reward: -0.175 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.965, 10.228], loss: 0.002482, mae: 0.049566, mean_q: -0.323183
 76700/100000: episode: 767, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.924, mean reward: -0.189 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.348, 10.113], loss: 0.002466, mae: 0.049489, mean_q: -0.311108
 76800/100000: episode: 768, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.711, mean reward: -0.167 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.396, 10.098], loss: 0.002625, mae: 0.052144, mean_q: -0.291480
 76900/100000: episode: 769, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.000, mean reward: -0.190 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.636, 10.234], loss: 0.002536, mae: 0.049989, mean_q: -0.321311
 77000/100000: episode: 770, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.467, mean reward: -0.155 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.178, 10.098], loss: 0.002426, mae: 0.051019, mean_q: -0.314555
 77100/100000: episode: 771, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.564, mean reward: -0.186 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.128, 10.098], loss: 0.002533, mae: 0.049704, mean_q: -0.303911
 77200/100000: episode: 772, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -12.450, mean reward: -0.124 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.580, 10.120], loss: 0.002549, mae: 0.050606, mean_q: -0.277677
 77300/100000: episode: 773, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -13.001, mean reward: -0.130 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.136, 10.098], loss: 0.002515, mae: 0.050032, mean_q: -0.309482
 77400/100000: episode: 774, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -13.701, mean reward: -0.137 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.348, 10.098], loss: 0.002430, mae: 0.048938, mean_q: -0.325776
 77500/100000: episode: 775, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.021, mean reward: -0.170 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.255, 10.184], loss: 0.004195, mae: 0.060664, mean_q: -0.317209
 77600/100000: episode: 776, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -5.913, mean reward: -0.059 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.291, 10.098], loss: 0.002672, mae: 0.054078, mean_q: -0.330487
 77700/100000: episode: 777, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.076, mean reward: -0.141 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.807, 10.098], loss: 0.002503, mae: 0.049428, mean_q: -0.323382
 77800/100000: episode: 778, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -20.904, mean reward: -0.209 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.995, 10.114], loss: 0.002401, mae: 0.049294, mean_q: -0.315374
 77900/100000: episode: 779, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.433, mean reward: -0.174 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.717, 10.098], loss: 0.002343, mae: 0.048683, mean_q: -0.307079
 78000/100000: episode: 780, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -13.767, mean reward: -0.138 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.924, 10.345], loss: 0.002532, mae: 0.049994, mean_q: -0.323013
 78100/100000: episode: 781, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.105, mean reward: -0.151 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.538, 10.098], loss: 0.002505, mae: 0.051251, mean_q: -0.291171
 78200/100000: episode: 782, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -13.891, mean reward: -0.139 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.515, 10.098], loss: 0.002704, mae: 0.052321, mean_q: -0.279909
 78300/100000: episode: 783, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.262, mean reward: -0.193 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.210, 10.098], loss: 0.002848, mae: 0.052912, mean_q: -0.322376
 78400/100000: episode: 784, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.391, mean reward: -0.164 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.132, 10.229], loss: 0.002640, mae: 0.052310, mean_q: -0.313653
 78500/100000: episode: 785, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.491, mean reward: -0.175 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.677, 10.098], loss: 0.002473, mae: 0.049283, mean_q: -0.301333
 78600/100000: episode: 786, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -12.964, mean reward: -0.130 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.569, 10.108], loss: 0.002322, mae: 0.047601, mean_q: -0.300048
 78700/100000: episode: 787, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -16.605, mean reward: -0.166 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.466, 10.403], loss: 0.002291, mae: 0.048377, mean_q: -0.296705
 78800/100000: episode: 788, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.283, mean reward: -0.143 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.460, 10.098], loss: 0.002352, mae: 0.047465, mean_q: -0.323077
 78900/100000: episode: 789, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.653, mean reward: -0.187 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.651, 10.167], loss: 0.002464, mae: 0.050023, mean_q: -0.300851
 79000/100000: episode: 790, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.550, mean reward: -0.155 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.546, 10.286], loss: 0.002558, mae: 0.051366, mean_q: -0.276165
 79100/100000: episode: 791, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.738, mean reward: -0.187 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.165, 10.250], loss: 0.002480, mae: 0.049929, mean_q: -0.297175
 79200/100000: episode: 792, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.521, mean reward: -0.165 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.780, 10.337], loss: 0.002312, mae: 0.048818, mean_q: -0.321852
 79300/100000: episode: 793, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.979, mean reward: -0.180 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.006, 10.098], loss: 0.002265, mae: 0.047285, mean_q: -0.314635
 79400/100000: episode: 794, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.167, mean reward: -0.172 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.043, 10.098], loss: 0.002444, mae: 0.050261, mean_q: -0.328472
 79500/100000: episode: 795, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.991, mean reward: -0.200 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.007, 10.181], loss: 0.002622, mae: 0.052223, mean_q: -0.286654
 79600/100000: episode: 796, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.765, mean reward: -0.168 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.522, 10.233], loss: 0.002533, mae: 0.050939, mean_q: -0.315990
 79700/100000: episode: 797, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.308, mean reward: -0.173 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.179, 10.310], loss: 0.002519, mae: 0.051086, mean_q: -0.288435
 79800/100000: episode: 798, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.812, mean reward: -0.188 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.575, 10.098], loss: 0.002378, mae: 0.048902, mean_q: -0.337561
 79900/100000: episode: 799, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -17.843, mean reward: -0.178 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.844, 10.098], loss: 0.007126, mae: 0.075528, mean_q: -0.305631
 80000/100000: episode: 800, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.366, mean reward: -0.144 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.279, 10.098], loss: 0.002941, mae: 0.055152, mean_q: -0.312995
 80100/100000: episode: 801, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.403, mean reward: -0.154 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.370, 10.098], loss: 0.002409, mae: 0.050505, mean_q: -0.293864
 80200/100000: episode: 802, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -14.478, mean reward: -0.145 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.893, 10.098], loss: 0.002496, mae: 0.050193, mean_q: -0.308570
 80300/100000: episode: 803, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -12.933, mean reward: -0.129 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.178, 10.098], loss: 0.002385, mae: 0.048229, mean_q: -0.297131
 80400/100000: episode: 804, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -13.507, mean reward: -0.135 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.941, 10.098], loss: 0.002316, mae: 0.047538, mean_q: -0.298873
 80500/100000: episode: 805, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.283, mean reward: -0.163 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.954, 10.465], loss: 0.002596, mae: 0.049779, mean_q: -0.296273
 80600/100000: episode: 806, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.659, mean reward: -0.187 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.252, 10.098], loss: 0.002420, mae: 0.048127, mean_q: -0.317670
 80700/100000: episode: 807, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -14.593, mean reward: -0.146 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.865, 10.363], loss: 0.002348, mae: 0.048004, mean_q: -0.315062
 80800/100000: episode: 808, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.179, mean reward: -0.182 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.848, 10.142], loss: 0.002744, mae: 0.052195, mean_q: -0.278647
 80900/100000: episode: 809, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.400, mean reward: -0.164 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.871, 10.098], loss: 0.002402, mae: 0.049189, mean_q: -0.310199
 81000/100000: episode: 810, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.783, mean reward: -0.178 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.441, 10.098], loss: 0.002403, mae: 0.048284, mean_q: -0.320430
 81100/100000: episode: 811, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.131, mean reward: -0.191 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.763, 10.098], loss: 0.002276, mae: 0.047298, mean_q: -0.285521
 81200/100000: episode: 812, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.174, mean reward: -0.172 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.104, 10.098], loss: 0.002383, mae: 0.048441, mean_q: -0.302622
 81300/100000: episode: 813, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -9.390, mean reward: -0.094 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.246, 10.451], loss: 0.002576, mae: 0.052218, mean_q: -0.305755
 81400/100000: episode: 814, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.938, mean reward: -0.199 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.958, 10.290], loss: 0.002602, mae: 0.050361, mean_q: -0.292410
 81500/100000: episode: 815, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.019, mean reward: -0.190 [-1.000, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.845, 10.098], loss: 0.002569, mae: 0.050738, mean_q: -0.306476
 81600/100000: episode: 816, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.692, mean reward: -0.187 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.944, 10.132], loss: 0.002384, mae: 0.047949, mean_q: -0.303265
 81700/100000: episode: 817, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.317, mean reward: -0.193 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.152, 10.098], loss: 0.002348, mae: 0.048535, mean_q: -0.321201
 81800/100000: episode: 818, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.099, mean reward: -0.171 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.885, 10.205], loss: 0.002270, mae: 0.048114, mean_q: -0.314161
 81900/100000: episode: 819, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -11.965, mean reward: -0.120 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.129, 10.299], loss: 0.002391, mae: 0.048045, mean_q: -0.301186
 82000/100000: episode: 820, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.271, mean reward: -0.183 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.130, 10.112], loss: 0.002435, mae: 0.050056, mean_q: -0.298065
 82100/100000: episode: 821, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.235, mean reward: -0.192 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.219, 10.098], loss: 0.002655, mae: 0.050864, mean_q: -0.304704
 82200/100000: episode: 822, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.799, mean reward: -0.198 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.891, 10.099], loss: 0.002340, mae: 0.048711, mean_q: -0.287081
 82300/100000: episode: 823, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.043, mean reward: -0.140 [-1.000, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.495, 10.325], loss: 0.002437, mae: 0.050200, mean_q: -0.317273
 82400/100000: episode: 824, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.824, mean reward: -0.178 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.560, 10.199], loss: 0.002186, mae: 0.045474, mean_q: -0.336245
 82500/100000: episode: 825, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.901, mean reward: -0.149 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.238, 10.105], loss: 0.002420, mae: 0.049420, mean_q: -0.315344
 82600/100000: episode: 826, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.001, mean reward: -0.180 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.633, 10.098], loss: 0.002567, mae: 0.050485, mean_q: -0.331337
 82700/100000: episode: 827, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.919, mean reward: -0.169 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.730, 10.098], loss: 0.002430, mae: 0.048094, mean_q: -0.336459
 82800/100000: episode: 828, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.641, mean reward: -0.156 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.170, 10.098], loss: 0.002746, mae: 0.052952, mean_q: -0.340010
 82900/100000: episode: 829, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.329, mean reward: -0.163 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.365, 10.098], loss: 0.002593, mae: 0.049835, mean_q: -0.326687
 83000/100000: episode: 830, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -10.950, mean reward: -0.110 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.190, 10.098], loss: 0.002467, mae: 0.048682, mean_q: -0.313585
 83100/100000: episode: 831, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -15.991, mean reward: -0.160 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.407, 10.170], loss: 0.002445, mae: 0.050377, mean_q: -0.291548
 83200/100000: episode: 832, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.010, mean reward: -0.170 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.662, 10.098], loss: 0.003019, mae: 0.055549, mean_q: -0.328729
 83300/100000: episode: 833, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.523, mean reward: -0.185 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.661, 10.098], loss: 0.004723, mae: 0.065578, mean_q: -0.324584
 83400/100000: episode: 834, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.693, mean reward: -0.187 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.120, 10.214], loss: 0.002912, mae: 0.052767, mean_q: -0.331494
 83500/100000: episode: 835, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.341, mean reward: -0.173 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.787, 10.098], loss: 0.002559, mae: 0.051472, mean_q: -0.315003
 83600/100000: episode: 836, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -16.850, mean reward: -0.168 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.361, 10.179], loss: 0.002387, mae: 0.048027, mean_q: -0.321091
 83700/100000: episode: 837, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.907, mean reward: -0.179 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.620, 10.258], loss: 0.002539, mae: 0.049883, mean_q: -0.305084
 83800/100000: episode: 838, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -14.759, mean reward: -0.148 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.317, 10.265], loss: 0.002429, mae: 0.049453, mean_q: -0.295737
 83900/100000: episode: 839, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -20.864, mean reward: -0.209 [-1.000, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.255, 10.098], loss: 0.002626, mae: 0.050529, mean_q: -0.303556
 84000/100000: episode: 840, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.243, mean reward: -0.172 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.870, 10.098], loss: 0.002479, mae: 0.048004, mean_q: -0.329951
 84100/100000: episode: 841, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -15.849, mean reward: -0.158 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.397, 10.127], loss: 0.002459, mae: 0.049512, mean_q: -0.290114
 84200/100000: episode: 842, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.912, mean reward: -0.179 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.532, 10.249], loss: 0.002227, mae: 0.046558, mean_q: -0.313346
 84300/100000: episode: 843, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.955, mean reward: -0.170 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.718, 10.236], loss: 0.002416, mae: 0.049191, mean_q: -0.289933
 84400/100000: episode: 844, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.495, mean reward: -0.185 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.814, 10.357], loss: 0.003493, mae: 0.060247, mean_q: -0.318021
 84500/100000: episode: 845, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.307, mean reward: -0.173 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.804, 10.309], loss: 0.002573, mae: 0.051350, mean_q: -0.343533
 84600/100000: episode: 846, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.835, mean reward: -0.148 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.908, 10.310], loss: 0.002162, mae: 0.046159, mean_q: -0.318779
 84700/100000: episode: 847, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -15.161, mean reward: -0.152 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.648, 10.132], loss: 0.002386, mae: 0.047784, mean_q: -0.300481
 84800/100000: episode: 848, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.560, mean reward: -0.156 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.663, 10.165], loss: 0.002419, mae: 0.048236, mean_q: -0.303011
 84900/100000: episode: 849, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.289, mean reward: -0.153 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.318, 10.098], loss: 0.002467, mae: 0.048703, mean_q: -0.294309
 85000/100000: episode: 850, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -15.138, mean reward: -0.151 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.372, 10.335], loss: 0.002516, mae: 0.049158, mean_q: -0.319945
 85100/100000: episode: 851, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.079, mean reward: -0.171 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.481, 10.154], loss: 0.002638, mae: 0.051618, mean_q: -0.291069
 85200/100000: episode: 852, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.979, mean reward: -0.170 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.525, 10.145], loss: 0.002328, mae: 0.048903, mean_q: -0.305089
 85300/100000: episode: 853, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -20.065, mean reward: -0.201 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.447, 10.137], loss: 0.002309, mae: 0.047838, mean_q: -0.318538
 85400/100000: episode: 854, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.151, mean reward: -0.202 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.710, 10.149], loss: 0.002313, mae: 0.047839, mean_q: -0.305576
 85500/100000: episode: 855, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -19.004, mean reward: -0.190 [-1.000, 0.235], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.241, 10.166], loss: 0.002321, mae: 0.046923, mean_q: -0.335403
 85600/100000: episode: 856, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.991, mean reward: -0.190 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.358, 10.127], loss: 0.002116, mae: 0.044709, mean_q: -0.332047
 85700/100000: episode: 857, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.798, mean reward: -0.168 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.799, 10.098], loss: 0.002455, mae: 0.049724, mean_q: -0.342564
 85800/100000: episode: 858, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.532, mean reward: -0.195 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.585, 10.098], loss: 0.002298, mae: 0.047743, mean_q: -0.330892
 85900/100000: episode: 859, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.527, mean reward: -0.165 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.830, 10.209], loss: 0.002332, mae: 0.048404, mean_q: -0.316702
 86000/100000: episode: 860, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.657, mean reward: -0.177 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.807, 10.098], loss: 0.002261, mae: 0.047711, mean_q: -0.333118
 86100/100000: episode: 861, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -15.557, mean reward: -0.156 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.391, 10.423], loss: 0.002270, mae: 0.047023, mean_q: -0.337003
 86200/100000: episode: 862, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.082, mean reward: -0.171 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.524, 10.289], loss: 0.002806, mae: 0.053365, mean_q: -0.325682
 86300/100000: episode: 863, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -19.785, mean reward: -0.198 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.741, 10.209], loss: 0.002304, mae: 0.048554, mean_q: -0.303057
 86400/100000: episode: 864, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.216, mean reward: -0.202 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.536, 10.098], loss: 0.002572, mae: 0.051406, mean_q: -0.362129
 86500/100000: episode: 865, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.119, mean reward: -0.181 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.198, 10.232], loss: 0.002299, mae: 0.047404, mean_q: -0.355704
 86600/100000: episode: 866, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -14.167, mean reward: -0.142 [-1.000, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.643, 10.155], loss: 0.002405, mae: 0.049619, mean_q: -0.307541
 86700/100000: episode: 867, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -19.504, mean reward: -0.195 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.915, 10.098], loss: 0.002385, mae: 0.048712, mean_q: -0.325253
 86800/100000: episode: 868, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.170, mean reward: -0.172 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.648, 10.153], loss: 0.002433, mae: 0.049779, mean_q: -0.316495
 86900/100000: episode: 869, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.364, mean reward: -0.184 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.601, 10.276], loss: 0.002486, mae: 0.049543, mean_q: -0.320774
 87000/100000: episode: 870, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.799, mean reward: -0.148 [-1.000, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.257, 10.327], loss: 0.002403, mae: 0.049233, mean_q: -0.303466
 87100/100000: episode: 871, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.567, mean reward: -0.156 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.113, 10.138], loss: 0.002611, mae: 0.050725, mean_q: -0.311695
 87200/100000: episode: 872, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.337, mean reward: -0.173 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.383, 10.098], loss: 0.002685, mae: 0.052977, mean_q: -0.322098
 87300/100000: episode: 873, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -16.673, mean reward: -0.167 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.635, 10.200], loss: 0.002942, mae: 0.058206, mean_q: -0.329583
 87400/100000: episode: 874, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -19.631, mean reward: -0.196 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.707, 10.098], loss: 0.002558, mae: 0.049816, mean_q: -0.317352
 87500/100000: episode: 875, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -20.433, mean reward: -0.204 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.146, 10.098], loss: 0.002452, mae: 0.050035, mean_q: -0.312083
 87600/100000: episode: 876, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.947, mean reward: -0.189 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.057, 10.098], loss: 0.002248, mae: 0.047982, mean_q: -0.331842
 87700/100000: episode: 877, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.192, mean reward: -0.182 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.789, 10.098], loss: 0.002381, mae: 0.048489, mean_q: -0.340517
 87800/100000: episode: 878, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -15.619, mean reward: -0.156 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.170, 10.138], loss: 0.003409, mae: 0.057446, mean_q: -0.339419
 87900/100000: episode: 879, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -11.953, mean reward: -0.120 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.641, 10.098], loss: 0.002516, mae: 0.050596, mean_q: -0.345952
 88000/100000: episode: 880, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.808, mean reward: -0.178 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.004, 10.212], loss: 0.002317, mae: 0.047070, mean_q: -0.363083
 88100/100000: episode: 881, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.199, mean reward: -0.182 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.122, 10.216], loss: 0.002346, mae: 0.048140, mean_q: -0.310770
 88200/100000: episode: 882, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.148, mean reward: -0.181 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.655, 10.230], loss: 0.002389, mae: 0.049263, mean_q: -0.310741
 88300/100000: episode: 883, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.079, mean reward: -0.121 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.383, 10.316], loss: 0.002380, mae: 0.048545, mean_q: -0.331973
 88400/100000: episode: 884, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.963, mean reward: -0.160 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.183, 10.149], loss: 0.002320, mae: 0.047832, mean_q: -0.333646
 88500/100000: episode: 885, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -18.200, mean reward: -0.182 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.641, 10.242], loss: 0.002382, mae: 0.048422, mean_q: -0.329020
 88600/100000: episode: 886, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.744, mean reward: -0.197 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.750, 10.122], loss: 0.002606, mae: 0.050485, mean_q: -0.303512
 88700/100000: episode: 887, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.779, mean reward: -0.148 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.569, 10.486], loss: 0.002618, mae: 0.053515, mean_q: -0.303956
 88800/100000: episode: 888, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -9.499, mean reward: -0.095 [-1.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.904, 10.098], loss: 0.002503, mae: 0.051655, mean_q: -0.342185
 88900/100000: episode: 889, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.830, mean reward: -0.178 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.555, 10.098], loss: 0.003581, mae: 0.059582, mean_q: -0.325465
 89000/100000: episode: 890, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.681, mean reward: -0.177 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.558, 10.300], loss: 0.002430, mae: 0.048498, mean_q: -0.341250
 89100/100000: episode: 891, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -12.989, mean reward: -0.130 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.535, 10.098], loss: 0.002350, mae: 0.048309, mean_q: -0.304973
 89200/100000: episode: 892, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -15.621, mean reward: -0.156 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.865, 10.098], loss: 0.002339, mae: 0.047646, mean_q: -0.338199
 89300/100000: episode: 893, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -18.513, mean reward: -0.185 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.633, 10.098], loss: 0.002577, mae: 0.050903, mean_q: -0.326030
 89400/100000: episode: 894, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -16.950, mean reward: -0.170 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.605, 10.098], loss: 0.002304, mae: 0.047669, mean_q: -0.351480
 89500/100000: episode: 895, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.818, mean reward: -0.178 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.605, 10.183], loss: 0.002407, mae: 0.049719, mean_q: -0.288433
 89600/100000: episode: 896, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.690, mean reward: -0.157 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.236, 10.169], loss: 0.002324, mae: 0.047893, mean_q: -0.340588
 89700/100000: episode: 897, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -13.676, mean reward: -0.137 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.788, 10.306], loss: 0.002455, mae: 0.048645, mean_q: -0.306831
 89800/100000: episode: 898, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.699, mean reward: -0.147 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.859, 10.301], loss: 0.002550, mae: 0.050768, mean_q: -0.275633
 89900/100000: episode: 899, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.250, mean reward: -0.163 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.045, 10.266], loss: 0.002288, mae: 0.047943, mean_q: -0.317109
 90000/100000: episode: 900, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -20.057, mean reward: -0.201 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.524, 10.098], loss: 0.002202, mae: 0.046217, mean_q: -0.311919
 90100/100000: episode: 901, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.299, mean reward: -0.163 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.455, 10.098], loss: 0.002294, mae: 0.047279, mean_q: -0.343212
 90200/100000: episode: 902, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.274, mean reward: -0.193 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.639, 10.098], loss: 0.002461, mae: 0.050229, mean_q: -0.297032
 90300/100000: episode: 903, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -12.513, mean reward: -0.125 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.831, 10.098], loss: 0.002589, mae: 0.050594, mean_q: -0.324910
 90400/100000: episode: 904, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -16.424, mean reward: -0.164 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.413, 10.215], loss: 0.002439, mae: 0.049268, mean_q: -0.306847
 90500/100000: episode: 905, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -11.374, mean reward: -0.114 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.887, 10.098], loss: 0.002472, mae: 0.049328, mean_q: -0.334452
 90600/100000: episode: 906, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.752, mean reward: -0.188 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.146, 10.235], loss: 0.002308, mae: 0.047873, mean_q: -0.302477
 90700/100000: episode: 907, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.846, mean reward: -0.178 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.513, 10.098], loss: 0.002497, mae: 0.049875, mean_q: -0.321167
 90800/100000: episode: 908, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -19.216, mean reward: -0.192 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.925, 10.098], loss: 0.002374, mae: 0.048577, mean_q: -0.324507
 90900/100000: episode: 909, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.588, mean reward: -0.146 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.026, 10.166], loss: 0.003116, mae: 0.057163, mean_q: -0.306908
 91000/100000: episode: 910, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -13.918, mean reward: -0.139 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.554, 10.098], loss: 0.002468, mae: 0.050683, mean_q: -0.290007
 91100/100000: episode: 911, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.986, mean reward: -0.180 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.374, 10.098], loss: 0.002346, mae: 0.047851, mean_q: -0.315520
 91200/100000: episode: 912, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.946, mean reward: -0.169 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.954, 10.263], loss: 0.002449, mae: 0.049705, mean_q: -0.286739
 91300/100000: episode: 913, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.165, mean reward: -0.182 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.853, 10.217], loss: 0.002439, mae: 0.048945, mean_q: -0.315466
 91400/100000: episode: 914, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -14.275, mean reward: -0.143 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.785, 10.098], loss: 0.002295, mae: 0.047031, mean_q: -0.341613
 91500/100000: episode: 915, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.492, mean reward: -0.195 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.065, 10.187], loss: 0.002494, mae: 0.049460, mean_q: -0.306126
 91600/100000: episode: 916, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -16.028, mean reward: -0.160 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.102, 10.188], loss: 0.002326, mae: 0.048094, mean_q: -0.319202
 91700/100000: episode: 917, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.350, mean reward: -0.203 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.961, 10.129], loss: 0.002396, mae: 0.048505, mean_q: -0.307702
 91800/100000: episode: 918, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.652, mean reward: -0.167 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.075, 10.098], loss: 0.002184, mae: 0.047049, mean_q: -0.304631
 91900/100000: episode: 919, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.806, mean reward: -0.168 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.546, 10.123], loss: 0.002330, mae: 0.048286, mean_q: -0.298224
 92000/100000: episode: 920, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -15.048, mean reward: -0.150 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.077, 10.098], loss: 0.002160, mae: 0.045804, mean_q: -0.311513
 92100/100000: episode: 921, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.965, mean reward: -0.180 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.687, 10.237], loss: 0.002239, mae: 0.046460, mean_q: -0.326234
 92200/100000: episode: 922, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -11.327, mean reward: -0.113 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.487, 10.239], loss: 0.002429, mae: 0.049716, mean_q: -0.314134
 92300/100000: episode: 923, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -18.116, mean reward: -0.181 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.275, 10.098], loss: 0.002228, mae: 0.047411, mean_q: -0.349405
 92400/100000: episode: 924, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -13.299, mean reward: -0.133 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.336, 10.188], loss: 0.002421, mae: 0.049824, mean_q: -0.289660
 92500/100000: episode: 925, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -12.748, mean reward: -0.127 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.753, 10.098], loss: 0.002283, mae: 0.048335, mean_q: -0.313588
 92600/100000: episode: 926, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -19.780, mean reward: -0.198 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.912, 10.098], loss: 0.005391, mae: 0.068105, mean_q: -0.276557
 92700/100000: episode: 927, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.845, mean reward: -0.178 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.152, 10.189], loss: 0.002454, mae: 0.050024, mean_q: -0.283465
 92800/100000: episode: 928, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -18.238, mean reward: -0.182 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.497, 10.098], loss: 0.002326, mae: 0.047314, mean_q: -0.309638
 92900/100000: episode: 929, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.396, mean reward: -0.164 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.864, 10.098], loss: 0.002386, mae: 0.048631, mean_q: -0.307575
 93000/100000: episode: 930, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.915, mean reward: -0.169 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.737, 10.098], loss: 0.002420, mae: 0.049158, mean_q: -0.286898
 93100/100000: episode: 931, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -16.927, mean reward: -0.169 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.625, 10.104], loss: 0.002379, mae: 0.049612, mean_q: -0.306251
 93200/100000: episode: 932, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.190, mean reward: -0.192 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.454, 10.178], loss: 0.002214, mae: 0.047637, mean_q: -0.279473
 93300/100000: episode: 933, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -12.679, mean reward: -0.127 [-1.000, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.390, 10.098], loss: 0.002483, mae: 0.049749, mean_q: -0.268387
 93400/100000: episode: 934, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.139, mean reward: -0.161 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.829, 10.243], loss: 0.002250, mae: 0.047405, mean_q: -0.319418
 93500/100000: episode: 935, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -14.454, mean reward: -0.145 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.219, 10.214], loss: 0.002350, mae: 0.048566, mean_q: -0.283380
 93600/100000: episode: 936, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -11.682, mean reward: -0.117 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.744, 10.138], loss: 0.002469, mae: 0.050926, mean_q: -0.259629
 93700/100000: episode: 937, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.992, mean reward: -0.190 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.139, 10.336], loss: 0.002364, mae: 0.048117, mean_q: -0.295517
 93800/100000: episode: 938, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -4.301, mean reward: -0.043 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.255, 10.098], loss: 0.002490, mae: 0.049675, mean_q: -0.291382
 93900/100000: episode: 939, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.046, mean reward: -0.170 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.835, 10.206], loss: 0.002410, mae: 0.048683, mean_q: -0.309807
 94000/100000: episode: 940, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.280, mean reward: -0.183 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.983, 10.287], loss: 0.002353, mae: 0.048370, mean_q: -0.275556
 94100/100000: episode: 941, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -13.299, mean reward: -0.133 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.676, 10.098], loss: 0.002328, mae: 0.048476, mean_q: -0.302802
 94200/100000: episode: 942, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.223, mean reward: -0.172 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.331, 10.190], loss: 0.002287, mae: 0.048413, mean_q: -0.308470
 94300/100000: episode: 943, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -21.234, mean reward: -0.212 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.564, 10.178], loss: 0.002211, mae: 0.045945, mean_q: -0.339249
 94400/100000: episode: 944, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.459, mean reward: -0.195 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.444, 10.126], loss: 0.002443, mae: 0.049375, mean_q: -0.304329
 94500/100000: episode: 945, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.244, mean reward: -0.172 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.424, 10.105], loss: 0.002540, mae: 0.052014, mean_q: -0.264867
 94600/100000: episode: 946, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.749, mean reward: -0.177 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.183, 10.159], loss: 0.002914, mae: 0.054089, mean_q: -0.305848
 94700/100000: episode: 947, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.959, mean reward: -0.180 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.524, 10.098], loss: 0.002503, mae: 0.050846, mean_q: -0.296438
 94800/100000: episode: 948, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.120, mean reward: -0.171 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.133, 10.135], loss: 0.002325, mae: 0.048235, mean_q: -0.300466
 94900/100000: episode: 949, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -9.564, mean reward: -0.096 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.417, 10.112], loss: 0.002443, mae: 0.050249, mean_q: -0.293208
 95000/100000: episode: 950, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.759, mean reward: -0.188 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.019, 10.098], loss: 0.002586, mae: 0.052536, mean_q: -0.295131
 95100/100000: episode: 951, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -13.793, mean reward: -0.138 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.970, 10.320], loss: 0.002469, mae: 0.051183, mean_q: -0.319344
 95200/100000: episode: 952, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.645, mean reward: -0.166 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.781, 10.126], loss: 0.002503, mae: 0.049466, mean_q: -0.298099
 95300/100000: episode: 953, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.159, mean reward: -0.162 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.734, 10.098], loss: 0.004487, mae: 0.062548, mean_q: -0.243636
 95400/100000: episode: 954, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -13.775, mean reward: -0.138 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.697, 10.225], loss: 0.002600, mae: 0.051803, mean_q: -0.316066
 95500/100000: episode: 955, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -14.749, mean reward: -0.147 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.034, 10.098], loss: 0.002364, mae: 0.048094, mean_q: -0.322368
 95600/100000: episode: 956, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -16.878, mean reward: -0.169 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.113, 10.135], loss: 0.002394, mae: 0.049363, mean_q: -0.283858
 95700/100000: episode: 957, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.849, mean reward: -0.158 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.287, 10.230], loss: 0.002545, mae: 0.050183, mean_q: -0.302895
 95800/100000: episode: 958, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.019, mean reward: -0.170 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.301, 10.311], loss: 0.002476, mae: 0.049430, mean_q: -0.312338
 95900/100000: episode: 959, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -15.098, mean reward: -0.151 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.610, 10.384], loss: 0.002474, mae: 0.048930, mean_q: -0.303285
 96000/100000: episode: 960, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.716, mean reward: -0.177 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.151, 10.219], loss: 0.002601, mae: 0.050718, mean_q: -0.262649
 96100/100000: episode: 961, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.257, mean reward: -0.183 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.779, 10.171], loss: 0.002351, mae: 0.048006, mean_q: -0.302807
 96200/100000: episode: 962, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.235, mean reward: -0.182 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.066, 10.166], loss: 0.002787, mae: 0.052056, mean_q: -0.305476
 96300/100000: episode: 963, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.383, mean reward: -0.194 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.736, 10.136], loss: 0.002567, mae: 0.051073, mean_q: -0.305620
 96400/100000: episode: 964, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.092, mean reward: -0.181 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.511, 10.161], loss: 0.002316, mae: 0.047526, mean_q: -0.302900
 96500/100000: episode: 965, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.531, mean reward: -0.175 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.215, 10.129], loss: 0.002551, mae: 0.050055, mean_q: -0.290057
 96600/100000: episode: 966, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -5.454, mean reward: -0.055 [-1.000, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.087, 10.098], loss: 0.002435, mae: 0.049118, mean_q: -0.306168
 96700/100000: episode: 967, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.646, mean reward: -0.146 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.831, 10.385], loss: 0.002524, mae: 0.049990, mean_q: -0.299759
 96800/100000: episode: 968, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -14.332, mean reward: -0.143 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.660, 10.098], loss: 0.002457, mae: 0.048817, mean_q: -0.323110
 96900/100000: episode: 969, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -12.360, mean reward: -0.124 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.666, 10.400], loss: 0.002433, mae: 0.049400, mean_q: -0.297108
 97000/100000: episode: 970, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.177, mean reward: -0.162 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.796, 10.218], loss: 0.002187, mae: 0.046354, mean_q: -0.324909
 97100/100000: episode: 971, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.652, mean reward: -0.177 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.563, 10.098], loss: 0.002161, mae: 0.045270, mean_q: -0.323707
 97200/100000: episode: 972, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -19.969, mean reward: -0.200 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.363, 10.143], loss: 0.002448, mae: 0.049440, mean_q: -0.295629
 97300/100000: episode: 973, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.350, mean reward: -0.163 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.891, 10.176], loss: 0.002557, mae: 0.049653, mean_q: -0.294367
 97400/100000: episode: 974, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.508, mean reward: -0.175 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.076, 10.252], loss: 0.002414, mae: 0.048896, mean_q: -0.291430
 97500/100000: episode: 975, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.583, mean reward: -0.166 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.091, 10.109], loss: 0.002579, mae: 0.051049, mean_q: -0.303353
 97600/100000: episode: 976, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.221, mean reward: -0.192 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.840, 10.098], loss: 0.002545, mae: 0.050297, mean_q: -0.272090
 97700/100000: episode: 977, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.654, mean reward: -0.177 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.464, 10.158], loss: 0.002899, mae: 0.052890, mean_q: -0.325484
 97800/100000: episode: 978, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.953, mean reward: -0.200 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.941, 10.145], loss: 0.002565, mae: 0.050298, mean_q: -0.303849
 97900/100000: episode: 979, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -19.833, mean reward: -0.198 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.482, 10.134], loss: 0.002314, mae: 0.048512, mean_q: -0.312257
 98000/100000: episode: 980, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.367, mean reward: -0.194 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.599, 10.098], loss: 0.002394, mae: 0.048707, mean_q: -0.314332
 98100/100000: episode: 981, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.602, mean reward: -0.146 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.737, 10.098], loss: 0.002453, mae: 0.049090, mean_q: -0.298569
 98200/100000: episode: 982, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -15.488, mean reward: -0.155 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.001, 10.098], loss: 0.002386, mae: 0.048887, mean_q: -0.286826
 98300/100000: episode: 983, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.084, mean reward: -0.181 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.462, 10.167], loss: 0.002470, mae: 0.049128, mean_q: -0.305314
 98400/100000: episode: 984, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.452, mean reward: -0.175 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.418, 10.315], loss: 0.002518, mae: 0.048761, mean_q: -0.317952
 98500/100000: episode: 985, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.073, mean reward: -0.181 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.322, 10.132], loss: 0.002291, mae: 0.046945, mean_q: -0.310312
 98600/100000: episode: 986, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -10.157, mean reward: -0.102 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.708, 10.502], loss: 0.002347, mae: 0.047376, mean_q: -0.325930
 98700/100000: episode: 987, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -14.241, mean reward: -0.142 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.464, 10.098], loss: 0.002336, mae: 0.048104, mean_q: -0.318905
 98800/100000: episode: 988, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.641, mean reward: -0.146 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.958, 10.098], loss: 0.002243, mae: 0.047058, mean_q: -0.338530
 98900/100000: episode: 989, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -17.239, mean reward: -0.172 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.599, 10.098], loss: 0.002389, mae: 0.047700, mean_q: -0.318897
 99000/100000: episode: 990, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -13.931, mean reward: -0.139 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.939, 10.354], loss: 0.003779, mae: 0.062237, mean_q: -0.265036
 99100/100000: episode: 991, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.114, mean reward: -0.171 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.660, 10.215], loss: 0.002201, mae: 0.046966, mean_q: -0.313635
 99200/100000: episode: 992, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -10.350, mean reward: -0.103 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.690, 10.098], loss: 0.002278, mae: 0.046887, mean_q: -0.307885
 99300/100000: episode: 993, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.635, mean reward: -0.156 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.919, 10.154], loss: 0.002603, mae: 0.049743, mean_q: -0.302222
 99400/100000: episode: 994, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.138, mean reward: -0.201 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.400, 10.158], loss: 0.002308, mae: 0.046815, mean_q: -0.330521
 99500/100000: episode: 995, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.001, mean reward: -0.170 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.821, 10.243], loss: 0.002465, mae: 0.048921, mean_q: -0.298255
 99600/100000: episode: 996, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -16.784, mean reward: -0.168 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.166, 10.226], loss: 0.002362, mae: 0.048740, mean_q: -0.297569
 99700/100000: episode: 997, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.035, mean reward: -0.140 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.774, 10.299], loss: 0.002285, mae: 0.047887, mean_q: -0.336648
 99800/100000: episode: 998, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -13.410, mean reward: -0.134 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.540, 10.098], loss: 0.002246, mae: 0.046027, mean_q: -0.321586
 99900/100000: episode: 999, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -14.941, mean reward: -0.149 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.376, 10.242], loss: 0.002347, mae: 0.048116, mean_q: -0.307512
 100000/100000: episode: 1000, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -11.527, mean reward: -0.115 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.393, 10.171], loss: 0.002242, mae: 0.047284, mean_q: -0.271042
done, took 536.268 seconds
[Info] End Uniform Random Simulation. Falsification occurred 0 times.
