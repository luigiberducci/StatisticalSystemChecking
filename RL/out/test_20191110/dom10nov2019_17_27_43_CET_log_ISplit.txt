Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.174s, episode steps: 100, steps per second: 573, episode reward: -12.300, mean reward: -0.123 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.910, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.066s, episode steps: 100, steps per second: 1518, episode reward: -16.738, mean reward: -0.167 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.105, 10.287], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.065s, episode steps: 100, steps per second: 1542, episode reward: -12.907, mean reward: -0.129 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.389, 10.393], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.065s, episode steps: 100, steps per second: 1534, episode reward: -18.228, mean reward: -0.182 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.532, 10.158], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.066s, episode steps: 100, steps per second: 1518, episode reward: -17.132, mean reward: -0.171 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.886, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.318s, episode steps: 100, steps per second: 76, episode reward: -16.932, mean reward: -0.169 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.471, 10.098], loss: 0.077811, mae: 0.257410, mean_q: -0.924742
   700/100000: episode: 7, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -16.438, mean reward: -0.164 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.828, 10.098], loss: 0.017587, mae: 0.133705, mean_q: -0.621847
   800/100000: episode: 8, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.520, mean reward: -0.175 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.123, 10.328], loss: 0.013746, mae: 0.114649, mean_q: -0.470828
   900/100000: episode: 9, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -17.197, mean reward: -0.172 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.683, 10.155], loss: 0.011248, mae: 0.101114, mean_q: -0.384342
  1000/100000: episode: 10, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -18.205, mean reward: -0.182 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.659, 10.130], loss: 0.010854, mae: 0.099089, mean_q: -0.362731
  1100/100000: episode: 11, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -19.185, mean reward: -0.192 [-1.000, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.713, 10.106], loss: 0.010332, mae: 0.096921, mean_q: -0.357463
  1200/100000: episode: 12, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -17.045, mean reward: -0.170 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.215, 10.190], loss: 0.009882, mae: 0.093042, mean_q: -0.318410
  1300/100000: episode: 13, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -14.660, mean reward: -0.147 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.514, 10.098], loss: 0.009798, mae: 0.093057, mean_q: -0.349734
  1400/100000: episode: 14, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -17.786, mean reward: -0.178 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.052, 10.098], loss: 0.009002, mae: 0.089000, mean_q: -0.333294
  1500/100000: episode: 15, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -19.464, mean reward: -0.195 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.536, 10.098], loss: 0.009220, mae: 0.092273, mean_q: -0.315565
  1600/100000: episode: 16, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -15.999, mean reward: -0.160 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.861, 10.400], loss: 0.007498, mae: 0.083687, mean_q: -0.336501
  1700/100000: episode: 17, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -13.083, mean reward: -0.131 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.776, 10.098], loss: 0.007925, mae: 0.085529, mean_q: -0.346470
  1800/100000: episode: 18, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -15.188, mean reward: -0.152 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.864, 10.098], loss: 0.007864, mae: 0.088600, mean_q: -0.308257
  1900/100000: episode: 19, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -18.469, mean reward: -0.185 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.897, 10.098], loss: 0.007270, mae: 0.083240, mean_q: -0.315749
  2000/100000: episode: 20, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -18.983, mean reward: -0.190 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.852, 10.098], loss: 0.008128, mae: 0.088009, mean_q: -0.327858
  2100/100000: episode: 21, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.204, mean reward: -0.192 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.766, 10.196], loss: 0.007840, mae: 0.088233, mean_q: -0.339760
  2200/100000: episode: 22, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -12.175, mean reward: -0.122 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.428, 10.098], loss: 0.007190, mae: 0.083056, mean_q: -0.321755
  2300/100000: episode: 23, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.345, mean reward: -0.173 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.571, 10.098], loss: 0.006747, mae: 0.080143, mean_q: -0.306140
  2400/100000: episode: 24, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -19.196, mean reward: -0.192 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.669, 10.275], loss: 0.007030, mae: 0.081691, mean_q: -0.298048
  2500/100000: episode: 25, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -15.238, mean reward: -0.152 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.911, 10.098], loss: 0.007120, mae: 0.081243, mean_q: -0.316177
  2600/100000: episode: 26, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -16.645, mean reward: -0.166 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.641, 10.098], loss: 0.006637, mae: 0.080690, mean_q: -0.319041
  2700/100000: episode: 27, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -14.392, mean reward: -0.144 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.179, 10.453], loss: 0.006165, mae: 0.078401, mean_q: -0.334532
  2800/100000: episode: 28, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -15.736, mean reward: -0.157 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.417, 10.098], loss: 0.007543, mae: 0.085591, mean_q: -0.311523
  2900/100000: episode: 29, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -10.917, mean reward: -0.109 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.704, 10.414], loss: 0.005905, mae: 0.075625, mean_q: -0.309745
  3000/100000: episode: 30, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -17.469, mean reward: -0.175 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.583, 10.098], loss: 0.005896, mae: 0.076266, mean_q: -0.325507
  3100/100000: episode: 31, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: -14.835, mean reward: -0.148 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.720, 10.098], loss: 0.005542, mae: 0.074944, mean_q: -0.297003
  3200/100000: episode: 32, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.979, mean reward: -0.180 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.691, 10.222], loss: 0.006793, mae: 0.083316, mean_q: -0.317566
  3300/100000: episode: 33, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -17.111, mean reward: -0.171 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.197, 10.098], loss: 0.006439, mae: 0.079296, mean_q: -0.328894
  3400/100000: episode: 34, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.429, mean reward: -0.174 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.044, 10.180], loss: 0.005934, mae: 0.077792, mean_q: -0.306326
  3500/100000: episode: 35, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.521, mean reward: -0.155 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.164, 10.098], loss: 0.006209, mae: 0.080112, mean_q: -0.290276
  3600/100000: episode: 36, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -19.422, mean reward: -0.194 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.349, 10.117], loss: 0.006910, mae: 0.084058, mean_q: -0.299335
  3700/100000: episode: 37, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.232, mean reward: -0.172 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.031, 10.098], loss: 0.005002, mae: 0.068914, mean_q: -0.331502
  3800/100000: episode: 38, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -16.037, mean reward: -0.160 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.974, 10.098], loss: 0.005902, mae: 0.075847, mean_q: -0.332060
  3900/100000: episode: 39, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -18.577, mean reward: -0.186 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.069, 10.098], loss: 0.005932, mae: 0.078101, mean_q: -0.307019
  4000/100000: episode: 40, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.264, mean reward: -0.163 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.385, 10.208], loss: 0.005928, mae: 0.075230, mean_q: -0.284245
  4100/100000: episode: 41, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -18.099, mean reward: -0.181 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.957, 10.169], loss: 0.005327, mae: 0.073104, mean_q: -0.310033
  4200/100000: episode: 42, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -15.983, mean reward: -0.160 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.739, 10.098], loss: 0.005200, mae: 0.072880, mean_q: -0.339465
  4300/100000: episode: 43, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -16.938, mean reward: -0.169 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.912, 10.297], loss: 0.004609, mae: 0.070020, mean_q: -0.337683
  4400/100000: episode: 44, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -17.365, mean reward: -0.174 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.856, 10.290], loss: 0.006219, mae: 0.079916, mean_q: -0.318551
  4500/100000: episode: 45, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.504, mean reward: -0.175 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.752, 10.198], loss: 0.004160, mae: 0.067158, mean_q: -0.299356
  4600/100000: episode: 46, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -18.251, mean reward: -0.183 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.057, 10.143], loss: 0.004796, mae: 0.070183, mean_q: -0.349671
  4700/100000: episode: 47, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -18.608, mean reward: -0.186 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.767, 10.235], loss: 0.004533, mae: 0.069539, mean_q: -0.299341
  4800/100000: episode: 48, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -15.765, mean reward: -0.158 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.484, 10.142], loss: 0.005156, mae: 0.072797, mean_q: -0.296169
  4900/100000: episode: 49, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -9.661, mean reward: -0.097 [-1.000, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.725, 10.098], loss: 0.004986, mae: 0.074170, mean_q: -0.319922
  5000/100000: episode: 50, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -12.445, mean reward: -0.124 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.550, 10.098], loss: 0.004723, mae: 0.070540, mean_q: -0.307703
  5100/100000: episode: 51, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.359, mean reward: -0.184 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.043, 10.110], loss: 0.003701, mae: 0.063193, mean_q: -0.300266
  5200/100000: episode: 52, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.700, mean reward: -0.187 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.411, 10.319], loss: 0.004210, mae: 0.067158, mean_q: -0.315886
  5300/100000: episode: 53, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -15.096, mean reward: -0.151 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.540, 10.310], loss: 0.004721, mae: 0.069452, mean_q: -0.304094
  5400/100000: episode: 54, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -14.408, mean reward: -0.144 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.166, 10.098], loss: 0.004230, mae: 0.067829, mean_q: -0.322602
  5500/100000: episode: 55, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -17.110, mean reward: -0.171 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.586, 10.098], loss: 0.003966, mae: 0.065606, mean_q: -0.305187
  5600/100000: episode: 56, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -18.090, mean reward: -0.181 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.921, 10.098], loss: 0.004455, mae: 0.069554, mean_q: -0.328869
  5700/100000: episode: 57, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -15.968, mean reward: -0.160 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.220, 10.098], loss: 0.005008, mae: 0.071484, mean_q: -0.279072
  5800/100000: episode: 58, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -15.999, mean reward: -0.160 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.020, 10.146], loss: 0.004962, mae: 0.073350, mean_q: -0.299709
  5900/100000: episode: 59, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.844, mean reward: -0.198 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.285, 10.180], loss: 0.004559, mae: 0.070912, mean_q: -0.319550
  6000/100000: episode: 60, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -17.378, mean reward: -0.174 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.530, 10.205], loss: 0.005187, mae: 0.073941, mean_q: -0.317373
  6100/100000: episode: 61, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -15.581, mean reward: -0.156 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.284, 10.215], loss: 0.004268, mae: 0.067977, mean_q: -0.319114
  6200/100000: episode: 62, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -17.585, mean reward: -0.176 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.512, 10.167], loss: 0.004576, mae: 0.070530, mean_q: -0.344418
  6300/100000: episode: 63, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.547, mean reward: -0.175 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.270, 10.098], loss: 0.003952, mae: 0.066565, mean_q: -0.291189
  6400/100000: episode: 64, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -13.539, mean reward: -0.135 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.572, 10.098], loss: 0.003902, mae: 0.065796, mean_q: -0.341280
  6500/100000: episode: 65, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.228, mean reward: -0.182 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.109, 10.098], loss: 0.004586, mae: 0.069595, mean_q: -0.294702
  6600/100000: episode: 66, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -15.814, mean reward: -0.158 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.125, 10.098], loss: 0.005391, mae: 0.075478, mean_q: -0.300678
  6700/100000: episode: 67, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -9.403, mean reward: -0.094 [-1.000, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.670, 10.385], loss: 0.004120, mae: 0.066739, mean_q: -0.293548
  6800/100000: episode: 68, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.886, mean reward: -0.179 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.230, 10.098], loss: 0.004190, mae: 0.068113, mean_q: -0.301046
  6900/100000: episode: 69, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -13.655, mean reward: -0.137 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.215, 10.098], loss: 0.006028, mae: 0.077774, mean_q: -0.323380
  7000/100000: episode: 70, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -15.041, mean reward: -0.150 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.785, 10.442], loss: 0.003992, mae: 0.066396, mean_q: -0.282854
  7100/100000: episode: 71, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.771, mean reward: -0.188 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.686, 10.098], loss: 0.005156, mae: 0.072784, mean_q: -0.340520
  7200/100000: episode: 72, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -20.174, mean reward: -0.202 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.008, 10.098], loss: 0.004309, mae: 0.068942, mean_q: -0.296621
  7300/100000: episode: 73, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.713, mean reward: -0.167 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.699, 10.315], loss: 0.004006, mae: 0.065897, mean_q: -0.291434
  7400/100000: episode: 74, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.033, mean reward: -0.180 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.361, 10.098], loss: 0.003787, mae: 0.065284, mean_q: -0.308833
  7500/100000: episode: 75, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -16.213, mean reward: -0.162 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.800, 10.147], loss: 0.004620, mae: 0.068463, mean_q: -0.320518
  7600/100000: episode: 76, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -15.067, mean reward: -0.151 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.719, 10.098], loss: 0.004316, mae: 0.068177, mean_q: -0.300675
  7700/100000: episode: 77, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -15.861, mean reward: -0.159 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.128, 10.098], loss: 0.005262, mae: 0.071964, mean_q: -0.309986
  7800/100000: episode: 78, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -17.097, mean reward: -0.171 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.682, 10.098], loss: 0.004834, mae: 0.073200, mean_q: -0.292446
  7900/100000: episode: 79, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -16.341, mean reward: -0.163 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.381, 10.103], loss: 0.004012, mae: 0.065555, mean_q: -0.321061
  8000/100000: episode: 80, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -20.067, mean reward: -0.201 [-1.000, 0.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.490, 10.098], loss: 0.003808, mae: 0.065578, mean_q: -0.310702
  8100/100000: episode: 81, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -15.623, mean reward: -0.156 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.949, 10.163], loss: 0.003964, mae: 0.068986, mean_q: -0.308185
  8200/100000: episode: 82, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: -18.053, mean reward: -0.181 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.324, 10.098], loss: 0.003981, mae: 0.064435, mean_q: -0.311504
  8300/100000: episode: 83, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -17.920, mean reward: -0.179 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.615, 10.227], loss: 0.004038, mae: 0.065641, mean_q: -0.312723
  8400/100000: episode: 84, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -17.801, mean reward: -0.178 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.069, 10.173], loss: 0.003776, mae: 0.066508, mean_q: -0.303122
  8500/100000: episode: 85, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -13.350, mean reward: -0.133 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.894, 10.098], loss: 0.003414, mae: 0.061455, mean_q: -0.306235
  8600/100000: episode: 86, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.452, mean reward: -0.185 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.556, 10.188], loss: 0.003533, mae: 0.062929, mean_q: -0.311301
  8700/100000: episode: 87, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -17.838, mean reward: -0.178 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.459, 10.098], loss: 0.005281, mae: 0.073477, mean_q: -0.342398
  8800/100000: episode: 88, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -16.525, mean reward: -0.165 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.815, 10.211], loss: 0.004710, mae: 0.071644, mean_q: -0.346330
  8900/100000: episode: 89, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -20.484, mean reward: -0.205 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.240, 10.098], loss: 0.003997, mae: 0.067242, mean_q: -0.328420
  9000/100000: episode: 90, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -19.770, mean reward: -0.198 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.425, 10.098], loss: 0.003319, mae: 0.061458, mean_q: -0.321427
  9100/100000: episode: 91, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -14.332, mean reward: -0.143 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.965, 10.098], loss: 0.003977, mae: 0.066700, mean_q: -0.303772
  9200/100000: episode: 92, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -10.854, mean reward: -0.109 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.254, 10.098], loss: 0.004672, mae: 0.071251, mean_q: -0.335307
  9300/100000: episode: 93, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -20.423, mean reward: -0.204 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.533, 10.312], loss: 0.003452, mae: 0.061585, mean_q: -0.336713
  9400/100000: episode: 94, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.238, mean reward: -0.182 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.912, 10.271], loss: 0.003901, mae: 0.067272, mean_q: -0.309951
  9500/100000: episode: 95, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -16.674, mean reward: -0.167 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.901, 10.098], loss: 0.003432, mae: 0.062241, mean_q: -0.292333
  9600/100000: episode: 96, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.646, mean reward: -0.186 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.436, 10.135], loss: 0.003360, mae: 0.061813, mean_q: -0.331409
  9700/100000: episode: 97, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -19.380, mean reward: -0.194 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.508, 10.104], loss: 0.003274, mae: 0.061075, mean_q: -0.313153
  9800/100000: episode: 98, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -14.457, mean reward: -0.145 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.715, 10.098], loss: 0.004441, mae: 0.070895, mean_q: -0.331357
  9900/100000: episode: 99, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -17.478, mean reward: -0.175 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.387, 10.098], loss: 0.003613, mae: 0.063677, mean_q: -0.305432
 10000/100000: episode: 100, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -18.976, mean reward: -0.190 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.280, 10.098], loss: 0.003872, mae: 0.064531, mean_q: -0.305062
 10100/100000: episode: 101, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -13.290, mean reward: -0.133 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.404, 10.380], loss: 0.003383, mae: 0.060124, mean_q: -0.351226
 10200/100000: episode: 102, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.703, mean reward: -0.167 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.772, 10.101], loss: 0.003596, mae: 0.064314, mean_q: -0.304756
 10300/100000: episode: 103, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.394, mean reward: -0.184 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.715, 10.306], loss: 0.005217, mae: 0.076475, mean_q: -0.318761
 10400/100000: episode: 104, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.451, mean reward: -0.175 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.386, 10.178], loss: 0.003240, mae: 0.060429, mean_q: -0.310414
 10500/100000: episode: 105, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -20.107, mean reward: -0.201 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.518, 10.218], loss: 0.002973, mae: 0.056641, mean_q: -0.295601
 10600/100000: episode: 106, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -18.894, mean reward: -0.189 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.014, 10.098], loss: 0.003197, mae: 0.059717, mean_q: -0.319319
 10700/100000: episode: 107, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -20.023, mean reward: -0.200 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.603, 10.098], loss: 0.003640, mae: 0.063874, mean_q: -0.311049
 10800/100000: episode: 108, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -18.335, mean reward: -0.183 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.496, 10.119], loss: 0.003485, mae: 0.063951, mean_q: -0.297026
 10900/100000: episode: 109, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.101, mean reward: -0.171 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.205, 10.176], loss: 0.004491, mae: 0.069277, mean_q: -0.327510
 11000/100000: episode: 110, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.079, mean reward: -0.161 [-1.000, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.699, 10.098], loss: 0.004140, mae: 0.067891, mean_q: -0.338029
 11100/100000: episode: 111, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -18.828, mean reward: -0.188 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.112, 10.098], loss: 0.004184, mae: 0.067971, mean_q: -0.276233
 11200/100000: episode: 112, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -14.937, mean reward: -0.149 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.659, 10.098], loss: 0.003716, mae: 0.064929, mean_q: -0.338637
 11300/100000: episode: 113, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -16.733, mean reward: -0.167 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.491, 10.167], loss: 0.003240, mae: 0.060466, mean_q: -0.298703
 11400/100000: episode: 114, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -15.500, mean reward: -0.155 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.296, 10.214], loss: 0.003590, mae: 0.064356, mean_q: -0.314206
 11500/100000: episode: 115, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -15.358, mean reward: -0.154 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.896, 10.195], loss: 0.003296, mae: 0.061580, mean_q: -0.305890
 11600/100000: episode: 116, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -15.692, mean reward: -0.157 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.415, 10.098], loss: 0.003452, mae: 0.060981, mean_q: -0.311617
 11700/100000: episode: 117, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -18.051, mean reward: -0.181 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.813, 10.098], loss: 0.005812, mae: 0.074596, mean_q: -0.301809
 11800/100000: episode: 118, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: -11.768, mean reward: -0.118 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.953, 10.098], loss: 0.002926, mae: 0.056844, mean_q: -0.287810
 11900/100000: episode: 119, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: -10.605, mean reward: -0.106 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.555, 10.452], loss: 0.003354, mae: 0.060597, mean_q: -0.315893
 12000/100000: episode: 120, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: -19.495, mean reward: -0.195 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.891, 10.159], loss: 0.003771, mae: 0.063546, mean_q: -0.333064
 12100/100000: episode: 121, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: -9.919, mean reward: -0.099 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.300, 10.098], loss: 0.003284, mae: 0.060462, mean_q: -0.304097
 12200/100000: episode: 122, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: -17.611, mean reward: -0.176 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.588, 10.225], loss: 0.003598, mae: 0.064012, mean_q: -0.311052
 12300/100000: episode: 123, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -15.957, mean reward: -0.160 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.488, 10.098], loss: 0.003049, mae: 0.057106, mean_q: -0.331873
 12400/100000: episode: 124, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -16.688, mean reward: -0.167 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.847, 10.282], loss: 0.004021, mae: 0.067051, mean_q: -0.321747
 12500/100000: episode: 125, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: -18.149, mean reward: -0.181 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.099, 10.098], loss: 0.003123, mae: 0.059306, mean_q: -0.326407
 12600/100000: episode: 126, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -16.808, mean reward: -0.168 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.305, 10.239], loss: 0.003011, mae: 0.057657, mean_q: -0.307554
 12700/100000: episode: 127, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: -18.266, mean reward: -0.183 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.979, 10.178], loss: 0.002791, mae: 0.055449, mean_q: -0.331218
 12800/100000: episode: 128, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -17.560, mean reward: -0.176 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.946, 10.196], loss: 0.002976, mae: 0.057322, mean_q: -0.322387
 12900/100000: episode: 129, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -19.323, mean reward: -0.193 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.397, 10.139], loss: 0.003296, mae: 0.060076, mean_q: -0.283448
 13000/100000: episode: 130, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: -18.394, mean reward: -0.184 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.905, 10.098], loss: 0.003485, mae: 0.063249, mean_q: -0.331094
 13100/100000: episode: 131, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: -20.223, mean reward: -0.202 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.694, 10.098], loss: 0.003021, mae: 0.058621, mean_q: -0.312170
 13200/100000: episode: 132, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: -18.760, mean reward: -0.188 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.720, 10.176], loss: 0.003454, mae: 0.061586, mean_q: -0.314540
 13300/100000: episode: 133, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -16.664, mean reward: -0.167 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.665, 10.121], loss: 0.003179, mae: 0.060397, mean_q: -0.326845
 13400/100000: episode: 134, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -12.093, mean reward: -0.121 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.110, 10.098], loss: 0.003103, mae: 0.059037, mean_q: -0.328087
 13500/100000: episode: 135, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -14.888, mean reward: -0.149 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.904, 10.098], loss: 0.006390, mae: 0.075965, mean_q: -0.296318
 13600/100000: episode: 136, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -15.746, mean reward: -0.157 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.073, 10.098], loss: 0.004001, mae: 0.064004, mean_q: -0.355140
 13700/100000: episode: 137, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -17.776, mean reward: -0.178 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.064, 10.340], loss: 0.003009, mae: 0.058372, mean_q: -0.326337
 13800/100000: episode: 138, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: -19.438, mean reward: -0.194 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.537, 10.131], loss: 0.002890, mae: 0.056867, mean_q: -0.282375
 13900/100000: episode: 139, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: -14.662, mean reward: -0.147 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.623, 10.289], loss: 0.002845, mae: 0.054975, mean_q: -0.338073
 14000/100000: episode: 140, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: -10.413, mean reward: -0.104 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.667, 10.098], loss: 0.002817, mae: 0.055401, mean_q: -0.323546
 14100/100000: episode: 141, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -14.294, mean reward: -0.143 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.975, 10.098], loss: 0.003039, mae: 0.056989, mean_q: -0.307557
 14200/100000: episode: 142, duration: 0.601s, episode steps: 100, steps per second: 167, episode reward: -17.696, mean reward: -0.177 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.735, 10.321], loss: 0.004686, mae: 0.068881, mean_q: -0.328668
 14300/100000: episode: 143, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -17.680, mean reward: -0.177 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.936, 10.250], loss: 0.003131, mae: 0.058703, mean_q: -0.291136
 14400/100000: episode: 144, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: -16.090, mean reward: -0.161 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.530, 10.098], loss: 0.002854, mae: 0.055114, mean_q: -0.301959
 14500/100000: episode: 145, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: -16.783, mean reward: -0.168 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.275, 10.506], loss: 0.003300, mae: 0.058976, mean_q: -0.317034
 14600/100000: episode: 146, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -18.506, mean reward: -0.185 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.397, 10.204], loss: 0.003080, mae: 0.058096, mean_q: -0.322058
 14700/100000: episode: 147, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -17.749, mean reward: -0.177 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.889, 10.164], loss: 0.002957, mae: 0.056773, mean_q: -0.316922
 14800/100000: episode: 148, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -14.270, mean reward: -0.143 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.078, 10.115], loss: 0.004048, mae: 0.065723, mean_q: -0.312948
 14900/100000: episode: 149, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.792, mean reward: -0.178 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.903, 10.129], loss: 0.003623, mae: 0.064226, mean_q: -0.320976
[Info] 100-TH LEVEL FOUND: 0.5037881731987, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.844s, episode steps: 100, steps per second: 21, episode reward: -20.059, mean reward: -0.201 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.067, 10.098], loss: 0.003087, mae: 0.058486, mean_q: -0.291582
 15030/100000: episode: 151, duration: 0.180s, episode steps: 30, steps per second: 167, episode reward: 9.337, mean reward: 0.311 [0.111, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.730, 10.100], loss: 0.003286, mae: 0.060384, mean_q: -0.341846
 15051/100000: episode: 152, duration: 0.133s, episode steps: 21, steps per second: 157, episode reward: 5.985, mean reward: 0.285 [0.221, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.267, 10.100], loss: 0.002888, mae: 0.055620, mean_q: -0.282399
 15092/100000: episode: 153, duration: 0.271s, episode steps: 41, steps per second: 151, episode reward: 8.364, mean reward: 0.204 [0.011, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.092, 10.100], loss: 0.003213, mae: 0.060083, mean_q: -0.283377
 15125/100000: episode: 154, duration: 0.190s, episode steps: 33, steps per second: 173, episode reward: 11.688, mean reward: 0.354 [0.211, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.720, 10.100], loss: 0.003027, mae: 0.057175, mean_q: -0.254342
 15166/100000: episode: 155, duration: 0.242s, episode steps: 41, steps per second: 170, episode reward: 11.770, mean reward: 0.287 [0.182, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.938, 10.299], loss: 0.002642, mae: 0.052555, mean_q: -0.270090
 15201/100000: episode: 156, duration: 0.190s, episode steps: 35, steps per second: 185, episode reward: 8.391, mean reward: 0.240 [0.121, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.108, 10.452], loss: 0.002715, mae: 0.054777, mean_q: -0.286870
 15231/100000: episode: 157, duration: 0.195s, episode steps: 30, steps per second: 154, episode reward: 11.924, mean reward: 0.397 [0.321, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.333, 10.100], loss: 0.002853, mae: 0.055487, mean_q: -0.270767
 15272/100000: episode: 158, duration: 0.236s, episode steps: 41, steps per second: 174, episode reward: 11.588, mean reward: 0.283 [0.142, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.399, 10.353], loss: 0.002685, mae: 0.053519, mean_q: -0.266584
 15305/100000: episode: 159, duration: 0.185s, episode steps: 33, steps per second: 179, episode reward: 7.544, mean reward: 0.229 [0.051, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.510, 10.232], loss: 0.002776, mae: 0.055200, mean_q: -0.258005
 15338/100000: episode: 160, duration: 0.194s, episode steps: 33, steps per second: 170, episode reward: 8.608, mean reward: 0.261 [0.089, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.043, 10.100], loss: 0.002988, mae: 0.057900, mean_q: -0.204071
 15368/100000: episode: 161, duration: 0.174s, episode steps: 30, steps per second: 173, episode reward: 8.304, mean reward: 0.277 [0.064, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.519, 10.100], loss: 0.002594, mae: 0.053441, mean_q: -0.289572
 15385/100000: episode: 162, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 4.030, mean reward: 0.237 [0.175, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.381], loss: 0.002387, mae: 0.050957, mean_q: -0.292675
 15402/100000: episode: 163, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 3.842, mean reward: 0.226 [0.131, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.308], loss: 0.002957, mae: 0.057056, mean_q: -0.299337
 15437/100000: episode: 164, duration: 0.216s, episode steps: 35, steps per second: 162, episode reward: 7.162, mean reward: 0.205 [0.044, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.651, 10.100], loss: 0.003210, mae: 0.058712, mean_q: -0.246627
 15458/100000: episode: 165, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 6.502, mean reward: 0.310 [0.222, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.380, 10.100], loss: 0.002646, mae: 0.055468, mean_q: -0.249609
 15475/100000: episode: 166, duration: 0.105s, episode steps: 17, steps per second: 163, episode reward: 3.908, mean reward: 0.230 [0.143, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.309], loss: 0.002958, mae: 0.056325, mean_q: -0.149406
 15508/100000: episode: 167, duration: 0.173s, episode steps: 33, steps per second: 190, episode reward: 8.005, mean reward: 0.243 [0.071, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.248, 10.100], loss: 0.002784, mae: 0.056070, mean_q: -0.237498
 15529/100000: episode: 168, duration: 0.125s, episode steps: 21, steps per second: 167, episode reward: 6.271, mean reward: 0.299 [0.137, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.383, 10.100], loss: 0.009425, mae: 0.088659, mean_q: -0.231662
 15552/100000: episode: 169, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 7.059, mean reward: 0.307 [0.206, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.835, 10.100], loss: 0.006060, mae: 0.083892, mean_q: -0.226665
 15573/100000: episode: 170, duration: 0.132s, episode steps: 21, steps per second: 159, episode reward: 7.723, mean reward: 0.368 [0.256, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.969, 10.100], loss: 0.006678, mae: 0.086386, mean_q: -0.252511
 15595/100000: episode: 171, duration: 0.127s, episode steps: 22, steps per second: 174, episode reward: 5.531, mean reward: 0.251 [0.128, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.751, 10.100], loss: 0.005633, mae: 0.070314, mean_q: -0.191399
 15619/100000: episode: 172, duration: 0.156s, episode steps: 24, steps per second: 154, episode reward: 6.767, mean reward: 0.282 [0.199, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.888, 10.328], loss: 0.004920, mae: 0.075757, mean_q: -0.226880
 15642/100000: episode: 173, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 6.915, mean reward: 0.301 [0.252, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.647, 10.100], loss: 0.010048, mae: 0.092252, mean_q: -0.234819
 15677/100000: episode: 174, duration: 0.203s, episode steps: 35, steps per second: 173, episode reward: 10.471, mean reward: 0.299 [0.191, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.361, 10.374], loss: 0.005558, mae: 0.074272, mean_q: -0.194190
 15694/100000: episode: 175, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 6.743, mean reward: 0.397 [0.251, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.442, 10.566], loss: 0.004859, mae: 0.076484, mean_q: -0.211911
 15717/100000: episode: 176, duration: 0.139s, episode steps: 23, steps per second: 165, episode reward: 7.008, mean reward: 0.305 [0.235, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.153, 10.100], loss: 0.003673, mae: 0.060083, mean_q: -0.195007
 15740/100000: episode: 177, duration: 0.139s, episode steps: 23, steps per second: 165, episode reward: 5.982, mean reward: 0.260 [0.195, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.174, 10.100], loss: 0.003418, mae: 0.060102, mean_q: -0.209570
 15766/100000: episode: 178, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 8.814, mean reward: 0.339 [0.254, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.324, 10.100], loss: 0.005165, mae: 0.074251, mean_q: -0.198454
 15787/100000: episode: 179, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 7.278, mean reward: 0.347 [0.222, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.395, 10.100], loss: 0.003936, mae: 0.063320, mean_q: -0.195056
 15808/100000: episode: 180, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 6.472, mean reward: 0.308 [0.202, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.223, 10.100], loss: 0.004225, mae: 0.064840, mean_q: -0.177261
 15832/100000: episode: 181, duration: 0.144s, episode steps: 24, steps per second: 167, episode reward: 7.118, mean reward: 0.297 [0.163, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.767, 10.496], loss: 0.004213, mae: 0.064577, mean_q: -0.202085
 15858/100000: episode: 182, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 7.254, mean reward: 0.279 [0.190, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.750, 10.100], loss: 0.004280, mae: 0.069292, mean_q: -0.131946
 15881/100000: episode: 183, duration: 0.135s, episode steps: 23, steps per second: 170, episode reward: 7.918, mean reward: 0.344 [0.182, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.446, 10.100], loss: 0.004008, mae: 0.062119, mean_q: -0.152828
 15903/100000: episode: 184, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 6.358, mean reward: 0.289 [0.152, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.297, 10.100], loss: 0.003267, mae: 0.058868, mean_q: -0.158332
 15925/100000: episode: 185, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 5.850, mean reward: 0.266 [0.198, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.213, 10.100], loss: 0.005106, mae: 0.070861, mean_q: -0.116808
 15955/100000: episode: 186, duration: 0.172s, episode steps: 30, steps per second: 174, episode reward: 10.599, mean reward: 0.353 [0.265, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.546, 10.100], loss: 0.005319, mae: 0.070904, mean_q: -0.184847
 15990/100000: episode: 187, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 7.307, mean reward: 0.209 [0.056, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.156, 10.100], loss: 0.003277, mae: 0.060848, mean_q: -0.103628
 16020/100000: episode: 188, duration: 0.172s, episode steps: 30, steps per second: 174, episode reward: 8.646, mean reward: 0.288 [0.231, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.661, 10.100], loss: 0.003565, mae: 0.060853, mean_q: -0.128420
 16041/100000: episode: 189, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 5.404, mean reward: 0.257 [0.151, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.216, 10.100], loss: 0.004198, mae: 0.063635, mean_q: -0.064754
 16076/100000: episode: 190, duration: 0.210s, episode steps: 35, steps per second: 167, episode reward: 7.333, mean reward: 0.210 [0.090, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.592, 10.203], loss: 0.004213, mae: 0.064061, mean_q: -0.136504
 16097/100000: episode: 191, duration: 0.126s, episode steps: 21, steps per second: 166, episode reward: 7.366, mean reward: 0.351 [0.232, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.204, 10.100], loss: 0.004939, mae: 0.075118, mean_q: -0.116542
 16114/100000: episode: 192, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 4.989, mean reward: 0.293 [0.178, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.398], loss: 0.008250, mae: 0.083640, mean_q: -0.095367
 16140/100000: episode: 193, duration: 0.159s, episode steps: 26, steps per second: 163, episode reward: 8.501, mean reward: 0.327 [0.153, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.570, 10.100], loss: 0.004922, mae: 0.075300, mean_q: -0.121922
 16170/100000: episode: 194, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 7.599, mean reward: 0.253 [0.095, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.147, 10.100], loss: 0.003977, mae: 0.064618, mean_q: -0.113066
 16211/100000: episode: 195, duration: 0.232s, episode steps: 41, steps per second: 177, episode reward: 9.557, mean reward: 0.233 [0.112, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.678, 10.211], loss: 0.004076, mae: 0.066802, mean_q: -0.056522
 16235/100000: episode: 196, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 6.668, mean reward: 0.278 [0.170, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.338], loss: 0.003831, mae: 0.062554, mean_q: -0.115756
 16261/100000: episode: 197, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 9.079, mean reward: 0.349 [0.256, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.515, 10.100], loss: 0.003284, mae: 0.058490, mean_q: -0.039897
 16282/100000: episode: 198, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 5.048, mean reward: 0.240 [0.044, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.847, 10.100], loss: 0.003763, mae: 0.064292, mean_q: -0.042782
 16303/100000: episode: 199, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 8.337, mean reward: 0.397 [0.258, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.631, 10.100], loss: 0.004468, mae: 0.073457, mean_q: -0.032298
 16333/100000: episode: 200, duration: 0.177s, episode steps: 30, steps per second: 170, episode reward: 9.804, mean reward: 0.327 [0.147, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.206, 10.100], loss: 0.003499, mae: 0.061525, mean_q: -0.070187
 16374/100000: episode: 201, duration: 0.246s, episode steps: 41, steps per second: 166, episode reward: 7.768, mean reward: 0.189 [0.071, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.483, 10.196], loss: 0.003672, mae: 0.066375, mean_q: -0.054500
 16415/100000: episode: 202, duration: 0.250s, episode steps: 41, steps per second: 164, episode reward: 7.224, mean reward: 0.176 [0.012, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-1.010, 10.120], loss: 0.003886, mae: 0.066102, mean_q: -0.063401
 16437/100000: episode: 203, duration: 0.127s, episode steps: 22, steps per second: 174, episode reward: 6.693, mean reward: 0.304 [0.223, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.348, 10.100], loss: 0.003430, mae: 0.062689, mean_q: -0.060668
 16459/100000: episode: 204, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 8.227, mean reward: 0.374 [0.252, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.434, 10.100], loss: 0.003298, mae: 0.061184, mean_q: -0.060537
 16481/100000: episode: 205, duration: 0.136s, episode steps: 22, steps per second: 162, episode reward: 4.240, mean reward: 0.193 [0.009, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.641, 10.111], loss: 0.003204, mae: 0.059786, mean_q: -0.007105
 16498/100000: episode: 206, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 4.442, mean reward: 0.261 [0.175, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.428], loss: 0.003664, mae: 0.063345, mean_q: -0.084998
 16519/100000: episode: 207, duration: 0.139s, episode steps: 21, steps per second: 151, episode reward: 6.180, mean reward: 0.294 [0.222, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.195, 10.100], loss: 0.003300, mae: 0.061123, mean_q: -0.065481
 16543/100000: episode: 208, duration: 0.165s, episode steps: 24, steps per second: 145, episode reward: 5.197, mean reward: 0.217 [0.097, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.905, 10.297], loss: 0.003584, mae: 0.064350, mean_q: -0.021068
 16573/100000: episode: 209, duration: 0.193s, episode steps: 30, steps per second: 156, episode reward: 9.247, mean reward: 0.308 [0.192, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.824, 10.100], loss: 0.003208, mae: 0.059216, mean_q: -0.017803
 16606/100000: episode: 210, duration: 0.183s, episode steps: 33, steps per second: 180, episode reward: 12.072, mean reward: 0.366 [0.214, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.541, 10.100], loss: 0.004334, mae: 0.070370, mean_q: -0.025398
 16636/100000: episode: 211, duration: 0.180s, episode steps: 30, steps per second: 166, episode reward: 8.029, mean reward: 0.268 [0.115, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.213, 10.100], loss: 0.003615, mae: 0.065595, mean_q: 0.006390
 16671/100000: episode: 212, duration: 0.213s, episode steps: 35, steps per second: 164, episode reward: 8.329, mean reward: 0.238 [0.137, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.454, 10.269], loss: 0.003408, mae: 0.064022, mean_q: 0.009354
 16688/100000: episode: 213, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 4.202, mean reward: 0.247 [0.097, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.519, 10.201], loss: 0.003571, mae: 0.061481, mean_q: -0.000864
 16709/100000: episode: 214, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 6.947, mean reward: 0.331 [0.231, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.232, 10.100], loss: 0.002878, mae: 0.056550, mean_q: -0.065694
 16750/100000: episode: 215, duration: 0.231s, episode steps: 41, steps per second: 177, episode reward: 7.576, mean reward: 0.185 [0.016, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.365, 10.168], loss: 0.003988, mae: 0.066757, mean_q: -0.042785
 16773/100000: episode: 216, duration: 0.146s, episode steps: 23, steps per second: 157, episode reward: 8.801, mean reward: 0.383 [0.258, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.747, 10.100], loss: 0.002889, mae: 0.056940, mean_q: -0.045254
 16799/100000: episode: 217, duration: 0.170s, episode steps: 26, steps per second: 153, episode reward: 5.408, mean reward: 0.208 [0.025, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.071, 10.100], loss: 0.004460, mae: 0.069600, mean_q: -0.006081
 16823/100000: episode: 218, duration: 0.149s, episode steps: 24, steps per second: 161, episode reward: 7.637, mean reward: 0.318 [0.132, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.467, 10.315], loss: 0.003086, mae: 0.061172, mean_q: -0.007537
 16856/100000: episode: 219, duration: 0.216s, episode steps: 33, steps per second: 152, episode reward: 9.528, mean reward: 0.289 [0.151, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.239, 10.100], loss: 0.003468, mae: 0.062733, mean_q: 0.003875
 16889/100000: episode: 220, duration: 0.198s, episode steps: 33, steps per second: 167, episode reward: 10.776, mean reward: 0.327 [0.205, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.551, 10.100], loss: 0.003669, mae: 0.065509, mean_q: 0.038091
 16911/100000: episode: 221, duration: 0.137s, episode steps: 22, steps per second: 161, episode reward: 5.358, mean reward: 0.244 [0.130, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.304, 10.100], loss: 0.003351, mae: 0.062923, mean_q: 0.042627
 16941/100000: episode: 222, duration: 0.185s, episode steps: 30, steps per second: 162, episode reward: 6.900, mean reward: 0.230 [0.071, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.310, 10.100], loss: 0.003331, mae: 0.062270, mean_q: 0.040894
 16971/100000: episode: 223, duration: 0.179s, episode steps: 30, steps per second: 168, episode reward: 8.203, mean reward: 0.273 [0.134, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.892, 10.100], loss: 0.003367, mae: 0.061772, mean_q: -0.007851
 16994/100000: episode: 224, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 8.762, mean reward: 0.381 [0.265, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.503, 10.100], loss: 0.003391, mae: 0.060867, mean_q: 0.039884
 17024/100000: episode: 225, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 5.718, mean reward: 0.191 [0.101, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.090, 10.100], loss: 0.003236, mae: 0.060644, mean_q: 0.059421
 17050/100000: episode: 226, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 6.727, mean reward: 0.259 [0.172, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.230, 10.100], loss: 0.005054, mae: 0.072562, mean_q: 0.039020
 17067/100000: episode: 227, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 4.375, mean reward: 0.257 [0.169, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.426], loss: 0.004170, mae: 0.072963, mean_q: 0.061616
 17090/100000: episode: 228, duration: 0.138s, episode steps: 23, steps per second: 167, episode reward: 7.323, mean reward: 0.318 [0.163, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.559, 10.100], loss: 0.003613, mae: 0.064720, mean_q: 0.045445
 17125/100000: episode: 229, duration: 0.224s, episode steps: 35, steps per second: 156, episode reward: 7.435, mean reward: 0.212 [0.048, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.035, 10.262], loss: 0.003426, mae: 0.061565, mean_q: 0.055040
 17155/100000: episode: 230, duration: 0.179s, episode steps: 30, steps per second: 168, episode reward: 9.225, mean reward: 0.307 [0.121, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.731, 10.100], loss: 0.003520, mae: 0.064341, mean_q: 0.082804
 17172/100000: episode: 231, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 6.161, mean reward: 0.362 [0.258, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.953, 10.392], loss: 0.003873, mae: 0.067822, mean_q: 0.059220
 17205/100000: episode: 232, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 11.020, mean reward: 0.334 [0.158, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.093, 10.100], loss: 0.003300, mae: 0.063835, mean_q: 0.059062
 17228/100000: episode: 233, duration: 0.142s, episode steps: 23, steps per second: 162, episode reward: 6.816, mean reward: 0.296 [0.207, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.294, 10.100], loss: 0.003379, mae: 0.063235, mean_q: 0.073618
 17258/100000: episode: 234, duration: 0.176s, episode steps: 30, steps per second: 170, episode reward: 8.527, mean reward: 0.284 [0.193, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.257, 10.100], loss: 0.003804, mae: 0.066409, mean_q: 0.073513
 17275/100000: episode: 235, duration: 0.118s, episode steps: 17, steps per second: 144, episode reward: 2.952, mean reward: 0.174 [0.104, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.093, 10.185], loss: 0.002922, mae: 0.060259, mean_q: 0.089468
 17308/100000: episode: 236, duration: 0.205s, episode steps: 33, steps per second: 161, episode reward: 13.163, mean reward: 0.399 [0.245, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.983, 10.100], loss: 0.002844, mae: 0.058251, mean_q: 0.037870
 17338/100000: episode: 237, duration: 0.175s, episode steps: 30, steps per second: 171, episode reward: 10.660, mean reward: 0.355 [0.203, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.240, 10.100], loss: 0.003680, mae: 0.063594, mean_q: 0.134638
 17379/100000: episode: 238, duration: 0.241s, episode steps: 41, steps per second: 170, episode reward: 10.090, mean reward: 0.246 [0.094, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.594, 10.196], loss: 0.003198, mae: 0.061479, mean_q: 0.110668
 17412/100000: episode: 239, duration: 0.200s, episode steps: 33, steps per second: 165, episode reward: 10.338, mean reward: 0.313 [0.137, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.171, 10.100], loss: 0.003201, mae: 0.059603, mean_q: 0.093283
[Info] 200-TH LEVEL FOUND: 0.724086344242096, Considering 10/90 traces
 17435/100000: episode: 240, duration: 4.367s, episode steps: 23, steps per second: 5, episode reward: 7.769, mean reward: 0.338 [0.182, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.358, 10.100], loss: 0.003565, mae: 0.065895, mean_q: 0.166309
 17444/100000: episode: 241, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 4.148, mean reward: 0.461 [0.389, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.607], loss: 0.003133, mae: 0.058741, mean_q: 0.083641
 17466/100000: episode: 242, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 5.546, mean reward: 0.252 [0.178, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.395], loss: 0.002791, mae: 0.057696, mean_q: 0.099688
 17487/100000: episode: 243, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 8.637, mean reward: 0.411 [0.308, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.056, 10.487], loss: 0.003001, mae: 0.058804, mean_q: 0.083539
 17509/100000: episode: 244, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 6.964, mean reward: 0.317 [0.250, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.254, 10.468], loss: 0.002951, mae: 0.058256, mean_q: 0.140123
 17531/100000: episode: 245, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 8.947, mean reward: 0.407 [0.349, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.230, 10.100], loss: 0.002955, mae: 0.059821, mean_q: 0.127841
 17548/100000: episode: 246, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 6.536, mean reward: 0.384 [0.249, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.635, 10.406], loss: 0.002956, mae: 0.058007, mean_q: 0.133863
 17569/100000: episode: 247, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 6.934, mean reward: 0.330 [0.216, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.258, 10.418], loss: 0.003279, mae: 0.061104, mean_q: 0.128538
 17591/100000: episode: 248, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 8.208, mean reward: 0.373 [0.218, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.535], loss: 0.002813, mae: 0.057355, mean_q: 0.103932
 17607/100000: episode: 249, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 5.703, mean reward: 0.356 [0.269, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.350, 10.100], loss: 0.003145, mae: 0.059246, mean_q: 0.180795
 17627/100000: episode: 250, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 4.755, mean reward: 0.238 [0.041, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.699, 10.156], loss: 0.003332, mae: 0.063255, mean_q: 0.165826
 17655/100000: episode: 251, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 12.536, mean reward: 0.448 [0.338, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.631, 10.100], loss: 0.003866, mae: 0.065542, mean_q: 0.142976
 17671/100000: episode: 252, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 6.841, mean reward: 0.428 [0.372, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.627, 10.100], loss: 0.003532, mae: 0.063865, mean_q: 0.208708
 17687/100000: episode: 253, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 7.493, mean reward: 0.468 [0.341, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.666, 10.100], loss: 0.003783, mae: 0.063521, mean_q: 0.176598
 17707/100000: episode: 254, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 7.089, mean reward: 0.354 [0.275, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.128, 10.437], loss: 0.003383, mae: 0.063185, mean_q: 0.210266
 17737/100000: episode: 255, duration: 0.174s, episode steps: 30, steps per second: 172, episode reward: 12.053, mean reward: 0.402 [0.222, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.312, 10.416], loss: 0.003776, mae: 0.067045, mean_q: 0.196921
 17765/100000: episode: 256, duration: 0.165s, episode steps: 28, steps per second: 170, episode reward: 11.304, mean reward: 0.404 [0.265, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.597, 10.100], loss: 0.003778, mae: 0.064971, mean_q: 0.186249
 17781/100000: episode: 257, duration: 0.102s, episode steps: 16, steps per second: 156, episode reward: 6.011, mean reward: 0.376 [0.268, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.500, 10.100], loss: 0.004252, mae: 0.070350, mean_q: 0.180840
 17802/100000: episode: 258, duration: 0.129s, episode steps: 21, steps per second: 163, episode reward: 7.147, mean reward: 0.340 [0.250, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.273, 10.460], loss: 0.003609, mae: 0.064845, mean_q: 0.246858
 17818/100000: episode: 259, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 6.262, mean reward: 0.391 [0.271, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.705, 10.100], loss: 0.002904, mae: 0.056951, mean_q: 0.165550
 17840/100000: episode: 260, duration: 0.136s, episode steps: 22, steps per second: 162, episode reward: 7.612, mean reward: 0.346 [0.220, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.439], loss: 0.003271, mae: 0.062552, mean_q: 0.214666
 17862/100000: episode: 261, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 4.592, mean reward: 0.209 [0.078, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.184], loss: 0.003495, mae: 0.062477, mean_q: 0.210335
 17884/100000: episode: 262, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 8.840, mean reward: 0.402 [0.276, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.925, 10.100], loss: 0.003095, mae: 0.059709, mean_q: 0.206089
 17904/100000: episode: 263, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 6.884, mean reward: 0.344 [0.270, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.223, 10.395], loss: 0.003047, mae: 0.059498, mean_q: 0.229380
 17920/100000: episode: 264, duration: 0.100s, episode steps: 16, steps per second: 161, episode reward: 6.839, mean reward: 0.427 [0.362, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.304, 10.100], loss: 0.003055, mae: 0.060212, mean_q: 0.233245
 17948/100000: episode: 265, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 10.657, mean reward: 0.381 [0.222, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.220, 10.100], loss: 0.003253, mae: 0.062345, mean_q: 0.197185
 17978/100000: episode: 266, duration: 0.161s, episode steps: 30, steps per second: 187, episode reward: 12.800, mean reward: 0.427 [0.340, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.944, 10.365], loss: 0.003143, mae: 0.060892, mean_q: 0.197690
 18008/100000: episode: 267, duration: 0.165s, episode steps: 30, steps per second: 181, episode reward: 9.237, mean reward: 0.308 [0.163, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.363], loss: 0.003349, mae: 0.061764, mean_q: 0.242066
 18036/100000: episode: 268, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 12.502, mean reward: 0.447 [0.299, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.191, 10.100], loss: 0.002936, mae: 0.057823, mean_q: 0.163985
 18052/100000: episode: 269, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 6.605, mean reward: 0.413 [0.340, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.436, 10.100], loss: 0.003030, mae: 0.060843, mean_q: 0.213962
 18072/100000: episode: 270, duration: 0.104s, episode steps: 20, steps per second: 191, episode reward: 7.564, mean reward: 0.378 [0.273, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.158, 10.543], loss: 0.004085, mae: 0.069356, mean_q: 0.310653
 18088/100000: episode: 271, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 7.035, mean reward: 0.440 [0.360, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.795, 10.100], loss: 0.003938, mae: 0.068187, mean_q: 0.240934
 18105/100000: episode: 272, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 6.419, mean reward: 0.378 [0.293, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.378, 10.496], loss: 0.003094, mae: 0.060410, mean_q: 0.299981
 18127/100000: episode: 273, duration: 0.132s, episode steps: 22, steps per second: 167, episode reward: 9.423, mean reward: 0.428 [0.306, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.379, 10.100], loss: 0.003590, mae: 0.064169, mean_q: 0.278683
 18143/100000: episode: 274, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 5.936, mean reward: 0.371 [0.311, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.811, 10.100], loss: 0.003095, mae: 0.059351, mean_q: 0.255490
 18152/100000: episode: 275, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 2.883, mean reward: 0.320 [0.264, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.503], loss: 0.003771, mae: 0.065872, mean_q: 0.326451
 18172/100000: episode: 276, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 6.682, mean reward: 0.334 [0.289, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-1.316, 10.413], loss: 0.002889, mae: 0.059296, mean_q: 0.246009
 18194/100000: episode: 277, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 5.733, mean reward: 0.261 [0.138, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.344], loss: 0.003171, mae: 0.061316, mean_q: 0.297456
 18214/100000: episode: 278, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 7.773, mean reward: 0.389 [0.312, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-1.560, 10.542], loss: 0.002949, mae: 0.059249, mean_q: 0.219389
 18236/100000: episode: 279, duration: 0.114s, episode steps: 22, steps per second: 192, episode reward: 10.670, mean reward: 0.485 [0.388, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.452, 10.100], loss: 0.004287, mae: 0.068183, mean_q: 0.295381
 18252/100000: episode: 280, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 6.756, mean reward: 0.422 [0.326, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.602, 10.100], loss: 0.003598, mae: 0.068674, mean_q: 0.192753
 18274/100000: episode: 281, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 9.507, mean reward: 0.432 [0.236, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.410, 10.585], loss: 0.004222, mae: 0.069747, mean_q: 0.258879
 18291/100000: episode: 282, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 5.483, mean reward: 0.323 [0.220, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.322], loss: 0.003541, mae: 0.065015, mean_q: 0.341376
 18307/100000: episode: 283, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 7.007, mean reward: 0.438 [0.370, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.852, 10.100], loss: 0.003929, mae: 0.067886, mean_q: 0.224027
 18329/100000: episode: 284, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 5.068, mean reward: 0.230 [0.145, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.224, 10.342], loss: 0.003719, mae: 0.066856, mean_q: 0.318207
 18351/100000: episode: 285, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 7.169, mean reward: 0.326 [0.188, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.257, 10.419], loss: 0.003631, mae: 0.066470, mean_q: 0.331904
 18368/100000: episode: 286, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 4.721, mean reward: 0.278 [0.198, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.215, 10.300], loss: 0.003216, mae: 0.062242, mean_q: 0.317505
 18384/100000: episode: 287, duration: 0.087s, episode steps: 16, steps per second: 185, episode reward: 6.543, mean reward: 0.409 [0.301, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.211, 10.100], loss: 0.004188, mae: 0.070778, mean_q: 0.291491
 18404/100000: episode: 288, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 8.118, mean reward: 0.406 [0.319, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.572], loss: 0.004222, mae: 0.068981, mean_q: 0.326019
 18432/100000: episode: 289, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 8.268, mean reward: 0.295 [0.183, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.042, 10.100], loss: 0.004484, mae: 0.069196, mean_q: 0.326346
 18453/100000: episode: 290, duration: 0.128s, episode steps: 21, steps per second: 164, episode reward: 6.798, mean reward: 0.324 [0.172, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.240, 10.342], loss: 0.003817, mae: 0.067134, mean_q: 0.344263
 18470/100000: episode: 291, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 7.229, mean reward: 0.425 [0.339, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.139, 10.496], loss: 0.004876, mae: 0.069568, mean_q: 0.350810
 18487/100000: episode: 292, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 6.363, mean reward: 0.374 [0.225, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.370], loss: 0.004880, mae: 0.069931, mean_q: 0.372064
 18509/100000: episode: 293, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 7.446, mean reward: 0.338 [0.201, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.228, 10.100], loss: 0.004795, mae: 0.071926, mean_q: 0.344391
 18537/100000: episode: 294, duration: 0.168s, episode steps: 28, steps per second: 167, episode reward: 14.867, mean reward: 0.531 [0.429, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.765, 10.100], loss: 0.005450, mae: 0.073943, mean_q: 0.360276
 18565/100000: episode: 295, duration: 0.165s, episode steps: 28, steps per second: 169, episode reward: 14.515, mean reward: 0.518 [0.337, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.532, 10.100], loss: 0.004832, mae: 0.076713, mean_q: 0.371053
 18581/100000: episode: 296, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 6.725, mean reward: 0.420 [0.321, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.789, 10.100], loss: 0.003510, mae: 0.066778, mean_q: 0.339478
 18601/100000: episode: 297, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 6.061, mean reward: 0.303 [0.253, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.040, 10.406], loss: 0.003523, mae: 0.065135, mean_q: 0.384481
 18629/100000: episode: 298, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 10.863, mean reward: 0.388 [0.206, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.797, 10.100], loss: 0.003142, mae: 0.062188, mean_q: 0.401450
 18645/100000: episode: 299, duration: 0.105s, episode steps: 16, steps per second: 152, episode reward: 6.388, mean reward: 0.399 [0.350, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-1.015, 10.100], loss: 0.003599, mae: 0.065786, mean_q: 0.339994
 18665/100000: episode: 300, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 7.709, mean reward: 0.385 [0.262, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.347, 10.632], loss: 0.003351, mae: 0.063841, mean_q: 0.376205
 18685/100000: episode: 301, duration: 0.131s, episode steps: 20, steps per second: 153, episode reward: 6.402, mean reward: 0.320 [0.181, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.270], loss: 0.003550, mae: 0.065130, mean_q: 0.407693
 18713/100000: episode: 302, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 12.004, mean reward: 0.429 [0.338, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.264, 10.100], loss: 0.003670, mae: 0.067291, mean_q: 0.388513
 18733/100000: episode: 303, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 7.589, mean reward: 0.379 [0.321, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.421], loss: 0.003501, mae: 0.063891, mean_q: 0.386905
 18754/100000: episode: 304, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 5.340, mean reward: 0.254 [0.045, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.514, 10.175], loss: 0.003536, mae: 0.064865, mean_q: 0.363448
 18775/100000: episode: 305, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 8.279, mean reward: 0.394 [0.274, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.710, 10.368], loss: 0.003816, mae: 0.066682, mean_q: 0.391193
 18797/100000: episode: 306, duration: 0.135s, episode steps: 22, steps per second: 163, episode reward: 8.605, mean reward: 0.391 [0.262, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.314, 10.100], loss: 0.003384, mae: 0.063456, mean_q: 0.426360
 18813/100000: episode: 307, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 7.732, mean reward: 0.483 [0.391, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.447, 10.100], loss: 0.004921, mae: 0.070166, mean_q: 0.401652
 18843/100000: episode: 308, duration: 0.158s, episode steps: 30, steps per second: 189, episode reward: 13.374, mean reward: 0.446 [0.344, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.279, 10.542], loss: 0.003900, mae: 0.066320, mean_q: 0.411640
 18852/100000: episode: 309, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 3.249, mean reward: 0.361 [0.276, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.390, 10.454], loss: 0.006548, mae: 0.081598, mean_q: 0.401999
 18882/100000: episode: 310, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 13.406, mean reward: 0.447 [0.332, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.489, 10.456], loss: 0.004378, mae: 0.071130, mean_q: 0.448607
 18912/100000: episode: 311, duration: 0.174s, episode steps: 30, steps per second: 173, episode reward: 9.632, mean reward: 0.321 [0.242, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.033, 10.333], loss: 0.007041, mae: 0.083511, mean_q: 0.414557
 18929/100000: episode: 312, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 5.653, mean reward: 0.333 [0.272, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.485], loss: 0.004512, mae: 0.073251, mean_q: 0.409944
 18938/100000: episode: 313, duration: 0.063s, episode steps: 9, steps per second: 143, episode reward: 2.977, mean reward: 0.331 [0.224, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.523], loss: 0.005885, mae: 0.076913, mean_q: 0.459121
 18947/100000: episode: 314, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 3.397, mean reward: 0.377 [0.299, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.520], loss: 0.004458, mae: 0.072466, mean_q: 0.467534
 18964/100000: episode: 315, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 4.870, mean reward: 0.286 [0.189, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.362], loss: 0.003635, mae: 0.066656, mean_q: 0.465149
 18980/100000: episode: 316, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 7.514, mean reward: 0.470 [0.413, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.719, 10.100], loss: 0.004626, mae: 0.068308, mean_q: 0.430478
 19002/100000: episode: 317, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 8.625, mean reward: 0.392 [0.323, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.282, 10.100], loss: 0.004088, mae: 0.069812, mean_q: 0.459868
 19018/100000: episode: 318, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 5.647, mean reward: 0.353 [0.290, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.297, 10.100], loss: 0.004893, mae: 0.068593, mean_q: 0.437906
 19039/100000: episode: 319, duration: 0.132s, episode steps: 21, steps per second: 159, episode reward: 5.846, mean reward: 0.278 [0.192, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-1.237, 10.341], loss: 0.004187, mae: 0.068437, mean_q: 0.490276
 19059/100000: episode: 320, duration: 0.130s, episode steps: 20, steps per second: 154, episode reward: 6.332, mean reward: 0.317 [0.207, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-1.078, 10.303], loss: 0.003937, mae: 0.069372, mean_q: 0.460279
 19081/100000: episode: 321, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 7.141, mean reward: 0.325 [0.202, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.416, 10.520], loss: 0.003679, mae: 0.065624, mean_q: 0.467216
 19097/100000: episode: 322, duration: 0.103s, episode steps: 16, steps per second: 156, episode reward: 5.411, mean reward: 0.338 [0.275, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.385, 10.100], loss: 0.003686, mae: 0.065986, mean_q: 0.464229
 19119/100000: episode: 323, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 5.991, mean reward: 0.272 [0.151, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.069, 10.410], loss: 0.003830, mae: 0.068609, mean_q: 0.455399
 19149/100000: episode: 324, duration: 0.185s, episode steps: 30, steps per second: 162, episode reward: 12.628, mean reward: 0.421 [0.237, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.035, 10.521], loss: 0.003886, mae: 0.067890, mean_q: 0.458224
 19171/100000: episode: 325, duration: 0.133s, episode steps: 22, steps per second: 165, episode reward: 5.422, mean reward: 0.246 [0.142, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.276], loss: 0.004626, mae: 0.074005, mean_q: 0.470767
 19180/100000: episode: 326, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 3.470, mean reward: 0.386 [0.327, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.275, 10.449], loss: 0.005254, mae: 0.078230, mean_q: 0.476365
 19189/100000: episode: 327, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 3.766, mean reward: 0.418 [0.307, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.529], loss: 0.003274, mae: 0.063413, mean_q: 0.433165
 19205/100000: episode: 328, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 6.466, mean reward: 0.404 [0.345, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.462, 10.100], loss: 0.003974, mae: 0.070117, mean_q: 0.496740
 19222/100000: episode: 329, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 5.400, mean reward: 0.318 [0.186, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.798, 10.284], loss: 0.003819, mae: 0.065572, mean_q: 0.477976
[Info] 300-TH LEVEL FOUND: 0.9195519089698792, Considering 10/90 traces
 19244/100000: episode: 330, duration: 4.235s, episode steps: 22, steps per second: 5, episode reward: 6.037, mean reward: 0.274 [0.126, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.353], loss: 0.003692, mae: 0.066654, mean_q: 0.501689
 19255/100000: episode: 331, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 6.008, mean reward: 0.546 [0.492, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.321, 10.100], loss: 0.005224, mae: 0.072302, mean_q: 0.455200
 19280/100000: episode: 332, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 10.255, mean reward: 0.410 [0.183, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.577, 10.100], loss: 0.004016, mae: 0.068761, mean_q: 0.515982
 19292/100000: episode: 333, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 5.597, mean reward: 0.466 [0.403, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.523, 10.100], loss: 0.004127, mae: 0.070334, mean_q: 0.527117
 19318/100000: episode: 334, duration: 0.154s, episode steps: 26, steps per second: 168, episode reward: 10.796, mean reward: 0.415 [0.312, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.819, 10.100], loss: 0.004000, mae: 0.068347, mean_q: 0.507947
 19344/100000: episode: 335, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 11.902, mean reward: 0.458 [0.339, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.424, 10.100], loss: 0.004918, mae: 0.071618, mean_q: 0.519490
 19355/100000: episode: 336, duration: 0.074s, episode steps: 11, steps per second: 150, episode reward: 4.883, mean reward: 0.444 [0.397, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.367, 10.100], loss: 0.003370, mae: 0.063661, mean_q: 0.529483
 19381/100000: episode: 337, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 11.730, mean reward: 0.451 [0.353, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.416, 10.100], loss: 0.004054, mae: 0.069039, mean_q: 0.547693
 19391/100000: episode: 338, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 4.761, mean reward: 0.476 [0.362, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.437, 10.100], loss: 0.003840, mae: 0.066279, mean_q: 0.527774
 19416/100000: episode: 339, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 9.967, mean reward: 0.399 [0.216, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.157, 10.100], loss: 0.003735, mae: 0.068201, mean_q: 0.526161
 19418/100000: episode: 340, duration: 0.022s, episode steps: 2, steps per second: 92, episode reward: 1.021, mean reward: 0.510 [0.476, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.035, 10.547], loss: 0.004034, mae: 0.070829, mean_q: 0.551013
 19431/100000: episode: 341, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 6.100, mean reward: 0.469 [0.420, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.388, 10.100], loss: 0.003313, mae: 0.062859, mean_q: 0.588310
 19456/100000: episode: 342, duration: 0.139s, episode steps: 25, steps per second: 179, episode reward: 12.757, mean reward: 0.510 [0.419, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.483, 10.100], loss: 0.003343, mae: 0.061946, mean_q: 0.534380
 19474/100000: episode: 343, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 7.582, mean reward: 0.421 [0.282, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.726, 10.418], loss: 0.003330, mae: 0.063244, mean_q: 0.572645
 19498/100000: episode: 344, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 11.268, mean reward: 0.470 [0.399, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.380, 10.100], loss: 0.003017, mae: 0.060276, mean_q: 0.574407
 19510/100000: episode: 345, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 5.799, mean reward: 0.483 [0.384, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.274, 10.100], loss: 0.003189, mae: 0.062502, mean_q: 0.573789
 19512/100000: episode: 346, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 1.047, mean reward: 0.523 [0.518, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.374 [-0.035, 10.553], loss: 0.003611, mae: 0.069138, mean_q: 0.610651
 19537/100000: episode: 347, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 10.276, mean reward: 0.411 [0.247, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.361, 10.100], loss: 0.003131, mae: 0.063087, mean_q: 0.570584
 19561/100000: episode: 348, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 11.045, mean reward: 0.460 [0.401, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.486, 10.100], loss: 0.003338, mae: 0.064023, mean_q: 0.582003
 19574/100000: episode: 349, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 5.770, mean reward: 0.444 [0.370, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.439, 10.100], loss: 0.003419, mae: 0.064668, mean_q: 0.608209
 19585/100000: episode: 350, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 5.769, mean reward: 0.524 [0.419, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.623, 10.100], loss: 0.003261, mae: 0.062772, mean_q: 0.604673
 19587/100000: episode: 351, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 1.177, mean reward: 0.589 [0.585, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.393 [-0.035, 10.657], loss: 0.006371, mae: 0.083551, mean_q: 0.629393
 19598/100000: episode: 352, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 4.738, mean reward: 0.431 [0.321, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.398, 10.100], loss: 0.003983, mae: 0.069914, mean_q: 0.570602
 19611/100000: episode: 353, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 5.983, mean reward: 0.460 [0.384, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.273, 10.100], loss: 0.003713, mae: 0.066190, mean_q: 0.584995
 19629/100000: episode: 354, duration: 0.106s, episode steps: 18, steps per second: 169, episode reward: 9.420, mean reward: 0.523 [0.455, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.306, 10.539], loss: 0.003303, mae: 0.063273, mean_q: 0.614741
 19631/100000: episode: 355, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 1.059, mean reward: 0.529 [0.521, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.543], loss: 0.003953, mae: 0.067961, mean_q: 0.618268
 19642/100000: episode: 356, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 5.344, mean reward: 0.486 [0.413, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.510, 10.100], loss: 0.003451, mae: 0.065492, mean_q: 0.611989
 19653/100000: episode: 357, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 6.494, mean reward: 0.590 [0.440, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.548, 10.100], loss: 0.003153, mae: 0.061987, mean_q: 0.598762
 19678/100000: episode: 358, duration: 0.127s, episode steps: 25, steps per second: 196, episode reward: 11.510, mean reward: 0.460 [0.278, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.228, 10.100], loss: 0.003457, mae: 0.065363, mean_q: 0.620548
 19703/100000: episode: 359, duration: 0.153s, episode steps: 25, steps per second: 163, episode reward: 11.832, mean reward: 0.473 [0.341, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.498, 10.100], loss: 0.003898, mae: 0.069311, mean_q: 0.616739
 19715/100000: episode: 360, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 6.338, mean reward: 0.528 [0.456, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.412, 10.100], loss: 0.003429, mae: 0.066638, mean_q: 0.614105
 19739/100000: episode: 361, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 11.987, mean reward: 0.499 [0.357, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.204, 10.100], loss: 0.003749, mae: 0.067392, mean_q: 0.619329
 19765/100000: episode: 362, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 11.986, mean reward: 0.461 [0.347, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.635, 10.100], loss: 0.004130, mae: 0.071451, mean_q: 0.640003
 19789/100000: episode: 363, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 9.666, mean reward: 0.403 [0.270, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.578, 10.100], loss: 0.003802, mae: 0.068506, mean_q: 0.642365
 19801/100000: episode: 364, duration: 0.070s, episode steps: 12, steps per second: 170, episode reward: 5.996, mean reward: 0.500 [0.443, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.496, 10.100], loss: 0.003314, mae: 0.065192, mean_q: 0.627027
 19812/100000: episode: 365, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 5.479, mean reward: 0.498 [0.436, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.505, 10.100], loss: 0.003554, mae: 0.067382, mean_q: 0.638927
 19836/100000: episode: 366, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 8.584, mean reward: 0.358 [0.155, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.899, 10.100], loss: 0.004069, mae: 0.070300, mean_q: 0.647710
 19854/100000: episode: 367, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 8.899, mean reward: 0.494 [0.419, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.607, 10.496], loss: 0.003652, mae: 0.065315, mean_q: 0.645936
 19879/100000: episode: 368, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 11.482, mean reward: 0.459 [0.307, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.344, 10.100], loss: 0.003485, mae: 0.065877, mean_q: 0.643994
 19890/100000: episode: 369, duration: 0.078s, episode steps: 11, steps per second: 140, episode reward: 5.705, mean reward: 0.519 [0.460, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.482, 10.100], loss: 0.003325, mae: 0.062773, mean_q: 0.646257
 19915/100000: episode: 370, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 9.965, mean reward: 0.399 [0.273, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.256, 10.100], loss: 0.004181, mae: 0.071462, mean_q: 0.655968
 19939/100000: episode: 371, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 10.821, mean reward: 0.451 [0.284, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.740, 10.100], loss: 0.003281, mae: 0.063259, mean_q: 0.649862
 19951/100000: episode: 372, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 6.160, mean reward: 0.513 [0.459, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.541, 10.100], loss: 0.003637, mae: 0.065815, mean_q: 0.658567
 19977/100000: episode: 373, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 12.778, mean reward: 0.491 [0.445, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.691, 10.100], loss: 0.003646, mae: 0.066754, mean_q: 0.659557
 20002/100000: episode: 374, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 12.593, mean reward: 0.504 [0.388, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.616, 10.100], loss: 0.003729, mae: 0.066783, mean_q: 0.651735
 20013/100000: episode: 375, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 5.660, mean reward: 0.515 [0.404, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.513, 10.100], loss: 0.003861, mae: 0.069332, mean_q: 0.659755
 20023/100000: episode: 376, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 4.026, mean reward: 0.403 [0.354, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.465, 10.100], loss: 0.003987, mae: 0.071508, mean_q: 0.666501
 20034/100000: episode: 377, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 5.563, mean reward: 0.506 [0.393, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.587, 10.100], loss: 0.003571, mae: 0.065548, mean_q: 0.643375
 20044/100000: episode: 378, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 4.886, mean reward: 0.489 [0.403, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.483, 10.100], loss: 0.003617, mae: 0.066685, mean_q: 0.649769
 20068/100000: episode: 379, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 7.200, mean reward: 0.300 [0.131, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.301, 10.100], loss: 0.003338, mae: 0.064795, mean_q: 0.663585
 20079/100000: episode: 380, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 5.655, mean reward: 0.514 [0.439, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.462, 10.100], loss: 0.003347, mae: 0.065321, mean_q: 0.670774
 20091/100000: episode: 381, duration: 0.077s, episode steps: 12, steps per second: 155, episode reward: 5.640, mean reward: 0.470 [0.427, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.377, 10.100], loss: 0.003824, mae: 0.068675, mean_q: 0.658362
 20109/100000: episode: 382, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 8.095, mean reward: 0.450 [0.367, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.484, 10.510], loss: 0.003524, mae: 0.066023, mean_q: 0.646604
 20119/100000: episode: 383, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 4.211, mean reward: 0.421 [0.387, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.423, 10.100], loss: 0.003870, mae: 0.066052, mean_q: 0.655197
 20129/100000: episode: 384, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 4.558, mean reward: 0.456 [0.391, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.338, 10.100], loss: 0.003567, mae: 0.065743, mean_q: 0.667035
 20131/100000: episode: 385, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.221, mean reward: 0.610 [0.554, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.156, 10.499], loss: 0.003483, mae: 0.066944, mean_q: 0.681869
 20142/100000: episode: 386, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 5.860, mean reward: 0.533 [0.471, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.426, 10.100], loss: 0.003738, mae: 0.067496, mean_q: 0.642099
 20144/100000: episode: 387, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 1.159, mean reward: 0.580 [0.539, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.035, 10.575], loss: 0.004619, mae: 0.077775, mean_q: 0.675438
 20168/100000: episode: 388, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 11.686, mean reward: 0.487 [0.397, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.303, 10.100], loss: 0.003835, mae: 0.068048, mean_q: 0.669770
 20194/100000: episode: 389, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 9.307, mean reward: 0.358 [0.247, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.775, 10.100], loss: 0.003776, mae: 0.068228, mean_q: 0.680094
 20219/100000: episode: 390, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 9.818, mean reward: 0.393 [0.231, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-1.469, 10.100], loss: 0.003318, mae: 0.062485, mean_q: 0.657368
 20229/100000: episode: 391, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 4.802, mean reward: 0.480 [0.440, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.416, 10.100], loss: 0.003427, mae: 0.067568, mean_q: 0.679914
 20240/100000: episode: 392, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 4.130, mean reward: 0.375 [0.279, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.620, 10.100], loss: 0.003395, mae: 0.065508, mean_q: 0.653717
 20242/100000: episode: 393, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 1.115, mean reward: 0.558 [0.535, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.505], loss: 0.004517, mae: 0.076429, mean_q: 0.696420
 20260/100000: episode: 394, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 7.802, mean reward: 0.433 [0.260, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.093, 10.403], loss: 0.003748, mae: 0.064618, mean_q: 0.676561
 20273/100000: episode: 395, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 5.616, mean reward: 0.432 [0.366, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.632, 10.100], loss: 0.003429, mae: 0.065007, mean_q: 0.665366
 20284/100000: episode: 396, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 5.686, mean reward: 0.517 [0.454, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.829, 10.100], loss: 0.003946, mae: 0.071268, mean_q: 0.648123
 20310/100000: episode: 397, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 12.070, mean reward: 0.464 [0.378, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.841, 10.100], loss: 0.003156, mae: 0.063362, mean_q: 0.676298
 20328/100000: episode: 398, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 7.093, mean reward: 0.394 [0.319, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.428], loss: 0.003815, mae: 0.068344, mean_q: 0.679762
 20354/100000: episode: 399, duration: 0.161s, episode steps: 26, steps per second: 161, episode reward: 11.237, mean reward: 0.432 [0.175, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.341, 10.100], loss: 0.003947, mae: 0.068839, mean_q: 0.671971
 20380/100000: episode: 400, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 10.029, mean reward: 0.386 [0.339, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.324, 10.100], loss: 0.003709, mae: 0.068005, mean_q: 0.680464
 20390/100000: episode: 401, duration: 0.091s, episode steps: 10, steps per second: 110, episode reward: 4.053, mean reward: 0.405 [0.378, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.977, 10.100], loss: 0.003951, mae: 0.067530, mean_q: 0.647702
 20401/100000: episode: 402, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 4.203, mean reward: 0.382 [0.349, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.256, 10.100], loss: 0.003553, mae: 0.065177, mean_q: 0.677363
 20413/100000: episode: 403, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 6.019, mean reward: 0.502 [0.391, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.728, 10.100], loss: 0.003282, mae: 0.063507, mean_q: 0.666999
 20439/100000: episode: 404, duration: 0.159s, episode steps: 26, steps per second: 164, episode reward: 12.023, mean reward: 0.462 [0.345, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.501, 10.100], loss: 0.003490, mae: 0.065928, mean_q: 0.678435
 20463/100000: episode: 405, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 9.324, mean reward: 0.388 [0.277, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-2.094, 10.100], loss: 0.003257, mae: 0.063072, mean_q: 0.677472
 20487/100000: episode: 406, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 10.898, mean reward: 0.454 [0.315, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.320, 10.100], loss: 0.003543, mae: 0.065413, mean_q: 0.670912
 20511/100000: episode: 407, duration: 0.130s, episode steps: 24, steps per second: 184, episode reward: 9.902, mean reward: 0.413 [0.285, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.351, 10.100], loss: 0.003328, mae: 0.064339, mean_q: 0.681544
 20521/100000: episode: 408, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 4.980, mean reward: 0.498 [0.384, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.346, 10.100], loss: 0.003046, mae: 0.062072, mean_q: 0.697093
 20547/100000: episode: 409, duration: 0.159s, episode steps: 26, steps per second: 163, episode reward: 10.466, mean reward: 0.403 [0.270, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.633, 10.100], loss: 0.003473, mae: 0.067155, mean_q: 0.673142
 20558/100000: episode: 410, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 5.618, mean reward: 0.511 [0.428, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.489, 10.100], loss: 0.003778, mae: 0.065618, mean_q: 0.678959
 20583/100000: episode: 411, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 12.585, mean reward: 0.503 [0.410, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.407, 10.100], loss: 0.003358, mae: 0.063658, mean_q: 0.681193
 20595/100000: episode: 412, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 5.690, mean reward: 0.474 [0.376, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.261, 10.100], loss: 0.004151, mae: 0.071146, mean_q: 0.701765
[Info] FALSIFICATION!
 20599/100000: episode: 413, duration: 0.028s, episode steps: 4, steps per second: 142, episode reward: 11.672, mean reward: 2.918 [0.525, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.009, 8.942], loss: 0.002411, mae: 0.052516, mean_q: 0.654455
 20699/100000: episode: 414, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -13.385, mean reward: -0.134 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.259, 10.428], loss: 0.016791, mae: 0.073520, mean_q: 0.672933
 20799/100000: episode: 415, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.880, mean reward: -0.169 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.393, 10.352], loss: 0.003514, mae: 0.065486, mean_q: 0.653889
 20899/100000: episode: 416, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.877, mean reward: -0.189 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.862, 10.198], loss: 0.045535, mae: 0.103175, mean_q: 0.652385
 20999/100000: episode: 417, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -17.838, mean reward: -0.178 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.852, 10.401], loss: 0.030820, mae: 0.082580, mean_q: 0.630857
 21099/100000: episode: 418, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.620, mean reward: -0.186 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.741, 10.209], loss: 0.003966, mae: 0.068006, mean_q: 0.602110
 21199/100000: episode: 419, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.281, mean reward: -0.163 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.822, 10.156], loss: 0.004160, mae: 0.067645, mean_q: 0.587764
 21299/100000: episode: 420, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -19.635, mean reward: -0.196 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.766, 10.098], loss: 0.004081, mae: 0.065697, mean_q: 0.567872
 21399/100000: episode: 421, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.516, mean reward: -0.185 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.453, 10.180], loss: 0.004432, mae: 0.068043, mean_q: 0.538036
 21499/100000: episode: 422, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.765, mean reward: -0.178 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.000, 10.098], loss: 0.004234, mae: 0.068488, mean_q: 0.521492
 21599/100000: episode: 423, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.241, mean reward: -0.182 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.243, 10.244], loss: 0.004361, mae: 0.068605, mean_q: 0.504307
 21699/100000: episode: 424, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -14.288, mean reward: -0.143 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.741, 10.316], loss: 0.003819, mae: 0.066193, mean_q: 0.488153
 21799/100000: episode: 425, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.605, mean reward: -0.196 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.831, 10.155], loss: 0.003962, mae: 0.066867, mean_q: 0.493345
 21899/100000: episode: 426, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.056, mean reward: -0.161 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.870, 10.098], loss: 0.004003, mae: 0.065571, mean_q: 0.461892
 21999/100000: episode: 427, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -13.962, mean reward: -0.140 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.056, 10.098], loss: 0.017305, mae: 0.074989, mean_q: 0.429464
 22099/100000: episode: 428, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -17.523, mean reward: -0.175 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.808, 10.098], loss: 0.003775, mae: 0.065020, mean_q: 0.426515
 22199/100000: episode: 429, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.514, mean reward: -0.195 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.906, 10.098], loss: 0.003884, mae: 0.067326, mean_q: 0.402131
 22299/100000: episode: 430, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.206, mean reward: -0.162 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.836, 10.098], loss: 0.017707, mae: 0.077251, mean_q: 0.372604
 22399/100000: episode: 431, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -6.585, mean reward: -0.066 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.857, 10.343], loss: 0.017846, mae: 0.078180, mean_q: 0.380294
 22499/100000: episode: 432, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.560, mean reward: -0.176 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.549, 10.098], loss: 0.016935, mae: 0.073237, mean_q: 0.339235
 22599/100000: episode: 433, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -17.412, mean reward: -0.174 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.882, 10.110], loss: 0.017052, mae: 0.073702, mean_q: 0.309275
 22699/100000: episode: 434, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.009, mean reward: -0.180 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.549, 10.131], loss: 0.004426, mae: 0.067414, mean_q: 0.307437
 22799/100000: episode: 435, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.688, mean reward: -0.177 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.607, 10.098], loss: 0.017027, mae: 0.072678, mean_q: 0.284647
 22899/100000: episode: 436, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.632, mean reward: -0.166 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.963, 10.098], loss: 0.017488, mae: 0.074635, mean_q: 0.263945
 22999/100000: episode: 437, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.076, mean reward: -0.151 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.378, 10.098], loss: 0.003540, mae: 0.062228, mean_q: 0.229435
 23099/100000: episode: 438, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.333, mean reward: -0.163 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.509, 10.098], loss: 0.017076, mae: 0.073413, mean_q: 0.209672
 23199/100000: episode: 439, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.874, mean reward: -0.179 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.527, 10.153], loss: 0.016608, mae: 0.069459, mean_q: 0.191770
 23299/100000: episode: 440, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -14.070, mean reward: -0.141 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.515, 10.098], loss: 0.003840, mae: 0.063493, mean_q: 0.180226
 23399/100000: episode: 441, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.485, mean reward: -0.175 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.269, 10.098], loss: 0.003621, mae: 0.063161, mean_q: 0.149650
 23499/100000: episode: 442, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -14.291, mean reward: -0.143 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.344, 10.298], loss: 0.005217, mae: 0.069192, mean_q: 0.097063
 23599/100000: episode: 443, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -17.056, mean reward: -0.171 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.304, 10.186], loss: 0.003921, mae: 0.064161, mean_q: 0.127035
 23699/100000: episode: 444, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.513, mean reward: -0.175 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.959, 10.154], loss: 0.003327, mae: 0.060929, mean_q: 0.103147
 23799/100000: episode: 445, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -18.762, mean reward: -0.188 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.556, 10.150], loss: 0.017519, mae: 0.075134, mean_q: 0.056916
 23899/100000: episode: 446, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.279, mean reward: -0.163 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.599, 10.098], loss: 0.030759, mae: 0.084042, mean_q: 0.064170
 23999/100000: episode: 447, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -13.938, mean reward: -0.139 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.837, 10.436], loss: 0.003948, mae: 0.064695, mean_q: 0.049740
 24099/100000: episode: 448, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.840, mean reward: -0.188 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.492, 10.098], loss: 0.003839, mae: 0.064124, mean_q: 0.036895
 24199/100000: episode: 449, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.035, mean reward: -0.160 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.344, 10.098], loss: 0.016534, mae: 0.067709, mean_q: 0.015675
 24299/100000: episode: 450, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.703, mean reward: -0.187 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.230, 10.098], loss: 0.029991, mae: 0.077998, mean_q: -0.002979
 24399/100000: episode: 451, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.332, mean reward: -0.193 [-1.000, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.009, 10.340], loss: 0.017487, mae: 0.074935, mean_q: -0.042599
 24499/100000: episode: 452, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -17.873, mean reward: -0.179 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.810, 10.098], loss: 0.029748, mae: 0.078642, mean_q: -0.075776
 24599/100000: episode: 453, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -12.830, mean reward: -0.128 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.279, 10.319], loss: 0.003408, mae: 0.060584, mean_q: -0.071858
 24699/100000: episode: 454, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.289, mean reward: -0.193 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.444, 10.144], loss: 0.016854, mae: 0.069611, mean_q: -0.142569
 24799/100000: episode: 455, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -11.124, mean reward: -0.111 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.382, 10.247], loss: 0.003621, mae: 0.061657, mean_q: -0.156508
 24899/100000: episode: 456, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.140, mean reward: -0.171 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.232, 10.098], loss: 0.003618, mae: 0.061451, mean_q: -0.151622
 24999/100000: episode: 457, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -17.069, mean reward: -0.171 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.776, 10.185], loss: 0.003196, mae: 0.058755, mean_q: -0.193642
 25099/100000: episode: 458, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -12.357, mean reward: -0.124 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.548, 10.098], loss: 0.016802, mae: 0.069647, mean_q: -0.227613
 25199/100000: episode: 459, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.870, mean reward: -0.189 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.683, 10.218], loss: 0.003918, mae: 0.064481, mean_q: -0.223421
 25299/100000: episode: 460, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.110, mean reward: -0.181 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.287, 10.229], loss: 0.003352, mae: 0.057962, mean_q: -0.226507
 25399/100000: episode: 461, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.990, mean reward: -0.170 [-1.000, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.515, 10.227], loss: 0.016066, mae: 0.062837, mean_q: -0.264740
 25499/100000: episode: 462, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.906, mean reward: -0.169 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.031, 10.130], loss: 0.003256, mae: 0.057830, mean_q: -0.274906
 25599/100000: episode: 463, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.206, mean reward: -0.192 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.562, 10.260], loss: 0.002988, mae: 0.055971, mean_q: -0.322238
 25699/100000: episode: 464, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -13.433, mean reward: -0.134 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.317, 10.385], loss: 0.004382, mae: 0.062413, mean_q: -0.309649
 25799/100000: episode: 465, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.797, mean reward: -0.158 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.423, 10.167], loss: 0.003342, mae: 0.059621, mean_q: -0.299168
 25899/100000: episode: 466, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -18.541, mean reward: -0.185 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.069, 10.138], loss: 0.003263, mae: 0.058103, mean_q: -0.292379
 25999/100000: episode: 467, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.702, mean reward: -0.167 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.947, 10.098], loss: 0.002974, mae: 0.055628, mean_q: -0.314682
 26099/100000: episode: 468, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.274, mean reward: -0.163 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.168, 10.098], loss: 0.002957, mae: 0.055886, mean_q: -0.304706
 26199/100000: episode: 469, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.464, mean reward: -0.185 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.267, 10.098], loss: 0.003302, mae: 0.058413, mean_q: -0.323555
 26299/100000: episode: 470, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -14.502, mean reward: -0.145 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.951, 10.372], loss: 0.002980, mae: 0.056588, mean_q: -0.297088
 26399/100000: episode: 471, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.560, mean reward: -0.146 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.727, 10.098], loss: 0.003228, mae: 0.058491, mean_q: -0.316465
 26499/100000: episode: 472, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -12.792, mean reward: -0.128 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.642, 10.098], loss: 0.003077, mae: 0.056263, mean_q: -0.302275
 26599/100000: episode: 473, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.538, mean reward: -0.175 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.162, 10.098], loss: 0.003014, mae: 0.056848, mean_q: -0.280578
 26699/100000: episode: 474, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.412, mean reward: -0.144 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.260, 10.098], loss: 0.003014, mae: 0.056281, mean_q: -0.299137
 26799/100000: episode: 475, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.619, mean reward: -0.156 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.879, 10.247], loss: 0.002909, mae: 0.056715, mean_q: -0.316754
 26899/100000: episode: 476, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.544, mean reward: -0.175 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.570, 10.219], loss: 0.003421, mae: 0.061041, mean_q: -0.317912
 26999/100000: episode: 477, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.095, mean reward: -0.161 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.466, 10.133], loss: 0.003044, mae: 0.057990, mean_q: -0.271587
 27099/100000: episode: 478, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.332, mean reward: -0.153 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.499, 10.098], loss: 0.002878, mae: 0.054957, mean_q: -0.342495
 27199/100000: episode: 479, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -11.263, mean reward: -0.113 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.854, 10.300], loss: 0.003494, mae: 0.060976, mean_q: -0.290570
 27299/100000: episode: 480, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.180, mean reward: -0.172 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.324, 10.098], loss: 0.003236, mae: 0.059377, mean_q: -0.309182
 27399/100000: episode: 481, duration: 0.623s, episode steps: 100, steps per second: 161, episode reward: -17.675, mean reward: -0.177 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.841, 10.116], loss: 0.002806, mae: 0.054928, mean_q: -0.331125
 27499/100000: episode: 482, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -20.139, mean reward: -0.201 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.204, 10.180], loss: 0.002817, mae: 0.055385, mean_q: -0.290547
 27599/100000: episode: 483, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.521, mean reward: -0.175 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.581, 10.098], loss: 0.002971, mae: 0.056696, mean_q: -0.357823
 27699/100000: episode: 484, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.840, mean reward: -0.168 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.896, 10.249], loss: 0.004139, mae: 0.065342, mean_q: -0.288351
 27799/100000: episode: 485, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.708, mean reward: -0.177 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.185, 10.208], loss: 0.002786, mae: 0.056264, mean_q: -0.303484
 27899/100000: episode: 486, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -16.888, mean reward: -0.169 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.178, 10.098], loss: 0.003047, mae: 0.057935, mean_q: -0.285637
 27999/100000: episode: 487, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.536, mean reward: -0.185 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.244, 10.145], loss: 0.002711, mae: 0.053622, mean_q: -0.310919
 28099/100000: episode: 488, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.629, mean reward: -0.176 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.313, 10.137], loss: 0.004114, mae: 0.062533, mean_q: -0.318911
 28199/100000: episode: 489, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.303, mean reward: -0.163 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.301, 10.127], loss: 0.002715, mae: 0.055595, mean_q: -0.299922
 28299/100000: episode: 490, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -11.652, mean reward: -0.117 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.407, 10.098], loss: 0.002563, mae: 0.053264, mean_q: -0.318902
 28399/100000: episode: 491, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -13.402, mean reward: -0.134 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.299, 10.098], loss: 0.002630, mae: 0.053870, mean_q: -0.313366
 28499/100000: episode: 492, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.471, mean reward: -0.185 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.553, 10.170], loss: 0.002700, mae: 0.055327, mean_q: -0.286554
 28599/100000: episode: 493, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -19.639, mean reward: -0.196 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.135, 10.171], loss: 0.002619, mae: 0.052224, mean_q: -0.318008
 28699/100000: episode: 494, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.504, mean reward: -0.165 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.716, 10.098], loss: 0.002771, mae: 0.054256, mean_q: -0.300103
 28799/100000: episode: 495, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.887, mean reward: -0.179 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.791, 10.098], loss: 0.002704, mae: 0.054251, mean_q: -0.326455
 28899/100000: episode: 496, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.874, mean reward: -0.189 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.892, 10.247], loss: 0.002894, mae: 0.055913, mean_q: -0.318178
 28999/100000: episode: 497, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.962, mean reward: -0.170 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.300, 10.221], loss: 0.002912, mae: 0.055599, mean_q: -0.327185
 29099/100000: episode: 498, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -17.949, mean reward: -0.179 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.296, 10.136], loss: 0.002638, mae: 0.052819, mean_q: -0.293718
 29199/100000: episode: 499, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.864, mean reward: -0.179 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.282, 10.098], loss: 0.002528, mae: 0.051743, mean_q: -0.322111
 29299/100000: episode: 500, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.267, mean reward: -0.193 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.249, 10.199], loss: 0.002474, mae: 0.051584, mean_q: -0.315005
 29399/100000: episode: 501, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -14.550, mean reward: -0.145 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.667, 10.202], loss: 0.002703, mae: 0.053683, mean_q: -0.296969
 29499/100000: episode: 502, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -20.824, mean reward: -0.208 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.189, 10.130], loss: 0.002861, mae: 0.057393, mean_q: -0.328503
 29599/100000: episode: 503, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.758, mean reward: -0.188 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.266, 10.127], loss: 0.002636, mae: 0.053531, mean_q: -0.321285
 29699/100000: episode: 504, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.584, mean reward: -0.176 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.576, 10.252], loss: 0.004662, mae: 0.070234, mean_q: -0.326755
 29799/100000: episode: 505, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.519, mean reward: -0.125 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.144, 10.098], loss: 0.002823, mae: 0.056355, mean_q: -0.351759
 29899/100000: episode: 506, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -17.801, mean reward: -0.178 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.871, 10.295], loss: 0.002617, mae: 0.052465, mean_q: -0.318676
 29999/100000: episode: 507, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.880, mean reward: -0.179 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.427, 10.415], loss: 0.002700, mae: 0.053451, mean_q: -0.314218
 30099/100000: episode: 508, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -10.893, mean reward: -0.109 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.616, 10.249], loss: 0.002826, mae: 0.055641, mean_q: -0.311249
 30199/100000: episode: 509, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.424, mean reward: -0.184 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.747, 10.098], loss: 0.002892, mae: 0.056560, mean_q: -0.311593
 30299/100000: episode: 510, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.289, mean reward: -0.173 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.742, 10.152], loss: 0.003278, mae: 0.061411, mean_q: -0.294075
 30399/100000: episode: 511, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.764, mean reward: -0.168 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.725, 10.143], loss: 0.002466, mae: 0.051361, mean_q: -0.325698
 30499/100000: episode: 512, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.128, mean reward: -0.191 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.195, 10.159], loss: 0.003418, mae: 0.060989, mean_q: -0.329286
[Info] 100-TH LEVEL FOUND: 0.6084758043289185, Considering 10/90 traces
 30599/100000: episode: 513, duration: 4.589s, episode steps: 100, steps per second: 22, episode reward: -17.730, mean reward: -0.177 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.257, 10.157], loss: 0.003013, mae: 0.057578, mean_q: -0.301736
 30625/100000: episode: 514, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 6.574, mean reward: 0.253 [0.133, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.792, 10.100], loss: 0.003860, mae: 0.066306, mean_q: -0.290512
 30660/100000: episode: 515, duration: 0.184s, episode steps: 35, steps per second: 190, episode reward: 15.994, mean reward: 0.457 [0.331, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.850, 10.590], loss: 0.002744, mae: 0.054816, mean_q: -0.308233
 30694/100000: episode: 516, duration: 0.181s, episode steps: 34, steps per second: 187, episode reward: 6.555, mean reward: 0.193 [0.057, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.827, 10.185], loss: 0.002942, mae: 0.057644, mean_q: -0.294616
 30716/100000: episode: 517, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 6.697, mean reward: 0.304 [0.177, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.431, 10.431], loss: 0.003114, mae: 0.060641, mean_q: -0.294021
 30751/100000: episode: 518, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 11.210, mean reward: 0.320 [0.179, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-1.566, 10.428], loss: 0.002758, mae: 0.056475, mean_q: -0.250878
 30773/100000: episode: 519, duration: 0.132s, episode steps: 22, steps per second: 167, episode reward: 5.703, mean reward: 0.259 [0.151, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.283], loss: 0.002850, mae: 0.057222, mean_q: -0.288382
 30808/100000: episode: 520, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 10.068, mean reward: 0.288 [0.089, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.586, 10.202], loss: 0.002353, mae: 0.049758, mean_q: -0.258140
 30843/100000: episode: 521, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 8.464, mean reward: 0.242 [0.156, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.998, 10.387], loss: 0.002478, mae: 0.051911, mean_q: -0.238043
 30870/100000: episode: 522, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 7.551, mean reward: 0.280 [0.153, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.092, 10.311], loss: 0.002328, mae: 0.049773, mean_q: -0.316694
 30905/100000: episode: 523, duration: 0.192s, episode steps: 35, steps per second: 183, episode reward: 6.137, mean reward: 0.175 [0.007, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.469, 10.119], loss: 0.002569, mae: 0.052230, mean_q: -0.273973
 30940/100000: episode: 524, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 12.018, mean reward: 0.343 [0.266, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.094, 10.404], loss: 0.002756, mae: 0.057099, mean_q: -0.210771
 30989/100000: episode: 525, duration: 0.266s, episode steps: 49, steps per second: 184, episode reward: 16.257, mean reward: 0.332 [0.190, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.345, 10.447], loss: 0.002552, mae: 0.053730, mean_q: -0.202423
 31024/100000: episode: 526, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 8.463, mean reward: 0.242 [0.115, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.843, 10.113], loss: 0.002453, mae: 0.050353, mean_q: -0.254879
 31059/100000: episode: 527, duration: 0.182s, episode steps: 35, steps per second: 193, episode reward: 9.315, mean reward: 0.266 [0.173, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.080, 10.388], loss: 0.002280, mae: 0.049658, mean_q: -0.223261
 31104/100000: episode: 528, duration: 0.255s, episode steps: 45, steps per second: 177, episode reward: 13.664, mean reward: 0.304 [0.071, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.822, 10.277], loss: 0.002558, mae: 0.052031, mean_q: -0.234335
 31135/100000: episode: 529, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 7.903, mean reward: 0.255 [0.051, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.577, 10.149], loss: 0.002883, mae: 0.058328, mean_q: -0.195188
 31170/100000: episode: 530, duration: 0.218s, episode steps: 35, steps per second: 161, episode reward: 8.785, mean reward: 0.251 [0.090, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.861, 10.201], loss: 0.002815, mae: 0.054793, mean_q: -0.234382
 31205/100000: episode: 531, duration: 0.197s, episode steps: 35, steps per second: 178, episode reward: 11.791, mean reward: 0.337 [0.194, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.035, 10.425], loss: 0.002262, mae: 0.050327, mean_q: -0.187737
 31240/100000: episode: 532, duration: 0.196s, episode steps: 35, steps per second: 178, episode reward: 6.561, mean reward: 0.187 [0.035, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.174, 10.273], loss: 0.002681, mae: 0.053071, mean_q: -0.218927
 31273/100000: episode: 533, duration: 0.200s, episode steps: 33, steps per second: 165, episode reward: 12.019, mean reward: 0.364 [0.228, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.359, 10.369], loss: 0.002446, mae: 0.051499, mean_q: -0.209069
 31295/100000: episode: 534, duration: 0.133s, episode steps: 22, steps per second: 165, episode reward: 5.438, mean reward: 0.247 [0.175, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.294], loss: 0.002261, mae: 0.049068, mean_q: -0.216606
 31330/100000: episode: 535, duration: 0.214s, episode steps: 35, steps per second: 163, episode reward: 11.126, mean reward: 0.318 [0.239, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.035, 10.382], loss: 0.002489, mae: 0.053327, mean_q: -0.164181
 31357/100000: episode: 536, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 12.012, mean reward: 0.445 [0.270, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.843, 10.644], loss: 0.002819, mae: 0.055587, mean_q: -0.169864
 31391/100000: episode: 537, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 7.897, mean reward: 0.232 [0.128, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.396, 10.212], loss: 0.002581, mae: 0.052203, mean_q: -0.177337
 31426/100000: episode: 538, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 12.161, mean reward: 0.347 [0.226, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.233, 10.576], loss: 0.002994, mae: 0.056590, mean_q: -0.145350
 31471/100000: episode: 539, duration: 0.252s, episode steps: 45, steps per second: 179, episode reward: 14.596, mean reward: 0.324 [0.184, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.216, 10.384], loss: 0.003776, mae: 0.059435, mean_q: -0.160065
 31520/100000: episode: 540, duration: 0.273s, episode steps: 49, steps per second: 180, episode reward: 12.538, mean reward: 0.256 [0.016, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.358, 10.178], loss: 0.002435, mae: 0.051518, mean_q: -0.144091
 31569/100000: episode: 541, duration: 0.279s, episode steps: 49, steps per second: 176, episode reward: 10.183, mean reward: 0.208 [0.070, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.347, 10.426], loss: 0.002682, mae: 0.054221, mean_q: -0.101271
 31591/100000: episode: 542, duration: 0.142s, episode steps: 22, steps per second: 155, episode reward: 4.847, mean reward: 0.220 [0.081, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.358, 10.218], loss: 0.002432, mae: 0.051108, mean_q: -0.164176
 31640/100000: episode: 543, duration: 0.271s, episode steps: 49, steps per second: 181, episode reward: 12.874, mean reward: 0.263 [0.035, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.356, 10.100], loss: 0.005913, mae: 0.074194, mean_q: -0.190218
 31662/100000: episode: 544, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 6.879, mean reward: 0.313 [0.135, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.259], loss: 0.006744, mae: 0.075744, mean_q: -0.100508
 31693/100000: episode: 545, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 6.421, mean reward: 0.207 [0.057, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.035, 10.202], loss: 0.005915, mae: 0.072832, mean_q: -0.147626
 31719/100000: episode: 546, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 5.076, mean reward: 0.195 [0.055, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.279, 10.100], loss: 0.003970, mae: 0.062429, mean_q: -0.152698
 31753/100000: episode: 547, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 6.124, mean reward: 0.180 [0.069, 0.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.567, 10.289], loss: 0.005235, mae: 0.072327, mean_q: -0.115872
 31779/100000: episode: 548, duration: 0.159s, episode steps: 26, steps per second: 164, episode reward: 8.668, mean reward: 0.333 [0.204, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.253, 10.100], loss: 0.003045, mae: 0.061902, mean_q: -0.149142
 31828/100000: episode: 549, duration: 0.277s, episode steps: 49, steps per second: 177, episode reward: 20.054, mean reward: 0.409 [0.247, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.811, 10.491], loss: 0.002963, mae: 0.058506, mean_q: -0.090720
 31850/100000: episode: 550, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 8.256, mean reward: 0.375 [0.273, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.368, 10.442], loss: 0.002812, mae: 0.058594, mean_q: -0.080465
 31899/100000: episode: 551, duration: 0.262s, episode steps: 49, steps per second: 187, episode reward: 18.217, mean reward: 0.372 [0.249, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.371, 10.594], loss: 0.002466, mae: 0.052416, mean_q: -0.118933
 31932/100000: episode: 552, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 10.569, mean reward: 0.320 [0.085, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.035, 10.285], loss: 0.002854, mae: 0.056349, mean_q: -0.069834
 31954/100000: episode: 553, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 8.159, mean reward: 0.371 [0.275, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.900, 10.473], loss: 0.002590, mae: 0.054230, mean_q: -0.088621
 31999/100000: episode: 554, duration: 0.237s, episode steps: 45, steps per second: 190, episode reward: 12.360, mean reward: 0.275 [0.030, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.328, 10.100], loss: 0.002431, mae: 0.051927, mean_q: -0.033618
 32044/100000: episode: 555, duration: 0.240s, episode steps: 45, steps per second: 187, episode reward: 15.844, mean reward: 0.352 [0.220, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.225, 10.327], loss: 0.002313, mae: 0.050501, mean_q: -0.086519
 32071/100000: episode: 556, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 6.131, mean reward: 0.227 [0.157, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.152, 10.310], loss: 0.002679, mae: 0.054604, mean_q: -0.003620
 32097/100000: episode: 557, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 7.850, mean reward: 0.302 [0.223, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.492, 10.100], loss: 0.002455, mae: 0.051261, mean_q: -0.046847
 32123/100000: episode: 558, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 7.760, mean reward: 0.298 [0.183, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.054, 10.100], loss: 0.002501, mae: 0.053384, mean_q: -0.070821
 32149/100000: episode: 559, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 4.147, mean reward: 0.160 [0.009, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.226, 10.202], loss: 0.002552, mae: 0.052714, mean_q: -0.057857
 32182/100000: episode: 560, duration: 0.192s, episode steps: 33, steps per second: 172, episode reward: 11.142, mean reward: 0.338 [0.219, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.641, 10.449], loss: 0.002549, mae: 0.052386, mean_q: -0.089507
 32213/100000: episode: 561, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 8.900, mean reward: 0.287 [0.162, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.378, 10.471], loss: 0.002752, mae: 0.053879, mean_q: -0.003370
 32248/100000: episode: 562, duration: 0.182s, episode steps: 35, steps per second: 193, episode reward: 8.130, mean reward: 0.232 [0.063, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.775, 10.100], loss: 0.002728, mae: 0.055172, mean_q: 0.029736
 32275/100000: episode: 563, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 9.681, mean reward: 0.359 [0.235, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.370, 10.479], loss: 0.002718, mae: 0.055526, mean_q: -0.092167
 32301/100000: episode: 564, duration: 0.186s, episode steps: 26, steps per second: 140, episode reward: 8.149, mean reward: 0.313 [0.219, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.161, 10.100], loss: 0.002749, mae: 0.055751, mean_q: -0.011942
 32336/100000: episode: 565, duration: 0.219s, episode steps: 35, steps per second: 160, episode reward: 9.546, mean reward: 0.273 [0.160, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.035, 10.306], loss: 0.002734, mae: 0.055594, mean_q: 0.006076
 32371/100000: episode: 566, duration: 0.196s, episode steps: 35, steps per second: 179, episode reward: 10.264, mean reward: 0.293 [0.191, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.335, 10.304], loss: 0.002611, mae: 0.054248, mean_q: -0.007858
 32402/100000: episode: 567, duration: 0.185s, episode steps: 31, steps per second: 168, episode reward: 8.424, mean reward: 0.272 [0.053, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.035, 10.151], loss: 0.002912, mae: 0.056377, mean_q: -0.025975
 32451/100000: episode: 568, duration: 0.271s, episode steps: 49, steps per second: 181, episode reward: 10.267, mean reward: 0.210 [0.090, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.688, 10.345], loss: 0.002797, mae: 0.056106, mean_q: 0.033629
 32486/100000: episode: 569, duration: 0.200s, episode steps: 35, steps per second: 175, episode reward: 8.041, mean reward: 0.230 [0.086, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.642, 10.207], loss: 0.002982, mae: 0.057414, mean_q: 0.009296
 32513/100000: episode: 570, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 9.495, mean reward: 0.352 [0.289, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.447], loss: 0.002680, mae: 0.056868, mean_q: 0.012278
 32548/100000: episode: 571, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 7.958, mean reward: 0.227 [0.039, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.094, 10.114], loss: 0.002574, mae: 0.053721, mean_q: 0.016645
 32582/100000: episode: 572, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 11.081, mean reward: 0.326 [0.222, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-1.205, 10.405], loss: 0.002777, mae: 0.056388, mean_q: 0.031438
 32617/100000: episode: 573, duration: 0.197s, episode steps: 35, steps per second: 178, episode reward: 12.136, mean reward: 0.347 [0.252, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.877, 10.370], loss: 0.002819, mae: 0.055585, mean_q: 0.072187
 32648/100000: episode: 574, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 8.540, mean reward: 0.275 [0.128, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.595, 10.222], loss: 0.004331, mae: 0.065005, mean_q: 0.077291
 32683/100000: episode: 575, duration: 0.203s, episode steps: 35, steps per second: 172, episode reward: 9.221, mean reward: 0.263 [0.020, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-1.269, 10.100], loss: 0.005226, mae: 0.070581, mean_q: 0.050728
 32728/100000: episode: 576, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 13.941, mean reward: 0.310 [0.161, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.488, 10.272], loss: 0.003364, mae: 0.065204, mean_q: 0.101230
 32777/100000: episode: 577, duration: 0.274s, episode steps: 49, steps per second: 179, episode reward: 13.280, mean reward: 0.271 [0.099, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.897, 10.390], loss: 0.002686, mae: 0.055929, mean_q: 0.054830
 32804/100000: episode: 578, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 9.039, mean reward: 0.335 [0.210, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-1.012, 10.482], loss: 0.003191, mae: 0.057996, mean_q: 0.057058
 32839/100000: episode: 579, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 7.548, mean reward: 0.216 [0.040, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.798, 10.171], loss: 0.002789, mae: 0.057754, mean_q: 0.080051
 32873/100000: episode: 580, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 8.776, mean reward: 0.258 [0.054, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.970, 10.160], loss: 0.002751, mae: 0.056652, mean_q: 0.094715
 32904/100000: episode: 581, duration: 0.174s, episode steps: 31, steps per second: 179, episode reward: 10.802, mean reward: 0.348 [0.220, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-1.256, 10.413], loss: 0.002721, mae: 0.055637, mean_q: 0.067189
 32937/100000: episode: 582, duration: 0.184s, episode steps: 33, steps per second: 180, episode reward: 10.924, mean reward: 0.331 [0.252, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.035, 10.454], loss: 0.003005, mae: 0.059058, mean_q: 0.103872
 32986/100000: episode: 583, duration: 0.280s, episode steps: 49, steps per second: 175, episode reward: 10.315, mean reward: 0.211 [0.038, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.633, 10.134], loss: 0.003192, mae: 0.060950, mean_q: 0.097272
 33035/100000: episode: 584, duration: 0.322s, episode steps: 49, steps per second: 152, episode reward: 12.811, mean reward: 0.261 [0.133, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.375, 10.449], loss: 0.002633, mae: 0.055697, mean_q: 0.112412
 33084/100000: episode: 585, duration: 0.268s, episode steps: 49, steps per second: 183, episode reward: 13.682, mean reward: 0.279 [0.153, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-1.455, 10.330], loss: 0.002931, mae: 0.058518, mean_q: 0.137599
 33133/100000: episode: 586, duration: 0.270s, episode steps: 49, steps per second: 182, episode reward: 13.428, mean reward: 0.274 [0.119, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.359, 10.185], loss: 0.002684, mae: 0.056000, mean_q: 0.128707
 33167/100000: episode: 587, duration: 0.198s, episode steps: 34, steps per second: 172, episode reward: 10.542, mean reward: 0.310 [0.226, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.206, 10.362], loss: 0.002645, mae: 0.055468, mean_q: 0.140837
 33200/100000: episode: 588, duration: 0.176s, episode steps: 33, steps per second: 187, episode reward: 13.535, mean reward: 0.410 [0.246, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.035, 10.420], loss: 0.002663, mae: 0.055254, mean_q: 0.147130
 33234/100000: episode: 589, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 10.319, mean reward: 0.303 [0.137, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.558, 10.224], loss: 0.003040, mae: 0.059241, mean_q: 0.128011
 33267/100000: episode: 590, duration: 0.194s, episode steps: 33, steps per second: 170, episode reward: 10.046, mean reward: 0.304 [0.198, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.724, 10.495], loss: 0.002539, mae: 0.054726, mean_q: 0.131556
 33301/100000: episode: 591, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 8.884, mean reward: 0.261 [0.025, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-1.171, 10.100], loss: 0.002610, mae: 0.054495, mean_q: 0.170753
 33346/100000: episode: 592, duration: 0.243s, episode steps: 45, steps per second: 186, episode reward: 12.646, mean reward: 0.281 [0.066, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.807, 10.177], loss: 0.002839, mae: 0.057874, mean_q: 0.148546
 33373/100000: episode: 593, duration: 0.160s, episode steps: 27, steps per second: 168, episode reward: 13.177, mean reward: 0.488 [0.273, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.674, 10.669], loss: 0.002252, mae: 0.051032, mean_q: 0.134374
 33418/100000: episode: 594, duration: 0.270s, episode steps: 45, steps per second: 167, episode reward: 14.464, mean reward: 0.321 [0.184, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.970, 10.325], loss: 0.002996, mae: 0.059298, mean_q: 0.196178
 33449/100000: episode: 595, duration: 0.180s, episode steps: 31, steps per second: 172, episode reward: 5.576, mean reward: 0.180 [0.020, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.035, 10.100], loss: 0.002849, mae: 0.056718, mean_q: 0.160140
 33482/100000: episode: 596, duration: 0.177s, episode steps: 33, steps per second: 187, episode reward: 11.990, mean reward: 0.363 [0.272, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.392, 10.525], loss: 0.002786, mae: 0.056461, mean_q: 0.179224
 33513/100000: episode: 597, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 8.619, mean reward: 0.278 [0.164, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.121, 10.361], loss: 0.002648, mae: 0.055469, mean_q: 0.213535
 33544/100000: episode: 598, duration: 0.184s, episode steps: 31, steps per second: 169, episode reward: 8.237, mean reward: 0.266 [0.113, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.719, 10.276], loss: 0.002626, mae: 0.055751, mean_q: 0.206017
 33578/100000: episode: 599, duration: 0.189s, episode steps: 34, steps per second: 180, episode reward: 10.925, mean reward: 0.321 [0.140, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.926, 10.287], loss: 0.003092, mae: 0.060880, mean_q: 0.271166
 33613/100000: episode: 600, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 11.364, mean reward: 0.325 [0.175, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.035, 10.340], loss: 0.002809, mae: 0.057281, mean_q: 0.172451
 33640/100000: episode: 601, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 9.753, mean reward: 0.361 [0.272, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.798, 10.378], loss: 0.002582, mae: 0.054473, mean_q: 0.257424
 33675/100000: episode: 602, duration: 0.197s, episode steps: 35, steps per second: 178, episode reward: 6.726, mean reward: 0.192 [0.049, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.102, 10.300], loss: 0.002498, mae: 0.054304, mean_q: 0.258562
[Info] 200-TH LEVEL FOUND: 0.8055233955383301, Considering 10/90 traces
 33697/100000: episode: 603, duration: 4.195s, episode steps: 22, steps per second: 5, episode reward: 4.949, mean reward: 0.225 [0.096, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.117, 10.301], loss: 0.002515, mae: 0.053682, mean_q: 0.183701
 33740/100000: episode: 604, duration: 0.225s, episode steps: 43, steps per second: 192, episode reward: 18.770, mean reward: 0.437 [0.275, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.288, 10.404], loss: 0.002776, mae: 0.055842, mean_q: 0.238569
 33754/100000: episode: 605, duration: 0.101s, episode steps: 14, steps per second: 139, episode reward: 5.800, mean reward: 0.414 [0.365, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.777, 10.561], loss: 0.004066, mae: 0.068881, mean_q: 0.205450
 33777/100000: episode: 606, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 8.506, mean reward: 0.370 [0.304, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.611, 10.460], loss: 0.006823, mae: 0.074107, mean_q: 0.229411
 33820/100000: episode: 607, duration: 0.232s, episode steps: 43, steps per second: 185, episode reward: 14.846, mean reward: 0.345 [0.150, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-1.426, 10.280], loss: 0.004909, mae: 0.069988, mean_q: 0.272861
 33843/100000: episode: 608, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 9.796, mean reward: 0.426 [0.323, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.161, 10.558], loss: 0.003449, mae: 0.056713, mean_q: 0.280073
 33865/100000: episode: 609, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 6.212, mean reward: 0.282 [0.178, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.408, 10.325], loss: 0.002729, mae: 0.055998, mean_q: 0.252901
 33879/100000: episode: 610, duration: 0.091s, episode steps: 14, steps per second: 153, episode reward: 6.616, mean reward: 0.473 [0.434, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.527], loss: 0.003116, mae: 0.060846, mean_q: 0.328341
 33901/100000: episode: 611, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 7.443, mean reward: 0.338 [0.159, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.472, 10.304], loss: 0.002686, mae: 0.056974, mean_q: 0.308000
 33942/100000: episode: 612, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 11.670, mean reward: 0.285 [0.059, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.421, 10.148], loss: 0.002572, mae: 0.054994, mean_q: 0.310597
 33983/100000: episode: 613, duration: 0.233s, episode steps: 41, steps per second: 176, episode reward: 11.753, mean reward: 0.287 [0.018, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.356, 10.154], loss: 0.002945, mae: 0.058595, mean_q: 0.306322
 34024/100000: episode: 614, duration: 0.226s, episode steps: 41, steps per second: 182, episode reward: 11.736, mean reward: 0.286 [0.179, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.700, 10.285], loss: 0.003301, mae: 0.060586, mean_q: 0.312825
 34040/100000: episode: 615, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 5.188, mean reward: 0.324 [0.204, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.518, 10.352], loss: 0.002487, mae: 0.054348, mean_q: 0.293252
 34081/100000: episode: 616, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 11.696, mean reward: 0.285 [0.009, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.184, 10.110], loss: 0.003074, mae: 0.059254, mean_q: 0.291474
 34122/100000: episode: 617, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 14.065, mean reward: 0.343 [0.210, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.139, 10.347], loss: 0.002807, mae: 0.058342, mean_q: 0.312977
 34145/100000: episode: 618, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 9.161, mean reward: 0.398 [0.228, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.228, 10.552], loss: 0.002932, mae: 0.059041, mean_q: 0.358449
 34168/100000: episode: 619, duration: 0.125s, episode steps: 23, steps per second: 183, episode reward: 8.522, mean reward: 0.371 [0.293, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.475], loss: 0.002928, mae: 0.058581, mean_q: 0.306597
 34211/100000: episode: 620, duration: 0.239s, episode steps: 43, steps per second: 180, episode reward: 16.148, mean reward: 0.376 [0.235, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.285, 10.315], loss: 0.002749, mae: 0.056303, mean_q: 0.319924
 34227/100000: episode: 621, duration: 0.100s, episode steps: 16, steps per second: 159, episode reward: 7.682, mean reward: 0.480 [0.369, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.564, 10.532], loss: 0.002915, mae: 0.057632, mean_q: 0.348754
 34268/100000: episode: 622, duration: 0.247s, episode steps: 41, steps per second: 166, episode reward: 15.050, mean reward: 0.367 [0.215, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.176, 10.371], loss: 0.002900, mae: 0.059488, mean_q: 0.348090
 34284/100000: episode: 623, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 7.231, mean reward: 0.452 [0.308, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.219, 10.514], loss: 0.002584, mae: 0.054031, mean_q: 0.316290
 34301/100000: episode: 624, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 6.739, mean reward: 0.396 [0.336, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.497, 10.463], loss: 0.002752, mae: 0.057175, mean_q: 0.343767
 34324/100000: episode: 625, duration: 0.135s, episode steps: 23, steps per second: 170, episode reward: 10.753, mean reward: 0.468 [0.310, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.087, 10.627], loss: 0.002951, mae: 0.058876, mean_q: 0.353103
 34338/100000: episode: 626, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 5.788, mean reward: 0.413 [0.329, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.472], loss: 0.003086, mae: 0.061365, mean_q: 0.363700
[Info] FALSIFICATION!
 34341/100000: episode: 627, duration: 0.023s, episode steps: 3, steps per second: 133, episode reward: 11.084, mean reward: 3.695 [0.498, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-1.019, 9.760], loss: 0.003008, mae: 0.061247, mean_q: 0.453550
 34441/100000: episode: 628, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.630, mean reward: -0.196 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.836, 10.247], loss: 0.045419, mae: 0.090775, mean_q: 0.380474
 34541/100000: episode: 629, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.367, mean reward: -0.164 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.767, 10.352], loss: 0.044678, mae: 0.085878, mean_q: 0.383946
 34641/100000: episode: 630, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -13.618, mean reward: -0.136 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.634, 10.098], loss: 0.004221, mae: 0.064130, mean_q: 0.369544
 34741/100000: episode: 631, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.170, mean reward: -0.172 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.009, 10.098], loss: 0.030737, mae: 0.076356, mean_q: 0.414196
 34841/100000: episode: 632, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.625, mean reward: -0.166 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.537, 10.169], loss: 0.002679, mae: 0.056898, mean_q: 0.361966
 34941/100000: episode: 633, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.681, mean reward: -0.147 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.932, 10.098], loss: 0.016633, mae: 0.066992, mean_q: 0.390224
 35041/100000: episode: 634, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.763, mean reward: -0.188 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.616, 10.395], loss: 0.002875, mae: 0.058793, mean_q: 0.381229
 35141/100000: episode: 635, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.236, mean reward: -0.152 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.744, 10.357], loss: 0.002841, mae: 0.058390, mean_q: 0.392597
 35241/100000: episode: 636, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -10.697, mean reward: -0.107 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.495, 10.098], loss: 0.016943, mae: 0.068158, mean_q: 0.378518
 35341/100000: episode: 637, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -10.608, mean reward: -0.106 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.075, 10.098], loss: 0.003165, mae: 0.060808, mean_q: 0.389414
 35441/100000: episode: 638, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.769, mean reward: -0.158 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.193, 10.154], loss: 0.057948, mae: 0.088663, mean_q: 0.395344
 35541/100000: episode: 639, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.790, mean reward: -0.178 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.673, 10.106], loss: 0.003414, mae: 0.063252, mean_q: 0.376673
 35641/100000: episode: 640, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.514, mean reward: -0.175 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.845, 10.170], loss: 0.016894, mae: 0.068184, mean_q: 0.347977
 35741/100000: episode: 641, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.238, mean reward: -0.152 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.142, 10.231], loss: 0.002803, mae: 0.057680, mean_q: 0.359212
 35841/100000: episode: 642, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -11.245, mean reward: -0.112 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.524, 10.170], loss: 0.002640, mae: 0.055287, mean_q: 0.330310
 35941/100000: episode: 643, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -14.763, mean reward: -0.148 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.496, 10.310], loss: 0.002784, mae: 0.057598, mean_q: 0.301335
 36041/100000: episode: 644, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.659, mean reward: -0.187 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.062, 10.116], loss: 0.002766, mae: 0.057008, mean_q: 0.276697
 36141/100000: episode: 645, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.170, mean reward: -0.182 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.650, 10.316], loss: 0.002882, mae: 0.056828, mean_q: 0.287157
 36241/100000: episode: 646, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -20.992, mean reward: -0.210 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.612, 10.098], loss: 0.002670, mae: 0.055913, mean_q: 0.208290
 36341/100000: episode: 647, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -14.513, mean reward: -0.145 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.198, 10.166], loss: 0.017361, mae: 0.070144, mean_q: 0.213394
 36441/100000: episode: 648, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -13.735, mean reward: -0.137 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.508, 10.098], loss: 0.030724, mae: 0.075687, mean_q: 0.216236
 36541/100000: episode: 649, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -20.017, mean reward: -0.200 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.098, 10.098], loss: 0.016386, mae: 0.064128, mean_q: 0.181283
 36641/100000: episode: 650, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.308, mean reward: -0.173 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.648, 10.400], loss: 0.002779, mae: 0.057160, mean_q: 0.197853
 36741/100000: episode: 651, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -15.382, mean reward: -0.154 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.966, 10.098], loss: 0.016621, mae: 0.065477, mean_q: 0.194573
 36841/100000: episode: 652, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.851, mean reward: -0.169 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.664, 10.231], loss: 0.002869, mae: 0.056683, mean_q: 0.151885
 36941/100000: episode: 653, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -7.826, mean reward: -0.078 [-1.000, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.457, 10.098], loss: 0.030627, mae: 0.071826, mean_q: 0.124759
 37041/100000: episode: 654, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -6.857, mean reward: -0.069 [-1.000, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.258, 10.098], loss: 0.002909, mae: 0.057210, mean_q: 0.106510
 37141/100000: episode: 655, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.449, mean reward: -0.184 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.613, 10.276], loss: 0.016498, mae: 0.065087, mean_q: 0.107317
 37241/100000: episode: 656, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.216, mean reward: -0.142 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.745, 10.211], loss: 0.002657, mae: 0.055167, mean_q: 0.053227
 37341/100000: episode: 657, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.705, mean reward: -0.177 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.234, 10.158], loss: 0.003530, mae: 0.061004, mean_q: 0.043843
 37441/100000: episode: 658, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.200, mean reward: -0.162 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.204, 10.417], loss: 0.003748, mae: 0.062535, mean_q: 0.064652
 37541/100000: episode: 659, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.206, mean reward: -0.182 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.761, 10.139], loss: 0.002592, mae: 0.053379, mean_q: 0.001730
 37641/100000: episode: 660, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.178, mean reward: -0.152 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.481, 10.331], loss: 0.002924, mae: 0.055780, mean_q: 0.023871
 37741/100000: episode: 661, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -19.058, mean reward: -0.191 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.804, 10.151], loss: 0.018125, mae: 0.074031, mean_q: 0.006647
 37841/100000: episode: 662, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.702, mean reward: -0.197 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.605, 10.201], loss: 0.002975, mae: 0.056817, mean_q: -0.031152
 37941/100000: episode: 663, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.515, mean reward: -0.175 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.414, 10.127], loss: 0.016558, mae: 0.061234, mean_q: -0.032748
 38041/100000: episode: 664, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -16.270, mean reward: -0.163 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.287, 10.123], loss: 0.002804, mae: 0.053865, mean_q: -0.076798
 38141/100000: episode: 665, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -19.180, mean reward: -0.192 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.825, 10.098], loss: 0.017311, mae: 0.067966, mean_q: -0.075344
 38241/100000: episode: 666, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.122, mean reward: -0.181 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.384, 10.098], loss: 0.030020, mae: 0.069889, mean_q: -0.072061
 38341/100000: episode: 667, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -20.068, mean reward: -0.201 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.548, 10.140], loss: 0.003080, mae: 0.057324, mean_q: -0.099104
 38441/100000: episode: 668, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -18.324, mean reward: -0.183 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.825, 10.098], loss: 0.002774, mae: 0.053637, mean_q: -0.130809
 38541/100000: episode: 669, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.758, mean reward: -0.188 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.515, 10.098], loss: 0.029883, mae: 0.067917, mean_q: -0.116324
 38641/100000: episode: 670, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.875, mean reward: -0.189 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.737, 10.098], loss: 0.003078, mae: 0.057748, mean_q: -0.157385
 38741/100000: episode: 671, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.180, mean reward: -0.182 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.087, 10.098], loss: 0.029453, mae: 0.069206, mean_q: -0.204049
 38841/100000: episode: 672, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.483, mean reward: -0.155 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.033, 10.098], loss: 0.002859, mae: 0.054979, mean_q: -0.200949
 38941/100000: episode: 673, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.925, mean reward: -0.159 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.914, 10.250], loss: 0.030272, mae: 0.072979, mean_q: -0.212451
 39041/100000: episode: 674, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.933, mean reward: -0.169 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.045, 10.098], loss: 0.016674, mae: 0.064724, mean_q: -0.232555
 39141/100000: episode: 675, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.400, mean reward: -0.174 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.906, 10.149], loss: 0.003020, mae: 0.057895, mean_q: -0.235823
 39241/100000: episode: 676, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -15.008, mean reward: -0.150 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.956, 10.383], loss: 0.002829, mae: 0.054461, mean_q: -0.300334
 39341/100000: episode: 677, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.938, mean reward: -0.169 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.676, 10.098], loss: 0.002790, mae: 0.053986, mean_q: -0.297346
 39441/100000: episode: 678, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.294, mean reward: -0.163 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.772, 10.334], loss: 0.002594, mae: 0.051654, mean_q: -0.321477
 39541/100000: episode: 679, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -15.180, mean reward: -0.152 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.278, 10.282], loss: 0.003585, mae: 0.061127, mean_q: -0.311263
 39641/100000: episode: 680, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -19.311, mean reward: -0.193 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.668, 10.107], loss: 0.002720, mae: 0.051892, mean_q: -0.315697
 39741/100000: episode: 681, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.572, mean reward: -0.176 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.347, 10.176], loss: 0.002576, mae: 0.050958, mean_q: -0.338445
 39841/100000: episode: 682, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.670, mean reward: -0.157 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.759, 10.154], loss: 0.002586, mae: 0.050543, mean_q: -0.329270
 39941/100000: episode: 683, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -13.893, mean reward: -0.139 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.711, 10.210], loss: 0.002538, mae: 0.050792, mean_q: -0.301513
 40041/100000: episode: 684, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.605, mean reward: -0.186 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.637, 10.098], loss: 0.002539, mae: 0.050357, mean_q: -0.337721
 40141/100000: episode: 685, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.116, mean reward: -0.171 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.728, 10.164], loss: 0.003003, mae: 0.058333, mean_q: -0.318892
 40241/100000: episode: 686, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -19.071, mean reward: -0.191 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.464, 10.098], loss: 0.002607, mae: 0.051506, mean_q: -0.305381
 40341/100000: episode: 687, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.111, mean reward: -0.171 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.925, 10.115], loss: 0.002688, mae: 0.051148, mean_q: -0.314555
 40441/100000: episode: 688, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -18.272, mean reward: -0.183 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.456, 10.223], loss: 0.002748, mae: 0.053031, mean_q: -0.309200
 40541/100000: episode: 689, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -17.358, mean reward: -0.174 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.884, 10.098], loss: 0.002556, mae: 0.050075, mean_q: -0.306940
 40641/100000: episode: 690, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -15.936, mean reward: -0.159 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.148, 10.399], loss: 0.002470, mae: 0.050201, mean_q: -0.314063
 40741/100000: episode: 691, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.814, mean reward: -0.188 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.948, 10.191], loss: 0.002641, mae: 0.051167, mean_q: -0.288184
 40841/100000: episode: 692, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.296, mean reward: -0.193 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.284, 10.119], loss: 0.002569, mae: 0.051648, mean_q: -0.306298
 40941/100000: episode: 693, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.367, mean reward: -0.154 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.286, 10.098], loss: 0.002789, mae: 0.053635, mean_q: -0.323614
 41041/100000: episode: 694, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.542, mean reward: -0.185 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.180, 10.129], loss: 0.002653, mae: 0.052135, mean_q: -0.332617
 41141/100000: episode: 695, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.636, mean reward: -0.186 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.774, 10.135], loss: 0.002606, mae: 0.051156, mean_q: -0.301865
 41241/100000: episode: 696, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -13.224, mean reward: -0.132 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.163, 10.098], loss: 0.002475, mae: 0.049780, mean_q: -0.318665
 41341/100000: episode: 697, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -10.198, mean reward: -0.102 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.523, 10.098], loss: 0.002961, mae: 0.055459, mean_q: -0.305982
 41441/100000: episode: 698, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.190, mean reward: -0.172 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.661, 10.098], loss: 0.002679, mae: 0.054694, mean_q: -0.312852
 41541/100000: episode: 699, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.627, mean reward: -0.156 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.628, 10.106], loss: 0.002451, mae: 0.050412, mean_q: -0.327263
 41641/100000: episode: 700, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.988, mean reward: -0.180 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.554, 10.218], loss: 0.002283, mae: 0.047590, mean_q: -0.332778
 41741/100000: episode: 701, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -18.425, mean reward: -0.184 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.584, 10.098], loss: 0.002489, mae: 0.049280, mean_q: -0.368880
 41841/100000: episode: 702, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -18.071, mean reward: -0.181 [-1.000, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.614, 10.098], loss: 0.002540, mae: 0.050849, mean_q: -0.285535
 41941/100000: episode: 703, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.408, mean reward: -0.154 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.921, 10.132], loss: 0.002348, mae: 0.048232, mean_q: -0.341696
 42041/100000: episode: 704, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -13.647, mean reward: -0.136 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.690, 10.226], loss: 0.002762, mae: 0.053473, mean_q: -0.308628
 42141/100000: episode: 705, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.697, mean reward: -0.167 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.221, 10.322], loss: 0.003220, mae: 0.055387, mean_q: -0.308430
 42241/100000: episode: 706, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.732, mean reward: -0.187 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.456, 10.132], loss: 0.002346, mae: 0.048339, mean_q: -0.332236
 42341/100000: episode: 707, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.144, mean reward: -0.181 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.512, 10.100], loss: 0.002727, mae: 0.051772, mean_q: -0.344831
 42441/100000: episode: 708, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.369, mean reward: -0.154 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.303, 10.098], loss: 0.002330, mae: 0.049411, mean_q: -0.365062
 42541/100000: episode: 709, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -11.886, mean reward: -0.119 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.944, 10.098], loss: 0.002188, mae: 0.047209, mean_q: -0.340350
 42641/100000: episode: 710, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.885, mean reward: -0.179 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.411, 10.098], loss: 0.002258, mae: 0.047660, mean_q: -0.352277
 42741/100000: episode: 711, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.016, mean reward: -0.160 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.360, 10.213], loss: 0.002490, mae: 0.050467, mean_q: -0.287646
 42841/100000: episode: 712, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -14.481, mean reward: -0.145 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.338, 10.098], loss: 0.002442, mae: 0.049408, mean_q: -0.326856
 42941/100000: episode: 713, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.143, mean reward: -0.171 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.884, 10.180], loss: 0.002407, mae: 0.049993, mean_q: -0.283465
 43041/100000: episode: 714, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -14.250, mean reward: -0.143 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.846, 10.098], loss: 0.002383, mae: 0.048784, mean_q: -0.332556
 43141/100000: episode: 715, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.755, mean reward: -0.158 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.695, 10.364], loss: 0.002357, mae: 0.048663, mean_q: -0.321454
 43241/100000: episode: 716, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -7.885, mean reward: -0.079 [-1.000, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.068, 10.595], loss: 0.002268, mae: 0.047903, mean_q: -0.362714
 43341/100000: episode: 717, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -10.423, mean reward: -0.104 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.023, 10.382], loss: 0.002629, mae: 0.052724, mean_q: -0.323252
 43441/100000: episode: 718, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -14.824, mean reward: -0.148 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.493, 10.348], loss: 0.005892, mae: 0.073903, mean_q: -0.303247
 43541/100000: episode: 719, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.022, mean reward: -0.170 [-1.000, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.171, 10.193], loss: 0.003416, mae: 0.060893, mean_q: -0.302394
 43641/100000: episode: 720, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -16.356, mean reward: -0.164 [-1.000, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.569, 10.098], loss: 0.002440, mae: 0.050679, mean_q: -0.297171
 43741/100000: episode: 721, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.651, mean reward: -0.177 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.753, 10.227], loss: 0.002403, mae: 0.049215, mean_q: -0.294743
 43841/100000: episode: 722, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -10.865, mean reward: -0.109 [-1.000, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.883, 10.230], loss: 0.002430, mae: 0.049032, mean_q: -0.292381
 43941/100000: episode: 723, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.310, mean reward: -0.173 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.069, 10.366], loss: 0.002475, mae: 0.050451, mean_q: -0.298885
 44041/100000: episode: 724, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.495, mean reward: -0.185 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.658, 10.134], loss: 0.002629, mae: 0.051684, mean_q: -0.334845
 44141/100000: episode: 725, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.703, mean reward: -0.157 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.365, 10.129], loss: 0.002441, mae: 0.049948, mean_q: -0.262412
 44241/100000: episode: 726, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.995, mean reward: -0.170 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.484, 10.098], loss: 0.002508, mae: 0.050606, mean_q: -0.305576
[Info] 100-TH LEVEL FOUND: 0.650822103023529, Considering 10/90 traces
 44341/100000: episode: 727, duration: 4.546s, episode steps: 100, steps per second: 22, episode reward: -19.554, mean reward: -0.196 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.470, 10.098], loss: 0.002418, mae: 0.049367, mean_q: -0.310975
 44359/100000: episode: 728, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 6.036, mean reward: 0.335 [0.260, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.216, 10.391], loss: 0.002509, mae: 0.048677, mean_q: -0.323063
 44394/100000: episode: 729, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 11.069, mean reward: 0.316 [0.150, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.071, 10.100], loss: 0.002175, mae: 0.045849, mean_q: -0.364694
 44442/100000: episode: 730, duration: 0.256s, episode steps: 48, steps per second: 187, episode reward: 9.811, mean reward: 0.204 [0.055, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.397, 10.183], loss: 0.002592, mae: 0.050719, mean_q: -0.283732
 44476/100000: episode: 731, duration: 0.179s, episode steps: 34, steps per second: 189, episode reward: 11.340, mean reward: 0.334 [0.222, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.570, 10.100], loss: 0.002489, mae: 0.050854, mean_q: -0.242653
 44498/100000: episode: 732, duration: 0.136s, episode steps: 22, steps per second: 162, episode reward: 6.690, mean reward: 0.304 [0.154, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.164, 10.296], loss: 0.002053, mae: 0.045267, mean_q: -0.313415
 44537/100000: episode: 733, duration: 0.205s, episode steps: 39, steps per second: 191, episode reward: 13.036, mean reward: 0.334 [0.165, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.405, 10.100], loss: 0.002376, mae: 0.048912, mean_q: -0.260971
 44571/100000: episode: 734, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 13.195, mean reward: 0.388 [0.262, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.613, 10.100], loss: 0.002327, mae: 0.049297, mean_q: -0.233459
 44593/100000: episode: 735, duration: 0.128s, episode steps: 22, steps per second: 173, episode reward: 6.774, mean reward: 0.308 [0.181, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.278], loss: 0.002531, mae: 0.050018, mean_q: -0.278166
 44618/100000: episode: 736, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 8.743, mean reward: 0.350 [0.201, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.633, 10.379], loss: 0.002706, mae: 0.056803, mean_q: -0.217417
 44636/100000: episode: 737, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 6.045, mean reward: 0.336 [0.280, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.119, 10.485], loss: 0.002541, mae: 0.052272, mean_q: -0.309735
 44654/100000: episode: 738, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 5.379, mean reward: 0.299 [0.162, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.326], loss: 0.002662, mae: 0.054367, mean_q: -0.245652
 44689/100000: episode: 739, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 10.568, mean reward: 0.302 [0.242, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.796, 10.100], loss: 0.002447, mae: 0.050686, mean_q: -0.263585
 44724/100000: episode: 740, duration: 0.184s, episode steps: 35, steps per second: 190, episode reward: 13.075, mean reward: 0.374 [0.224, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.840, 10.100], loss: 0.002329, mae: 0.049213, mean_q: -0.196797
 44736/100000: episode: 741, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 5.159, mean reward: 0.430 [0.340, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.570, 10.485], loss: 0.002639, mae: 0.051755, mean_q: -0.181940
 44758/100000: episode: 742, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 9.348, mean reward: 0.425 [0.289, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.095, 10.388], loss: 0.002390, mae: 0.047664, mean_q: -0.214334
 44794/100000: episode: 743, duration: 0.189s, episode steps: 36, steps per second: 191, episode reward: 10.280, mean reward: 0.286 [0.132, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.296, 10.100], loss: 0.002652, mae: 0.052325, mean_q: -0.229407
 44812/100000: episode: 744, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 5.718, mean reward: 0.318 [0.228, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.708, 10.407], loss: 0.002364, mae: 0.051301, mean_q: -0.233349
 44834/100000: episode: 745, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 6.624, mean reward: 0.301 [0.143, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.515], loss: 0.002452, mae: 0.052248, mean_q: -0.182008
 44882/100000: episode: 746, duration: 0.271s, episode steps: 48, steps per second: 177, episode reward: 13.361, mean reward: 0.278 [0.142, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.799, 10.318], loss: 0.002484, mae: 0.051434, mean_q: -0.190422
 44900/100000: episode: 747, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 5.214, mean reward: 0.290 [0.175, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-1.489, 10.257], loss: 0.002414, mae: 0.050923, mean_q: -0.215884
 44922/100000: episode: 748, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 5.629, mean reward: 0.256 [0.088, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.785, 10.300], loss: 0.002270, mae: 0.048690, mean_q: -0.178239
 44961/100000: episode: 749, duration: 0.223s, episode steps: 39, steps per second: 175, episode reward: 7.648, mean reward: 0.196 [0.012, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-1.922, 10.103], loss: 0.002518, mae: 0.050681, mean_q: -0.219913
 44986/100000: episode: 750, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 7.674, mean reward: 0.307 [0.159, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.696, 10.243], loss: 0.002630, mae: 0.052834, mean_q: -0.190591
 45007/100000: episode: 751, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 5.211, mean reward: 0.248 [0.106, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.797, 10.232], loss: 0.002333, mae: 0.049225, mean_q: -0.230426
 45055/100000: episode: 752, duration: 0.248s, episode steps: 48, steps per second: 193, episode reward: 19.439, mean reward: 0.405 [0.292, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-1.531, 10.470], loss: 0.002455, mae: 0.050607, mean_q: -0.190601
 45080/100000: episode: 753, duration: 0.148s, episode steps: 25, steps per second: 169, episode reward: 6.674, mean reward: 0.267 [0.186, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.380], loss: 0.002860, mae: 0.055744, mean_q: -0.115905
 45092/100000: episode: 754, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 4.962, mean reward: 0.414 [0.358, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.282, 10.451], loss: 0.002631, mae: 0.051016, mean_q: -0.192449
 45114/100000: episode: 755, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 7.399, mean reward: 0.336 [0.242, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.095, 10.418], loss: 0.002470, mae: 0.051597, mean_q: -0.184578
 45136/100000: episode: 756, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 8.191, mean reward: 0.372 [0.282, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.609, 10.483], loss: 0.002471, mae: 0.051000, mean_q: -0.147844
 45171/100000: episode: 757, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 10.162, mean reward: 0.290 [0.152, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.188, 10.100], loss: 0.002580, mae: 0.052255, mean_q: -0.140357
 45189/100000: episode: 758, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 7.046, mean reward: 0.391 [0.267, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.323, 10.666], loss: 0.002473, mae: 0.051324, mean_q: -0.164037
 45228/100000: episode: 759, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 10.621, mean reward: 0.272 [0.130, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.456, 10.100], loss: 0.002614, mae: 0.052072, mean_q: -0.143954
 45246/100000: episode: 760, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 6.294, mean reward: 0.350 [0.291, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.351, 10.407], loss: 0.002483, mae: 0.051103, mean_q: -0.159598
 45280/100000: episode: 761, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 5.945, mean reward: 0.175 [0.011, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.035, 10.331], loss: 0.002417, mae: 0.051579, mean_q: -0.033734
 45305/100000: episode: 762, duration: 0.150s, episode steps: 25, steps per second: 167, episode reward: 6.308, mean reward: 0.252 [0.027, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.225, 10.100], loss: 0.002400, mae: 0.050517, mean_q: -0.141549
 45323/100000: episode: 763, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 4.934, mean reward: 0.274 [0.166, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.169, 10.289], loss: 0.002778, mae: 0.053902, mean_q: -0.119264
 45358/100000: episode: 764, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 8.628, mean reward: 0.247 [0.072, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.741, 10.242], loss: 0.002907, mae: 0.055786, mean_q: -0.126391
 45376/100000: episode: 765, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 8.846, mean reward: 0.491 [0.423, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.510], loss: 0.002982, mae: 0.056116, mean_q: -0.108141
 45424/100000: episode: 766, duration: 0.263s, episode steps: 48, steps per second: 182, episode reward: 13.979, mean reward: 0.291 [0.054, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-1.045, 10.100], loss: 0.002700, mae: 0.053273, mean_q: -0.174367
 45445/100000: episode: 767, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 7.603, mean reward: 0.362 [0.179, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.133, 10.421], loss: 0.002449, mae: 0.050761, mean_q: -0.136181
 45481/100000: episode: 768, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 9.557, mean reward: 0.265 [0.136, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.156, 10.100], loss: 0.002717, mae: 0.054345, mean_q: -0.076506
 45503/100000: episode: 769, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 9.238, mean reward: 0.420 [0.299, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.092, 10.564], loss: 0.002463, mae: 0.051283, mean_q: -0.065181
 45542/100000: episode: 770, duration: 0.216s, episode steps: 39, steps per second: 181, episode reward: 7.718, mean reward: 0.198 [0.045, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.579, 10.196], loss: 0.002495, mae: 0.051474, mean_q: -0.105048
 45563/100000: episode: 771, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 6.782, mean reward: 0.323 [0.207, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.451, 10.305], loss: 0.002641, mae: 0.054234, mean_q: -0.076713
 45584/100000: episode: 772, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 6.170, mean reward: 0.294 [0.191, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.359], loss: 0.002617, mae: 0.052432, mean_q: -0.116023
 45620/100000: episode: 773, duration: 0.206s, episode steps: 36, steps per second: 175, episode reward: 11.046, mean reward: 0.307 [0.225, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.656, 10.100], loss: 0.002616, mae: 0.053229, mean_q: -0.061164
 45642/100000: episode: 774, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 11.238, mean reward: 0.511 [0.344, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.128, 10.526], loss: 0.002488, mae: 0.051414, mean_q: -0.038828
 45664/100000: episode: 775, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 7.860, mean reward: 0.357 [0.261, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.101, 10.346], loss: 0.002704, mae: 0.054574, mean_q: -0.070607
 45712/100000: episode: 776, duration: 0.287s, episode steps: 48, steps per second: 167, episode reward: 10.873, mean reward: 0.227 [0.023, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.689, 10.100], loss: 0.002828, mae: 0.056320, mean_q: 0.003515
 45760/100000: episode: 777, duration: 0.267s, episode steps: 48, steps per second: 180, episode reward: 12.638, mean reward: 0.263 [0.009, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.323, 10.310], loss: 0.002635, mae: 0.053406, mean_q: -0.001100
 45794/100000: episode: 778, duration: 0.204s, episode steps: 34, steps per second: 167, episode reward: 12.466, mean reward: 0.367 [0.216, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.655, 10.100], loss: 0.002621, mae: 0.053123, mean_q: -0.050859
 45816/100000: episode: 779, duration: 0.133s, episode steps: 22, steps per second: 165, episode reward: 7.713, mean reward: 0.351 [0.219, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-1.260, 10.347], loss: 0.002638, mae: 0.053681, mean_q: 0.002699
 45834/100000: episode: 780, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 5.991, mean reward: 0.333 [0.277, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.321, 10.445], loss: 0.002833, mae: 0.055279, mean_q: -0.049252
 45868/100000: episode: 781, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 9.107, mean reward: 0.268 [0.097, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.339, 10.100], loss: 0.003212, mae: 0.060596, mean_q: -0.036048
 45889/100000: episode: 782, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 5.048, mean reward: 0.240 [0.146, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.296], loss: 0.002931, mae: 0.059292, mean_q: -0.031668
 45911/100000: episode: 783, duration: 0.132s, episode steps: 22, steps per second: 167, episode reward: 7.869, mean reward: 0.358 [0.152, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.496, 10.329], loss: 0.003170, mae: 0.061349, mean_q: -0.009697
 45932/100000: episode: 784, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 7.475, mean reward: 0.356 [0.277, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.389], loss: 0.002394, mae: 0.052856, mean_q: 0.010637
 45957/100000: episode: 785, duration: 0.151s, episode steps: 25, steps per second: 166, episode reward: 7.008, mean reward: 0.280 [0.071, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.250, 10.152], loss: 0.002653, mae: 0.054285, mean_q: -0.087325
 46005/100000: episode: 786, duration: 0.268s, episode steps: 48, steps per second: 179, episode reward: 12.783, mean reward: 0.266 [0.044, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.308, 10.100], loss: 0.002745, mae: 0.054414, mean_q: -0.024892
 46023/100000: episode: 787, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 6.217, mean reward: 0.345 [0.182, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.847, 10.435], loss: 0.003229, mae: 0.060243, mean_q: 0.058918
 46048/100000: episode: 788, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 7.487, mean reward: 0.299 [0.187, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.518, 10.433], loss: 0.003163, mae: 0.059144, mean_q: 0.028038
 46066/100000: episode: 789, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 5.974, mean reward: 0.332 [0.240, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.195, 10.405], loss: 0.002573, mae: 0.055714, mean_q: -0.016766
 46102/100000: episode: 790, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 11.787, mean reward: 0.327 [0.239, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.156, 10.100], loss: 0.002334, mae: 0.050302, mean_q: 0.026586
 46138/100000: episode: 791, duration: 0.189s, episode steps: 36, steps per second: 191, episode reward: 5.780, mean reward: 0.161 [0.006, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.363, 10.136], loss: 0.002517, mae: 0.051559, mean_q: 0.027343
 46186/100000: episode: 792, duration: 0.276s, episode steps: 48, steps per second: 174, episode reward: 11.827, mean reward: 0.246 [0.057, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.882, 10.100], loss: 0.002734, mae: 0.054274, mean_q: 0.031555
 46198/100000: episode: 793, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 4.279, mean reward: 0.357 [0.292, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.253, 10.374], loss: 0.002955, mae: 0.056164, mean_q: 0.065417
 46246/100000: episode: 794, duration: 0.279s, episode steps: 48, steps per second: 172, episode reward: 11.011, mean reward: 0.229 [0.039, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.792, 10.254], loss: 0.002745, mae: 0.055229, mean_q: 0.053990
 46294/100000: episode: 795, duration: 0.289s, episode steps: 48, steps per second: 166, episode reward: 13.467, mean reward: 0.281 [0.082, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.337, 10.238], loss: 0.002711, mae: 0.054948, mean_q: 0.035402
 46333/100000: episode: 796, duration: 0.204s, episode steps: 39, steps per second: 191, episode reward: 9.085, mean reward: 0.233 [0.059, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.297, 10.100], loss: 0.002927, mae: 0.057841, mean_q: 0.109876
 46369/100000: episode: 797, duration: 0.195s, episode steps: 36, steps per second: 185, episode reward: 10.447, mean reward: 0.290 [0.163, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.271, 10.100], loss: 0.007398, mae: 0.076034, mean_q: 0.080494
 46408/100000: episode: 798, duration: 0.245s, episode steps: 39, steps per second: 159, episode reward: 7.817, mean reward: 0.200 [0.038, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-1.460, 10.320], loss: 0.004404, mae: 0.069844, mean_q: 0.129105
 46442/100000: episode: 799, duration: 0.189s, episode steps: 34, steps per second: 180, episode reward: 11.601, mean reward: 0.341 [0.195, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.262, 10.100], loss: 0.002911, mae: 0.057259, mean_q: 0.064991
 46463/100000: episode: 800, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 6.263, mean reward: 0.298 [0.155, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.253], loss: 0.002444, mae: 0.052514, mean_q: -0.000237
 46499/100000: episode: 801, duration: 0.187s, episode steps: 36, steps per second: 193, episode reward: 17.939, mean reward: 0.498 [0.207, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.513, 10.100], loss: 0.002832, mae: 0.056305, mean_q: 0.120000
 46524/100000: episode: 802, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 8.572, mean reward: 0.343 [0.273, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.086, 10.444], loss: 0.002817, mae: 0.057818, mean_q: 0.117933
 46536/100000: episode: 803, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 3.572, mean reward: 0.298 [0.211, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.472, 10.331], loss: 0.002202, mae: 0.051021, mean_q: 0.060505
 46548/100000: episode: 804, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 4.746, mean reward: 0.395 [0.320, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.510], loss: 0.002917, mae: 0.058200, mean_q: 0.058941
 46584/100000: episode: 805, duration: 0.198s, episode steps: 36, steps per second: 181, episode reward: 9.947, mean reward: 0.276 [0.170, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.687, 10.100], loss: 0.002783, mae: 0.057557, mean_q: 0.122742
 46606/100000: episode: 806, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 8.334, mean reward: 0.379 [0.269, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.085, 10.447], loss: 0.003248, mae: 0.059779, mean_q: 0.104169
 46628/100000: episode: 807, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 7.521, mean reward: 0.342 [0.214, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.409], loss: 0.002859, mae: 0.057854, mean_q: 0.143374
 46663/100000: episode: 808, duration: 0.196s, episode steps: 35, steps per second: 178, episode reward: 7.934, mean reward: 0.227 [0.032, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.487, 10.183], loss: 0.002962, mae: 0.057505, mean_q: 0.136886
 46685/100000: episode: 809, duration: 0.134s, episode steps: 22, steps per second: 165, episode reward: 6.835, mean reward: 0.311 [0.178, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.181, 10.328], loss: 0.002631, mae: 0.053771, mean_q: 0.147161
 46719/100000: episode: 810, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 10.659, mean reward: 0.313 [0.241, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.625, 10.100], loss: 0.002966, mae: 0.057180, mean_q: 0.138131
 46737/100000: episode: 811, duration: 0.115s, episode steps: 18, steps per second: 157, episode reward: 5.682, mean reward: 0.316 [0.195, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.577, 10.356], loss: 0.002577, mae: 0.053205, mean_q: 0.115936
 46776/100000: episode: 812, duration: 0.233s, episode steps: 39, steps per second: 167, episode reward: 12.725, mean reward: 0.326 [0.225, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-1.275, 10.100], loss: 0.002975, mae: 0.057758, mean_q: 0.161903
 46815/100000: episode: 813, duration: 0.220s, episode steps: 39, steps per second: 177, episode reward: 6.075, mean reward: 0.156 [0.050, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.209, 10.235], loss: 0.003246, mae: 0.061870, mean_q: 0.164693
 46854/100000: episode: 814, duration: 0.207s, episode steps: 39, steps per second: 189, episode reward: 7.415, mean reward: 0.190 [0.040, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-1.423, 10.163], loss: 0.003189, mae: 0.060147, mean_q: 0.156276
 46890/100000: episode: 815, duration: 0.204s, episode steps: 36, steps per second: 176, episode reward: 12.562, mean reward: 0.349 [0.199, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.967, 10.100], loss: 0.003202, mae: 0.060994, mean_q: 0.150662
 46926/100000: episode: 816, duration: 0.205s, episode steps: 36, steps per second: 176, episode reward: 7.175, mean reward: 0.199 [0.062, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.436, 10.225], loss: 0.004120, mae: 0.069003, mean_q: 0.140728
[Info] 200-TH LEVEL FOUND: 0.8313937783241272, Considering 10/90 traces
 46951/100000: episode: 817, duration: 4.387s, episode steps: 25, steps per second: 6, episode reward: 9.093, mean reward: 0.364 [0.233, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.288, 10.476], loss: 0.002990, mae: 0.059552, mean_q: 0.169651
 46965/100000: episode: 818, duration: 0.091s, episode steps: 14, steps per second: 154, episode reward: 6.436, mean reward: 0.460 [0.392, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.388], loss: 0.003328, mae: 0.062278, mean_q: 0.194913
 46983/100000: episode: 819, duration: 0.118s, episode steps: 18, steps per second: 152, episode reward: 9.411, mean reward: 0.523 [0.397, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.766], loss: 0.003257, mae: 0.060093, mean_q: 0.186658
 47001/100000: episode: 820, duration: 0.113s, episode steps: 18, steps per second: 159, episode reward: 9.576, mean reward: 0.532 [0.472, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.092, 10.579], loss: 0.003349, mae: 0.063465, mean_q: 0.209317
 47019/100000: episode: 821, duration: 0.109s, episode steps: 18, steps per second: 166, episode reward: 8.753, mean reward: 0.486 [0.323, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.400], loss: 0.003042, mae: 0.060719, mean_q: 0.240578
 47037/100000: episode: 822, duration: 0.114s, episode steps: 18, steps per second: 158, episode reward: 7.910, mean reward: 0.439 [0.376, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.183, 10.583], loss: 0.002740, mae: 0.057215, mean_q: 0.177218
 47068/100000: episode: 823, duration: 0.187s, episode steps: 31, steps per second: 166, episode reward: 14.841, mean reward: 0.479 [0.359, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.303, 10.100], loss: 0.002678, mae: 0.054859, mean_q: 0.193239
 47099/100000: episode: 824, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 12.114, mean reward: 0.391 [0.271, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.405, 10.100], loss: 0.002771, mae: 0.056551, mean_q: 0.186829
 47120/100000: episode: 825, duration: 0.128s, episode steps: 21, steps per second: 164, episode reward: 8.223, mean reward: 0.392 [0.303, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.552], loss: 0.003011, mae: 0.058957, mean_q: 0.231084
 47138/100000: episode: 826, duration: 0.113s, episode steps: 18, steps per second: 159, episode reward: 7.956, mean reward: 0.442 [0.333, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.343, 10.473], loss: 0.002542, mae: 0.055058, mean_q: 0.218056
 47152/100000: episode: 827, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 4.944, mean reward: 0.353 [0.206, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-1.240, 10.432], loss: 0.002633, mae: 0.053289, mean_q: 0.212070
 47170/100000: episode: 828, duration: 0.119s, episode steps: 18, steps per second: 151, episode reward: 7.242, mean reward: 0.402 [0.293, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.432, 10.410], loss: 0.002911, mae: 0.058939, mean_q: 0.251118
 47188/100000: episode: 829, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 6.062, mean reward: 0.337 [0.211, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.342, 10.334], loss: 0.002863, mae: 0.054578, mean_q: 0.177273
 47204/100000: episode: 830, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 6.830, mean reward: 0.427 [0.360, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.481], loss: 0.002302, mae: 0.051137, mean_q: 0.258840
 47234/100000: episode: 831, duration: 0.182s, episode steps: 30, steps per second: 165, episode reward: 15.038, mean reward: 0.501 [0.394, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.461, 10.100], loss: 0.002661, mae: 0.055378, mean_q: 0.242072
 47252/100000: episode: 832, duration: 0.123s, episode steps: 18, steps per second: 146, episode reward: 8.810, mean reward: 0.489 [0.392, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.277, 10.589], loss: 0.002691, mae: 0.056876, mean_q: 0.221755
 47270/100000: episode: 833, duration: 0.104s, episode steps: 18, steps per second: 174, episode reward: 7.814, mean reward: 0.434 [0.298, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.647, 10.440], loss: 0.002701, mae: 0.056222, mean_q: 0.276020
 47301/100000: episode: 834, duration: 0.184s, episode steps: 31, steps per second: 169, episode reward: 12.949, mean reward: 0.418 [0.313, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.417, 10.100], loss: 0.002998, mae: 0.059710, mean_q: 0.274856
 47319/100000: episode: 835, duration: 0.133s, episode steps: 18, steps per second: 135, episode reward: 8.004, mean reward: 0.445 [0.349, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.475], loss: 0.002568, mae: 0.053777, mean_q: 0.248315
 47350/100000: episode: 836, duration: 0.187s, episode steps: 31, steps per second: 166, episode reward: 11.608, mean reward: 0.374 [0.208, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.561, 10.100], loss: 0.002805, mae: 0.057016, mean_q: 0.262678
 47366/100000: episode: 837, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 7.117, mean reward: 0.445 [0.315, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.283, 10.431], loss: 0.002603, mae: 0.053384, mean_q: 0.290235
 47396/100000: episode: 838, duration: 0.185s, episode steps: 30, steps per second: 162, episode reward: 10.008, mean reward: 0.334 [0.065, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.003, 10.100], loss: 0.002850, mae: 0.057584, mean_q: 0.303106
 47414/100000: episode: 839, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 8.411, mean reward: 0.467 [0.343, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.476], loss: 0.002566, mae: 0.053916, mean_q: 0.272567
 47444/100000: episode: 840, duration: 0.185s, episode steps: 30, steps per second: 162, episode reward: 11.550, mean reward: 0.385 [0.276, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.202, 10.100], loss: 0.002608, mae: 0.054004, mean_q: 0.283239
 47465/100000: episode: 841, duration: 0.144s, episode steps: 21, steps per second: 145, episode reward: 8.305, mean reward: 0.395 [0.301, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.489, 10.432], loss: 0.002699, mae: 0.058102, mean_q: 0.324040
 47483/100000: episode: 842, duration: 0.109s, episode steps: 18, steps per second: 164, episode reward: 7.153, mean reward: 0.397 [0.322, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.373, 10.414], loss: 0.003295, mae: 0.061396, mean_q: 0.287280
 47497/100000: episode: 843, duration: 0.101s, episode steps: 14, steps per second: 139, episode reward: 7.054, mean reward: 0.504 [0.400, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.466], loss: 0.002793, mae: 0.057052, mean_q: 0.239373
 47513/100000: episode: 844, duration: 0.102s, episode steps: 16, steps per second: 157, episode reward: 7.316, mean reward: 0.457 [0.370, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-1.159, 10.456], loss: 0.002561, mae: 0.055273, mean_q: 0.281452
 47527/100000: episode: 845, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 4.903, mean reward: 0.350 [0.126, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.294], loss: 0.002646, mae: 0.056013, mean_q: 0.255915
 47545/100000: episode: 846, duration: 0.116s, episode steps: 18, steps per second: 155, episode reward: 7.037, mean reward: 0.391 [0.217, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-1.129, 10.328], loss: 0.002704, mae: 0.055768, mean_q: 0.300018
 47563/100000: episode: 847, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 7.508, mean reward: 0.417 [0.309, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.384], loss: 0.002933, mae: 0.057636, mean_q: 0.267207
 47577/100000: episode: 848, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 5.614, mean reward: 0.401 [0.327, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.264, 10.491], loss: 0.002804, mae: 0.056795, mean_q: 0.319979
 47591/100000: episode: 849, duration: 0.089s, episode steps: 14, steps per second: 158, episode reward: 6.435, mean reward: 0.460 [0.402, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.263, 10.494], loss: 0.002485, mae: 0.053086, mean_q: 0.300023
 47609/100000: episode: 850, duration: 0.111s, episode steps: 18, steps per second: 163, episode reward: 7.250, mean reward: 0.403 [0.344, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.682, 10.514], loss: 0.003412, mae: 0.062651, mean_q: 0.322682
 47625/100000: episode: 851, duration: 0.105s, episode steps: 16, steps per second: 152, episode reward: 6.443, mean reward: 0.403 [0.334, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.226, 10.478], loss: 0.003870, mae: 0.064105, mean_q: 0.344189
 47646/100000: episode: 852, duration: 0.128s, episode steps: 21, steps per second: 164, episode reward: 7.801, mean reward: 0.371 [0.195, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.392, 10.100], loss: 0.004115, mae: 0.066690, mean_q: 0.347634
 47676/100000: episode: 853, duration: 0.180s, episode steps: 30, steps per second: 167, episode reward: 11.082, mean reward: 0.369 [0.220, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.210, 10.100], loss: 0.003491, mae: 0.063349, mean_q: 0.392777
 47706/100000: episode: 854, duration: 0.168s, episode steps: 30, steps per second: 178, episode reward: 11.125, mean reward: 0.371 [0.201, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.430, 10.100], loss: 0.003047, mae: 0.058864, mean_q: 0.301675
 47724/100000: episode: 855, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 6.638, mean reward: 0.369 [0.226, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.158, 10.385], loss: 0.003193, mae: 0.060856, mean_q: 0.341600
 47738/100000: episode: 856, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 5.380, mean reward: 0.384 [0.209, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.442, 10.341], loss: 0.003372, mae: 0.061894, mean_q: 0.362820
 47759/100000: episode: 857, duration: 0.149s, episode steps: 21, steps per second: 141, episode reward: 9.585, mean reward: 0.456 [0.355, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.670], loss: 0.003173, mae: 0.060099, mean_q: 0.351418
 47790/100000: episode: 858, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 11.867, mean reward: 0.383 [0.249, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.060, 10.100], loss: 0.003134, mae: 0.060859, mean_q: 0.409265
 47808/100000: episode: 859, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 7.305, mean reward: 0.406 [0.321, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.337, 10.489], loss: 0.002724, mae: 0.057565, mean_q: 0.410148
 47839/100000: episode: 860, duration: 0.183s, episode steps: 31, steps per second: 169, episode reward: 15.540, mean reward: 0.501 [0.408, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.472, 10.100], loss: 0.002906, mae: 0.058193, mean_q: 0.383559
 47857/100000: episode: 861, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 8.236, mean reward: 0.458 [0.399, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-1.075, 10.556], loss: 0.002894, mae: 0.059465, mean_q: 0.401102
 47887/100000: episode: 862, duration: 0.177s, episode steps: 30, steps per second: 169, episode reward: 12.920, mean reward: 0.431 [0.346, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.469, 10.100], loss: 0.002780, mae: 0.055997, mean_q: 0.398823
 47908/100000: episode: 863, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 8.138, mean reward: 0.388 [0.297, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.010, 10.100], loss: 0.002851, mae: 0.058428, mean_q: 0.427116
 47929/100000: episode: 864, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 7.469, mean reward: 0.356 [0.279, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.128, 10.512], loss: 0.003031, mae: 0.059287, mean_q: 0.403580
 47947/100000: episode: 865, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 8.482, mean reward: 0.471 [0.327, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.442, 10.432], loss: 0.002948, mae: 0.060093, mean_q: 0.401754
 47961/100000: episode: 866, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 5.914, mean reward: 0.422 [0.211, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.458], loss: 0.003075, mae: 0.061036, mean_q: 0.392628
 47979/100000: episode: 867, duration: 0.113s, episode steps: 18, steps per second: 159, episode reward: 8.486, mean reward: 0.471 [0.403, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.563, 10.472], loss: 0.002373, mae: 0.051946, mean_q: 0.430318
 47997/100000: episode: 868, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 7.836, mean reward: 0.435 [0.309, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.097, 10.503], loss: 0.002746, mae: 0.055596, mean_q: 0.394057
 48015/100000: episode: 869, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 7.441, mean reward: 0.413 [0.312, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.995, 10.524], loss: 0.002619, mae: 0.056138, mean_q: 0.394894
 48046/100000: episode: 870, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 9.711, mean reward: 0.313 [0.217, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.187, 10.100], loss: 0.002678, mae: 0.056469, mean_q: 0.419814
 48060/100000: episode: 871, duration: 0.078s, episode steps: 14, steps per second: 181, episode reward: 5.077, mean reward: 0.363 [0.285, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.514, 10.475], loss: 0.002997, mae: 0.059972, mean_q: 0.414663
 48078/100000: episode: 872, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 7.723, mean reward: 0.429 [0.325, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.459], loss: 0.002923, mae: 0.059031, mean_q: 0.405191
 48096/100000: episode: 873, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 8.046, mean reward: 0.447 [0.336, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.359], loss: 0.002877, mae: 0.058575, mean_q: 0.451689
 48114/100000: episode: 874, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 7.768, mean reward: 0.432 [0.348, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.518], loss: 0.002511, mae: 0.055554, mean_q: 0.434137
 48130/100000: episode: 875, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 6.665, mean reward: 0.417 [0.261, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.665, 10.379], loss: 0.002547, mae: 0.054532, mean_q: 0.460618
 48146/100000: episode: 876, duration: 0.091s, episode steps: 16, steps per second: 177, episode reward: 5.481, mean reward: 0.343 [0.218, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.933, 10.272], loss: 0.002733, mae: 0.055853, mean_q: 0.459655
 48167/100000: episode: 877, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 8.473, mean reward: 0.403 [0.281, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.343, 10.100], loss: 0.002758, mae: 0.056807, mean_q: 0.434984
 48183/100000: episode: 878, duration: 0.112s, episode steps: 16, steps per second: 143, episode reward: 8.588, mean reward: 0.537 [0.416, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.576], loss: 0.002955, mae: 0.059509, mean_q: 0.447732
 48204/100000: episode: 879, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 8.700, mean reward: 0.414 [0.239, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.846, 10.414], loss: 0.003016, mae: 0.060024, mean_q: 0.472949
 48235/100000: episode: 880, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 13.517, mean reward: 0.436 [0.238, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.226, 10.100], loss: 0.003059, mae: 0.059953, mean_q: 0.460006
 48262/100000: episode: 881, duration: 0.140s, episode steps: 27, steps per second: 192, episode reward: 10.226, mean reward: 0.379 [0.299, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.411, 10.100], loss: 0.003183, mae: 0.061576, mean_q: 0.467534
 48280/100000: episode: 882, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 6.770, mean reward: 0.376 [0.252, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.615, 10.400], loss: 0.003284, mae: 0.061955, mean_q: 0.450166
 48296/100000: episode: 883, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 5.414, mean reward: 0.338 [0.269, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.245, 10.402], loss: 0.002824, mae: 0.057706, mean_q: 0.476868
 48310/100000: episode: 884, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 6.757, mean reward: 0.483 [0.407, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.500, 10.560], loss: 0.002801, mae: 0.059855, mean_q: 0.473439
 48331/100000: episode: 885, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 9.137, mean reward: 0.435 [0.401, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.499, 10.100], loss: 0.002748, mae: 0.056743, mean_q: 0.470369
 48349/100000: episode: 886, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 9.603, mean reward: 0.533 [0.368, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.249, 10.528], loss: 0.003031, mae: 0.060870, mean_q: 0.482392
 48367/100000: episode: 887, duration: 0.124s, episode steps: 18, steps per second: 145, episode reward: 7.207, mean reward: 0.400 [0.206, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-1.197, 10.300], loss: 0.002532, mae: 0.056463, mean_q: 0.472673
 48388/100000: episode: 888, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 8.437, mean reward: 0.402 [0.330, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.430], loss: 0.002971, mae: 0.060126, mean_q: 0.464276
 48409/100000: episode: 889, duration: 0.120s, episode steps: 21, steps per second: 174, episode reward: 9.629, mean reward: 0.459 [0.347, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.684, 10.367], loss: 0.002709, mae: 0.057537, mean_q: 0.514955
 48430/100000: episode: 890, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 10.040, mean reward: 0.478 [0.389, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.346, 10.100], loss: 0.002629, mae: 0.056750, mean_q: 0.508172
 48457/100000: episode: 891, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 11.116, mean reward: 0.412 [0.295, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.866, 10.100], loss: 0.002851, mae: 0.059078, mean_q: 0.527631
 48475/100000: episode: 892, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 9.026, mean reward: 0.501 [0.429, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.063, 10.623], loss: 0.003006, mae: 0.061824, mean_q: 0.566169
 48491/100000: episode: 893, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 7.412, mean reward: 0.463 [0.380, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.118, 10.509], loss: 0.003490, mae: 0.065698, mean_q: 0.534945
 48509/100000: episode: 894, duration: 0.121s, episode steps: 18, steps per second: 149, episode reward: 7.041, mean reward: 0.391 [0.223, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.409], loss: 0.002490, mae: 0.055948, mean_q: 0.526252
 48527/100000: episode: 895, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 7.684, mean reward: 0.427 [0.271, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.580], loss: 0.002800, mae: 0.058856, mean_q: 0.561546
 48545/100000: episode: 896, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 7.544, mean reward: 0.419 [0.332, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.347], loss: 0.003168, mae: 0.062110, mean_q: 0.525654
 48566/100000: episode: 897, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 8.638, mean reward: 0.411 [0.221, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.221, 10.100], loss: 0.002621, mae: 0.056664, mean_q: 0.515587
 48584/100000: episode: 898, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 8.305, mean reward: 0.461 [0.328, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.117, 10.543], loss: 0.002833, mae: 0.060277, mean_q: 0.561187
 48602/100000: episode: 899, duration: 0.130s, episode steps: 18, steps per second: 139, episode reward: 10.061, mean reward: 0.559 [0.484, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.642, 10.528], loss: 0.002832, mae: 0.058165, mean_q: 0.542599
 48633/100000: episode: 900, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 12.572, mean reward: 0.406 [0.225, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.688, 10.100], loss: 0.002499, mae: 0.055663, mean_q: 0.546119
 48651/100000: episode: 901, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 6.219, mean reward: 0.345 [0.213, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.373], loss: 0.002499, mae: 0.054177, mean_q: 0.551763
 48681/100000: episode: 902, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 12.480, mean reward: 0.416 [0.321, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.420, 10.100], loss: 0.002478, mae: 0.055531, mean_q: 0.553456
 48699/100000: episode: 903, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 7.010, mean reward: 0.389 [0.343, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.517], loss: 0.002707, mae: 0.057909, mean_q: 0.584342
 48720/100000: episode: 904, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 8.047, mean reward: 0.383 [0.257, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.100], loss: 0.003002, mae: 0.061741, mean_q: 0.599560
 48747/100000: episode: 905, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 12.116, mean reward: 0.449 [0.297, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.651, 10.100], loss: 0.002577, mae: 0.056631, mean_q: 0.578494
 48768/100000: episode: 906, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 8.167, mean reward: 0.389 [0.290, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-1.019, 10.373], loss: 0.002704, mae: 0.056834, mean_q: 0.577627
[Info] 300-TH LEVEL FOUND: 1.0472362041473389, Considering 10/90 traces
 48798/100000: episode: 907, duration: 4.280s, episode steps: 30, steps per second: 7, episode reward: 15.249, mean reward: 0.508 [0.339, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.774, 10.100], loss: 0.002443, mae: 0.054092, mean_q: 0.579385
 48815/100000: episode: 908, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 8.124, mean reward: 0.478 [0.434, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.412, 10.100], loss: 0.002531, mae: 0.055955, mean_q: 0.612641
 48831/100000: episode: 909, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 9.197, mean reward: 0.575 [0.502, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.545, 10.100], loss: 0.003465, mae: 0.064748, mean_q: 0.586868
 48844/100000: episode: 910, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 7.290, mean reward: 0.561 [0.481, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-1.047, 10.512], loss: 0.003274, mae: 0.064849, mean_q: 0.588544
[Info] FALSIFICATION!
 48845/100000: episode: 911, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.035, 9.559], loss: 0.002945, mae: 0.064413, mean_q: 0.662855
 48945/100000: episode: 912, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.494, mean reward: -0.175 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.490, 10.098], loss: 0.016368, mae: 0.069684, mean_q: 0.620830
 49045/100000: episode: 913, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.430, mean reward: -0.164 [-1.000, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.705, 10.098], loss: 0.029547, mae: 0.076167, mean_q: 0.621037
 49145/100000: episode: 914, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.012, mean reward: -0.190 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.734, 10.098], loss: 0.016816, mae: 0.071937, mean_q: 0.618272
 49245/100000: episode: 915, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.623, mean reward: -0.186 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.824, 10.209], loss: 0.002922, mae: 0.059158, mean_q: 0.589062
 49345/100000: episode: 916, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -13.398, mean reward: -0.134 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.233, 10.098], loss: 0.017226, mae: 0.068388, mean_q: 0.584302
 49445/100000: episode: 917, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.189, mean reward: -0.162 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.279, 10.268], loss: 0.029596, mae: 0.080481, mean_q: 0.560747
 49545/100000: episode: 918, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -14.895, mean reward: -0.149 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.682, 10.417], loss: 0.016125, mae: 0.067427, mean_q: 0.531984
 49645/100000: episode: 919, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.769, mean reward: -0.178 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.428, 10.112], loss: 0.003823, mae: 0.062266, mean_q: 0.531433
 49745/100000: episode: 920, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.810, mean reward: -0.148 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.297, 10.098], loss: 0.004399, mae: 0.060957, mean_q: 0.515035
 49845/100000: episode: 921, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.137, mean reward: -0.151 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.616, 10.254], loss: 0.015907, mae: 0.066744, mean_q: 0.509824
 49945/100000: episode: 922, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.264, mean reward: -0.193 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.701, 10.156], loss: 0.015782, mae: 0.065491, mean_q: 0.483530
 50045/100000: episode: 923, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -15.832, mean reward: -0.158 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.410, 10.174], loss: 0.016805, mae: 0.069714, mean_q: 0.460375
 50145/100000: episode: 924, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.393, mean reward: -0.194 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.277, 10.098], loss: 0.002763, mae: 0.056451, mean_q: 0.433380
 50245/100000: episode: 925, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.557, mean reward: -0.176 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.940, 10.204], loss: 0.002618, mae: 0.055368, mean_q: 0.413100
 50345/100000: episode: 926, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -17.078, mean reward: -0.171 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.691, 10.215], loss: 0.016178, mae: 0.068060, mean_q: 0.392721
 50445/100000: episode: 927, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.772, mean reward: -0.178 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.481, 10.313], loss: 0.015788, mae: 0.064611, mean_q: 0.355983
 50545/100000: episode: 928, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -18.166, mean reward: -0.182 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.743, 10.313], loss: 0.015911, mae: 0.066683, mean_q: 0.348317
 50645/100000: episode: 929, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.160, mean reward: -0.162 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.664, 10.121], loss: 0.015241, mae: 0.058880, mean_q: 0.317513
 50745/100000: episode: 930, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.383, mean reward: -0.174 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.486, 10.190], loss: 0.028639, mae: 0.073370, mean_q: 0.324310
 50845/100000: episode: 931, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -14.724, mean reward: -0.147 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.717, 10.294], loss: 0.002567, mae: 0.053718, mean_q: 0.307956
 50945/100000: episode: 932, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -19.876, mean reward: -0.199 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.746, 10.231], loss: 0.041424, mae: 0.081106, mean_q: 0.308053
 51045/100000: episode: 933, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.376, mean reward: -0.164 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.314, 10.307], loss: 0.029214, mae: 0.074629, mean_q: 0.245988
 51145/100000: episode: 934, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.871, mean reward: -0.179 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.202, 10.229], loss: 0.015583, mae: 0.063412, mean_q: 0.264479
 51245/100000: episode: 935, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.018, mean reward: -0.140 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.712, 10.117], loss: 0.015514, mae: 0.062984, mean_q: 0.221996
 51345/100000: episode: 936, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -13.378, mean reward: -0.134 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.765, 10.098], loss: 0.015379, mae: 0.062798, mean_q: 0.241523
 51445/100000: episode: 937, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -18.007, mean reward: -0.180 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.653, 10.256], loss: 0.028093, mae: 0.069094, mean_q: 0.198082
 51545/100000: episode: 938, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.927, mean reward: -0.189 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.795, 10.098], loss: 0.015967, mae: 0.067759, mean_q: 0.196188
 51645/100000: episode: 939, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -17.588, mean reward: -0.176 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.613, 10.098], loss: 0.002484, mae: 0.052058, mean_q: 0.123001
 51745/100000: episode: 940, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.440, mean reward: -0.194 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.036, 10.155], loss: 0.002363, mae: 0.050523, mean_q: 0.124988
 51845/100000: episode: 941, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.963, mean reward: -0.180 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.019, 10.157], loss: 0.002538, mae: 0.052106, mean_q: 0.124789
 51945/100000: episode: 942, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -15.173, mean reward: -0.152 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.359, 10.316], loss: 0.002495, mae: 0.051813, mean_q: 0.090593
 52045/100000: episode: 943, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -10.555, mean reward: -0.106 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.638, 10.098], loss: 0.002500, mae: 0.053225, mean_q: 0.083244
 52145/100000: episode: 944, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.314, mean reward: -0.183 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.958, 10.251], loss: 0.002339, mae: 0.049280, mean_q: 0.024200
 52245/100000: episode: 945, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -20.149, mean reward: -0.201 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.388, 10.098], loss: 0.002553, mae: 0.051307, mean_q: 0.039483
 52345/100000: episode: 946, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.313, mean reward: -0.143 [-1.000, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.651, 10.310], loss: 0.015915, mae: 0.062348, mean_q: 0.010901
 52445/100000: episode: 947, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.885, mean reward: -0.179 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.666, 10.098], loss: 0.015781, mae: 0.065794, mean_q: -0.026252
 52545/100000: episode: 948, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -12.659, mean reward: -0.127 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.767, 10.219], loss: 0.002653, mae: 0.053702, mean_q: -0.029537
 52645/100000: episode: 949, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.600, mean reward: -0.166 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.571, 10.214], loss: 0.002636, mae: 0.052652, mean_q: -0.056741
 52745/100000: episode: 950, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -20.092, mean reward: -0.201 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.481, 10.098], loss: 0.028739, mae: 0.068250, mean_q: -0.059717
 52845/100000: episode: 951, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -16.396, mean reward: -0.164 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.722, 10.098], loss: 0.015617, mae: 0.061483, mean_q: -0.089321
 52945/100000: episode: 952, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -15.689, mean reward: -0.157 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.694, 10.098], loss: 0.018167, mae: 0.078920, mean_q: -0.103577
 53045/100000: episode: 953, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.987, mean reward: -0.190 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.562, 10.180], loss: 0.015577, mae: 0.061550, mean_q: -0.145863
 53145/100000: episode: 954, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -19.551, mean reward: -0.196 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.449, 10.098], loss: 0.028241, mae: 0.066879, mean_q: -0.150509
 53245/100000: episode: 955, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.762, mean reward: -0.178 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.650, 10.100], loss: 0.015359, mae: 0.060594, mean_q: -0.186239
 53345/100000: episode: 956, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.595, mean reward: -0.186 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.227, 10.098], loss: 0.002559, mae: 0.049654, mean_q: -0.240373
 53445/100000: episode: 957, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -10.844, mean reward: -0.108 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.376, 10.098], loss: 0.002556, mae: 0.050239, mean_q: -0.255063
 53545/100000: episode: 958, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.656, mean reward: -0.177 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.621, 10.098], loss: 0.002577, mae: 0.050185, mean_q: -0.244091
 53645/100000: episode: 959, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -19.429, mean reward: -0.194 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.861, 10.120], loss: 0.002619, mae: 0.050760, mean_q: -0.275560
 53745/100000: episode: 960, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.541, mean reward: -0.165 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.281, 10.121], loss: 0.015507, mae: 0.060117, mean_q: -0.311171
 53845/100000: episode: 961, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: -16.518, mean reward: -0.165 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.417, 10.098], loss: 0.002478, mae: 0.049921, mean_q: -0.328966
 53945/100000: episode: 962, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: -15.922, mean reward: -0.159 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.695, 10.098], loss: 0.002674, mae: 0.051016, mean_q: -0.312282
 54045/100000: episode: 963, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -20.497, mean reward: -0.205 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.134, 10.207], loss: 0.002379, mae: 0.048716, mean_q: -0.317164
 54145/100000: episode: 964, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -14.873, mean reward: -0.149 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.395, 10.098], loss: 0.002202, mae: 0.047179, mean_q: -0.315907
 54245/100000: episode: 965, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -16.960, mean reward: -0.170 [-1.000, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.128, 10.098], loss: 0.002503, mae: 0.050784, mean_q: -0.282703
 54345/100000: episode: 966, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -12.873, mean reward: -0.129 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.332, 10.098], loss: 0.002313, mae: 0.048156, mean_q: -0.345466
 54445/100000: episode: 967, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.643, mean reward: -0.176 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.899, 10.146], loss: 0.002414, mae: 0.048543, mean_q: -0.309497
 54545/100000: episode: 968, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.131, mean reward: -0.181 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.884, 10.098], loss: 0.002386, mae: 0.048200, mean_q: -0.324639
 54645/100000: episode: 969, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -16.036, mean reward: -0.160 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.221, 10.098], loss: 0.002458, mae: 0.050831, mean_q: -0.317407
 54745/100000: episode: 970, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -16.920, mean reward: -0.169 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.555, 10.166], loss: 0.002505, mae: 0.050336, mean_q: -0.336302
 54845/100000: episode: 971, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -17.170, mean reward: -0.172 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.928, 10.098], loss: 0.002434, mae: 0.050248, mean_q: -0.283508
 54945/100000: episode: 972, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.170, mean reward: -0.182 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.543, 10.125], loss: 0.002320, mae: 0.048994, mean_q: -0.320411
 55045/100000: episode: 973, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -19.009, mean reward: -0.190 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.394, 10.155], loss: 0.002493, mae: 0.049608, mean_q: -0.300479
 55145/100000: episode: 974, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -11.355, mean reward: -0.114 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.693, 10.098], loss: 0.002471, mae: 0.049027, mean_q: -0.315995
 55245/100000: episode: 975, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.041, mean reward: -0.160 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.375, 10.098], loss: 0.002421, mae: 0.049160, mean_q: -0.312143
 55345/100000: episode: 976, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -12.907, mean reward: -0.129 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.667, 10.126], loss: 0.002370, mae: 0.048957, mean_q: -0.283884
 55445/100000: episode: 977, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -16.264, mean reward: -0.163 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.075, 10.175], loss: 0.003624, mae: 0.060837, mean_q: -0.288518
 55545/100000: episode: 978, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.080, mean reward: -0.171 [-1.000, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.015, 10.098], loss: 0.002389, mae: 0.049047, mean_q: -0.328406
 55645/100000: episode: 979, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: -15.300, mean reward: -0.153 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.798, 10.369], loss: 0.002507, mae: 0.049284, mean_q: -0.324834
 55745/100000: episode: 980, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.731, mean reward: -0.187 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.802, 10.098], loss: 0.002557, mae: 0.050322, mean_q: -0.295994
 55845/100000: episode: 981, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -18.007, mean reward: -0.180 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.836, 10.098], loss: 0.002641, mae: 0.051125, mean_q: -0.297431
 55945/100000: episode: 982, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -18.990, mean reward: -0.190 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.568, 10.098], loss: 0.002550, mae: 0.049683, mean_q: -0.317297
 56045/100000: episode: 983, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.297, mean reward: -0.183 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.904, 10.098], loss: 0.002377, mae: 0.048674, mean_q: -0.309671
 56145/100000: episode: 984, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -15.541, mean reward: -0.155 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.432, 10.098], loss: 0.002377, mae: 0.048970, mean_q: -0.326637
 56245/100000: episode: 985, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.392, mean reward: -0.154 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.791, 10.309], loss: 0.002518, mae: 0.049696, mean_q: -0.334796
 56345/100000: episode: 986, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -13.996, mean reward: -0.140 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.898, 10.207], loss: 0.002641, mae: 0.050977, mean_q: -0.335978
 56445/100000: episode: 987, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -13.690, mean reward: -0.137 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.292, 10.265], loss: 0.002338, mae: 0.048869, mean_q: -0.319917
 56545/100000: episode: 988, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.880, mean reward: -0.169 [-1.000, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.591, 10.191], loss: 0.002512, mae: 0.051770, mean_q: -0.290064
 56645/100000: episode: 989, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.439, mean reward: -0.174 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.221, 10.363], loss: 0.002499, mae: 0.049642, mean_q: -0.329094
 56745/100000: episode: 990, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.735, mean reward: -0.177 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.848, 10.210], loss: 0.002557, mae: 0.050324, mean_q: -0.334361
 56845/100000: episode: 991, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.836, mean reward: -0.158 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.961, 10.498], loss: 0.002662, mae: 0.051460, mean_q: -0.331305
 56945/100000: episode: 992, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.579, mean reward: -0.196 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.617, 10.239], loss: 0.002608, mae: 0.050434, mean_q: -0.295127
 57045/100000: episode: 993, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.698, mean reward: -0.167 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.792, 10.098], loss: 0.002353, mae: 0.049321, mean_q: -0.317038
 57145/100000: episode: 994, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -16.288, mean reward: -0.163 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.350, 10.348], loss: 0.002459, mae: 0.050659, mean_q: -0.283658
 57245/100000: episode: 995, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -15.432, mean reward: -0.154 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.696, 10.238], loss: 0.002628, mae: 0.050047, mean_q: -0.330260
 57345/100000: episode: 996, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.731, mean reward: -0.187 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.782, 10.267], loss: 0.002582, mae: 0.051302, mean_q: -0.292642
 57445/100000: episode: 997, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.763, mean reward: -0.148 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.320, 10.098], loss: 0.002586, mae: 0.051904, mean_q: -0.281599
 57545/100000: episode: 998, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.063, mean reward: -0.181 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.921, 10.275], loss: 0.002444, mae: 0.049529, mean_q: -0.347413
 57645/100000: episode: 999, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.483, mean reward: -0.185 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.392, 10.324], loss: 0.003785, mae: 0.058443, mean_q: -0.316612
 57745/100000: episode: 1000, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.872, mean reward: -0.169 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.803, 10.223], loss: 0.002636, mae: 0.051635, mean_q: -0.342083
 57845/100000: episode: 1001, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -16.764, mean reward: -0.168 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.698, 10.098], loss: 0.002446, mae: 0.049680, mean_q: -0.330629
 57945/100000: episode: 1002, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.600, mean reward: -0.166 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.445, 10.098], loss: 0.003045, mae: 0.056838, mean_q: -0.319066
 58045/100000: episode: 1003, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -12.960, mean reward: -0.130 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.573, 10.350], loss: 0.002594, mae: 0.050692, mean_q: -0.299768
 58145/100000: episode: 1004, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.506, mean reward: -0.185 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.087, 10.148], loss: 0.002545, mae: 0.050104, mean_q: -0.321886
 58245/100000: episode: 1005, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.504, mean reward: -0.185 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.484, 10.098], loss: 0.002527, mae: 0.050015, mean_q: -0.336551
 58345/100000: episode: 1006, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.029, mean reward: -0.160 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.833, 10.100], loss: 0.002516, mae: 0.049711, mean_q: -0.324148
 58445/100000: episode: 1007, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.968, mean reward: -0.160 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.930, 10.326], loss: 0.002589, mae: 0.051202, mean_q: -0.293176
 58545/100000: episode: 1008, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -12.847, mean reward: -0.128 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.699, 10.259], loss: 0.002617, mae: 0.051187, mean_q: -0.320973
 58645/100000: episode: 1009, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.343, mean reward: -0.123 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.258, 10.280], loss: 0.002430, mae: 0.049809, mean_q: -0.313835
 58745/100000: episode: 1010, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.855, mean reward: -0.199 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.736, 10.240], loss: 0.002420, mae: 0.049160, mean_q: -0.322704
[Info] 100-TH LEVEL FOUND: 0.5997512340545654, Considering 10/90 traces
 58845/100000: episode: 1011, duration: 4.595s, episode steps: 100, steps per second: 22, episode reward: -16.951, mean reward: -0.170 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.771, 10.098], loss: 0.002547, mae: 0.050021, mean_q: -0.310503
 58902/100000: episode: 1012, duration: 0.307s, episode steps: 57, steps per second: 186, episode reward: 12.549, mean reward: 0.220 [0.027, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-0.250, 10.224], loss: 0.002429, mae: 0.049071, mean_q: -0.326614
 58944/100000: episode: 1013, duration: 0.231s, episode steps: 42, steps per second: 181, episode reward: 9.517, mean reward: 0.227 [0.020, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.118, 10.100], loss: 0.002621, mae: 0.051001, mean_q: -0.260923
 58979/100000: episode: 1014, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 10.985, mean reward: 0.314 [0.133, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.867, 10.100], loss: 0.002963, mae: 0.054314, mean_q: -0.270857
 58989/100000: episode: 1015, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 4.257, mean reward: 0.426 [0.392, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.366, 10.100], loss: 0.003234, mae: 0.055886, mean_q: -0.233515
 59022/100000: episode: 1016, duration: 0.188s, episode steps: 33, steps per second: 175, episode reward: 9.982, mean reward: 0.302 [0.218, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.931, 10.100], loss: 0.003125, mae: 0.055878, mean_q: -0.271159
 59063/100000: episode: 1017, duration: 0.221s, episode steps: 41, steps per second: 186, episode reward: 8.714, mean reward: 0.213 [0.094, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.549, 10.204], loss: 0.002778, mae: 0.052510, mean_q: -0.279494
 59098/100000: episode: 1018, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 15.502, mean reward: 0.443 [0.299, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.763, 10.100], loss: 0.002683, mae: 0.051817, mean_q: -0.234280
 59115/100000: episode: 1019, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 4.762, mean reward: 0.280 [0.125, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.997, 10.100], loss: 0.002807, mae: 0.053001, mean_q: -0.210016
 59172/100000: episode: 1020, duration: 0.347s, episode steps: 57, steps per second: 164, episode reward: 13.173, mean reward: 0.231 [0.096, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.839 [-0.954, 10.288], loss: 0.002808, mae: 0.054021, mean_q: -0.228835
 59225/100000: episode: 1021, duration: 0.290s, episode steps: 53, steps per second: 183, episode reward: 10.122, mean reward: 0.191 [0.003, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.917, 10.191], loss: 0.002788, mae: 0.055650, mean_q: -0.232003
 59260/100000: episode: 1022, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 12.584, mean reward: 0.360 [0.198, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.298, 10.100], loss: 0.002799, mae: 0.055457, mean_q: -0.234360
 59270/100000: episode: 1023, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 2.646, mean reward: 0.265 [0.198, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.237, 10.100], loss: 0.002448, mae: 0.050952, mean_q: -0.283655
 59311/100000: episode: 1024, duration: 0.208s, episode steps: 41, steps per second: 197, episode reward: 11.721, mean reward: 0.286 [0.183, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-1.721, 10.447], loss: 0.004322, mae: 0.064810, mean_q: -0.258717
 59354/100000: episode: 1025, duration: 0.236s, episode steps: 43, steps per second: 182, episode reward: 8.471, mean reward: 0.197 [0.089, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.666, 10.266], loss: 0.006531, mae: 0.074941, mean_q: -0.220420
 59407/100000: episode: 1026, duration: 0.304s, episode steps: 53, steps per second: 174, episode reward: 13.331, mean reward: 0.252 [0.012, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.336, 10.142], loss: 0.002553, mae: 0.052859, mean_q: -0.193110
 59417/100000: episode: 1027, duration: 0.070s, episode steps: 10, steps per second: 142, episode reward: 4.268, mean reward: 0.427 [0.358, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.689, 10.100], loss: 0.003025, mae: 0.053792, mean_q: -0.192666
 59427/100000: episode: 1028, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 3.499, mean reward: 0.350 [0.295, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.267, 10.100], loss: 0.002682, mae: 0.054835, mean_q: -0.250524
 59469/100000: episode: 1029, duration: 0.226s, episode steps: 42, steps per second: 185, episode reward: 11.178, mean reward: 0.266 [0.166, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.843, 10.425], loss: 0.002726, mae: 0.053115, mean_q: -0.211486
 59504/100000: episode: 1030, duration: 0.204s, episode steps: 35, steps per second: 172, episode reward: 14.415, mean reward: 0.412 [0.262, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.743, 10.100], loss: 0.002417, mae: 0.051043, mean_q: -0.215913
 59539/100000: episode: 1031, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 10.247, mean reward: 0.293 [0.133, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.154, 10.100], loss: 0.002600, mae: 0.050617, mean_q: -0.260826
 59556/100000: episode: 1032, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 6.007, mean reward: 0.353 [0.271, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.518, 10.100], loss: 0.002673, mae: 0.053890, mean_q: -0.176854
 59598/100000: episode: 1033, duration: 0.226s, episode steps: 42, steps per second: 186, episode reward: 10.284, mean reward: 0.245 [0.072, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.252, 10.338], loss: 0.002719, mae: 0.052510, mean_q: -0.152378
 59631/100000: episode: 1034, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 9.301, mean reward: 0.282 [0.184, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.096, 10.100], loss: 0.003381, mae: 0.059349, mean_q: -0.215251
 59673/100000: episode: 1035, duration: 0.235s, episode steps: 42, steps per second: 179, episode reward: 9.725, mean reward: 0.232 [0.055, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.273, 10.280], loss: 0.003045, mae: 0.057539, mean_q: -0.170250
 59714/100000: episode: 1036, duration: 0.223s, episode steps: 41, steps per second: 183, episode reward: 11.065, mean reward: 0.270 [0.053, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.633, 10.230], loss: 0.002723, mae: 0.055352, mean_q: -0.162155
 59767/100000: episode: 1037, duration: 0.293s, episode steps: 53, steps per second: 181, episode reward: 18.808, mean reward: 0.355 [0.207, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.851, 10.388], loss: 0.002771, mae: 0.054634, mean_q: -0.143536
 59809/100000: episode: 1038, duration: 0.268s, episode steps: 42, steps per second: 157, episode reward: 10.629, mean reward: 0.253 [0.106, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.531, 10.378], loss: 0.002806, mae: 0.055248, mean_q: -0.134383
 59866/100000: episode: 1039, duration: 0.319s, episode steps: 57, steps per second: 179, episode reward: 17.204, mean reward: 0.302 [0.145, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.838 [-1.400, 10.284], loss: 0.002816, mae: 0.054199, mean_q: -0.094909
 59923/100000: episode: 1040, duration: 0.321s, episode steps: 57, steps per second: 178, episode reward: 11.241, mean reward: 0.197 [0.044, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.840 [-0.270, 10.100], loss: 0.002607, mae: 0.052954, mean_q: -0.158553
 59940/100000: episode: 1041, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 5.812, mean reward: 0.342 [0.232, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.400, 10.100], loss: 0.002860, mae: 0.054813, mean_q: -0.136206
 59982/100000: episode: 1042, duration: 0.241s, episode steps: 42, steps per second: 174, episode reward: 12.240, mean reward: 0.291 [0.036, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-1.918, 10.201], loss: 0.002706, mae: 0.052673, mean_q: -0.110580
 60039/100000: episode: 1043, duration: 0.321s, episode steps: 57, steps per second: 178, episode reward: 14.379, mean reward: 0.252 [0.102, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-0.413, 10.233], loss: 0.002648, mae: 0.051315, mean_q: -0.141100
 60082/100000: episode: 1044, duration: 0.231s, episode steps: 43, steps per second: 186, episode reward: 15.677, mean reward: 0.365 [0.258, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.559, 10.495], loss: 0.005323, mae: 0.071392, mean_q: -0.102082
 60092/100000: episode: 1045, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 3.853, mean reward: 0.385 [0.304, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.600, 10.100], loss: 0.003665, mae: 0.063702, mean_q: -0.115929
 60133/100000: episode: 1046, duration: 0.241s, episode steps: 41, steps per second: 170, episode reward: 8.172, mean reward: 0.199 [0.013, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.404, 10.152], loss: 0.002556, mae: 0.053450, mean_q: -0.080421
 60174/100000: episode: 1047, duration: 0.247s, episode steps: 41, steps per second: 166, episode reward: 18.909, mean reward: 0.461 [0.269, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.950, 10.100], loss: 0.002659, mae: 0.052510, mean_q: -0.119663
 60191/100000: episode: 1048, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 4.150, mean reward: 0.244 [0.049, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.134, 10.100], loss: 0.002380, mae: 0.050954, mean_q: -0.081454
 60201/100000: episode: 1049, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 3.568, mean reward: 0.357 [0.333, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.194, 10.100], loss: 0.002479, mae: 0.052422, mean_q: -0.124532
 60242/100000: episode: 1050, duration: 0.229s, episode steps: 41, steps per second: 179, episode reward: 13.414, mean reward: 0.327 [0.175, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.370, 10.100], loss: 0.002949, mae: 0.055029, mean_q: -0.079307
 60284/100000: episode: 1051, duration: 0.228s, episode steps: 42, steps per second: 184, episode reward: 11.304, mean reward: 0.269 [0.125, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.083, 10.230], loss: 0.002974, mae: 0.054171, mean_q: -0.072756
 60337/100000: episode: 1052, duration: 0.300s, episode steps: 53, steps per second: 177, episode reward: 10.481, mean reward: 0.198 [0.047, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.320, 10.100], loss: 0.002954, mae: 0.055484, mean_q: -0.055490
 60354/100000: episode: 1053, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 5.470, mean reward: 0.322 [0.235, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.239, 10.100], loss: 0.002641, mae: 0.054234, mean_q: 0.007022
 60407/100000: episode: 1054, duration: 0.288s, episode steps: 53, steps per second: 184, episode reward: 10.286, mean reward: 0.194 [0.016, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.843, 10.100], loss: 0.002945, mae: 0.053856, mean_q: -0.057808
 60440/100000: episode: 1055, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 12.199, mean reward: 0.370 [0.237, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.502, 10.100], loss: 0.002795, mae: 0.054812, mean_q: -0.028731
 60475/100000: episode: 1056, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 12.759, mean reward: 0.365 [0.155, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.429, 10.100], loss: 0.002729, mae: 0.055164, mean_q: 0.002179
 60517/100000: episode: 1057, duration: 0.236s, episode steps: 42, steps per second: 178, episode reward: 7.895, mean reward: 0.188 [0.040, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.232, 10.218], loss: 0.003015, mae: 0.057292, mean_q: -0.052570
 60552/100000: episode: 1058, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 14.737, mean reward: 0.421 [0.240, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-1.183, 10.100], loss: 0.002868, mae: 0.054569, mean_q: 0.006363
 60562/100000: episode: 1059, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 4.082, mean reward: 0.408 [0.287, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.412, 10.100], loss: 0.002323, mae: 0.050408, mean_q: 0.015983
 60597/100000: episode: 1060, duration: 0.195s, episode steps: 35, steps per second: 179, episode reward: 13.963, mean reward: 0.399 [0.257, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.621, 10.100], loss: 0.003298, mae: 0.059170, mean_q: -0.052034
 60630/100000: episode: 1061, duration: 0.183s, episode steps: 33, steps per second: 180, episode reward: 7.875, mean reward: 0.239 [0.036, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.118, 10.177], loss: 0.002634, mae: 0.052217, mean_q: 0.007728
 60647/100000: episode: 1062, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 5.945, mean reward: 0.350 [0.270, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.411, 10.100], loss: 0.002732, mae: 0.055183, mean_q: -0.073950
 60689/100000: episode: 1063, duration: 0.241s, episode steps: 42, steps per second: 174, episode reward: 11.487, mean reward: 0.274 [0.154, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.838, 10.465], loss: 0.003001, mae: 0.056783, mean_q: 0.047487
 60724/100000: episode: 1064, duration: 0.209s, episode steps: 35, steps per second: 168, episode reward: 11.020, mean reward: 0.315 [0.176, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.424, 10.100], loss: 0.002845, mae: 0.055304, mean_q: 0.022620
 60765/100000: episode: 1065, duration: 0.224s, episode steps: 41, steps per second: 183, episode reward: 12.982, mean reward: 0.317 [0.149, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.471, 10.280], loss: 0.002913, mae: 0.055676, mean_q: 0.053328
 60806/100000: episode: 1066, duration: 0.220s, episode steps: 41, steps per second: 186, episode reward: 10.158, mean reward: 0.248 [0.071, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.656, 10.236], loss: 0.002831, mae: 0.055551, mean_q: -0.007240
 60863/100000: episode: 1067, duration: 0.325s, episode steps: 57, steps per second: 176, episode reward: 10.407, mean reward: 0.183 [0.010, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-0.650, 10.285], loss: 0.002806, mae: 0.055468, mean_q: 0.042731
 60880/100000: episode: 1068, duration: 0.113s, episode steps: 17, steps per second: 150, episode reward: 3.893, mean reward: 0.229 [0.153, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.089, 10.100], loss: 0.002701, mae: 0.056940, mean_q: 0.055180
 60915/100000: episode: 1069, duration: 0.193s, episode steps: 35, steps per second: 182, episode reward: 13.539, mean reward: 0.387 [0.248, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.714, 10.100], loss: 0.003075, mae: 0.057289, mean_q: 0.113634
 60972/100000: episode: 1070, duration: 0.300s, episode steps: 57, steps per second: 190, episode reward: 14.587, mean reward: 0.256 [0.063, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.836 [-1.271, 10.157], loss: 0.003558, mae: 0.064323, mean_q: 0.061789
 60989/100000: episode: 1071, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 5.457, mean reward: 0.321 [0.219, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.848, 10.100], loss: 0.002569, mae: 0.055589, mean_q: 0.001488
 61031/100000: episode: 1072, duration: 0.231s, episode steps: 42, steps per second: 182, episode reward: 12.724, mean reward: 0.303 [0.132, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-1.489, 10.425], loss: 0.003567, mae: 0.063707, mean_q: 0.047948
 61072/100000: episode: 1073, duration: 0.223s, episode steps: 41, steps per second: 184, episode reward: 16.690, mean reward: 0.407 [0.286, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.402, 10.100], loss: 0.004500, mae: 0.064637, mean_q: 0.053793
 61114/100000: episode: 1074, duration: 0.233s, episode steps: 42, steps per second: 180, episode reward: 8.886, mean reward: 0.212 [0.014, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.571, 10.108], loss: 0.003071, mae: 0.058864, mean_q: 0.102794
 61156/100000: episode: 1075, duration: 0.224s, episode steps: 42, steps per second: 188, episode reward: 9.090, mean reward: 0.216 [0.075, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.549, 10.193], loss: 0.002596, mae: 0.054170, mean_q: 0.062310
 61198/100000: episode: 1076, duration: 0.244s, episode steps: 42, steps per second: 172, episode reward: 13.486, mean reward: 0.321 [0.147, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.800, 10.376], loss: 0.002589, mae: 0.054338, mean_q: 0.071312
 61239/100000: episode: 1077, duration: 0.217s, episode steps: 41, steps per second: 189, episode reward: 15.268, mean reward: 0.372 [0.143, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-1.025, 10.100], loss: 0.003052, mae: 0.059052, mean_q: 0.125015
 61281/100000: episode: 1078, duration: 0.232s, episode steps: 42, steps per second: 181, episode reward: 11.857, mean reward: 0.282 [0.154, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.467, 10.451], loss: 0.003025, mae: 0.056970, mean_q: 0.161334
 61316/100000: episode: 1079, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 15.358, mean reward: 0.439 [0.303, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.331, 10.100], loss: 0.002581, mae: 0.053911, mean_q: 0.135810
 61369/100000: episode: 1080, duration: 0.291s, episode steps: 53, steps per second: 182, episode reward: 10.632, mean reward: 0.201 [0.019, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.613, 10.100], loss: 0.002667, mae: 0.053736, mean_q: 0.130814
 61379/100000: episode: 1081, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 3.743, mean reward: 0.374 [0.323, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.403, 10.100], loss: 0.002919, mae: 0.058381, mean_q: 0.176931
 61422/100000: episode: 1082, duration: 0.221s, episode steps: 43, steps per second: 195, episode reward: 10.657, mean reward: 0.248 [0.066, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-1.096, 10.171], loss: 0.002847, mae: 0.056757, mean_q: 0.198787
 61479/100000: episode: 1083, duration: 0.297s, episode steps: 57, steps per second: 192, episode reward: 14.762, mean reward: 0.259 [0.141, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.840 [-0.319, 10.276], loss: 0.002786, mae: 0.055699, mean_q: 0.149671
 61520/100000: episode: 1084, duration: 0.225s, episode steps: 41, steps per second: 182, episode reward: 9.279, mean reward: 0.226 [0.131, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.915, 10.364], loss: 0.002691, mae: 0.055819, mean_q: 0.150714
 61562/100000: episode: 1085, duration: 0.239s, episode steps: 42, steps per second: 176, episode reward: 10.096, mean reward: 0.240 [0.083, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-1.543, 10.248], loss: 0.003512, mae: 0.059259, mean_q: 0.142677
 61595/100000: episode: 1086, duration: 0.185s, episode steps: 33, steps per second: 178, episode reward: 13.608, mean reward: 0.412 [0.293, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.354, 10.100], loss: 0.003303, mae: 0.061124, mean_q: 0.165966
 61652/100000: episode: 1087, duration: 0.313s, episode steps: 57, steps per second: 182, episode reward: 11.284, mean reward: 0.198 [0.022, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-0.402, 10.137], loss: 0.003693, mae: 0.062776, mean_q: 0.187134
 61709/100000: episode: 1088, duration: 0.310s, episode steps: 57, steps per second: 184, episode reward: 17.398, mean reward: 0.305 [0.213, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.849 [-0.489, 10.507], loss: 0.002761, mae: 0.056568, mean_q: 0.179970
 61766/100000: episode: 1089, duration: 0.310s, episode steps: 57, steps per second: 184, episode reward: 16.291, mean reward: 0.286 [0.088, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.841 [-0.932, 10.327], loss: 0.002583, mae: 0.054956, mean_q: 0.173718
 61819/100000: episode: 1090, duration: 0.304s, episode steps: 53, steps per second: 174, episode reward: 11.556, mean reward: 0.218 [0.032, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-0.316, 10.100], loss: 0.002815, mae: 0.056662, mean_q: 0.211711
 61876/100000: episode: 1091, duration: 0.322s, episode steps: 57, steps per second: 177, episode reward: 13.285, mean reward: 0.233 [0.055, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.841 [-0.722, 10.159], loss: 0.002881, mae: 0.056908, mean_q: 0.215433
 61917/100000: episode: 1092, duration: 0.211s, episode steps: 41, steps per second: 194, episode reward: 11.598, mean reward: 0.283 [0.137, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.682, 10.271], loss: 0.003287, mae: 0.060765, mean_q: 0.195302
 61958/100000: episode: 1093, duration: 0.235s, episode steps: 41, steps per second: 174, episode reward: 10.358, mean reward: 0.253 [0.065, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.785, 10.307], loss: 0.002892, mae: 0.058630, mean_q: 0.268560
 61968/100000: episode: 1094, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 3.474, mean reward: 0.347 [0.268, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.315, 10.100], loss: 0.003178, mae: 0.061166, mean_q: 0.330977
 62001/100000: episode: 1095, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 9.827, mean reward: 0.298 [0.174, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.211, 10.100], loss: 0.002969, mae: 0.058633, mean_q: 0.215009
 62036/100000: episode: 1096, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 12.542, mean reward: 0.358 [0.229, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.567, 10.100], loss: 0.002938, mae: 0.058637, mean_q: 0.248784
 62069/100000: episode: 1097, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 8.127, mean reward: 0.246 [0.143, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.636, 10.100], loss: 0.002714, mae: 0.056571, mean_q: 0.256535
 62086/100000: episode: 1098, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 3.905, mean reward: 0.230 [0.104, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.119, 10.100], loss: 0.002600, mae: 0.055009, mean_q: 0.255402
 62129/100000: episode: 1099, duration: 0.220s, episode steps: 43, steps per second: 196, episode reward: 11.200, mean reward: 0.260 [0.077, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.768, 10.309], loss: 0.002784, mae: 0.056457, mean_q: 0.276769
 62162/100000: episode: 1100, duration: 0.180s, episode steps: 33, steps per second: 184, episode reward: 10.393, mean reward: 0.315 [0.207, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.824, 10.100], loss: 0.002737, mae: 0.056207, mean_q: 0.287145
[Info] 200-TH LEVEL FOUND: 0.9265832901000977, Considering 10/90 traces
 62197/100000: episode: 1101, duration: 4.273s, episode steps: 35, steps per second: 8, episode reward: 12.279, mean reward: 0.351 [0.178, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.123, 10.100], loss: 0.002903, mae: 0.058914, mean_q: 0.341239
 62228/100000: episode: 1102, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 13.427, mean reward: 0.433 [0.253, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.212, 10.100], loss: 0.003092, mae: 0.060372, mean_q: 0.270350
 62255/100000: episode: 1103, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 13.532, mean reward: 0.501 [0.364, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-1.396, 10.100], loss: 0.002944, mae: 0.058686, mean_q: 0.290826
 62276/100000: episode: 1104, duration: 0.131s, episode steps: 21, steps per second: 160, episode reward: 10.475, mean reward: 0.499 [0.354, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.726, 10.100], loss: 0.002515, mae: 0.054143, mean_q: 0.277731
 62312/100000: episode: 1105, duration: 0.247s, episode steps: 36, steps per second: 146, episode reward: 11.970, mean reward: 0.332 [0.210, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.251, 10.100], loss: 0.002723, mae: 0.056842, mean_q: 0.265108
 62324/100000: episode: 1106, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 5.886, mean reward: 0.490 [0.354, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.275, 10.100], loss: 0.002871, mae: 0.057197, mean_q: 0.294650
 62352/100000: episode: 1107, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 12.812, mean reward: 0.458 [0.290, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.384, 10.100], loss: 0.002982, mae: 0.058021, mean_q: 0.291009
 62374/100000: episode: 1108, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 11.312, mean reward: 0.514 [0.424, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.468, 10.100], loss: 0.002674, mae: 0.056419, mean_q: 0.306657
 62393/100000: episode: 1109, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 7.978, mean reward: 0.420 [0.319, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.358, 10.100], loss: 0.002957, mae: 0.059714, mean_q: 0.315573
[Info] FALSIFICATION!
 62403/100000: episode: 1110, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 14.256, mean reward: 1.426 [0.334, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.546, 9.320], loss: 0.002692, mae: 0.056385, mean_q: 0.254099
 62503/100000: episode: 1111, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -20.607, mean reward: -0.206 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.705, 10.098], loss: 0.017174, mae: 0.076242, mean_q: 0.323469
 62603/100000: episode: 1112, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -12.669, mean reward: -0.127 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.238, 10.098], loss: 0.002889, mae: 0.058587, mean_q: 0.333088
 62703/100000: episode: 1113, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -12.752, mean reward: -0.128 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.963, 10.454], loss: 0.003278, mae: 0.059792, mean_q: 0.347490
 62803/100000: episode: 1114, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.598, mean reward: -0.176 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.168, 10.296], loss: 0.016839, mae: 0.073340, mean_q: 0.334746
 62903/100000: episode: 1115, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.087, mean reward: -0.161 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.865, 10.246], loss: 0.002519, mae: 0.055391, mean_q: 0.323629
 63003/100000: episode: 1116, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -10.263, mean reward: -0.103 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.622, 10.098], loss: 0.015166, mae: 0.059680, mean_q: 0.324013
 63103/100000: episode: 1117, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.603, mean reward: -0.166 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.254, 10.151], loss: 0.015612, mae: 0.064555, mean_q: 0.348105
 63203/100000: episode: 1118, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -16.625, mean reward: -0.166 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.879, 10.136], loss: 0.002777, mae: 0.057710, mean_q: 0.332044
 63303/100000: episode: 1119, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.133, mean reward: -0.171 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.855, 10.130], loss: 0.002749, mae: 0.056990, mean_q: 0.332108
 63403/100000: episode: 1120, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.897, mean reward: -0.189 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.645, 10.208], loss: 0.016009, mae: 0.067771, mean_q: 0.340361
 63503/100000: episode: 1121, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.156, mean reward: -0.172 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.580, 10.098], loss: 0.017064, mae: 0.073546, mean_q: 0.336847
 63603/100000: episode: 1122, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -13.176, mean reward: -0.132 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.728, 10.098], loss: 0.003707, mae: 0.061066, mean_q: 0.308286
 63703/100000: episode: 1123, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -9.333, mean reward: -0.093 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.628, 10.488], loss: 0.016086, mae: 0.064748, mean_q: 0.335695
 63803/100000: episode: 1124, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.295, mean reward: -0.193 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.562, 10.214], loss: 0.003253, mae: 0.062141, mean_q: 0.345369
 63903/100000: episode: 1125, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.644, mean reward: -0.176 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.818, 10.206], loss: 0.002498, mae: 0.054775, mean_q: 0.313146
 64003/100000: episode: 1126, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -5.944, mean reward: -0.059 [-1.000, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.847, 10.098], loss: 0.015784, mae: 0.067556, mean_q: 0.324154
 64103/100000: episode: 1127, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.068, mean reward: -0.171 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.718, 10.098], loss: 0.002739, mae: 0.056199, mean_q: 0.264333
 64203/100000: episode: 1128, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.076, mean reward: -0.171 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.225, 10.114], loss: 0.015313, mae: 0.061719, mean_q: 0.258917
 64303/100000: episode: 1129, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -9.133, mean reward: -0.091 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.578, 10.403], loss: 0.015693, mae: 0.064614, mean_q: 0.273816
 64403/100000: episode: 1130, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -16.622, mean reward: -0.166 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.420, 10.098], loss: 0.002660, mae: 0.055141, mean_q: 0.238767
 64503/100000: episode: 1131, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -16.757, mean reward: -0.168 [-1.000, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.389, 10.276], loss: 0.016010, mae: 0.065494, mean_q: 0.208917
 64603/100000: episode: 1132, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.075, mean reward: -0.171 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.213, 10.255], loss: 0.016461, mae: 0.070619, mean_q: 0.191404
 64703/100000: episode: 1133, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -19.122, mean reward: -0.191 [-1.000, 0.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.392, 10.098], loss: 0.015669, mae: 0.065908, mean_q: 0.196133
 64803/100000: episode: 1134, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -12.831, mean reward: -0.128 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.436, 10.533], loss: 0.015226, mae: 0.061815, mean_q: 0.160782
 64903/100000: episode: 1135, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -12.997, mean reward: -0.130 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.684, 10.315], loss: 0.002686, mae: 0.054783, mean_q: 0.152397
 65003/100000: episode: 1136, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -14.435, mean reward: -0.144 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.882, 10.176], loss: 0.015319, mae: 0.061996, mean_q: 0.110300
 65103/100000: episode: 1137, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -19.391, mean reward: -0.194 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.243, 10.098], loss: 0.002644, mae: 0.053059, mean_q: 0.100178
 65203/100000: episode: 1138, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -16.141, mean reward: -0.161 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.811, 10.098], loss: 0.027040, mae: 0.062611, mean_q: 0.111111
 65303/100000: episode: 1139, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -13.343, mean reward: -0.133 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.283, 10.257], loss: 0.002621, mae: 0.053653, mean_q: 0.078675
 65403/100000: episode: 1140, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -20.481, mean reward: -0.205 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.491, 10.105], loss: 0.002552, mae: 0.053031, mean_q: 0.041324
 65503/100000: episode: 1141, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -12.130, mean reward: -0.121 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.263, 10.554], loss: 0.002422, mae: 0.050879, mean_q: 0.051000
 65603/100000: episode: 1142, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -19.472, mean reward: -0.195 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.639, 10.112], loss: 0.002388, mae: 0.051082, mean_q: 0.025196
 65703/100000: episode: 1143, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.171, mean reward: -0.162 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.695, 10.209], loss: 0.002450, mae: 0.050782, mean_q: 0.002560
 65803/100000: episode: 1144, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.262, mean reward: -0.163 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.661, 10.098], loss: 0.015471, mae: 0.060781, mean_q: 0.011295
 65903/100000: episode: 1145, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -14.518, mean reward: -0.145 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.798, 10.139], loss: 0.015288, mae: 0.060530, mean_q: -0.030657
 66003/100000: episode: 1146, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.918, mean reward: -0.179 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.081, 10.098], loss: 0.015678, mae: 0.066853, mean_q: -0.014819
 66103/100000: episode: 1147, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.390, mean reward: -0.174 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.648, 10.156], loss: 0.002513, mae: 0.052108, mean_q: -0.065744
 66203/100000: episode: 1148, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -15.642, mean reward: -0.156 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.447, 10.277], loss: 0.002628, mae: 0.052024, mean_q: -0.081597
 66303/100000: episode: 1149, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.321, mean reward: -0.183 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.917, 10.098], loss: 0.015454, mae: 0.060279, mean_q: -0.072820
 66403/100000: episode: 1150, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -13.127, mean reward: -0.131 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.208, 10.098], loss: 0.015492, mae: 0.060203, mean_q: -0.104441
 66503/100000: episode: 1151, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -17.770, mean reward: -0.178 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.351, 10.304], loss: 0.014669, mae: 0.054346, mean_q: -0.142028
 66603/100000: episode: 1152, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.626, mean reward: -0.186 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.376, 10.098], loss: 0.015236, mae: 0.062248, mean_q: -0.138856
 66703/100000: episode: 1153, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.536, mean reward: -0.155 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.252, 10.261], loss: 0.002455, mae: 0.050030, mean_q: -0.167054
 66803/100000: episode: 1154, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.582, mean reward: -0.156 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.485, 10.174], loss: 0.015683, mae: 0.064620, mean_q: -0.175437
 66903/100000: episode: 1155, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.582, mean reward: -0.156 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.811, 10.098], loss: 0.015583, mae: 0.064051, mean_q: -0.196754
 67003/100000: episode: 1156, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -20.223, mean reward: -0.202 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.830, 10.098], loss: 0.002603, mae: 0.050882, mean_q: -0.212288
 67103/100000: episode: 1157, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.499, mean reward: -0.195 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.059, 10.098], loss: 0.027818, mae: 0.068548, mean_q: -0.229248
 67203/100000: episode: 1158, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.085, mean reward: -0.181 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.330, 10.098], loss: 0.002630, mae: 0.052874, mean_q: -0.224417
 67303/100000: episode: 1159, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.325, mean reward: -0.143 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.362, 10.098], loss: 0.002220, mae: 0.046919, mean_q: -0.324207
 67403/100000: episode: 1160, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -12.880, mean reward: -0.129 [-1.000, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.495, 10.098], loss: 0.015563, mae: 0.061708, mean_q: -0.298764
 67503/100000: episode: 1161, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.480, mean reward: -0.175 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.147, 10.173], loss: 0.002628, mae: 0.050458, mean_q: -0.281285
 67603/100000: episode: 1162, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.745, mean reward: -0.167 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.675, 10.292], loss: 0.002375, mae: 0.049302, mean_q: -0.285594
 67703/100000: episode: 1163, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.001, mean reward: -0.180 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.274, 10.291], loss: 0.002415, mae: 0.047906, mean_q: -0.305374
 67803/100000: episode: 1164, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.236, mean reward: -0.182 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.769, 10.098], loss: 0.002381, mae: 0.047934, mean_q: -0.277375
 67903/100000: episode: 1165, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.061, mean reward: -0.171 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.335, 10.442], loss: 0.002392, mae: 0.047905, mean_q: -0.302227
 68003/100000: episode: 1166, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.537, mean reward: -0.185 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.113, 10.098], loss: 0.002396, mae: 0.048381, mean_q: -0.299919
 68103/100000: episode: 1167, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.189, mean reward: -0.152 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.859, 10.098], loss: 0.002325, mae: 0.047285, mean_q: -0.326089
 68203/100000: episode: 1168, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.912, mean reward: -0.179 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.824, 10.098], loss: 0.002393, mae: 0.048379, mean_q: -0.278605
 68303/100000: episode: 1169, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.050, mean reward: -0.170 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.747, 10.240], loss: 0.002506, mae: 0.049230, mean_q: -0.259359
 68403/100000: episode: 1170, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.123, mean reward: -0.191 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.869, 10.178], loss: 0.002454, mae: 0.049154, mean_q: -0.311925
 68503/100000: episode: 1171, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.193, mean reward: -0.192 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.035, 10.143], loss: 0.003487, mae: 0.055481, mean_q: -0.277381
 68603/100000: episode: 1172, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.139, mean reward: -0.141 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.303, 10.431], loss: 0.005907, mae: 0.067260, mean_q: -0.288535
 68703/100000: episode: 1173, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -12.796, mean reward: -0.128 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.805, 10.098], loss: 0.003176, mae: 0.056679, mean_q: -0.311305
 68803/100000: episode: 1174, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.938, mean reward: -0.149 [-1.000, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.615, 10.098], loss: 0.002353, mae: 0.048796, mean_q: -0.319893
 68903/100000: episode: 1175, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -13.247, mean reward: -0.132 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.915, 10.098], loss: 0.002408, mae: 0.047636, mean_q: -0.343961
 69003/100000: episode: 1176, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.118, mean reward: -0.191 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.858, 10.117], loss: 0.002404, mae: 0.048976, mean_q: -0.302436
 69103/100000: episode: 1177, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -13.012, mean reward: -0.130 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.651, 10.137], loss: 0.002458, mae: 0.049179, mean_q: -0.315189
 69203/100000: episode: 1178, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.192, mean reward: -0.172 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.586, 10.215], loss: 0.002382, mae: 0.048991, mean_q: -0.319815
 69303/100000: episode: 1179, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -14.221, mean reward: -0.142 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.570, 10.367], loss: 0.002559, mae: 0.050663, mean_q: -0.275566
 69403/100000: episode: 1180, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.918, mean reward: -0.199 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.391, 10.098], loss: 0.002458, mae: 0.049106, mean_q: -0.338161
 69503/100000: episode: 1181, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.098, mean reward: -0.181 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.564, 10.098], loss: 0.002392, mae: 0.048213, mean_q: -0.328431
 69603/100000: episode: 1182, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -14.884, mean reward: -0.149 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.787, 10.098], loss: 0.002411, mae: 0.048412, mean_q: -0.294839
 69703/100000: episode: 1183, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -16.969, mean reward: -0.170 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.527, 10.194], loss: 0.002386, mae: 0.048034, mean_q: -0.319936
 69803/100000: episode: 1184, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.069, mean reward: -0.161 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.485, 10.098], loss: 0.002482, mae: 0.048571, mean_q: -0.315200
 69903/100000: episode: 1185, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -10.173, mean reward: -0.102 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.069, 10.098], loss: 0.002787, mae: 0.051516, mean_q: -0.316404
 70003/100000: episode: 1186, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.189, mean reward: -0.172 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.664, 10.098], loss: 0.002711, mae: 0.051575, mean_q: -0.288525
 70103/100000: episode: 1187, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: -14.587, mean reward: -0.146 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.398, 10.098], loss: 0.002554, mae: 0.050373, mean_q: -0.327569
 70203/100000: episode: 1188, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.834, mean reward: -0.158 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.512, 10.225], loss: 0.002949, mae: 0.056874, mean_q: -0.321621
 70303/100000: episode: 1189, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: -19.793, mean reward: -0.198 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.260, 10.098], loss: 0.002437, mae: 0.049138, mean_q: -0.350707
 70403/100000: episode: 1190, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.368, mean reward: -0.184 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.062, 10.098], loss: 0.002414, mae: 0.048929, mean_q: -0.314301
 70503/100000: episode: 1191, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -13.457, mean reward: -0.135 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.689, 10.098], loss: 0.002638, mae: 0.051127, mean_q: -0.322155
 70603/100000: episode: 1192, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -13.459, mean reward: -0.135 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.704, 10.098], loss: 0.002707, mae: 0.052571, mean_q: -0.296665
 70703/100000: episode: 1193, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.609, mean reward: -0.186 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.606, 10.208], loss: 0.002529, mae: 0.050117, mean_q: -0.319799
 70803/100000: episode: 1194, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.359, mean reward: -0.174 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.481, 10.105], loss: 0.002518, mae: 0.050592, mean_q: -0.287492
 70903/100000: episode: 1195, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.316, mean reward: -0.173 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.381, 10.194], loss: 0.003098, mae: 0.056454, mean_q: -0.320151
 71003/100000: episode: 1196, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -12.928, mean reward: -0.129 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.484, 10.531], loss: 0.004177, mae: 0.062897, mean_q: -0.335953
 71103/100000: episode: 1197, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -10.907, mean reward: -0.109 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.677, 10.098], loss: 0.002546, mae: 0.049983, mean_q: -0.297197
 71203/100000: episode: 1198, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.893, mean reward: -0.199 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.608, 10.139], loss: 0.002638, mae: 0.051575, mean_q: -0.270555
 71303/100000: episode: 1199, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -13.153, mean reward: -0.132 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.775, 10.098], loss: 0.002642, mae: 0.050104, mean_q: -0.322270
 71403/100000: episode: 1200, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -20.624, mean reward: -0.206 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.621, 10.119], loss: 0.002471, mae: 0.048774, mean_q: -0.324297
 71503/100000: episode: 1201, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.379, mean reward: -0.194 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.687, 10.210], loss: 0.002489, mae: 0.049430, mean_q: -0.343565
 71603/100000: episode: 1202, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.790, mean reward: -0.188 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.011, 10.098], loss: 0.002631, mae: 0.051243, mean_q: -0.305706
 71703/100000: episode: 1203, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.015, mean reward: -0.180 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.613, 10.098], loss: 0.002449, mae: 0.048931, mean_q: -0.308940
 71803/100000: episode: 1204, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -16.636, mean reward: -0.166 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.813, 10.219], loss: 0.002572, mae: 0.050085, mean_q: -0.319324
 71903/100000: episode: 1205, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.973, mean reward: -0.180 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.635, 10.098], loss: 0.002604, mae: 0.050147, mean_q: -0.309315
 72003/100000: episode: 1206, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -18.180, mean reward: -0.182 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.361, 10.098], loss: 0.002704, mae: 0.052077, mean_q: -0.303775
 72103/100000: episode: 1207, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -15.625, mean reward: -0.156 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.497, 10.098], loss: 0.002601, mae: 0.051274, mean_q: -0.281858
 72203/100000: episode: 1208, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -13.480, mean reward: -0.135 [-1.000, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.030, 10.098], loss: 0.002444, mae: 0.049774, mean_q: -0.314627
 72303/100000: episode: 1209, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.184, mean reward: -0.162 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.836, 10.317], loss: 0.002586, mae: 0.050554, mean_q: -0.301653
[Info] 100-TH LEVEL FOUND: 0.7405807971954346, Considering 10/90 traces
 72403/100000: episode: 1210, duration: 4.580s, episode steps: 100, steps per second: 22, episode reward: -13.816, mean reward: -0.138 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.274, 10.098], loss: 0.002475, mae: 0.049949, mean_q: -0.325357
 72411/100000: episode: 1211, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 3.238, mean reward: 0.405 [0.367, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.682, 10.470], loss: 0.002398, mae: 0.049535, mean_q: -0.340826
 72420/100000: episode: 1212, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 3.612, mean reward: 0.401 [0.320, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.430, 10.100], loss: 0.002073, mae: 0.046627, mean_q: -0.293844
 72428/100000: episode: 1213, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 3.409, mean reward: 0.426 [0.350, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.573, 10.433], loss: 0.002602, mae: 0.053372, mean_q: -0.245934
 72437/100000: episode: 1214, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 3.140, mean reward: 0.349 [0.310, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.372, 10.100], loss: 0.002649, mae: 0.051162, mean_q: -0.266534
 72451/100000: episode: 1215, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 5.066, mean reward: 0.362 [0.214, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.262, 10.100], loss: 0.002981, mae: 0.054283, mean_q: -0.300981
 72512/100000: episode: 1216, duration: 0.325s, episode steps: 61, steps per second: 188, episode reward: 21.658, mean reward: 0.355 [0.188, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.799 [-0.359, 10.435], loss: 0.002454, mae: 0.049671, mean_q: -0.284002
 72517/100000: episode: 1217, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 1.734, mean reward: 0.347 [0.311, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.422], loss: 0.003091, mae: 0.059412, mean_q: -0.307411
 72522/100000: episode: 1218, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 1.763, mean reward: 0.353 [0.325, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.463], loss: 0.002746, mae: 0.055118, mean_q: -0.258623
 72531/100000: episode: 1219, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 4.065, mean reward: 0.452 [0.361, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.483, 10.100], loss: 0.002779, mae: 0.054374, mean_q: -0.236991
 72545/100000: episode: 1220, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 6.468, mean reward: 0.462 [0.383, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.472, 10.100], loss: 0.002538, mae: 0.049300, mean_q: -0.281896
 72606/100000: episode: 1221, duration: 0.329s, episode steps: 61, steps per second: 185, episode reward: 12.797, mean reward: 0.210 [0.055, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.800 [-1.367, 10.189], loss: 0.002749, mae: 0.054449, mean_q: -0.214080
 72667/100000: episode: 1222, duration: 0.314s, episode steps: 61, steps per second: 194, episode reward: 13.272, mean reward: 0.218 [0.014, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.804 [-0.943, 10.100], loss: 0.002646, mae: 0.051472, mean_q: -0.286484
 72683/100000: episode: 1223, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 4.582, mean reward: 0.286 [0.216, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.392], loss: 0.002650, mae: 0.051618, mean_q: -0.313038
 72691/100000: episode: 1224, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 3.138, mean reward: 0.392 [0.307, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.541], loss: 0.002334, mae: 0.051107, mean_q: -0.206513
 72699/100000: episode: 1225, duration: 0.057s, episode steps: 8, steps per second: 141, episode reward: 3.286, mean reward: 0.411 [0.338, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.457], loss: 0.002507, mae: 0.052487, mean_q: -0.211644
 72736/100000: episode: 1226, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 12.292, mean reward: 0.332 [0.169, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.537, 10.100], loss: 0.002620, mae: 0.054019, mean_q: -0.222218
 72745/100000: episode: 1227, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 3.574, mean reward: 0.397 [0.315, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.345, 10.100], loss: 0.002768, mae: 0.056225, mean_q: -0.316647
 72753/100000: episode: 1228, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 3.219, mean reward: 0.402 [0.356, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.496], loss: 0.002998, mae: 0.055113, mean_q: -0.259400
 72767/100000: episode: 1229, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 5.902, mean reward: 0.422 [0.330, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.507, 10.100], loss: 0.002462, mae: 0.049709, mean_q: -0.366633
 72804/100000: episode: 1230, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 16.933, mean reward: 0.458 [0.325, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-1.203, 10.100], loss: 0.002730, mae: 0.054944, mean_q: -0.171210
 72841/100000: episode: 1231, duration: 0.197s, episode steps: 37, steps per second: 187, episode reward: 13.007, mean reward: 0.352 [0.084, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.739, 10.100], loss: 0.002416, mae: 0.050069, mean_q: -0.217266
 72857/100000: episode: 1232, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 6.759, mean reward: 0.422 [0.359, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.038, 10.468], loss: 0.002919, mae: 0.055524, mean_q: -0.246983
 72866/100000: episode: 1233, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 3.877, mean reward: 0.431 [0.369, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.484, 10.100], loss: 0.003490, mae: 0.058484, mean_q: -0.159697
 72871/100000: episode: 1234, duration: 0.036s, episode steps: 5, steps per second: 140, episode reward: 2.050, mean reward: 0.410 [0.330, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.462], loss: 0.002314, mae: 0.047078, mean_q: -0.303420
 72890/100000: episode: 1235, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 6.342, mean reward: 0.334 [0.251, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.302, 10.338], loss: 0.002775, mae: 0.053146, mean_q: -0.278080
 72951/100000: episode: 1236, duration: 0.345s, episode steps: 61, steps per second: 177, episode reward: 12.805, mean reward: 0.210 [0.017, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.800 [-0.319, 10.100], loss: 0.006355, mae: 0.073734, mean_q: -0.187584
 72959/100000: episode: 1237, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 2.864, mean reward: 0.358 [0.258, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.165, 10.468], loss: 0.009214, mae: 0.082573, mean_q: -0.205097
 72964/100000: episode: 1238, duration: 0.035s, episode steps: 5, steps per second: 141, episode reward: 2.098, mean reward: 0.420 [0.413, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.512], loss: 0.004098, mae: 0.071029, mean_q: -0.150610
 72978/100000: episode: 1239, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 5.149, mean reward: 0.368 [0.299, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.865, 10.100], loss: 0.003402, mae: 0.061626, mean_q: -0.145729
 72997/100000: episode: 1240, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 5.234, mean reward: 0.275 [0.161, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.838, 10.287], loss: 0.002591, mae: 0.054950, mean_q: -0.188347
 73012/100000: episode: 1241, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 5.155, mean reward: 0.344 [0.297, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.422], loss: 0.002796, mae: 0.056634, mean_q: -0.128777
 73026/100000: episode: 1242, duration: 0.090s, episode steps: 14, steps per second: 155, episode reward: 4.495, mean reward: 0.321 [0.264, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.325, 10.100], loss: 0.002515, mae: 0.051417, mean_q: -0.273951
 73042/100000: episode: 1243, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 6.655, mean reward: 0.416 [0.346, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.568], loss: 0.002357, mae: 0.049994, mean_q: -0.185720
 73061/100000: episode: 1244, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 6.792, mean reward: 0.357 [0.274, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.462], loss: 0.002747, mae: 0.054230, mean_q: -0.151181
 73082/100000: episode: 1245, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 7.928, mean reward: 0.378 [0.296, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.975, 10.404], loss: 0.002646, mae: 0.052382, mean_q: -0.217635
 73096/100000: episode: 1246, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 6.230, mean reward: 0.445 [0.346, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.446, 10.100], loss: 0.002704, mae: 0.053479, mean_q: -0.125287
 73110/100000: episode: 1247, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 6.288, mean reward: 0.449 [0.334, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.875, 10.100], loss: 0.002653, mae: 0.052602, mean_q: -0.174810
 73118/100000: episode: 1248, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 2.979, mean reward: 0.372 [0.322, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.488], loss: 0.002553, mae: 0.052160, mean_q: -0.254957
 73137/100000: episode: 1249, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 5.700, mean reward: 0.300 [0.207, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.111, 10.362], loss: 0.002814, mae: 0.054977, mean_q: -0.197659
 73142/100000: episode: 1250, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 2.404, mean reward: 0.481 [0.443, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.608], loss: 0.002579, mae: 0.051303, mean_q: -0.122985
 73156/100000: episode: 1251, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 5.214, mean reward: 0.372 [0.284, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.457, 10.100], loss: 0.002732, mae: 0.054042, mean_q: -0.124851
 73217/100000: episode: 1252, duration: 0.333s, episode steps: 61, steps per second: 183, episode reward: 14.086, mean reward: 0.231 [0.113, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.802 [-0.599, 10.234], loss: 0.002676, mae: 0.052435, mean_q: -0.178288
 73238/100000: episode: 1253, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 6.385, mean reward: 0.304 [0.205, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.360], loss: 0.002839, mae: 0.053460, mean_q: -0.230476
 73252/100000: episode: 1254, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 6.369, mean reward: 0.455 [0.346, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.312, 10.100], loss: 0.002896, mae: 0.056149, mean_q: -0.105148
 73268/100000: episode: 1255, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 5.759, mean reward: 0.360 [0.292, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.486], loss: 0.002375, mae: 0.049423, mean_q: -0.134196
 73289/100000: episode: 1256, duration: 0.111s, episode steps: 21, steps per second: 190, episode reward: 6.625, mean reward: 0.315 [0.225, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.280], loss: 0.002292, mae: 0.048099, mean_q: -0.169601
 73308/100000: episode: 1257, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 6.605, mean reward: 0.348 [0.304, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.466], loss: 0.002075, mae: 0.047655, mean_q: -0.081909
 73345/100000: episode: 1258, duration: 0.203s, episode steps: 37, steps per second: 182, episode reward: 18.634, mean reward: 0.504 [0.208, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.513, 10.100], loss: 0.002673, mae: 0.054163, mean_q: -0.085696
 73359/100000: episode: 1259, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 4.635, mean reward: 0.331 [0.278, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.369, 10.100], loss: 0.002379, mae: 0.048946, mean_q: -0.199968
 73374/100000: episode: 1260, duration: 0.084s, episode steps: 15, steps per second: 180, episode reward: 5.586, mean reward: 0.372 [0.318, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.557, 10.450], loss: 0.002811, mae: 0.051728, mean_q: -0.179105
 73411/100000: episode: 1261, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 14.090, mean reward: 0.381 [0.250, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.602, 10.100], loss: 0.002527, mae: 0.052426, mean_q: -0.107289
 73432/100000: episode: 1262, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 6.006, mean reward: 0.286 [0.187, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.208, 10.488], loss: 0.002714, mae: 0.053729, mean_q: -0.100055
 73469/100000: episode: 1263, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 10.072, mean reward: 0.272 [0.052, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.192, 10.100], loss: 0.002457, mae: 0.051209, mean_q: -0.102452
 73506/100000: episode: 1264, duration: 0.206s, episode steps: 37, steps per second: 179, episode reward: 13.910, mean reward: 0.376 [0.190, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.561, 10.100], loss: 0.002309, mae: 0.049878, mean_q: -0.140025
 73522/100000: episode: 1265, duration: 0.105s, episode steps: 16, steps per second: 153, episode reward: 6.282, mean reward: 0.393 [0.305, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.319, 10.552], loss: 0.002335, mae: 0.049914, mean_q: -0.167575
 73538/100000: episode: 1266, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 4.596, mean reward: 0.287 [0.172, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.375], loss: 0.002604, mae: 0.052798, mean_q: -0.129405
 73599/100000: episode: 1267, duration: 0.346s, episode steps: 61, steps per second: 176, episode reward: 21.390, mean reward: 0.351 [0.234, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.814 [-0.596, 10.457], loss: 0.002461, mae: 0.050517, mean_q: -0.107222
 73607/100000: episode: 1268, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 3.169, mean reward: 0.396 [0.345, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.732, 10.491], loss: 0.002963, mae: 0.057406, mean_q: -0.001623
 73623/100000: episode: 1269, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 5.882, mean reward: 0.368 [0.315, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.288, 10.511], loss: 0.003013, mae: 0.058032, mean_q: -0.017562
 73632/100000: episode: 1270, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 3.619, mean reward: 0.402 [0.363, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.419, 10.100], loss: 0.002368, mae: 0.050420, mean_q: -0.109036
 73647/100000: episode: 1271, duration: 0.094s, episode steps: 15, steps per second: 159, episode reward: 5.888, mean reward: 0.393 [0.278, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.513, 10.388], loss: 0.002829, mae: 0.053196, mean_q: -0.096366
 73663/100000: episode: 1272, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 5.523, mean reward: 0.345 [0.131, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.827, 10.259], loss: 0.002532, mae: 0.051206, mean_q: -0.029991
 73684/100000: episode: 1273, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 6.706, mean reward: 0.319 [0.234, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.226, 10.356], loss: 0.002504, mae: 0.050780, mean_q: -0.071678
 73745/100000: episode: 1274, duration: 0.347s, episode steps: 61, steps per second: 176, episode reward: 16.775, mean reward: 0.275 [0.045, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.790 [-0.674, 10.100], loss: 0.002529, mae: 0.052415, mean_q: -0.041757
 73753/100000: episode: 1275, duration: 0.055s, episode steps: 8, steps per second: 145, episode reward: 3.041, mean reward: 0.380 [0.322, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.450], loss: 0.003156, mae: 0.060372, mean_q: 0.058614
 73768/100000: episode: 1276, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 5.763, mean reward: 0.384 [0.303, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.684, 10.515], loss: 0.002643, mae: 0.053961, mean_q: -0.032864
 73776/100000: episode: 1277, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 2.634, mean reward: 0.329 [0.261, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.456], loss: 0.002899, mae: 0.055105, mean_q: -0.032798
 73792/100000: episode: 1278, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 5.332, mean reward: 0.333 [0.182, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.300], loss: 0.003271, mae: 0.058444, mean_q: 0.036202
 73797/100000: episode: 1279, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 1.866, mean reward: 0.373 [0.358, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.497], loss: 0.002329, mae: 0.048938, mean_q: -0.177589
 73802/100000: episode: 1280, duration: 0.035s, episode steps: 5, steps per second: 144, episode reward: 2.336, mean reward: 0.467 [0.438, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.513], loss: 0.002792, mae: 0.056631, mean_q: -0.040836
 73816/100000: episode: 1281, duration: 0.075s, episode steps: 14, steps per second: 188, episode reward: 5.748, mean reward: 0.411 [0.368, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.259, 10.100], loss: 0.002710, mae: 0.052542, mean_q: -0.054062
 73830/100000: episode: 1282, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 5.756, mean reward: 0.411 [0.340, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.538, 10.100], loss: 0.002775, mae: 0.057419, mean_q: 0.000371
 73849/100000: episode: 1283, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 6.246, mean reward: 0.329 [0.241, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.409], loss: 0.002717, mae: 0.056085, mean_q: 0.071992
 73886/100000: episode: 1284, duration: 0.204s, episode steps: 37, steps per second: 181, episode reward: 12.358, mean reward: 0.334 [0.193, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.364, 10.100], loss: 0.002441, mae: 0.051128, mean_q: -0.037528
 73905/100000: episode: 1285, duration: 0.113s, episode steps: 19, steps per second: 167, episode reward: 6.258, mean reward: 0.329 [0.250, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.411], loss: 0.002738, mae: 0.053725, mean_q: 0.078290
 73926/100000: episode: 1286, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 7.169, mean reward: 0.341 [0.246, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.325, 10.503], loss: 0.002809, mae: 0.053831, mean_q: 0.031474
 73940/100000: episode: 1287, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 5.381, mean reward: 0.384 [0.336, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.870, 10.100], loss: 0.002872, mae: 0.055743, mean_q: 0.034579
 73961/100000: episode: 1288, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 7.553, mean reward: 0.360 [0.306, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.405, 10.559], loss: 0.002689, mae: 0.052664, mean_q: -0.029613
 73976/100000: episode: 1289, duration: 0.102s, episode steps: 15, steps per second: 147, episode reward: 4.598, mean reward: 0.307 [0.190, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.366], loss: 0.002670, mae: 0.054370, mean_q: -0.021190
 74037/100000: episode: 1290, duration: 0.338s, episode steps: 61, steps per second: 180, episode reward: 15.680, mean reward: 0.257 [0.099, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.806 [-0.407, 10.258], loss: 0.002543, mae: 0.053200, mean_q: 0.031338
 74045/100000: episode: 1291, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 3.403, mean reward: 0.425 [0.408, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.511], loss: 0.002636, mae: 0.055360, mean_q: 0.022898
 74106/100000: episode: 1292, duration: 0.324s, episode steps: 61, steps per second: 188, episode reward: 17.047, mean reward: 0.279 [0.012, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.796 [-0.409, 10.207], loss: 0.002550, mae: 0.052037, mean_q: 0.034992
 74127/100000: episode: 1293, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 8.587, mean reward: 0.409 [0.327, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.382, 10.462], loss: 0.002588, mae: 0.054172, mean_q: 0.081420
 74188/100000: episode: 1294, duration: 0.347s, episode steps: 61, steps per second: 176, episode reward: 18.734, mean reward: 0.307 [0.146, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.810 [-0.438, 10.448], loss: 0.002611, mae: 0.052796, mean_q: 0.002418
 74203/100000: episode: 1295, duration: 0.099s, episode steps: 15, steps per second: 152, episode reward: 4.758, mean reward: 0.317 [0.263, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.727, 10.409], loss: 0.002691, mae: 0.052571, mean_q: -0.036612
 74264/100000: episode: 1296, duration: 0.349s, episode steps: 61, steps per second: 175, episode reward: 15.257, mean reward: 0.250 [0.069, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.794 [-0.872, 10.100], loss: 0.002754, mae: 0.054618, mean_q: 0.042690
 74280/100000: episode: 1297, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 6.070, mean reward: 0.379 [0.323, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.445, 10.440], loss: 0.002467, mae: 0.052032, mean_q: 0.051519
 74341/100000: episode: 1298, duration: 0.322s, episode steps: 61, steps per second: 189, episode reward: 14.403, mean reward: 0.236 [0.027, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.804 [-0.794, 10.100], loss: 0.002577, mae: 0.053264, mean_q: 0.057940
 74356/100000: episode: 1299, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 5.433, mean reward: 0.362 [0.300, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.364, 10.400], loss: 0.002328, mae: 0.051227, mean_q: 0.093117
[Info] 200-TH LEVEL FOUND: 0.907960057258606, Considering 10/90 traces
 74372/100000: episode: 1300, duration: 4.169s, episode steps: 16, steps per second: 4, episode reward: 5.010, mean reward: 0.313 [0.211, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.477, 10.420], loss: 0.002597, mae: 0.052701, mean_q: 0.037181
 74382/100000: episode: 1301, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 4.424, mean reward: 0.442 [0.360, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.445, 10.100], loss: 0.002638, mae: 0.052263, mean_q: 0.086867
 74417/100000: episode: 1302, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 8.677, mean reward: 0.248 [0.031, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.396, 10.156], loss: 0.002519, mae: 0.053476, mean_q: 0.089613
 74428/100000: episode: 1303, duration: 0.065s, episode steps: 11, steps per second: 171, episode reward: 5.428, mean reward: 0.493 [0.417, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.571, 10.100], loss: 0.003172, mae: 0.061425, mean_q: 0.086635
 74437/100000: episode: 1304, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 4.050, mean reward: 0.450 [0.385, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.323, 10.100], loss: 0.003394, mae: 0.059818, mean_q: 0.075072
 74473/100000: episode: 1305, duration: 0.205s, episode steps: 36, steps per second: 176, episode reward: 13.159, mean reward: 0.366 [0.183, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.256, 10.100], loss: 0.002599, mae: 0.053161, mean_q: 0.083696
 74506/100000: episode: 1306, duration: 0.189s, episode steps: 33, steps per second: 174, episode reward: 13.277, mean reward: 0.402 [0.312, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.581, 10.100], loss: 0.002439, mae: 0.051045, mean_q: 0.114071
 74541/100000: episode: 1307, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 14.099, mean reward: 0.403 [0.267, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-1.081, 10.100], loss: 0.002469, mae: 0.051948, mean_q: 0.112927
 74576/100000: episode: 1308, duration: 0.209s, episode steps: 35, steps per second: 167, episode reward: 9.936, mean reward: 0.284 [0.148, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.264, 10.100], loss: 0.002523, mae: 0.053733, mean_q: 0.120992
 74612/100000: episode: 1309, duration: 0.201s, episode steps: 36, steps per second: 179, episode reward: 15.027, mean reward: 0.417 [0.264, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.474, 10.100], loss: 0.002697, mae: 0.054538, mean_q: 0.130234
 74623/100000: episode: 1310, duration: 0.061s, episode steps: 11, steps per second: 182, episode reward: 5.324, mean reward: 0.484 [0.353, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.347, 10.100], loss: 0.002335, mae: 0.050014, mean_q: 0.166264
 74634/100000: episode: 1311, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 4.591, mean reward: 0.417 [0.294, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.530, 10.100], loss: 0.002634, mae: 0.055539, mean_q: 0.120014
 74670/100000: episode: 1312, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 15.780, mean reward: 0.438 [0.228, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.959, 10.100], loss: 0.002799, mae: 0.056875, mean_q: 0.198887
 74681/100000: episode: 1313, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 4.967, mean reward: 0.452 [0.376, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.555, 10.100], loss: 0.006671, mae: 0.069529, mean_q: 0.094957
 74717/100000: episode: 1314, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 15.289, mean reward: 0.425 [0.144, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.155, 10.100], loss: 0.005528, mae: 0.078448, mean_q: 0.139832
 74754/100000: episode: 1315, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 14.633, mean reward: 0.395 [0.289, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.561, 10.486], loss: 0.003082, mae: 0.061653, mean_q: 0.135318
 74765/100000: episode: 1316, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 4.523, mean reward: 0.411 [0.358, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.524, 10.100], loss: 0.003189, mae: 0.060542, mean_q: 0.208153
 74776/100000: episode: 1317, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 5.140, mean reward: 0.467 [0.420, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.462, 10.100], loss: 0.002537, mae: 0.052317, mean_q: 0.120766
 74811/100000: episode: 1318, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 12.721, mean reward: 0.363 [0.185, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.313, 10.100], loss: 0.002977, mae: 0.059048, mean_q: 0.155693
 74844/100000: episode: 1319, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 10.061, mean reward: 0.305 [0.004, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.273, 10.100], loss: 0.002636, mae: 0.054747, mean_q: 0.169258
 74880/100000: episode: 1320, duration: 0.201s, episode steps: 36, steps per second: 179, episode reward: 16.229, mean reward: 0.451 [0.304, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.476, 10.100], loss: 0.002460, mae: 0.053254, mean_q: 0.198716
 74913/100000: episode: 1321, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 9.677, mean reward: 0.293 [0.129, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.121, 10.100], loss: 0.002771, mae: 0.055062, mean_q: 0.177967
 74924/100000: episode: 1322, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 4.998, mean reward: 0.454 [0.410, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.514, 10.100], loss: 0.002347, mae: 0.050949, mean_q: 0.195064
 74935/100000: episode: 1323, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 5.085, mean reward: 0.462 [0.353, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.482, 10.100], loss: 0.002724, mae: 0.054809, mean_q: 0.200486
 74971/100000: episode: 1324, duration: 0.266s, episode steps: 36, steps per second: 135, episode reward: 10.976, mean reward: 0.305 [0.094, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.583, 10.100], loss: 0.002824, mae: 0.056376, mean_q: 0.184452
 74981/100000: episode: 1325, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 4.868, mean reward: 0.487 [0.333, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.540, 10.100], loss: 0.002375, mae: 0.051650, mean_q: 0.253885
 75014/100000: episode: 1326, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 11.136, mean reward: 0.337 [0.115, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.184, 10.100], loss: 0.002801, mae: 0.056028, mean_q: 0.230131
 75049/100000: episode: 1327, duration: 0.209s, episode steps: 35, steps per second: 167, episode reward: 9.014, mean reward: 0.258 [0.043, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.175, 10.100], loss: 0.002408, mae: 0.052157, mean_q: 0.222588
 75082/100000: episode: 1328, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 12.873, mean reward: 0.390 [0.299, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.263, 10.100], loss: 0.002741, mae: 0.055568, mean_q: 0.229149
 75092/100000: episode: 1329, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 4.676, mean reward: 0.468 [0.364, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.704, 10.100], loss: 0.002464, mae: 0.051528, mean_q: 0.190739
 75125/100000: episode: 1330, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 13.734, mean reward: 0.416 [0.267, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.313, 10.100], loss: 0.002698, mae: 0.054786, mean_q: 0.252519
 75135/100000: episode: 1331, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 4.150, mean reward: 0.415 [0.329, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.518, 10.100], loss: 0.003035, mae: 0.057332, mean_q: 0.280124
 75145/100000: episode: 1332, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 4.137, mean reward: 0.414 [0.344, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.278, 10.100], loss: 0.002923, mae: 0.056617, mean_q: 0.214533
 75180/100000: episode: 1333, duration: 0.211s, episode steps: 35, steps per second: 166, episode reward: 9.637, mean reward: 0.275 [0.027, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.084, 10.111], loss: 0.002499, mae: 0.054307, mean_q: 0.243663
 75216/100000: episode: 1334, duration: 0.206s, episode steps: 36, steps per second: 175, episode reward: 18.194, mean reward: 0.505 [0.384, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-1.307, 10.100], loss: 0.002805, mae: 0.057303, mean_q: 0.231272
 75251/100000: episode: 1335, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 15.516, mean reward: 0.443 [0.289, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.429, 10.100], loss: 0.002683, mae: 0.054648, mean_q: 0.249381
 75262/100000: episode: 1336, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 4.392, mean reward: 0.399 [0.334, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.280, 10.100], loss: 0.002948, mae: 0.056080, mean_q: 0.252596
 75271/100000: episode: 1337, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 4.745, mean reward: 0.527 [0.493, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.559, 10.100], loss: 0.003468, mae: 0.062716, mean_q: 0.285777
 75307/100000: episode: 1338, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 14.264, mean reward: 0.396 [0.227, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.662, 10.100], loss: 0.002736, mae: 0.055817, mean_q: 0.299155
 75344/100000: episode: 1339, duration: 0.211s, episode steps: 37, steps per second: 175, episode reward: 13.635, mean reward: 0.369 [0.234, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.496, 10.460], loss: 0.002764, mae: 0.055798, mean_q: 0.263970
 75380/100000: episode: 1340, duration: 0.191s, episode steps: 36, steps per second: 188, episode reward: 13.783, mean reward: 0.383 [0.077, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.992, 10.100], loss: 0.002681, mae: 0.056269, mean_q: 0.280901
 75389/100000: episode: 1341, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 4.793, mean reward: 0.533 [0.471, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.579, 10.100], loss: 0.002838, mae: 0.057643, mean_q: 0.311441
 75426/100000: episode: 1342, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 13.466, mean reward: 0.364 [0.163, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.162, 10.282], loss: 0.002630, mae: 0.055185, mean_q: 0.320365
 75461/100000: episode: 1343, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 15.417, mean reward: 0.440 [0.324, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-1.371, 10.100], loss: 0.002649, mae: 0.053796, mean_q: 0.241377
 75494/100000: episode: 1344, duration: 0.192s, episode steps: 33, steps per second: 172, episode reward: 10.692, mean reward: 0.324 [0.100, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.547, 10.100], loss: 0.002670, mae: 0.055931, mean_q: 0.354080
 75529/100000: episode: 1345, duration: 0.197s, episode steps: 35, steps per second: 177, episode reward: 11.462, mean reward: 0.327 [0.191, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.334, 10.100], loss: 0.002599, mae: 0.055082, mean_q: 0.355245
 75562/100000: episode: 1346, duration: 0.176s, episode steps: 33, steps per second: 188, episode reward: 9.203, mean reward: 0.279 [0.079, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.455, 10.100], loss: 0.002322, mae: 0.052231, mean_q: 0.318787
 75597/100000: episode: 1347, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 12.962, mean reward: 0.370 [0.259, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.293, 10.100], loss: 0.002729, mae: 0.056476, mean_q: 0.329436
 75633/100000: episode: 1348, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 15.006, mean reward: 0.417 [0.338, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.662, 10.100], loss: 0.002737, mae: 0.055798, mean_q: 0.322227
 75666/100000: episode: 1349, duration: 0.174s, episode steps: 33, steps per second: 189, episode reward: 10.406, mean reward: 0.315 [0.029, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.151, 10.104], loss: 0.002759, mae: 0.056590, mean_q: 0.361372
 75701/100000: episode: 1350, duration: 0.205s, episode steps: 35, steps per second: 171, episode reward: 9.098, mean reward: 0.260 [0.015, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.776, 10.227], loss: 0.002602, mae: 0.054253, mean_q: 0.297818
 75737/100000: episode: 1351, duration: 0.187s, episode steps: 36, steps per second: 193, episode reward: 13.260, mean reward: 0.368 [0.127, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.172, 10.203], loss: 0.002489, mae: 0.052874, mean_q: 0.362842
 75748/100000: episode: 1352, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 3.936, mean reward: 0.358 [0.323, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.440, 10.100], loss: 0.002640, mae: 0.054086, mean_q: 0.346667
 75758/100000: episode: 1353, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 4.154, mean reward: 0.415 [0.367, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.511, 10.100], loss: 0.002799, mae: 0.057425, mean_q: 0.413266
 75768/100000: episode: 1354, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 4.386, mean reward: 0.439 [0.386, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.576, 10.100], loss: 0.002104, mae: 0.048291, mean_q: 0.314573
 75804/100000: episode: 1355, duration: 0.192s, episode steps: 36, steps per second: 187, episode reward: 13.825, mean reward: 0.384 [0.233, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.716, 10.100], loss: 0.002738, mae: 0.056381, mean_q: 0.369607
 75813/100000: episode: 1356, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 4.025, mean reward: 0.447 [0.410, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.404, 10.100], loss: 0.002532, mae: 0.055637, mean_q: 0.326460
 75849/100000: episode: 1357, duration: 0.195s, episode steps: 36, steps per second: 185, episode reward: 14.066, mean reward: 0.391 [0.205, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.466, 10.100], loss: 0.002373, mae: 0.051363, mean_q: 0.367742
 75858/100000: episode: 1358, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 4.801, mean reward: 0.533 [0.494, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.491, 10.100], loss: 0.002837, mae: 0.056214, mean_q: 0.375440
 75868/100000: episode: 1359, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 4.046, mean reward: 0.405 [0.323, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.385, 10.100], loss: 0.002532, mae: 0.055419, mean_q: 0.396678
 75877/100000: episode: 1360, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 4.263, mean reward: 0.474 [0.412, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.419, 10.100], loss: 0.002907, mae: 0.058330, mean_q: 0.457899
 75913/100000: episode: 1361, duration: 0.202s, episode steps: 36, steps per second: 178, episode reward: 11.215, mean reward: 0.312 [0.093, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.316, 10.100], loss: 0.003009, mae: 0.059708, mean_q: 0.397081
 75948/100000: episode: 1362, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 9.262, mean reward: 0.265 [0.055, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.146, 10.100], loss: 0.002750, mae: 0.057053, mean_q: 0.380576
 75984/100000: episode: 1363, duration: 0.199s, episode steps: 36, steps per second: 181, episode reward: 11.370, mean reward: 0.316 [0.024, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.766, 10.216], loss: 0.002457, mae: 0.053637, mean_q: 0.415895
 76020/100000: episode: 1364, duration: 0.184s, episode steps: 36, steps per second: 195, episode reward: 13.867, mean reward: 0.385 [0.254, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.904, 10.100], loss: 0.002641, mae: 0.055365, mean_q: 0.446632
 76056/100000: episode: 1365, duration: 0.194s, episode steps: 36, steps per second: 185, episode reward: 10.896, mean reward: 0.303 [0.116, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.639, 10.100], loss: 0.002664, mae: 0.056173, mean_q: 0.406946
 76093/100000: episode: 1366, duration: 0.218s, episode steps: 37, steps per second: 170, episode reward: 12.912, mean reward: 0.349 [0.145, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.273, 10.280], loss: 0.002600, mae: 0.054864, mean_q: 0.403687
 76130/100000: episode: 1367, duration: 0.214s, episode steps: 37, steps per second: 173, episode reward: 12.941, mean reward: 0.350 [0.191, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.264, 10.278], loss: 0.002510, mae: 0.054547, mean_q: 0.476411
 76166/100000: episode: 1368, duration: 0.187s, episode steps: 36, steps per second: 193, episode reward: 12.477, mean reward: 0.347 [0.047, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.423, 10.100], loss: 0.002807, mae: 0.057509, mean_q: 0.434065
 76203/100000: episode: 1369, duration: 0.188s, episode steps: 37, steps per second: 196, episode reward: 15.284, mean reward: 0.413 [0.121, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.268, 10.246], loss: 0.002202, mae: 0.050785, mean_q: 0.421376
 76240/100000: episode: 1370, duration: 0.204s, episode steps: 37, steps per second: 181, episode reward: 8.985, mean reward: 0.243 [0.100, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.429, 10.237], loss: 0.002405, mae: 0.054073, mean_q: 0.464459
 76275/100000: episode: 1371, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 8.606, mean reward: 0.246 [0.058, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.337, 10.115], loss: 0.003223, mae: 0.061414, mean_q: 0.469583
 76311/100000: episode: 1372, duration: 0.197s, episode steps: 36, steps per second: 182, episode reward: 10.791, mean reward: 0.300 [0.061, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.866, 10.144], loss: 0.002508, mae: 0.054778, mean_q: 0.475728
 76346/100000: episode: 1373, duration: 0.204s, episode steps: 35, steps per second: 172, episode reward: 10.124, mean reward: 0.289 [0.113, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.458, 10.100], loss: 0.002509, mae: 0.054198, mean_q: 0.456583
 76356/100000: episode: 1374, duration: 0.066s, episode steps: 10, steps per second: 153, episode reward: 4.486, mean reward: 0.449 [0.367, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.388, 10.100], loss: 0.002702, mae: 0.053797, mean_q: 0.436557
 76389/100000: episode: 1375, duration: 0.181s, episode steps: 33, steps per second: 183, episode reward: 12.018, mean reward: 0.364 [0.216, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.388, 10.100], loss: 0.002672, mae: 0.056011, mean_q: 0.498133
 76422/100000: episode: 1376, duration: 0.176s, episode steps: 33, steps per second: 188, episode reward: 15.087, mean reward: 0.457 [0.349, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-1.403, 10.100], loss: 0.002660, mae: 0.055440, mean_q: 0.465173
 76457/100000: episode: 1377, duration: 0.196s, episode steps: 35, steps per second: 179, episode reward: 11.494, mean reward: 0.328 [0.183, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.291, 10.100], loss: 0.002636, mae: 0.056151, mean_q: 0.545987
 76466/100000: episode: 1378, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 3.840, mean reward: 0.427 [0.354, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.363, 10.100], loss: 0.002273, mae: 0.051070, mean_q: 0.512046
 76476/100000: episode: 1379, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 4.360, mean reward: 0.436 [0.363, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.636, 10.100], loss: 0.002346, mae: 0.053230, mean_q: 0.496635
 76512/100000: episode: 1380, duration: 0.203s, episode steps: 36, steps per second: 178, episode reward: 12.811, mean reward: 0.356 [0.190, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.177, 10.100], loss: 0.002641, mae: 0.055168, mean_q: 0.530555
 76523/100000: episode: 1381, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 5.190, mean reward: 0.472 [0.330, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.734, 10.100], loss: 0.002916, mae: 0.059148, mean_q: 0.512137
 76559/100000: episode: 1382, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 11.344, mean reward: 0.315 [0.175, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.653, 10.100], loss: 0.002558, mae: 0.055485, mean_q: 0.535422
 76568/100000: episode: 1383, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 4.375, mean reward: 0.486 [0.402, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.601, 10.100], loss: 0.002654, mae: 0.055486, mean_q: 0.507686
 76579/100000: episode: 1384, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 4.429, mean reward: 0.403 [0.320, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.520, 10.100], loss: 0.002882, mae: 0.059797, mean_q: 0.556904
 76589/100000: episode: 1385, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 4.625, mean reward: 0.462 [0.369, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.356, 10.100], loss: 0.002476, mae: 0.054584, mean_q: 0.567562
 76624/100000: episode: 1386, duration: 0.181s, episode steps: 35, steps per second: 194, episode reward: 11.714, mean reward: 0.335 [0.156, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.256, 10.100], loss: 0.002502, mae: 0.055511, mean_q: 0.521968
 76633/100000: episode: 1387, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 5.498, mean reward: 0.611 [0.509, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.512, 10.100], loss: 0.002769, mae: 0.060076, mean_q: 0.559225
 76669/100000: episode: 1388, duration: 0.187s, episode steps: 36, steps per second: 192, episode reward: 11.733, mean reward: 0.326 [0.223, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.294, 10.100], loss: 0.003805, mae: 0.064129, mean_q: 0.559637
 76705/100000: episode: 1389, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 11.009, mean reward: 0.306 [0.022, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.768, 10.138], loss: 0.004428, mae: 0.064357, mean_q: 0.567242
[Info] 300-TH LEVEL FOUND: 1.001251459121704, Considering 10/90 traces
 76741/100000: episode: 1390, duration: 4.280s, episode steps: 36, steps per second: 8, episode reward: 12.681, mean reward: 0.352 [0.157, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-1.634, 10.100], loss: 0.003017, mae: 0.060011, mean_q: 0.575359
 76774/100000: episode: 1391, duration: 0.186s, episode steps: 33, steps per second: 178, episode reward: 11.610, mean reward: 0.352 [0.088, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.035, 10.100], loss: 0.002560, mae: 0.055899, mean_q: 0.565985
 76809/100000: episode: 1392, duration: 0.194s, episode steps: 35, steps per second: 180, episode reward: 14.233, mean reward: 0.407 [0.262, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.384, 10.100], loss: 0.002830, mae: 0.057189, mean_q: 0.583455
 76844/100000: episode: 1393, duration: 0.185s, episode steps: 35, steps per second: 190, episode reward: 10.730, mean reward: 0.307 [0.067, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.678, 10.100], loss: 0.003395, mae: 0.059266, mean_q: 0.597624
 76874/100000: episode: 1394, duration: 0.179s, episode steps: 30, steps per second: 167, episode reward: 11.820, mean reward: 0.394 [0.240, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.648, 10.100], loss: 0.002757, mae: 0.058676, mean_q: 0.579039
 76892/100000: episode: 1395, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 8.100, mean reward: 0.450 [0.367, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.441, 10.100], loss: 0.003101, mae: 0.060123, mean_q: 0.604981
 76927/100000: episode: 1396, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 12.133, mean reward: 0.347 [0.095, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.171, 10.100], loss: 0.002868, mae: 0.058833, mean_q: 0.616801
 76955/100000: episode: 1397, duration: 0.169s, episode steps: 28, steps per second: 166, episode reward: 14.461, mean reward: 0.516 [0.397, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.428, 10.100], loss: 0.003618, mae: 0.061485, mean_q: 0.656273
 76982/100000: episode: 1398, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 8.951, mean reward: 0.332 [0.121, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.700, 10.100], loss: 0.002733, mae: 0.056066, mean_q: 0.641814
 77017/100000: episode: 1399, duration: 0.196s, episode steps: 35, steps per second: 179, episode reward: 15.057, mean reward: 0.430 [0.210, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.156, 10.100], loss: 0.003137, mae: 0.055875, mean_q: 0.640674
 77044/100000: episode: 1400, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 9.725, mean reward: 0.360 [0.193, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.366, 10.100], loss: 0.002845, mae: 0.058144, mean_q: 0.642556
 77077/100000: episode: 1401, duration: 0.184s, episode steps: 33, steps per second: 179, episode reward: 12.691, mean reward: 0.385 [0.282, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.306, 10.100], loss: 0.002723, mae: 0.057155, mean_q: 0.650734
 77095/100000: episode: 1402, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 7.310, mean reward: 0.406 [0.184, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.571, 10.100], loss: 0.002618, mae: 0.056699, mean_q: 0.621049
 77130/100000: episode: 1403, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 15.057, mean reward: 0.430 [0.333, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.237, 10.100], loss: 0.002712, mae: 0.057336, mean_q: 0.668815
 77165/100000: episode: 1404, duration: 0.200s, episode steps: 35, steps per second: 175, episode reward: 10.684, mean reward: 0.305 [0.036, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.142, 10.219], loss: 0.002627, mae: 0.056105, mean_q: 0.675515
 77198/100000: episode: 1405, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 12.592, mean reward: 0.382 [0.236, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.571, 10.100], loss: 0.002578, mae: 0.056244, mean_q: 0.671678
 77226/100000: episode: 1406, duration: 0.154s, episode steps: 28, steps per second: 181, episode reward: 9.446, mean reward: 0.337 [0.202, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.145, 10.100], loss: 0.002916, mae: 0.058502, mean_q: 0.665922
 77261/100000: episode: 1407, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 14.639, mean reward: 0.418 [0.274, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.468, 10.100], loss: 0.002775, mae: 0.059406, mean_q: 0.662403
 77291/100000: episode: 1408, duration: 0.172s, episode steps: 30, steps per second: 174, episode reward: 12.810, mean reward: 0.427 [0.306, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.214, 10.100], loss: 0.002605, mae: 0.056389, mean_q: 0.671460
 77324/100000: episode: 1409, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 14.489, mean reward: 0.439 [0.321, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.371, 10.100], loss: 0.002573, mae: 0.056084, mean_q: 0.678114
 77357/100000: episode: 1410, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 15.562, mean reward: 0.472 [0.342, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.967, 10.100], loss: 0.002667, mae: 0.057620, mean_q: 0.686882
 77392/100000: episode: 1411, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 11.606, mean reward: 0.332 [0.092, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.780, 10.100], loss: 0.002579, mae: 0.056117, mean_q: 0.669277
 77427/100000: episode: 1412, duration: 0.206s, episode steps: 35, steps per second: 170, episode reward: 14.211, mean reward: 0.406 [0.223, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.903, 10.100], loss: 0.002575, mae: 0.056575, mean_q: 0.673875
 77454/100000: episode: 1413, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 11.145, mean reward: 0.413 [0.281, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.370, 10.100], loss: 0.002533, mae: 0.055861, mean_q: 0.687748
 77472/100000: episode: 1414, duration: 0.127s, episode steps: 18, steps per second: 141, episode reward: 7.428, mean reward: 0.413 [0.247, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.381, 10.100], loss: 0.002824, mae: 0.058274, mean_q: 0.677273
 77505/100000: episode: 1415, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 11.816, mean reward: 0.358 [0.038, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.488, 10.210], loss: 0.002865, mae: 0.059605, mean_q: 0.678236
 77540/100000: episode: 1416, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 14.270, mean reward: 0.408 [0.186, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.652, 10.100], loss: 0.002725, mae: 0.058229, mean_q: 0.681559
 77567/100000: episode: 1417, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 9.020, mean reward: 0.334 [0.136, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.213, 10.100], loss: 0.002911, mae: 0.059998, mean_q: 0.686287
 77585/100000: episode: 1418, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 9.765, mean reward: 0.543 [0.428, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.419, 10.100], loss: 0.002737, mae: 0.058608, mean_q: 0.682302
 77619/100000: episode: 1419, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 14.468, mean reward: 0.426 [0.326, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-1.589, 10.434], loss: 0.002460, mae: 0.055258, mean_q: 0.684862
 77652/100000: episode: 1420, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 14.870, mean reward: 0.451 [0.304, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.902, 10.100], loss: 0.002348, mae: 0.054299, mean_q: 0.680392
 77679/100000: episode: 1421, duration: 0.153s, episode steps: 27, steps per second: 177, episode reward: 12.668, mean reward: 0.469 [0.357, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.293, 10.100], loss: 0.002174, mae: 0.051607, mean_q: 0.686018
 77697/100000: episode: 1422, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 9.121, mean reward: 0.507 [0.441, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.627, 10.100], loss: 0.002281, mae: 0.054258, mean_q: 0.692108
 77732/100000: episode: 1423, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 19.246, mean reward: 0.550 [0.458, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.561, 10.100], loss: 0.002636, mae: 0.056908, mean_q: 0.687844
 77759/100000: episode: 1424, duration: 0.159s, episode steps: 27, steps per second: 169, episode reward: 12.693, mean reward: 0.470 [0.353, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.509, 10.100], loss: 0.002761, mae: 0.056900, mean_q: 0.683831
 77789/100000: episode: 1425, duration: 0.155s, episode steps: 30, steps per second: 193, episode reward: 8.523, mean reward: 0.284 [0.054, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.179, 10.100], loss: 0.002550, mae: 0.054630, mean_q: 0.695077
 77816/100000: episode: 1426, duration: 0.163s, episode steps: 27, steps per second: 165, episode reward: 10.381, mean reward: 0.384 [0.219, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.219, 10.100], loss: 0.002737, mae: 0.057801, mean_q: 0.694119
[Info] FALSIFICATION!
 77838/100000: episode: 1427, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 18.886, mean reward: 0.858 [0.241, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-1.010, 9.695], loss: 0.003177, mae: 0.064499, mean_q: 0.694530
 77938/100000: episode: 1428, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -14.292, mean reward: -0.143 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.778, 10.110], loss: 0.002757, mae: 0.057523, mean_q: 0.675671
 78038/100000: episode: 1429, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.637, mean reward: -0.156 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.761, 10.320], loss: 0.002576, mae: 0.055665, mean_q: 0.655070
 78138/100000: episode: 1430, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -9.138, mean reward: -0.091 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.689, 10.173], loss: 0.017449, mae: 0.065778, mean_q: 0.629238
 78238/100000: episode: 1431, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.980, mean reward: -0.160 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.381, 10.098], loss: 0.018556, mae: 0.076754, mean_q: 0.634063
 78338/100000: episode: 1432, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.556, mean reward: -0.166 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.255, 10.098], loss: 0.031971, mae: 0.076793, mean_q: 0.613816
 78438/100000: episode: 1433, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.191, mean reward: -0.162 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.156, 10.098], loss: 0.002930, mae: 0.055987, mean_q: 0.578437
 78538/100000: episode: 1434, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.155, mean reward: -0.162 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.460, 10.220], loss: 0.002768, mae: 0.056005, mean_q: 0.564919
 78638/100000: episode: 1435, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.770, mean reward: -0.168 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.529, 10.420], loss: 0.017476, mae: 0.068145, mean_q: 0.550850
 78738/100000: episode: 1436, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.620, mean reward: -0.186 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.822, 10.113], loss: 0.002588, mae: 0.055179, mean_q: 0.530800
 78838/100000: episode: 1437, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.934, mean reward: -0.189 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.346, 10.098], loss: 0.003149, mae: 0.057359, mean_q: 0.491394
 78938/100000: episode: 1438, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -16.970, mean reward: -0.170 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.724, 10.183], loss: 0.017436, mae: 0.067280, mean_q: 0.476047
 79038/100000: episode: 1439, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.920, mean reward: -0.179 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.693, 10.194], loss: 0.002394, mae: 0.053920, mean_q: 0.474220
 79138/100000: episode: 1440, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -18.627, mean reward: -0.186 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.226, 10.098], loss: 0.002219, mae: 0.051231, mean_q: 0.430462
 79238/100000: episode: 1441, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.974, mean reward: -0.180 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.843, 10.212], loss: 0.017272, mae: 0.066635, mean_q: 0.428535
 79338/100000: episode: 1442, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.067, mean reward: -0.171 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.140, 10.205], loss: 0.016238, mae: 0.057333, mean_q: 0.436157
 79438/100000: episode: 1443, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.529, mean reward: -0.195 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.983, 10.098], loss: 0.017410, mae: 0.069365, mean_q: 0.395645
 79538/100000: episode: 1444, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -18.530, mean reward: -0.185 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.610, 10.270], loss: 0.030808, mae: 0.067807, mean_q: 0.369196
 79638/100000: episode: 1445, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.932, mean reward: -0.169 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.282, 10.102], loss: 0.016621, mae: 0.060984, mean_q: 0.345459
 79738/100000: episode: 1446, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -11.454, mean reward: -0.115 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.806, 10.199], loss: 0.016526, mae: 0.060056, mean_q: 0.339887
 79838/100000: episode: 1447, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.191, mean reward: -0.152 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.101, 10.098], loss: 0.016357, mae: 0.057847, mean_q: 0.347409
 79938/100000: episode: 1448, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.394, mean reward: -0.184 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.895, 10.247], loss: 0.002396, mae: 0.052574, mean_q: 0.256068
 80038/100000: episode: 1449, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.709, mean reward: -0.177 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.639, 10.277], loss: 0.002339, mae: 0.051018, mean_q: 0.253185
 80138/100000: episode: 1450, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.967, mean reward: -0.160 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.570, 10.098], loss: 0.016190, mae: 0.055279, mean_q: 0.211284
 80238/100000: episode: 1451, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -21.297, mean reward: -0.213 [-1.000, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.377, 10.098], loss: 0.002536, mae: 0.053314, mean_q: 0.207425
 80338/100000: episode: 1452, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.974, mean reward: -0.140 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.082, 10.238], loss: 0.002285, mae: 0.050859, mean_q: 0.202203
 80438/100000: episode: 1453, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -13.802, mean reward: -0.138 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.705, 10.098], loss: 0.016833, mae: 0.062539, mean_q: 0.194872
 80538/100000: episode: 1454, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -15.923, mean reward: -0.159 [-1.000, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.583, 10.297], loss: 0.002708, mae: 0.054828, mean_q: 0.171834
 80638/100000: episode: 1455, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -20.803, mean reward: -0.208 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.523, 10.211], loss: 0.002300, mae: 0.049753, mean_q: 0.133362
 80738/100000: episode: 1456, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -5.878, mean reward: -0.059 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.024, 10.098], loss: 0.002269, mae: 0.050299, mean_q: 0.062830
 80838/100000: episode: 1457, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -16.810, mean reward: -0.168 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.792, 10.253], loss: 0.002401, mae: 0.050780, mean_q: 0.121777
 80938/100000: episode: 1458, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.054, mean reward: -0.161 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.756, 10.465], loss: 0.002358, mae: 0.050225, mean_q: 0.096278
 81038/100000: episode: 1459, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.825, mean reward: -0.178 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.279, 10.098], loss: 0.002349, mae: 0.050348, mean_q: 0.098350
 81138/100000: episode: 1460, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.811, mean reward: -0.188 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.609, 10.098], loss: 0.002569, mae: 0.052358, mean_q: 0.096207
 81238/100000: episode: 1461, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.247, mean reward: -0.172 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.777, 10.198], loss: 0.016782, mae: 0.059820, mean_q: 0.044146
 81338/100000: episode: 1462, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.904, mean reward: -0.189 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.888, 10.105], loss: 0.002525, mae: 0.052624, mean_q: 0.002503
 81438/100000: episode: 1463, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.359, mean reward: -0.184 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.115, 10.098], loss: 0.030325, mae: 0.063423, mean_q: -0.039197
 81538/100000: episode: 1464, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -11.431, mean reward: -0.114 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.487, 10.439], loss: 0.002456, mae: 0.050861, mean_q: -0.011240
 81638/100000: episode: 1465, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -12.194, mean reward: -0.122 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.700, 10.098], loss: 0.002405, mae: 0.049873, mean_q: -0.044170
 81738/100000: episode: 1466, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.092, mean reward: -0.171 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.733, 10.098], loss: 0.002475, mae: 0.050354, mean_q: -0.101935
 81838/100000: episode: 1467, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.583, mean reward: -0.176 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.702, 10.203], loss: 0.017002, mae: 0.061528, mean_q: -0.104829
 81938/100000: episode: 1468, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -20.217, mean reward: -0.202 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.222, 10.184], loss: 0.002483, mae: 0.051071, mean_q: -0.134317
 82038/100000: episode: 1469, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -11.622, mean reward: -0.116 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.812, 10.098], loss: 0.002467, mae: 0.050309, mean_q: -0.141893
 82138/100000: episode: 1470, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.304, mean reward: -0.183 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.586, 10.279], loss: 0.002500, mae: 0.050046, mean_q: -0.159629
 82238/100000: episode: 1471, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -11.528, mean reward: -0.115 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.845, 10.558], loss: 0.018062, mae: 0.066377, mean_q: -0.172276
 82338/100000: episode: 1472, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.020, mean reward: -0.150 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.813, 10.117], loss: 0.017479, mae: 0.067955, mean_q: -0.202130
 82438/100000: episode: 1473, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -15.075, mean reward: -0.151 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.564, 10.098], loss: 0.002416, mae: 0.049554, mean_q: -0.194633
 82538/100000: episode: 1474, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -19.746, mean reward: -0.197 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.330, 10.308], loss: 0.030863, mae: 0.068435, mean_q: -0.234604
 82638/100000: episode: 1475, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -9.750, mean reward: -0.098 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.619, 10.496], loss: 0.017198, mae: 0.064687, mean_q: -0.259614
 82738/100000: episode: 1476, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -20.818, mean reward: -0.208 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.159, 10.098], loss: 0.016853, mae: 0.061273, mean_q: -0.306531
 82838/100000: episode: 1477, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.394, mean reward: -0.134 [-1.000, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.961, 10.098], loss: 0.002355, mae: 0.048873, mean_q: -0.289665
 82938/100000: episode: 1478, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -16.593, mean reward: -0.166 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.061, 10.196], loss: 0.002339, mae: 0.047726, mean_q: -0.276516
 83038/100000: episode: 1479, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.784, mean reward: -0.158 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.378, 10.098], loss: 0.002325, mae: 0.046804, mean_q: -0.310370
 83138/100000: episode: 1480, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.033, mean reward: -0.170 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.599, 10.098], loss: 0.002402, mae: 0.048297, mean_q: -0.292983
 83238/100000: episode: 1481, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -13.873, mean reward: -0.139 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.607, 10.332], loss: 0.002467, mae: 0.048696, mean_q: -0.298543
 83338/100000: episode: 1482, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -8.854, mean reward: -0.089 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.627, 10.098], loss: 0.002290, mae: 0.046900, mean_q: -0.308454
 83438/100000: episode: 1483, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.575, mean reward: -0.176 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.727, 10.278], loss: 0.002500, mae: 0.048805, mean_q: -0.293978
 83538/100000: episode: 1484, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -20.215, mean reward: -0.202 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.531, 10.197], loss: 0.002190, mae: 0.046615, mean_q: -0.305294
 83638/100000: episode: 1485, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -9.682, mean reward: -0.097 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.582, 10.228], loss: 0.002444, mae: 0.049031, mean_q: -0.281630
 83738/100000: episode: 1486, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.747, mean reward: -0.167 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.306, 10.098], loss: 0.002344, mae: 0.047572, mean_q: -0.318186
 83838/100000: episode: 1487, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.869, mean reward: -0.169 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.174, 10.360], loss: 0.002510, mae: 0.049208, mean_q: -0.291099
 83938/100000: episode: 1488, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.579, mean reward: -0.176 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.151, 10.098], loss: 0.002311, mae: 0.048080, mean_q: -0.309544
 84038/100000: episode: 1489, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.370, mean reward: -0.174 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.722, 10.331], loss: 0.002524, mae: 0.049187, mean_q: -0.317590
 84138/100000: episode: 1490, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.966, mean reward: -0.170 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.814, 10.098], loss: 0.002524, mae: 0.050089, mean_q: -0.298292
 84238/100000: episode: 1491, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -14.813, mean reward: -0.148 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.201, 10.392], loss: 0.002409, mae: 0.048835, mean_q: -0.297108
 84338/100000: episode: 1492, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.957, mean reward: -0.180 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.464, 10.239], loss: 0.002317, mae: 0.047847, mean_q: -0.302907
 84438/100000: episode: 1493, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -19.262, mean reward: -0.193 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.031, 10.228], loss: 0.002186, mae: 0.045982, mean_q: -0.295889
 84538/100000: episode: 1494, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.356, mean reward: -0.174 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.329, 10.386], loss: 0.002620, mae: 0.051118, mean_q: -0.273797
 84638/100000: episode: 1495, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -11.967, mean reward: -0.120 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.715, 10.098], loss: 0.002390, mae: 0.048744, mean_q: -0.286113
 84738/100000: episode: 1496, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.991, mean reward: -0.160 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.474, 10.147], loss: 0.002543, mae: 0.050326, mean_q: -0.303876
 84838/100000: episode: 1497, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.557, mean reward: -0.186 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.184, 10.137], loss: 0.002452, mae: 0.049031, mean_q: -0.273076
 84938/100000: episode: 1498, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.389, mean reward: -0.164 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.928, 10.211], loss: 0.002459, mae: 0.049342, mean_q: -0.328235
 85038/100000: episode: 1499, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -13.756, mean reward: -0.138 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.600, 10.098], loss: 0.002478, mae: 0.049677, mean_q: -0.282310
 85138/100000: episode: 1500, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -10.302, mean reward: -0.103 [-1.000, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.688, 10.533], loss: 0.004992, mae: 0.066957, mean_q: -0.304257
 85238/100000: episode: 1501, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -12.552, mean reward: -0.126 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.598, 10.098], loss: 0.003206, mae: 0.058051, mean_q: -0.305800
 85338/100000: episode: 1502, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -14.634, mean reward: -0.146 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.662, 10.327], loss: 0.002647, mae: 0.051404, mean_q: -0.309067
 85438/100000: episode: 1503, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.290, mean reward: -0.183 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.140, 10.119], loss: 0.002536, mae: 0.050070, mean_q: -0.273670
 85538/100000: episode: 1504, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -9.848, mean reward: -0.098 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.808, 10.098], loss: 0.002509, mae: 0.049573, mean_q: -0.308768
 85638/100000: episode: 1505, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.452, mean reward: -0.155 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.162, 10.098], loss: 0.002342, mae: 0.048053, mean_q: -0.292830
 85738/100000: episode: 1506, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -13.172, mean reward: -0.132 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.360, 10.098], loss: 0.002381, mae: 0.048143, mean_q: -0.287087
 85838/100000: episode: 1507, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -15.012, mean reward: -0.150 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.971, 10.223], loss: 0.002467, mae: 0.048916, mean_q: -0.298325
 85938/100000: episode: 1508, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.511, mean reward: -0.155 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.870, 10.098], loss: 0.002577, mae: 0.050537, mean_q: -0.273120
 86038/100000: episode: 1509, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.711, mean reward: -0.187 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.796, 10.181], loss: 0.002358, mae: 0.047805, mean_q: -0.335903
 86138/100000: episode: 1510, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.267, mean reward: -0.173 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.244, 10.248], loss: 0.002416, mae: 0.049053, mean_q: -0.309056
 86238/100000: episode: 1511, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.691, mean reward: -0.187 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.098, 10.138], loss: 0.002499, mae: 0.049997, mean_q: -0.278255
 86338/100000: episode: 1512, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -20.217, mean reward: -0.202 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.619, 10.098], loss: 0.002535, mae: 0.050121, mean_q: -0.268794
 86438/100000: episode: 1513, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -14.106, mean reward: -0.141 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-0.767, 10.098], loss: 0.002227, mae: 0.046147, mean_q: -0.328704
 86538/100000: episode: 1514, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.866, mean reward: -0.189 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.053, 10.098], loss: 0.002580, mae: 0.050776, mean_q: -0.244313
 86638/100000: episode: 1515, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.368, mean reward: -0.164 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.852, 10.144], loss: 0.002480, mae: 0.052425, mean_q: -0.305827
 86738/100000: episode: 1516, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.357, mean reward: -0.194 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.447, 10.239], loss: 0.002539, mae: 0.049452, mean_q: -0.294810
 86838/100000: episode: 1517, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.453, mean reward: -0.205 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.621, 10.098], loss: 0.002296, mae: 0.047709, mean_q: -0.280700
 86938/100000: episode: 1518, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.628, mean reward: -0.176 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.935, 10.129], loss: 0.002363, mae: 0.047502, mean_q: -0.306059
 87038/100000: episode: 1519, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.406, mean reward: -0.164 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.056, 10.261], loss: 0.002445, mae: 0.048786, mean_q: -0.300584
 87138/100000: episode: 1520, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.641, mean reward: -0.146 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.730, 10.298], loss: 0.002218, mae: 0.047051, mean_q: -0.304163
 87238/100000: episode: 1521, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -15.669, mean reward: -0.157 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.985, 10.098], loss: 0.002338, mae: 0.047798, mean_q: -0.302392
 87338/100000: episode: 1522, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.209, mean reward: -0.172 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.914, 10.098], loss: 0.002291, mae: 0.047569, mean_q: -0.311480
 87438/100000: episode: 1523, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.555, mean reward: -0.176 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.868, 10.098], loss: 0.002318, mae: 0.047577, mean_q: -0.331979
 87538/100000: episode: 1524, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: -15.930, mean reward: -0.159 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.294, 10.183], loss: 0.002347, mae: 0.048763, mean_q: -0.316239
 87638/100000: episode: 1525, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -14.703, mean reward: -0.147 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.119, 10.098], loss: 0.002344, mae: 0.047488, mean_q: -0.311240
 87738/100000: episode: 1526, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.755, mean reward: -0.168 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.980, 10.098], loss: 0.002316, mae: 0.047619, mean_q: -0.300609
[Info] 100-TH LEVEL FOUND: 0.7102153897285461, Considering 10/90 traces
 87838/100000: episode: 1527, duration: 4.590s, episode steps: 100, steps per second: 22, episode reward: -15.779, mean reward: -0.158 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.999, 10.098], loss: 0.002308, mae: 0.047354, mean_q: -0.290189
 87861/100000: episode: 1528, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 11.933, mean reward: 0.519 [0.388, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.595, 10.603], loss: 0.002229, mae: 0.046304, mean_q: -0.356792
 87876/100000: episode: 1529, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 5.511, mean reward: 0.367 [0.247, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.407, 10.100], loss: 0.002197, mae: 0.047935, mean_q: -0.273338
 87916/100000: episode: 1530, duration: 0.212s, episode steps: 40, steps per second: 189, episode reward: 10.149, mean reward: 0.254 [0.039, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.633, 10.157], loss: 0.002028, mae: 0.044525, mean_q: -0.302960
 87939/100000: episode: 1531, duration: 0.146s, episode steps: 23, steps per second: 158, episode reward: 7.691, mean reward: 0.334 [0.194, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.898, 10.300], loss: 0.002131, mae: 0.046829, mean_q: -0.255770
 87972/100000: episode: 1532, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 11.191, mean reward: 0.339 [0.222, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.427, 10.100], loss: 0.002769, mae: 0.055114, mean_q: -0.228930
 87989/100000: episode: 1533, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 6.691, mean reward: 0.394 [0.322, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.586, 10.491], loss: 0.002243, mae: 0.049026, mean_q: -0.215205
 88000/100000: episode: 1534, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 3.692, mean reward: 0.336 [0.250, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.478], loss: 0.003143, mae: 0.056135, mean_q: -0.321046
 88015/100000: episode: 1535, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 5.249, mean reward: 0.350 [0.261, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.243, 10.100], loss: 0.002967, mae: 0.054818, mean_q: -0.243882
 88064/100000: episode: 1536, duration: 0.267s, episode steps: 49, steps per second: 183, episode reward: 13.162, mean reward: 0.269 [0.104, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.891, 10.315], loss: 0.002109, mae: 0.047682, mean_q: -0.271271
 88097/100000: episode: 1537, duration: 0.189s, episode steps: 33, steps per second: 175, episode reward: 9.765, mean reward: 0.296 [0.113, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.903, 10.100], loss: 0.002044, mae: 0.045953, mean_q: -0.272741
 88112/100000: episode: 1538, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 5.435, mean reward: 0.362 [0.217, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.123, 10.100], loss: 0.002531, mae: 0.051692, mean_q: -0.246242
 88121/100000: episode: 1539, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 2.570, mean reward: 0.286 [0.249, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.370], loss: 0.002044, mae: 0.045243, mean_q: -0.240162
 88144/100000: episode: 1540, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 7.397, mean reward: 0.322 [0.240, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.566, 10.394], loss: 0.002150, mae: 0.045594, mean_q: -0.220499
 88190/100000: episode: 1541, duration: 0.260s, episode steps: 46, steps per second: 177, episode reward: 17.042, mean reward: 0.370 [0.169, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.257, 10.328], loss: 0.002257, mae: 0.048211, mean_q: -0.225515
 88239/100000: episode: 1542, duration: 0.289s, episode steps: 49, steps per second: 170, episode reward: 15.689, mean reward: 0.320 [0.008, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.483, 10.196], loss: 0.004107, mae: 0.061378, mean_q: -0.236356
 88262/100000: episode: 1543, duration: 0.125s, episode steps: 23, steps per second: 185, episode reward: 7.282, mean reward: 0.317 [0.119, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.226, 10.217], loss: 0.002967, mae: 0.060850, mean_q: -0.200925
 88273/100000: episode: 1544, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 3.888, mean reward: 0.353 [0.327, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.456], loss: 0.003025, mae: 0.054701, mean_q: -0.189453
 88322/100000: episode: 1545, duration: 0.264s, episode steps: 49, steps per second: 186, episode reward: 8.486, mean reward: 0.173 [0.027, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.506, 10.157], loss: 0.002990, mae: 0.055098, mean_q: -0.197254
 88362/100000: episode: 1546, duration: 0.229s, episode steps: 40, steps per second: 175, episode reward: 10.593, mean reward: 0.265 [0.038, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-1.239, 10.100], loss: 0.002591, mae: 0.051266, mean_q: -0.223102
 88384/100000: episode: 1547, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 4.955, mean reward: 0.225 [0.108, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.105, 10.269], loss: 0.002495, mae: 0.049075, mean_q: -0.237911
 88424/100000: episode: 1548, duration: 0.216s, episode steps: 40, steps per second: 185, episode reward: 14.563, mean reward: 0.364 [0.215, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.682, 10.100], loss: 0.002485, mae: 0.049369, mean_q: -0.198467
 88439/100000: episode: 1549, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 5.597, mean reward: 0.373 [0.255, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.284, 10.100], loss: 0.002321, mae: 0.049082, mean_q: -0.200299
 88488/100000: episode: 1550, duration: 0.275s, episode steps: 49, steps per second: 178, episode reward: 9.565, mean reward: 0.195 [0.043, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.386, 10.192], loss: 0.002487, mae: 0.049990, mean_q: -0.207306
 88510/100000: episode: 1551, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 8.778, mean reward: 0.399 [0.289, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.407], loss: 0.002400, mae: 0.049802, mean_q: -0.112370
 88550/100000: episode: 1552, duration: 0.220s, episode steps: 40, steps per second: 182, episode reward: 16.227, mean reward: 0.406 [0.223, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.291, 10.100], loss: 0.002128, mae: 0.047801, mean_q: -0.143375
 88599/100000: episode: 1553, duration: 0.294s, episode steps: 49, steps per second: 167, episode reward: 10.283, mean reward: 0.210 [0.090, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.830, 10.282], loss: 0.002461, mae: 0.049260, mean_q: -0.151781
 88632/100000: episode: 1554, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 9.834, mean reward: 0.298 [0.163, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.194, 10.100], loss: 0.002599, mae: 0.052675, mean_q: -0.153164
 88647/100000: episode: 1555, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 5.285, mean reward: 0.352 [0.305, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.329, 10.100], loss: 0.002458, mae: 0.051251, mean_q: -0.130580
 88664/100000: episode: 1556, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 5.947, mean reward: 0.350 [0.286, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.166, 10.437], loss: 0.002385, mae: 0.050876, mean_q: -0.167606
 88686/100000: episode: 1557, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 8.045, mean reward: 0.366 [0.244, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.743, 10.357], loss: 0.002643, mae: 0.052741, mean_q: -0.139187
 88708/100000: episode: 1558, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 9.768, mean reward: 0.444 [0.323, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-1.145, 10.593], loss: 0.002151, mae: 0.047573, mean_q: -0.159829
 88725/100000: episode: 1559, duration: 0.097s, episode steps: 17, steps per second: 174, episode reward: 5.744, mean reward: 0.338 [0.286, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.715, 10.414], loss: 0.002454, mae: 0.050041, mean_q: -0.101870
 88740/100000: episode: 1560, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 5.313, mean reward: 0.354 [0.201, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.288, 10.100], loss: 0.002082, mae: 0.046854, mean_q: -0.063949
 88749/100000: episode: 1561, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 2.866, mean reward: 0.318 [0.280, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.385], loss: 0.002121, mae: 0.047130, mean_q: -0.082030
 88795/100000: episode: 1562, duration: 0.253s, episode steps: 46, steps per second: 182, episode reward: 11.240, mean reward: 0.244 [0.023, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.261, 10.100], loss: 0.002298, mae: 0.048584, mean_q: -0.140837
 88817/100000: episode: 1563, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 6.871, mean reward: 0.312 [0.225, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.456, 10.438], loss: 0.002462, mae: 0.048384, mean_q: -0.140089
 88850/100000: episode: 1564, duration: 0.177s, episode steps: 33, steps per second: 187, episode reward: 8.919, mean reward: 0.270 [0.124, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.640, 10.100], loss: 0.002170, mae: 0.047863, mean_q: -0.113702
 88883/100000: episode: 1565, duration: 0.176s, episode steps: 33, steps per second: 187, episode reward: 10.213, mean reward: 0.309 [0.229, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.969, 10.100], loss: 0.002388, mae: 0.050289, mean_q: -0.133809
 88905/100000: episode: 1566, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 6.981, mean reward: 0.317 [0.251, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.399, 10.411], loss: 0.002077, mae: 0.046603, mean_q: -0.142002
 88914/100000: episode: 1567, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 3.391, mean reward: 0.377 [0.309, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.270, 10.478], loss: 0.002180, mae: 0.047511, mean_q: -0.123858
 88931/100000: episode: 1568, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 7.514, mean reward: 0.442 [0.378, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.489], loss: 0.002783, mae: 0.051153, mean_q: -0.114922
 88980/100000: episode: 1569, duration: 0.284s, episode steps: 49, steps per second: 173, episode reward: 8.155, mean reward: 0.166 [0.011, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.687, 10.227], loss: 0.002488, mae: 0.050821, mean_q: -0.081622
 89003/100000: episode: 1570, duration: 0.141s, episode steps: 23, steps per second: 163, episode reward: 8.575, mean reward: 0.373 [0.205, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.303, 10.353], loss: 0.002478, mae: 0.051280, mean_q: -0.091249
 89036/100000: episode: 1571, duration: 0.198s, episode steps: 33, steps per second: 167, episode reward: 12.004, mean reward: 0.364 [0.303, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.257, 10.100], loss: 0.002313, mae: 0.051047, mean_q: -0.073411
 89069/100000: episode: 1572, duration: 0.183s, episode steps: 33, steps per second: 180, episode reward: 8.797, mean reward: 0.267 [0.045, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.177, 10.100], loss: 0.002046, mae: 0.046321, mean_q: -0.072897
 89092/100000: episode: 1573, duration: 0.138s, episode steps: 23, steps per second: 167, episode reward: 8.106, mean reward: 0.352 [0.280, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.788, 10.427], loss: 0.002621, mae: 0.052265, mean_q: -0.059595
 89141/100000: episode: 1574, duration: 0.275s, episode steps: 49, steps per second: 178, episode reward: 10.303, mean reward: 0.210 [0.004, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.294, 10.154], loss: 0.002348, mae: 0.050692, mean_q: -0.060986
 89174/100000: episode: 1575, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 13.322, mean reward: 0.404 [0.297, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.931, 10.100], loss: 0.002206, mae: 0.047918, mean_q: -0.060247
 89207/100000: episode: 1576, duration: 0.191s, episode steps: 33, steps per second: 173, episode reward: 9.453, mean reward: 0.286 [0.214, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.609, 10.100], loss: 0.002822, mae: 0.053799, mean_q: -0.031630
 89224/100000: episode: 1577, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 7.593, mean reward: 0.447 [0.339, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.726, 10.594], loss: 0.002656, mae: 0.054700, mean_q: -0.051268
 89270/100000: episode: 1578, duration: 0.247s, episode steps: 46, steps per second: 186, episode reward: 16.606, mean reward: 0.361 [0.232, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.401, 10.450], loss: 0.002659, mae: 0.053941, mean_q: -0.029084
 89292/100000: episode: 1579, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 9.629, mean reward: 0.438 [0.322, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.630], loss: 0.002225, mae: 0.049808, mean_q: -0.044439
 89332/100000: episode: 1580, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 9.383, mean reward: 0.235 [0.032, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.060, 10.116], loss: 0.002381, mae: 0.051059, mean_q: -0.045734
 89381/100000: episode: 1581, duration: 0.264s, episode steps: 49, steps per second: 186, episode reward: 11.095, mean reward: 0.226 [0.035, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-1.394, 10.171], loss: 0.002316, mae: 0.049950, mean_q: -0.060627
 89390/100000: episode: 1582, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 3.068, mean reward: 0.341 [0.259, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.656, 10.340], loss: 0.002055, mae: 0.045893, mean_q: 0.061218
 89399/100000: episode: 1583, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 3.273, mean reward: 0.364 [0.289, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.089, 10.512], loss: 0.002507, mae: 0.050806, mean_q: -0.032209
 89432/100000: episode: 1584, duration: 0.237s, episode steps: 33, steps per second: 139, episode reward: 10.461, mean reward: 0.317 [0.146, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.152, 10.100], loss: 0.002435, mae: 0.051382, mean_q: -0.001048
 89481/100000: episode: 1585, duration: 0.270s, episode steps: 49, steps per second: 181, episode reward: 10.734, mean reward: 0.219 [0.063, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.607, 10.239], loss: 0.002345, mae: 0.050849, mean_q: 0.030344
 89527/100000: episode: 1586, duration: 0.248s, episode steps: 46, steps per second: 185, episode reward: 11.084, mean reward: 0.241 [0.056, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-1.016, 10.210], loss: 0.002279, mae: 0.049988, mean_q: 0.008473
 89549/100000: episode: 1587, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 8.002, mean reward: 0.364 [0.251, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.186, 10.530], loss: 0.002862, mae: 0.055827, mean_q: 0.069105
 89558/100000: episode: 1588, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 3.277, mean reward: 0.364 [0.309, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.488], loss: 0.002337, mae: 0.049585, mean_q: -0.109177
 89607/100000: episode: 1589, duration: 0.250s, episode steps: 49, steps per second: 196, episode reward: 7.203, mean reward: 0.147 [0.009, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.567, 10.212], loss: 0.002673, mae: 0.054221, mean_q: 0.017344
 89624/100000: episode: 1590, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 6.723, mean reward: 0.395 [0.336, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.487], loss: 0.002592, mae: 0.054433, mean_q: 0.063642
 89647/100000: episode: 1591, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 7.526, mean reward: 0.327 [0.098, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.294], loss: 0.002792, mae: 0.053832, mean_q: 0.072660
 89669/100000: episode: 1592, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 6.896, mean reward: 0.313 [0.220, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.571, 10.415], loss: 0.002723, mae: 0.054793, mean_q: 0.006289
 89680/100000: episode: 1593, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 4.403, mean reward: 0.400 [0.370, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.994, 10.513], loss: 0.002465, mae: 0.052159, mean_q: -0.025556
 89691/100000: episode: 1594, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 3.158, mean reward: 0.287 [0.220, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.328, 10.421], loss: 0.002596, mae: 0.053344, mean_q: 0.097353
 89740/100000: episode: 1595, duration: 0.274s, episode steps: 49, steps per second: 179, episode reward: 14.376, mean reward: 0.293 [0.217, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-1.450, 10.329], loss: 0.002433, mae: 0.051447, mean_q: 0.002342
 89786/100000: episode: 1596, duration: 0.249s, episode steps: 46, steps per second: 185, episode reward: 18.457, mean reward: 0.401 [0.288, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-1.115, 10.526], loss: 0.002595, mae: 0.052982, mean_q: 0.072704
 89803/100000: episode: 1597, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 7.388, mean reward: 0.435 [0.345, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.624], loss: 0.002434, mae: 0.053195, mean_q: 0.149900
 89812/100000: episode: 1598, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 2.796, mean reward: 0.311 [0.264, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.466], loss: 0.002660, mae: 0.052080, mean_q: 0.066069
 89852/100000: episode: 1599, duration: 0.224s, episode steps: 40, steps per second: 179, episode reward: 10.498, mean reward: 0.262 [0.094, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.260, 10.100], loss: 0.002532, mae: 0.054557, mean_q: 0.069657
 89867/100000: episode: 1600, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 6.914, mean reward: 0.461 [0.379, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.682, 10.100], loss: 0.002672, mae: 0.052193, mean_q: -0.006923
 89878/100000: episode: 1601, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 3.716, mean reward: 0.338 [0.277, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.398], loss: 0.001953, mae: 0.047043, mean_q: 0.033605
 89895/100000: episode: 1602, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 5.608, mean reward: 0.330 [0.268, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.419], loss: 0.002816, mae: 0.056980, mean_q: 0.094005
 89910/100000: episode: 1603, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 6.779, mean reward: 0.452 [0.374, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.503, 10.100], loss: 0.002663, mae: 0.053892, mean_q: 0.007422
 89921/100000: episode: 1604, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 3.880, mean reward: 0.353 [0.249, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.462, 10.291], loss: 0.002545, mae: 0.055330, mean_q: 0.146791
 89954/100000: episode: 1605, duration: 0.184s, episode steps: 33, steps per second: 179, episode reward: 11.087, mean reward: 0.336 [0.232, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.132, 10.100], loss: 0.002367, mae: 0.051338, mean_q: 0.085424
 89987/100000: episode: 1606, duration: 0.195s, episode steps: 33, steps per second: 169, episode reward: 11.539, mean reward: 0.350 [0.183, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.978, 10.100], loss: 0.002329, mae: 0.050123, mean_q: 0.056908
 90020/100000: episode: 1607, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 9.442, mean reward: 0.286 [0.169, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.192, 10.100], loss: 0.002391, mae: 0.051988, mean_q: 0.097646
 90069/100000: episode: 1608, duration: 0.266s, episode steps: 49, steps per second: 184, episode reward: 10.797, mean reward: 0.220 [0.114, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.296, 10.272], loss: 0.002444, mae: 0.052913, mean_q: 0.106799
 90092/100000: episode: 1609, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 7.869, mean reward: 0.342 [0.250, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.393], loss: 0.002278, mae: 0.050706, mean_q: 0.080385
 90138/100000: episode: 1610, duration: 0.251s, episode steps: 46, steps per second: 183, episode reward: 13.548, mean reward: 0.295 [0.034, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.827, 10.100], loss: 0.002601, mae: 0.054252, mean_q: 0.138083
 90161/100000: episode: 1611, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 8.722, mean reward: 0.379 [0.267, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.044, 10.434], loss: 0.002885, mae: 0.056798, mean_q: 0.177322
 90201/100000: episode: 1612, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 15.144, mean reward: 0.379 [0.247, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.447, 10.100], loss: 0.002728, mae: 0.055245, mean_q: 0.155742
 90224/100000: episode: 1613, duration: 0.131s, episode steps: 23, steps per second: 175, episode reward: 7.613, mean reward: 0.331 [0.228, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.489, 10.368], loss: 0.002720, mae: 0.055345, mean_q: 0.094435
 90270/100000: episode: 1614, duration: 0.256s, episode steps: 46, steps per second: 179, episode reward: 11.853, mean reward: 0.258 [0.048, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.899, 10.265], loss: 0.002431, mae: 0.052711, mean_q: 0.147286
 90319/100000: episode: 1615, duration: 0.257s, episode steps: 49, steps per second: 191, episode reward: 13.811, mean reward: 0.282 [0.127, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.595, 10.285], loss: 0.002341, mae: 0.052180, mean_q: 0.140715
 90352/100000: episode: 1616, duration: 0.212s, episode steps: 33, steps per second: 156, episode reward: 8.433, mean reward: 0.256 [0.106, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.287, 10.100], loss: 0.002410, mae: 0.051663, mean_q: 0.145227
[Info] 200-TH LEVEL FOUND: 0.9153856039047241, Considering 10/90 traces
 90385/100000: episode: 1617, duration: 4.255s, episode steps: 33, steps per second: 8, episode reward: 13.049, mean reward: 0.395 [0.268, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.142, 10.100], loss: 0.002375, mae: 0.051193, mean_q: 0.149743
 90394/100000: episode: 1618, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 3.651, mean reward: 0.406 [0.359, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.656, 10.430], loss: 0.002085, mae: 0.048577, mean_q: -0.002088
 90430/100000: episode: 1619, duration: 0.192s, episode steps: 36, steps per second: 187, episode reward: 9.321, mean reward: 0.259 [0.050, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.035, 10.100], loss: 0.002305, mae: 0.051231, mean_q: 0.173223
 90439/100000: episode: 1620, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 5.018, mean reward: 0.558 [0.450, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.791, 10.680], loss: 0.002682, mae: 0.055687, mean_q: 0.228037
 90468/100000: episode: 1621, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 14.870, mean reward: 0.513 [0.317, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.186, 10.100], loss: 0.002577, mae: 0.054367, mean_q: 0.159225
 90503/100000: episode: 1622, duration: 0.188s, episode steps: 35, steps per second: 187, episode reward: 9.773, mean reward: 0.279 [0.114, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.154, 10.100], loss: 0.002539, mae: 0.054324, mean_q: 0.212126
 90534/100000: episode: 1623, duration: 0.182s, episode steps: 31, steps per second: 170, episode reward: 11.306, mean reward: 0.365 [0.199, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.228, 10.100], loss: 0.002313, mae: 0.051768, mean_q: 0.172285
 90554/100000: episode: 1624, duration: 0.120s, episode steps: 20, steps per second: 166, episode reward: 7.674, mean reward: 0.384 [0.234, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.206, 10.355], loss: 0.002486, mae: 0.053772, mean_q: 0.209624
 90585/100000: episode: 1625, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 11.808, mean reward: 0.381 [0.222, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.685, 10.100], loss: 0.002281, mae: 0.051505, mean_q: 0.179516
 90621/100000: episode: 1626, duration: 0.209s, episode steps: 36, steps per second: 172, episode reward: 14.224, mean reward: 0.395 [0.166, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.915, 10.100], loss: 0.002522, mae: 0.054530, mean_q: 0.167125
 90640/100000: episode: 1627, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 9.834, mean reward: 0.518 [0.428, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.643], loss: 0.002640, mae: 0.053669, mean_q: 0.157717
 90671/100000: episode: 1628, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 11.066, mean reward: 0.357 [0.208, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.233, 10.100], loss: 0.002315, mae: 0.050757, mean_q: 0.204315
 90707/100000: episode: 1629, duration: 0.211s, episode steps: 36, steps per second: 170, episode reward: 11.167, mean reward: 0.310 [0.179, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-1.093, 10.100], loss: 0.002352, mae: 0.051854, mean_q: 0.235455
 90742/100000: episode: 1630, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 10.154, mean reward: 0.290 [0.051, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.661, 10.100], loss: 0.002413, mae: 0.052450, mean_q: 0.226586
 90778/100000: episode: 1631, duration: 0.205s, episode steps: 36, steps per second: 176, episode reward: 13.854, mean reward: 0.385 [0.279, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.294, 10.100], loss: 0.002611, mae: 0.054872, mean_q: 0.257988
[Info] FALSIFICATION!
 90791/100000: episode: 1632, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 16.414, mean reward: 1.263 [0.444, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.931, 9.484], loss: 0.002544, mae: 0.054104, mean_q: 0.200097
 90891/100000: episode: 1633, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -14.591, mean reward: -0.146 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.853, 10.098], loss: 0.015921, mae: 0.066578, mean_q: 0.226531
 90991/100000: episode: 1634, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.057, mean reward: -0.191 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.407, 10.098], loss: 0.002458, mae: 0.053605, mean_q: 0.249958
 91091/100000: episode: 1635, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.551, mean reward: -0.166 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.960, 10.098], loss: 0.002583, mae: 0.055427, mean_q: 0.237354
 91191/100000: episode: 1636, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.443, mean reward: -0.164 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.637, 10.249], loss: 0.002465, mae: 0.053386, mean_q: 0.248135
 91291/100000: episode: 1637, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.969, mean reward: -0.170 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.368, 10.291], loss: 0.002492, mae: 0.054151, mean_q: 0.242898
 91391/100000: episode: 1638, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.401, mean reward: -0.174 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.310, 10.098], loss: 0.002515, mae: 0.055007, mean_q: 0.239707
 91491/100000: episode: 1639, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.614, mean reward: -0.176 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.760, 10.098], loss: 0.002565, mae: 0.055037, mean_q: 0.220719
 91591/100000: episode: 1640, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.303, mean reward: -0.183 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.239, 10.098], loss: 0.015883, mae: 0.065220, mean_q: 0.248245
 91691/100000: episode: 1641, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.518, mean reward: -0.175 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.838, 10.098], loss: 0.028529, mae: 0.071731, mean_q: 0.242591
 91791/100000: episode: 1642, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -10.395, mean reward: -0.104 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.278, 10.441], loss: 0.015464, mae: 0.064183, mean_q: 0.242213
 91891/100000: episode: 1643, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -12.927, mean reward: -0.129 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.036, 10.372], loss: 0.002701, mae: 0.056008, mean_q: 0.235542
 91991/100000: episode: 1644, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -16.191, mean reward: -0.162 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.962, 10.098], loss: 0.014918, mae: 0.059469, mean_q: 0.253545
 92091/100000: episode: 1645, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.574, mean reward: -0.166 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.797, 10.341], loss: 0.002458, mae: 0.053278, mean_q: 0.241711
 92191/100000: episode: 1646, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -14.800, mean reward: -0.148 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.307, 10.143], loss: 0.002404, mae: 0.052438, mean_q: 0.232117
 92291/100000: episode: 1647, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.455, mean reward: -0.165 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.973, 10.208], loss: 0.002435, mae: 0.052592, mean_q: 0.231352
 92391/100000: episode: 1648, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.007, mean reward: -0.180 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.229, 10.191], loss: 0.028272, mae: 0.074215, mean_q: 0.256024
 92491/100000: episode: 1649, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -19.825, mean reward: -0.198 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.708, 10.098], loss: 0.005168, mae: 0.066889, mean_q: 0.246679
 92591/100000: episode: 1650, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -11.479, mean reward: -0.115 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.507, 10.485], loss: 0.028770, mae: 0.079572, mean_q: 0.243624
 92691/100000: episode: 1651, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -16.072, mean reward: -0.161 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.600, 10.098], loss: 0.002873, mae: 0.058712, mean_q: 0.245368
 92791/100000: episode: 1652, duration: 0.671s, episode steps: 100, steps per second: 149, episode reward: -19.442, mean reward: -0.194 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.447, 10.223], loss: 0.027444, mae: 0.066939, mean_q: 0.208822
 92891/100000: episode: 1653, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: -2.720, mean reward: -0.027 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.929, 10.489], loss: 0.003034, mae: 0.059599, mean_q: 0.219714
 92991/100000: episode: 1654, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: -6.243, mean reward: -0.062 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.870, 10.506], loss: 0.002587, mae: 0.054022, mean_q: 0.207574
 93091/100000: episode: 1655, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: -19.275, mean reward: -0.193 [-1.000, 0.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.233, 10.098], loss: 0.027053, mae: 0.067156, mean_q: 0.199745
 93191/100000: episode: 1656, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -18.731, mean reward: -0.187 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.334, 10.098], loss: 0.015044, mae: 0.062020, mean_q: 0.159931
 93291/100000: episode: 1657, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -16.161, mean reward: -0.162 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.930, 10.145], loss: 0.027794, mae: 0.072605, mean_q: 0.165443
 93391/100000: episode: 1658, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.976, mean reward: -0.170 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.538, 10.120], loss: 0.027641, mae: 0.075232, mean_q: 0.123101
 93491/100000: episode: 1659, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -16.570, mean reward: -0.166 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.005, 10.252], loss: 0.014906, mae: 0.063365, mean_q: 0.079783
 93591/100000: episode: 1660, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.175, mean reward: -0.172 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.094, 10.098], loss: 0.002596, mae: 0.053410, mean_q: 0.113461
 93691/100000: episode: 1661, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -16.303, mean reward: -0.163 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.677, 10.339], loss: 0.038951, mae: 0.077765, mean_q: 0.120703
 93791/100000: episode: 1662, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.783, mean reward: -0.168 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.636, 10.098], loss: 0.014782, mae: 0.063325, mean_q: 0.038874
 93891/100000: episode: 1663, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.796, mean reward: -0.168 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.743, 10.148], loss: 0.015490, mae: 0.069622, mean_q: 0.079720
 93991/100000: episode: 1664, duration: 0.738s, episode steps: 100, steps per second: 136, episode reward: -15.088, mean reward: -0.151 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.369, 10.098], loss: 0.002522, mae: 0.051914, mean_q: 0.030895
 94091/100000: episode: 1665, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -14.393, mean reward: -0.144 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.634, 10.098], loss: 0.002483, mae: 0.051457, mean_q: 0.008081
 94191/100000: episode: 1666, duration: 0.635s, episode steps: 100, steps per second: 158, episode reward: -19.020, mean reward: -0.190 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.156, 10.210], loss: 0.002549, mae: 0.052012, mean_q: -0.015309
 94291/100000: episode: 1667, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -19.529, mean reward: -0.195 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.692, 10.197], loss: 0.002448, mae: 0.050633, mean_q: -0.034290
 94391/100000: episode: 1668, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.404, mean reward: -0.164 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.100, 10.098], loss: 0.002321, mae: 0.049329, mean_q: -0.055068
 94491/100000: episode: 1669, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.242, mean reward: -0.162 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.584, 10.216], loss: 0.002372, mae: 0.049032, mean_q: -0.085957
 94591/100000: episode: 1670, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -20.330, mean reward: -0.203 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.884, 10.237], loss: 0.014533, mae: 0.054341, mean_q: -0.062114
 94691/100000: episode: 1671, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.272, mean reward: -0.163 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.078, 10.098], loss: 0.002621, mae: 0.052878, mean_q: -0.078067
 94791/100000: episode: 1672, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -16.532, mean reward: -0.165 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.922, 10.288], loss: 0.002415, mae: 0.049598, mean_q: -0.118584
 94891/100000: episode: 1673, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -17.290, mean reward: -0.173 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.307, 10.098], loss: 0.002389, mae: 0.048416, mean_q: -0.141900
 94991/100000: episode: 1674, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -18.429, mean reward: -0.184 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.583, 10.098], loss: 0.015183, mae: 0.060506, mean_q: -0.139895
 95091/100000: episode: 1675, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -17.234, mean reward: -0.172 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.736, 10.408], loss: 0.002395, mae: 0.048857, mean_q: -0.182479
 95191/100000: episode: 1676, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -11.469, mean reward: -0.115 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.114, 10.136], loss: 0.002309, mae: 0.049309, mean_q: -0.161482
 95291/100000: episode: 1677, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.417, mean reward: -0.174 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.928, 10.378], loss: 0.014288, mae: 0.051928, mean_q: -0.213310
 95391/100000: episode: 1678, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -19.679, mean reward: -0.197 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.067, 10.098], loss: 0.003117, mae: 0.055565, mean_q: -0.224883
 95491/100000: episode: 1679, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.634, mean reward: -0.186 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.070, 10.098], loss: 0.002587, mae: 0.050763, mean_q: -0.232369
 95591/100000: episode: 1680, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -19.415, mean reward: -0.194 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.847, 10.098], loss: 0.002369, mae: 0.049192, mean_q: -0.276702
 95691/100000: episode: 1681, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -17.303, mean reward: -0.173 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.995, 10.098], loss: 0.002423, mae: 0.048991, mean_q: -0.300846
 95791/100000: episode: 1682, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -14.495, mean reward: -0.145 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.545, 10.567], loss: 0.002335, mae: 0.047356, mean_q: -0.304322
 95891/100000: episode: 1683, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.245, mean reward: -0.182 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.425, 10.174], loss: 0.002405, mae: 0.048352, mean_q: -0.291105
 95991/100000: episode: 1684, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -20.124, mean reward: -0.201 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.322, 10.098], loss: 0.002382, mae: 0.046798, mean_q: -0.336343
 96091/100000: episode: 1685, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -17.549, mean reward: -0.175 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.225, 10.406], loss: 0.002423, mae: 0.049257, mean_q: -0.311480
 96191/100000: episode: 1686, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.641, mean reward: -0.176 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.251, 10.283], loss: 0.002316, mae: 0.047740, mean_q: -0.297253
 96291/100000: episode: 1687, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.630, mean reward: -0.186 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.593, 10.112], loss: 0.002305, mae: 0.047376, mean_q: -0.311625
 96391/100000: episode: 1688, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.896, mean reward: -0.179 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.673, 10.262], loss: 0.002199, mae: 0.045712, mean_q: -0.327673
 96491/100000: episode: 1689, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -15.687, mean reward: -0.157 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.635, 10.111], loss: 0.002383, mae: 0.048640, mean_q: -0.334934
 96591/100000: episode: 1690, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.248, mean reward: -0.172 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.781, 10.098], loss: 0.002275, mae: 0.047041, mean_q: -0.293108
 96691/100000: episode: 1691, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.904, mean reward: -0.149 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.685, 10.424], loss: 0.002527, mae: 0.049906, mean_q: -0.287256
 96791/100000: episode: 1692, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.782, mean reward: -0.178 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.968, 10.106], loss: 0.002325, mae: 0.047260, mean_q: -0.333272
 96891/100000: episode: 1693, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.141, mean reward: -0.161 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.598, 10.098], loss: 0.002211, mae: 0.046614, mean_q: -0.331516
 96991/100000: episode: 1694, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -13.506, mean reward: -0.135 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.515, 10.337], loss: 0.004591, mae: 0.065941, mean_q: -0.313773
 97091/100000: episode: 1695, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.957, mean reward: -0.190 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.297, 10.156], loss: 0.002234, mae: 0.045947, mean_q: -0.345970
 97191/100000: episode: 1696, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.288, mean reward: -0.143 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.736, 10.098], loss: 0.002504, mae: 0.048275, mean_q: -0.322366
 97291/100000: episode: 1697, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -20.011, mean reward: -0.200 [-1.000, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.157, 10.243], loss: 0.002308, mae: 0.047104, mean_q: -0.333081
 97391/100000: episode: 1698, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.595, mean reward: -0.186 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.920, 10.219], loss: 0.002256, mae: 0.046738, mean_q: -0.320067
 97491/100000: episode: 1699, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -13.950, mean reward: -0.140 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.919, 10.098], loss: 0.002384, mae: 0.047201, mean_q: -0.296268
 97591/100000: episode: 1700, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.016, mean reward: -0.180 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.848, 10.194], loss: 0.002343, mae: 0.047572, mean_q: -0.289533
 97691/100000: episode: 1701, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.557, mean reward: -0.166 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.809, 10.216], loss: 0.002462, mae: 0.048894, mean_q: -0.294373
 97791/100000: episode: 1702, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.730, mean reward: -0.167 [-1.000, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.667, 10.427], loss: 0.002302, mae: 0.046916, mean_q: -0.316865
 97891/100000: episode: 1703, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -19.718, mean reward: -0.197 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.870, 10.140], loss: 0.002435, mae: 0.048250, mean_q: -0.304997
 97991/100000: episode: 1704, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -13.926, mean reward: -0.139 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.608, 10.098], loss: 0.002305, mae: 0.047062, mean_q: -0.340648
 98091/100000: episode: 1705, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.521, mean reward: -0.175 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.964, 10.098], loss: 0.002370, mae: 0.047833, mean_q: -0.324587
 98191/100000: episode: 1706, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -12.104, mean reward: -0.121 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.655, 10.210], loss: 0.002287, mae: 0.046321, mean_q: -0.367359
 98291/100000: episode: 1707, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.282, mean reward: -0.183 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.826, 10.264], loss: 0.002377, mae: 0.048138, mean_q: -0.354726
 98391/100000: episode: 1708, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.607, mean reward: -0.186 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.882, 10.098], loss: 0.002388, mae: 0.048123, mean_q: -0.328898
 98491/100000: episode: 1709, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -20.921, mean reward: -0.209 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.017, 10.098], loss: 0.002357, mae: 0.048579, mean_q: -0.331041
 98591/100000: episode: 1710, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -19.724, mean reward: -0.197 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.647, 10.155], loss: 0.002151, mae: 0.045704, mean_q: -0.327689
 98691/100000: episode: 1711, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.597, mean reward: -0.176 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.701, 10.171], loss: 0.002487, mae: 0.049854, mean_q: -0.341885
 98791/100000: episode: 1712, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -17.549, mean reward: -0.175 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.369, 10.106], loss: 0.002357, mae: 0.047838, mean_q: -0.331309
 98891/100000: episode: 1713, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -19.133, mean reward: -0.191 [-1.000, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.056, 10.138], loss: 0.002264, mae: 0.046193, mean_q: -0.357823
 98991/100000: episode: 1714, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.772, mean reward: -0.158 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.473, 10.098], loss: 0.002388, mae: 0.047249, mean_q: -0.313897
 99091/100000: episode: 1715, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -15.534, mean reward: -0.155 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.655, 10.200], loss: 0.002387, mae: 0.048125, mean_q: -0.327326
 99191/100000: episode: 1716, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -15.321, mean reward: -0.153 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.695, 10.209], loss: 0.002151, mae: 0.045033, mean_q: -0.360506
 99291/100000: episode: 1717, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -18.617, mean reward: -0.186 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.875, 10.220], loss: 0.002441, mae: 0.047483, mean_q: -0.345582
 99391/100000: episode: 1718, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.861, mean reward: -0.179 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.465, 10.416], loss: 0.002513, mae: 0.049855, mean_q: -0.327748
 99491/100000: episode: 1719, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -19.760, mean reward: -0.198 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.401, 10.098], loss: 0.002373, mae: 0.047173, mean_q: -0.334776
 99591/100000: episode: 1720, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -19.715, mean reward: -0.197 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.678, 10.139], loss: 0.002399, mae: 0.048244, mean_q: -0.338978
 99691/100000: episode: 1721, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.958, mean reward: -0.160 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.395, 10.240], loss: 0.002121, mae: 0.045185, mean_q: -0.332600
 99791/100000: episode: 1722, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -19.259, mean reward: -0.193 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.407, 10.234], loss: 0.002262, mae: 0.047379, mean_q: -0.326760
 99891/100000: episode: 1723, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.107, mean reward: -0.161 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.736, 10.098], loss: 0.002264, mae: 0.046146, mean_q: -0.348876
 99991/100000: episode: 1724, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -15.204, mean reward: -0.152 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.567, 10.098], loss: 0.002644, mae: 0.051580, mean_q: -0.321235
done, took 618.938 seconds
[Info] End Importance Splitting. Falsification occurred 6 times.
