Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.187s, episode steps: 100, steps per second: 536, episode reward: -14.613, mean reward: -0.146 [-1.000, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.812, 10.117], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.066s, episode steps: 100, steps per second: 1519, episode reward: -19.323, mean reward: -0.193 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.810, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.066s, episode steps: 100, steps per second: 1510, episode reward: -15.953, mean reward: -0.160 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.317, 10.246], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.068s, episode steps: 100, steps per second: 1461, episode reward: -17.112, mean reward: -0.171 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.713, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.076s, episode steps: 100, steps per second: 1322, episode reward: -19.062, mean reward: -0.191 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.751, 10.113], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.378s, episode steps: 100, steps per second: 73, episode reward: -18.278, mean reward: -0.183 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.959, 10.098], loss: 0.062615, mae: 0.256119, mean_q: -0.089523
   700/100000: episode: 7, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -18.866, mean reward: -0.189 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.194, 10.160], loss: 0.018515, mae: 0.134764, mean_q: -0.255887
   800/100000: episode: 8, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -17.064, mean reward: -0.171 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.012, 10.251], loss: 0.013518, mae: 0.111485, mean_q: -0.277634
   900/100000: episode: 9, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -18.992, mean reward: -0.190 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.095, 10.205], loss: 0.011947, mae: 0.101471, mean_q: -0.305076
  1000/100000: episode: 10, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.415, mean reward: -0.154 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.696, 10.167], loss: 0.010967, mae: 0.095763, mean_q: -0.303941
  1100/100000: episode: 11, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.704, mean reward: -0.187 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.889, 10.336], loss: 0.009567, mae: 0.091005, mean_q: -0.333980
  1200/100000: episode: 12, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -17.441, mean reward: -0.174 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.074, 10.107], loss: 0.009562, mae: 0.089597, mean_q: -0.326349
  1300/100000: episode: 13, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -20.710, mean reward: -0.207 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.960, 10.098], loss: 0.008760, mae: 0.083503, mean_q: -0.329645
  1400/100000: episode: 14, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -15.624, mean reward: -0.156 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.204, 10.312], loss: 0.008022, mae: 0.078533, mean_q: -0.360518
  1500/100000: episode: 15, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -19.034, mean reward: -0.190 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.511, 10.315], loss: 0.008245, mae: 0.082282, mean_q: -0.367414
  1600/100000: episode: 16, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.512, mean reward: -0.165 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.409, 10.227], loss: 0.006789, mae: 0.072495, mean_q: -0.345162
  1700/100000: episode: 17, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.656, mean reward: -0.167 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.786, 10.098], loss: 0.005970, mae: 0.070661, mean_q: -0.320654
  1800/100000: episode: 18, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -16.696, mean reward: -0.167 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.106, 10.137], loss: 0.006960, mae: 0.075882, mean_q: -0.348272
  1900/100000: episode: 19, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -3.379, mean reward: -0.034 [-1.000, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.464, 10.098], loss: 0.006543, mae: 0.073148, mean_q: -0.330304
  2000/100000: episode: 20, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.906, mean reward: -0.179 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.586, 10.123], loss: 0.007651, mae: 0.079316, mean_q: -0.343277
  2100/100000: episode: 21, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -12.786, mean reward: -0.128 [-1.000, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.338, 10.098], loss: 0.007971, mae: 0.082100, mean_q: -0.353864
  2200/100000: episode: 22, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -11.624, mean reward: -0.116 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.146, 10.525], loss: 0.007679, mae: 0.080362, mean_q: -0.335232
  2300/100000: episode: 23, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -17.274, mean reward: -0.173 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.834, 10.098], loss: 0.007096, mae: 0.078539, mean_q: -0.310868
  2400/100000: episode: 24, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -19.232, mean reward: -0.192 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.869, 10.142], loss: 0.007344, mae: 0.080740, mean_q: -0.295407
  2500/100000: episode: 25, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -18.084, mean reward: -0.181 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.088, 10.098], loss: 0.006504, mae: 0.075404, mean_q: -0.302474
  2600/100000: episode: 26, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.421, mean reward: -0.154 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.420, 10.098], loss: 0.007078, mae: 0.079702, mean_q: -0.335581
  2700/100000: episode: 27, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -15.964, mean reward: -0.160 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.295, 10.499], loss: 0.005769, mae: 0.073313, mean_q: -0.332683
  2800/100000: episode: 28, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -16.409, mean reward: -0.164 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.022, 10.098], loss: 0.006903, mae: 0.079881, mean_q: -0.337804
  2900/100000: episode: 29, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.599, mean reward: -0.176 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.282, 10.249], loss: 0.005579, mae: 0.071140, mean_q: -0.338078
  3000/100000: episode: 30, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -19.271, mean reward: -0.193 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.936, 10.098], loss: 0.006543, mae: 0.078132, mean_q: -0.270337
  3100/100000: episode: 31, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: -16.549, mean reward: -0.165 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.738, 10.099], loss: 0.006213, mae: 0.072772, mean_q: -0.316346
  3200/100000: episode: 32, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -8.601, mean reward: -0.086 [-1.000, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.653, 10.098], loss: 0.006273, mae: 0.075735, mean_q: -0.329994
  3300/100000: episode: 33, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.384, mean reward: -0.194 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.206, 10.098], loss: 0.006134, mae: 0.075293, mean_q: -0.321384
  3400/100000: episode: 34, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.252, mean reward: -0.163 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.440, 10.220], loss: 0.005485, mae: 0.072130, mean_q: -0.327910
  3500/100000: episode: 35, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.466, mean reward: -0.185 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.038, 10.319], loss: 0.006220, mae: 0.076378, mean_q: -0.324353
  3600/100000: episode: 36, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -18.545, mean reward: -0.185 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.816, 10.276], loss: 0.006231, mae: 0.077420, mean_q: -0.304793
  3700/100000: episode: 37, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.285, mean reward: -0.163 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.791, 10.264], loss: 0.005970, mae: 0.074857, mean_q: -0.296746
  3800/100000: episode: 38, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -15.761, mean reward: -0.158 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.187, 10.098], loss: 0.006054, mae: 0.075281, mean_q: -0.316748
  3900/100000: episode: 39, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -17.965, mean reward: -0.180 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.159, 10.196], loss: 0.006088, mae: 0.073456, mean_q: -0.356117
  4000/100000: episode: 40, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -18.976, mean reward: -0.190 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.895, 10.272], loss: 0.005517, mae: 0.070542, mean_q: -0.320366
  4100/100000: episode: 41, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -15.983, mean reward: -0.160 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.669, 10.098], loss: 0.006008, mae: 0.073546, mean_q: -0.345154
  4200/100000: episode: 42, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -17.942, mean reward: -0.179 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.037, 10.208], loss: 0.005881, mae: 0.073819, mean_q: -0.337748
  4300/100000: episode: 43, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -18.581, mean reward: -0.186 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.711, 10.117], loss: 0.006235, mae: 0.075519, mean_q: -0.280928
  4400/100000: episode: 44, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -16.639, mean reward: -0.166 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.908, 10.263], loss: 0.006128, mae: 0.074484, mean_q: -0.317387
  4500/100000: episode: 45, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.384, mean reward: -0.184 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.307, 10.177], loss: 0.005395, mae: 0.070457, mean_q: -0.313682
  4600/100000: episode: 46, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -18.409, mean reward: -0.184 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.945, 10.098], loss: 0.004761, mae: 0.067458, mean_q: -0.313569
  4700/100000: episode: 47, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -18.332, mean reward: -0.183 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.962, 10.098], loss: 0.006665, mae: 0.077797, mean_q: -0.326698
  4800/100000: episode: 48, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: -11.795, mean reward: -0.118 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.275, 10.306], loss: 0.005210, mae: 0.069544, mean_q: -0.298371
  4900/100000: episode: 49, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: -19.461, mean reward: -0.195 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.451, 10.105], loss: 0.005512, mae: 0.074521, mean_q: -0.316085
  5000/100000: episode: 50, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -9.446, mean reward: -0.094 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.414, 10.368], loss: 0.005371, mae: 0.070285, mean_q: -0.325153
  5100/100000: episode: 51, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -14.625, mean reward: -0.146 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.569, 10.108], loss: 0.005905, mae: 0.076673, mean_q: -0.321260
  5200/100000: episode: 52, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -19.184, mean reward: -0.192 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.251, 10.098], loss: 0.004922, mae: 0.068744, mean_q: -0.335525
  5300/100000: episode: 53, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -15.946, mean reward: -0.159 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.214, 10.297], loss: 0.004897, mae: 0.067764, mean_q: -0.311114
  5400/100000: episode: 54, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -16.411, mean reward: -0.164 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.519, 10.266], loss: 0.005013, mae: 0.071863, mean_q: -0.294915
  5500/100000: episode: 55, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.712, mean reward: -0.197 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.903, 10.124], loss: 0.005628, mae: 0.073172, mean_q: -0.324089
  5600/100000: episode: 56, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -14.990, mean reward: -0.150 [-1.000, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.890, 10.098], loss: 0.005591, mae: 0.074710, mean_q: -0.319903
  5700/100000: episode: 57, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: -17.532, mean reward: -0.175 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.826, 10.098], loss: 0.005581, mae: 0.073464, mean_q: -0.290172
  5800/100000: episode: 58, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -15.947, mean reward: -0.159 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.965, 10.098], loss: 0.005422, mae: 0.072014, mean_q: -0.333964
  5900/100000: episode: 59, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -17.312, mean reward: -0.173 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.550, 10.098], loss: 0.005080, mae: 0.069542, mean_q: -0.321441
  6000/100000: episode: 60, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.897, mean reward: -0.179 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.119, 10.298], loss: 0.004981, mae: 0.068663, mean_q: -0.319391
  6100/100000: episode: 61, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -13.681, mean reward: -0.137 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.490, 10.408], loss: 0.005470, mae: 0.073748, mean_q: -0.332716
  6200/100000: episode: 62, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.979, mean reward: -0.180 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.268, 10.098], loss: 0.005393, mae: 0.072007, mean_q: -0.292198
  6300/100000: episode: 63, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.609, mean reward: -0.166 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.553, 10.165], loss: 0.004967, mae: 0.070996, mean_q: -0.307880
  6400/100000: episode: 64, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -18.971, mean reward: -0.190 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.585, 10.232], loss: 0.005045, mae: 0.071726, mean_q: -0.267283
  6500/100000: episode: 65, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.460, mean reward: -0.175 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.728, 10.112], loss: 0.005108, mae: 0.070866, mean_q: -0.273004
  6600/100000: episode: 66, duration: 0.615s, episode steps: 100, steps per second: 162, episode reward: -13.207, mean reward: -0.132 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.953, 10.455], loss: 0.004510, mae: 0.067591, mean_q: -0.330070
  6700/100000: episode: 67, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: -10.795, mean reward: -0.108 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.563, 10.098], loss: 0.005256, mae: 0.073983, mean_q: -0.312265
  6800/100000: episode: 68, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -18.447, mean reward: -0.184 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.401, 10.098], loss: 0.005570, mae: 0.075712, mean_q: -0.291474
  6900/100000: episode: 69, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.119, mean reward: -0.171 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.268, 10.197], loss: 0.006773, mae: 0.078300, mean_q: -0.287943
  7000/100000: episode: 70, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -13.143, mean reward: -0.131 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.955, 10.348], loss: 0.004570, mae: 0.069043, mean_q: -0.296212
  7100/100000: episode: 71, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -18.916, mean reward: -0.189 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.817, 10.224], loss: 0.006707, mae: 0.076345, mean_q: -0.287750
  7200/100000: episode: 72, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -16.221, mean reward: -0.162 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.314, 10.207], loss: 0.004416, mae: 0.066483, mean_q: -0.285261
  7300/100000: episode: 73, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.459, mean reward: -0.175 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.919, 10.215], loss: 0.004873, mae: 0.069845, mean_q: -0.302588
  7400/100000: episode: 74, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: -20.919, mean reward: -0.209 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.924, 10.098], loss: 0.004484, mae: 0.065950, mean_q: -0.305912
  7500/100000: episode: 75, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -18.081, mean reward: -0.181 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.005, 10.299], loss: 0.004337, mae: 0.064684, mean_q: -0.349241
  7600/100000: episode: 76, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -14.332, mean reward: -0.143 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.099, 10.256], loss: 0.004883, mae: 0.068575, mean_q: -0.331758
  7700/100000: episode: 77, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -13.549, mean reward: -0.135 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.830, 10.098], loss: 0.005027, mae: 0.072139, mean_q: -0.303083
  7800/100000: episode: 78, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -17.613, mean reward: -0.176 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.466, 10.174], loss: 0.005129, mae: 0.070201, mean_q: -0.323478
  7900/100000: episode: 79, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -16.352, mean reward: -0.164 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.193, 10.098], loss: 0.005088, mae: 0.070578, mean_q: -0.322085
  8000/100000: episode: 80, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -15.826, mean reward: -0.158 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.172, 10.098], loss: 0.004496, mae: 0.067049, mean_q: -0.337450
  8100/100000: episode: 81, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -19.024, mean reward: -0.190 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.216, 10.098], loss: 0.004343, mae: 0.067510, mean_q: -0.323693
  8200/100000: episode: 82, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -18.443, mean reward: -0.184 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.746, 10.105], loss: 0.004846, mae: 0.070669, mean_q: -0.342428
  8300/100000: episode: 83, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: -19.091, mean reward: -0.191 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.993, 10.258], loss: 0.003945, mae: 0.064008, mean_q: -0.290414
  8400/100000: episode: 84, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -17.604, mean reward: -0.176 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.548, 10.184], loss: 0.004286, mae: 0.064687, mean_q: -0.331550
  8500/100000: episode: 85, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -16.174, mean reward: -0.162 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.638, 10.290], loss: 0.005602, mae: 0.074164, mean_q: -0.309533
  8600/100000: episode: 86, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -21.399, mean reward: -0.214 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.859, 10.098], loss: 0.005836, mae: 0.074780, mean_q: -0.279182
  8700/100000: episode: 87, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -15.031, mean reward: -0.150 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.012, 10.153], loss: 0.004753, mae: 0.068567, mean_q: -0.315372
  8800/100000: episode: 88, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -16.734, mean reward: -0.167 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.007, 10.174], loss: 0.003949, mae: 0.064133, mean_q: -0.306437
  8900/100000: episode: 89, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -19.335, mean reward: -0.193 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.605, 10.111], loss: 0.004766, mae: 0.068572, mean_q: -0.321504
  9000/100000: episode: 90, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -18.555, mean reward: -0.186 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.597, 10.167], loss: 0.003505, mae: 0.060810, mean_q: -0.311685
  9100/100000: episode: 91, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: -16.400, mean reward: -0.164 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.246, 10.230], loss: 0.003950, mae: 0.064032, mean_q: -0.331986
  9200/100000: episode: 92, duration: 0.645s, episode steps: 100, steps per second: 155, episode reward: -14.648, mean reward: -0.146 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.705, 10.098], loss: 0.003824, mae: 0.062170, mean_q: -0.308758
  9300/100000: episode: 93, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -10.129, mean reward: -0.101 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.498, 10.457], loss: 0.004539, mae: 0.067785, mean_q: -0.303170
  9400/100000: episode: 94, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -17.836, mean reward: -0.178 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.720, 10.123], loss: 0.003929, mae: 0.064303, mean_q: -0.302643
  9500/100000: episode: 95, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -18.906, mean reward: -0.189 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.890, 10.222], loss: 0.004588, mae: 0.070704, mean_q: -0.363570
  9600/100000: episode: 96, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -17.691, mean reward: -0.177 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.496, 10.098], loss: 0.004851, mae: 0.072325, mean_q: -0.320070
  9700/100000: episode: 97, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -18.604, mean reward: -0.186 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.865, 10.105], loss: 0.003485, mae: 0.060491, mean_q: -0.289805
  9800/100000: episode: 98, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -19.218, mean reward: -0.192 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.050, 10.157], loss: 0.005088, mae: 0.074321, mean_q: -0.316054
  9900/100000: episode: 99, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -14.676, mean reward: -0.147 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.017, 10.098], loss: 0.003863, mae: 0.064295, mean_q: -0.302549
 10000/100000: episode: 100, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: -17.895, mean reward: -0.179 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.634, 10.098], loss: 0.004288, mae: 0.066446, mean_q: -0.318478
 10100/100000: episode: 101, duration: 0.612s, episode steps: 100, steps per second: 164, episode reward: -15.550, mean reward: -0.156 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.146, 10.098], loss: 0.003990, mae: 0.065531, mean_q: -0.340583
 10200/100000: episode: 102, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: -18.228, mean reward: -0.182 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.924, 10.098], loss: 0.003735, mae: 0.062060, mean_q: -0.312606
 10300/100000: episode: 103, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -15.453, mean reward: -0.155 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.097, 10.347], loss: 0.004123, mae: 0.065849, mean_q: -0.327858
 10400/100000: episode: 104, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.369, mean reward: -0.184 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.875, 10.098], loss: 0.003621, mae: 0.061590, mean_q: -0.313174
 10500/100000: episode: 105, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -19.548, mean reward: -0.195 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.479, 10.129], loss: 0.004034, mae: 0.067711, mean_q: -0.296439
 10600/100000: episode: 106, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.782, mean reward: -0.168 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.200, 10.340], loss: 0.004119, mae: 0.066530, mean_q: -0.300710
 10700/100000: episode: 107, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -17.621, mean reward: -0.176 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.518, 10.342], loss: 0.004752, mae: 0.072069, mean_q: -0.285097
 10800/100000: episode: 108, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: -17.566, mean reward: -0.176 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.951, 10.098], loss: 0.003464, mae: 0.059513, mean_q: -0.333147
 10900/100000: episode: 109, duration: 0.639s, episode steps: 100, steps per second: 156, episode reward: -16.999, mean reward: -0.170 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.370, 10.159], loss: 0.003945, mae: 0.065651, mean_q: -0.333999
 11000/100000: episode: 110, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -7.947, mean reward: -0.079 [-1.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.186, 10.363], loss: 0.003539, mae: 0.061897, mean_q: -0.325925
 11100/100000: episode: 111, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -19.203, mean reward: -0.192 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.786, 10.098], loss: 0.003931, mae: 0.064780, mean_q: -0.305169
 11200/100000: episode: 112, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.845, mean reward: -0.188 [-1.000, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.514, 10.222], loss: 0.003844, mae: 0.063545, mean_q: -0.313069
 11300/100000: episode: 113, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -19.575, mean reward: -0.196 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.751, 10.185], loss: 0.003507, mae: 0.060177, mean_q: -0.393273
 11400/100000: episode: 114, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -17.122, mean reward: -0.171 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.832, 10.114], loss: 0.003289, mae: 0.059291, mean_q: -0.341683
 11500/100000: episode: 115, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -20.768, mean reward: -0.208 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.436, 10.098], loss: 0.003684, mae: 0.060893, mean_q: -0.348172
 11600/100000: episode: 116, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -15.772, mean reward: -0.158 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.934, 10.225], loss: 0.004037, mae: 0.063714, mean_q: -0.334823
 11700/100000: episode: 117, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -12.695, mean reward: -0.127 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.364, 10.098], loss: 0.003808, mae: 0.064827, mean_q: -0.298019
 11800/100000: episode: 118, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: -16.541, mean reward: -0.165 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.856, 10.207], loss: 0.003576, mae: 0.060847, mean_q: -0.338462
 11900/100000: episode: 119, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -19.028, mean reward: -0.190 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.859, 10.098], loss: 0.003900, mae: 0.065634, mean_q: -0.329103
 12000/100000: episode: 120, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -17.129, mean reward: -0.171 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.939, 10.098], loss: 0.005465, mae: 0.074442, mean_q: -0.332403
 12100/100000: episode: 121, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.020, mean reward: -0.190 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.765, 10.159], loss: 0.003950, mae: 0.065378, mean_q: -0.317132
 12200/100000: episode: 122, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -15.059, mean reward: -0.151 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.274, 10.329], loss: 0.004318, mae: 0.066735, mean_q: -0.291126
 12300/100000: episode: 123, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.268, mean reward: -0.173 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.561, 10.098], loss: 0.003484, mae: 0.060312, mean_q: -0.322143
 12400/100000: episode: 124, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -16.636, mean reward: -0.166 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.632, 10.098], loss: 0.003815, mae: 0.064080, mean_q: -0.317748
 12500/100000: episode: 125, duration: 0.587s, episode steps: 100, steps per second: 171, episode reward: -19.234, mean reward: -0.192 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.223, 10.240], loss: 0.003750, mae: 0.062948, mean_q: -0.315659
 12600/100000: episode: 126, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: -15.702, mean reward: -0.157 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.391, 10.098], loss: 0.003673, mae: 0.063241, mean_q: -0.290800
 12700/100000: episode: 127, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: -16.110, mean reward: -0.161 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.220, 10.188], loss: 0.003644, mae: 0.061796, mean_q: -0.321057
 12800/100000: episode: 128, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -16.317, mean reward: -0.163 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.508, 10.173], loss: 0.003752, mae: 0.061516, mean_q: -0.328027
 12900/100000: episode: 129, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -16.560, mean reward: -0.166 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.744, 10.108], loss: 0.003432, mae: 0.058993, mean_q: -0.330766
 13000/100000: episode: 130, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -17.185, mean reward: -0.172 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.560, 10.098], loss: 0.003526, mae: 0.060543, mean_q: -0.329265
 13100/100000: episode: 131, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.085, mean reward: -0.171 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.534, 10.235], loss: 0.003628, mae: 0.061955, mean_q: -0.333642
 13200/100000: episode: 132, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.498, mean reward: -0.175 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.663, 10.260], loss: 0.004087, mae: 0.064372, mean_q: -0.351462
 13300/100000: episode: 133, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -17.543, mean reward: -0.175 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.231, 10.149], loss: 0.003313, mae: 0.058471, mean_q: -0.304605
 13400/100000: episode: 134, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -14.646, mean reward: -0.146 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.489, 10.098], loss: 0.003230, mae: 0.058397, mean_q: -0.343029
 13500/100000: episode: 135, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: -19.594, mean reward: -0.196 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.175, 10.221], loss: 0.003238, mae: 0.057815, mean_q: -0.325923
 13600/100000: episode: 136, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.289, mean reward: -0.183 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.360, 10.098], loss: 0.003135, mae: 0.057120, mean_q: -0.336397
 13700/100000: episode: 137, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: -8.469, mean reward: -0.085 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.648, 10.098], loss: 0.004389, mae: 0.067274, mean_q: -0.321837
 13800/100000: episode: 138, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: -18.483, mean reward: -0.185 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.698, 10.161], loss: 0.003190, mae: 0.058230, mean_q: -0.342099
 13900/100000: episode: 139, duration: 0.766s, episode steps: 100, steps per second: 130, episode reward: -18.986, mean reward: -0.190 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.668, 10.098], loss: 0.003397, mae: 0.059434, mean_q: -0.302217
 14000/100000: episode: 140, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -16.592, mean reward: -0.166 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.631, 10.098], loss: 0.003238, mae: 0.057349, mean_q: -0.348110
 14100/100000: episode: 141, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -15.887, mean reward: -0.159 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.591, 10.098], loss: 0.003852, mae: 0.065664, mean_q: -0.322263
 14200/100000: episode: 142, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: -16.185, mean reward: -0.162 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.731, 10.098], loss: 0.003703, mae: 0.061613, mean_q: -0.325138
 14300/100000: episode: 143, duration: 0.669s, episode steps: 100, steps per second: 149, episode reward: -10.844, mean reward: -0.108 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.387, 10.197], loss: 0.003736, mae: 0.062325, mean_q: -0.341616
 14400/100000: episode: 144, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: -16.638, mean reward: -0.166 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.526, 10.355], loss: 0.004988, mae: 0.072327, mean_q: -0.325900
 14500/100000: episode: 145, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.495, mean reward: -0.185 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.865, 10.098], loss: 0.003815, mae: 0.063216, mean_q: -0.340282
 14600/100000: episode: 146, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -16.700, mean reward: -0.167 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.988, 10.098], loss: 0.006477, mae: 0.081012, mean_q: -0.316119
 14700/100000: episode: 147, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -18.367, mean reward: -0.184 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.213, 10.098], loss: 0.004763, mae: 0.069961, mean_q: -0.331008
 14800/100000: episode: 148, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: -16.346, mean reward: -0.163 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.780, 10.429], loss: 0.004751, mae: 0.069424, mean_q: -0.315363
 14900/100000: episode: 149, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -18.508, mean reward: -0.185 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.825, 10.192], loss: 0.003290, mae: 0.058564, mean_q: -0.333352
 15000/100000: episode: 150, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -17.585, mean reward: -0.176 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.196, 10.116], loss: 0.003190, mae: 0.057239, mean_q: -0.333257
 15100/100000: episode: 151, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: -15.686, mean reward: -0.157 [-1.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.493, 10.098], loss: 0.003187, mae: 0.057604, mean_q: -0.323271
 15200/100000: episode: 152, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.499, mean reward: -0.175 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.951, 10.098], loss: 0.003608, mae: 0.061383, mean_q: -0.311703
 15300/100000: episode: 153, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -16.601, mean reward: -0.166 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.653, 10.193], loss: 0.003042, mae: 0.057041, mean_q: -0.315856
 15400/100000: episode: 154, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -19.606, mean reward: -0.196 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.705, 10.123], loss: 0.004629, mae: 0.067260, mean_q: -0.336418
 15500/100000: episode: 155, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -16.266, mean reward: -0.163 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.020, 10.195], loss: 0.003597, mae: 0.062379, mean_q: -0.339892
 15600/100000: episode: 156, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -14.158, mean reward: -0.142 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.505, 10.134], loss: 0.003216, mae: 0.057835, mean_q: -0.317763
 15700/100000: episode: 157, duration: 0.678s, episode steps: 100, steps per second: 148, episode reward: -18.595, mean reward: -0.186 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.810, 10.098], loss: 0.003565, mae: 0.060062, mean_q: -0.313358
 15800/100000: episode: 158, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -15.457, mean reward: -0.155 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.051, 10.189], loss: 0.003260, mae: 0.057672, mean_q: -0.351826
 15900/100000: episode: 159, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: -18.153, mean reward: -0.182 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.578, 10.260], loss: 0.003074, mae: 0.056416, mean_q: -0.324569
 16000/100000: episode: 160, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -16.199, mean reward: -0.162 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.851, 10.098], loss: 0.003121, mae: 0.056739, mean_q: -0.324533
 16100/100000: episode: 161, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.590, mean reward: -0.176 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.906, 10.123], loss: 0.003022, mae: 0.056508, mean_q: -0.300157
 16200/100000: episode: 162, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -14.591, mean reward: -0.146 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.458, 10.533], loss: 0.003012, mae: 0.055599, mean_q: -0.334057
 16300/100000: episode: 163, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -15.907, mean reward: -0.159 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.705, 10.098], loss: 0.003227, mae: 0.058711, mean_q: -0.291623
 16400/100000: episode: 164, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -13.591, mean reward: -0.136 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.076, 10.348], loss: 0.003288, mae: 0.058839, mean_q: -0.349734
 16500/100000: episode: 165, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -13.359, mean reward: -0.134 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.023, 10.248], loss: 0.003453, mae: 0.061585, mean_q: -0.302099
 16600/100000: episode: 166, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: -14.174, mean reward: -0.142 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.646, 10.098], loss: 0.002791, mae: 0.054247, mean_q: -0.338327
 16700/100000: episode: 167, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: -18.765, mean reward: -0.188 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.234, 10.311], loss: 0.003038, mae: 0.056250, mean_q: -0.309923
 16800/100000: episode: 168, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: -18.496, mean reward: -0.185 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.786, 10.237], loss: 0.003253, mae: 0.058567, mean_q: -0.328224
 16900/100000: episode: 169, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.930, mean reward: -0.179 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.769, 10.215], loss: 0.003086, mae: 0.056966, mean_q: -0.295086
 17000/100000: episode: 170, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -19.701, mean reward: -0.197 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.206, 10.129], loss: 0.003013, mae: 0.056273, mean_q: -0.306628
 17100/100000: episode: 171, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -9.385, mean reward: -0.094 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.483, 10.137], loss: 0.002985, mae: 0.056073, mean_q: -0.297586
 17200/100000: episode: 172, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -19.245, mean reward: -0.192 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.540, 10.098], loss: 0.003242, mae: 0.058051, mean_q: -0.308829
 17300/100000: episode: 173, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -18.409, mean reward: -0.184 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.788, 10.305], loss: 0.003387, mae: 0.060720, mean_q: -0.323119
 17400/100000: episode: 174, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -17.338, mean reward: -0.173 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.745, 10.098], loss: 0.003208, mae: 0.057962, mean_q: -0.308773
 17500/100000: episode: 175, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -15.999, mean reward: -0.160 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.844, 10.098], loss: 0.003352, mae: 0.060437, mean_q: -0.313148
 17600/100000: episode: 176, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: -11.915, mean reward: -0.119 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.489, 10.098], loss: 0.004755, mae: 0.067415, mean_q: -0.320319
 17700/100000: episode: 177, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -17.082, mean reward: -0.171 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.309, 10.098], loss: 0.004705, mae: 0.069466, mean_q: -0.354768
 17800/100000: episode: 178, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -16.025, mean reward: -0.160 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.686, 10.098], loss: 0.003438, mae: 0.060799, mean_q: -0.306539
 17900/100000: episode: 179, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -15.074, mean reward: -0.151 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.893, 10.249], loss: 0.004220, mae: 0.063646, mean_q: -0.333933
 18000/100000: episode: 180, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -13.908, mean reward: -0.139 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.247, 10.261], loss: 0.003668, mae: 0.064060, mean_q: -0.290389
 18100/100000: episode: 181, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -16.821, mean reward: -0.168 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.085, 10.098], loss: 0.003351, mae: 0.060037, mean_q: -0.293751
 18200/100000: episode: 182, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -19.537, mean reward: -0.195 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.655, 10.216], loss: 0.003218, mae: 0.057582, mean_q: -0.322874
 18300/100000: episode: 183, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.975, mean reward: -0.190 [-1.000, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.542, 10.191], loss: 0.003446, mae: 0.058581, mean_q: -0.331099
 18400/100000: episode: 184, duration: 0.678s, episode steps: 100, steps per second: 148, episode reward: -12.735, mean reward: -0.127 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.426, 10.320], loss: 0.003526, mae: 0.059167, mean_q: -0.327344
 18500/100000: episode: 185, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -17.538, mean reward: -0.175 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.882, 10.098], loss: 0.006125, mae: 0.075723, mean_q: -0.270601
 18600/100000: episode: 186, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -14.104, mean reward: -0.141 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.997, 10.098], loss: 0.003407, mae: 0.059580, mean_q: -0.291996
 18700/100000: episode: 187, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -20.074, mean reward: -0.201 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.272, 10.178], loss: 0.003068, mae: 0.056638, mean_q: -0.289581
 18800/100000: episode: 188, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -19.791, mean reward: -0.198 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.920, 10.181], loss: 0.004366, mae: 0.066379, mean_q: -0.315354
 18900/100000: episode: 189, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -11.729, mean reward: -0.117 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.387, 10.502], loss: 0.003787, mae: 0.063400, mean_q: -0.288866
 19000/100000: episode: 190, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -18.499, mean reward: -0.185 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.805, 10.098], loss: 0.003936, mae: 0.065065, mean_q: -0.286919
 19100/100000: episode: 191, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -15.409, mean reward: -0.154 [-1.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.460, 10.194], loss: 0.003016, mae: 0.054945, mean_q: -0.331408
 19200/100000: episode: 192, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: -16.231, mean reward: -0.162 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.153, 10.322], loss: 0.003345, mae: 0.058494, mean_q: -0.328806
 19300/100000: episode: 193, duration: 0.661s, episode steps: 100, steps per second: 151, episode reward: -14.682, mean reward: -0.147 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.109, 10.098], loss: 0.003476, mae: 0.062348, mean_q: -0.282931
 19400/100000: episode: 194, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -15.394, mean reward: -0.154 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.467, 10.374], loss: 0.003709, mae: 0.063057, mean_q: -0.323107
 19500/100000: episode: 195, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.113, mean reward: -0.171 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.025, 10.258], loss: 0.003618, mae: 0.059818, mean_q: -0.358235
 19600/100000: episode: 196, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -18.668, mean reward: -0.187 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.564, 10.098], loss: 0.003105, mae: 0.056313, mean_q: -0.299820
 19700/100000: episode: 197, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -20.004, mean reward: -0.200 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.424, 10.098], loss: 0.003479, mae: 0.060058, mean_q: -0.297521
 19800/100000: episode: 198, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -11.413, mean reward: -0.114 [-1.000, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.547, 10.441], loss: 0.002987, mae: 0.056146, mean_q: -0.314751
 19900/100000: episode: 199, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -17.710, mean reward: -0.177 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.805, 10.267], loss: 0.004061, mae: 0.065463, mean_q: -0.265180
 20000/100000: episode: 200, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -18.147, mean reward: -0.181 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.399, 10.307], loss: 0.003115, mae: 0.056417, mean_q: -0.321912
 20100/100000: episode: 201, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: -18.271, mean reward: -0.183 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.596, 10.228], loss: 0.003304, mae: 0.059214, mean_q: -0.330614
 20200/100000: episode: 202, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: -19.500, mean reward: -0.195 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.843, 10.201], loss: 0.003131, mae: 0.057010, mean_q: -0.295157
 20300/100000: episode: 203, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -19.730, mean reward: -0.197 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.114, 10.130], loss: 0.003264, mae: 0.058331, mean_q: -0.326766
 20400/100000: episode: 204, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -14.045, mean reward: -0.140 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.458, 10.298], loss: 0.003137, mae: 0.056910, mean_q: -0.288685
 20500/100000: episode: 205, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.040, mean reward: -0.170 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.001, 10.129], loss: 0.003268, mae: 0.058539, mean_q: -0.315360
 20600/100000: episode: 206, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -15.579, mean reward: -0.156 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.204, 10.313], loss: 0.003575, mae: 0.062266, mean_q: -0.320151
 20700/100000: episode: 207, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.718, mean reward: -0.167 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.686, 10.221], loss: 0.003180, mae: 0.057824, mean_q: -0.315420
 20800/100000: episode: 208, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -14.987, mean reward: -0.150 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-1.242, 10.098], loss: 0.003091, mae: 0.057699, mean_q: -0.302314
 20900/100000: episode: 209, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -18.179, mean reward: -0.182 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.753, 10.248], loss: 0.003050, mae: 0.055688, mean_q: -0.314660
 21000/100000: episode: 210, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: -14.395, mean reward: -0.144 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.761, 10.098], loss: 0.003052, mae: 0.056389, mean_q: -0.302056
 21100/100000: episode: 211, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -18.356, mean reward: -0.184 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.697, 10.130], loss: 0.003425, mae: 0.057972, mean_q: -0.306778
 21200/100000: episode: 212, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -14.945, mean reward: -0.149 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.826, 10.120], loss: 0.003869, mae: 0.064832, mean_q: -0.277464
 21300/100000: episode: 213, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -17.781, mean reward: -0.178 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.100, 10.243], loss: 0.004381, mae: 0.064743, mean_q: -0.314377
 21400/100000: episode: 214, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -10.703, mean reward: -0.107 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.662, 10.348], loss: 0.003273, mae: 0.058705, mean_q: -0.322242
 21500/100000: episode: 215, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -14.529, mean reward: -0.145 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.710, 10.098], loss: 0.003032, mae: 0.055956, mean_q: -0.316621
 21600/100000: episode: 216, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -19.771, mean reward: -0.198 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.457, 10.169], loss: 0.002838, mae: 0.053670, mean_q: -0.344284
 21700/100000: episode: 217, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: -18.137, mean reward: -0.181 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.857, 10.100], loss: 0.005449, mae: 0.070131, mean_q: -0.287345
 21800/100000: episode: 218, duration: 0.669s, episode steps: 100, steps per second: 149, episode reward: -15.236, mean reward: -0.152 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.798, 10.098], loss: 0.003757, mae: 0.063381, mean_q: -0.318625
 21900/100000: episode: 219, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -18.180, mean reward: -0.182 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.137, 10.192], loss: 0.003302, mae: 0.059908, mean_q: -0.308242
 22000/100000: episode: 220, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.763, mean reward: -0.198 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.829, 10.098], loss: 0.002961, mae: 0.055739, mean_q: -0.280755
 22100/100000: episode: 221, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.480, mean reward: -0.185 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.671, 10.184], loss: 0.002958, mae: 0.054904, mean_q: -0.304324
 22200/100000: episode: 222, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -17.110, mean reward: -0.171 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.566, 10.275], loss: 0.002946, mae: 0.054928, mean_q: -0.321635
 22300/100000: episode: 223, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -15.250, mean reward: -0.153 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.723, 10.319], loss: 0.003256, mae: 0.058791, mean_q: -0.305775
 22400/100000: episode: 224, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -18.487, mean reward: -0.185 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.474, 10.227], loss: 0.003020, mae: 0.055347, mean_q: -0.339609
 22500/100000: episode: 225, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: -18.495, mean reward: -0.185 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.437, 10.098], loss: 0.003670, mae: 0.063480, mean_q: -0.313766
 22600/100000: episode: 226, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -16.242, mean reward: -0.162 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.263, 10.193], loss: 0.004189, mae: 0.069559, mean_q: -0.291312
 22700/100000: episode: 227, duration: 0.676s, episode steps: 100, steps per second: 148, episode reward: -17.799, mean reward: -0.178 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.326, 10.098], loss: 0.002803, mae: 0.054599, mean_q: -0.324601
 22800/100000: episode: 228, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -10.588, mean reward: -0.106 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.951, 10.098], loss: 0.003141, mae: 0.056698, mean_q: -0.306688
 22900/100000: episode: 229, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -14.930, mean reward: -0.149 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.255, 10.098], loss: 0.002797, mae: 0.053875, mean_q: -0.304510
 23000/100000: episode: 230, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -17.290, mean reward: -0.173 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.532, 10.225], loss: 0.002967, mae: 0.055041, mean_q: -0.323308
 23100/100000: episode: 231, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -10.976, mean reward: -0.110 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.529, 10.239], loss: 0.002971, mae: 0.055152, mean_q: -0.297138
 23200/100000: episode: 232, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -12.579, mean reward: -0.126 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.231, 10.368], loss: 0.003078, mae: 0.056380, mean_q: -0.276455
 23300/100000: episode: 233, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -19.318, mean reward: -0.193 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.302, 10.107], loss: 0.003033, mae: 0.056761, mean_q: -0.317741
 23400/100000: episode: 234, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -19.974, mean reward: -0.200 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.720, 10.126], loss: 0.002997, mae: 0.055350, mean_q: -0.323360
 23500/100000: episode: 235, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: -16.202, mean reward: -0.162 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.343, 10.241], loss: 0.003054, mae: 0.057289, mean_q: -0.291462
 23600/100000: episode: 236, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -19.454, mean reward: -0.195 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.825, 10.115], loss: 0.002929, mae: 0.055690, mean_q: -0.292566
 23700/100000: episode: 237, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -15.966, mean reward: -0.160 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.098, 10.098], loss: 0.002964, mae: 0.055339, mean_q: -0.326191
 23800/100000: episode: 238, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -9.697, mean reward: -0.097 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.225, 10.098], loss: 0.003122, mae: 0.056767, mean_q: -0.329984
 23900/100000: episode: 239, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.775, mean reward: -0.178 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.600, 10.098], loss: 0.004043, mae: 0.060728, mean_q: -0.311677
 24000/100000: episode: 240, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -15.626, mean reward: -0.156 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.887, 10.116], loss: 0.003600, mae: 0.061606, mean_q: -0.325593
 24100/100000: episode: 241, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -16.148, mean reward: -0.161 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.575, 10.098], loss: 0.003023, mae: 0.057900, mean_q: -0.312964
 24200/100000: episode: 242, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: -20.067, mean reward: -0.201 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.970, 10.098], loss: 0.003168, mae: 0.058667, mean_q: -0.332501
 24300/100000: episode: 243, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: -19.489, mean reward: -0.195 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.524, 10.277], loss: 0.002865, mae: 0.054218, mean_q: -0.307386
 24400/100000: episode: 244, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: -14.713, mean reward: -0.147 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.000, 10.239], loss: 0.002858, mae: 0.054866, mean_q: -0.324157
 24500/100000: episode: 245, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -19.159, mean reward: -0.192 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.515, 10.202], loss: 0.002869, mae: 0.053867, mean_q: -0.351090
 24600/100000: episode: 246, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: -17.966, mean reward: -0.180 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.000, 10.190], loss: 0.002903, mae: 0.055234, mean_q: -0.317956
 24700/100000: episode: 247, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -19.166, mean reward: -0.192 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.025, 10.163], loss: 0.003097, mae: 0.057410, mean_q: -0.311612
 24800/100000: episode: 248, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: -19.844, mean reward: -0.198 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.854, 10.098], loss: 0.002732, mae: 0.053400, mean_q: -0.340718
 24900/100000: episode: 249, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: -7.677, mean reward: -0.077 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.369, 10.098], loss: 0.002900, mae: 0.055283, mean_q: -0.315690
 25000/100000: episode: 250, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: -17.418, mean reward: -0.174 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.113, 10.098], loss: 0.002737, mae: 0.053344, mean_q: -0.309794
 25100/100000: episode: 251, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: -19.660, mean reward: -0.197 [-1.000, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.275, 10.098], loss: 0.002884, mae: 0.054589, mean_q: -0.319746
 25200/100000: episode: 252, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -13.628, mean reward: -0.136 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.984, 10.098], loss: 0.002967, mae: 0.056520, mean_q: -0.311944
 25300/100000: episode: 253, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -14.210, mean reward: -0.142 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.150, 10.098], loss: 0.002808, mae: 0.054664, mean_q: -0.302476
 25400/100000: episode: 254, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -9.203, mean reward: -0.092 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.148, 10.335], loss: 0.002865, mae: 0.055379, mean_q: -0.307918
 25500/100000: episode: 255, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: -14.901, mean reward: -0.149 [-1.000, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.805, 10.098], loss: 0.002908, mae: 0.054701, mean_q: -0.305180
 25600/100000: episode: 256, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -15.357, mean reward: -0.154 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.483, 10.554], loss: 0.002921, mae: 0.054769, mean_q: -0.293987
 25700/100000: episode: 257, duration: 0.623s, episode steps: 100, steps per second: 161, episode reward: -18.589, mean reward: -0.186 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.838, 10.215], loss: 0.005704, mae: 0.074649, mean_q: -0.311256
 25800/100000: episode: 258, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -17.971, mean reward: -0.180 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.279, 10.106], loss: 0.003192, mae: 0.058776, mean_q: -0.304146
 25900/100000: episode: 259, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -12.550, mean reward: -0.125 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.906, 10.249], loss: 0.003147, mae: 0.056146, mean_q: -0.305685
 26000/100000: episode: 260, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: -20.049, mean reward: -0.200 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.527, 10.196], loss: 0.004583, mae: 0.069144, mean_q: -0.296097
 26100/100000: episode: 261, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -18.224, mean reward: -0.182 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.033, 10.098], loss: 0.003061, mae: 0.056686, mean_q: -0.272352
 26200/100000: episode: 262, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.299, mean reward: -0.153 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.000, 10.098], loss: 0.003190, mae: 0.057372, mean_q: -0.299765
 26300/100000: episode: 263, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -14.882, mean reward: -0.149 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.450, 10.098], loss: 0.003125, mae: 0.057085, mean_q: -0.292791
 26400/100000: episode: 264, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.554, mean reward: -0.196 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.854, 10.168], loss: 0.002823, mae: 0.054134, mean_q: -0.284658
 26500/100000: episode: 265, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -18.952, mean reward: -0.190 [-1.000, 0.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.324, 10.098], loss: 0.003247, mae: 0.059373, mean_q: -0.323846
 26600/100000: episode: 266, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -19.472, mean reward: -0.195 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.821, 10.126], loss: 0.003050, mae: 0.056641, mean_q: -0.337894
 26700/100000: episode: 267, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -14.025, mean reward: -0.140 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.709, 10.223], loss: 0.002761, mae: 0.053201, mean_q: -0.308256
 26800/100000: episode: 268, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: -15.456, mean reward: -0.155 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.077, 10.098], loss: 0.002832, mae: 0.053845, mean_q: -0.328743
 26900/100000: episode: 269, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -19.034, mean reward: -0.190 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.755, 10.098], loss: 0.002685, mae: 0.052714, mean_q: -0.321647
 27000/100000: episode: 270, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -18.868, mean reward: -0.189 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.050, 10.185], loss: 0.002925, mae: 0.054733, mean_q: -0.323770
 27100/100000: episode: 271, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -19.407, mean reward: -0.194 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.718, 10.223], loss: 0.003112, mae: 0.056567, mean_q: -0.293165
 27200/100000: episode: 272, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -13.678, mean reward: -0.137 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.738, 10.098], loss: 0.002805, mae: 0.053449, mean_q: -0.285437
 27300/100000: episode: 273, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -14.819, mean reward: -0.148 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.907, 10.098], loss: 0.002806, mae: 0.053333, mean_q: -0.289758
 27400/100000: episode: 274, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -19.843, mean reward: -0.198 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.722, 10.098], loss: 0.002817, mae: 0.053992, mean_q: -0.301131
 27500/100000: episode: 275, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -18.939, mean reward: -0.189 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.619, 10.131], loss: 0.002785, mae: 0.054191, mean_q: -0.286375
 27600/100000: episode: 276, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: -20.577, mean reward: -0.206 [-1.000, 0.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.793, 10.098], loss: 0.002883, mae: 0.055023, mean_q: -0.297549
 27700/100000: episode: 277, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: -18.067, mean reward: -0.181 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.652, 10.119], loss: 0.004746, mae: 0.066902, mean_q: -0.320787
 27800/100000: episode: 278, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -15.599, mean reward: -0.156 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.531, 10.257], loss: 0.003560, mae: 0.060723, mean_q: -0.307445
 27900/100000: episode: 279, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -15.454, mean reward: -0.155 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.653, 10.098], loss: 0.002957, mae: 0.054973, mean_q: -0.299309
 28000/100000: episode: 280, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -19.026, mean reward: -0.190 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.006, 10.159], loss: 0.002838, mae: 0.052442, mean_q: -0.336108
 28100/100000: episode: 281, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -18.298, mean reward: -0.183 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.647, 10.098], loss: 0.002893, mae: 0.054452, mean_q: -0.327158
 28200/100000: episode: 282, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -19.754, mean reward: -0.198 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.534, 10.098], loss: 0.003019, mae: 0.055154, mean_q: -0.289075
 28300/100000: episode: 283, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.762, mean reward: -0.158 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.898, 10.347], loss: 0.002992, mae: 0.055350, mean_q: -0.288193
 28400/100000: episode: 284, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -18.348, mean reward: -0.183 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.961, 10.098], loss: 0.002634, mae: 0.051715, mean_q: -0.326602
 28500/100000: episode: 285, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: -17.467, mean reward: -0.175 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.537, 10.172], loss: 0.002884, mae: 0.053826, mean_q: -0.320779
 28600/100000: episode: 286, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -18.531, mean reward: -0.185 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.213, 10.139], loss: 0.002873, mae: 0.052968, mean_q: -0.328082
 28700/100000: episode: 287, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -6.534, mean reward: -0.065 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.890, 10.534], loss: 0.002529, mae: 0.049962, mean_q: -0.355599
 28800/100000: episode: 288, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -9.192, mean reward: -0.092 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.508, 10.432], loss: 0.002757, mae: 0.053810, mean_q: -0.333069
 28900/100000: episode: 289, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -19.740, mean reward: -0.197 [-1.000, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.987, 10.098], loss: 0.003131, mae: 0.056567, mean_q: -0.330258
 29000/100000: episode: 290, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -15.027, mean reward: -0.150 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.341, 10.098], loss: 0.002729, mae: 0.052692, mean_q: -0.326122
 29100/100000: episode: 291, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -17.650, mean reward: -0.177 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.014, 10.159], loss: 0.002739, mae: 0.053003, mean_q: -0.316993
 29200/100000: episode: 292, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -18.375, mean reward: -0.184 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.701, 10.174], loss: 0.002827, mae: 0.052949, mean_q: -0.337767
 29300/100000: episode: 293, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: -16.868, mean reward: -0.169 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.574, 10.101], loss: 0.008590, mae: 0.077206, mean_q: -0.343210
 29400/100000: episode: 294, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -12.052, mean reward: -0.121 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.795, 10.243], loss: 0.004184, mae: 0.065486, mean_q: -0.308267
 29500/100000: episode: 295, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -14.151, mean reward: -0.142 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.641, 10.098], loss: 0.002844, mae: 0.053429, mean_q: -0.294707
 29600/100000: episode: 296, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -16.507, mean reward: -0.165 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.561, 10.342], loss: 0.005594, mae: 0.072889, mean_q: -0.281072
 29700/100000: episode: 297, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -14.020, mean reward: -0.140 [-1.000, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.345, 10.098], loss: 0.002661, mae: 0.051566, mean_q: -0.312997
 29800/100000: episode: 298, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -13.852, mean reward: -0.139 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.843, 10.327], loss: 0.002651, mae: 0.051610, mean_q: -0.321431
 29900/100000: episode: 299, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -17.388, mean reward: -0.174 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.533, 10.328], loss: 0.002837, mae: 0.053251, mean_q: -0.289786
 30000/100000: episode: 300, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.733, mean reward: -0.167 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.314, 10.386], loss: 0.002786, mae: 0.052196, mean_q: -0.313963
 30100/100000: episode: 301, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -19.034, mean reward: -0.190 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.453, 10.173], loss: 0.002820, mae: 0.052739, mean_q: -0.323191
 30200/100000: episode: 302, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: -20.523, mean reward: -0.205 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.213, 10.131], loss: 0.002642, mae: 0.051046, mean_q: -0.325862
 30300/100000: episode: 303, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: -17.063, mean reward: -0.171 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.200, 10.098], loss: 0.003030, mae: 0.054518, mean_q: -0.315125
 30400/100000: episode: 304, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.668, mean reward: -0.177 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.143, 10.248], loss: 0.002902, mae: 0.054388, mean_q: -0.301089
 30500/100000: episode: 305, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -15.319, mean reward: -0.153 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.908, 10.098], loss: 0.002740, mae: 0.052103, mean_q: -0.297982
 30600/100000: episode: 306, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -15.627, mean reward: -0.156 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.591, 10.454], loss: 0.002811, mae: 0.053085, mean_q: -0.336656
 30700/100000: episode: 307, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: -16.942, mean reward: -0.169 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.127, 10.207], loss: 0.002683, mae: 0.051699, mean_q: -0.336381
 30800/100000: episode: 308, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -13.209, mean reward: -0.132 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.545, 10.338], loss: 0.002894, mae: 0.053259, mean_q: -0.313159
 30900/100000: episode: 309, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -16.190, mean reward: -0.162 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.540, 10.098], loss: 0.003637, mae: 0.061797, mean_q: -0.290262
 31000/100000: episode: 310, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -18.540, mean reward: -0.185 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.574, 10.098], loss: 0.003327, mae: 0.058236, mean_q: -0.302576
 31100/100000: episode: 311, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: -14.583, mean reward: -0.146 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.677, 10.098], loss: 0.002783, mae: 0.052955, mean_q: -0.334707
 31200/100000: episode: 312, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -19.140, mean reward: -0.191 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.390, 10.123], loss: 0.002771, mae: 0.052804, mean_q: -0.293636
 31300/100000: episode: 313, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -19.172, mean reward: -0.192 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.192, 10.098], loss: 0.002764, mae: 0.052964, mean_q: -0.290421
 31400/100000: episode: 314, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -17.602, mean reward: -0.176 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.003, 10.467], loss: 0.002725, mae: 0.052141, mean_q: -0.346563
 31500/100000: episode: 315, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -19.477, mean reward: -0.195 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.854, 10.238], loss: 0.004885, mae: 0.064196, mean_q: -0.326084
 31600/100000: episode: 316, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -15.365, mean reward: -0.154 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.941, 10.098], loss: 0.005769, mae: 0.073487, mean_q: -0.317036
 31700/100000: episode: 317, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -16.127, mean reward: -0.161 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.871, 10.184], loss: 0.002926, mae: 0.054689, mean_q: -0.317080
 31800/100000: episode: 318, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -17.175, mean reward: -0.172 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.635, 10.098], loss: 0.002758, mae: 0.052469, mean_q: -0.326874
 31900/100000: episode: 319, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: -18.898, mean reward: -0.189 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.178, 10.098], loss: 0.003123, mae: 0.056095, mean_q: -0.310552
 32000/100000: episode: 320, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: -15.944, mean reward: -0.159 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.653, 10.098], loss: 0.002847, mae: 0.052602, mean_q: -0.334633
 32100/100000: episode: 321, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: -18.743, mean reward: -0.187 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.895, 10.098], loss: 0.002827, mae: 0.052759, mean_q: -0.315650
 32200/100000: episode: 322, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -16.233, mean reward: -0.162 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.589, 10.136], loss: 0.002755, mae: 0.051677, mean_q: -0.349436
 32300/100000: episode: 323, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -15.115, mean reward: -0.151 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.583, 10.098], loss: 0.002978, mae: 0.053875, mean_q: -0.319119
 32400/100000: episode: 324, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -18.095, mean reward: -0.181 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.742, 10.098], loss: 0.002821, mae: 0.053130, mean_q: -0.295830
 32500/100000: episode: 325, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.425, mean reward: -0.194 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.617, 10.098], loss: 0.002762, mae: 0.051151, mean_q: -0.318550
 32600/100000: episode: 326, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.138, mean reward: -0.161 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.826, 10.098], loss: 0.002793, mae: 0.051973, mean_q: -0.294952
 32700/100000: episode: 327, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -13.011, mean reward: -0.130 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.769, 10.297], loss: 0.002851, mae: 0.053308, mean_q: -0.303817
 32800/100000: episode: 328, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: -17.585, mean reward: -0.176 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.827, 10.112], loss: 0.002918, mae: 0.053674, mean_q: -0.330848
 32900/100000: episode: 329, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -17.947, mean reward: -0.179 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.385, 10.098], loss: 0.002855, mae: 0.054144, mean_q: -0.308946
 33000/100000: episode: 330, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: -16.493, mean reward: -0.165 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.341, 10.154], loss: 0.002810, mae: 0.052810, mean_q: -0.284889
 33100/100000: episode: 331, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.767, mean reward: -0.198 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.289, 10.098], loss: 0.002933, mae: 0.053712, mean_q: -0.291374
 33200/100000: episode: 332, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -19.186, mean reward: -0.192 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.462, 10.150], loss: 0.002880, mae: 0.053488, mean_q: -0.306458
 33300/100000: episode: 333, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -18.460, mean reward: -0.185 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.811, 10.098], loss: 0.002771, mae: 0.052995, mean_q: -0.315677
 33400/100000: episode: 334, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -17.897, mean reward: -0.179 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.277, 10.098], loss: 0.003671, mae: 0.059745, mean_q: -0.288189
 33500/100000: episode: 335, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -14.510, mean reward: -0.145 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.940, 10.098], loss: 0.005221, mae: 0.066853, mean_q: -0.335434
 33600/100000: episode: 336, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: -21.001, mean reward: -0.210 [-1.000, 0.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.596, 10.295], loss: 0.002887, mae: 0.054724, mean_q: -0.312274
 33700/100000: episode: 337, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -16.253, mean reward: -0.163 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.904, 10.307], loss: 0.002989, mae: 0.054828, mean_q: -0.298127
 33800/100000: episode: 338, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -19.071, mean reward: -0.191 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.527, 10.098], loss: 0.002672, mae: 0.051493, mean_q: -0.341582
 33900/100000: episode: 339, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: -15.297, mean reward: -0.153 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.207, 10.098], loss: 0.002777, mae: 0.052591, mean_q: -0.342420
 34000/100000: episode: 340, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -16.932, mean reward: -0.169 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.979, 10.373], loss: 0.002823, mae: 0.053336, mean_q: -0.306759
 34100/100000: episode: 341, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -16.267, mean reward: -0.163 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.921, 10.175], loss: 0.002624, mae: 0.051547, mean_q: -0.312802
 34200/100000: episode: 342, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -18.622, mean reward: -0.186 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.307, 10.098], loss: 0.002659, mae: 0.052539, mean_q: -0.280758
 34300/100000: episode: 343, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: -17.346, mean reward: -0.173 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.988, 10.170], loss: 0.003005, mae: 0.055206, mean_q: -0.295936
 34400/100000: episode: 344, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -18.758, mean reward: -0.188 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.911, 10.098], loss: 0.002754, mae: 0.053373, mean_q: -0.307202
 34500/100000: episode: 345, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: -17.102, mean reward: -0.171 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.291, 10.098], loss: 0.002868, mae: 0.054467, mean_q: -0.313421
 34600/100000: episode: 346, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -16.681, mean reward: -0.167 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.985, 10.098], loss: 0.002623, mae: 0.051084, mean_q: -0.328095
 34700/100000: episode: 347, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -19.195, mean reward: -0.192 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.503, 10.164], loss: 0.002519, mae: 0.050267, mean_q: -0.337775
 34800/100000: episode: 348, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -18.958, mean reward: -0.190 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.414, 10.107], loss: 0.002742, mae: 0.052820, mean_q: -0.338892
 34900/100000: episode: 349, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.476, mean reward: -0.185 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.700, 10.188], loss: 0.002459, mae: 0.050244, mean_q: -0.341796
 35000/100000: episode: 350, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: -15.979, mean reward: -0.160 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.997, 10.322], loss: 0.002572, mae: 0.051362, mean_q: -0.329502
 35100/100000: episode: 351, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -12.773, mean reward: -0.128 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.744, 10.156], loss: 0.002489, mae: 0.050274, mean_q: -0.328505
 35200/100000: episode: 352, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: -14.473, mean reward: -0.145 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.810, 10.098], loss: 0.002731, mae: 0.051919, mean_q: -0.310430
 35300/100000: episode: 353, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: -14.877, mean reward: -0.149 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.322, 10.180], loss: 0.002595, mae: 0.050882, mean_q: -0.310332
 35400/100000: episode: 354, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.130, mean reward: -0.161 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.062, 10.384], loss: 0.003561, mae: 0.059515, mean_q: -0.303564
 35500/100000: episode: 355, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -16.475, mean reward: -0.165 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.202, 10.296], loss: 0.002717, mae: 0.052506, mean_q: -0.307561
 35600/100000: episode: 356, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -19.864, mean reward: -0.199 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.177, 10.187], loss: 0.002627, mae: 0.053535, mean_q: -0.291317
 35700/100000: episode: 357, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.623, mean reward: -0.176 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.841, 10.098], loss: 0.002464, mae: 0.050265, mean_q: -0.331570
 35800/100000: episode: 358, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -17.058, mean reward: -0.171 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.519, 10.098], loss: 0.002542, mae: 0.051445, mean_q: -0.356585
 35900/100000: episode: 359, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -19.171, mean reward: -0.192 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.889, 10.159], loss: 0.002570, mae: 0.051937, mean_q: -0.340551
 36000/100000: episode: 360, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: -17.449, mean reward: -0.174 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.969, 10.098], loss: 0.002466, mae: 0.049753, mean_q: -0.329180
 36100/100000: episode: 361, duration: 0.661s, episode steps: 100, steps per second: 151, episode reward: -19.073, mean reward: -0.191 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.887, 10.247], loss: 0.002487, mae: 0.049440, mean_q: -0.334793
 36200/100000: episode: 362, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: -14.903, mean reward: -0.149 [-1.000, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.068, 10.235], loss: 0.002303, mae: 0.047369, mean_q: -0.361634
 36300/100000: episode: 363, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -18.679, mean reward: -0.187 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.864, 10.308], loss: 0.002665, mae: 0.051393, mean_q: -0.338973
 36400/100000: episode: 364, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -7.546, mean reward: -0.075 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.110, 10.098], loss: 0.002516, mae: 0.050505, mean_q: -0.331709
 36500/100000: episode: 365, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -19.161, mean reward: -0.192 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.902, 10.174], loss: 0.002522, mae: 0.051772, mean_q: -0.342497
 36600/100000: episode: 366, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -17.256, mean reward: -0.173 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.075, 10.116], loss: 0.002601, mae: 0.051240, mean_q: -0.287414
 36700/100000: episode: 367, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -16.021, mean reward: -0.160 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.328, 10.350], loss: 0.003169, mae: 0.054364, mean_q: -0.301992
 36800/100000: episode: 368, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -17.214, mean reward: -0.172 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.858, 10.185], loss: 0.006385, mae: 0.075092, mean_q: -0.311151
 36900/100000: episode: 369, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -19.040, mean reward: -0.190 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.133, 10.182], loss: 0.002836, mae: 0.054224, mean_q: -0.339491
 37000/100000: episode: 370, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: -16.719, mean reward: -0.167 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.956, 10.164], loss: 0.002660, mae: 0.052274, mean_q: -0.290302
 37100/100000: episode: 371, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: -10.843, mean reward: -0.108 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.253, 10.098], loss: 0.002528, mae: 0.049938, mean_q: -0.357994
 37200/100000: episode: 372, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -15.997, mean reward: -0.160 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.306, 10.098], loss: 0.002439, mae: 0.049453, mean_q: -0.344430
 37300/100000: episode: 373, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -16.407, mean reward: -0.164 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.949, 10.098], loss: 0.002519, mae: 0.049327, mean_q: -0.331829
 37400/100000: episode: 374, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -18.595, mean reward: -0.186 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.671, 10.098], loss: 0.002593, mae: 0.051504, mean_q: -0.325413
 37500/100000: episode: 375, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -11.460, mean reward: -0.115 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.778, 10.357], loss: 0.002697, mae: 0.052835, mean_q: -0.345009
 37600/100000: episode: 376, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -19.464, mean reward: -0.195 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.314, 10.151], loss: 0.002717, mae: 0.052399, mean_q: -0.302908
 37700/100000: episode: 377, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -17.291, mean reward: -0.173 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.077, 10.389], loss: 0.002859, mae: 0.053074, mean_q: -0.329549
 37800/100000: episode: 378, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -20.706, mean reward: -0.207 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.586, 10.098], loss: 0.002578, mae: 0.050372, mean_q: -0.364067
 37900/100000: episode: 379, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: -15.406, mean reward: -0.154 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.353, 10.098], loss: 0.002579, mae: 0.051898, mean_q: -0.336783
 38000/100000: episode: 380, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.483, mean reward: -0.175 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.610, 10.098], loss: 0.002728, mae: 0.052505, mean_q: -0.311172
 38100/100000: episode: 381, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -15.443, mean reward: -0.154 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.571, 10.098], loss: 0.002730, mae: 0.052154, mean_q: -0.329533
 38200/100000: episode: 382, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -19.358, mean reward: -0.194 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.040, 10.135], loss: 0.002706, mae: 0.051560, mean_q: -0.345135
 38300/100000: episode: 383, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.532, mean reward: -0.175 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.625, 10.222], loss: 0.003046, mae: 0.056925, mean_q: -0.335808
 38400/100000: episode: 384, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -13.784, mean reward: -0.138 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.817, 10.116], loss: 0.002886, mae: 0.053168, mean_q: -0.344456
 38500/100000: episode: 385, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -16.937, mean reward: -0.169 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.911, 10.098], loss: 0.002780, mae: 0.052552, mean_q: -0.289892
 38600/100000: episode: 386, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -20.572, mean reward: -0.206 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.616, 10.150], loss: 0.004316, mae: 0.063734, mean_q: -0.321418
 38700/100000: episode: 387, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: -20.224, mean reward: -0.202 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.450, 10.098], loss: 0.002992, mae: 0.055660, mean_q: -0.347125
 38800/100000: episode: 388, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -12.607, mean reward: -0.126 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.716, 10.372], loss: 0.002719, mae: 0.051576, mean_q: -0.313454
 38900/100000: episode: 389, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.526, mean reward: -0.185 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.331, 10.304], loss: 0.002677, mae: 0.051026, mean_q: -0.319331
 39000/100000: episode: 390, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -15.069, mean reward: -0.151 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.718, 10.098], loss: 0.002801, mae: 0.052593, mean_q: -0.309787
 39100/100000: episode: 391, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -17.322, mean reward: -0.173 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.912, 10.154], loss: 0.002831, mae: 0.052971, mean_q: -0.296724
 39200/100000: episode: 392, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: -17.763, mean reward: -0.178 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.385, 10.098], loss: 0.002653, mae: 0.051583, mean_q: -0.295816
 39300/100000: episode: 393, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: -13.895, mean reward: -0.139 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-2.071, 10.121], loss: 0.002809, mae: 0.054043, mean_q: -0.308532
 39400/100000: episode: 394, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -11.730, mean reward: -0.117 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.703, 10.098], loss: 0.002674, mae: 0.051328, mean_q: -0.343569
 39500/100000: episode: 395, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: -18.949, mean reward: -0.189 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.441, 10.098], loss: 0.002765, mae: 0.052155, mean_q: -0.311149
 39600/100000: episode: 396, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: -18.979, mean reward: -0.190 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.772, 10.239], loss: 0.002713, mae: 0.052134, mean_q: -0.309845
 39700/100000: episode: 397, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.974, mean reward: -0.180 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.740, 10.184], loss: 0.002888, mae: 0.052824, mean_q: -0.309911
 39800/100000: episode: 398, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -12.816, mean reward: -0.128 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.774, 10.098], loss: 0.002845, mae: 0.052536, mean_q: -0.354751
 39900/100000: episode: 399, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -18.623, mean reward: -0.186 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.393, 10.135], loss: 0.002764, mae: 0.051724, mean_q: -0.325737
 40000/100000: episode: 400, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -15.083, mean reward: -0.151 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.373, 10.279], loss: 0.002640, mae: 0.052092, mean_q: -0.355578
 40100/100000: episode: 401, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -14.885, mean reward: -0.149 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.059, 10.098], loss: 0.002725, mae: 0.052163, mean_q: -0.325494
 40200/100000: episode: 402, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -17.117, mean reward: -0.171 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.665, 10.118], loss: 0.002681, mae: 0.051917, mean_q: -0.317078
 40300/100000: episode: 403, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -18.592, mean reward: -0.186 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.774, 10.098], loss: 0.003303, mae: 0.058421, mean_q: -0.317387
 40400/100000: episode: 404, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: -18.207, mean reward: -0.182 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.562, 10.257], loss: 0.002714, mae: 0.051004, mean_q: -0.342017
 40500/100000: episode: 405, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -14.712, mean reward: -0.147 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.853, 10.271], loss: 0.002862, mae: 0.053181, mean_q: -0.314381
 40600/100000: episode: 406, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -15.942, mean reward: -0.159 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.735, 10.360], loss: 0.002783, mae: 0.052584, mean_q: -0.314414
 40700/100000: episode: 407, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -17.781, mean reward: -0.178 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.340, 10.192], loss: 0.002693, mae: 0.051074, mean_q: -0.315888
 40800/100000: episode: 408, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.433, mean reward: -0.194 [-1.000, 0.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.662, 10.222], loss: 0.002822, mae: 0.052083, mean_q: -0.339781
 40900/100000: episode: 409, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -16.161, mean reward: -0.162 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.955, 10.098], loss: 0.002518, mae: 0.049711, mean_q: -0.305733
 41000/100000: episode: 410, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -19.691, mean reward: -0.197 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.159, 10.098], loss: 0.002533, mae: 0.049989, mean_q: -0.315623
 41100/100000: episode: 411, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -14.714, mean reward: -0.147 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.658, 10.168], loss: 0.004135, mae: 0.062371, mean_q: -0.322724
 41200/100000: episode: 412, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: -17.029, mean reward: -0.170 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.472, 10.242], loss: 0.002819, mae: 0.052316, mean_q: -0.308964
 41300/100000: episode: 413, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -18.867, mean reward: -0.189 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.839, 10.098], loss: 0.002518, mae: 0.050193, mean_q: -0.309213
 41400/100000: episode: 414, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -16.723, mean reward: -0.167 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.043, 10.098], loss: 0.002788, mae: 0.051543, mean_q: -0.305855
 41500/100000: episode: 415, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.125, mean reward: -0.181 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.693, 10.214], loss: 0.002590, mae: 0.050503, mean_q: -0.284585
 41600/100000: episode: 416, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -11.668, mean reward: -0.117 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.107, 10.397], loss: 0.002612, mae: 0.050532, mean_q: -0.342227
 41700/100000: episode: 417, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -13.696, mean reward: -0.137 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.545, 10.098], loss: 0.002786, mae: 0.052688, mean_q: -0.306496
 41800/100000: episode: 418, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.672, mean reward: -0.187 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.480, 10.141], loss: 0.002652, mae: 0.050947, mean_q: -0.314853
 41900/100000: episode: 419, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -18.162, mean reward: -0.182 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.612, 10.098], loss: 0.002578, mae: 0.050439, mean_q: -0.288909
 42000/100000: episode: 420, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -12.957, mean reward: -0.130 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.802, 10.330], loss: 0.002550, mae: 0.050408, mean_q: -0.314898
 42100/100000: episode: 421, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: -15.210, mean reward: -0.152 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.045, 10.342], loss: 0.002733, mae: 0.051971, mean_q: -0.280631
 42200/100000: episode: 422, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -21.722, mean reward: -0.217 [-1.000, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.587, 10.164], loss: 0.002711, mae: 0.052355, mean_q: -0.324619
 42300/100000: episode: 423, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -18.187, mean reward: -0.182 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.623, 10.142], loss: 0.002594, mae: 0.051327, mean_q: -0.305489
 42400/100000: episode: 424, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -17.113, mean reward: -0.171 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.386, 10.098], loss: 0.002667, mae: 0.051096, mean_q: -0.314969
 42500/100000: episode: 425, duration: 0.612s, episode steps: 100, steps per second: 164, episode reward: -13.098, mean reward: -0.131 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.644, 10.098], loss: 0.002707, mae: 0.051712, mean_q: -0.302240
 42600/100000: episode: 426, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.163, mean reward: -0.182 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.737, 10.127], loss: 0.002618, mae: 0.051199, mean_q: -0.328933
 42700/100000: episode: 427, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -14.493, mean reward: -0.145 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.904, 10.352], loss: 0.002671, mae: 0.051557, mean_q: -0.341381
 42800/100000: episode: 428, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -17.890, mean reward: -0.179 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.810, 10.139], loss: 0.002694, mae: 0.052263, mean_q: -0.315003
 42900/100000: episode: 429, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -19.106, mean reward: -0.191 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.726, 10.169], loss: 0.002727, mae: 0.052021, mean_q: -0.316804
 43000/100000: episode: 430, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -16.396, mean reward: -0.164 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.328, 10.098], loss: 0.003515, mae: 0.054317, mean_q: -0.302308
 43100/100000: episode: 431, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -12.551, mean reward: -0.126 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.017, 10.098], loss: 0.007020, mae: 0.081338, mean_q: -0.324643
 43200/100000: episode: 432, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.048, mean reward: -0.190 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.457, 10.235], loss: 0.002622, mae: 0.052687, mean_q: -0.318468
 43300/100000: episode: 433, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -16.957, mean reward: -0.170 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.444, 10.098], loss: 0.002738, mae: 0.052422, mean_q: -0.303427
 43400/100000: episode: 434, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -14.843, mean reward: -0.148 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.716, 10.098], loss: 0.002356, mae: 0.048345, mean_q: -0.325452
 43500/100000: episode: 435, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -18.843, mean reward: -0.188 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.392, 10.250], loss: 0.002714, mae: 0.051925, mean_q: -0.324320
 43600/100000: episode: 436, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -18.529, mean reward: -0.185 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.605, 10.306], loss: 0.002535, mae: 0.049316, mean_q: -0.332436
 43700/100000: episode: 437, duration: 0.675s, episode steps: 100, steps per second: 148, episode reward: -18.048, mean reward: -0.180 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.403, 10.098], loss: 0.002524, mae: 0.049566, mean_q: -0.321593
 43800/100000: episode: 438, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: -14.665, mean reward: -0.147 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.783, 10.098], loss: 0.002564, mae: 0.049655, mean_q: -0.314321
 43900/100000: episode: 439, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -19.719, mean reward: -0.197 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.710, 10.249], loss: 0.002636, mae: 0.051559, mean_q: -0.298740
 44000/100000: episode: 440, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -18.854, mean reward: -0.189 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.830, 10.132], loss: 0.002569, mae: 0.050700, mean_q: -0.278017
 44100/100000: episode: 441, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: -18.386, mean reward: -0.184 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.694, 10.219], loss: 0.002681, mae: 0.050294, mean_q: -0.355270
 44200/100000: episode: 442, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: -17.803, mean reward: -0.178 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.444, 10.233], loss: 0.002528, mae: 0.049858, mean_q: -0.287198
 44300/100000: episode: 443, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -15.917, mean reward: -0.159 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.637, 10.191], loss: 0.002539, mae: 0.050194, mean_q: -0.317995
 44400/100000: episode: 444, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -17.354, mean reward: -0.174 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.278, 10.098], loss: 0.002490, mae: 0.048861, mean_q: -0.315547
 44500/100000: episode: 445, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -18.402, mean reward: -0.184 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.455, 10.232], loss: 0.002581, mae: 0.050308, mean_q: -0.318220
 44600/100000: episode: 446, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: -15.398, mean reward: -0.154 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.174, 10.098], loss: 0.002648, mae: 0.049706, mean_q: -0.352432
 44700/100000: episode: 447, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -20.177, mean reward: -0.202 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.527, 10.106], loss: 0.002205, mae: 0.046869, mean_q: -0.297660
 44800/100000: episode: 448, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.105, mean reward: -0.171 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.574, 10.098], loss: 0.002595, mae: 0.050413, mean_q: -0.326298
 44900/100000: episode: 449, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -15.153, mean reward: -0.152 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.489, 10.337], loss: 0.002366, mae: 0.048440, mean_q: -0.349865
 45000/100000: episode: 450, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.737, mean reward: -0.167 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.251, 10.389], loss: 0.002519, mae: 0.049523, mean_q: -0.344204
 45100/100000: episode: 451, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -15.926, mean reward: -0.159 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.596, 10.389], loss: 0.002371, mae: 0.048281, mean_q: -0.339319
 45200/100000: episode: 452, duration: 0.639s, episode steps: 100, steps per second: 157, episode reward: -18.392, mean reward: -0.184 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.396, 10.098], loss: 0.002389, mae: 0.048487, mean_q: -0.346216
 45300/100000: episode: 453, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -19.894, mean reward: -0.199 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.618, 10.098], loss: 0.002518, mae: 0.049885, mean_q: -0.330887
 45400/100000: episode: 454, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: -18.720, mean reward: -0.187 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.835, 10.098], loss: 0.002251, mae: 0.047166, mean_q: -0.348861
 45500/100000: episode: 455, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: -15.099, mean reward: -0.151 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.439, 10.348], loss: 0.003140, mae: 0.056058, mean_q: -0.314155
 45600/100000: episode: 456, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -16.220, mean reward: -0.162 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.699, 10.098], loss: 0.003321, mae: 0.058826, mean_q: -0.335828
 45700/100000: episode: 457, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -20.225, mean reward: -0.202 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.433, 10.098], loss: 0.002583, mae: 0.050797, mean_q: -0.304118
 45800/100000: episode: 458, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -11.205, mean reward: -0.112 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.610, 10.098], loss: 0.002609, mae: 0.051933, mean_q: -0.302592
 45900/100000: episode: 459, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -7.546, mean reward: -0.075 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.683, 10.523], loss: 0.002534, mae: 0.051429, mean_q: -0.291699
 46000/100000: episode: 460, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: -17.792, mean reward: -0.178 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.540, 10.121], loss: 0.002398, mae: 0.049708, mean_q: -0.331825
 46100/100000: episode: 461, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -18.000, mean reward: -0.180 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.398, 10.098], loss: 0.002579, mae: 0.050681, mean_q: -0.331396
 46200/100000: episode: 462, duration: 0.652s, episode steps: 100, steps per second: 153, episode reward: -18.197, mean reward: -0.182 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.906, 10.194], loss: 0.002370, mae: 0.049280, mean_q: -0.307552
 46300/100000: episode: 463, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: -15.801, mean reward: -0.158 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.267, 10.416], loss: 0.002539, mae: 0.049728, mean_q: -0.307855
 46400/100000: episode: 464, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -19.073, mean reward: -0.191 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.490, 10.098], loss: 0.002588, mae: 0.050997, mean_q: -0.340875
 46500/100000: episode: 465, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -15.425, mean reward: -0.154 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.350, 10.224], loss: 0.002574, mae: 0.049612, mean_q: -0.357540
 46600/100000: episode: 466, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -18.594, mean reward: -0.186 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.929, 10.098], loss: 0.002463, mae: 0.049725, mean_q: -0.301811
 46700/100000: episode: 467, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -18.643, mean reward: -0.186 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.889, 10.143], loss: 0.002709, mae: 0.051264, mean_q: -0.333300
 46800/100000: episode: 468, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: -7.549, mean reward: -0.075 [-1.000, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.380, 10.315], loss: 0.002658, mae: 0.051038, mean_q: -0.310268
 46900/100000: episode: 469, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -17.510, mean reward: -0.175 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.315, 10.098], loss: 0.002433, mae: 0.050569, mean_q: -0.304733
 47000/100000: episode: 470, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: -13.971, mean reward: -0.140 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.162, 10.371], loss: 0.002863, mae: 0.054362, mean_q: -0.339959
 47100/100000: episode: 471, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: -20.534, mean reward: -0.205 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.564, 10.098], loss: 0.006765, mae: 0.073686, mean_q: -0.293164
 47200/100000: episode: 472, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.274, mean reward: -0.183 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.533, 10.098], loss: 0.002678, mae: 0.052664, mean_q: -0.301737
 47300/100000: episode: 473, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -16.021, mean reward: -0.160 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.242, 10.183], loss: 0.002542, mae: 0.049645, mean_q: -0.314922
 47400/100000: episode: 474, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.227, mean reward: -0.162 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.605, 10.098], loss: 0.002555, mae: 0.050267, mean_q: -0.310365
 47500/100000: episode: 475, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -20.487, mean reward: -0.205 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.670, 10.103], loss: 0.002470, mae: 0.049321, mean_q: -0.319346
 47600/100000: episode: 476, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -19.887, mean reward: -0.199 [-1.000, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.692, 10.200], loss: 0.002514, mae: 0.049052, mean_q: -0.311822
 47700/100000: episode: 477, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.080, mean reward: -0.151 [-1.000, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.985, 10.098], loss: 0.002659, mae: 0.050725, mean_q: -0.327709
 47800/100000: episode: 478, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.636, mean reward: -0.186 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.264, 10.150], loss: 0.002357, mae: 0.048234, mean_q: -0.361914
 47900/100000: episode: 479, duration: 0.652s, episode steps: 100, steps per second: 153, episode reward: -18.171, mean reward: -0.182 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.232, 10.098], loss: 0.002622, mae: 0.049236, mean_q: -0.331640
 48000/100000: episode: 480, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -17.457, mean reward: -0.175 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.326, 10.098], loss: 0.002721, mae: 0.051873, mean_q: -0.310938
 48100/100000: episode: 481, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -15.269, mean reward: -0.153 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.528, 10.210], loss: 0.002552, mae: 0.049403, mean_q: -0.366361
 48200/100000: episode: 482, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -17.369, mean reward: -0.174 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.385, 10.098], loss: 0.002630, mae: 0.050268, mean_q: -0.340259
 48300/100000: episode: 483, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -15.752, mean reward: -0.158 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.798, 10.159], loss: 0.002554, mae: 0.049748, mean_q: -0.328147
 48400/100000: episode: 484, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -16.618, mean reward: -0.166 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.442, 10.160], loss: 0.002544, mae: 0.048928, mean_q: -0.345064
 48500/100000: episode: 485, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -12.184, mean reward: -0.122 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.338, 10.258], loss: 0.002451, mae: 0.048964, mean_q: -0.344243
 48600/100000: episode: 486, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -16.699, mean reward: -0.167 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.369, 10.306], loss: 0.002708, mae: 0.050961, mean_q: -0.351567
 48700/100000: episode: 487, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: -18.177, mean reward: -0.182 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.209, 10.098], loss: 0.004165, mae: 0.060780, mean_q: -0.354021
 48800/100000: episode: 488, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: -18.285, mean reward: -0.183 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.343, 10.327], loss: 0.002854, mae: 0.054226, mean_q: -0.337823
 48900/100000: episode: 489, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: -13.823, mean reward: -0.138 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.161, 10.098], loss: 0.002571, mae: 0.050084, mean_q: -0.316419
 49000/100000: episode: 490, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: -17.197, mean reward: -0.172 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.955, 10.156], loss: 0.002651, mae: 0.051719, mean_q: -0.305975
 49100/100000: episode: 491, duration: 0.697s, episode steps: 100, steps per second: 143, episode reward: -17.847, mean reward: -0.178 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.829, 10.206], loss: 0.002676, mae: 0.051410, mean_q: -0.309713
 49200/100000: episode: 492, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -19.119, mean reward: -0.191 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.315, 10.166], loss: 0.002824, mae: 0.052297, mean_q: -0.319683
 49300/100000: episode: 493, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: -17.165, mean reward: -0.172 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.752, 10.098], loss: 0.002424, mae: 0.047789, mean_q: -0.363459
 49400/100000: episode: 494, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: -16.675, mean reward: -0.167 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.283, 10.158], loss: 0.003156, mae: 0.055550, mean_q: -0.314733
 49500/100000: episode: 495, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: -16.756, mean reward: -0.168 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.254, 10.098], loss: 0.002599, mae: 0.050006, mean_q: -0.320772
 49600/100000: episode: 496, duration: 0.661s, episode steps: 100, steps per second: 151, episode reward: -15.252, mean reward: -0.153 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.855, 10.210], loss: 0.002618, mae: 0.050052, mean_q: -0.330466
 49700/100000: episode: 497, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: -15.033, mean reward: -0.150 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.859, 10.254], loss: 0.002634, mae: 0.050537, mean_q: -0.317690
 49800/100000: episode: 498, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: -7.517, mean reward: -0.075 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.390, 10.536], loss: 0.002535, mae: 0.049242, mean_q: -0.319672
 49900/100000: episode: 499, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: -15.159, mean reward: -0.152 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.935, 10.393], loss: 0.002595, mae: 0.050448, mean_q: -0.286142
 50000/100000: episode: 500, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: -18.169, mean reward: -0.182 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.591, 10.148], loss: 0.002350, mae: 0.048509, mean_q: -0.294069
 50100/100000: episode: 501, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: -18.067, mean reward: -0.181 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.475, 10.169], loss: 0.002433, mae: 0.049219, mean_q: -0.308162
 50200/100000: episode: 502, duration: 1.044s, episode steps: 100, steps per second: 96, episode reward: -17.147, mean reward: -0.171 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.695, 10.098], loss: 0.002846, mae: 0.052366, mean_q: -0.295627
 50300/100000: episode: 503, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: -19.780, mean reward: -0.198 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.627, 10.098], loss: 0.002619, mae: 0.050582, mean_q: -0.331990
 50400/100000: episode: 504, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: -16.370, mean reward: -0.164 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.632, 10.119], loss: 0.002721, mae: 0.052222, mean_q: -0.312086
 50500/100000: episode: 505, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: -16.546, mean reward: -0.165 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.768, 10.232], loss: 0.002755, mae: 0.052044, mean_q: -0.309986
 50600/100000: episode: 506, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: -13.158, mean reward: -0.132 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.358, 10.307], loss: 0.002722, mae: 0.052039, mean_q: -0.311554
 50700/100000: episode: 507, duration: 0.689s, episode steps: 100, steps per second: 145, episode reward: -20.161, mean reward: -0.202 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.751, 10.098], loss: 0.002542, mae: 0.050071, mean_q: -0.319212
 50800/100000: episode: 508, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: -16.304, mean reward: -0.163 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.844, 10.183], loss: 0.002728, mae: 0.052084, mean_q: -0.274452
 50900/100000: episode: 509, duration: 0.661s, episode steps: 100, steps per second: 151, episode reward: -10.273, mean reward: -0.103 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.204, 10.297], loss: 0.002696, mae: 0.051859, mean_q: -0.306225
 51000/100000: episode: 510, duration: 0.631s, episode steps: 100, steps per second: 158, episode reward: -16.320, mean reward: -0.163 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.530, 10.158], loss: 0.002876, mae: 0.053366, mean_q: -0.273459
 51100/100000: episode: 511, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: -19.472, mean reward: -0.195 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.361, 10.098], loss: 0.003264, mae: 0.059665, mean_q: -0.284020
 51200/100000: episode: 512, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: -20.285, mean reward: -0.203 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-2.053, 10.098], loss: 0.003072, mae: 0.058178, mean_q: -0.290905
 51300/100000: episode: 513, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: -18.600, mean reward: -0.186 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.869, 10.098], loss: 0.002793, mae: 0.053779, mean_q: -0.315260
 51400/100000: episode: 514, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: -14.197, mean reward: -0.142 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.986, 10.098], loss: 0.002686, mae: 0.052426, mean_q: -0.294562
 51500/100000: episode: 515, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: -14.295, mean reward: -0.143 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.896, 10.244], loss: 0.002746, mae: 0.051771, mean_q: -0.329869
 51600/100000: episode: 516, duration: 0.671s, episode steps: 100, steps per second: 149, episode reward: -14.379, mean reward: -0.144 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.607, 10.098], loss: 0.002743, mae: 0.051914, mean_q: -0.315364
 51700/100000: episode: 517, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: -15.970, mean reward: -0.160 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.244, 10.098], loss: 0.002722, mae: 0.053114, mean_q: -0.310255
 51800/100000: episode: 518, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -13.467, mean reward: -0.135 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.495, 10.229], loss: 0.002867, mae: 0.052581, mean_q: -0.313633
 51900/100000: episode: 519, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -19.628, mean reward: -0.196 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.616, 10.166], loss: 0.002775, mae: 0.052494, mean_q: -0.275704
 52000/100000: episode: 520, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: -16.751, mean reward: -0.168 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.840, 10.098], loss: 0.002493, mae: 0.049606, mean_q: -0.328225
 52100/100000: episode: 521, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -18.246, mean reward: -0.182 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.965, 10.147], loss: 0.002535, mae: 0.050162, mean_q: -0.293461
 52200/100000: episode: 522, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -16.326, mean reward: -0.163 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.612, 10.098], loss: 0.002838, mae: 0.052929, mean_q: -0.315061
 52300/100000: episode: 523, duration: 0.688s, episode steps: 100, steps per second: 145, episode reward: -18.136, mean reward: -0.181 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.070, 10.258], loss: 0.002496, mae: 0.050190, mean_q: -0.346844
 52400/100000: episode: 524, duration: 0.678s, episode steps: 100, steps per second: 148, episode reward: -12.636, mean reward: -0.126 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.765, 10.183], loss: 0.003860, mae: 0.058221, mean_q: -0.299912
 52500/100000: episode: 525, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -16.188, mean reward: -0.162 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.887, 10.255], loss: 0.005115, mae: 0.072365, mean_q: -0.311275
 52600/100000: episode: 526, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -16.803, mean reward: -0.168 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.748, 10.129], loss: 0.004829, mae: 0.069746, mean_q: -0.312997
 52700/100000: episode: 527, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -18.623, mean reward: -0.186 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.893, 10.098], loss: 0.002953, mae: 0.056262, mean_q: -0.344107
 52800/100000: episode: 528, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -14.582, mean reward: -0.146 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.844, 10.326], loss: 0.002705, mae: 0.053470, mean_q: -0.312580
 52900/100000: episode: 529, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: -18.837, mean reward: -0.188 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.923, 10.276], loss: 0.002638, mae: 0.051397, mean_q: -0.308054
 53000/100000: episode: 530, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -18.612, mean reward: -0.186 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.893, 10.207], loss: 0.002638, mae: 0.051538, mean_q: -0.315390
 53100/100000: episode: 531, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -18.169, mean reward: -0.182 [-1.000, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.345, 10.098], loss: 0.002803, mae: 0.053200, mean_q: -0.291852
 53200/100000: episode: 532, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: -16.925, mean reward: -0.169 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.731, 10.139], loss: 0.002661, mae: 0.051398, mean_q: -0.340162
 53300/100000: episode: 533, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: -15.030, mean reward: -0.150 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.205, 10.241], loss: 0.002748, mae: 0.052324, mean_q: -0.304264
 53400/100000: episode: 534, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -16.323, mean reward: -0.163 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.357, 10.201], loss: 0.002719, mae: 0.051735, mean_q: -0.340271
 53500/100000: episode: 535, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -18.807, mean reward: -0.188 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.574, 10.268], loss: 0.002507, mae: 0.050304, mean_q: -0.305166
 53600/100000: episode: 536, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: -20.544, mean reward: -0.205 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.461, 10.098], loss: 0.002639, mae: 0.052281, mean_q: -0.279375
 53700/100000: episode: 537, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: -15.262, mean reward: -0.153 [-1.000, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.795, 10.165], loss: 0.002680, mae: 0.051759, mean_q: -0.335924
 53800/100000: episode: 538, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: -17.246, mean reward: -0.172 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.839, 10.098], loss: 0.002475, mae: 0.050084, mean_q: -0.325420
 53900/100000: episode: 539, duration: 1.125s, episode steps: 100, steps per second: 89, episode reward: -18.238, mean reward: -0.182 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.878, 10.151], loss: 0.002691, mae: 0.051068, mean_q: -0.321349
 54000/100000: episode: 540, duration: 0.761s, episode steps: 100, steps per second: 131, episode reward: -18.691, mean reward: -0.187 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.289, 10.212], loss: 0.002700, mae: 0.052501, mean_q: -0.319838
 54100/100000: episode: 541, duration: 0.993s, episode steps: 100, steps per second: 101, episode reward: -15.906, mean reward: -0.159 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.858, 10.098], loss: 0.002572, mae: 0.050579, mean_q: -0.320784
 54200/100000: episode: 542, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: -19.013, mean reward: -0.190 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.762, 10.098], loss: 0.002720, mae: 0.052599, mean_q: -0.288543
 54300/100000: episode: 543, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: -14.127, mean reward: -0.141 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.240, 10.169], loss: 0.002481, mae: 0.049324, mean_q: -0.313947
 54400/100000: episode: 544, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -16.265, mean reward: -0.163 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.760, 10.098], loss: 0.002629, mae: 0.050009, mean_q: -0.300401
 54500/100000: episode: 545, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -15.939, mean reward: -0.159 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.759, 10.293], loss: 0.002502, mae: 0.049770, mean_q: -0.326419
 54600/100000: episode: 546, duration: 0.690s, episode steps: 100, steps per second: 145, episode reward: -18.601, mean reward: -0.186 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.132, 10.179], loss: 0.002730, mae: 0.052267, mean_q: -0.346893
 54700/100000: episode: 547, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -16.526, mean reward: -0.165 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.092, 10.294], loss: 0.002712, mae: 0.051985, mean_q: -0.308685
 54800/100000: episode: 548, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: -18.331, mean reward: -0.183 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.872, 10.098], loss: 0.004041, mae: 0.058534, mean_q: -0.317082
 54900/100000: episode: 549, duration: 0.619s, episode steps: 100, steps per second: 161, episode reward: -13.712, mean reward: -0.137 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.991, 10.098], loss: 0.004414, mae: 0.069167, mean_q: -0.311789
 55000/100000: episode: 550, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: -14.907, mean reward: -0.149 [-1.000, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.225, 10.098], loss: 0.003040, mae: 0.056384, mean_q: -0.306468
 55100/100000: episode: 551, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -19.165, mean reward: -0.192 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.645, 10.098], loss: 0.002491, mae: 0.049055, mean_q: -0.328865
 55200/100000: episode: 552, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -18.175, mean reward: -0.182 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.184, 10.264], loss: 0.002545, mae: 0.050013, mean_q: -0.308462
 55300/100000: episode: 553, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.070, mean reward: -0.181 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.519, 10.156], loss: 0.002605, mae: 0.050239, mean_q: -0.348302
 55400/100000: episode: 554, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: -19.841, mean reward: -0.198 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.626, 10.199], loss: 0.002654, mae: 0.051115, mean_q: -0.300330
 55500/100000: episode: 555, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: -17.206, mean reward: -0.172 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.883, 10.098], loss: 0.002596, mae: 0.049920, mean_q: -0.332223
 55600/100000: episode: 556, duration: 0.692s, episode steps: 100, steps per second: 144, episode reward: -19.473, mean reward: -0.195 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.221, 10.104], loss: 0.002501, mae: 0.049810, mean_q: -0.331532
 55700/100000: episode: 557, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: -18.416, mean reward: -0.184 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.193, 10.155], loss: 0.002506, mae: 0.050029, mean_q: -0.312638
 55800/100000: episode: 558, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: -16.975, mean reward: -0.170 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.430, 10.098], loss: 0.002465, mae: 0.049072, mean_q: -0.345332
 55900/100000: episode: 559, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -19.018, mean reward: -0.190 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.387, 10.098], loss: 0.002357, mae: 0.048092, mean_q: -0.325911
 56000/100000: episode: 560, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: -16.432, mean reward: -0.164 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.686, 10.098], loss: 0.002543, mae: 0.050239, mean_q: -0.311346
 56100/100000: episode: 561, duration: 0.672s, episode steps: 100, steps per second: 149, episode reward: -14.630, mean reward: -0.146 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.909, 10.495], loss: 0.002672, mae: 0.051595, mean_q: -0.293966
 56200/100000: episode: 562, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: -16.582, mean reward: -0.166 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.886, 10.098], loss: 0.002881, mae: 0.053599, mean_q: -0.328352
 56300/100000: episode: 563, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: -11.893, mean reward: -0.119 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.008, 10.098], loss: 0.002972, mae: 0.054682, mean_q: -0.306198
 56400/100000: episode: 564, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: -15.942, mean reward: -0.159 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.012, 10.098], loss: 0.002530, mae: 0.050671, mean_q: -0.307950
 56500/100000: episode: 565, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -14.140, mean reward: -0.141 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.673, 10.098], loss: 0.002511, mae: 0.049093, mean_q: -0.338934
 56600/100000: episode: 566, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -16.364, mean reward: -0.164 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.621, 10.098], loss: 0.002558, mae: 0.050927, mean_q: -0.344621
 56700/100000: episode: 567, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: -15.604, mean reward: -0.156 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.174, 10.098], loss: 0.002637, mae: 0.051148, mean_q: -0.305867
 56800/100000: episode: 568, duration: 0.727s, episode steps: 100, steps per second: 138, episode reward: -17.769, mean reward: -0.178 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.594, 10.098], loss: 0.002641, mae: 0.051737, mean_q: -0.321629
 56900/100000: episode: 569, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: -15.387, mean reward: -0.154 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.661, 10.351], loss: 0.002354, mae: 0.049171, mean_q: -0.358716
 57000/100000: episode: 570, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: -18.887, mean reward: -0.189 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.528, 10.258], loss: 0.003360, mae: 0.057097, mean_q: -0.303396
 57100/100000: episode: 571, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: -12.191, mean reward: -0.122 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.646, 10.098], loss: 0.002369, mae: 0.047965, mean_q: -0.326672
 57200/100000: episode: 572, duration: 1.051s, episode steps: 100, steps per second: 95, episode reward: -19.137, mean reward: -0.191 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.289, 10.186], loss: 0.002359, mae: 0.047923, mean_q: -0.339498
 57300/100000: episode: 573, duration: 0.966s, episode steps: 100, steps per second: 104, episode reward: -16.156, mean reward: -0.162 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.145, 10.153], loss: 0.002484, mae: 0.049261, mean_q: -0.281627
 57400/100000: episode: 574, duration: 0.744s, episode steps: 100, steps per second: 134, episode reward: -19.338, mean reward: -0.193 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.144, 10.098], loss: 0.002556, mae: 0.049693, mean_q: -0.305412
 57500/100000: episode: 575, duration: 0.631s, episode steps: 100, steps per second: 158, episode reward: -19.533, mean reward: -0.195 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.966, 10.308], loss: 0.002692, mae: 0.050369, mean_q: -0.322350
 57600/100000: episode: 576, duration: 0.662s, episode steps: 100, steps per second: 151, episode reward: -15.880, mean reward: -0.159 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.152, 10.125], loss: 0.002410, mae: 0.048399, mean_q: -0.346369
 57700/100000: episode: 577, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: -13.638, mean reward: -0.136 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.646, 10.175], loss: 0.002629, mae: 0.050630, mean_q: -0.331175
 57800/100000: episode: 578, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: -8.435, mean reward: -0.084 [-1.000, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.613, 10.331], loss: 0.002540, mae: 0.050180, mean_q: -0.317257
 57900/100000: episode: 579, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: -9.299, mean reward: -0.093 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.002, 10.098], loss: 0.002624, mae: 0.050128, mean_q: -0.289871
 58000/100000: episode: 580, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: -16.894, mean reward: -0.169 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.837, 10.230], loss: 0.002764, mae: 0.051214, mean_q: -0.294980
 58100/100000: episode: 581, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: -18.479, mean reward: -0.185 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.914, 10.190], loss: 0.002669, mae: 0.050363, mean_q: -0.335885
 58200/100000: episode: 582, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: -18.009, mean reward: -0.180 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.009, 10.098], loss: 0.003450, mae: 0.057333, mean_q: -0.301739
 58300/100000: episode: 583, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: -19.034, mean reward: -0.190 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.318, 10.263], loss: 0.002961, mae: 0.055008, mean_q: -0.287386
 58400/100000: episode: 584, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: -18.160, mean reward: -0.182 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.476, 10.098], loss: 0.002634, mae: 0.049959, mean_q: -0.353127
 58500/100000: episode: 585, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: -17.307, mean reward: -0.173 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.662, 10.334], loss: 0.002628, mae: 0.049763, mean_q: -0.321227
 58600/100000: episode: 586, duration: 0.645s, episode steps: 100, steps per second: 155, episode reward: -19.931, mean reward: -0.199 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.487, 10.098], loss: 0.003074, mae: 0.053647, mean_q: -0.316403
 58700/100000: episode: 587, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: -18.668, mean reward: -0.187 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.130, 10.424], loss: 0.002488, mae: 0.049204, mean_q: -0.320180
 58800/100000: episode: 588, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: -17.137, mean reward: -0.171 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.479, 10.098], loss: 0.002762, mae: 0.051613, mean_q: -0.271597
 58900/100000: episode: 589, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: -17.363, mean reward: -0.174 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.764, 10.178], loss: 0.002591, mae: 0.050661, mean_q: -0.305332
 59000/100000: episode: 590, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: -19.760, mean reward: -0.198 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.683, 10.172], loss: 0.002840, mae: 0.052736, mean_q: -0.329744
 59100/100000: episode: 591, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: -13.341, mean reward: -0.133 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.564, 10.406], loss: 0.002699, mae: 0.052201, mean_q: -0.337632
 59200/100000: episode: 592, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -20.476, mean reward: -0.205 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.151, 10.124], loss: 0.002808, mae: 0.052974, mean_q: -0.290055
 59300/100000: episode: 593, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: -16.141, mean reward: -0.161 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.626, 10.098], loss: 0.002553, mae: 0.051051, mean_q: -0.299842
 59400/100000: episode: 594, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: -14.411, mean reward: -0.144 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.638, 10.098], loss: 0.002448, mae: 0.049774, mean_q: -0.316545
 59500/100000: episode: 595, duration: 1.276s, episode steps: 100, steps per second: 78, episode reward: -17.816, mean reward: -0.178 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.785, 10.219], loss: 0.002678, mae: 0.051629, mean_q: -0.312593
 59600/100000: episode: 596, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: -17.961, mean reward: -0.180 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.611, 10.111], loss: 0.002586, mae: 0.049961, mean_q: -0.312969
 59700/100000: episode: 597, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: -16.567, mean reward: -0.166 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.828, 10.098], loss: 0.004890, mae: 0.065368, mean_q: -0.294704
 59800/100000: episode: 598, duration: 1.101s, episode steps: 100, steps per second: 91, episode reward: -12.152, mean reward: -0.122 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.776, 10.098], loss: 0.003409, mae: 0.058506, mean_q: -0.338756
 59900/100000: episode: 599, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -18.766, mean reward: -0.188 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.046, 10.151], loss: 0.002559, mae: 0.050609, mean_q: -0.325514
 60000/100000: episode: 600, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: -15.911, mean reward: -0.159 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.177, 10.480], loss: 0.003487, mae: 0.060270, mean_q: -0.304436
 60100/100000: episode: 601, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: -14.726, mean reward: -0.147 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.349, 10.098], loss: 0.002420, mae: 0.049645, mean_q: -0.307243
 60200/100000: episode: 602, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: -16.721, mean reward: -0.167 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.896, 10.176], loss: 0.002215, mae: 0.045896, mean_q: -0.320816
 60300/100000: episode: 603, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: -21.018, mean reward: -0.210 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.523, 10.175], loss: 0.002404, mae: 0.048262, mean_q: -0.283013
 60400/100000: episode: 604, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: -16.664, mean reward: -0.167 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.172, 10.195], loss: 0.002857, mae: 0.053203, mean_q: -0.284306
 60500/100000: episode: 605, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -13.141, mean reward: -0.131 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.373, 10.174], loss: 0.002518, mae: 0.050097, mean_q: -0.310328
 60600/100000: episode: 606, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: -16.715, mean reward: -0.167 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.382, 10.098], loss: 0.003474, mae: 0.056626, mean_q: -0.306783
 60700/100000: episode: 607, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: -18.010, mean reward: -0.180 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.542, 10.129], loss: 0.003571, mae: 0.059759, mean_q: -0.303012
 60800/100000: episode: 608, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: -19.039, mean reward: -0.190 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.303, 10.098], loss: 0.002540, mae: 0.049477, mean_q: -0.335713
 60900/100000: episode: 609, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -20.554, mean reward: -0.206 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.387, 10.159], loss: 0.002369, mae: 0.048449, mean_q: -0.322997
 61000/100000: episode: 610, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: -11.763, mean reward: -0.118 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.747, 10.308], loss: 0.002595, mae: 0.049768, mean_q: -0.342156
 61100/100000: episode: 611, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: -16.526, mean reward: -0.165 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.179, 10.216], loss: 0.002428, mae: 0.048094, mean_q: -0.299453
 61200/100000: episode: 612, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: -18.663, mean reward: -0.187 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.079, 10.258], loss: 0.002447, mae: 0.048928, mean_q: -0.298692
 61300/100000: episode: 613, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: -17.127, mean reward: -0.171 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.303, 10.098], loss: 0.002683, mae: 0.051672, mean_q: -0.298317
 61400/100000: episode: 614, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: -19.575, mean reward: -0.196 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.100, 10.098], loss: 0.002577, mae: 0.050243, mean_q: -0.290013
 61500/100000: episode: 615, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: -19.231, mean reward: -0.192 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.384, 10.152], loss: 0.002535, mae: 0.049859, mean_q: -0.335726
 61600/100000: episode: 616, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: -16.731, mean reward: -0.167 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.234, 10.261], loss: 0.002389, mae: 0.048392, mean_q: -0.333581
 61700/100000: episode: 617, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: -18.038, mean reward: -0.180 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.795, 10.198], loss: 0.002779, mae: 0.052710, mean_q: -0.343737
 61800/100000: episode: 618, duration: 0.726s, episode steps: 100, steps per second: 138, episode reward: -19.075, mean reward: -0.191 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.505, 10.192], loss: 0.002739, mae: 0.053701, mean_q: -0.290929
 61900/100000: episode: 619, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -18.467, mean reward: -0.185 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.366, 10.366], loss: 0.002686, mae: 0.051637, mean_q: -0.336109
 62000/100000: episode: 620, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: -19.786, mean reward: -0.198 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.155, 10.198], loss: 0.002603, mae: 0.051995, mean_q: -0.344249
 62100/100000: episode: 621, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: -13.482, mean reward: -0.135 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.237, 10.224], loss: 0.002665, mae: 0.051595, mean_q: -0.297678
 62200/100000: episode: 622, duration: 1.194s, episode steps: 100, steps per second: 84, episode reward: -11.782, mean reward: -0.118 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.801, 10.098], loss: 0.002380, mae: 0.047669, mean_q: -0.321787
 62300/100000: episode: 623, duration: 1.365s, episode steps: 100, steps per second: 73, episode reward: -15.702, mean reward: -0.157 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.458, 10.098], loss: 0.002558, mae: 0.049025, mean_q: -0.318832
 62400/100000: episode: 624, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: -15.620, mean reward: -0.156 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.118, 10.098], loss: 0.002980, mae: 0.055586, mean_q: -0.326213
 62500/100000: episode: 625, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: -19.559, mean reward: -0.196 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.382, 10.153], loss: 0.002912, mae: 0.054797, mean_q: -0.281996
 62600/100000: episode: 626, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: -17.156, mean reward: -0.172 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.768, 10.171], loss: 0.002565, mae: 0.049722, mean_q: -0.330990
 62700/100000: episode: 627, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: -19.449, mean reward: -0.194 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.806, 10.098], loss: 0.002864, mae: 0.053028, mean_q: -0.320155
 62800/100000: episode: 628, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: -17.866, mean reward: -0.179 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.582, 10.159], loss: 0.002530, mae: 0.049725, mean_q: -0.327142
 62900/100000: episode: 629, duration: 0.707s, episode steps: 100, steps per second: 142, episode reward: -13.751, mean reward: -0.138 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.030, 10.098], loss: 0.002706, mae: 0.050789, mean_q: -0.337322
 63000/100000: episode: 630, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: -9.410, mean reward: -0.094 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.403, 10.122], loss: 0.002477, mae: 0.049224, mean_q: -0.314309
 63100/100000: episode: 631, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: -16.931, mean reward: -0.169 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.024, 10.246], loss: 0.002741, mae: 0.052143, mean_q: -0.320981
 63200/100000: episode: 632, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -18.093, mean reward: -0.181 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.769, 10.098], loss: 0.002721, mae: 0.051392, mean_q: -0.361919
 63300/100000: episode: 633, duration: 0.770s, episode steps: 100, steps per second: 130, episode reward: -15.282, mean reward: -0.153 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.406, 10.098], loss: 0.002614, mae: 0.051077, mean_q: -0.295414
 63400/100000: episode: 634, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: -19.731, mean reward: -0.197 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.140, 10.098], loss: 0.002787, mae: 0.052460, mean_q: -0.302269
 63500/100000: episode: 635, duration: 0.689s, episode steps: 100, steps per second: 145, episode reward: -21.240, mean reward: -0.212 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.624, 10.171], loss: 0.002747, mae: 0.052292, mean_q: -0.329651
 63600/100000: episode: 636, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: -19.096, mean reward: -0.191 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.420, 10.098], loss: 0.002594, mae: 0.050066, mean_q: -0.332729
 63700/100000: episode: 637, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: -21.618, mean reward: -0.216 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.795, 10.100], loss: 0.002816, mae: 0.051717, mean_q: -0.333175
 63800/100000: episode: 638, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: -17.676, mean reward: -0.177 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.114, 10.175], loss: 0.002665, mae: 0.052003, mean_q: -0.296950
 63900/100000: episode: 639, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: -15.687, mean reward: -0.157 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.657, 10.098], loss: 0.002719, mae: 0.052714, mean_q: -0.321200
 64000/100000: episode: 640, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: -20.626, mean reward: -0.206 [-1.000, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.587, 10.112], loss: 0.004020, mae: 0.062116, mean_q: -0.294373
 64100/100000: episode: 641, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -17.489, mean reward: -0.175 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.264, 10.098], loss: 0.003564, mae: 0.061613, mean_q: -0.339799
 64200/100000: episode: 642, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: -14.874, mean reward: -0.149 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.989, 10.098], loss: 0.002770, mae: 0.052697, mean_q: -0.303348
 64300/100000: episode: 643, duration: 0.692s, episode steps: 100, steps per second: 144, episode reward: -14.858, mean reward: -0.149 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.841, 10.348], loss: 0.002661, mae: 0.050101, mean_q: -0.335365
 64400/100000: episode: 644, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: -17.748, mean reward: -0.177 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.620, 10.098], loss: 0.002718, mae: 0.052051, mean_q: -0.316650
 64500/100000: episode: 645, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: -18.288, mean reward: -0.183 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.385, 10.098], loss: 0.002732, mae: 0.051113, mean_q: -0.343287
 64600/100000: episode: 646, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: -18.704, mean reward: -0.187 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.890, 10.128], loss: 0.002741, mae: 0.052062, mean_q: -0.313706
 64700/100000: episode: 647, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: -15.906, mean reward: -0.159 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.777, 10.113], loss: 0.002605, mae: 0.050410, mean_q: -0.312411
 64800/100000: episode: 648, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: -21.234, mean reward: -0.212 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.096, 10.098], loss: 0.002833, mae: 0.052791, mean_q: -0.312565
 64900/100000: episode: 649, duration: 0.661s, episode steps: 100, steps per second: 151, episode reward: -17.871, mean reward: -0.179 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.913, 10.258], loss: 0.002825, mae: 0.052338, mean_q: -0.316589
 65000/100000: episode: 650, duration: 0.676s, episode steps: 100, steps per second: 148, episode reward: -14.032, mean reward: -0.140 [-1.000, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.102, 10.098], loss: 0.002991, mae: 0.054656, mean_q: -0.361450
 65100/100000: episode: 651, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -15.935, mean reward: -0.159 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.801, 10.098], loss: 0.002823, mae: 0.051718, mean_q: -0.321617
 65200/100000: episode: 652, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -11.833, mean reward: -0.118 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.318, 10.385], loss: 0.002905, mae: 0.054364, mean_q: -0.309048
 65300/100000: episode: 653, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: -18.389, mean reward: -0.184 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.109, 10.098], loss: 0.002812, mae: 0.052256, mean_q: -0.338393
 65400/100000: episode: 654, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -20.551, mean reward: -0.206 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.985, 10.165], loss: 0.002737, mae: 0.051316, mean_q: -0.322461
 65500/100000: episode: 655, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -12.429, mean reward: -0.124 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.177, 10.356], loss: 0.002766, mae: 0.052788, mean_q: -0.300844
 65600/100000: episode: 656, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -16.052, mean reward: -0.161 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.398, 10.098], loss: 0.002577, mae: 0.050300, mean_q: -0.316707
 65700/100000: episode: 657, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: -16.579, mean reward: -0.166 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.622, 10.192], loss: 0.002795, mae: 0.052345, mean_q: -0.314457
 65800/100000: episode: 658, duration: 0.672s, episode steps: 100, steps per second: 149, episode reward: -15.293, mean reward: -0.153 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.614, 10.295], loss: 0.002695, mae: 0.051347, mean_q: -0.333772
 65900/100000: episode: 659, duration: 1.171s, episode steps: 100, steps per second: 85, episode reward: -19.342, mean reward: -0.193 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.603, 10.253], loss: 0.002825, mae: 0.053213, mean_q: -0.290511
 66000/100000: episode: 660, duration: 0.712s, episode steps: 100, steps per second: 140, episode reward: -19.122, mean reward: -0.191 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.819, 10.307], loss: 0.002784, mae: 0.052034, mean_q: -0.331824
 66100/100000: episode: 661, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: -14.970, mean reward: -0.150 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.008, 10.328], loss: 0.002845, mae: 0.052148, mean_q: -0.324596
 66200/100000: episode: 662, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: -15.579, mean reward: -0.156 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.885, 10.417], loss: 0.002577, mae: 0.049710, mean_q: -0.302545
 66300/100000: episode: 663, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: -15.348, mean reward: -0.153 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.602, 10.098], loss: 0.002529, mae: 0.049470, mean_q: -0.342035
 66400/100000: episode: 664, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: -16.621, mean reward: -0.166 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.527, 10.336], loss: 0.002725, mae: 0.051095, mean_q: -0.375418
 66500/100000: episode: 665, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: -14.536, mean reward: -0.145 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.401, 10.098], loss: 0.002592, mae: 0.049466, mean_q: -0.338454
 66600/100000: episode: 666, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: -15.719, mean reward: -0.157 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.999, 10.125], loss: 0.002636, mae: 0.050935, mean_q: -0.325719
 66700/100000: episode: 667, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.633, mean reward: -0.186 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.326, 10.098], loss: 0.002682, mae: 0.052259, mean_q: -0.307970
 66800/100000: episode: 668, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -20.460, mean reward: -0.205 [-1.000, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.729, 10.154], loss: 0.002613, mae: 0.050101, mean_q: -0.329411
 66900/100000: episode: 669, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -17.689, mean reward: -0.177 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.803, 10.098], loss: 0.002619, mae: 0.050274, mean_q: -0.298380
 67000/100000: episode: 670, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: -18.918, mean reward: -0.189 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.550, 10.098], loss: 0.002670, mae: 0.050372, mean_q: -0.345061
 67100/100000: episode: 671, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: -17.569, mean reward: -0.176 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.340, 10.098], loss: 0.002798, mae: 0.052285, mean_q: -0.290006
 67200/100000: episode: 672, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -20.354, mean reward: -0.204 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.833, 10.387], loss: 0.003125, mae: 0.056662, mean_q: -0.310919
 67300/100000: episode: 673, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.898, mean reward: -0.189 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.347, 10.253], loss: 0.002909, mae: 0.053663, mean_q: -0.332717
 67400/100000: episode: 674, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: -18.541, mean reward: -0.185 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.315, 10.156], loss: 0.006215, mae: 0.072668, mean_q: -0.332552
 67500/100000: episode: 675, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.651, mean reward: -0.187 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.177, 10.137], loss: 0.006508, mae: 0.072945, mean_q: -0.360840
 67600/100000: episode: 676, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -9.795, mean reward: -0.098 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.640, 10.098], loss: 0.004324, mae: 0.064483, mean_q: -0.333148
 67700/100000: episode: 677, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -17.230, mean reward: -0.172 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.508, 10.098], loss: 0.002382, mae: 0.049401, mean_q: -0.316317
 67800/100000: episode: 678, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -9.717, mean reward: -0.097 [-1.000, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.831, 10.098], loss: 0.002471, mae: 0.049986, mean_q: -0.304201
 67900/100000: episode: 679, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -17.432, mean reward: -0.174 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.081, 10.098], loss: 0.002411, mae: 0.047709, mean_q: -0.317822
 68000/100000: episode: 680, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -6.621, mean reward: -0.066 [-1.000, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.171, 10.098], loss: 0.002517, mae: 0.048650, mean_q: -0.318602
 68100/100000: episode: 681, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.467, mean reward: -0.185 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.650, 10.173], loss: 0.002209, mae: 0.046400, mean_q: -0.315767
 68200/100000: episode: 682, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -16.530, mean reward: -0.165 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.889, 10.109], loss: 0.002581, mae: 0.049733, mean_q: -0.308257
 68300/100000: episode: 683, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: -20.116, mean reward: -0.201 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.048, 10.123], loss: 0.002521, mae: 0.048628, mean_q: -0.326013
 68400/100000: episode: 684, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -19.189, mean reward: -0.192 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.351, 10.323], loss: 0.002472, mae: 0.049330, mean_q: -0.311131
 68500/100000: episode: 685, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -10.985, mean reward: -0.110 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.274, 10.352], loss: 0.002588, mae: 0.049311, mean_q: -0.323856
 68600/100000: episode: 686, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -19.979, mean reward: -0.200 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.391, 10.180], loss: 0.002463, mae: 0.048034, mean_q: -0.309886
 68700/100000: episode: 687, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.944, mean reward: -0.159 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.870, 10.098], loss: 0.002501, mae: 0.047770, mean_q: -0.344370
 68800/100000: episode: 688, duration: 0.612s, episode steps: 100, steps per second: 164, episode reward: -19.739, mean reward: -0.197 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.922, 10.115], loss: 0.002597, mae: 0.049115, mean_q: -0.306138
 68900/100000: episode: 689, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -18.146, mean reward: -0.181 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.491, 10.098], loss: 0.002371, mae: 0.047385, mean_q: -0.331748
 69000/100000: episode: 690, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -15.729, mean reward: -0.157 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.223, 10.170], loss: 0.002376, mae: 0.047651, mean_q: -0.352390
 69100/100000: episode: 691, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -18.920, mean reward: -0.189 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.236, 10.193], loss: 0.002334, mae: 0.047120, mean_q: -0.365602
 69200/100000: episode: 692, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -16.303, mean reward: -0.163 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.614, 10.283], loss: 0.002524, mae: 0.049190, mean_q: -0.296541
 69300/100000: episode: 693, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: -18.177, mean reward: -0.182 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.153, 10.098], loss: 0.002523, mae: 0.048244, mean_q: -0.294822
 69400/100000: episode: 694, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.924, mean reward: -0.179 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.415, 10.098], loss: 0.002693, mae: 0.050255, mean_q: -0.322829
 69500/100000: episode: 695, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -18.848, mean reward: -0.188 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.474, 10.098], loss: 0.002488, mae: 0.048634, mean_q: -0.312745
 69600/100000: episode: 696, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -17.498, mean reward: -0.175 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.056, 10.098], loss: 0.002577, mae: 0.049287, mean_q: -0.319909
 69700/100000: episode: 697, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -16.506, mean reward: -0.165 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.653, 10.098], loss: 0.002539, mae: 0.049349, mean_q: -0.314624
 69800/100000: episode: 698, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -15.002, mean reward: -0.150 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.078, 10.098], loss: 0.002499, mae: 0.049090, mean_q: -0.305366
 69900/100000: episode: 699, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -18.597, mean reward: -0.186 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.901, 10.169], loss: 0.002639, mae: 0.050353, mean_q: -0.302649
 70000/100000: episode: 700, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -18.245, mean reward: -0.182 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.351, 10.173], loss: 0.002365, mae: 0.048172, mean_q: -0.298796
 70100/100000: episode: 701, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -15.056, mean reward: -0.151 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.909, 10.424], loss: 0.002553, mae: 0.049603, mean_q: -0.335002
 70200/100000: episode: 702, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -17.530, mean reward: -0.175 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.201, 10.098], loss: 0.002529, mae: 0.048625, mean_q: -0.350444
 70300/100000: episode: 703, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -13.220, mean reward: -0.132 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.380, 10.298], loss: 0.002512, mae: 0.050404, mean_q: -0.266287
 70400/100000: episode: 704, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -19.202, mean reward: -0.192 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.564, 10.107], loss: 0.002815, mae: 0.053614, mean_q: -0.304893
 70500/100000: episode: 705, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: -14.504, mean reward: -0.145 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.265, 10.098], loss: 0.002393, mae: 0.048652, mean_q: -0.327704
 70600/100000: episode: 706, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -13.972, mean reward: -0.140 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.015, 10.098], loss: 0.002564, mae: 0.049675, mean_q: -0.305637
 70700/100000: episode: 707, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -16.603, mean reward: -0.166 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.766, 10.098], loss: 0.002500, mae: 0.048177, mean_q: -0.335169
 70800/100000: episode: 708, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: -17.671, mean reward: -0.177 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.571, 10.286], loss: 0.002618, mae: 0.051569, mean_q: -0.329187
 70900/100000: episode: 709, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -17.124, mean reward: -0.171 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.783, 10.258], loss: 0.002720, mae: 0.051763, mean_q: -0.298676
 71000/100000: episode: 710, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -18.737, mean reward: -0.187 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.132, 10.164], loss: 0.002467, mae: 0.048746, mean_q: -0.322146
 71100/100000: episode: 711, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: -17.600, mean reward: -0.176 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.736, 10.098], loss: 0.002490, mae: 0.048572, mean_q: -0.358687
 71200/100000: episode: 712, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -18.842, mean reward: -0.188 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.853, 10.111], loss: 0.002573, mae: 0.050297, mean_q: -0.308610
 71300/100000: episode: 713, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -18.041, mean reward: -0.180 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.234, 10.196], loss: 0.002634, mae: 0.051041, mean_q: -0.316936
 71400/100000: episode: 714, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -14.590, mean reward: -0.146 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.716, 10.264], loss: 0.002578, mae: 0.050689, mean_q: -0.344836
 71500/100000: episode: 715, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: -17.144, mean reward: -0.171 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.644, 10.336], loss: 0.002551, mae: 0.049231, mean_q: -0.333210
 71600/100000: episode: 716, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -13.949, mean reward: -0.139 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.734, 10.120], loss: 0.002669, mae: 0.052432, mean_q: -0.290131
 71700/100000: episode: 717, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -15.798, mean reward: -0.158 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.365, 10.098], loss: 0.002627, mae: 0.050923, mean_q: -0.317082
 71800/100000: episode: 718, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -17.188, mean reward: -0.172 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.408, 10.177], loss: 0.002773, mae: 0.051777, mean_q: -0.298624
 71900/100000: episode: 719, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -17.133, mean reward: -0.171 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.267, 10.216], loss: 0.002518, mae: 0.049094, mean_q: -0.306200
 72000/100000: episode: 720, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -16.300, mean reward: -0.163 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.004, 10.126], loss: 0.002702, mae: 0.050809, mean_q: -0.331862
 72100/100000: episode: 721, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: -14.977, mean reward: -0.150 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.321, 10.098], loss: 0.002684, mae: 0.051656, mean_q: -0.302075
 72200/100000: episode: 722, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: -10.423, mean reward: -0.104 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.444, 10.098], loss: 0.002681, mae: 0.051518, mean_q: -0.290898
 72300/100000: episode: 723, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -21.633, mean reward: -0.216 [-1.000, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.608, 10.098], loss: 0.007020, mae: 0.073962, mean_q: -0.321826
 72400/100000: episode: 724, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -15.966, mean reward: -0.160 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.303, 10.098], loss: 0.006113, mae: 0.072888, mean_q: -0.299365
 72500/100000: episode: 725, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -14.930, mean reward: -0.149 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.867, 10.174], loss: 0.002800, mae: 0.055974, mean_q: -0.308728
 72600/100000: episode: 726, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.228, mean reward: -0.162 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.618, 10.098], loss: 0.002776, mae: 0.053192, mean_q: -0.299881
 72700/100000: episode: 727, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -19.428, mean reward: -0.194 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.439, 10.107], loss: 0.002611, mae: 0.050205, mean_q: -0.329038
 72800/100000: episode: 728, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -19.745, mean reward: -0.197 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.264, 10.102], loss: 0.002806, mae: 0.052788, mean_q: -0.282068
 72900/100000: episode: 729, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -16.876, mean reward: -0.169 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.556, 10.196], loss: 0.002690, mae: 0.050578, mean_q: -0.335161
 73000/100000: episode: 730, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -16.618, mean reward: -0.166 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.798, 10.098], loss: 0.002634, mae: 0.050377, mean_q: -0.347045
 73100/100000: episode: 731, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -17.356, mean reward: -0.174 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.465, 10.132], loss: 0.002711, mae: 0.051465, mean_q: -0.306065
 73200/100000: episode: 732, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: -15.530, mean reward: -0.155 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.829, 10.215], loss: 0.002696, mae: 0.049922, mean_q: -0.328784
 73300/100000: episode: 733, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -16.793, mean reward: -0.168 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.258, 10.140], loss: 0.002675, mae: 0.050446, mean_q: -0.309494
 73400/100000: episode: 734, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.590, mean reward: -0.186 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.924, 10.154], loss: 0.002672, mae: 0.050600, mean_q: -0.328587
 73500/100000: episode: 735, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -20.331, mean reward: -0.203 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.713, 10.100], loss: 0.002835, mae: 0.053642, mean_q: -0.315880
 73600/100000: episode: 736, duration: 0.631s, episode steps: 100, steps per second: 158, episode reward: -16.858, mean reward: -0.169 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.281, 10.336], loss: 0.002619, mae: 0.050380, mean_q: -0.316791
 73700/100000: episode: 737, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: -14.377, mean reward: -0.144 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.963, 10.098], loss: 0.002808, mae: 0.051416, mean_q: -0.308158
 73800/100000: episode: 738, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -19.166, mean reward: -0.192 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.257, 10.166], loss: 0.003185, mae: 0.057034, mean_q: -0.292544
 73900/100000: episode: 739, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -20.285, mean reward: -0.203 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.801, 10.208], loss: 0.002484, mae: 0.049490, mean_q: -0.336445
 74000/100000: episode: 740, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -16.629, mean reward: -0.166 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.074, 10.156], loss: 0.002743, mae: 0.051079, mean_q: -0.310252
 74100/100000: episode: 741, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -18.677, mean reward: -0.187 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.597, 10.180], loss: 0.002796, mae: 0.051248, mean_q: -0.323370
 74200/100000: episode: 742, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.894, mean reward: -0.169 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.573, 10.126], loss: 0.002698, mae: 0.051444, mean_q: -0.351566
 74300/100000: episode: 743, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: -17.998, mean reward: -0.180 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.766, 10.279], loss: 0.002753, mae: 0.052470, mean_q: -0.309593
 74400/100000: episode: 744, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -16.815, mean reward: -0.168 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.719, 10.098], loss: 0.002683, mae: 0.051180, mean_q: -0.328474
 74500/100000: episode: 745, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -18.189, mean reward: -0.182 [-1.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.912, 10.130], loss: 0.002860, mae: 0.052564, mean_q: -0.280922
 74600/100000: episode: 746, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -18.727, mean reward: -0.187 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.027, 10.182], loss: 0.002675, mae: 0.050546, mean_q: -0.343858
 74700/100000: episode: 747, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: -17.336, mean reward: -0.173 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.664, 10.225], loss: 0.002685, mae: 0.050783, mean_q: -0.300761
 74800/100000: episode: 748, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -16.656, mean reward: -0.167 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.340, 10.265], loss: 0.002931, mae: 0.052880, mean_q: -0.315495
 74900/100000: episode: 749, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -18.287, mean reward: -0.183 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.357, 10.155], loss: 0.002789, mae: 0.052346, mean_q: -0.300885
 75000/100000: episode: 750, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: -18.137, mean reward: -0.181 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.484, 10.494], loss: 0.003103, mae: 0.055224, mean_q: -0.273020
 75100/100000: episode: 751, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -17.633, mean reward: -0.176 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.386, 10.173], loss: 0.002652, mae: 0.051399, mean_q: -0.346606
 75200/100000: episode: 752, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -19.745, mean reward: -0.197 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.959, 10.098], loss: 0.002674, mae: 0.050252, mean_q: -0.342813
 75300/100000: episode: 753, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -13.763, mean reward: -0.138 [-1.000, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.643, 10.132], loss: 0.002733, mae: 0.051277, mean_q: -0.327755
 75400/100000: episode: 754, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -20.924, mean reward: -0.209 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.916, 10.181], loss: 0.002693, mae: 0.051170, mean_q: -0.315092
 75500/100000: episode: 755, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -13.163, mean reward: -0.132 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.157, 10.098], loss: 0.002748, mae: 0.050697, mean_q: -0.333183
 75600/100000: episode: 756, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -18.582, mean reward: -0.186 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.866, 10.184], loss: 0.002780, mae: 0.053063, mean_q: -0.313993
 75700/100000: episode: 757, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: -14.663, mean reward: -0.147 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.217, 10.377], loss: 0.002725, mae: 0.052413, mean_q: -0.305348
 75800/100000: episode: 758, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -18.906, mean reward: -0.189 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.335, 10.098], loss: 0.002694, mae: 0.051227, mean_q: -0.321859
 75900/100000: episode: 759, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.146, mean reward: -0.171 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.452, 10.167], loss: 0.003660, mae: 0.058223, mean_q: -0.307124
 76000/100000: episode: 760, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -13.025, mean reward: -0.130 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.102, 10.098], loss: 0.002882, mae: 0.054715, mean_q: -0.333479
 76100/100000: episode: 761, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: -15.991, mean reward: -0.160 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.753, 10.098], loss: 0.002563, mae: 0.050027, mean_q: -0.328040
 76200/100000: episode: 762, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -17.162, mean reward: -0.172 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.793, 10.164], loss: 0.002718, mae: 0.051040, mean_q: -0.298108
 76300/100000: episode: 763, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -13.991, mean reward: -0.140 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.504, 10.098], loss: 0.002441, mae: 0.048902, mean_q: -0.335317
 76400/100000: episode: 764, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -17.791, mean reward: -0.178 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.182, 10.314], loss: 0.002683, mae: 0.051258, mean_q: -0.350170
 76500/100000: episode: 765, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -17.782, mean reward: -0.178 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.484, 10.098], loss: 0.002392, mae: 0.048779, mean_q: -0.301154
 76600/100000: episode: 766, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -15.086, mean reward: -0.151 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.247, 10.098], loss: 0.002877, mae: 0.052500, mean_q: -0.314256
 76700/100000: episode: 767, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -15.977, mean reward: -0.160 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.345, 10.109], loss: 0.002806, mae: 0.052565, mean_q: -0.338071
 76800/100000: episode: 768, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.228, mean reward: -0.162 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.825, 10.266], loss: 0.002636, mae: 0.051185, mean_q: -0.313910
 76900/100000: episode: 769, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.339, mean reward: -0.153 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.543, 10.127], loss: 0.002697, mae: 0.050629, mean_q: -0.327788
 77000/100000: episode: 770, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: -10.667, mean reward: -0.107 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.367, 10.098], loss: 0.002477, mae: 0.048566, mean_q: -0.375778
 77100/100000: episode: 771, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -13.251, mean reward: -0.133 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.910, 10.098], loss: 0.002431, mae: 0.049096, mean_q: -0.316234
 77200/100000: episode: 772, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -14.118, mean reward: -0.141 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.796, 10.098], loss: 0.002756, mae: 0.051679, mean_q: -0.306029
 77300/100000: episode: 773, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -19.230, mean reward: -0.192 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.348, 10.098], loss: 0.002732, mae: 0.051328, mean_q: -0.317352
 77400/100000: episode: 774, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: -19.677, mean reward: -0.197 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.621, 10.223], loss: 0.002720, mae: 0.051617, mean_q: -0.333853
 77500/100000: episode: 775, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -18.813, mean reward: -0.188 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.652, 10.098], loss: 0.003979, mae: 0.061481, mean_q: -0.323283
 77600/100000: episode: 776, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -17.727, mean reward: -0.177 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.735, 10.233], loss: 0.002738, mae: 0.051516, mean_q: -0.323406
 77700/100000: episode: 777, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -17.319, mean reward: -0.173 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.050, 10.246], loss: 0.002400, mae: 0.048071, mean_q: -0.341677
 77800/100000: episode: 778, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -15.241, mean reward: -0.152 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.131, 10.098], loss: 0.002673, mae: 0.051366, mean_q: -0.316253
 77900/100000: episode: 779, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: -15.271, mean reward: -0.153 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.403, 10.098], loss: 0.002425, mae: 0.048769, mean_q: -0.322651
 78000/100000: episode: 780, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -18.501, mean reward: -0.185 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.755, 10.269], loss: 0.003150, mae: 0.055630, mean_q: -0.304528
 78100/100000: episode: 781, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -15.915, mean reward: -0.159 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.418, 10.098], loss: 0.002797, mae: 0.053155, mean_q: -0.302599
 78200/100000: episode: 782, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -18.665, mean reward: -0.187 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.229, 10.098], loss: 0.002717, mae: 0.052562, mean_q: -0.287970
 78300/100000: episode: 783, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.187, mean reward: -0.182 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.223, 10.124], loss: 0.002518, mae: 0.048453, mean_q: -0.335077
 78400/100000: episode: 784, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -18.138, mean reward: -0.181 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.116, 10.098], loss: 0.002508, mae: 0.049054, mean_q: -0.341703
 78500/100000: episode: 785, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -18.587, mean reward: -0.186 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.677, 10.151], loss: 0.002869, mae: 0.052593, mean_q: -0.309610
 78600/100000: episode: 786, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: -18.667, mean reward: -0.187 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.302, 10.240], loss: 0.002740, mae: 0.052840, mean_q: -0.289135
 78700/100000: episode: 787, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: -13.481, mean reward: -0.135 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.005, 10.098], loss: 0.002361, mae: 0.048406, mean_q: -0.315686
 78800/100000: episode: 788, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -16.839, mean reward: -0.168 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.013, 10.347], loss: 0.002214, mae: 0.046349, mean_q: -0.348174
 78900/100000: episode: 789, duration: 0.612s, episode steps: 100, steps per second: 164, episode reward: -18.633, mean reward: -0.186 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.849, 10.157], loss: 0.002499, mae: 0.049257, mean_q: -0.308559
 79000/100000: episode: 790, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -15.462, mean reward: -0.155 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.445, 10.130], loss: 0.002635, mae: 0.051001, mean_q: -0.320100
 79100/100000: episode: 791, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -18.929, mean reward: -0.189 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.517, 10.098], loss: 0.002615, mae: 0.050672, mean_q: -0.280281
 79200/100000: episode: 792, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.627, mean reward: -0.176 [-1.000, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.267, 10.179], loss: 0.002481, mae: 0.048883, mean_q: -0.343164
 79300/100000: episode: 793, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -8.337, mean reward: -0.083 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.188, 10.304], loss: 0.002579, mae: 0.049785, mean_q: -0.301681
 79400/100000: episode: 794, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: -18.051, mean reward: -0.181 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.040, 10.176], loss: 0.002651, mae: 0.050569, mean_q: -0.304074
 79500/100000: episode: 795, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -16.726, mean reward: -0.167 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.640, 10.342], loss: 0.006609, mae: 0.074183, mean_q: -0.311756
 79600/100000: episode: 796, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -17.952, mean reward: -0.180 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.679, 10.098], loss: 0.002721, mae: 0.053504, mean_q: -0.301297
 79700/100000: episode: 797, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -14.582, mean reward: -0.146 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.266, 10.128], loss: 0.002453, mae: 0.049085, mean_q: -0.334671
 79800/100000: episode: 798, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -16.332, mean reward: -0.163 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.147, 10.283], loss: 0.002479, mae: 0.048500, mean_q: -0.310469
 79900/100000: episode: 799, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -19.080, mean reward: -0.191 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.172, 10.185], loss: 0.002440, mae: 0.047842, mean_q: -0.328523
 80000/100000: episode: 800, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: -16.967, mean reward: -0.170 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.537, 10.109], loss: 0.002665, mae: 0.050698, mean_q: -0.316730
 80100/100000: episode: 801, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: -18.448, mean reward: -0.184 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.597, 10.109], loss: 0.002524, mae: 0.048256, mean_q: -0.356938
 80200/100000: episode: 802, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -19.971, mean reward: -0.200 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.773, 10.217], loss: 0.002683, mae: 0.051115, mean_q: -0.302263
 80300/100000: episode: 803, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -14.145, mean reward: -0.141 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.474, 10.438], loss: 0.002697, mae: 0.050005, mean_q: -0.328805
 80400/100000: episode: 804, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: -15.919, mean reward: -0.159 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.037, 10.125], loss: 0.002422, mae: 0.048009, mean_q: -0.288916
 80500/100000: episode: 805, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -19.722, mean reward: -0.197 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.902, 10.170], loss: 0.002504, mae: 0.049055, mean_q: -0.317631
 80600/100000: episode: 806, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: -15.476, mean reward: -0.155 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.010, 10.098], loss: 0.002318, mae: 0.047773, mean_q: -0.321040
 80700/100000: episode: 807, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -14.500, mean reward: -0.145 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.691, 10.175], loss: 0.002441, mae: 0.048669, mean_q: -0.354309
 80800/100000: episode: 808, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -17.474, mean reward: -0.175 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.854, 10.098], loss: 0.002840, mae: 0.051856, mean_q: -0.325479
 80900/100000: episode: 809, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -19.576, mean reward: -0.196 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.794, 10.126], loss: 0.002499, mae: 0.048790, mean_q: -0.345872
 81000/100000: episode: 810, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -15.210, mean reward: -0.152 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.517, 10.262], loss: 0.002768, mae: 0.052142, mean_q: -0.298626
 81100/100000: episode: 811, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: -9.740, mean reward: -0.097 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.683, 10.098], loss: 0.002523, mae: 0.049570, mean_q: -0.331179
 81200/100000: episode: 812, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -15.107, mean reward: -0.151 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.177, 10.238], loss: 0.002649, mae: 0.051699, mean_q: -0.269371
 81300/100000: episode: 813, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -15.865, mean reward: -0.159 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.177, 10.098], loss: 0.002589, mae: 0.049503, mean_q: -0.318362
 81400/100000: episode: 814, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -18.081, mean reward: -0.181 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.624, 10.098], loss: 0.002880, mae: 0.052950, mean_q: -0.293357
 81500/100000: episode: 815, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -15.632, mean reward: -0.156 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.845, 10.333], loss: 0.004736, mae: 0.064048, mean_q: -0.312073
 81600/100000: episode: 816, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -14.334, mean reward: -0.143 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.596, 10.148], loss: 0.002563, mae: 0.051182, mean_q: -0.312806
 81700/100000: episode: 817, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: -17.673, mean reward: -0.177 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.565, 10.217], loss: 0.002558, mae: 0.049743, mean_q: -0.296088
 81800/100000: episode: 818, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -16.586, mean reward: -0.166 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.941, 10.098], loss: 0.002765, mae: 0.050595, mean_q: -0.306456
 81900/100000: episode: 819, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -18.728, mean reward: -0.187 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.040, 10.130], loss: 0.002407, mae: 0.048235, mean_q: -0.313609
 82000/100000: episode: 820, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -14.949, mean reward: -0.149 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.675, 10.129], loss: 0.002545, mae: 0.049219, mean_q: -0.329035
 82100/100000: episode: 821, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -17.716, mean reward: -0.177 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.550, 10.153], loss: 0.002770, mae: 0.050936, mean_q: -0.316605
 82200/100000: episode: 822, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -19.135, mean reward: -0.191 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.446, 10.098], loss: 0.002809, mae: 0.051307, mean_q: -0.312876
 82300/100000: episode: 823, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -19.374, mean reward: -0.194 [-1.000, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.909, 10.185], loss: 0.002787, mae: 0.051777, mean_q: -0.302954
 82400/100000: episode: 824, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: -18.164, mean reward: -0.182 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.937, 10.190], loss: 0.002762, mae: 0.051360, mean_q: -0.302194
 82500/100000: episode: 825, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: -18.470, mean reward: -0.185 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.775, 10.098], loss: 0.002482, mae: 0.049961, mean_q: -0.321057
 82600/100000: episode: 826, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: -17.357, mean reward: -0.174 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.664, 10.319], loss: 0.002556, mae: 0.049768, mean_q: -0.343656
 82700/100000: episode: 827, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -17.138, mean reward: -0.171 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.419, 10.098], loss: 0.002496, mae: 0.048441, mean_q: -0.339176
 82800/100000: episode: 828, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -15.015, mean reward: -0.150 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.592, 10.124], loss: 0.002639, mae: 0.050071, mean_q: -0.302832
 82900/100000: episode: 829, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.301, mean reward: -0.183 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.875, 10.098], loss: 0.002672, mae: 0.050341, mean_q: -0.336223
 83000/100000: episode: 830, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -15.329, mean reward: -0.153 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.990, 10.147], loss: 0.004487, mae: 0.063376, mean_q: -0.309372
 83100/100000: episode: 831, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -11.356, mean reward: -0.114 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.588, 10.098], loss: 0.002720, mae: 0.051993, mean_q: -0.274337
 83200/100000: episode: 832, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -18.615, mean reward: -0.186 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.678, 10.160], loss: 0.002657, mae: 0.050652, mean_q: -0.332182
 83300/100000: episode: 833, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.800, mean reward: -0.178 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.108, 10.098], loss: 0.002920, mae: 0.053473, mean_q: -0.310403
 83400/100000: episode: 834, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -20.089, mean reward: -0.201 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.435, 10.099], loss: 0.002511, mae: 0.049402, mean_q: -0.326895
 83500/100000: episode: 835, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -17.414, mean reward: -0.174 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.210, 10.098], loss: 0.002629, mae: 0.049513, mean_q: -0.340496
 83600/100000: episode: 836, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: -12.027, mean reward: -0.120 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.228, 10.409], loss: 0.002858, mae: 0.052033, mean_q: -0.337795
 83700/100000: episode: 837, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.576, mean reward: -0.176 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.713, 10.292], loss: 0.002481, mae: 0.048946, mean_q: -0.352980
 83800/100000: episode: 838, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -20.313, mean reward: -0.203 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.595, 10.098], loss: 0.002501, mae: 0.049802, mean_q: -0.333747
 83900/100000: episode: 839, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -11.714, mean reward: -0.117 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.695, 10.510], loss: 0.002821, mae: 0.051869, mean_q: -0.289575
 84000/100000: episode: 840, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: -15.007, mean reward: -0.150 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.058, 10.284], loss: 0.002711, mae: 0.051500, mean_q: -0.337770
 84100/100000: episode: 841, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -14.508, mean reward: -0.145 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.736, 10.098], loss: 0.002585, mae: 0.050072, mean_q: -0.348882
 84200/100000: episode: 842, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -11.876, mean reward: -0.119 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.809, 10.098], loss: 0.002804, mae: 0.052757, mean_q: -0.286469
 84300/100000: episode: 843, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -14.500, mean reward: -0.145 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.306, 10.098], loss: 0.002569, mae: 0.049720, mean_q: -0.313908
 84400/100000: episode: 844, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.519, mean reward: -0.145 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.037, 10.264], loss: 0.002528, mae: 0.048936, mean_q: -0.332537
 84500/100000: episode: 845, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -18.478, mean reward: -0.185 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.131, 10.098], loss: 0.002583, mae: 0.050342, mean_q: -0.285372
 84600/100000: episode: 846, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -17.132, mean reward: -0.171 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.861, 10.232], loss: 0.002439, mae: 0.048854, mean_q: -0.312485
 84700/100000: episode: 847, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -18.908, mean reward: -0.189 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.687, 10.098], loss: 0.002686, mae: 0.050832, mean_q: -0.332663
 84800/100000: episode: 848, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -17.971, mean reward: -0.180 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.355, 10.131], loss: 0.007034, mae: 0.070747, mean_q: -0.325393
 84900/100000: episode: 849, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -20.118, mean reward: -0.201 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.524, 10.098], loss: 0.005271, mae: 0.065536, mean_q: -0.317339
 85000/100000: episode: 850, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -13.676, mean reward: -0.137 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.889, 10.098], loss: 0.002979, mae: 0.053696, mean_q: -0.306983
 85100/100000: episode: 851, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -15.660, mean reward: -0.157 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.526, 10.257], loss: 0.002367, mae: 0.047523, mean_q: -0.322696
 85200/100000: episode: 852, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -17.523, mean reward: -0.175 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.948, 10.111], loss: 0.002491, mae: 0.049334, mean_q: -0.315017
 85300/100000: episode: 853, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -17.366, mean reward: -0.174 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.027, 10.098], loss: 0.002487, mae: 0.049146, mean_q: -0.299836
 85400/100000: episode: 854, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -13.305, mean reward: -0.133 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.758, 10.253], loss: 0.002552, mae: 0.049656, mean_q: -0.323156
 85500/100000: episode: 855, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -14.110, mean reward: -0.141 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.845, 10.382], loss: 0.002536, mae: 0.049660, mean_q: -0.298980
 85600/100000: episode: 856, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -12.808, mean reward: -0.128 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.886, 10.392], loss: 0.002624, mae: 0.050529, mean_q: -0.307855
 85700/100000: episode: 857, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -19.784, mean reward: -0.198 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.320, 10.098], loss: 0.002484, mae: 0.048856, mean_q: -0.314776
 85800/100000: episode: 858, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -19.135, mean reward: -0.191 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.445, 10.098], loss: 0.002578, mae: 0.049966, mean_q: -0.273501
 85900/100000: episode: 859, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -11.972, mean reward: -0.120 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.485, 10.129], loss: 0.002590, mae: 0.049844, mean_q: -0.318507
 86000/100000: episode: 860, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.281, mean reward: -0.193 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.254, 10.185], loss: 0.002521, mae: 0.049316, mean_q: -0.311650
 86100/100000: episode: 861, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -20.017, mean reward: -0.200 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.008, 10.098], loss: 0.002506, mae: 0.049413, mean_q: -0.274089
 86200/100000: episode: 862, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -12.376, mean reward: -0.124 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.207, 10.098], loss: 0.002552, mae: 0.049388, mean_q: -0.318223
 86300/100000: episode: 863, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -15.645, mean reward: -0.156 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.475, 10.275], loss: 0.002808, mae: 0.052000, mean_q: -0.312366
 86400/100000: episode: 864, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -13.793, mean reward: -0.138 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.914, 10.098], loss: 0.002674, mae: 0.050520, mean_q: -0.304684
 86500/100000: episode: 865, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -17.894, mean reward: -0.179 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.598, 10.221], loss: 0.002577, mae: 0.049416, mean_q: -0.359363
 86600/100000: episode: 866, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.261, mean reward: -0.153 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.865, 10.137], loss: 0.002674, mae: 0.051270, mean_q: -0.305852
 86700/100000: episode: 867, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.623, mean reward: -0.196 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.683, 10.175], loss: 0.002618, mae: 0.050608, mean_q: -0.324675
 86800/100000: episode: 868, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -10.604, mean reward: -0.106 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.335, 10.257], loss: 0.002717, mae: 0.051559, mean_q: -0.309913
 86900/100000: episode: 869, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.716, mean reward: -0.177 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.311, 10.098], loss: 0.002613, mae: 0.050932, mean_q: -0.315468
 87000/100000: episode: 870, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: -19.029, mean reward: -0.190 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.340, 10.098], loss: 0.002678, mae: 0.050079, mean_q: -0.331230
 87100/100000: episode: 871, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.323, mean reward: -0.153 [-1.000, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.730, 10.098], loss: 0.002620, mae: 0.049657, mean_q: -0.329430
 87200/100000: episode: 872, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -18.196, mean reward: -0.182 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.555, 10.098], loss: 0.002658, mae: 0.050773, mean_q: -0.316702
 87300/100000: episode: 873, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -18.054, mean reward: -0.181 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.681, 10.121], loss: 0.002689, mae: 0.050788, mean_q: -0.326625
 87400/100000: episode: 874, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.712, mean reward: -0.187 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.418, 10.098], loss: 0.002837, mae: 0.052132, mean_q: -0.319556
 87500/100000: episode: 875, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -18.152, mean reward: -0.182 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.307, 10.098], loss: 0.002643, mae: 0.050982, mean_q: -0.319175
 87600/100000: episode: 876, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -14.428, mean reward: -0.144 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.176, 10.442], loss: 0.002605, mae: 0.051078, mean_q: -0.290468
 87700/100000: episode: 877, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -12.611, mean reward: -0.126 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.843, 10.442], loss: 0.002561, mae: 0.051565, mean_q: -0.296977
 87800/100000: episode: 878, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -14.836, mean reward: -0.148 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.697, 10.098], loss: 0.002494, mae: 0.050349, mean_q: -0.276901
 87900/100000: episode: 879, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -18.478, mean reward: -0.185 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.247, 10.098], loss: 0.002678, mae: 0.051123, mean_q: -0.317596
 88000/100000: episode: 880, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -15.680, mean reward: -0.157 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.107, 10.098], loss: 0.002799, mae: 0.053794, mean_q: -0.275586
 88100/100000: episode: 881, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -13.041, mean reward: -0.130 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.368, 10.111], loss: 0.003334, mae: 0.058801, mean_q: -0.278337
 88200/100000: episode: 882, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -18.915, mean reward: -0.189 [-1.000, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.645, 10.235], loss: 0.011039, mae: 0.086855, mean_q: -0.318381
 88300/100000: episode: 883, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -20.161, mean reward: -0.202 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.350, 10.232], loss: 0.002765, mae: 0.054039, mean_q: -0.300658
 88400/100000: episode: 884, duration: 0.738s, episode steps: 100, steps per second: 135, episode reward: -16.931, mean reward: -0.169 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.659, 10.104], loss: 0.002694, mae: 0.052001, mean_q: -0.265080
 88500/100000: episode: 885, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -17.370, mean reward: -0.174 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.416, 10.098], loss: 0.002517, mae: 0.049776, mean_q: -0.334901
 88600/100000: episode: 886, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: -11.460, mean reward: -0.115 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.457, 10.205], loss: 0.002367, mae: 0.048349, mean_q: -0.339501
 88700/100000: episode: 887, duration: 0.707s, episode steps: 100, steps per second: 141, episode reward: -17.146, mean reward: -0.171 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.496, 10.098], loss: 0.002412, mae: 0.048234, mean_q: -0.315922
 88800/100000: episode: 888, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -17.594, mean reward: -0.176 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.125, 10.446], loss: 0.002601, mae: 0.050258, mean_q: -0.309118
 88900/100000: episode: 889, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -14.912, mean reward: -0.149 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.519, 10.284], loss: 0.002522, mae: 0.049669, mean_q: -0.278129
 89000/100000: episode: 890, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: -18.414, mean reward: -0.184 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.711, 10.117], loss: 0.002415, mae: 0.048766, mean_q: -0.280926
 89100/100000: episode: 891, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -15.851, mean reward: -0.159 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.777, 10.098], loss: 0.002599, mae: 0.050122, mean_q: -0.305082
 89200/100000: episode: 892, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: -16.144, mean reward: -0.161 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.003, 10.098], loss: 0.002558, mae: 0.049303, mean_q: -0.289346
 89300/100000: episode: 893, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: -17.501, mean reward: -0.175 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.025, 10.098], loss: 0.002500, mae: 0.049605, mean_q: -0.313852
 89400/100000: episode: 894, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: -17.887, mean reward: -0.179 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.848, 10.286], loss: 0.002687, mae: 0.050348, mean_q: -0.331864
 89500/100000: episode: 895, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: -13.959, mean reward: -0.140 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.282, 10.098], loss: 0.002516, mae: 0.049374, mean_q: -0.326295
 89600/100000: episode: 896, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -16.689, mean reward: -0.167 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.234, 10.191], loss: 0.002677, mae: 0.050342, mean_q: -0.322155
 89700/100000: episode: 897, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -15.319, mean reward: -0.153 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.531, 10.098], loss: 0.002494, mae: 0.048841, mean_q: -0.296162
 89800/100000: episode: 898, duration: 0.738s, episode steps: 100, steps per second: 136, episode reward: -16.215, mean reward: -0.162 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.855, 10.263], loss: 0.002794, mae: 0.051909, mean_q: -0.280939
 89900/100000: episode: 899, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: -16.671, mean reward: -0.167 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.616, 10.272], loss: 0.002459, mae: 0.048397, mean_q: -0.340928
 90000/100000: episode: 900, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: -17.114, mean reward: -0.171 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.396, 10.248], loss: 0.002589, mae: 0.049519, mean_q: -0.319409
 90100/100000: episode: 901, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: -16.982, mean reward: -0.170 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.533, 10.098], loss: 0.002804, mae: 0.051537, mean_q: -0.304437
 90200/100000: episode: 902, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: -16.996, mean reward: -0.170 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.785, 10.356], loss: 0.002717, mae: 0.051112, mean_q: -0.292572
 90300/100000: episode: 903, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -16.016, mean reward: -0.160 [-1.000, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.156, 10.098], loss: 0.002689, mae: 0.051279, mean_q: -0.309677
 90400/100000: episode: 904, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: -18.791, mean reward: -0.188 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.850, 10.098], loss: 0.003014, mae: 0.052174, mean_q: -0.314617
 90500/100000: episode: 905, duration: 0.652s, episode steps: 100, steps per second: 153, episode reward: -15.477, mean reward: -0.155 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.592, 10.098], loss: 0.002489, mae: 0.048527, mean_q: -0.326005
 90600/100000: episode: 906, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: -17.593, mean reward: -0.176 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.912, 10.098], loss: 0.002764, mae: 0.050780, mean_q: -0.343263
 90700/100000: episode: 907, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -14.050, mean reward: -0.141 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.202, 10.196], loss: 0.002497, mae: 0.049212, mean_q: -0.337263
 90800/100000: episode: 908, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: -15.172, mean reward: -0.152 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.826, 10.280], loss: 0.002446, mae: 0.048519, mean_q: -0.310330
 90900/100000: episode: 909, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: -14.426, mean reward: -0.144 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.328, 10.247], loss: 0.002577, mae: 0.050059, mean_q: -0.325141
 91000/100000: episode: 910, duration: 0.639s, episode steps: 100, steps per second: 157, episode reward: -16.113, mean reward: -0.161 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.074, 10.342], loss: 0.002549, mae: 0.049618, mean_q: -0.315560
 91100/100000: episode: 911, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.773, mean reward: -0.168 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.703, 10.317], loss: 0.002732, mae: 0.050706, mean_q: -0.330417
 91200/100000: episode: 912, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -18.771, mean reward: -0.188 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.326, 10.113], loss: 0.002742, mae: 0.050614, mean_q: -0.319115
 91300/100000: episode: 913, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -20.239, mean reward: -0.202 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.729, 10.098], loss: 0.002725, mae: 0.050955, mean_q: -0.322247
 91400/100000: episode: 914, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -11.411, mean reward: -0.114 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.655, 10.098], loss: 0.002689, mae: 0.050219, mean_q: -0.290664
 91500/100000: episode: 915, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: -12.632, mean reward: -0.126 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.175, 10.100], loss: 0.002410, mae: 0.048348, mean_q: -0.299631
 91600/100000: episode: 916, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: -14.160, mean reward: -0.142 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.848, 10.136], loss: 0.002644, mae: 0.050779, mean_q: -0.310769
 91700/100000: episode: 917, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.945, mean reward: -0.179 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.591, 10.098], loss: 0.002527, mae: 0.049863, mean_q: -0.296951
 91800/100000: episode: 918, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -19.295, mean reward: -0.193 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.100, 10.098], loss: 0.002597, mae: 0.049490, mean_q: -0.292713
 91900/100000: episode: 919, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -13.893, mean reward: -0.139 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.998, 10.262], loss: 0.003458, mae: 0.056340, mean_q: -0.281815
 92000/100000: episode: 920, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -17.255, mean reward: -0.173 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.261, 10.098], loss: 0.004786, mae: 0.065463, mean_q: -0.314708
 92100/100000: episode: 921, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -16.446, mean reward: -0.164 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.626, 10.246], loss: 0.002574, mae: 0.050021, mean_q: -0.319105
 92200/100000: episode: 922, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -17.208, mean reward: -0.172 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.656, 10.098], loss: 0.002497, mae: 0.048379, mean_q: -0.308218
 92300/100000: episode: 923, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: -15.868, mean reward: -0.159 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.360, 10.098], loss: 0.002694, mae: 0.050051, mean_q: -0.311270
 92400/100000: episode: 924, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: -20.682, mean reward: -0.207 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.568, 10.278], loss: 0.002794, mae: 0.051619, mean_q: -0.279782
 92500/100000: episode: 925, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: -17.120, mean reward: -0.171 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.151, 10.098], loss: 0.002517, mae: 0.048875, mean_q: -0.299510
 92600/100000: episode: 926, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: -15.423, mean reward: -0.154 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.701, 10.122], loss: 0.002502, mae: 0.049390, mean_q: -0.305494
 92700/100000: episode: 927, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: -15.977, mean reward: -0.160 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.705, 10.098], loss: 0.002672, mae: 0.049912, mean_q: -0.309241
 92800/100000: episode: 928, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: -19.985, mean reward: -0.200 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.215, 10.121], loss: 0.002417, mae: 0.048721, mean_q: -0.308504
 92900/100000: episode: 929, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -14.857, mean reward: -0.149 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.600, 10.321], loss: 0.002692, mae: 0.050625, mean_q: -0.285843
 93000/100000: episode: 930, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -11.468, mean reward: -0.115 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.804, 10.098], loss: 0.002506, mae: 0.048180, mean_q: -0.309228
 93100/100000: episode: 931, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: -19.244, mean reward: -0.192 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.908, 10.098], loss: 0.002411, mae: 0.047902, mean_q: -0.303887
 93200/100000: episode: 932, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: -18.307, mean reward: -0.183 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.487, 10.121], loss: 0.002747, mae: 0.051502, mean_q: -0.270929
 93300/100000: episode: 933, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: -15.738, mean reward: -0.157 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.707, 10.435], loss: 0.002766, mae: 0.050993, mean_q: -0.316933
 93400/100000: episode: 934, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: -20.389, mean reward: -0.204 [-1.000, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.852, 10.126], loss: 0.002796, mae: 0.050924, mean_q: -0.299577
 93500/100000: episode: 935, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: -17.868, mean reward: -0.179 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.177, 10.150], loss: 0.002912, mae: 0.052891, mean_q: -0.334415
 93600/100000: episode: 936, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: -17.864, mean reward: -0.179 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.530, 10.098], loss: 0.002692, mae: 0.050062, mean_q: -0.306407
 93700/100000: episode: 937, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: -17.180, mean reward: -0.172 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.961, 10.107], loss: 0.002532, mae: 0.049263, mean_q: -0.309795
 93800/100000: episode: 938, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: -16.179, mean reward: -0.162 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.373, 10.134], loss: 0.002663, mae: 0.050718, mean_q: -0.308828
 93900/100000: episode: 939, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: -18.232, mean reward: -0.182 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.469, 10.294], loss: 0.002657, mae: 0.049408, mean_q: -0.351816
 94000/100000: episode: 940, duration: 0.643s, episode steps: 100, steps per second: 155, episode reward: -20.066, mean reward: -0.201 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.896, 10.270], loss: 0.002663, mae: 0.049803, mean_q: -0.317596
 94100/100000: episode: 941, duration: 0.631s, episode steps: 100, steps per second: 158, episode reward: -16.055, mean reward: -0.161 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.276, 10.098], loss: 0.002843, mae: 0.052774, mean_q: -0.288526
 94200/100000: episode: 942, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -18.949, mean reward: -0.189 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.557, 10.189], loss: 0.002743, mae: 0.050732, mean_q: -0.320167
 94300/100000: episode: 943, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -11.692, mean reward: -0.117 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.442, 10.375], loss: 0.002721, mae: 0.051440, mean_q: -0.295024
 94400/100000: episode: 944, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: -17.870, mean reward: -0.179 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.353, 10.098], loss: 0.002673, mae: 0.050678, mean_q: -0.299159
 94500/100000: episode: 945, duration: 0.673s, episode steps: 100, steps per second: 148, episode reward: -15.544, mean reward: -0.155 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.937, 10.312], loss: 0.002494, mae: 0.049332, mean_q: -0.272713
 94600/100000: episode: 946, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: -13.320, mean reward: -0.133 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.624, 10.098], loss: 0.002825, mae: 0.051806, mean_q: -0.350538
 94700/100000: episode: 947, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: -18.032, mean reward: -0.180 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.637, 10.105], loss: 0.002666, mae: 0.051545, mean_q: -0.319303
 94800/100000: episode: 948, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: -19.615, mean reward: -0.196 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.480, 10.098], loss: 0.002566, mae: 0.049801, mean_q: -0.319917
 94900/100000: episode: 949, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: -18.820, mean reward: -0.188 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.677, 10.144], loss: 0.002728, mae: 0.051578, mean_q: -0.278336
 95000/100000: episode: 950, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -16.904, mean reward: -0.169 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.780, 10.201], loss: 0.002737, mae: 0.051586, mean_q: -0.293417
 95100/100000: episode: 951, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -18.676, mean reward: -0.187 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.518, 10.098], loss: 0.002773, mae: 0.051234, mean_q: -0.319942
 95200/100000: episode: 952, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -17.021, mean reward: -0.170 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.427, 10.098], loss: 0.002722, mae: 0.050334, mean_q: -0.338188
 95300/100000: episode: 953, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: -14.771, mean reward: -0.148 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.105, 10.098], loss: 0.002732, mae: 0.051132, mean_q: -0.285908
 95400/100000: episode: 954, duration: 0.660s, episode steps: 100, steps per second: 152, episode reward: -18.116, mean reward: -0.181 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.611, 10.098], loss: 0.003019, mae: 0.052634, mean_q: -0.338579
 95500/100000: episode: 955, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: -16.460, mean reward: -0.165 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.754, 10.366], loss: 0.007667, mae: 0.078376, mean_q: -0.314632
 95600/100000: episode: 956, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: -15.139, mean reward: -0.151 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.236, 10.141], loss: 0.003093, mae: 0.056632, mean_q: -0.357512
 95700/100000: episode: 957, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -10.552, mean reward: -0.106 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.752, 10.372], loss: 0.002507, mae: 0.050087, mean_q: -0.301774
 95800/100000: episode: 958, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -17.199, mean reward: -0.172 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.677, 10.098], loss: 0.002625, mae: 0.049746, mean_q: -0.334483
 95900/100000: episode: 959, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: -17.673, mean reward: -0.177 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.826, 10.098], loss: 0.002591, mae: 0.049380, mean_q: -0.323031
 96000/100000: episode: 960, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.553, mean reward: -0.196 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.045, 10.098], loss: 0.002563, mae: 0.050241, mean_q: -0.335810
 96100/100000: episode: 961, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: -14.765, mean reward: -0.148 [-1.000, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.443, 10.098], loss: 0.002543, mae: 0.049075, mean_q: -0.301902
 96200/100000: episode: 962, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: -11.864, mean reward: -0.119 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.007, 10.196], loss: 0.002680, mae: 0.050900, mean_q: -0.312578
 96300/100000: episode: 963, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: -19.344, mean reward: -0.193 [-1.000, 0.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.346, 10.098], loss: 0.002763, mae: 0.051371, mean_q: -0.309703
 96400/100000: episode: 964, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: -16.855, mean reward: -0.169 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.810, 10.098], loss: 0.002755, mae: 0.052125, mean_q: -0.284781
 96500/100000: episode: 965, duration: 0.692s, episode steps: 100, steps per second: 145, episode reward: -20.024, mean reward: -0.200 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.991, 10.098], loss: 0.002829, mae: 0.051453, mean_q: -0.331526
 96600/100000: episode: 966, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: -15.767, mean reward: -0.158 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.189, 10.301], loss: 0.002858, mae: 0.052460, mean_q: -0.295078
 96700/100000: episode: 967, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -19.031, mean reward: -0.190 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.445, 10.254], loss: 0.002717, mae: 0.051157, mean_q: -0.323760
 96800/100000: episode: 968, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: -16.852, mean reward: -0.169 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.728, 10.098], loss: 0.002674, mae: 0.051536, mean_q: -0.315354
 96900/100000: episode: 969, duration: 0.678s, episode steps: 100, steps per second: 147, episode reward: -18.135, mean reward: -0.181 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.688, 10.129], loss: 0.002847, mae: 0.052869, mean_q: -0.334737
 97000/100000: episode: 970, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: -16.698, mean reward: -0.167 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.645, 10.098], loss: 0.002632, mae: 0.050651, mean_q: -0.360276
 97100/100000: episode: 971, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -18.878, mean reward: -0.189 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.712, 10.126], loss: 0.002874, mae: 0.052852, mean_q: -0.299320
 97200/100000: episode: 972, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: -15.469, mean reward: -0.155 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.358, 10.098], loss: 0.003153, mae: 0.055222, mean_q: -0.304191
 97300/100000: episode: 973, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -17.721, mean reward: -0.177 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.516, 10.123], loss: 0.002841, mae: 0.052002, mean_q: -0.304650
 97400/100000: episode: 974, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: -15.097, mean reward: -0.151 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.965, 10.134], loss: 0.002890, mae: 0.053667, mean_q: -0.289738
 97500/100000: episode: 975, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: -19.229, mean reward: -0.192 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.957, 10.098], loss: 0.003012, mae: 0.054449, mean_q: -0.313895
 97600/100000: episode: 976, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -18.462, mean reward: -0.185 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.703, 10.173], loss: 0.002780, mae: 0.051825, mean_q: -0.332459
 97700/100000: episode: 977, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -16.619, mean reward: -0.166 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.568, 10.098], loss: 0.002806, mae: 0.052511, mean_q: -0.299233
 97800/100000: episode: 978, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: -16.323, mean reward: -0.163 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.656, 10.098], loss: 0.002878, mae: 0.052782, mean_q: -0.338582
 97900/100000: episode: 979, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -17.187, mean reward: -0.172 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.976, 10.098], loss: 0.007117, mae: 0.073156, mean_q: -0.322859
 98000/100000: episode: 980, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -15.525, mean reward: -0.155 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.930, 10.213], loss: 0.004713, mae: 0.066418, mean_q: -0.353766
 98100/100000: episode: 981, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: -18.654, mean reward: -0.187 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.921, 10.098], loss: 0.002840, mae: 0.052357, mean_q: -0.329636
 98200/100000: episode: 982, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -16.837, mean reward: -0.168 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.949, 10.098], loss: 0.002484, mae: 0.050217, mean_q: -0.326506
 98300/100000: episode: 983, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -17.238, mean reward: -0.172 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.762, 10.098], loss: 0.002785, mae: 0.051586, mean_q: -0.340780
 98400/100000: episode: 984, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: -15.076, mean reward: -0.151 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.053, 10.098], loss: 0.002804, mae: 0.051872, mean_q: -0.300217
 98500/100000: episode: 985, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -17.118, mean reward: -0.171 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.856, 10.098], loss: 0.002751, mae: 0.051700, mean_q: -0.312311
 98600/100000: episode: 986, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -15.188, mean reward: -0.152 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.670, 10.150], loss: 0.002837, mae: 0.052846, mean_q: -0.298519
 98700/100000: episode: 987, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -17.590, mean reward: -0.176 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.960, 10.098], loss: 0.002810, mae: 0.051955, mean_q: -0.318190
 98800/100000: episode: 988, duration: 0.692s, episode steps: 100, steps per second: 145, episode reward: -14.771, mean reward: -0.148 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.317, 10.259], loss: 0.002994, mae: 0.053659, mean_q: -0.296111
 98900/100000: episode: 989, duration: 0.736s, episode steps: 100, steps per second: 136, episode reward: -16.419, mean reward: -0.164 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.154, 10.098], loss: 0.002747, mae: 0.051482, mean_q: -0.332081
 99000/100000: episode: 990, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: -17.307, mean reward: -0.173 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.201, 10.098], loss: 0.002989, mae: 0.052745, mean_q: -0.299238
 99100/100000: episode: 991, duration: 0.660s, episode steps: 100, steps per second: 152, episode reward: -20.481, mean reward: -0.205 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.497, 10.168], loss: 0.003136, mae: 0.054996, mean_q: -0.319659
 99200/100000: episode: 992, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: -17.427, mean reward: -0.174 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.727, 10.098], loss: 0.002976, mae: 0.053509, mean_q: -0.309293
 99300/100000: episode: 993, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: -18.376, mean reward: -0.184 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.306, 10.141], loss: 0.002746, mae: 0.051050, mean_q: -0.341693
 99400/100000: episode: 994, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -17.471, mean reward: -0.175 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.536, 10.154], loss: 0.002903, mae: 0.052689, mean_q: -0.326213
 99500/100000: episode: 995, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: -17.111, mean reward: -0.171 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.960, 10.184], loss: 0.002797, mae: 0.052896, mean_q: -0.298740
 99600/100000: episode: 996, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.663, mean reward: -0.167 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.547, 10.098], loss: 0.003124, mae: 0.054650, mean_q: -0.288320
 99700/100000: episode: 997, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -18.457, mean reward: -0.185 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.910, 10.098], loss: 0.002824, mae: 0.051528, mean_q: -0.343221
 99800/100000: episode: 998, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.662, mean reward: -0.197 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.090, 10.140], loss: 0.003003, mae: 0.053459, mean_q: -0.315791
 99900/100000: episode: 999, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -17.702, mean reward: -0.177 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.498, 10.098], loss: 0.002856, mae: 0.052441, mean_q: -0.340936
 100000/100000: episode: 1000, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -18.714, mean reward: -0.187 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.664, 10.340], loss: 0.002984, mae: 0.053365, mean_q: -0.329855
done, took 616.460 seconds
[Info] End Uniform Random Simulation. Falsification occurred 0 times.
