Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.174s, episode steps: 100, steps per second: 576, episode reward: -17.710, mean reward: -0.177 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.702, 10.140], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.062s, episode steps: 100, steps per second: 1620, episode reward: -17.389, mean reward: -0.174 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.160, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.062s, episode steps: 100, steps per second: 1621, episode reward: -14.259, mean reward: -0.143 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.793, 10.393], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.061s, episode steps: 100, steps per second: 1626, episode reward: -19.763, mean reward: -0.198 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.977, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.062s, episode steps: 100, steps per second: 1622, episode reward: -18.003, mean reward: -0.180 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.355, 10.165], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.165s, episode steps: 100, steps per second: 86, episode reward: -19.168, mean reward: -0.192 [-1.000, 0.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.054, 10.098], loss: 0.047210, mae: 0.215179, mean_q: -0.115412
   700/100000: episode: 7, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -10.799, mean reward: -0.108 [-1.000, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.763, 10.462], loss: 0.013937, mae: 0.108471, mean_q: -0.212150
   800/100000: episode: 8, duration: 0.473s, episode steps: 100, steps per second: 212, episode reward: -12.665, mean reward: -0.127 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.806, 10.098], loss: 0.013572, mae: 0.109310, mean_q: -0.248365
   900/100000: episode: 9, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: -17.229, mean reward: -0.172 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.695, 10.154], loss: 0.011448, mae: 0.098444, mean_q: -0.289723
  1000/100000: episode: 10, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -15.335, mean reward: -0.153 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.017, 10.230], loss: 0.009310, mae: 0.091343, mean_q: -0.308750
  1100/100000: episode: 11, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -14.955, mean reward: -0.150 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.408, 10.098], loss: 0.009450, mae: 0.092051, mean_q: -0.307938
  1200/100000: episode: 12, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -19.072, mean reward: -0.191 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.083, 10.098], loss: 0.008794, mae: 0.090778, mean_q: -0.330075
  1300/100000: episode: 13, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -13.718, mean reward: -0.137 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.587, 10.098], loss: 0.008827, mae: 0.092735, mean_q: -0.328935
  1400/100000: episode: 14, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -16.118, mean reward: -0.161 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.632, 10.160], loss: 0.008115, mae: 0.086427, mean_q: -0.296912
  1500/100000: episode: 15, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.636, mean reward: -0.166 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.039, 10.215], loss: 0.007550, mae: 0.084206, mean_q: -0.292309
  1600/100000: episode: 16, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.925, mean reward: -0.179 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.749, 10.129], loss: 0.006566, mae: 0.079444, mean_q: -0.329503
  1700/100000: episode: 17, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -14.202, mean reward: -0.142 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.981, 10.098], loss: 0.006755, mae: 0.079874, mean_q: -0.325674
  1800/100000: episode: 18, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.086, mean reward: -0.151 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.696, 10.142], loss: 0.006629, mae: 0.078568, mean_q: -0.286164
  1900/100000: episode: 19, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.465, mean reward: -0.185 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.590, 10.098], loss: 0.006868, mae: 0.080298, mean_q: -0.344298
  2000/100000: episode: 20, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.418, mean reward: -0.154 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.391, 10.364], loss: 0.006586, mae: 0.078737, mean_q: -0.321585
  2100/100000: episode: 21, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -14.124, mean reward: -0.141 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.927, 10.098], loss: 0.005766, mae: 0.075211, mean_q: -0.302804
  2200/100000: episode: 22, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.036, mean reward: -0.180 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.740, 10.141], loss: 0.005410, mae: 0.074023, mean_q: -0.331643
  2300/100000: episode: 23, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.994, mean reward: -0.170 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.539, 10.351], loss: 0.006987, mae: 0.081150, mean_q: -0.318628
  2400/100000: episode: 24, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.425, mean reward: -0.184 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.521, 10.313], loss: 0.006575, mae: 0.080465, mean_q: -0.284009
  2500/100000: episode: 25, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -11.760, mean reward: -0.118 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.855, 10.098], loss: 0.006884, mae: 0.080584, mean_q: -0.299290
  2600/100000: episode: 26, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.656, mean reward: -0.167 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.439, 10.287], loss: 0.005751, mae: 0.075980, mean_q: -0.306420
  2700/100000: episode: 27, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -12.000, mean reward: -0.120 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.494, 10.098], loss: 0.005818, mae: 0.074076, mean_q: -0.296259
  2800/100000: episode: 28, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -20.170, mean reward: -0.202 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.340, 10.173], loss: 0.006472, mae: 0.078624, mean_q: -0.311949
  2900/100000: episode: 29, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.124, mean reward: -0.141 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.474, 10.098], loss: 0.006870, mae: 0.083086, mean_q: -0.346169
  3000/100000: episode: 30, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.682, mean reward: -0.177 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.991, 10.157], loss: 0.006108, mae: 0.077146, mean_q: -0.298744
  3100/100000: episode: 31, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.244, mean reward: -0.182 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.734, 10.148], loss: 0.007272, mae: 0.081365, mean_q: -0.307725
  3200/100000: episode: 32, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -15.169, mean reward: -0.152 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.792, 10.098], loss: 0.006772, mae: 0.081074, mean_q: -0.346088
  3300/100000: episode: 33, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -15.483, mean reward: -0.155 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.460, 10.098], loss: 0.006190, mae: 0.078916, mean_q: -0.316720
  3400/100000: episode: 34, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -18.498, mean reward: -0.185 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.249, 10.217], loss: 0.005582, mae: 0.070399, mean_q: -0.330655
  3500/100000: episode: 35, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -14.860, mean reward: -0.149 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.387, 10.098], loss: 0.005630, mae: 0.074580, mean_q: -0.322241
  3600/100000: episode: 36, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -20.191, mean reward: -0.202 [-1.000, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.050, 10.098], loss: 0.005817, mae: 0.075362, mean_q: -0.313587
  3700/100000: episode: 37, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -14.177, mean reward: -0.142 [-1.000, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.189, 10.155], loss: 0.005627, mae: 0.071210, mean_q: -0.324208
  3800/100000: episode: 38, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.817, mean reward: -0.168 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.018, 10.153], loss: 0.007618, mae: 0.084287, mean_q: -0.286067
  3900/100000: episode: 39, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -18.393, mean reward: -0.184 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.128, 10.260], loss: 0.006978, mae: 0.081179, mean_q: -0.318322
  4000/100000: episode: 40, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -18.232, mean reward: -0.182 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.078, 10.128], loss: 0.006236, mae: 0.077015, mean_q: -0.321036
  4100/100000: episode: 41, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -18.620, mean reward: -0.186 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.697, 10.125], loss: 0.006104, mae: 0.073966, mean_q: -0.292100
  4200/100000: episode: 42, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -18.401, mean reward: -0.184 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.760, 10.264], loss: 0.004585, mae: 0.068021, mean_q: -0.319361
  4300/100000: episode: 43, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -20.566, mean reward: -0.206 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.279, 10.098], loss: 0.005131, mae: 0.070193, mean_q: -0.347811
  4400/100000: episode: 44, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -14.331, mean reward: -0.143 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.777, 10.098], loss: 0.005981, mae: 0.073884, mean_q: -0.335647
  4500/100000: episode: 45, duration: 0.480s, episode steps: 100, steps per second: 209, episode reward: -16.063, mean reward: -0.161 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.015, 10.098], loss: 0.004720, mae: 0.067431, mean_q: -0.306971
  4600/100000: episode: 46, duration: 0.465s, episode steps: 100, steps per second: 215, episode reward: -19.344, mean reward: -0.193 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.572, 10.098], loss: 0.006102, mae: 0.078405, mean_q: -0.324594
  4700/100000: episode: 47, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.235, mean reward: -0.172 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.337, 10.162], loss: 0.004565, mae: 0.067537, mean_q: -0.312986
  4800/100000: episode: 48, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -18.098, mean reward: -0.181 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.059, 10.098], loss: 0.005145, mae: 0.069447, mean_q: -0.337979
  4900/100000: episode: 49, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -20.851, mean reward: -0.209 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.114, 10.124], loss: 0.004733, mae: 0.067700, mean_q: -0.324150
  5000/100000: episode: 50, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -18.869, mean reward: -0.189 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.340, 10.218], loss: 0.005027, mae: 0.071191, mean_q: -0.327703
  5100/100000: episode: 51, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -18.101, mean reward: -0.181 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.702, 10.127], loss: 0.004925, mae: 0.072854, mean_q: -0.348099
  5200/100000: episode: 52, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -11.441, mean reward: -0.114 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.410, 10.295], loss: 0.005240, mae: 0.073322, mean_q: -0.320248
  5300/100000: episode: 53, duration: 0.464s, episode steps: 100, steps per second: 216, episode reward: -19.302, mean reward: -0.193 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.703, 10.098], loss: 0.004491, mae: 0.066094, mean_q: -0.328343
  5400/100000: episode: 54, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -14.402, mean reward: -0.144 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.718, 10.109], loss: 0.005009, mae: 0.069845, mean_q: -0.329568
  5500/100000: episode: 55, duration: 0.468s, episode steps: 100, steps per second: 214, episode reward: -16.197, mean reward: -0.162 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.577, 10.286], loss: 0.005350, mae: 0.072508, mean_q: -0.318069
  5600/100000: episode: 56, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.490, mean reward: -0.175 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.685, 10.098], loss: 0.005276, mae: 0.070400, mean_q: -0.306985
  5700/100000: episode: 57, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -17.909, mean reward: -0.179 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.839, 10.214], loss: 0.004525, mae: 0.067370, mean_q: -0.319581
  5800/100000: episode: 58, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -18.283, mean reward: -0.183 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.182, 10.098], loss: 0.004932, mae: 0.071854, mean_q: -0.343600
  5900/100000: episode: 59, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -17.275, mean reward: -0.173 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.732, 10.349], loss: 0.004587, mae: 0.066545, mean_q: -0.344768
  6000/100000: episode: 60, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.574, mean reward: -0.166 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.730, 10.221], loss: 0.005966, mae: 0.076706, mean_q: -0.322392
  6100/100000: episode: 61, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -19.931, mean reward: -0.199 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.288, 10.098], loss: 0.004851, mae: 0.068636, mean_q: -0.320657
  6200/100000: episode: 62, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -16.326, mean reward: -0.163 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.583, 10.098], loss: 0.004602, mae: 0.068737, mean_q: -0.303492
  6300/100000: episode: 63, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -18.401, mean reward: -0.184 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.681, 10.202], loss: 0.005271, mae: 0.072877, mean_q: -0.314310
  6400/100000: episode: 64, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -14.513, mean reward: -0.145 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.950, 10.431], loss: 0.004202, mae: 0.065395, mean_q: -0.294122
  6500/100000: episode: 65, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -18.314, mean reward: -0.183 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.405, 10.098], loss: 0.004099, mae: 0.065613, mean_q: -0.328718
  6600/100000: episode: 66, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -11.644, mean reward: -0.116 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.939, 10.439], loss: 0.004778, mae: 0.068494, mean_q: -0.339714
  6700/100000: episode: 67, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -16.749, mean reward: -0.167 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.599, 10.098], loss: 0.003968, mae: 0.063300, mean_q: -0.323435
  6800/100000: episode: 68, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -10.736, mean reward: -0.107 [-1.000, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.350, 10.098], loss: 0.004217, mae: 0.065917, mean_q: -0.316333
  6900/100000: episode: 69, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -11.709, mean reward: -0.117 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.868, 10.098], loss: 0.004386, mae: 0.067176, mean_q: -0.335356
  7000/100000: episode: 70, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -16.664, mean reward: -0.167 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.119, 10.185], loss: 0.004990, mae: 0.072276, mean_q: -0.328706
  7100/100000: episode: 71, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -10.023, mean reward: -0.100 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.969, 10.098], loss: 0.004598, mae: 0.068596, mean_q: -0.335626
  7200/100000: episode: 72, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -18.630, mean reward: -0.186 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.039, 10.174], loss: 0.005420, mae: 0.072383, mean_q: -0.291403
  7300/100000: episode: 73, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -16.540, mean reward: -0.165 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.799, 10.249], loss: 0.004041, mae: 0.065133, mean_q: -0.319451
  7400/100000: episode: 74, duration: 0.462s, episode steps: 100, steps per second: 216, episode reward: -15.746, mean reward: -0.157 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.156, 10.098], loss: 0.004450, mae: 0.067496, mean_q: -0.294475
  7500/100000: episode: 75, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -15.965, mean reward: -0.160 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.099, 10.227], loss: 0.005337, mae: 0.073450, mean_q: -0.291409
  7600/100000: episode: 76, duration: 0.465s, episode steps: 100, steps per second: 215, episode reward: -19.478, mean reward: -0.195 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.119, 10.206], loss: 0.004755, mae: 0.071465, mean_q: -0.335110
  7700/100000: episode: 77, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -20.442, mean reward: -0.204 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.853, 10.191], loss: 0.004176, mae: 0.065335, mean_q: -0.299518
  7800/100000: episode: 78, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -17.267, mean reward: -0.173 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.704, 10.098], loss: 0.004322, mae: 0.066879, mean_q: -0.331640
  7900/100000: episode: 79, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -15.776, mean reward: -0.158 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.588, 10.183], loss: 0.004467, mae: 0.066352, mean_q: -0.280058
  8000/100000: episode: 80, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -20.077, mean reward: -0.201 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.649, 10.098], loss: 0.004192, mae: 0.064369, mean_q: -0.328757
  8100/100000: episode: 81, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.204, mean reward: -0.182 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.728, 10.317], loss: 0.005101, mae: 0.069955, mean_q: -0.332302
  8200/100000: episode: 82, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.087, mean reward: -0.171 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.932, 10.098], loss: 0.004443, mae: 0.066685, mean_q: -0.340108
  8300/100000: episode: 83, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -16.189, mean reward: -0.162 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.961, 10.098], loss: 0.005507, mae: 0.072842, mean_q: -0.316182
  8400/100000: episode: 84, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -17.729, mean reward: -0.177 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.045, 10.156], loss: 0.004450, mae: 0.066648, mean_q: -0.294815
  8500/100000: episode: 85, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -13.946, mean reward: -0.139 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.149, 10.098], loss: 0.004580, mae: 0.068643, mean_q: -0.315271
  8600/100000: episode: 86, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -14.007, mean reward: -0.140 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.504, 10.098], loss: 0.004672, mae: 0.066534, mean_q: -0.361727
  8700/100000: episode: 87, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -13.236, mean reward: -0.132 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.773, 10.098], loss: 0.004793, mae: 0.067597, mean_q: -0.324148
  8800/100000: episode: 88, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -19.862, mean reward: -0.199 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.761, 10.181], loss: 0.003976, mae: 0.064861, mean_q: -0.319001
  8900/100000: episode: 89, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.616, mean reward: -0.186 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.856, 10.098], loss: 0.005110, mae: 0.071357, mean_q: -0.349866
  9000/100000: episode: 90, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -14.165, mean reward: -0.142 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.617, 10.098], loss: 0.004381, mae: 0.067967, mean_q: -0.323189
  9100/100000: episode: 91, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -19.700, mean reward: -0.197 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.113, 10.159], loss: 0.005214, mae: 0.068264, mean_q: -0.329825
  9200/100000: episode: 92, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -8.138, mean reward: -0.081 [-1.000, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.678, 10.098], loss: 0.004668, mae: 0.069931, mean_q: -0.308508
  9300/100000: episode: 93, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -17.855, mean reward: -0.179 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.730, 10.098], loss: 0.004778, mae: 0.068894, mean_q: -0.303222
  9400/100000: episode: 94, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -15.613, mean reward: -0.156 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.853, 10.098], loss: 0.004458, mae: 0.067362, mean_q: -0.333412
  9500/100000: episode: 95, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -7.407, mean reward: -0.074 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.765, 10.098], loss: 0.004451, mae: 0.067761, mean_q: -0.338235
  9600/100000: episode: 96, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -19.183, mean reward: -0.192 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.678, 10.147], loss: 0.004427, mae: 0.065971, mean_q: -0.322624
  9700/100000: episode: 97, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -15.442, mean reward: -0.154 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.641, 10.098], loss: 0.003661, mae: 0.061712, mean_q: -0.310198
  9800/100000: episode: 98, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -18.852, mean reward: -0.189 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.079, 10.098], loss: 0.003968, mae: 0.063093, mean_q: -0.312218
  9900/100000: episode: 99, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.548, mean reward: -0.185 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.416, 10.124], loss: 0.004652, mae: 0.068469, mean_q: -0.324363
 10000/100000: episode: 100, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -16.795, mean reward: -0.168 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.961, 10.349], loss: 0.004394, mae: 0.067934, mean_q: -0.300300
 10100/100000: episode: 101, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -16.618, mean reward: -0.166 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.697, 10.098], loss: 0.003547, mae: 0.060849, mean_q: -0.321103
 10200/100000: episode: 102, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -19.890, mean reward: -0.199 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.091, 10.146], loss: 0.003933, mae: 0.061559, mean_q: -0.304812
 10300/100000: episode: 103, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -15.763, mean reward: -0.158 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.163, 10.110], loss: 0.003608, mae: 0.061044, mean_q: -0.299291
 10400/100000: episode: 104, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -15.830, mean reward: -0.158 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.368, 10.103], loss: 0.003554, mae: 0.062495, mean_q: -0.295685
 10500/100000: episode: 105, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -14.394, mean reward: -0.144 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.889, 10.194], loss: 0.004158, mae: 0.067376, mean_q: -0.303021
 10600/100000: episode: 106, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -19.043, mean reward: -0.190 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.180, 10.145], loss: 0.003273, mae: 0.059909, mean_q: -0.285769
 10700/100000: episode: 107, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -11.218, mean reward: -0.112 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.233, 10.098], loss: 0.004310, mae: 0.066882, mean_q: -0.328918
 10800/100000: episode: 108, duration: 0.466s, episode steps: 100, steps per second: 215, episode reward: -15.322, mean reward: -0.153 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.289, 10.098], loss: 0.003673, mae: 0.061922, mean_q: -0.299692
 10900/100000: episode: 109, duration: 0.473s, episode steps: 100, steps per second: 212, episode reward: -18.961, mean reward: -0.190 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.639, 10.098], loss: 0.003286, mae: 0.059249, mean_q: -0.299744
 11000/100000: episode: 110, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -19.705, mean reward: -0.197 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.434, 10.138], loss: 0.004019, mae: 0.064607, mean_q: -0.300874
 11100/100000: episode: 111, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -16.202, mean reward: -0.162 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.561, 10.259], loss: 0.003484, mae: 0.061467, mean_q: -0.289251
 11200/100000: episode: 112, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.431, mean reward: -0.184 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.748, 10.228], loss: 0.004141, mae: 0.065313, mean_q: -0.322109
 11300/100000: episode: 113, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -19.806, mean reward: -0.198 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.420, 10.098], loss: 0.003843, mae: 0.062657, mean_q: -0.294127
 11400/100000: episode: 114, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: -14.173, mean reward: -0.142 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.905, 10.275], loss: 0.003796, mae: 0.063046, mean_q: -0.291199
 11500/100000: episode: 115, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.146, mean reward: -0.181 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.473, 10.192], loss: 0.003677, mae: 0.062080, mean_q: -0.299602
 11600/100000: episode: 116, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -17.483, mean reward: -0.175 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.348, 10.098], loss: 0.004229, mae: 0.067681, mean_q: -0.314710
 11700/100000: episode: 117, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -10.317, mean reward: -0.103 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.766, 10.098], loss: 0.003193, mae: 0.058503, mean_q: -0.310230
 11800/100000: episode: 118, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -17.983, mean reward: -0.180 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.228, 10.098], loss: 0.003319, mae: 0.058983, mean_q: -0.300894
 11900/100000: episode: 119, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -16.252, mean reward: -0.163 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.924, 10.244], loss: 0.003573, mae: 0.060138, mean_q: -0.363188
 12000/100000: episode: 120, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -17.417, mean reward: -0.174 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.662, 10.098], loss: 0.003220, mae: 0.058112, mean_q: -0.335913
 12100/100000: episode: 121, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -13.595, mean reward: -0.136 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.507, 10.098], loss: 0.004212, mae: 0.063863, mean_q: -0.305431
 12200/100000: episode: 122, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -17.880, mean reward: -0.179 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.768, 10.098], loss: 0.003468, mae: 0.060900, mean_q: -0.332751
 12300/100000: episode: 123, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -14.207, mean reward: -0.142 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.241, 10.098], loss: 0.003185, mae: 0.057349, mean_q: -0.335205
 12400/100000: episode: 124, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -19.355, mean reward: -0.194 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.151, 10.126], loss: 0.003551, mae: 0.060286, mean_q: -0.305501
 12500/100000: episode: 125, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.737, mean reward: -0.167 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.894, 10.276], loss: 0.003262, mae: 0.058571, mean_q: -0.316720
 12600/100000: episode: 126, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -20.420, mean reward: -0.204 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.730, 10.098], loss: 0.003610, mae: 0.060326, mean_q: -0.312322
 12700/100000: episode: 127, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -14.216, mean reward: -0.142 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.219, 10.098], loss: 0.003623, mae: 0.062289, mean_q: -0.298346
 12800/100000: episode: 128, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -10.607, mean reward: -0.106 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.601, 10.098], loss: 0.004239, mae: 0.065233, mean_q: -0.300028
 12900/100000: episode: 129, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.537, mean reward: -0.175 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.675, 10.098], loss: 0.003330, mae: 0.060495, mean_q: -0.331196
 13000/100000: episode: 130, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -17.388, mean reward: -0.174 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.064, 10.153], loss: 0.003616, mae: 0.061156, mean_q: -0.320217
 13100/100000: episode: 131, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.390, mean reward: -0.184 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.814, 10.257], loss: 0.004316, mae: 0.065751, mean_q: -0.343353
 13200/100000: episode: 132, duration: 0.466s, episode steps: 100, steps per second: 215, episode reward: -18.493, mean reward: -0.185 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.871, 10.098], loss: 0.003156, mae: 0.056521, mean_q: -0.322302
 13300/100000: episode: 133, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -12.016, mean reward: -0.120 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.774, 10.098], loss: 0.003849, mae: 0.061124, mean_q: -0.323881
 13400/100000: episode: 134, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -17.229, mean reward: -0.172 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.880, 10.243], loss: 0.003762, mae: 0.063770, mean_q: -0.283643
 13500/100000: episode: 135, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -20.973, mean reward: -0.210 [-1.000, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.854, 10.098], loss: 0.003200, mae: 0.058011, mean_q: -0.286754
 13600/100000: episode: 136, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -16.483, mean reward: -0.165 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.131, 10.338], loss: 0.002760, mae: 0.054029, mean_q: -0.305063
 13700/100000: episode: 137, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -17.709, mean reward: -0.177 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.688, 10.240], loss: 0.003127, mae: 0.058169, mean_q: -0.287670
 13800/100000: episode: 138, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.268, mean reward: -0.183 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.747, 10.204], loss: 0.003554, mae: 0.058882, mean_q: -0.330407
 13900/100000: episode: 139, duration: 0.464s, episode steps: 100, steps per second: 216, episode reward: -19.837, mean reward: -0.198 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.158, 10.145], loss: 0.003413, mae: 0.059391, mean_q: -0.290690
 14000/100000: episode: 140, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -13.617, mean reward: -0.136 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.837, 10.098], loss: 0.002802, mae: 0.055436, mean_q: -0.314810
 14100/100000: episode: 141, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -19.180, mean reward: -0.192 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.445, 10.100], loss: 0.002691, mae: 0.054251, mean_q: -0.312134
 14200/100000: episode: 142, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -15.307, mean reward: -0.153 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.196, 10.244], loss: 0.002773, mae: 0.053933, mean_q: -0.341546
 14300/100000: episode: 143, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -15.241, mean reward: -0.152 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.541, 10.098], loss: 0.002863, mae: 0.054674, mean_q: -0.334912
 14400/100000: episode: 144, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -19.946, mean reward: -0.199 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.727, 10.098], loss: 0.002845, mae: 0.054691, mean_q: -0.288812
 14500/100000: episode: 145, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -15.490, mean reward: -0.155 [-1.000, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.625, 10.145], loss: 0.005019, mae: 0.071278, mean_q: -0.304582
 14600/100000: episode: 146, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -19.679, mean reward: -0.197 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.339, 10.119], loss: 0.003201, mae: 0.060444, mean_q: -0.326138
 14700/100000: episode: 147, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -18.158, mean reward: -0.182 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.496, 10.248], loss: 0.003351, mae: 0.058516, mean_q: -0.329425
 14800/100000: episode: 148, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -18.208, mean reward: -0.182 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.539, 10.098], loss: 0.002771, mae: 0.053357, mean_q: -0.343126
 14900/100000: episode: 149, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.510, mean reward: -0.175 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.056, 10.314], loss: 0.003035, mae: 0.057034, mean_q: -0.334349
 15000/100000: episode: 150, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -14.176, mean reward: -0.142 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.592, 10.357], loss: 0.003006, mae: 0.056476, mean_q: -0.293444
 15100/100000: episode: 151, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -19.188, mean reward: -0.192 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.570, 10.192], loss: 0.003413, mae: 0.059956, mean_q: -0.335313
 15200/100000: episode: 152, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -17.151, mean reward: -0.172 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.193, 10.098], loss: 0.003397, mae: 0.058768, mean_q: -0.304634
 15300/100000: episode: 153, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -18.073, mean reward: -0.181 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.098, 10.189], loss: 0.003356, mae: 0.059044, mean_q: -0.361152
 15400/100000: episode: 154, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -17.408, mean reward: -0.174 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.370, 10.098], loss: 0.002755, mae: 0.054792, mean_q: -0.308020
 15500/100000: episode: 155, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -19.084, mean reward: -0.191 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.748, 10.109], loss: 0.002871, mae: 0.056014, mean_q: -0.310107
 15600/100000: episode: 156, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.804, mean reward: -0.178 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.884, 10.239], loss: 0.002948, mae: 0.056320, mean_q: -0.335995
 15700/100000: episode: 157, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.596, mean reward: -0.176 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.510, 10.120], loss: 0.003002, mae: 0.057570, mean_q: -0.320300
 15800/100000: episode: 158, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -18.640, mean reward: -0.186 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.496, 10.098], loss: 0.002884, mae: 0.057937, mean_q: -0.349344
 15900/100000: episode: 159, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -20.081, mean reward: -0.201 [-1.000, 0.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.785, 10.332], loss: 0.002730, mae: 0.053578, mean_q: -0.339294
 16000/100000: episode: 160, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -15.969, mean reward: -0.160 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.800, 10.098], loss: 0.003072, mae: 0.056701, mean_q: -0.341318
 16100/100000: episode: 161, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -20.062, mean reward: -0.201 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.801, 10.285], loss: 0.002865, mae: 0.055319, mean_q: -0.300877
 16200/100000: episode: 162, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -14.438, mean reward: -0.144 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.108, 10.405], loss: 0.003535, mae: 0.058622, mean_q: -0.320478
 16300/100000: episode: 163, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -18.758, mean reward: -0.188 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.873, 10.230], loss: 0.003110, mae: 0.057893, mean_q: -0.332502
 16400/100000: episode: 164, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -15.865, mean reward: -0.159 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.738, 10.098], loss: 0.002813, mae: 0.053176, mean_q: -0.352969
 16500/100000: episode: 165, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -14.199, mean reward: -0.142 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.745, 10.449], loss: 0.002982, mae: 0.056555, mean_q: -0.345557
 16600/100000: episode: 166, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -21.723, mean reward: -0.217 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.530, 10.098], loss: 0.002675, mae: 0.053681, mean_q: -0.269675
 16700/100000: episode: 167, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -20.532, mean reward: -0.205 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.419, 10.186], loss: 0.002782, mae: 0.054352, mean_q: -0.301883
 16800/100000: episode: 168, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -18.090, mean reward: -0.181 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.836, 10.098], loss: 0.003189, mae: 0.058266, mean_q: -0.347402
 16900/100000: episode: 169, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -19.270, mean reward: -0.193 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.797, 10.098], loss: 0.003310, mae: 0.060014, mean_q: -0.375859
 17000/100000: episode: 170, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -15.713, mean reward: -0.157 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.364, 10.311], loss: 0.002884, mae: 0.055816, mean_q: -0.320342
 17100/100000: episode: 171, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -11.436, mean reward: -0.114 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.678, 10.116], loss: 0.002714, mae: 0.052695, mean_q: -0.330771
 17200/100000: episode: 172, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -16.794, mean reward: -0.168 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.248, 10.172], loss: 0.002773, mae: 0.054126, mean_q: -0.310977
 17300/100000: episode: 173, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -17.427, mean reward: -0.174 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.275, 10.302], loss: 0.002705, mae: 0.053605, mean_q: -0.340785
 17400/100000: episode: 174, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.730, mean reward: -0.177 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.332, 10.128], loss: 0.002662, mae: 0.053288, mean_q: -0.338885
 17500/100000: episode: 175, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -16.851, mean reward: -0.169 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.903, 10.173], loss: 0.002733, mae: 0.053691, mean_q: -0.296691
 17600/100000: episode: 176, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -13.951, mean reward: -0.140 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.484, 10.442], loss: 0.002631, mae: 0.052512, mean_q: -0.320840
 17700/100000: episode: 177, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -14.149, mean reward: -0.141 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.954, 10.395], loss: 0.002901, mae: 0.056189, mean_q: -0.313891
 17800/100000: episode: 178, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -14.648, mean reward: -0.146 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.591, 10.098], loss: 0.002904, mae: 0.056487, mean_q: -0.294669
 17900/100000: episode: 179, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -19.583, mean reward: -0.196 [-1.000, 0.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.716, 10.098], loss: 0.002577, mae: 0.051916, mean_q: -0.318240
 18000/100000: episode: 180, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -19.887, mean reward: -0.199 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.512, 10.098], loss: 0.002774, mae: 0.054074, mean_q: -0.297572
 18100/100000: episode: 181, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -20.644, mean reward: -0.206 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.857, 10.136], loss: 0.002951, mae: 0.055579, mean_q: -0.321417
 18200/100000: episode: 182, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -18.528, mean reward: -0.185 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.982, 10.111], loss: 0.002703, mae: 0.053058, mean_q: -0.305431
 18300/100000: episode: 183, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.797, mean reward: -0.168 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.579, 10.098], loss: 0.002690, mae: 0.053412, mean_q: -0.308648
 18400/100000: episode: 184, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -13.168, mean reward: -0.132 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.017, 10.098], loss: 0.002926, mae: 0.057306, mean_q: -0.331837
 18500/100000: episode: 185, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -18.383, mean reward: -0.184 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.743, 10.100], loss: 0.003071, mae: 0.057744, mean_q: -0.345451
 18600/100000: episode: 186, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -18.379, mean reward: -0.184 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.906, 10.307], loss: 0.002449, mae: 0.051460, mean_q: -0.319548
[Info] FALSIFICATION!
 18658/100000: episode: 187, duration: 0.284s, episode steps: 58, steps per second: 204, episode reward: -12.091, mean reward: -0.208 [-1.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.045 [-0.608, 6.566], loss: 0.002329, mae: 0.049522, mean_q: -0.379791
 18758/100000: episode: 188, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -19.282, mean reward: -0.193 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.879, 10.098], loss: 0.002348, mae: 0.048796, mean_q: -0.329834
 18858/100000: episode: 189, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.497, mean reward: -0.165 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.607, 10.431], loss: 0.002545, mae: 0.050767, mean_q: -0.340860
 18958/100000: episode: 190, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -15.741, mean reward: -0.157 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.570, 10.098], loss: 0.017190, mae: 0.064337, mean_q: -0.354734
 19058/100000: episode: 191, duration: 0.468s, episode steps: 100, steps per second: 214, episode reward: -14.777, mean reward: -0.148 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.370, 10.098], loss: 0.016133, mae: 0.057208, mean_q: -0.322872
 19158/100000: episode: 192, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.991, mean reward: -0.180 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.705, 10.098], loss: 0.030433, mae: 0.072338, mean_q: -0.327184
 19258/100000: episode: 193, duration: 0.463s, episode steps: 100, steps per second: 216, episode reward: -19.906, mean reward: -0.199 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.652, 10.154], loss: 0.003674, mae: 0.064418, mean_q: -0.366607
 19358/100000: episode: 194, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -18.382, mean reward: -0.184 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.189, 10.098], loss: 0.017194, mae: 0.072332, mean_q: -0.343344
 19458/100000: episode: 195, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -18.755, mean reward: -0.188 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.306, 10.098], loss: 0.002612, mae: 0.053066, mean_q: -0.299490
 19558/100000: episode: 196, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -11.945, mean reward: -0.119 [-1.000, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.785, 10.301], loss: 0.016187, mae: 0.061355, mean_q: -0.341907
 19658/100000: episode: 197, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -19.271, mean reward: -0.193 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.153, 10.102], loss: 0.027787, mae: 0.060816, mean_q: -0.318485
 19758/100000: episode: 198, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -20.197, mean reward: -0.202 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.664, 10.191], loss: 0.016369, mae: 0.067507, mean_q: -0.331587
 19858/100000: episode: 199, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -16.158, mean reward: -0.162 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.913, 10.098], loss: 0.028706, mae: 0.074606, mean_q: -0.318743
 19958/100000: episode: 200, duration: 0.465s, episode steps: 100, steps per second: 215, episode reward: -17.082, mean reward: -0.171 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.954, 10.196], loss: 0.003134, mae: 0.058528, mean_q: -0.317649
 20058/100000: episode: 201, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -18.585, mean reward: -0.186 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.346, 10.098], loss: 0.002643, mae: 0.051894, mean_q: -0.330234
 20158/100000: episode: 202, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -17.926, mean reward: -0.179 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.944, 10.098], loss: 0.002683, mae: 0.052766, mean_q: -0.315840
 20258/100000: episode: 203, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -19.415, mean reward: -0.194 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.508, 10.159], loss: 0.002562, mae: 0.050983, mean_q: -0.355185
 20358/100000: episode: 204, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -14.941, mean reward: -0.149 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.844, 10.249], loss: 0.002927, mae: 0.054807, mean_q: -0.328590
 20458/100000: episode: 205, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -18.637, mean reward: -0.186 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.046, 10.098], loss: 0.002540, mae: 0.050515, mean_q: -0.366599
 20558/100000: episode: 206, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -13.578, mean reward: -0.136 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.278, 10.211], loss: 0.016245, mae: 0.066019, mean_q: -0.319992
 20658/100000: episode: 207, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -16.198, mean reward: -0.162 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.209, 10.195], loss: 0.029741, mae: 0.083370, mean_q: -0.349961
 20758/100000: episode: 208, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -16.335, mean reward: -0.163 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.606, 10.280], loss: 0.002688, mae: 0.052336, mean_q: -0.335248
 20858/100000: episode: 209, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -9.122, mean reward: -0.091 [-1.000, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.512, 10.385], loss: 0.015764, mae: 0.063054, mean_q: -0.330787
[Info] FALSIFICATION!
 20947/100000: episode: 210, duration: 0.429s, episode steps: 89, steps per second: 207, episode reward: -9.405, mean reward: -0.106 [-1.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.336 [-0.804, 10.395], loss: 0.017279, mae: 0.066488, mean_q: -0.309610
 21047/100000: episode: 211, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -18.887, mean reward: -0.189 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.704, 10.228], loss: 0.002783, mae: 0.052128, mean_q: -0.318978
 21147/100000: episode: 212, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.812, mean reward: -0.178 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.371, 10.181], loss: 0.015644, mae: 0.064104, mean_q: -0.308463
 21247/100000: episode: 213, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -14.278, mean reward: -0.143 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.021, 10.321], loss: 0.003058, mae: 0.055395, mean_q: -0.351147
 21347/100000: episode: 214, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -15.064, mean reward: -0.151 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.966, 10.244], loss: 0.002867, mae: 0.053670, mean_q: -0.315803
 21447/100000: episode: 215, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -17.455, mean reward: -0.175 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.711, 10.164], loss: 0.030672, mae: 0.072889, mean_q: -0.311461
 21547/100000: episode: 216, duration: 0.466s, episode steps: 100, steps per second: 215, episode reward: -17.781, mean reward: -0.178 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.450, 10.148], loss: 0.017918, mae: 0.064292, mean_q: -0.330752
 21647/100000: episode: 217, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -15.636, mean reward: -0.156 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.960, 10.393], loss: 0.041596, mae: 0.077048, mean_q: -0.330752
 21747/100000: episode: 218, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -10.836, mean reward: -0.108 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.384, 10.098], loss: 0.044473, mae: 0.080173, mean_q: -0.312742
 21847/100000: episode: 219, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -16.979, mean reward: -0.170 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.504, 10.098], loss: 0.017689, mae: 0.060712, mean_q: -0.308702
 21947/100000: episode: 220, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -18.964, mean reward: -0.190 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.567, 10.098], loss: 0.015128, mae: 0.062816, mean_q: -0.311902
 22047/100000: episode: 221, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -20.098, mean reward: -0.201 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.334, 10.155], loss: 0.040431, mae: 0.073691, mean_q: -0.313975
 22147/100000: episode: 222, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -18.007, mean reward: -0.180 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.875, 10.098], loss: 0.060617, mae: 0.096737, mean_q: -0.283370
 22247/100000: episode: 223, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -18.503, mean reward: -0.185 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.331, 10.189], loss: 0.045123, mae: 0.099061, mean_q: -0.328379
 22347/100000: episode: 224, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -18.861, mean reward: -0.189 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.975, 10.238], loss: 0.014185, mae: 0.061192, mean_q: -0.319415
 22447/100000: episode: 225, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -12.655, mean reward: -0.127 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.261, 10.247], loss: 0.003206, mae: 0.056736, mean_q: -0.324125
 22547/100000: episode: 226, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -15.524, mean reward: -0.155 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.527, 10.328], loss: 0.027663, mae: 0.075479, mean_q: -0.342582
 22647/100000: episode: 227, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -13.450, mean reward: -0.135 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.772, 10.098], loss: 0.014353, mae: 0.062545, mean_q: -0.317919
 22747/100000: episode: 228, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -20.545, mean reward: -0.205 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.322, 10.100], loss: 0.003146, mae: 0.056504, mean_q: -0.320078
 22847/100000: episode: 229, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -15.656, mean reward: -0.157 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.448, 10.098], loss: 0.002828, mae: 0.054370, mean_q: -0.320936
 22947/100000: episode: 230, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.676, mean reward: -0.177 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.352, 10.098], loss: 0.045296, mae: 0.081066, mean_q: -0.330847
 23047/100000: episode: 231, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -20.726, mean reward: -0.207 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.781, 10.151], loss: 0.018464, mae: 0.067410, mean_q: -0.306591
 23147/100000: episode: 232, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -19.166, mean reward: -0.192 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.266, 10.135], loss: 0.003067, mae: 0.056068, mean_q: -0.299119
 23247/100000: episode: 233, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -17.214, mean reward: -0.172 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.603, 10.101], loss: 0.072554, mae: 0.084798, mean_q: -0.298223
 23347/100000: episode: 234, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -12.130, mean reward: -0.121 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.036, 10.280], loss: 0.043072, mae: 0.074205, mean_q: -0.331702
 23447/100000: episode: 235, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -19.724, mean reward: -0.197 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.409, 10.146], loss: 0.003238, mae: 0.055972, mean_q: -0.326967
 23547/100000: episode: 236, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -19.306, mean reward: -0.193 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.679, 10.098], loss: 0.003383, mae: 0.058150, mean_q: -0.291571
 23647/100000: episode: 237, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.116, mean reward: -0.161 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.971, 10.355], loss: 0.046752, mae: 0.071879, mean_q: -0.348486
 23747/100000: episode: 238, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.124, mean reward: -0.161 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.112, 10.098], loss: 0.003015, mae: 0.056515, mean_q: -0.301963
 23847/100000: episode: 239, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -18.167, mean reward: -0.182 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.252, 10.109], loss: 0.017351, mae: 0.059144, mean_q: -0.304072
 23947/100000: episode: 240, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.266, mean reward: -0.173 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.664, 10.098], loss: 0.002925, mae: 0.054835, mean_q: -0.320030
 24047/100000: episode: 241, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -15.864, mean reward: -0.159 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.630, 10.098], loss: 0.002932, mae: 0.054866, mean_q: -0.341415
 24147/100000: episode: 242, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -19.140, mean reward: -0.191 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.561, 10.162], loss: 0.017288, mae: 0.059116, mean_q: -0.315720
 24247/100000: episode: 243, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -10.291, mean reward: -0.103 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.708, 10.216], loss: 0.018219, mae: 0.066343, mean_q: -0.304111
 24347/100000: episode: 244, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -14.426, mean reward: -0.144 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.497, 10.098], loss: 0.017590, mae: 0.063645, mean_q: -0.315996
 24447/100000: episode: 245, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -14.745, mean reward: -0.147 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.371, 10.098], loss: 0.018327, mae: 0.069645, mean_q: -0.278326
 24547/100000: episode: 246, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -19.596, mean reward: -0.196 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.343, 10.098], loss: 0.017445, mae: 0.059584, mean_q: -0.298345
 24647/100000: episode: 247, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -14.390, mean reward: -0.144 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.683, 10.239], loss: 0.003338, mae: 0.058236, mean_q: -0.304096
 24747/100000: episode: 248, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -9.431, mean reward: -0.094 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.099, 10.098], loss: 0.006518, mae: 0.080505, mean_q: -0.284730
 24847/100000: episode: 249, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -19.664, mean reward: -0.197 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.799, 10.098], loss: 0.004316, mae: 0.064633, mean_q: -0.305999
 24947/100000: episode: 250, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.792, mean reward: -0.178 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.416, 10.160], loss: 0.018711, mae: 0.068280, mean_q: -0.351790
 25047/100000: episode: 251, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -14.232, mean reward: -0.142 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.504, 10.182], loss: 0.018695, mae: 0.069116, mean_q: -0.299262
 25147/100000: episode: 252, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -17.926, mean reward: -0.179 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.112, 10.098], loss: 0.004531, mae: 0.067442, mean_q: -0.345521
 25247/100000: episode: 253, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -15.544, mean reward: -0.155 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.930, 10.316], loss: 0.032475, mae: 0.070608, mean_q: -0.299849
 25347/100000: episode: 254, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -17.041, mean reward: -0.170 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.007, 10.098], loss: 0.003595, mae: 0.060616, mean_q: -0.302402
 25447/100000: episode: 255, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -18.242, mean reward: -0.182 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.843, 10.195], loss: 0.003525, mae: 0.061027, mean_q: -0.293255
 25547/100000: episode: 256, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -15.830, mean reward: -0.158 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.949, 10.158], loss: 0.003579, mae: 0.061286, mean_q: -0.295774
 25647/100000: episode: 257, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.173, mean reward: -0.162 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.278, 10.220], loss: 0.003179, mae: 0.058262, mean_q: -0.312920
 25747/100000: episode: 258, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -17.708, mean reward: -0.177 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.128, 10.098], loss: 0.003394, mae: 0.059679, mean_q: -0.274545
 25847/100000: episode: 259, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -17.926, mean reward: -0.179 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.676, 10.098], loss: 0.003528, mae: 0.060963, mean_q: -0.379854
 25947/100000: episode: 260, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -17.656, mean reward: -0.177 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.707, 10.334], loss: 0.003494, mae: 0.061158, mean_q: -0.303950
 26047/100000: episode: 261, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -18.787, mean reward: -0.188 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.346, 10.098], loss: 0.003070, mae: 0.056221, mean_q: -0.329727
 26147/100000: episode: 262, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -16.746, mean reward: -0.167 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.253, 10.127], loss: 0.003238, mae: 0.058721, mean_q: -0.291159
 26247/100000: episode: 263, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -18.650, mean reward: -0.187 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.954, 10.283], loss: 0.003581, mae: 0.061876, mean_q: -0.297939
 26347/100000: episode: 264, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -17.509, mean reward: -0.175 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.758, 10.099], loss: 0.003188, mae: 0.058791, mean_q: -0.350554
 26447/100000: episode: 265, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -17.648, mean reward: -0.176 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.142, 10.098], loss: 0.003121, mae: 0.056209, mean_q: -0.325335
 26547/100000: episode: 266, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -20.299, mean reward: -0.203 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.175, 10.148], loss: 0.003089, mae: 0.058126, mean_q: -0.335031
 26647/100000: episode: 267, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -16.897, mean reward: -0.169 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.219, 10.098], loss: 0.003553, mae: 0.062226, mean_q: -0.349897
 26747/100000: episode: 268, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -11.514, mean reward: -0.115 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.295, 10.098], loss: 0.003530, mae: 0.061329, mean_q: -0.291922
 26847/100000: episode: 269, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -18.126, mean reward: -0.181 [-1.000, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.183, 10.109], loss: 0.003268, mae: 0.059444, mean_q: -0.275741
 26947/100000: episode: 270, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.870, mean reward: -0.179 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.816, 10.248], loss: 0.003301, mae: 0.058631, mean_q: -0.317673
 27047/100000: episode: 271, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -11.916, mean reward: -0.119 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.807, 10.314], loss: 0.003673, mae: 0.063014, mean_q: -0.343314
 27147/100000: episode: 272, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.568, mean reward: -0.166 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.194, 10.421], loss: 0.003213, mae: 0.059339, mean_q: -0.321791
 27247/100000: episode: 273, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -18.342, mean reward: -0.183 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.323, 10.178], loss: 0.002933, mae: 0.056542, mean_q: -0.327058
 27347/100000: episode: 274, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -14.550, mean reward: -0.145 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.832, 10.098], loss: 0.003217, mae: 0.059043, mean_q: -0.324185
 27447/100000: episode: 275, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -19.123, mean reward: -0.191 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.683, 10.129], loss: 0.005142, mae: 0.072150, mean_q: -0.322263
 27547/100000: episode: 276, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -15.935, mean reward: -0.159 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.708, 10.381], loss: 0.003069, mae: 0.058864, mean_q: -0.333354
 27647/100000: episode: 277, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -19.223, mean reward: -0.192 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.641, 10.311], loss: 0.003273, mae: 0.059066, mean_q: -0.346845
 27747/100000: episode: 278, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -16.489, mean reward: -0.165 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.733, 10.098], loss: 0.003094, mae: 0.057813, mean_q: -0.320373
 27847/100000: episode: 279, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.066, mean reward: -0.161 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.794, 10.098], loss: 0.003088, mae: 0.057950, mean_q: -0.317819
 27947/100000: episode: 280, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -15.321, mean reward: -0.153 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.417, 10.098], loss: 0.003265, mae: 0.059740, mean_q: -0.320923
 28047/100000: episode: 281, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.362, mean reward: -0.184 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.541, 10.098], loss: 0.002971, mae: 0.057046, mean_q: -0.308230
 28147/100000: episode: 282, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.124, mean reward: -0.171 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.361, 10.437], loss: 0.003575, mae: 0.061707, mean_q: -0.302117
 28247/100000: episode: 283, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -17.438, mean reward: -0.174 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.405, 10.098], loss: 0.003091, mae: 0.058585, mean_q: -0.332796
 28347/100000: episode: 284, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -18.680, mean reward: -0.187 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.243, 10.138], loss: 0.003007, mae: 0.056977, mean_q: -0.322392
 28447/100000: episode: 285, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -17.176, mean reward: -0.172 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.114, 10.188], loss: 0.003318, mae: 0.059140, mean_q: -0.328615
 28547/100000: episode: 286, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -14.734, mean reward: -0.147 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.883, 10.452], loss: 0.003100, mae: 0.056439, mean_q: -0.348615
 28647/100000: episode: 287, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -14.116, mean reward: -0.141 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.470, 10.310], loss: 0.003505, mae: 0.061553, mean_q: -0.320275
 28747/100000: episode: 288, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -14.813, mean reward: -0.148 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.854, 10.195], loss: 0.003068, mae: 0.056432, mean_q: -0.297189
 28847/100000: episode: 289, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -15.905, mean reward: -0.159 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.707, 10.307], loss: 0.002813, mae: 0.055461, mean_q: -0.290907
 28947/100000: episode: 290, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -16.914, mean reward: -0.169 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.076, 10.182], loss: 0.003185, mae: 0.058839, mean_q: -0.304688
 29047/100000: episode: 291, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -13.580, mean reward: -0.136 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.651, 10.098], loss: 0.003261, mae: 0.059997, mean_q: -0.330671
 29147/100000: episode: 292, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -14.675, mean reward: -0.147 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.761, 10.098], loss: 0.002812, mae: 0.054893, mean_q: -0.351851
 29247/100000: episode: 293, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -11.008, mean reward: -0.110 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.789, 10.098], loss: 0.002774, mae: 0.053454, mean_q: -0.310471
 29347/100000: episode: 294, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -19.414, mean reward: -0.194 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.419, 10.219], loss: 0.002937, mae: 0.056577, mean_q: -0.318989
 29447/100000: episode: 295, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -17.868, mean reward: -0.179 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.347, 10.098], loss: 0.002764, mae: 0.054434, mean_q: -0.330853
 29547/100000: episode: 296, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -16.245, mean reward: -0.162 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.435, 10.098], loss: 0.002755, mae: 0.054434, mean_q: -0.327912
 29647/100000: episode: 297, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -16.308, mean reward: -0.163 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.018, 10.098], loss: 0.002661, mae: 0.052982, mean_q: -0.292791
 29747/100000: episode: 298, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -8.323, mean reward: -0.083 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.961, 10.477], loss: 0.002949, mae: 0.055991, mean_q: -0.316527
 29847/100000: episode: 299, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -9.298, mean reward: -0.093 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.496, 10.510], loss: 0.002936, mae: 0.055093, mean_q: -0.284506
 29947/100000: episode: 300, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -16.052, mean reward: -0.161 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.250, 10.098], loss: 0.002802, mae: 0.054201, mean_q: -0.332732
 30047/100000: episode: 301, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -16.912, mean reward: -0.169 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.814, 10.098], loss: 0.002935, mae: 0.056329, mean_q: -0.324296
 30147/100000: episode: 302, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.242, mean reward: -0.182 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.230, 10.145], loss: 0.003611, mae: 0.064534, mean_q: -0.309156
 30247/100000: episode: 303, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -18.720, mean reward: -0.187 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.470, 10.099], loss: 0.003147, mae: 0.060076, mean_q: -0.297571
 30347/100000: episode: 304, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -14.639, mean reward: -0.146 [-1.000, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.673, 10.183], loss: 0.003580, mae: 0.063698, mean_q: -0.311914
 30447/100000: episode: 305, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -15.271, mean reward: -0.153 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.815, 10.098], loss: 0.004249, mae: 0.066764, mean_q: -0.314666
 30547/100000: episode: 306, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -14.410, mean reward: -0.144 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.922, 10.098], loss: 0.003269, mae: 0.059640, mean_q: -0.302785
 30647/100000: episode: 307, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -15.941, mean reward: -0.159 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.934, 10.185], loss: 0.003013, mae: 0.057091, mean_q: -0.288108
 30747/100000: episode: 308, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -19.143, mean reward: -0.191 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.935, 10.098], loss: 0.003000, mae: 0.055883, mean_q: -0.295331
 30847/100000: episode: 309, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -12.660, mean reward: -0.127 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.281, 10.098], loss: 0.003145, mae: 0.057464, mean_q: -0.311288
 30947/100000: episode: 310, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -18.780, mean reward: -0.188 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.359, 10.098], loss: 0.003703, mae: 0.061729, mean_q: -0.291563
 31047/100000: episode: 311, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -16.610, mean reward: -0.166 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.637, 10.228], loss: 0.003328, mae: 0.060348, mean_q: -0.312497
 31147/100000: episode: 312, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.962, mean reward: -0.170 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.006, 10.098], loss: 0.002950, mae: 0.056397, mean_q: -0.300688
 31247/100000: episode: 313, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -18.721, mean reward: -0.187 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.889, 10.321], loss: 0.003261, mae: 0.059067, mean_q: -0.304849
 31347/100000: episode: 314, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -19.080, mean reward: -0.191 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.841, 10.098], loss: 0.003518, mae: 0.061157, mean_q: -0.275448
 31447/100000: episode: 315, duration: 0.466s, episode steps: 100, steps per second: 215, episode reward: -12.465, mean reward: -0.125 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.556, 10.499], loss: 0.003415, mae: 0.059258, mean_q: -0.322473
 31547/100000: episode: 316, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.674, mean reward: -0.167 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.467, 10.098], loss: 0.003000, mae: 0.056531, mean_q: -0.335784
 31647/100000: episode: 317, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -19.566, mean reward: -0.196 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.287, 10.104], loss: 0.003105, mae: 0.057784, mean_q: -0.290202
 31747/100000: episode: 318, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -20.009, mean reward: -0.200 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.352, 10.127], loss: 0.003359, mae: 0.059259, mean_q: -0.306196
 31847/100000: episode: 319, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -14.884, mean reward: -0.149 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.612, 10.259], loss: 0.003037, mae: 0.055090, mean_q: -0.298304
 31947/100000: episode: 320, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -17.357, mean reward: -0.174 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.550, 10.098], loss: 0.003343, mae: 0.059282, mean_q: -0.330143
 32047/100000: episode: 321, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -17.479, mean reward: -0.175 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.670, 10.237], loss: 0.002978, mae: 0.056907, mean_q: -0.291227
 32147/100000: episode: 322, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -14.545, mean reward: -0.145 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.327, 10.344], loss: 0.005099, mae: 0.065627, mean_q: -0.318419
 32247/100000: episode: 323, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.608, mean reward: -0.186 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.859, 10.098], loss: 0.003502, mae: 0.060403, mean_q: -0.335759
 32347/100000: episode: 324, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -14.222, mean reward: -0.142 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.895, 10.280], loss: 0.003162, mae: 0.057393, mean_q: -0.323560
 32447/100000: episode: 325, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.938, mean reward: -0.189 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.103, 10.098], loss: 0.003242, mae: 0.057719, mean_q: -0.295234
 32547/100000: episode: 326, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -15.169, mean reward: -0.152 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.626, 10.098], loss: 0.006369, mae: 0.074644, mean_q: -0.304248
 32647/100000: episode: 327, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -21.116, mean reward: -0.211 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.059, 10.143], loss: 0.003503, mae: 0.058071, mean_q: -0.300181
 32747/100000: episode: 328, duration: 0.473s, episode steps: 100, steps per second: 212, episode reward: -19.221, mean reward: -0.192 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.967, 10.214], loss: 0.003326, mae: 0.059476, mean_q: -0.296701
 32847/100000: episode: 329, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -16.799, mean reward: -0.168 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.367, 10.309], loss: 0.003001, mae: 0.056113, mean_q: -0.317301
 32947/100000: episode: 330, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -13.051, mean reward: -0.131 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.846, 10.098], loss: 0.002717, mae: 0.052971, mean_q: -0.309555
 33047/100000: episode: 331, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -14.142, mean reward: -0.141 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.665, 10.475], loss: 0.002864, mae: 0.054973, mean_q: -0.333012
 33147/100000: episode: 332, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.528, mean reward: -0.175 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.732, 10.098], loss: 0.002809, mae: 0.053752, mean_q: -0.322070
 33247/100000: episode: 333, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -14.992, mean reward: -0.150 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.846, 10.399], loss: 0.002799, mae: 0.054882, mean_q: -0.326324
 33347/100000: episode: 334, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -16.940, mean reward: -0.169 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.700, 10.098], loss: 0.002704, mae: 0.053066, mean_q: -0.319856
 33447/100000: episode: 335, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -12.264, mean reward: -0.123 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.125, 10.420], loss: 0.003030, mae: 0.056014, mean_q: -0.332069
 33547/100000: episode: 336, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -18.485, mean reward: -0.185 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.408, 10.119], loss: 0.003714, mae: 0.061747, mean_q: -0.270362
 33647/100000: episode: 337, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -15.737, mean reward: -0.157 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.377, 10.098], loss: 0.002688, mae: 0.052476, mean_q: -0.308803
 33747/100000: episode: 338, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -15.196, mean reward: -0.152 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.674, 10.243], loss: 0.002908, mae: 0.055159, mean_q: -0.304495
 33847/100000: episode: 339, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -18.922, mean reward: -0.189 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.018, 10.098], loss: 0.002912, mae: 0.053963, mean_q: -0.333090
 33947/100000: episode: 340, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.860, mean reward: -0.169 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.477, 10.202], loss: 0.003307, mae: 0.059344, mean_q: -0.336618
 34047/100000: episode: 341, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -18.386, mean reward: -0.184 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.862, 10.299], loss: 0.002963, mae: 0.056368, mean_q: -0.310036
 34147/100000: episode: 342, duration: 0.466s, episode steps: 100, steps per second: 214, episode reward: -13.330, mean reward: -0.133 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.844, 10.256], loss: 0.002786, mae: 0.053573, mean_q: -0.316535
 34247/100000: episode: 343, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -17.417, mean reward: -0.174 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.977, 10.109], loss: 0.003239, mae: 0.057709, mean_q: -0.331560
 34347/100000: episode: 344, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -19.189, mean reward: -0.192 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.619, 10.226], loss: 0.002764, mae: 0.053373, mean_q: -0.323697
 34447/100000: episode: 345, duration: 0.466s, episode steps: 100, steps per second: 215, episode reward: -18.296, mean reward: -0.183 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.220, 10.098], loss: 0.003071, mae: 0.056981, mean_q: -0.339507
 34547/100000: episode: 346, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -18.342, mean reward: -0.183 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.232, 10.098], loss: 0.002960, mae: 0.055995, mean_q: -0.291474
 34647/100000: episode: 347, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -11.098, mean reward: -0.111 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.751, 10.098], loss: 0.002694, mae: 0.052803, mean_q: -0.318158
 34747/100000: episode: 348, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -14.390, mean reward: -0.144 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.647, 10.199], loss: 0.002917, mae: 0.055354, mean_q: -0.303923
 34847/100000: episode: 349, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.912, mean reward: -0.179 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.453, 10.159], loss: 0.002991, mae: 0.057069, mean_q: -0.304009
 34947/100000: episode: 350, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -17.698, mean reward: -0.177 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.986, 10.245], loss: 0.003035, mae: 0.057545, mean_q: -0.306890
 35047/100000: episode: 351, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -17.242, mean reward: -0.172 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.415, 10.098], loss: 0.002702, mae: 0.053373, mean_q: -0.322591
 35147/100000: episode: 352, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.784, mean reward: -0.178 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.058, 10.140], loss: 0.003850, mae: 0.058416, mean_q: -0.298430
 35247/100000: episode: 353, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -16.690, mean reward: -0.167 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.828, 10.138], loss: 0.003151, mae: 0.055544, mean_q: -0.359535
 35347/100000: episode: 354, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -17.535, mean reward: -0.175 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.692, 10.262], loss: 0.002555, mae: 0.051810, mean_q: -0.332745
 35447/100000: episode: 355, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -11.154, mean reward: -0.112 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.212, 10.150], loss: 0.002605, mae: 0.052320, mean_q: -0.326472
 35547/100000: episode: 356, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -18.931, mean reward: -0.189 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.927, 10.098], loss: 0.002620, mae: 0.053008, mean_q: -0.310352
 35647/100000: episode: 357, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -15.755, mean reward: -0.158 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.496, 10.265], loss: 0.002623, mae: 0.051999, mean_q: -0.314810
 35747/100000: episode: 358, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -18.514, mean reward: -0.185 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.230, 10.098], loss: 0.002418, mae: 0.050059, mean_q: -0.316896
 35847/100000: episode: 359, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -16.470, mean reward: -0.165 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.712, 10.196], loss: 0.002689, mae: 0.052994, mean_q: -0.282619
 35947/100000: episode: 360, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -17.232, mean reward: -0.172 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.270, 10.209], loss: 0.002520, mae: 0.051606, mean_q: -0.312890
 36047/100000: episode: 361, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.234, mean reward: -0.162 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.674, 10.262], loss: 0.002548, mae: 0.051821, mean_q: -0.307481
 36147/100000: episode: 362, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -15.456, mean reward: -0.155 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.281, 10.098], loss: 0.002582, mae: 0.051930, mean_q: -0.346242
 36247/100000: episode: 363, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -13.341, mean reward: -0.133 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.731, 10.267], loss: 0.002435, mae: 0.049545, mean_q: -0.352983
 36347/100000: episode: 364, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -19.665, mean reward: -0.197 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.914, 10.098], loss: 0.003143, mae: 0.058016, mean_q: -0.288427
 36447/100000: episode: 365, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -16.553, mean reward: -0.166 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.832, 10.435], loss: 0.002621, mae: 0.053757, mean_q: -0.339181
 36547/100000: episode: 366, duration: 0.468s, episode steps: 100, steps per second: 214, episode reward: -15.268, mean reward: -0.153 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.446, 10.114], loss: 0.002458, mae: 0.050042, mean_q: -0.326449
 36647/100000: episode: 367, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -17.545, mean reward: -0.175 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.700, 10.098], loss: 0.002655, mae: 0.052771, mean_q: -0.298697
 36747/100000: episode: 368, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.396, mean reward: -0.174 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.723, 10.230], loss: 0.002514, mae: 0.051697, mean_q: -0.296577
 36847/100000: episode: 369, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -13.625, mean reward: -0.136 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.590, 10.377], loss: 0.004117, mae: 0.063422, mean_q: -0.301597
 36947/100000: episode: 370, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -15.037, mean reward: -0.150 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.547, 10.098], loss: 0.002417, mae: 0.050768, mean_q: -0.315880
 37047/100000: episode: 371, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -13.940, mean reward: -0.139 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.755, 10.098], loss: 0.002466, mae: 0.051588, mean_q: -0.313428
 37147/100000: episode: 372, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -16.466, mean reward: -0.165 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.208, 10.146], loss: 0.002340, mae: 0.050353, mean_q: -0.290655
 37247/100000: episode: 373, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -19.864, mean reward: -0.199 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.682, 10.098], loss: 0.002394, mae: 0.051493, mean_q: -0.304030
 37347/100000: episode: 374, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -18.277, mean reward: -0.183 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.538, 10.201], loss: 0.002509, mae: 0.051685, mean_q: -0.301460
 37447/100000: episode: 375, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.609, mean reward: -0.166 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.620, 10.293], loss: 0.002612, mae: 0.052422, mean_q: -0.301402
 37547/100000: episode: 376, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -16.430, mean reward: -0.164 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.168, 10.098], loss: 0.002456, mae: 0.051484, mean_q: -0.300115
 37647/100000: episode: 377, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -14.097, mean reward: -0.141 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.292, 10.511], loss: 0.002440, mae: 0.051400, mean_q: -0.297856
 37747/100000: episode: 378, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.046, mean reward: -0.150 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.340, 10.179], loss: 0.002325, mae: 0.050174, mean_q: -0.307356
 37847/100000: episode: 379, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -17.730, mean reward: -0.177 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.057, 10.098], loss: 0.002385, mae: 0.050439, mean_q: -0.307383
 37947/100000: episode: 380, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -14.761, mean reward: -0.148 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.010, 10.345], loss: 0.002479, mae: 0.052170, mean_q: -0.316192
 38047/100000: episode: 381, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.236, mean reward: -0.172 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.667, 10.098], loss: 0.002291, mae: 0.048971, mean_q: -0.321331
 38147/100000: episode: 382, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -15.993, mean reward: -0.160 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.485, 10.117], loss: 0.002621, mae: 0.053471, mean_q: -0.312957
 38247/100000: episode: 383, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -14.759, mean reward: -0.148 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.060, 10.330], loss: 0.002321, mae: 0.049580, mean_q: -0.311224
 38347/100000: episode: 384, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -19.409, mean reward: -0.194 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.617, 10.098], loss: 0.002385, mae: 0.050922, mean_q: -0.325125
 38447/100000: episode: 385, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -18.994, mean reward: -0.190 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.626, 10.098], loss: 0.002373, mae: 0.050887, mean_q: -0.297754
 38547/100000: episode: 386, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -15.004, mean reward: -0.150 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.102, 10.098], loss: 0.002419, mae: 0.051138, mean_q: -0.342991
 38647/100000: episode: 387, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.087, mean reward: -0.191 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.614, 10.148], loss: 0.002701, mae: 0.053422, mean_q: -0.277542
 38747/100000: episode: 388, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.311, mean reward: -0.173 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.004, 10.098], loss: 0.004456, mae: 0.062882, mean_q: -0.309259
 38847/100000: episode: 389, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -16.857, mean reward: -0.169 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.866, 10.098], loss: 0.003674, mae: 0.059027, mean_q: -0.307555
 38947/100000: episode: 390, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.479, mean reward: -0.175 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.592, 10.098], loss: 0.002452, mae: 0.051336, mean_q: -0.277059
 39047/100000: episode: 391, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -19.878, mean reward: -0.199 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.730, 10.120], loss: 0.002659, mae: 0.052216, mean_q: -0.301529
 39147/100000: episode: 392, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -15.486, mean reward: -0.155 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.855, 10.098], loss: 0.002427, mae: 0.050706, mean_q: -0.312531
 39247/100000: episode: 393, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -12.270, mean reward: -0.123 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.225, 10.164], loss: 0.002344, mae: 0.049464, mean_q: -0.310900
 39347/100000: episode: 394, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -18.274, mean reward: -0.183 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.674, 10.098], loss: 0.002435, mae: 0.050231, mean_q: -0.294001
 39447/100000: episode: 395, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -12.438, mean reward: -0.124 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.205, 10.098], loss: 0.002450, mae: 0.050736, mean_q: -0.319144
 39547/100000: episode: 396, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -12.780, mean reward: -0.128 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.282, 10.249], loss: 0.002553, mae: 0.052114, mean_q: -0.302463
 39647/100000: episode: 397, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -16.736, mean reward: -0.167 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.507, 10.098], loss: 0.002561, mae: 0.050777, mean_q: -0.335929
 39747/100000: episode: 398, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -14.624, mean reward: -0.146 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.383, 10.098], loss: 0.002786, mae: 0.054439, mean_q: -0.317819
 39847/100000: episode: 399, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -17.028, mean reward: -0.170 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.998, 10.125], loss: 0.002783, mae: 0.054315, mean_q: -0.299124
 39947/100000: episode: 400, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.982, mean reward: -0.160 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.743, 10.098], loss: 0.002392, mae: 0.050334, mean_q: -0.344568
 40047/100000: episode: 401, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -17.772, mean reward: -0.178 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.288, 10.125], loss: 0.002517, mae: 0.051882, mean_q: -0.277370
 40147/100000: episode: 402, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -18.468, mean reward: -0.185 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.515, 10.098], loss: 0.002402, mae: 0.050035, mean_q: -0.319934
 40247/100000: episode: 403, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -11.387, mean reward: -0.114 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.651, 10.460], loss: 0.002538, mae: 0.050833, mean_q: -0.307266
 40347/100000: episode: 404, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -15.018, mean reward: -0.150 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.653, 10.098], loss: 0.003111, mae: 0.057946, mean_q: -0.303094
 40447/100000: episode: 405, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -16.717, mean reward: -0.167 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.129, 10.186], loss: 0.002315, mae: 0.049696, mean_q: -0.294048
 40547/100000: episode: 406, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.789, mean reward: -0.178 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.841, 10.143], loss: 0.002399, mae: 0.049707, mean_q: -0.301527
 40647/100000: episode: 407, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -16.640, mean reward: -0.166 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.042, 10.183], loss: 0.002533, mae: 0.051661, mean_q: -0.260797
 40747/100000: episode: 408, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.952, mean reward: -0.180 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.468, 10.098], loss: 0.002566, mae: 0.052084, mean_q: -0.290857
 40847/100000: episode: 409, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -15.818, mean reward: -0.158 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.582, 10.216], loss: 0.002744, mae: 0.054686, mean_q: -0.283035
 40947/100000: episode: 410, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.668, mean reward: -0.167 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.925, 10.262], loss: 0.002543, mae: 0.051074, mean_q: -0.324462
 41047/100000: episode: 411, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -12.165, mean reward: -0.122 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.189, 10.098], loss: 0.003169, mae: 0.058640, mean_q: -0.296139
 41147/100000: episode: 412, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -14.403, mean reward: -0.144 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.615, 10.098], loss: 0.002740, mae: 0.053990, mean_q: -0.282676
 41247/100000: episode: 413, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -16.753, mean reward: -0.168 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.431, 10.098], loss: 0.002543, mae: 0.050844, mean_q: -0.316883
 41347/100000: episode: 414, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -18.335, mean reward: -0.183 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.019, 10.098], loss: 0.004927, mae: 0.065062, mean_q: -0.294957
 41447/100000: episode: 415, duration: 0.465s, episode steps: 100, steps per second: 215, episode reward: -18.834, mean reward: -0.188 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.488, 10.098], loss: 0.003833, mae: 0.059941, mean_q: -0.344209
 41547/100000: episode: 416, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -12.872, mean reward: -0.129 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.753, 10.208], loss: 0.003137, mae: 0.056999, mean_q: -0.323128
 41647/100000: episode: 417, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -20.137, mean reward: -0.201 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.414, 10.203], loss: 0.003374, mae: 0.059995, mean_q: -0.293963
 41747/100000: episode: 418, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.724, mean reward: -0.187 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.390, 10.098], loss: 0.002661, mae: 0.053327, mean_q: -0.337632
 41847/100000: episode: 419, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -13.193, mean reward: -0.132 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.056, 10.098], loss: 0.002400, mae: 0.049433, mean_q: -0.329315
 41947/100000: episode: 420, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -17.380, mean reward: -0.174 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.493, 10.287], loss: 0.002667, mae: 0.052520, mean_q: -0.301370
 42047/100000: episode: 421, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -14.165, mean reward: -0.142 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.196, 10.098], loss: 0.002678, mae: 0.052063, mean_q: -0.316546
 42147/100000: episode: 422, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.356, mean reward: -0.184 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.258, 10.124], loss: 0.002653, mae: 0.052504, mean_q: -0.317953
 42247/100000: episode: 423, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -18.630, mean reward: -0.186 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.872, 10.154], loss: 0.002866, mae: 0.054633, mean_q: -0.298172
 42347/100000: episode: 424, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.050, mean reward: -0.171 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.488, 10.098], loss: 0.002571, mae: 0.051139, mean_q: -0.342088
 42447/100000: episode: 425, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -17.103, mean reward: -0.171 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.026, 10.167], loss: 0.002546, mae: 0.051696, mean_q: -0.286864
 42547/100000: episode: 426, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -15.563, mean reward: -0.156 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.430, 10.098], loss: 0.002739, mae: 0.053680, mean_q: -0.281369
 42647/100000: episode: 427, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.738, mean reward: -0.177 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.463, 10.098], loss: 0.002728, mae: 0.053992, mean_q: -0.307570
 42747/100000: episode: 428, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -16.236, mean reward: -0.162 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.997, 10.248], loss: 0.002607, mae: 0.052024, mean_q: -0.284960
 42847/100000: episode: 429, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -17.267, mean reward: -0.173 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.949, 10.168], loss: 0.002602, mae: 0.052473, mean_q: -0.296426
 42947/100000: episode: 430, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.575, mean reward: -0.176 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.324, 10.184], loss: 0.002545, mae: 0.051682, mean_q: -0.305887
 43047/100000: episode: 431, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -19.830, mean reward: -0.198 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.187, 10.098], loss: 0.002451, mae: 0.050703, mean_q: -0.301105
 43147/100000: episode: 432, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.484, mean reward: -0.165 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.642, 10.209], loss: 0.002586, mae: 0.052082, mean_q: -0.297753
 43247/100000: episode: 433, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -13.375, mean reward: -0.134 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.293, 10.443], loss: 0.002469, mae: 0.050449, mean_q: -0.318891
 43347/100000: episode: 434, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: -17.607, mean reward: -0.176 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.260, 10.098], loss: 0.002511, mae: 0.051522, mean_q: -0.299267
 43447/100000: episode: 435, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -11.238, mean reward: -0.112 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.774, 10.098], loss: 0.002819, mae: 0.055143, mean_q: -0.291735
 43547/100000: episode: 436, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -16.693, mean reward: -0.167 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.874, 10.098], loss: 0.003488, mae: 0.059798, mean_q: -0.305415
 43647/100000: episode: 437, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.665, mean reward: -0.167 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.762, 10.144], loss: 0.002727, mae: 0.054206, mean_q: -0.319881
 43747/100000: episode: 438, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.769, mean reward: -0.178 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.041, 10.156], loss: 0.002648, mae: 0.054175, mean_q: -0.304089
 43847/100000: episode: 439, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -17.555, mean reward: -0.176 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.962, 10.132], loss: 0.002669, mae: 0.053325, mean_q: -0.320501
 43947/100000: episode: 440, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.433, mean reward: -0.164 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.421, 10.098], loss: 0.002656, mae: 0.052834, mean_q: -0.300232
 44047/100000: episode: 441, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -16.407, mean reward: -0.164 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.439, 10.098], loss: 0.002555, mae: 0.051346, mean_q: -0.324137
 44147/100000: episode: 442, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.050, mean reward: -0.170 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.554, 10.098], loss: 0.002312, mae: 0.048695, mean_q: -0.308963
 44247/100000: episode: 443, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -18.339, mean reward: -0.183 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.187, 10.147], loss: 0.002375, mae: 0.048942, mean_q: -0.296965
 44347/100000: episode: 444, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -16.982, mean reward: -0.170 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.322, 10.313], loss: 0.002342, mae: 0.049979, mean_q: -0.300222
 44447/100000: episode: 445, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.019, mean reward: -0.160 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.067, 10.263], loss: 0.002456, mae: 0.050076, mean_q: -0.334616
 44547/100000: episode: 446, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -19.814, mean reward: -0.198 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.546, 10.121], loss: 0.002435, mae: 0.049034, mean_q: -0.317728
 44647/100000: episode: 447, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -17.678, mean reward: -0.177 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.060, 10.098], loss: 0.003133, mae: 0.058636, mean_q: -0.296284
 44747/100000: episode: 448, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.452, mean reward: -0.175 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.595, 10.140], loss: 0.002540, mae: 0.051229, mean_q: -0.294182
 44847/100000: episode: 449, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -17.486, mean reward: -0.175 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.024, 10.287], loss: 0.002539, mae: 0.051585, mean_q: -0.324653
 44947/100000: episode: 450, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.655, mean reward: -0.177 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.842, 10.098], loss: 0.002599, mae: 0.051567, mean_q: -0.336839
 45047/100000: episode: 451, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -19.574, mean reward: -0.196 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.535, 10.098], loss: 0.002466, mae: 0.049559, mean_q: -0.319999
 45147/100000: episode: 452, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -17.016, mean reward: -0.170 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.541, 10.098], loss: 0.002540, mae: 0.050459, mean_q: -0.321566
 45247/100000: episode: 453, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -12.788, mean reward: -0.128 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.670, 10.098], loss: 0.002558, mae: 0.051223, mean_q: -0.307105
 45347/100000: episode: 454, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -18.928, mean reward: -0.189 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.449, 10.098], loss: 0.002765, mae: 0.053511, mean_q: -0.296027
 45447/100000: episode: 455, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -19.485, mean reward: -0.195 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.725, 10.159], loss: 0.003194, mae: 0.059624, mean_q: -0.285821
 45547/100000: episode: 456, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -17.459, mean reward: -0.175 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.516, 10.370], loss: 0.002630, mae: 0.053436, mean_q: -0.309909
 45647/100000: episode: 457, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -9.061, mean reward: -0.091 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.830, 10.098], loss: 0.002739, mae: 0.052725, mean_q: -0.342007
 45747/100000: episode: 458, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -17.960, mean reward: -0.180 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.889, 10.098], loss: 0.002621, mae: 0.051264, mean_q: -0.301867
 45847/100000: episode: 459, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -14.459, mean reward: -0.145 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.207, 10.373], loss: 0.002469, mae: 0.049268, mean_q: -0.343161
 45947/100000: episode: 460, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -14.020, mean reward: -0.140 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.553, 10.147], loss: 0.002641, mae: 0.051025, mean_q: -0.326455
 46047/100000: episode: 461, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -15.219, mean reward: -0.152 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.504, 10.352], loss: 0.002682, mae: 0.052016, mean_q: -0.327110
 46147/100000: episode: 462, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -18.947, mean reward: -0.189 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.597, 10.098], loss: 0.002712, mae: 0.052664, mean_q: -0.309811
 46247/100000: episode: 463, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.940, mean reward: -0.179 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.462, 10.098], loss: 0.002585, mae: 0.051047, mean_q: -0.307752
 46347/100000: episode: 464, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.956, mean reward: -0.180 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.732, 10.098], loss: 0.002753, mae: 0.053810, mean_q: -0.296078
 46447/100000: episode: 465, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -18.833, mean reward: -0.188 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.384, 10.098], loss: 0.002712, mae: 0.053965, mean_q: -0.303460
 46547/100000: episode: 466, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -19.982, mean reward: -0.200 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.329, 10.264], loss: 0.002813, mae: 0.053181, mean_q: -0.286869
 46647/100000: episode: 467, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -11.758, mean reward: -0.118 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.448, 10.146], loss: 0.002654, mae: 0.051988, mean_q: -0.308079
 46747/100000: episode: 468, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: -18.084, mean reward: -0.181 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.902, 10.181], loss: 0.003195, mae: 0.058170, mean_q: -0.282160
 46847/100000: episode: 469, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.001, mean reward: -0.180 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.736, 10.184], loss: 0.002648, mae: 0.051718, mean_q: -0.329233
 46947/100000: episode: 470, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -17.680, mean reward: -0.177 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.264, 10.098], loss: 0.002570, mae: 0.050507, mean_q: -0.346108
 47047/100000: episode: 471, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -19.114, mean reward: -0.191 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.594, 10.098], loss: 0.002585, mae: 0.052064, mean_q: -0.323278
 47147/100000: episode: 472, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -19.555, mean reward: -0.196 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.802, 10.325], loss: 0.003006, mae: 0.057057, mean_q: -0.314447
 47247/100000: episode: 473, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.740, mean reward: -0.177 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.983, 10.436], loss: 0.002884, mae: 0.053211, mean_q: -0.328157
 47347/100000: episode: 474, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -16.626, mean reward: -0.166 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.106, 10.114], loss: 0.002624, mae: 0.051367, mean_q: -0.333103
 47447/100000: episode: 475, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.702, mean reward: -0.187 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.300, 10.210], loss: 0.002562, mae: 0.050702, mean_q: -0.325750
 47547/100000: episode: 476, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -16.811, mean reward: -0.168 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.175, 10.098], loss: 0.002734, mae: 0.052198, mean_q: -0.315876
 47647/100000: episode: 477, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -18.220, mean reward: -0.182 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.061, 10.098], loss: 0.003061, mae: 0.057374, mean_q: -0.305800
 47747/100000: episode: 478, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -16.163, mean reward: -0.162 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.286, 10.322], loss: 0.003733, mae: 0.059806, mean_q: -0.328574
 47847/100000: episode: 479, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -17.189, mean reward: -0.172 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.943, 10.098], loss: 0.002677, mae: 0.051803, mean_q: -0.317198
 47947/100000: episode: 480, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -20.055, mean reward: -0.201 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.155, 10.180], loss: 0.002953, mae: 0.053729, mean_q: -0.335853
 48047/100000: episode: 481, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.003, mean reward: -0.170 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.199, 10.171], loss: 0.002943, mae: 0.054189, mean_q: -0.316429
 48147/100000: episode: 482, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -14.171, mean reward: -0.142 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.339, 10.098], loss: 0.002917, mae: 0.053886, mean_q: -0.316338
 48247/100000: episode: 483, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -15.529, mean reward: -0.155 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.639, 10.299], loss: 0.002880, mae: 0.053621, mean_q: -0.303375
 48347/100000: episode: 484, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -19.103, mean reward: -0.191 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.332, 10.098], loss: 0.002726, mae: 0.052632, mean_q: -0.298996
 48447/100000: episode: 485, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -17.132, mean reward: -0.171 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.626, 10.098], loss: 0.002676, mae: 0.050586, mean_q: -0.340654
 48547/100000: episode: 486, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -15.727, mean reward: -0.157 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.081, 10.216], loss: 0.002749, mae: 0.051862, mean_q: -0.312295
 48647/100000: episode: 487, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -19.038, mean reward: -0.190 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.254, 10.098], loss: 0.002850, mae: 0.054222, mean_q: -0.319249
 48747/100000: episode: 488, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -15.028, mean reward: -0.150 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.538, 10.250], loss: 0.002606, mae: 0.051292, mean_q: -0.338114
 48847/100000: episode: 489, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.608, mean reward: -0.176 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.317, 10.098], loss: 0.003566, mae: 0.058139, mean_q: -0.326892
 48947/100000: episode: 490, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -20.358, mean reward: -0.204 [-1.000, 0.243], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.352, 10.098], loss: 0.002874, mae: 0.054485, mean_q: -0.346840
 49047/100000: episode: 491, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -19.974, mean reward: -0.200 [-1.000, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.604, 10.098], loss: 0.004327, mae: 0.067854, mean_q: -0.313974
 49147/100000: episode: 492, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -18.402, mean reward: -0.184 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.139, 10.098], loss: 0.002690, mae: 0.052186, mean_q: -0.334356
 49247/100000: episode: 493, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -12.695, mean reward: -0.127 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.826, 10.356], loss: 0.002574, mae: 0.050181, mean_q: -0.296140
 49347/100000: episode: 494, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.658, mean reward: -0.167 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.028, 10.230], loss: 0.002661, mae: 0.051364, mean_q: -0.308168
 49447/100000: episode: 495, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.544, mean reward: -0.185 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.629, 10.215], loss: 0.002642, mae: 0.050311, mean_q: -0.322610
 49547/100000: episode: 496, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -16.797, mean reward: -0.168 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.212, 10.302], loss: 0.002675, mae: 0.051200, mean_q: -0.326006
 49647/100000: episode: 497, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -12.147, mean reward: -0.121 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.803, 10.441], loss: 0.002592, mae: 0.050077, mean_q: -0.365227
 49747/100000: episode: 498, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -15.849, mean reward: -0.158 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.089, 10.098], loss: 0.002594, mae: 0.050418, mean_q: -0.330294
 49847/100000: episode: 499, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.060, mean reward: -0.171 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.047, 10.098], loss: 0.002595, mae: 0.049981, mean_q: -0.345441
 49947/100000: episode: 500, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -18.764, mean reward: -0.188 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.748, 10.127], loss: 0.002738, mae: 0.052056, mean_q: -0.320454
 50047/100000: episode: 501, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.666, mean reward: -0.177 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.249, 10.098], loss: 0.002453, mae: 0.048944, mean_q: -0.345824
 50147/100000: episode: 502, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -16.179, mean reward: -0.162 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.912, 10.147], loss: 0.002717, mae: 0.052352, mean_q: -0.338082
 50247/100000: episode: 503, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -18.309, mean reward: -0.183 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.324, 10.250], loss: 0.002845, mae: 0.053725, mean_q: -0.305142
 50347/100000: episode: 504, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -15.714, mean reward: -0.157 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.630, 10.229], loss: 0.002610, mae: 0.051288, mean_q: -0.315049
 50447/100000: episode: 505, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -12.492, mean reward: -0.125 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.735, 10.098], loss: 0.002638, mae: 0.050536, mean_q: -0.321433
 50547/100000: episode: 506, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.699, mean reward: -0.167 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.772, 10.098], loss: 0.002588, mae: 0.050869, mean_q: -0.330532
 50647/100000: episode: 507, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -14.654, mean reward: -0.147 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.668, 10.334], loss: 0.002426, mae: 0.049490, mean_q: -0.310405
 50747/100000: episode: 508, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -18.107, mean reward: -0.181 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.590, 10.103], loss: 0.002600, mae: 0.051119, mean_q: -0.275123
 50847/100000: episode: 509, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.478, mean reward: -0.175 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.750, 10.098], loss: 0.002489, mae: 0.049423, mean_q: -0.319986
 50947/100000: episode: 510, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.098, mean reward: -0.181 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.331, 10.098], loss: 0.002808, mae: 0.053446, mean_q: -0.352753
 51047/100000: episode: 511, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -18.288, mean reward: -0.183 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.612, 10.108], loss: 0.004955, mae: 0.068256, mean_q: -0.317406
 51147/100000: episode: 512, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -16.594, mean reward: -0.166 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.095, 10.216], loss: 0.002673, mae: 0.051535, mean_q: -0.316213
 51247/100000: episode: 513, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.235, mean reward: -0.182 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.400, 10.199], loss: 0.002397, mae: 0.049182, mean_q: -0.316234
 51347/100000: episode: 514, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.582, mean reward: -0.176 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.353, 10.290], loss: 0.002492, mae: 0.049920, mean_q: -0.307646
 51447/100000: episode: 515, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -17.515, mean reward: -0.175 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.339, 10.209], loss: 0.002480, mae: 0.049247, mean_q: -0.319909
 51547/100000: episode: 516, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.213, mean reward: -0.182 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.815, 10.212], loss: 0.002472, mae: 0.049033, mean_q: -0.336029
 51647/100000: episode: 517, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -18.610, mean reward: -0.186 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.263, 10.116], loss: 0.002413, mae: 0.049116, mean_q: -0.282749
 51747/100000: episode: 518, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -16.710, mean reward: -0.167 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.433, 10.098], loss: 0.002305, mae: 0.047786, mean_q: -0.310919
 51847/100000: episode: 519, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -18.851, mean reward: -0.189 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.983, 10.125], loss: 0.002372, mae: 0.048461, mean_q: -0.323415
 51947/100000: episode: 520, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.905, mean reward: -0.189 [-1.000, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.013, 10.121], loss: 0.002190, mae: 0.046693, mean_q: -0.347825
 52047/100000: episode: 521, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -15.477, mean reward: -0.155 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.476, 10.244], loss: 0.002504, mae: 0.050336, mean_q: -0.366255
 52147/100000: episode: 522, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -12.599, mean reward: -0.126 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.794, 10.098], loss: 0.002483, mae: 0.050296, mean_q: -0.304407
 52247/100000: episode: 523, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.964, mean reward: -0.180 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.247, 10.229], loss: 0.002393, mae: 0.048138, mean_q: -0.312100
 52347/100000: episode: 524, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -20.259, mean reward: -0.203 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.223, 10.206], loss: 0.002477, mae: 0.049239, mean_q: -0.323818
 52447/100000: episode: 525, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -15.624, mean reward: -0.156 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.601, 10.184], loss: 0.002323, mae: 0.048131, mean_q: -0.326845
 52547/100000: episode: 526, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -20.340, mean reward: -0.203 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.992, 10.098], loss: 0.002339, mae: 0.048249, mean_q: -0.304604
 52647/100000: episode: 527, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -15.527, mean reward: -0.155 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.033, 10.248], loss: 0.002322, mae: 0.047248, mean_q: -0.327677
 52747/100000: episode: 528, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -14.572, mean reward: -0.146 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.105, 10.216], loss: 0.002295, mae: 0.046515, mean_q: -0.353110
 52847/100000: episode: 529, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.602, mean reward: -0.186 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.068, 10.098], loss: 0.002419, mae: 0.048972, mean_q: -0.317409
 52947/100000: episode: 530, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -19.045, mean reward: -0.190 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.562, 10.242], loss: 0.002328, mae: 0.048484, mean_q: -0.307244
 53047/100000: episode: 531, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -15.209, mean reward: -0.152 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.016, 10.177], loss: 0.002487, mae: 0.049951, mean_q: -0.302431
 53147/100000: episode: 532, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -18.416, mean reward: -0.184 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.095, 10.203], loss: 0.002366, mae: 0.048531, mean_q: -0.283886
 53247/100000: episode: 533, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -18.516, mean reward: -0.185 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.582, 10.196], loss: 0.002431, mae: 0.049194, mean_q: -0.293929
 53347/100000: episode: 534, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.059, mean reward: -0.171 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.369, 10.269], loss: 0.002479, mae: 0.050548, mean_q: -0.363864
 53447/100000: episode: 535, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.717, mean reward: -0.187 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.293, 10.312], loss: 0.002704, mae: 0.053173, mean_q: -0.303013
 53547/100000: episode: 536, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -15.379, mean reward: -0.154 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.349, 10.116], loss: 0.004803, mae: 0.064241, mean_q: -0.324998
 53647/100000: episode: 537, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -14.781, mean reward: -0.148 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.212, 10.098], loss: 0.002634, mae: 0.052535, mean_q: -0.294583
 53747/100000: episode: 538, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: -18.290, mean reward: -0.183 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.295, 10.098], loss: 0.002503, mae: 0.049913, mean_q: -0.337496
 53847/100000: episode: 539, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -20.178, mean reward: -0.202 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.070, 10.098], loss: 0.002518, mae: 0.049914, mean_q: -0.309643
 53947/100000: episode: 540, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -8.475, mean reward: -0.085 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.827, 10.418], loss: 0.002456, mae: 0.048997, mean_q: -0.331497
 54047/100000: episode: 541, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.628, mean reward: -0.176 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.669, 10.127], loss: 0.002396, mae: 0.048604, mean_q: -0.332597
 54147/100000: episode: 542, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -19.738, mean reward: -0.197 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.432, 10.098], loss: 0.002459, mae: 0.049192, mean_q: -0.307933
 54247/100000: episode: 543, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -16.925, mean reward: -0.169 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.558, 10.098], loss: 0.002575, mae: 0.050240, mean_q: -0.277729
 54347/100000: episode: 544, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -17.910, mean reward: -0.179 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.064, 10.265], loss: 0.002261, mae: 0.047137, mean_q: -0.320663
 54447/100000: episode: 545, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -16.778, mean reward: -0.168 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.589, 10.207], loss: 0.002364, mae: 0.047595, mean_q: -0.348717
 54547/100000: episode: 546, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -7.388, mean reward: -0.074 [-1.000, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.960, 10.187], loss: 0.002535, mae: 0.049839, mean_q: -0.299609
 54647/100000: episode: 547, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -14.365, mean reward: -0.144 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.534, 10.098], loss: 0.002310, mae: 0.047931, mean_q: -0.324741
 54747/100000: episode: 548, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -16.479, mean reward: -0.165 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.186, 10.145], loss: 0.002507, mae: 0.049330, mean_q: -0.314947
 54847/100000: episode: 549, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.679, mean reward: -0.187 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.776, 10.201], loss: 0.002668, mae: 0.051702, mean_q: -0.319981
 54947/100000: episode: 550, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -14.934, mean reward: -0.149 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.459, 10.098], loss: 0.002587, mae: 0.050478, mean_q: -0.289012
 55047/100000: episode: 551, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -13.976, mean reward: -0.140 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.821, 10.412], loss: 0.002352, mae: 0.048734, mean_q: -0.350527
 55147/100000: episode: 552, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -19.547, mean reward: -0.195 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.653, 10.140], loss: 0.002400, mae: 0.048896, mean_q: -0.339992
 55247/100000: episode: 553, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -14.291, mean reward: -0.143 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.817, 10.098], loss: 0.002511, mae: 0.050124, mean_q: -0.284979
 55347/100000: episode: 554, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -17.434, mean reward: -0.174 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.463, 10.128], loss: 0.002519, mae: 0.049229, mean_q: -0.309782
 55447/100000: episode: 555, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -17.438, mean reward: -0.174 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.823, 10.098], loss: 0.002288, mae: 0.046255, mean_q: -0.359568
 55547/100000: episode: 556, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -19.131, mean reward: -0.191 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.264, 10.098], loss: 0.002477, mae: 0.048719, mean_q: -0.328389
 55647/100000: episode: 557, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -18.324, mean reward: -0.183 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.911, 10.232], loss: 0.002380, mae: 0.048989, mean_q: -0.300966
 55747/100000: episode: 558, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -12.912, mean reward: -0.129 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.653, 10.157], loss: 0.002453, mae: 0.049159, mean_q: -0.299221
 55847/100000: episode: 559, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -16.209, mean reward: -0.162 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.082, 10.101], loss: 0.002391, mae: 0.047715, mean_q: -0.323844
 55947/100000: episode: 560, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -14.138, mean reward: -0.141 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.954, 10.424], loss: 0.002491, mae: 0.048888, mean_q: -0.312209
 56047/100000: episode: 561, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -18.531, mean reward: -0.185 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.960, 10.167], loss: 0.002583, mae: 0.049996, mean_q: -0.295989
 56147/100000: episode: 562, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -15.578, mean reward: -0.156 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.384, 10.098], loss: 0.002523, mae: 0.050256, mean_q: -0.269409
 56247/100000: episode: 563, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -9.549, mean reward: -0.095 [-1.000, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.704, 10.347], loss: 0.002528, mae: 0.050815, mean_q: -0.284183
 56347/100000: episode: 564, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -19.666, mean reward: -0.197 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.289, 10.253], loss: 0.002500, mae: 0.049974, mean_q: -0.320499
 56447/100000: episode: 565, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -12.374, mean reward: -0.124 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.305, 10.098], loss: 0.002503, mae: 0.050211, mean_q: -0.286372
 56547/100000: episode: 566, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.577, mean reward: -0.176 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.200, 10.098], loss: 0.002500, mae: 0.049661, mean_q: -0.347944
 56647/100000: episode: 567, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -13.232, mean reward: -0.132 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.891, 10.332], loss: 0.002558, mae: 0.049596, mean_q: -0.275911
 56747/100000: episode: 568, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -19.198, mean reward: -0.192 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.964, 10.147], loss: 0.002350, mae: 0.047338, mean_q: -0.327481
 56847/100000: episode: 569, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -15.061, mean reward: -0.151 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.290, 10.098], loss: 0.003058, mae: 0.055757, mean_q: -0.292867
 56947/100000: episode: 570, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -8.999, mean reward: -0.090 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.505, 10.162], loss: 0.002535, mae: 0.050565, mean_q: -0.279212
 57047/100000: episode: 571, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -13.214, mean reward: -0.132 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.831, 10.266], loss: 0.002720, mae: 0.052712, mean_q: -0.292329
 57147/100000: episode: 572, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.937, mean reward: -0.179 [-1.000, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.462, 10.222], loss: 0.002520, mae: 0.050126, mean_q: -0.297588
 57247/100000: episode: 573, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -13.379, mean reward: -0.134 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.484, 10.252], loss: 0.002203, mae: 0.045768, mean_q: -0.314295
 57347/100000: episode: 574, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -14.506, mean reward: -0.145 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.916, 10.098], loss: 0.002470, mae: 0.048819, mean_q: -0.295791
 57447/100000: episode: 575, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -16.896, mean reward: -0.169 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.245, 10.098], loss: 0.002205, mae: 0.046460, mean_q: -0.301820
 57547/100000: episode: 576, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.755, mean reward: -0.188 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.716, 10.160], loss: 0.002350, mae: 0.047723, mean_q: -0.313241
 57647/100000: episode: 577, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -17.006, mean reward: -0.170 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.604, 10.110], loss: 0.002242, mae: 0.047117, mean_q: -0.290824
 57747/100000: episode: 578, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -14.534, mean reward: -0.145 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.486, 10.098], loss: 0.002556, mae: 0.050930, mean_q: -0.301090
 57847/100000: episode: 579, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -17.888, mean reward: -0.179 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.646, 10.112], loss: 0.002578, mae: 0.048953, mean_q: -0.290256
 57947/100000: episode: 580, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -12.459, mean reward: -0.125 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.004, 10.098], loss: 0.002945, mae: 0.053683, mean_q: -0.304173
 58047/100000: episode: 581, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -17.956, mean reward: -0.180 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.452, 10.098], loss: 0.002723, mae: 0.053066, mean_q: -0.293897
 58147/100000: episode: 582, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.279, mean reward: -0.163 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.814, 10.146], loss: 0.002484, mae: 0.049526, mean_q: -0.298147
 58247/100000: episode: 583, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -19.139, mean reward: -0.191 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.743, 10.098], loss: 0.002540, mae: 0.049202, mean_q: -0.270346
 58347/100000: episode: 584, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -15.992, mean reward: -0.160 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.904, 10.283], loss: 0.002363, mae: 0.048421, mean_q: -0.324607
 58447/100000: episode: 585, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -17.585, mean reward: -0.176 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.279, 10.098], loss: 0.002369, mae: 0.047461, mean_q: -0.301885
 58547/100000: episode: 586, duration: 0.460s, episode steps: 100, steps per second: 217, episode reward: -15.085, mean reward: -0.151 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.378, 10.330], loss: 0.002324, mae: 0.047866, mean_q: -0.312719
 58647/100000: episode: 587, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -18.685, mean reward: -0.187 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.496, 10.098], loss: 0.002394, mae: 0.048162, mean_q: -0.292009
 58747/100000: episode: 588, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -16.520, mean reward: -0.165 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.175, 10.098], loss: 0.002115, mae: 0.044978, mean_q: -0.340760
 58847/100000: episode: 589, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -16.986, mean reward: -0.170 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.549, 10.098], loss: 0.002540, mae: 0.050893, mean_q: -0.284807
 58947/100000: episode: 590, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -16.080, mean reward: -0.161 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.036, 10.130], loss: 0.005535, mae: 0.068681, mean_q: -0.338911
 59047/100000: episode: 591, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -18.547, mean reward: -0.185 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.423, 10.098], loss: 0.002531, mae: 0.051025, mean_q: -0.303670
 59147/100000: episode: 592, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -15.243, mean reward: -0.152 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.935, 10.262], loss: 0.002385, mae: 0.048491, mean_q: -0.274368
 59247/100000: episode: 593, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -12.247, mean reward: -0.122 [-1.000, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.495, 10.299], loss: 0.002241, mae: 0.046974, mean_q: -0.303513
 59347/100000: episode: 594, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -18.479, mean reward: -0.185 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.008, 10.098], loss: 0.002489, mae: 0.049016, mean_q: -0.268532
 59447/100000: episode: 595, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -14.640, mean reward: -0.146 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.547, 10.098], loss: 0.002260, mae: 0.046309, mean_q: -0.312125
 59547/100000: episode: 596, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -18.497, mean reward: -0.185 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.136, 10.152], loss: 0.002433, mae: 0.048094, mean_q: -0.289081
 59647/100000: episode: 597, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.096, mean reward: -0.161 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.082, 10.225], loss: 0.002429, mae: 0.048573, mean_q: -0.322479
 59747/100000: episode: 598, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -17.288, mean reward: -0.173 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.222, 10.370], loss: 0.002366, mae: 0.047803, mean_q: -0.284323
 59847/100000: episode: 599, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -18.502, mean reward: -0.185 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.692, 10.098], loss: 0.002336, mae: 0.047411, mean_q: -0.295269
 59947/100000: episode: 600, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.133, mean reward: -0.181 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.337, 10.098], loss: 0.002211, mae: 0.046475, mean_q: -0.316102
 60047/100000: episode: 601, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.606, mean reward: -0.186 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.430, 10.228], loss: 0.002444, mae: 0.048041, mean_q: -0.309847
 60147/100000: episode: 602, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: -14.641, mean reward: -0.146 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.619, 10.098], loss: 0.002522, mae: 0.049027, mean_q: -0.293601
 60247/100000: episode: 603, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -14.156, mean reward: -0.142 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.030, 10.197], loss: 0.002223, mae: 0.046458, mean_q: -0.311979
 60347/100000: episode: 604, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -17.035, mean reward: -0.170 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.718, 10.098], loss: 0.002340, mae: 0.047842, mean_q: -0.306393
 60447/100000: episode: 605, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -18.491, mean reward: -0.185 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.822, 10.098], loss: 0.002480, mae: 0.048310, mean_q: -0.331528
 60547/100000: episode: 606, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.519, mean reward: -0.175 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.310, 10.192], loss: 0.002360, mae: 0.047667, mean_q: -0.312651
 60647/100000: episode: 607, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -17.198, mean reward: -0.172 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.153, 10.298], loss: 0.002366, mae: 0.047672, mean_q: -0.327278
 60747/100000: episode: 608, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -17.502, mean reward: -0.175 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.098, 10.098], loss: 0.002472, mae: 0.048996, mean_q: -0.291262
 60847/100000: episode: 609, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.663, mean reward: -0.177 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.104, 10.165], loss: 0.002545, mae: 0.050170, mean_q: -0.320486
 60947/100000: episode: 610, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -13.990, mean reward: -0.140 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.158, 10.098], loss: 0.002364, mae: 0.048041, mean_q: -0.318601
 61047/100000: episode: 611, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -17.612, mean reward: -0.176 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.483, 10.128], loss: 0.002410, mae: 0.048535, mean_q: -0.295163
 61147/100000: episode: 612, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.454, mean reward: -0.175 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.397, 10.098], loss: 0.002313, mae: 0.048309, mean_q: -0.266944
 61247/100000: episode: 613, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -17.445, mean reward: -0.174 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.149, 10.205], loss: 0.002371, mae: 0.047560, mean_q: -0.317168
 61347/100000: episode: 614, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.103, mean reward: -0.171 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.332, 10.272], loss: 0.002243, mae: 0.047072, mean_q: -0.302642
 61447/100000: episode: 615, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -17.950, mean reward: -0.179 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.110, 10.098], loss: 0.002471, mae: 0.050437, mean_q: -0.278870
 61547/100000: episode: 616, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -11.628, mean reward: -0.116 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.040, 10.363], loss: 0.002395, mae: 0.049780, mean_q: -0.285725
 61647/100000: episode: 617, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.488, mean reward: -0.165 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.877, 10.098], loss: 0.002384, mae: 0.048556, mean_q: -0.285388
 61747/100000: episode: 618, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -17.262, mean reward: -0.173 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.881, 10.284], loss: 0.002329, mae: 0.048371, mean_q: -0.273968
 61847/100000: episode: 619, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -18.696, mean reward: -0.187 [-1.000, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.600, 10.098], loss: 0.002363, mae: 0.047526, mean_q: -0.307683
 61947/100000: episode: 620, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -18.457, mean reward: -0.185 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.844, 10.136], loss: 0.004438, mae: 0.062436, mean_q: -0.283886
 62047/100000: episode: 621, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -14.052, mean reward: -0.141 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.493, 10.098], loss: 0.002355, mae: 0.047642, mean_q: -0.328009
 62147/100000: episode: 622, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -16.378, mean reward: -0.164 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.441, 10.098], loss: 0.002197, mae: 0.045538, mean_q: -0.302921
 62247/100000: episode: 623, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -19.974, mean reward: -0.200 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.794, 10.163], loss: 0.002427, mae: 0.048771, mean_q: -0.290382
 62347/100000: episode: 624, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -14.948, mean reward: -0.149 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.556, 10.098], loss: 0.002496, mae: 0.049051, mean_q: -0.316952
 62447/100000: episode: 625, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -11.406, mean reward: -0.114 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.930, 10.098], loss: 0.002432, mae: 0.048707, mean_q: -0.325025
 62547/100000: episode: 626, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -15.893, mean reward: -0.159 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.975, 10.098], loss: 0.002306, mae: 0.046914, mean_q: -0.314934
 62647/100000: episode: 627, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -15.576, mean reward: -0.156 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.436, 10.098], loss: 0.002345, mae: 0.047096, mean_q: -0.299052
 62747/100000: episode: 628, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -7.756, mean reward: -0.078 [-1.000, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.824, 10.098], loss: 0.002562, mae: 0.049414, mean_q: -0.314027
 62847/100000: episode: 629, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -18.048, mean reward: -0.180 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.760, 10.136], loss: 0.002404, mae: 0.047447, mean_q: -0.337943
 62947/100000: episode: 630, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.058, mean reward: -0.191 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.822, 10.236], loss: 0.002589, mae: 0.049088, mean_q: -0.309949
 63047/100000: episode: 631, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -20.518, mean reward: -0.205 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.261, 10.205], loss: 0.002245, mae: 0.045125, mean_q: -0.348426
 63147/100000: episode: 632, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -11.885, mean reward: -0.119 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.733, 10.098], loss: 0.002360, mae: 0.046965, mean_q: -0.304010
 63247/100000: episode: 633, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -14.738, mean reward: -0.147 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.356, 10.285], loss: 0.002534, mae: 0.049117, mean_q: -0.301035
 63347/100000: episode: 634, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -15.759, mean reward: -0.158 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.070, 10.098], loss: 0.002437, mae: 0.048192, mean_q: -0.284311
 63447/100000: episode: 635, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -16.478, mean reward: -0.165 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.761, 10.135], loss: 0.002607, mae: 0.049516, mean_q: -0.317422
 63547/100000: episode: 636, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -19.332, mean reward: -0.193 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.003, 10.098], loss: 0.002462, mae: 0.048819, mean_q: -0.314763
 63647/100000: episode: 637, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -19.533, mean reward: -0.195 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.205, 10.098], loss: 0.002435, mae: 0.047203, mean_q: -0.334361
 63747/100000: episode: 638, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -17.946, mean reward: -0.179 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.921, 10.158], loss: 0.002564, mae: 0.050098, mean_q: -0.313654
 63847/100000: episode: 639, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -17.169, mean reward: -0.172 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.709, 10.342], loss: 0.002472, mae: 0.048199, mean_q: -0.311336
 63947/100000: episode: 640, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -16.870, mean reward: -0.169 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.500, 10.098], loss: 0.002603, mae: 0.050256, mean_q: -0.333384
 64047/100000: episode: 641, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -19.801, mean reward: -0.198 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.447, 10.099], loss: 0.002731, mae: 0.051483, mean_q: -0.285650
 64147/100000: episode: 642, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -19.732, mean reward: -0.197 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.523, 10.098], loss: 0.002535, mae: 0.049207, mean_q: -0.322059
 64247/100000: episode: 643, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -13.182, mean reward: -0.132 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.387, 10.098], loss: 0.002578, mae: 0.049975, mean_q: -0.290103
 64347/100000: episode: 644, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -15.960, mean reward: -0.160 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.704, 10.098], loss: 0.002525, mae: 0.049103, mean_q: -0.287654
 64447/100000: episode: 645, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -12.635, mean reward: -0.126 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.372, 10.098], loss: 0.002369, mae: 0.048169, mean_q: -0.320457
 64547/100000: episode: 646, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -17.612, mean reward: -0.176 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.367, 10.226], loss: 0.002314, mae: 0.046357, mean_q: -0.347283
 64647/100000: episode: 647, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -13.398, mean reward: -0.134 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.503, 10.161], loss: 0.002458, mae: 0.049208, mean_q: -0.322349
 64747/100000: episode: 648, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -19.284, mean reward: -0.193 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.268, 10.098], loss: 0.002522, mae: 0.049628, mean_q: -0.284104
 64847/100000: episode: 649, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.111, mean reward: -0.181 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.008, 10.354], loss: 0.002543, mae: 0.050204, mean_q: -0.314761
 64947/100000: episode: 650, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -20.480, mean reward: -0.205 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.914, 10.098], loss: 0.002329, mae: 0.047388, mean_q: -0.332672
 65047/100000: episode: 651, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -17.652, mean reward: -0.177 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.384, 10.119], loss: 0.003440, mae: 0.059929, mean_q: -0.311906
 65147/100000: episode: 652, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -14.869, mean reward: -0.149 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.057, 10.251], loss: 0.002526, mae: 0.051284, mean_q: -0.305180
 65247/100000: episode: 653, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.299, mean reward: -0.163 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.940, 10.224], loss: 0.002565, mae: 0.049417, mean_q: -0.323403
 65347/100000: episode: 654, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -15.953, mean reward: -0.160 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.298, 10.098], loss: 0.002401, mae: 0.048211, mean_q: -0.300824
 65447/100000: episode: 655, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -16.453, mean reward: -0.165 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.936, 10.148], loss: 0.002588, mae: 0.049583, mean_q: -0.295288
 65547/100000: episode: 656, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -16.826, mean reward: -0.168 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.684, 10.267], loss: 0.002489, mae: 0.049001, mean_q: -0.280185
 65647/100000: episode: 657, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -17.272, mean reward: -0.173 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.721, 10.098], loss: 0.002354, mae: 0.047371, mean_q: -0.315910
 65747/100000: episode: 658, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -15.500, mean reward: -0.155 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.650, 10.098], loss: 0.002522, mae: 0.047962, mean_q: -0.332056
 65847/100000: episode: 659, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -16.329, mean reward: -0.163 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.378, 10.098], loss: 0.002651, mae: 0.049507, mean_q: -0.322342
 65947/100000: episode: 660, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -12.808, mean reward: -0.128 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.766, 10.451], loss: 0.002610, mae: 0.049799, mean_q: -0.332212
 66047/100000: episode: 661, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -14.910, mean reward: -0.149 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.492, 10.103], loss: 0.002399, mae: 0.047918, mean_q: -0.301150
 66147/100000: episode: 662, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -18.766, mean reward: -0.188 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.289, 10.108], loss: 0.002405, mae: 0.047577, mean_q: -0.296227
 66247/100000: episode: 663, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -17.305, mean reward: -0.173 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.014, 10.215], loss: 0.002647, mae: 0.050424, mean_q: -0.271509
 66347/100000: episode: 664, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.492, mean reward: -0.165 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.653, 10.200], loss: 0.002627, mae: 0.050864, mean_q: -0.290966
 66447/100000: episode: 665, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.213, mean reward: -0.182 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.320, 10.098], loss: 0.005307, mae: 0.064995, mean_q: -0.310291
 66547/100000: episode: 666, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.159, mean reward: -0.182 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.873, 10.098], loss: 0.002795, mae: 0.052552, mean_q: -0.319759
 66647/100000: episode: 667, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -19.148, mean reward: -0.191 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.526, 10.098], loss: 0.002569, mae: 0.050006, mean_q: -0.323000
 66747/100000: episode: 668, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -14.093, mean reward: -0.141 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.991, 10.098], loss: 0.002397, mae: 0.048018, mean_q: -0.349831
 66847/100000: episode: 669, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -19.534, mean reward: -0.195 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.940, 10.121], loss: 0.002586, mae: 0.049776, mean_q: -0.309316
 66947/100000: episode: 670, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -14.313, mean reward: -0.143 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.161, 10.098], loss: 0.002485, mae: 0.048498, mean_q: -0.313838
 67047/100000: episode: 671, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.151, mean reward: -0.172 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.664, 10.222], loss: 0.002760, mae: 0.050279, mean_q: -0.324986
 67147/100000: episode: 672, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -14.014, mean reward: -0.140 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.069, 10.098], loss: 0.002562, mae: 0.048614, mean_q: -0.303863
 67247/100000: episode: 673, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -15.586, mean reward: -0.156 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.219, 10.098], loss: 0.002696, mae: 0.049667, mean_q: -0.335831
 67347/100000: episode: 674, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -17.589, mean reward: -0.176 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.981, 10.222], loss: 0.002554, mae: 0.048749, mean_q: -0.315890
 67447/100000: episode: 675, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -17.192, mean reward: -0.172 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.007, 10.296], loss: 0.002736, mae: 0.050557, mean_q: -0.293363
 67547/100000: episode: 676, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.859, mean reward: -0.169 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.519, 10.235], loss: 0.002500, mae: 0.049587, mean_q: -0.269351
 67647/100000: episode: 677, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -13.122, mean reward: -0.131 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.567, 10.098], loss: 0.002610, mae: 0.049130, mean_q: -0.306684
 67747/100000: episode: 678, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -15.883, mean reward: -0.159 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.312, 10.318], loss: 0.002460, mae: 0.048234, mean_q: -0.319528
 67847/100000: episode: 679, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.308, mean reward: -0.173 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.779, 10.132], loss: 0.002474, mae: 0.048375, mean_q: -0.316978
 67947/100000: episode: 680, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -16.556, mean reward: -0.166 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.334, 10.332], loss: 0.002396, mae: 0.047581, mean_q: -0.333913
 68047/100000: episode: 681, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -14.372, mean reward: -0.144 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.311, 10.098], loss: 0.002542, mae: 0.049666, mean_q: -0.324957
 68147/100000: episode: 682, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -18.020, mean reward: -0.180 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.960, 10.098], loss: 0.002615, mae: 0.049166, mean_q: -0.306884
 68247/100000: episode: 683, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -19.208, mean reward: -0.192 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.638, 10.258], loss: 0.002472, mae: 0.048531, mean_q: -0.331627
 68347/100000: episode: 684, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.622, mean reward: -0.166 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.963, 10.263], loss: 0.002465, mae: 0.047996, mean_q: -0.329884
 68447/100000: episode: 685, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -12.819, mean reward: -0.128 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.803, 10.098], loss: 0.002617, mae: 0.050777, mean_q: -0.318480
 68547/100000: episode: 686, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -19.161, mean reward: -0.192 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.569, 10.240], loss: 0.002570, mae: 0.049261, mean_q: -0.317004
 68647/100000: episode: 687, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -14.648, mean reward: -0.146 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.267, 10.175], loss: 0.002303, mae: 0.046270, mean_q: -0.356983
 68747/100000: episode: 688, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -19.974, mean reward: -0.200 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.726, 10.098], loss: 0.002290, mae: 0.048192, mean_q: -0.311089
 68847/100000: episode: 689, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -20.431, mean reward: -0.204 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.685, 10.278], loss: 0.002522, mae: 0.048771, mean_q: -0.299910
 68947/100000: episode: 690, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -15.196, mean reward: -0.152 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.249, 10.098], loss: 0.002353, mae: 0.047907, mean_q: -0.336609
 69047/100000: episode: 691, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -16.765, mean reward: -0.168 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.958, 10.098], loss: 0.002461, mae: 0.048362, mean_q: -0.313270
 69147/100000: episode: 692, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.281, mean reward: -0.183 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.685, 10.098], loss: 0.002572, mae: 0.049461, mean_q: -0.339795
 69247/100000: episode: 693, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -19.124, mean reward: -0.191 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.914, 10.266], loss: 0.002434, mae: 0.048760, mean_q: -0.318354
 69347/100000: episode: 694, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -20.327, mean reward: -0.203 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.092, 10.184], loss: 0.002604, mae: 0.050456, mean_q: -0.300066
 69447/100000: episode: 695, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -14.060, mean reward: -0.141 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.682, 10.098], loss: 0.002374, mae: 0.048688, mean_q: -0.306888
 69547/100000: episode: 696, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -12.317, mean reward: -0.123 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.789, 10.098], loss: 0.002399, mae: 0.048077, mean_q: -0.315741
 69647/100000: episode: 697, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -16.122, mean reward: -0.161 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.498, 10.158], loss: 0.002247, mae: 0.046140, mean_q: -0.324305
 69747/100000: episode: 698, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -17.344, mean reward: -0.173 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.420, 10.154], loss: 0.002542, mae: 0.049327, mean_q: -0.337176
 69847/100000: episode: 699, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -19.496, mean reward: -0.195 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.488, 10.098], loss: 0.005051, mae: 0.060379, mean_q: -0.293271
 69947/100000: episode: 700, duration: 0.466s, episode steps: 100, steps per second: 215, episode reward: -18.671, mean reward: -0.187 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.898, 10.098], loss: 0.002974, mae: 0.056278, mean_q: -0.320490
 70047/100000: episode: 701, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.287, mean reward: -0.173 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.543, 10.098], loss: 0.002487, mae: 0.049097, mean_q: -0.283429
 70147/100000: episode: 702, duration: 0.473s, episode steps: 100, steps per second: 212, episode reward: -19.926, mean reward: -0.199 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.871, 10.177], loss: 0.002395, mae: 0.047033, mean_q: -0.315277
 70247/100000: episode: 703, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.940, mean reward: -0.169 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.541, 10.098], loss: 0.002339, mae: 0.047679, mean_q: -0.287354
 70347/100000: episode: 704, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -17.688, mean reward: -0.177 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.171, 10.184], loss: 0.002419, mae: 0.047093, mean_q: -0.353235
 70447/100000: episode: 705, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -18.963, mean reward: -0.190 [-1.000, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.386, 10.340], loss: 0.002441, mae: 0.048059, mean_q: -0.295881
 70547/100000: episode: 706, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -16.329, mean reward: -0.163 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.998, 10.257], loss: 0.002385, mae: 0.047284, mean_q: -0.287539
 70647/100000: episode: 707, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -7.261, mean reward: -0.073 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.730, 10.098], loss: 0.002556, mae: 0.048766, mean_q: -0.324429
 70747/100000: episode: 708, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -15.114, mean reward: -0.151 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.148, 10.101], loss: 0.002424, mae: 0.048052, mean_q: -0.321862
 70847/100000: episode: 709, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.291, mean reward: -0.173 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.504, 10.098], loss: 0.002152, mae: 0.044769, mean_q: -0.341531
 70947/100000: episode: 710, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -19.176, mean reward: -0.192 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.054, 10.235], loss: 0.002198, mae: 0.045249, mean_q: -0.316380
 71047/100000: episode: 711, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -19.659, mean reward: -0.197 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.726, 10.098], loss: 0.002231, mae: 0.045866, mean_q: -0.335851
 71147/100000: episode: 712, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -14.856, mean reward: -0.149 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.164, 10.366], loss: 0.002324, mae: 0.046285, mean_q: -0.329024
 71247/100000: episode: 713, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -17.837, mean reward: -0.178 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.549, 10.098], loss: 0.002254, mae: 0.046238, mean_q: -0.340728
 71347/100000: episode: 714, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -15.948, mean reward: -0.159 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.627, 10.098], loss: 0.002462, mae: 0.047891, mean_q: -0.332121
 71447/100000: episode: 715, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.117, mean reward: -0.181 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.393, 10.154], loss: 0.002456, mae: 0.048432, mean_q: -0.335150
 71547/100000: episode: 716, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -14.235, mean reward: -0.142 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.582, 10.098], loss: 0.002390, mae: 0.047367, mean_q: -0.310062
 71647/100000: episode: 717, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -15.166, mean reward: -0.152 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.961, 10.098], loss: 0.002176, mae: 0.044725, mean_q: -0.345579
 71747/100000: episode: 718, duration: 0.461s, episode steps: 100, steps per second: 217, episode reward: -16.661, mean reward: -0.167 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.003, 10.275], loss: 0.002257, mae: 0.046101, mean_q: -0.287010
 71847/100000: episode: 719, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -14.083, mean reward: -0.141 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.774, 10.139], loss: 0.002429, mae: 0.047591, mean_q: -0.314960
 71947/100000: episode: 720, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: -11.744, mean reward: -0.117 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.560, 10.439], loss: 0.002361, mae: 0.047279, mean_q: -0.323071
 72047/100000: episode: 721, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.445, mean reward: -0.164 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.461, 10.224], loss: 0.004099, mae: 0.060238, mean_q: -0.306229
 72147/100000: episode: 722, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -15.319, mean reward: -0.153 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.862, 10.098], loss: 0.003615, mae: 0.056569, mean_q: -0.306579
 72247/100000: episode: 723, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -14.328, mean reward: -0.143 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.102, 10.098], loss: 0.002290, mae: 0.046813, mean_q: -0.322635
 72347/100000: episode: 724, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -13.355, mean reward: -0.134 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.052, 10.098], loss: 0.002334, mae: 0.047070, mean_q: -0.298699
 72447/100000: episode: 725, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -19.567, mean reward: -0.196 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.343, 10.215], loss: 0.002164, mae: 0.045299, mean_q: -0.334991
 72547/100000: episode: 726, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -17.681, mean reward: -0.177 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.064, 10.185], loss: 0.002434, mae: 0.048257, mean_q: -0.292832
 72647/100000: episode: 727, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -18.582, mean reward: -0.186 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.181, 10.098], loss: 0.002465, mae: 0.048457, mean_q: -0.303416
 72747/100000: episode: 728, duration: 0.473s, episode steps: 100, steps per second: 212, episode reward: -16.538, mean reward: -0.165 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.639, 10.098], loss: 0.002291, mae: 0.046385, mean_q: -0.305592
 72847/100000: episode: 729, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -15.127, mean reward: -0.151 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.538, 10.291], loss: 0.002377, mae: 0.046509, mean_q: -0.323135
 72947/100000: episode: 730, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -17.690, mean reward: -0.177 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.764, 10.180], loss: 0.002327, mae: 0.046158, mean_q: -0.317653
 73047/100000: episode: 731, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -13.352, mean reward: -0.134 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.497, 10.285], loss: 0.002341, mae: 0.046454, mean_q: -0.323545
 73147/100000: episode: 732, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -17.719, mean reward: -0.177 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.863, 10.098], loss: 0.002336, mae: 0.047036, mean_q: -0.282324
 73247/100000: episode: 733, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -20.169, mean reward: -0.202 [-1.000, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.552, 10.121], loss: 0.002477, mae: 0.047945, mean_q: -0.311070
 73347/100000: episode: 734, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -17.330, mean reward: -0.173 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.188, 10.242], loss: 0.002365, mae: 0.047090, mean_q: -0.311321
 73447/100000: episode: 735, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -14.636, mean reward: -0.146 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.375, 10.204], loss: 0.002353, mae: 0.046716, mean_q: -0.345479
 73547/100000: episode: 736, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.932, mean reward: -0.159 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.833, 10.245], loss: 0.002443, mae: 0.048381, mean_q: -0.293056
 73647/100000: episode: 737, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.650, mean reward: -0.176 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.742, 10.311], loss: 0.002378, mae: 0.047410, mean_q: -0.335866
 73747/100000: episode: 738, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -17.200, mean reward: -0.172 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.033, 10.098], loss: 0.002355, mae: 0.047201, mean_q: -0.326090
 73847/100000: episode: 739, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -15.350, mean reward: -0.153 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.247, 10.246], loss: 0.002367, mae: 0.048359, mean_q: -0.298172
 73947/100000: episode: 740, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -9.812, mean reward: -0.098 [-1.000, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-1.618, 10.098], loss: 0.002303, mae: 0.046592, mean_q: -0.342787
 74047/100000: episode: 741, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -13.438, mean reward: -0.134 [-1.000, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.175, 10.249], loss: 0.002388, mae: 0.047600, mean_q: -0.297083
 74147/100000: episode: 742, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -17.045, mean reward: -0.170 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.259, 10.144], loss: 0.002400, mae: 0.047305, mean_q: -0.322478
 74247/100000: episode: 743, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -18.578, mean reward: -0.186 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.282, 10.311], loss: 0.002409, mae: 0.047905, mean_q: -0.291755
 74347/100000: episode: 744, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -16.810, mean reward: -0.168 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.726, 10.278], loss: 0.002285, mae: 0.046426, mean_q: -0.308957
 74447/100000: episode: 745, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.860, mean reward: -0.159 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.024, 10.198], loss: 0.002483, mae: 0.047841, mean_q: -0.316300
 74547/100000: episode: 746, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -20.639, mean reward: -0.206 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.973, 10.140], loss: 0.002439, mae: 0.049199, mean_q: -0.293536
 74647/100000: episode: 747, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.966, mean reward: -0.170 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.088, 10.098], loss: 0.002327, mae: 0.047371, mean_q: -0.320958
 74747/100000: episode: 748, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -16.887, mean reward: -0.169 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.954, 10.098], loss: 0.002284, mae: 0.046713, mean_q: -0.304504
 74847/100000: episode: 749, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -9.643, mean reward: -0.096 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.906, 10.346], loss: 0.002456, mae: 0.048736, mean_q: -0.291479
 74947/100000: episode: 750, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -16.062, mean reward: -0.161 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.408, 10.098], loss: 0.002478, mae: 0.052166, mean_q: -0.297750
 75047/100000: episode: 751, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -15.736, mean reward: -0.157 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.730, 10.098], loss: 0.002390, mae: 0.049027, mean_q: -0.290202
 75147/100000: episode: 752, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -18.579, mean reward: -0.186 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.303, 10.207], loss: 0.002574, mae: 0.052379, mean_q: -0.294173
 75247/100000: episode: 753, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -18.643, mean reward: -0.186 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.001, 10.113], loss: 0.002200, mae: 0.045940, mean_q: -0.316826
 75347/100000: episode: 754, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -19.605, mean reward: -0.196 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.085, 10.222], loss: 0.002315, mae: 0.046138, mean_q: -0.334876
 75447/100000: episode: 755, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -15.341, mean reward: -0.153 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.419, 10.098], loss: 0.002584, mae: 0.051140, mean_q: -0.293688
 75547/100000: episode: 756, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -15.614, mean reward: -0.156 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.941, 10.448], loss: 0.005333, mae: 0.068276, mean_q: -0.328795
 75647/100000: episode: 757, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -18.468, mean reward: -0.185 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.738, 10.098], loss: 0.002779, mae: 0.053988, mean_q: -0.262595
 75747/100000: episode: 758, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -17.468, mean reward: -0.175 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.679, 10.160], loss: 0.002230, mae: 0.046303, mean_q: -0.308722
 75847/100000: episode: 759, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.542, mean reward: -0.175 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.685, 10.253], loss: 0.002519, mae: 0.049719, mean_q: -0.280534
 75947/100000: episode: 760, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -8.418, mean reward: -0.084 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.382, 10.098], loss: 0.002317, mae: 0.047626, mean_q: -0.283461
 76047/100000: episode: 761, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -19.457, mean reward: -0.195 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.857, 10.098], loss: 0.002381, mae: 0.047439, mean_q: -0.322376
 76147/100000: episode: 762, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -19.408, mean reward: -0.194 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.590, 10.200], loss: 0.002361, mae: 0.048095, mean_q: -0.297311
 76247/100000: episode: 763, duration: 0.466s, episode steps: 100, steps per second: 214, episode reward: -19.080, mean reward: -0.191 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.217, 10.131], loss: 0.002351, mae: 0.046735, mean_q: -0.343662
 76347/100000: episode: 764, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -14.940, mean reward: -0.149 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.885, 10.098], loss: 0.002397, mae: 0.048048, mean_q: -0.306088
 76447/100000: episode: 765, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -19.661, mean reward: -0.197 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.844, 10.243], loss: 0.002459, mae: 0.048637, mean_q: -0.292050
 76547/100000: episode: 766, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -17.940, mean reward: -0.179 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.221, 10.164], loss: 0.002321, mae: 0.047292, mean_q: -0.312484
 76647/100000: episode: 767, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -15.794, mean reward: -0.158 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.277, 10.203], loss: 0.002395, mae: 0.048181, mean_q: -0.276492
 76747/100000: episode: 768, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.187, mean reward: -0.172 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.632, 10.107], loss: 0.002342, mae: 0.047862, mean_q: -0.278527
 76847/100000: episode: 769, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -16.881, mean reward: -0.169 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.633, 10.098], loss: 0.002470, mae: 0.048816, mean_q: -0.289742
 76947/100000: episode: 770, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -15.729, mean reward: -0.157 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.734, 10.248], loss: 0.002336, mae: 0.047335, mean_q: -0.302213
 77047/100000: episode: 771, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -18.954, mean reward: -0.190 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.171, 10.156], loss: 0.002343, mae: 0.047368, mean_q: -0.304054
 77147/100000: episode: 772, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -13.309, mean reward: -0.133 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.409, 10.098], loss: 0.002463, mae: 0.047769, mean_q: -0.304488
 77247/100000: episode: 773, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -16.422, mean reward: -0.164 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.783, 10.098], loss: 0.002315, mae: 0.047086, mean_q: -0.302323
 77347/100000: episode: 774, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -17.388, mean reward: -0.174 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.965, 10.115], loss: 0.002287, mae: 0.046181, mean_q: -0.315755
 77447/100000: episode: 775, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -19.540, mean reward: -0.195 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.343, 10.098], loss: 0.002471, mae: 0.048146, mean_q: -0.338207
 77547/100000: episode: 776, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -16.266, mean reward: -0.163 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.872, 10.361], loss: 0.002466, mae: 0.047784, mean_q: -0.317145
 77647/100000: episode: 777, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -17.137, mean reward: -0.171 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.319, 10.098], loss: 0.002342, mae: 0.046465, mean_q: -0.326930
 77747/100000: episode: 778, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -13.241, mean reward: -0.132 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.597, 10.098], loss: 0.002319, mae: 0.046700, mean_q: -0.345693
 77847/100000: episode: 779, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.620, mean reward: -0.166 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.660, 10.098], loss: 0.002267, mae: 0.045909, mean_q: -0.343479
 77947/100000: episode: 780, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -15.677, mean reward: -0.157 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.969, 10.098], loss: 0.002380, mae: 0.047266, mean_q: -0.278891
 78047/100000: episode: 781, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -14.074, mean reward: -0.141 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.614, 10.098], loss: 0.002444, mae: 0.047860, mean_q: -0.322852
 78147/100000: episode: 782, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.333, mean reward: -0.173 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.938, 10.098], loss: 0.002359, mae: 0.047600, mean_q: -0.319852
 78247/100000: episode: 783, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -11.857, mean reward: -0.119 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.532, 10.098], loss: 0.002373, mae: 0.047258, mean_q: -0.306594
 78347/100000: episode: 784, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -12.416, mean reward: -0.124 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.793, 10.118], loss: 0.002295, mae: 0.046589, mean_q: -0.313880
 78447/100000: episode: 785, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.496, mean reward: -0.175 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.049, 10.098], loss: 0.002365, mae: 0.046879, mean_q: -0.304644
 78547/100000: episode: 786, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -15.497, mean reward: -0.155 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.988, 10.386], loss: 0.002389, mae: 0.047101, mean_q: -0.286135
 78647/100000: episode: 787, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -17.760, mean reward: -0.178 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.567, 10.127], loss: 0.002309, mae: 0.047833, mean_q: -0.334408
 78747/100000: episode: 788, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -20.219, mean reward: -0.202 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.496, 10.330], loss: 0.002411, mae: 0.047311, mean_q: -0.312007
 78847/100000: episode: 789, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -15.456, mean reward: -0.155 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.741, 10.098], loss: 0.002477, mae: 0.048205, mean_q: -0.311612
 78947/100000: episode: 790, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -11.958, mean reward: -0.120 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.943, 10.098], loss: 0.002308, mae: 0.046606, mean_q: -0.335839
 79047/100000: episode: 791, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -13.366, mean reward: -0.134 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.066, 10.098], loss: 0.002391, mae: 0.047812, mean_q: -0.340274
 79147/100000: episode: 792, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -19.956, mean reward: -0.200 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.591, 10.098], loss: 0.002358, mae: 0.046822, mean_q: -0.326748
 79247/100000: episode: 793, duration: 0.473s, episode steps: 100, steps per second: 212, episode reward: -16.963, mean reward: -0.170 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.835, 10.098], loss: 0.002162, mae: 0.044498, mean_q: -0.345946
 79347/100000: episode: 794, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -20.008, mean reward: -0.200 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.571, 10.127], loss: 0.002372, mae: 0.047636, mean_q: -0.301626
 79447/100000: episode: 795, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -18.403, mean reward: -0.184 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.851, 10.098], loss: 0.002468, mae: 0.048785, mean_q: -0.297727
 79547/100000: episode: 796, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.497, mean reward: -0.195 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.558, 10.098], loss: 0.002503, mae: 0.049170, mean_q: -0.275919
 79647/100000: episode: 797, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.875, mean reward: -0.199 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.919, 10.158], loss: 0.002421, mae: 0.047606, mean_q: -0.316501
 79747/100000: episode: 798, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.237, mean reward: -0.172 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.738, 10.291], loss: 0.003640, mae: 0.058811, mean_q: -0.303244
 79847/100000: episode: 799, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.831, mean reward: -0.158 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.981, 10.246], loss: 0.004332, mae: 0.066091, mean_q: -0.317790
 79947/100000: episode: 800, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -13.716, mean reward: -0.137 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.688, 10.098], loss: 0.002436, mae: 0.049694, mean_q: -0.286353
 80047/100000: episode: 801, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.776, mean reward: -0.178 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.020, 10.098], loss: 0.002479, mae: 0.048611, mean_q: -0.301564
 80147/100000: episode: 802, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.540, mean reward: -0.165 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.952, 10.098], loss: 0.002324, mae: 0.046591, mean_q: -0.329548
 80247/100000: episode: 803, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -18.978, mean reward: -0.190 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.379, 10.102], loss: 0.002351, mae: 0.047350, mean_q: -0.304565
 80347/100000: episode: 804, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -16.769, mean reward: -0.168 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.873, 10.157], loss: 0.002273, mae: 0.046104, mean_q: -0.341473
 80447/100000: episode: 805, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.915, mean reward: -0.189 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.747, 10.194], loss: 0.002366, mae: 0.046971, mean_q: -0.310161
 80547/100000: episode: 806, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -16.111, mean reward: -0.161 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.949, 10.098], loss: 0.002557, mae: 0.049579, mean_q: -0.277228
 80647/100000: episode: 807, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -20.379, mean reward: -0.204 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.377, 10.098], loss: 0.002316, mae: 0.047117, mean_q: -0.325702
 80747/100000: episode: 808, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.992, mean reward: -0.170 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.126, 10.098], loss: 0.002378, mae: 0.046894, mean_q: -0.313462
 80847/100000: episode: 809, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -15.192, mean reward: -0.152 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.725, 10.291], loss: 0.002526, mae: 0.048961, mean_q: -0.308716
 80947/100000: episode: 810, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -16.429, mean reward: -0.164 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.317, 10.098], loss: 0.002311, mae: 0.046516, mean_q: -0.344512
 81047/100000: episode: 811, duration: 0.465s, episode steps: 100, steps per second: 215, episode reward: -19.118, mean reward: -0.191 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.764, 10.231], loss: 0.002385, mae: 0.047122, mean_q: -0.307799
 81147/100000: episode: 812, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -15.992, mean reward: -0.160 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.312, 10.098], loss: 0.002348, mae: 0.046282, mean_q: -0.322169
 81247/100000: episode: 813, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -12.638, mean reward: -0.126 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.088, 10.098], loss: 0.002490, mae: 0.048435, mean_q: -0.298876
 81347/100000: episode: 814, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -16.352, mean reward: -0.164 [-1.000, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.960, 10.098], loss: 0.002415, mae: 0.047478, mean_q: -0.318800
 81447/100000: episode: 815, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -15.035, mean reward: -0.150 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.928, 10.279], loss: 0.002457, mae: 0.047935, mean_q: -0.324695
 81547/100000: episode: 816, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -17.391, mean reward: -0.174 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.122, 10.098], loss: 0.002601, mae: 0.049602, mean_q: -0.319764
 81647/100000: episode: 817, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -18.807, mean reward: -0.188 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.961, 10.098], loss: 0.002231, mae: 0.046419, mean_q: -0.302974
 81747/100000: episode: 818, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -15.129, mean reward: -0.151 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.099, 10.098], loss: 0.002534, mae: 0.049587, mean_q: -0.303397
 81847/100000: episode: 819, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -10.827, mean reward: -0.108 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.660, 10.098], loss: 0.002258, mae: 0.047293, mean_q: -0.291834
 81947/100000: episode: 820, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.903, mean reward: -0.189 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.341, 10.303], loss: 0.002507, mae: 0.049222, mean_q: -0.300901
 82047/100000: episode: 821, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -15.683, mean reward: -0.157 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.861, 10.501], loss: 0.002375, mae: 0.046912, mean_q: -0.326425
 82147/100000: episode: 822, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -14.225, mean reward: -0.142 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.951, 10.098], loss: 0.002509, mae: 0.049140, mean_q: -0.279456
 82247/100000: episode: 823, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -16.296, mean reward: -0.163 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.454, 10.098], loss: 0.002278, mae: 0.046607, mean_q: -0.314278
 82347/100000: episode: 824, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -9.791, mean reward: -0.098 [-1.000, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.230, 10.602], loss: 0.002489, mae: 0.049054, mean_q: -0.311192
 82447/100000: episode: 825, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.999, mean reward: -0.190 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.490, 10.098], loss: 0.002289, mae: 0.047051, mean_q: -0.309663
 82547/100000: episode: 826, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -19.668, mean reward: -0.197 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.684, 10.098], loss: 0.002360, mae: 0.046822, mean_q: -0.332341
 82647/100000: episode: 827, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -13.497, mean reward: -0.135 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.081, 10.502], loss: 0.002373, mae: 0.048287, mean_q: -0.335194
 82747/100000: episode: 828, duration: 0.480s, episode steps: 100, steps per second: 209, episode reward: -14.101, mean reward: -0.141 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.880, 10.173], loss: 0.005534, mae: 0.068066, mean_q: -0.316794
 82847/100000: episode: 829, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.467, mean reward: -0.175 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.542, 10.103], loss: 0.002906, mae: 0.052227, mean_q: -0.333753
 82947/100000: episode: 830, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -16.850, mean reward: -0.168 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.513, 10.296], loss: 0.002301, mae: 0.046048, mean_q: -0.354420
 83047/100000: episode: 831, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -19.323, mean reward: -0.193 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.519, 10.220], loss: 0.002477, mae: 0.048636, mean_q: -0.317707
 83147/100000: episode: 832, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -17.230, mean reward: -0.172 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.203, 10.222], loss: 0.002502, mae: 0.049156, mean_q: -0.279708
 83247/100000: episode: 833, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.087, mean reward: -0.171 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.784, 10.219], loss: 0.002424, mae: 0.047179, mean_q: -0.325966
 83347/100000: episode: 834, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -18.702, mean reward: -0.187 [-1.000, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.418, 10.155], loss: 0.002318, mae: 0.046600, mean_q: -0.340267
 83447/100000: episode: 835, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: -17.641, mean reward: -0.176 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.637, 10.098], loss: 0.002335, mae: 0.047326, mean_q: -0.291308
 83547/100000: episode: 836, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -18.859, mean reward: -0.189 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.730, 10.098], loss: 0.002544, mae: 0.049213, mean_q: -0.286583
 83647/100000: episode: 837, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -15.817, mean reward: -0.158 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.147, 10.098], loss: 0.002366, mae: 0.046709, mean_q: -0.317302
 83747/100000: episode: 838, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -12.178, mean reward: -0.122 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.501, 10.098], loss: 0.002395, mae: 0.046834, mean_q: -0.307425
 83847/100000: episode: 839, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -17.881, mean reward: -0.179 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.813, 10.302], loss: 0.002249, mae: 0.046683, mean_q: -0.308994
 83947/100000: episode: 840, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -21.139, mean reward: -0.211 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.359, 10.098], loss: 0.002367, mae: 0.047277, mean_q: -0.310543
 84047/100000: episode: 841, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -20.628, mean reward: -0.206 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.902, 10.328], loss: 0.002412, mae: 0.046924, mean_q: -0.313454
 84147/100000: episode: 842, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -20.039, mean reward: -0.200 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.902, 10.098], loss: 0.002545, mae: 0.049402, mean_q: -0.282693
 84247/100000: episode: 843, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -16.784, mean reward: -0.168 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.323, 10.098], loss: 0.002375, mae: 0.047310, mean_q: -0.339213
 84347/100000: episode: 844, duration: 0.473s, episode steps: 100, steps per second: 212, episode reward: -17.635, mean reward: -0.176 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.367, 10.120], loss: 0.002426, mae: 0.047873, mean_q: -0.308857
 84447/100000: episode: 845, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -12.352, mean reward: -0.124 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.995, 10.136], loss: 0.002298, mae: 0.046405, mean_q: -0.321322
 84547/100000: episode: 846, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -15.038, mean reward: -0.150 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.975, 10.111], loss: 0.002442, mae: 0.047741, mean_q: -0.307321
 84647/100000: episode: 847, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.937, mean reward: -0.169 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.529, 10.290], loss: 0.002359, mae: 0.045970, mean_q: -0.332869
 84747/100000: episode: 848, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -16.635, mean reward: -0.166 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.297, 10.282], loss: 0.002290, mae: 0.046379, mean_q: -0.319399
 84847/100000: episode: 849, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: -15.777, mean reward: -0.158 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.250, 10.210], loss: 0.002285, mae: 0.046154, mean_q: -0.297367
 84947/100000: episode: 850, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.175, mean reward: -0.162 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.622, 10.098], loss: 0.002365, mae: 0.047534, mean_q: -0.306119
 85047/100000: episode: 851, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -14.250, mean reward: -0.143 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.638, 10.287], loss: 0.002298, mae: 0.046511, mean_q: -0.308075
 85147/100000: episode: 852, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -14.736, mean reward: -0.147 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.504, 10.098], loss: 0.002249, mae: 0.045501, mean_q: -0.315139
 85247/100000: episode: 853, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -11.036, mean reward: -0.110 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.186, 10.437], loss: 0.002370, mae: 0.046696, mean_q: -0.309996
 85347/100000: episode: 854, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -17.401, mean reward: -0.174 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.076, 10.098], loss: 0.002352, mae: 0.046845, mean_q: -0.327637
 85447/100000: episode: 855, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -12.726, mean reward: -0.127 [-1.000, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.954, 10.098], loss: 0.002578, mae: 0.050043, mean_q: -0.305917
 85547/100000: episode: 856, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -13.036, mean reward: -0.130 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.411, 10.220], loss: 0.002466, mae: 0.049270, mean_q: -0.295895
 85647/100000: episode: 857, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -19.559, mean reward: -0.196 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.037, 10.251], loss: 0.002476, mae: 0.047852, mean_q: -0.277541
 85747/100000: episode: 858, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.850, mean reward: -0.178 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.068, 10.238], loss: 0.002515, mae: 0.049056, mean_q: -0.312683
 85847/100000: episode: 859, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.985, mean reward: -0.160 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.213, 10.239], loss: 0.002555, mae: 0.048787, mean_q: -0.296236
 85947/100000: episode: 860, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.995, mean reward: -0.160 [-1.000, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.732, 10.161], loss: 0.002447, mae: 0.048109, mean_q: -0.301495
 86047/100000: episode: 861, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -18.207, mean reward: -0.182 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.532, 10.161], loss: 0.002559, mae: 0.049376, mean_q: -0.315738
 86147/100000: episode: 862, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.809, mean reward: -0.178 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.365, 10.098], loss: 0.002465, mae: 0.048476, mean_q: -0.302093
 86247/100000: episode: 863, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.731, mean reward: -0.167 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.821, 10.098], loss: 0.002313, mae: 0.045693, mean_q: -0.317905
 86347/100000: episode: 864, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -17.587, mean reward: -0.176 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.910, 10.098], loss: 0.002464, mae: 0.047582, mean_q: -0.306530
 86447/100000: episode: 865, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -15.774, mean reward: -0.158 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.499, 10.260], loss: 0.002568, mae: 0.050012, mean_q: -0.307438
 86547/100000: episode: 866, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -15.871, mean reward: -0.159 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.600, 10.230], loss: 0.002707, mae: 0.052617, mean_q: -0.319721
 86647/100000: episode: 867, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -17.642, mean reward: -0.176 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.892, 10.166], loss: 0.005512, mae: 0.068880, mean_q: -0.275926
 86747/100000: episode: 868, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -12.000, mean reward: -0.120 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.390, 10.279], loss: 0.002302, mae: 0.046626, mean_q: -0.311365
 86847/100000: episode: 869, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.281, mean reward: -0.173 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.317, 10.098], loss: 0.002424, mae: 0.047329, mean_q: -0.320804
 86947/100000: episode: 870, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.522, mean reward: -0.185 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.488, 10.197], loss: 0.002558, mae: 0.048135, mean_q: -0.298190
 87047/100000: episode: 871, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.402, mean reward: -0.184 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.073, 10.098], loss: 0.002617, mae: 0.048837, mean_q: -0.335548
 87147/100000: episode: 872, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.172, mean reward: -0.172 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.512, 10.151], loss: 0.002357, mae: 0.045919, mean_q: -0.331883
 87247/100000: episode: 873, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.809, mean reward: -0.178 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.519, 10.098], loss: 0.002391, mae: 0.047541, mean_q: -0.291670
 87347/100000: episode: 874, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -14.064, mean reward: -0.141 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.129, 10.265], loss: 0.002510, mae: 0.048056, mean_q: -0.294232
 87447/100000: episode: 875, duration: 0.471s, episode steps: 100, steps per second: 213, episode reward: -17.654, mean reward: -0.177 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.193, 10.098], loss: 0.002710, mae: 0.049358, mean_q: -0.308327
 87547/100000: episode: 876, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -20.333, mean reward: -0.203 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.166, 10.187], loss: 0.002387, mae: 0.046790, mean_q: -0.335156
 87647/100000: episode: 877, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -18.839, mean reward: -0.188 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.381, 10.145], loss: 0.002434, mae: 0.046306, mean_q: -0.324550
 87747/100000: episode: 878, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -16.239, mean reward: -0.162 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.688, 10.098], loss: 0.002471, mae: 0.047493, mean_q: -0.319373
 87847/100000: episode: 879, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -18.099, mean reward: -0.181 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.594, 10.098], loss: 0.002407, mae: 0.046209, mean_q: -0.337674
 87947/100000: episode: 880, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -17.244, mean reward: -0.172 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.551, 10.207], loss: 0.002311, mae: 0.045698, mean_q: -0.336894
 88047/100000: episode: 881, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -21.246, mean reward: -0.212 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.723, 10.098], loss: 0.002339, mae: 0.045515, mean_q: -0.306883
 88147/100000: episode: 882, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -20.433, mean reward: -0.204 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.700, 10.219], loss: 0.002447, mae: 0.045725, mean_q: -0.345928
 88247/100000: episode: 883, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -19.635, mean reward: -0.196 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.404, 10.135], loss: 0.002382, mae: 0.046829, mean_q: -0.304467
 88347/100000: episode: 884, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -16.472, mean reward: -0.165 [-1.000, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.014, 10.098], loss: 0.002487, mae: 0.048092, mean_q: -0.328217
 88447/100000: episode: 885, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -17.670, mean reward: -0.177 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.244, 10.098], loss: 0.004511, mae: 0.064060, mean_q: -0.327670
 88547/100000: episode: 886, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -16.932, mean reward: -0.169 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.466, 10.456], loss: 0.002968, mae: 0.052393, mean_q: -0.322479
 88647/100000: episode: 887, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -7.362, mean reward: -0.074 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.378, 10.098], loss: 0.002363, mae: 0.046942, mean_q: -0.312552
 88747/100000: episode: 888, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.783, mean reward: -0.168 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.807, 10.278], loss: 0.002562, mae: 0.048255, mean_q: -0.301397
 88847/100000: episode: 889, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -18.008, mean reward: -0.180 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.296, 10.145], loss: 0.002569, mae: 0.047875, mean_q: -0.269176
 88947/100000: episode: 890, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -14.264, mean reward: -0.143 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.543, 10.210], loss: 0.002289, mae: 0.045395, mean_q: -0.320812
 89047/100000: episode: 891, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.575, mean reward: -0.186 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.522, 10.183], loss: 0.002386, mae: 0.045866, mean_q: -0.338313
 89147/100000: episode: 892, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.841, mean reward: -0.178 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.352, 10.098], loss: 0.002518, mae: 0.047146, mean_q: -0.336399
 89247/100000: episode: 893, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -19.565, mean reward: -0.196 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.799, 10.132], loss: 0.002545, mae: 0.048218, mean_q: -0.310746
 89347/100000: episode: 894, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -20.605, mean reward: -0.206 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.808, 10.119], loss: 0.002389, mae: 0.047189, mean_q: -0.293693
 89447/100000: episode: 895, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -18.025, mean reward: -0.180 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.492, 10.098], loss: 0.002515, mae: 0.047533, mean_q: -0.290525
 89547/100000: episode: 896, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -10.722, mean reward: -0.107 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.636, 10.098], loss: 0.002522, mae: 0.047913, mean_q: -0.321719
 89647/100000: episode: 897, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -12.205, mean reward: -0.122 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.509, 10.117], loss: 0.002494, mae: 0.047664, mean_q: -0.293253
 89747/100000: episode: 898, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -16.638, mean reward: -0.166 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.004, 10.098], loss: 0.002484, mae: 0.047758, mean_q: -0.305089
 89847/100000: episode: 899, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.262, mean reward: -0.173 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.354, 10.434], loss: 0.002541, mae: 0.048329, mean_q: -0.321386
 89947/100000: episode: 900, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.188, mean reward: -0.182 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.720, 10.431], loss: 0.002414, mae: 0.047331, mean_q: -0.327530
 90047/100000: episode: 901, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -19.227, mean reward: -0.192 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.237, 10.119], loss: 0.002320, mae: 0.046550, mean_q: -0.282855
 90147/100000: episode: 902, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -14.788, mean reward: -0.148 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.706, 10.098], loss: 0.002488, mae: 0.048410, mean_q: -0.281143
 90247/100000: episode: 903, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -19.517, mean reward: -0.195 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.939, 10.098], loss: 0.002460, mae: 0.046854, mean_q: -0.329887
 90347/100000: episode: 904, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -16.531, mean reward: -0.165 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.893, 10.098], loss: 0.002409, mae: 0.046527, mean_q: -0.357584
 90447/100000: episode: 905, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -18.870, mean reward: -0.189 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.923, 10.098], loss: 0.002318, mae: 0.046510, mean_q: -0.329714
 90547/100000: episode: 906, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -19.240, mean reward: -0.192 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.519, 10.098], loss: 0.002576, mae: 0.049233, mean_q: -0.327742
 90647/100000: episode: 907, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.997, mean reward: -0.180 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.714, 10.098], loss: 0.002400, mae: 0.047247, mean_q: -0.319399
 90747/100000: episode: 908, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -19.629, mean reward: -0.196 [-1.000, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.670, 10.143], loss: 0.005354, mae: 0.058149, mean_q: -0.358856
 90847/100000: episode: 909, duration: 0.468s, episode steps: 100, steps per second: 214, episode reward: -15.522, mean reward: -0.155 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.868, 10.098], loss: 0.003819, mae: 0.058707, mean_q: -0.325651
 90947/100000: episode: 910, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -13.066, mean reward: -0.131 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.916, 10.260], loss: 0.002179, mae: 0.046205, mean_q: -0.316629
 91047/100000: episode: 911, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -17.911, mean reward: -0.179 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.308, 10.098], loss: 0.002179, mae: 0.044820, mean_q: -0.325181
 91147/100000: episode: 912, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: -9.025, mean reward: -0.090 [-1.000, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.641, 10.598], loss: 0.002170, mae: 0.044806, mean_q: -0.326795
 91247/100000: episode: 913, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -19.180, mean reward: -0.192 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.240, 10.145], loss: 0.002346, mae: 0.046863, mean_q: -0.313467
 91347/100000: episode: 914, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -14.601, mean reward: -0.146 [-1.000, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.753, 10.098], loss: 0.002248, mae: 0.044944, mean_q: -0.337210
 91447/100000: episode: 915, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -14.726, mean reward: -0.147 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.610, 10.174], loss: 0.002207, mae: 0.044474, mean_q: -0.329833
 91547/100000: episode: 916, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -16.873, mean reward: -0.169 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.925, 10.098], loss: 0.002386, mae: 0.047114, mean_q: -0.314940
 91647/100000: episode: 917, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -16.517, mean reward: -0.165 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.536, 10.098], loss: 0.002393, mae: 0.045881, mean_q: -0.334208
 91747/100000: episode: 918, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -17.197, mean reward: -0.172 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.519, 10.098], loss: 0.002186, mae: 0.044819, mean_q: -0.336643
 91847/100000: episode: 919, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -14.222, mean reward: -0.142 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.436, 10.098], loss: 0.002219, mae: 0.044888, mean_q: -0.307782
 91947/100000: episode: 920, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -20.409, mean reward: -0.204 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.267, 10.098], loss: 0.002354, mae: 0.047161, mean_q: -0.341611
 92047/100000: episode: 921, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -16.692, mean reward: -0.167 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.888, 10.252], loss: 0.002421, mae: 0.047455, mean_q: -0.290787
 92147/100000: episode: 922, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.057, mean reward: -0.181 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.540, 10.136], loss: 0.002406, mae: 0.046908, mean_q: -0.320299
 92247/100000: episode: 923, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -16.672, mean reward: -0.167 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.352, 10.098], loss: 0.002371, mae: 0.046498, mean_q: -0.326476
 92347/100000: episode: 924, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -18.363, mean reward: -0.184 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.045, 10.343], loss: 0.002231, mae: 0.045351, mean_q: -0.323863
 92447/100000: episode: 925, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -18.794, mean reward: -0.188 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.703, 10.098], loss: 0.002409, mae: 0.047354, mean_q: -0.312094
 92547/100000: episode: 926, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.583, mean reward: -0.166 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.653, 10.167], loss: 0.002538, mae: 0.047923, mean_q: -0.326743
 92647/100000: episode: 927, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -13.284, mean reward: -0.133 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.659, 10.098], loss: 0.002496, mae: 0.047466, mean_q: -0.322339
 92747/100000: episode: 928, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.781, mean reward: -0.168 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.937, 10.109], loss: 0.002496, mae: 0.048769, mean_q: -0.267615
 92847/100000: episode: 929, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.448, mean reward: -0.184 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.489, 10.335], loss: 0.002555, mae: 0.049120, mean_q: -0.274776
 92947/100000: episode: 930, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.773, mean reward: -0.168 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.799, 10.098], loss: 0.002661, mae: 0.049040, mean_q: -0.303735
 93047/100000: episode: 931, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -14.859, mean reward: -0.149 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.379, 10.098], loss: 0.002348, mae: 0.046465, mean_q: -0.344834
 93147/100000: episode: 932, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -10.167, mean reward: -0.102 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.752, 10.098], loss: 0.002714, mae: 0.049491, mean_q: -0.324019
 93247/100000: episode: 933, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.287, mean reward: -0.163 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.335, 10.192], loss: 0.002599, mae: 0.048982, mean_q: -0.328761
 93347/100000: episode: 934, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -14.973, mean reward: -0.150 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.502, 10.098], loss: 0.002569, mae: 0.048721, mean_q: -0.314629
 93447/100000: episode: 935, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -9.440, mean reward: -0.094 [-1.000, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.398, 10.098], loss: 0.002609, mae: 0.049002, mean_q: -0.286333
 93547/100000: episode: 936, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -18.818, mean reward: -0.188 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.074, 10.147], loss: 0.002493, mae: 0.047698, mean_q: -0.329016
 93647/100000: episode: 937, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -16.313, mean reward: -0.163 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.449, 10.098], loss: 0.002394, mae: 0.047129, mean_q: -0.306675
 93747/100000: episode: 938, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.122, mean reward: -0.181 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.816, 10.098], loss: 0.002619, mae: 0.049721, mean_q: -0.284632
 93847/100000: episode: 939, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -12.314, mean reward: -0.123 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.904, 10.098], loss: 0.002579, mae: 0.048814, mean_q: -0.307812
 93947/100000: episode: 940, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -17.975, mean reward: -0.180 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.132, 10.218], loss: 0.002298, mae: 0.045961, mean_q: -0.330305
 94047/100000: episode: 941, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -12.060, mean reward: -0.121 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.372, 10.271], loss: 0.002593, mae: 0.049560, mean_q: -0.288420
 94147/100000: episode: 942, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -10.831, mean reward: -0.108 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.726, 10.329], loss: 0.004716, mae: 0.063871, mean_q: -0.301020
 94247/100000: episode: 943, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -17.539, mean reward: -0.175 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.993, 10.343], loss: 0.002705, mae: 0.051080, mean_q: -0.270120
 94347/100000: episode: 944, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -18.199, mean reward: -0.182 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.690, 10.098], loss: 0.002434, mae: 0.047857, mean_q: -0.275769
 94447/100000: episode: 945, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -15.300, mean reward: -0.153 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.253, 10.354], loss: 0.002666, mae: 0.048896, mean_q: -0.291321
 94547/100000: episode: 946, duration: 0.473s, episode steps: 100, steps per second: 212, episode reward: -18.554, mean reward: -0.186 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.887, 10.195], loss: 0.002617, mae: 0.048860, mean_q: -0.284180
 94647/100000: episode: 947, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -15.478, mean reward: -0.155 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.299, 10.224], loss: 0.002693, mae: 0.050196, mean_q: -0.287164
 94747/100000: episode: 948, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.792, mean reward: -0.188 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.279, 10.100], loss: 0.002641, mae: 0.049289, mean_q: -0.296682
 94847/100000: episode: 949, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.531, mean reward: -0.185 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.443, 10.098], loss: 0.002620, mae: 0.049493, mean_q: -0.290723
 94947/100000: episode: 950, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -16.814, mean reward: -0.168 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.659, 10.098], loss: 0.002586, mae: 0.048358, mean_q: -0.332758
 95047/100000: episode: 951, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -17.854, mean reward: -0.179 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.388, 10.098], loss: 0.002539, mae: 0.048955, mean_q: -0.302321
 95147/100000: episode: 952, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -16.724, mean reward: -0.167 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.248, 10.262], loss: 0.002631, mae: 0.049339, mean_q: -0.326683
 95247/100000: episode: 953, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -15.363, mean reward: -0.154 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.847, 10.098], loss: 0.002364, mae: 0.046511, mean_q: -0.332163
 95347/100000: episode: 954, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -13.265, mean reward: -0.133 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.455, 10.265], loss: 0.002665, mae: 0.048495, mean_q: -0.295655
 95447/100000: episode: 955, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -18.053, mean reward: -0.181 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.040, 10.098], loss: 0.002502, mae: 0.047928, mean_q: -0.303398
 95547/100000: episode: 956, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -18.530, mean reward: -0.185 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.887, 10.128], loss: 0.002438, mae: 0.048712, mean_q: -0.295288
 95647/100000: episode: 957, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -15.438, mean reward: -0.154 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.990, 10.098], loss: 0.002579, mae: 0.048295, mean_q: -0.333749
 95747/100000: episode: 958, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -18.494, mean reward: -0.185 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.470, 10.173], loss: 0.002582, mae: 0.048556, mean_q: -0.293435
 95847/100000: episode: 959, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -16.045, mean reward: -0.160 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.613, 10.098], loss: 0.002520, mae: 0.048428, mean_q: -0.331736
 95947/100000: episode: 960, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -15.683, mean reward: -0.157 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.455, 10.296], loss: 0.002389, mae: 0.046748, mean_q: -0.324459
 96047/100000: episode: 961, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -12.837, mean reward: -0.128 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.482, 10.098], loss: 0.002530, mae: 0.048062, mean_q: -0.295917
 96147/100000: episode: 962, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -11.379, mean reward: -0.114 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.646, 10.098], loss: 0.002569, mae: 0.049485, mean_q: -0.276645
 96247/100000: episode: 963, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -11.271, mean reward: -0.113 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.731, 10.271], loss: 0.002707, mae: 0.050596, mean_q: -0.303656
 96347/100000: episode: 964, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -17.496, mean reward: -0.175 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.543, 10.191], loss: 0.002506, mae: 0.048823, mean_q: -0.273381
 96447/100000: episode: 965, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -20.614, mean reward: -0.206 [-1.000, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.717, 10.104], loss: 0.002508, mae: 0.048739, mean_q: -0.293406
 96547/100000: episode: 966, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -14.713, mean reward: -0.147 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.514, 10.098], loss: 0.002595, mae: 0.049391, mean_q: -0.300605
 96647/100000: episode: 967, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -20.237, mean reward: -0.202 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.577, 10.098], loss: 0.002496, mae: 0.048944, mean_q: -0.266692
 96747/100000: episode: 968, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -19.111, mean reward: -0.191 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.740, 10.098], loss: 0.002447, mae: 0.048164, mean_q: -0.328200
 96847/100000: episode: 969, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -19.244, mean reward: -0.192 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.371, 10.152], loss: 0.002487, mae: 0.048814, mean_q: -0.292858
 96947/100000: episode: 970, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -19.888, mean reward: -0.199 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.676, 10.098], loss: 0.002419, mae: 0.047682, mean_q: -0.309671
 97047/100000: episode: 971, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.430, mean reward: -0.164 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.588, 10.098], loss: 0.002374, mae: 0.046903, mean_q: -0.286379
 97147/100000: episode: 972, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -18.295, mean reward: -0.183 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.155, 10.098], loss: 0.002277, mae: 0.045963, mean_q: -0.333881
 97247/100000: episode: 973, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.409, mean reward: -0.164 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.497, 10.209], loss: 0.002694, mae: 0.051128, mean_q: -0.307586
 97347/100000: episode: 974, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -18.192, mean reward: -0.182 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.069, 10.098], loss: 0.005126, mae: 0.064168, mean_q: -0.319809
 97447/100000: episode: 975, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.916, mean reward: -0.189 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.781, 10.226], loss: 0.002515, mae: 0.049008, mean_q: -0.289073
 97547/100000: episode: 976, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -16.781, mean reward: -0.168 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.484, 10.098], loss: 0.002335, mae: 0.046544, mean_q: -0.309492
 97647/100000: episode: 977, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -19.067, mean reward: -0.191 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.682, 10.098], loss: 0.002310, mae: 0.046586, mean_q: -0.301153
 97747/100000: episode: 978, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -13.791, mean reward: -0.138 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.699, 10.393], loss: 0.002300, mae: 0.046454, mean_q: -0.257044
 97847/100000: episode: 979, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -11.203, mean reward: -0.112 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.018, 10.178], loss: 0.002592, mae: 0.047935, mean_q: -0.342485
 97947/100000: episode: 980, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -17.885, mean reward: -0.179 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.206, 10.098], loss: 0.002340, mae: 0.045927, mean_q: -0.295333
 98047/100000: episode: 981, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -14.679, mean reward: -0.147 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.978, 10.239], loss: 0.002295, mae: 0.045820, mean_q: -0.288999
 98147/100000: episode: 982, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -18.321, mean reward: -0.183 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.139, 10.098], loss: 0.002381, mae: 0.047015, mean_q: -0.305373
 98247/100000: episode: 983, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -14.056, mean reward: -0.141 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.975, 10.098], loss: 0.002152, mae: 0.044847, mean_q: -0.305793
 98347/100000: episode: 984, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -17.123, mean reward: -0.171 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.703, 10.158], loss: 0.002400, mae: 0.046427, mean_q: -0.320826
 98447/100000: episode: 985, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -12.132, mean reward: -0.121 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.592, 10.098], loss: 0.002400, mae: 0.046797, mean_q: -0.290491
 98547/100000: episode: 986, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -14.517, mean reward: -0.145 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.826, 10.098], loss: 0.002240, mae: 0.045295, mean_q: -0.328503
 98647/100000: episode: 987, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.954, mean reward: -0.160 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.363, 10.098], loss: 0.002444, mae: 0.047740, mean_q: -0.257246
 98747/100000: episode: 988, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -15.197, mean reward: -0.152 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.362, 10.370], loss: 0.002298, mae: 0.045645, mean_q: -0.316747
 98847/100000: episode: 989, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -15.819, mean reward: -0.158 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.680, 10.098], loss: 0.002297, mae: 0.045875, mean_q: -0.323423
 98947/100000: episode: 990, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -19.237, mean reward: -0.192 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.721, 10.212], loss: 0.002344, mae: 0.047150, mean_q: -0.295742
 99047/100000: episode: 991, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -9.732, mean reward: -0.097 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.929, 10.445], loss: 0.002293, mae: 0.046034, mean_q: -0.308556
 99147/100000: episode: 992, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -15.867, mean reward: -0.159 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.798, 10.298], loss: 0.002125, mae: 0.045373, mean_q: -0.313004
 99247/100000: episode: 993, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -14.556, mean reward: -0.146 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.332, 10.098], loss: 0.002278, mae: 0.046476, mean_q: -0.279441
 99347/100000: episode: 994, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.970, mean reward: -0.180 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.420, 10.098], loss: 0.006885, mae: 0.062253, mean_q: -0.324403
 99447/100000: episode: 995, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -15.030, mean reward: -0.150 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.764, 10.219], loss: 0.003083, mae: 0.051677, mean_q: -0.315013
 99547/100000: episode: 996, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.012, mean reward: -0.170 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.665, 10.098], loss: 0.002281, mae: 0.045949, mean_q: -0.316607
 99647/100000: episode: 997, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -18.491, mean reward: -0.185 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.896, 10.147], loss: 0.002288, mae: 0.046059, mean_q: -0.302800
 99747/100000: episode: 998, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -18.755, mean reward: -0.188 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.635, 10.208], loss: 0.002138, mae: 0.044474, mean_q: -0.312994
 99847/100000: episode: 999, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -17.548, mean reward: -0.175 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.963, 10.098], loss: 0.002161, mae: 0.044511, mean_q: -0.294421
 99947/100000: episode: 1000, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -8.416, mean reward: -0.084 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.274, 10.272], loss: 0.002073, mae: 0.043042, mean_q: -0.315084
done, took 483.032 seconds
[Info] End Uniform Random Simulation. Falsification occurred 2 times.
