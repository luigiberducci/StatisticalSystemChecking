Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.170s, episode steps: 100, steps per second: 589, episode reward: -17.296, mean reward: -0.173 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.490, 10.318], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.071s, episode steps: 100, steps per second: 1416, episode reward: -19.440, mean reward: -0.194 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.086, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: -18.493, mean reward: -0.185 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.990, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.065s, episode steps: 100, steps per second: 1542, episode reward: -18.488, mean reward: -0.185 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.499, 10.221], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.063s, episode steps: 100, steps per second: 1581, episode reward: -15.837, mean reward: -0.158 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.158, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.294s, episode steps: 100, steps per second: 77, episode reward: -17.609, mean reward: -0.176 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.640, 10.098], loss: 0.050309, mae: 0.223007, mean_q: 0.170849
   700/100000: episode: 7, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: -18.173, mean reward: -0.182 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.720, 10.151], loss: 0.013177, mae: 0.107872, mean_q: -0.034976
   800/100000: episode: 8, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.893, mean reward: -0.159 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.955, 10.098], loss: 0.011247, mae: 0.100710, mean_q: -0.148724
   900/100000: episode: 9, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -18.557, mean reward: -0.186 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.087, 10.098], loss: 0.010668, mae: 0.093721, mean_q: -0.185804
  1000/100000: episode: 10, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -17.450, mean reward: -0.175 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.407, 10.143], loss: 0.009965, mae: 0.091401, mean_q: -0.274431
  1100/100000: episode: 11, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: -18.472, mean reward: -0.185 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.650, 10.224], loss: 0.009161, mae: 0.087870, mean_q: -0.294458
  1200/100000: episode: 12, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -14.175, mean reward: -0.142 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.818, 10.264], loss: 0.009565, mae: 0.089523, mean_q: -0.331057
  1300/100000: episode: 13, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.997, mean reward: -0.170 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.237, 10.258], loss: 0.007747, mae: 0.080127, mean_q: -0.328443
  1400/100000: episode: 14, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -21.109, mean reward: -0.211 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.560, 10.166], loss: 0.006918, mae: 0.077799, mean_q: -0.351753
  1500/100000: episode: 15, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -14.792, mean reward: -0.148 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.730, 10.354], loss: 0.007648, mae: 0.080540, mean_q: -0.315033
  1600/100000: episode: 16, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: -18.562, mean reward: -0.186 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.010, 10.103], loss: 0.006994, mae: 0.075940, mean_q: -0.372318
  1700/100000: episode: 17, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -18.556, mean reward: -0.186 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.555, 10.174], loss: 0.007734, mae: 0.081095, mean_q: -0.341904
  1800/100000: episode: 18, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.746, mean reward: -0.177 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.198, 10.098], loss: 0.006700, mae: 0.074270, mean_q: -0.340788
  1900/100000: episode: 19, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -16.372, mean reward: -0.164 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.204, 10.122], loss: 0.005932, mae: 0.073382, mean_q: -0.359286
  2000/100000: episode: 20, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -15.652, mean reward: -0.157 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.896, 10.098], loss: 0.006527, mae: 0.078555, mean_q: -0.321766
  2100/100000: episode: 21, duration: 0.608s, episode steps: 100, steps per second: 165, episode reward: -14.751, mean reward: -0.148 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.652, 10.221], loss: 0.007001, mae: 0.076604, mean_q: -0.320969
  2200/100000: episode: 22, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -16.406, mean reward: -0.164 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.378, 10.278], loss: 0.006134, mae: 0.074082, mean_q: -0.299381
  2300/100000: episode: 23, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -19.211, mean reward: -0.192 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.901, 10.098], loss: 0.005766, mae: 0.070488, mean_q: -0.339292
  2400/100000: episode: 24, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: -9.586, mean reward: -0.096 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.614, 10.098], loss: 0.005503, mae: 0.071302, mean_q: -0.340005
  2500/100000: episode: 25, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -18.045, mean reward: -0.180 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.861, 10.117], loss: 0.005969, mae: 0.074690, mean_q: -0.318362
  2600/100000: episode: 26, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -19.151, mean reward: -0.192 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.582, 10.098], loss: 0.006554, mae: 0.077299, mean_q: -0.327508
  2700/100000: episode: 27, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -16.641, mean reward: -0.166 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.003, 10.098], loss: 0.005600, mae: 0.069906, mean_q: -0.316910
  2800/100000: episode: 28, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -11.739, mean reward: -0.117 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.665, 10.098], loss: 0.006057, mae: 0.075536, mean_q: -0.335359
  2900/100000: episode: 29, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.890, mean reward: -0.189 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.226, 10.166], loss: 0.006438, mae: 0.076047, mean_q: -0.345674
  3000/100000: episode: 30, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -19.345, mean reward: -0.193 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.575, 10.117], loss: 0.006782, mae: 0.080684, mean_q: -0.315096
  3100/100000: episode: 31, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -18.873, mean reward: -0.189 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.484, 10.310], loss: 0.005557, mae: 0.073261, mean_q: -0.327525
  3200/100000: episode: 32, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: -16.974, mean reward: -0.170 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.323, 10.098], loss: 0.005161, mae: 0.069947, mean_q: -0.322209
  3300/100000: episode: 33, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -12.564, mean reward: -0.126 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.546, 10.413], loss: 0.005398, mae: 0.069739, mean_q: -0.336172
  3400/100000: episode: 34, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -17.422, mean reward: -0.174 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.180, 10.381], loss: 0.005353, mae: 0.074227, mean_q: -0.338480
  3500/100000: episode: 35, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.020, mean reward: -0.190 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.700, 10.098], loss: 0.005492, mae: 0.073903, mean_q: -0.296378
  3600/100000: episode: 36, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.250, mean reward: -0.143 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.404, 10.459], loss: 0.005536, mae: 0.072896, mean_q: -0.331652
  3700/100000: episode: 37, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -19.215, mean reward: -0.192 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.327, 10.229], loss: 0.007546, mae: 0.082243, mean_q: -0.356024
  3800/100000: episode: 38, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -19.226, mean reward: -0.192 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.712, 10.098], loss: 0.004585, mae: 0.067264, mean_q: -0.339353
  3900/100000: episode: 39, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -18.946, mean reward: -0.189 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.587, 10.128], loss: 0.005312, mae: 0.072467, mean_q: -0.325252
  4000/100000: episode: 40, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -18.898, mean reward: -0.189 [-1.000, 0.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.394, 10.102], loss: 0.004481, mae: 0.063812, mean_q: -0.354634
  4100/100000: episode: 41, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -14.656, mean reward: -0.147 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.483, 10.098], loss: 0.004793, mae: 0.068062, mean_q: -0.319428
  4200/100000: episode: 42, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -16.033, mean reward: -0.160 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.546, 10.098], loss: 0.006002, mae: 0.076038, mean_q: -0.348327
  4300/100000: episode: 43, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -16.853, mean reward: -0.169 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.410, 10.405], loss: 0.004611, mae: 0.067604, mean_q: -0.306450
  4400/100000: episode: 44, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: -14.327, mean reward: -0.143 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.746, 10.098], loss: 0.004500, mae: 0.068466, mean_q: -0.307073
  4500/100000: episode: 45, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -13.819, mean reward: -0.138 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.906, 10.098], loss: 0.006183, mae: 0.073669, mean_q: -0.327439
  4600/100000: episode: 46, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -16.887, mean reward: -0.169 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.294, 10.186], loss: 0.004997, mae: 0.070889, mean_q: -0.337074
  4700/100000: episode: 47, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -16.807, mean reward: -0.168 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.674, 10.098], loss: 0.005085, mae: 0.069139, mean_q: -0.304524
  4800/100000: episode: 48, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -18.278, mean reward: -0.183 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.977, 10.103], loss: 0.004927, mae: 0.069872, mean_q: -0.333290
  4900/100000: episode: 49, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: -19.944, mean reward: -0.199 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.169, 10.098], loss: 0.004628, mae: 0.069969, mean_q: -0.342402
  5000/100000: episode: 50, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: -17.034, mean reward: -0.170 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.785, 10.253], loss: 0.004604, mae: 0.068558, mean_q: -0.338590
  5100/100000: episode: 51, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -16.948, mean reward: -0.169 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.086, 10.164], loss: 0.005054, mae: 0.070138, mean_q: -0.317503
  5200/100000: episode: 52, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -17.018, mean reward: -0.170 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.701, 10.165], loss: 0.004097, mae: 0.066647, mean_q: -0.296469
  5300/100000: episode: 53, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -9.905, mean reward: -0.099 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.167, 10.098], loss: 0.004958, mae: 0.068114, mean_q: -0.311638
  5400/100000: episode: 54, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -19.387, mean reward: -0.194 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.710, 10.173], loss: 0.004318, mae: 0.067240, mean_q: -0.303952
  5500/100000: episode: 55, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.608, mean reward: -0.176 [-1.000, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.104, 10.098], loss: 0.004443, mae: 0.067281, mean_q: -0.346334
  5600/100000: episode: 56, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -18.025, mean reward: -0.180 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.272, 10.098], loss: 0.004438, mae: 0.069191, mean_q: -0.305334
  5700/100000: episode: 57, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -19.353, mean reward: -0.194 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.486, 10.335], loss: 0.004440, mae: 0.068508, mean_q: -0.305461
  5800/100000: episode: 58, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -18.680, mean reward: -0.187 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.333, 10.224], loss: 0.003921, mae: 0.063386, mean_q: -0.291195
  5900/100000: episode: 59, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: -15.394, mean reward: -0.154 [-1.000, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.658, 10.291], loss: 0.004200, mae: 0.065272, mean_q: -0.327460
  6000/100000: episode: 60, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: -18.062, mean reward: -0.181 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.876, 10.098], loss: 0.003936, mae: 0.062924, mean_q: -0.316130
  6100/100000: episode: 61, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -16.058, mean reward: -0.161 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.504, 10.208], loss: 0.004673, mae: 0.068281, mean_q: -0.319696
  6200/100000: episode: 62, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -18.969, mean reward: -0.190 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.387, 10.286], loss: 0.004516, mae: 0.068375, mean_q: -0.337989
  6300/100000: episode: 63, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -16.005, mean reward: -0.160 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.701, 10.270], loss: 0.004314, mae: 0.067122, mean_q: -0.331773
  6400/100000: episode: 64, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -16.881, mean reward: -0.169 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.173, 10.203], loss: 0.004660, mae: 0.070804, mean_q: -0.296319
  6500/100000: episode: 65, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -16.448, mean reward: -0.164 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.678, 10.098], loss: 0.004705, mae: 0.069709, mean_q: -0.293815
  6600/100000: episode: 66, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -17.067, mean reward: -0.171 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.289, 10.098], loss: 0.004066, mae: 0.065654, mean_q: -0.311809
  6700/100000: episode: 67, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -18.082, mean reward: -0.181 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.403, 10.098], loss: 0.003555, mae: 0.062019, mean_q: -0.332443
  6800/100000: episode: 68, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -15.961, mean reward: -0.160 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.746, 10.098], loss: 0.004284, mae: 0.065861, mean_q: -0.302478
  6900/100000: episode: 69, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -18.114, mean reward: -0.181 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.955, 10.098], loss: 0.004457, mae: 0.067845, mean_q: -0.320225
  7000/100000: episode: 70, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: -18.600, mean reward: -0.186 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.163, 10.098], loss: 0.003545, mae: 0.061896, mean_q: -0.322261
  7100/100000: episode: 71, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -18.665, mean reward: -0.187 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.553, 10.294], loss: 0.004359, mae: 0.067295, mean_q: -0.290199
  7200/100000: episode: 72, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -15.875, mean reward: -0.159 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.402, 10.098], loss: 0.004178, mae: 0.066434, mean_q: -0.320339
  7300/100000: episode: 73, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.488, mean reward: -0.165 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.877, 10.277], loss: 0.004598, mae: 0.065971, mean_q: -0.323356
  7400/100000: episode: 74, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -14.987, mean reward: -0.150 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.111, 10.103], loss: 0.003675, mae: 0.061782, mean_q: -0.336304
  7500/100000: episode: 75, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -11.447, mean reward: -0.114 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.140, 10.098], loss: 0.003642, mae: 0.062330, mean_q: -0.293673
  7600/100000: episode: 76, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -10.187, mean reward: -0.102 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.723, 10.098], loss: 0.004275, mae: 0.065443, mean_q: -0.350195
  7700/100000: episode: 77, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.695, mean reward: -0.177 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.172, 10.098], loss: 0.004724, mae: 0.069330, mean_q: -0.319093
  7800/100000: episode: 78, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -18.619, mean reward: -0.186 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.546, 10.347], loss: 0.003776, mae: 0.061378, mean_q: -0.348401
  7900/100000: episode: 79, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -16.287, mean reward: -0.163 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.077, 10.279], loss: 0.003822, mae: 0.063783, mean_q: -0.309542
  8000/100000: episode: 80, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -18.077, mean reward: -0.181 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.120, 10.158], loss: 0.004937, mae: 0.071413, mean_q: -0.320760
  8100/100000: episode: 81, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -19.022, mean reward: -0.190 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.645, 10.305], loss: 0.004228, mae: 0.066208, mean_q: -0.274048
  8200/100000: episode: 82, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -17.606, mean reward: -0.176 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.009, 10.098], loss: 0.004177, mae: 0.066722, mean_q: -0.309549
  8300/100000: episode: 83, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -13.474, mean reward: -0.135 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.998, 10.098], loss: 0.003986, mae: 0.064723, mean_q: -0.323930
  8400/100000: episode: 84, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.040, mean reward: -0.180 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.396, 10.167], loss: 0.003770, mae: 0.063217, mean_q: -0.348642
  8500/100000: episode: 85, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -12.579, mean reward: -0.126 [-1.000, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.607, 10.259], loss: 0.003986, mae: 0.065819, mean_q: -0.309767
  8600/100000: episode: 86, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -18.604, mean reward: -0.186 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.154, 10.139], loss: 0.004152, mae: 0.065002, mean_q: -0.310286
  8700/100000: episode: 87, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -19.500, mean reward: -0.195 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.896, 10.098], loss: 0.004333, mae: 0.065981, mean_q: -0.325335
  8800/100000: episode: 88, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: -17.736, mean reward: -0.177 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.128, 10.221], loss: 0.003713, mae: 0.063036, mean_q: -0.339502
  8900/100000: episode: 89, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -19.131, mean reward: -0.191 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.360, 10.098], loss: 0.003782, mae: 0.063899, mean_q: -0.330515
  9000/100000: episode: 90, duration: 0.619s, episode steps: 100, steps per second: 161, episode reward: -17.333, mean reward: -0.173 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.712, 10.098], loss: 0.003723, mae: 0.063549, mean_q: -0.336624
  9100/100000: episode: 91, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -15.744, mean reward: -0.157 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.183, 10.098], loss: 0.003319, mae: 0.059669, mean_q: -0.296864
  9200/100000: episode: 92, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -18.057, mean reward: -0.181 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.797, 10.244], loss: 0.003618, mae: 0.062325, mean_q: -0.279042
  9300/100000: episode: 93, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -12.019, mean reward: -0.120 [-1.000, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.824, 10.591], loss: 0.003898, mae: 0.063535, mean_q: -0.329997
  9400/100000: episode: 94, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -16.629, mean reward: -0.166 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.743, 10.098], loss: 0.003934, mae: 0.063546, mean_q: -0.307695
  9500/100000: episode: 95, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -18.385, mean reward: -0.184 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.686, 10.098], loss: 0.003989, mae: 0.064168, mean_q: -0.316686
  9600/100000: episode: 96, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -15.042, mean reward: -0.150 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.611, 10.098], loss: 0.003966, mae: 0.066969, mean_q: -0.323775
  9700/100000: episode: 97, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -9.531, mean reward: -0.095 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.921, 10.325], loss: 0.003511, mae: 0.061485, mean_q: -0.312963
  9800/100000: episode: 98, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -15.013, mean reward: -0.150 [-1.000, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.418, 10.258], loss: 0.003924, mae: 0.063785, mean_q: -0.324795
  9900/100000: episode: 99, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.975, mean reward: -0.160 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.417, 10.356], loss: 0.003958, mae: 0.063368, mean_q: -0.319704
 10000/100000: episode: 100, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -16.606, mean reward: -0.166 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.924, 10.289], loss: 0.004203, mae: 0.065716, mean_q: -0.333308
 10100/100000: episode: 101, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: -14.771, mean reward: -0.148 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.896, 10.098], loss: 0.007247, mae: 0.082928, mean_q: -0.307983
 10200/100000: episode: 102, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -13.443, mean reward: -0.134 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.273, 10.098], loss: 0.003829, mae: 0.064004, mean_q: -0.281800
 10300/100000: episode: 103, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: -19.983, mean reward: -0.200 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.355, 10.098], loss: 0.003751, mae: 0.062300, mean_q: -0.294698
 10400/100000: episode: 104, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -16.192, mean reward: -0.162 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.578, 10.184], loss: 0.003593, mae: 0.060917, mean_q: -0.306578
 10500/100000: episode: 105, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -16.791, mean reward: -0.168 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.156, 10.154], loss: 0.005132, mae: 0.071718, mean_q: -0.315040
 10600/100000: episode: 106, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -18.540, mean reward: -0.185 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.040, 10.098], loss: 0.003644, mae: 0.061925, mean_q: -0.330953
 10700/100000: episode: 107, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -16.843, mean reward: -0.168 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.696, 10.247], loss: 0.004471, mae: 0.065896, mean_q: -0.323070
 10800/100000: episode: 108, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -19.371, mean reward: -0.194 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.583, 10.205], loss: 0.003445, mae: 0.059551, mean_q: -0.285392
 10900/100000: episode: 109, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -15.526, mean reward: -0.155 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.330, 10.098], loss: 0.004231, mae: 0.069709, mean_q: -0.309979
 11000/100000: episode: 110, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -20.277, mean reward: -0.203 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.332, 10.098], loss: 0.003618, mae: 0.063562, mean_q: -0.286360
 11100/100000: episode: 111, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -15.817, mean reward: -0.158 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.659, 10.241], loss: 0.003206, mae: 0.057553, mean_q: -0.329687
 11200/100000: episode: 112, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -12.855, mean reward: -0.129 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.089, 10.098], loss: 0.004294, mae: 0.066845, mean_q: -0.302330
 11300/100000: episode: 113, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -18.136, mean reward: -0.181 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.761, 10.171], loss: 0.005559, mae: 0.072752, mean_q: -0.299259
 11400/100000: episode: 114, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.717, mean reward: -0.177 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.247, 10.322], loss: 0.003655, mae: 0.062927, mean_q: -0.318479
 11500/100000: episode: 115, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: -9.498, mean reward: -0.095 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.217, 10.286], loss: 0.003807, mae: 0.063865, mean_q: -0.328918
 11600/100000: episode: 116, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -14.554, mean reward: -0.146 [-1.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.906, 10.107], loss: 0.003476, mae: 0.061251, mean_q: -0.323851
 11700/100000: episode: 117, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -18.667, mean reward: -0.187 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.874, 10.216], loss: 0.003589, mae: 0.060974, mean_q: -0.293684
 11800/100000: episode: 118, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -14.072, mean reward: -0.141 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.610, 10.175], loss: 0.003538, mae: 0.060936, mean_q: -0.326765
 11900/100000: episode: 119, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -19.424, mean reward: -0.194 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.698, 10.198], loss: 0.004371, mae: 0.066615, mean_q: -0.308860
 12000/100000: episode: 120, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -11.050, mean reward: -0.110 [-1.000, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.389, 10.292], loss: 0.003389, mae: 0.059177, mean_q: -0.335947
 12100/100000: episode: 121, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: -9.545, mean reward: -0.095 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.655, 10.521], loss: 0.003752, mae: 0.063631, mean_q: -0.311172
 12200/100000: episode: 122, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: -16.298, mean reward: -0.163 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.368, 10.098], loss: 0.004046, mae: 0.065392, mean_q: -0.299334
 12300/100000: episode: 123, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.353, mean reward: -0.184 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.899, 10.218], loss: 0.004378, mae: 0.068487, mean_q: -0.295457
 12400/100000: episode: 124, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -16.667, mean reward: -0.167 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.900, 10.338], loss: 0.003892, mae: 0.064272, mean_q: -0.287938
 12500/100000: episode: 125, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: -11.450, mean reward: -0.114 [-1.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.733, 10.165], loss: 0.004643, mae: 0.070573, mean_q: -0.264671
 12600/100000: episode: 126, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -19.659, mean reward: -0.197 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.840, 10.296], loss: 0.004045, mae: 0.067613, mean_q: -0.277519
 12700/100000: episode: 127, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.158, mean reward: -0.172 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.661, 10.098], loss: 0.003765, mae: 0.063297, mean_q: -0.314642
 12800/100000: episode: 128, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -20.241, mean reward: -0.202 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.723, 10.130], loss: 0.004020, mae: 0.065570, mean_q: -0.287035
 12900/100000: episode: 129, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -17.490, mean reward: -0.175 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.939, 10.330], loss: 0.004794, mae: 0.074001, mean_q: -0.286874
 13000/100000: episode: 130, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -17.045, mean reward: -0.170 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.836, 10.218], loss: 0.003691, mae: 0.063125, mean_q: -0.317212
 13100/100000: episode: 131, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.640, mean reward: -0.176 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.326, 10.098], loss: 0.006572, mae: 0.081117, mean_q: -0.297524
 13200/100000: episode: 132, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -17.167, mean reward: -0.172 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.646, 10.284], loss: 0.004268, mae: 0.069156, mean_q: -0.314010
 13300/100000: episode: 133, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -14.787, mean reward: -0.148 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.290, 10.098], loss: 0.003783, mae: 0.062988, mean_q: -0.307069
 13400/100000: episode: 134, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: -15.746, mean reward: -0.157 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.887, 10.098], loss: 0.004043, mae: 0.065806, mean_q: -0.284335
 13500/100000: episode: 135, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -16.584, mean reward: -0.166 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.482, 10.373], loss: 0.003628, mae: 0.062562, mean_q: -0.333289
 13600/100000: episode: 136, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -14.892, mean reward: -0.149 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.739, 10.098], loss: 0.003891, mae: 0.064600, mean_q: -0.266820
 13700/100000: episode: 137, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -13.324, mean reward: -0.133 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.993, 10.228], loss: 0.004536, mae: 0.066776, mean_q: -0.311611
 13800/100000: episode: 138, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -17.491, mean reward: -0.175 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.633, 10.143], loss: 0.003861, mae: 0.064259, mean_q: -0.329735
 13900/100000: episode: 139, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -15.775, mean reward: -0.158 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.800, 10.387], loss: 0.004111, mae: 0.068298, mean_q: -0.280988
 14000/100000: episode: 140, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -18.337, mean reward: -0.183 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.859, 10.433], loss: 0.003954, mae: 0.066217, mean_q: -0.261108
 14100/100000: episode: 141, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -18.334, mean reward: -0.183 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.270, 10.098], loss: 0.003951, mae: 0.066341, mean_q: -0.301036
 14200/100000: episode: 142, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.721, mean reward: -0.177 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.683, 10.275], loss: 0.003738, mae: 0.063218, mean_q: -0.293766
 14300/100000: episode: 143, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -17.136, mean reward: -0.171 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.772, 10.098], loss: 0.003939, mae: 0.065227, mean_q: -0.293620
 14400/100000: episode: 144, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -17.930, mean reward: -0.179 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.512, 10.098], loss: 0.005306, mae: 0.072967, mean_q: -0.301839
 14500/100000: episode: 145, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -17.454, mean reward: -0.175 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.392, 10.260], loss: 0.004025, mae: 0.066112, mean_q: -0.330277
 14600/100000: episode: 146, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: -12.274, mean reward: -0.123 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.129, 10.352], loss: 0.003417, mae: 0.060019, mean_q: -0.317147
 14700/100000: episode: 147, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.607, mean reward: -0.176 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.152, 10.098], loss: 0.003889, mae: 0.064715, mean_q: -0.309496
 14800/100000: episode: 148, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: -18.136, mean reward: -0.181 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.610, 10.176], loss: 0.003701, mae: 0.064021, mean_q: -0.315506
 14900/100000: episode: 149, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.412, mean reward: -0.174 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.888, 10.385], loss: 0.004437, mae: 0.071667, mean_q: -0.314365
[Info] 100-TH LEVEL FOUND: 0.42258572578430176, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.792s, episode steps: 100, steps per second: 21, episode reward: -20.653, mean reward: -0.207 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.435, 10.098], loss: 0.003357, mae: 0.060600, mean_q: -0.305463
 15010/100000: episode: 151, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 2.909, mean reward: 0.291 [0.217, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.314, 10.100], loss: 0.003606, mae: 0.061440, mean_q: -0.266994
 15018/100000: episode: 152, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 2.112, mean reward: 0.264 [0.210, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.761, 10.100], loss: 0.003352, mae: 0.058596, mean_q: -0.303287
 15028/100000: episode: 153, duration: 0.067s, episode steps: 10, steps per second: 149, episode reward: 1.445, mean reward: 0.145 [0.079, 0.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.231, 10.100], loss: 0.003465, mae: 0.060040, mean_q: -0.284854
 15072/100000: episode: 154, duration: 0.257s, episode steps: 44, steps per second: 171, episode reward: 8.772, mean reward: 0.199 [0.101, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.119, 10.100], loss: 0.003817, mae: 0.064949, mean_q: -0.285918
 15078/100000: episode: 155, duration: 0.046s, episode steps: 6, steps per second: 131, episode reward: 0.606, mean reward: 0.101 [0.012, 0.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.174, 10.103], loss: 0.003561, mae: 0.066045, mean_q: -0.213423
 15086/100000: episode: 156, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 2.186, mean reward: 0.273 [0.163, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.465, 10.100], loss: 0.006027, mae: 0.068831, mean_q: -0.306770
 15130/100000: episode: 157, duration: 0.251s, episode steps: 44, steps per second: 175, episode reward: 12.770, mean reward: 0.290 [0.117, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.539, 10.100], loss: 0.009771, mae: 0.093704, mean_q: -0.314657
 15138/100000: episode: 158, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 1.401, mean reward: 0.175 [0.031, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.100], loss: 0.004384, mae: 0.073691, mean_q: -0.122758
 15148/100000: episode: 159, duration: 0.073s, episode steps: 10, steps per second: 138, episode reward: 2.010, mean reward: 0.201 [0.100, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.203, 10.100], loss: 0.004229, mae: 0.070559, mean_q: -0.259297
 15160/100000: episode: 160, duration: 0.078s, episode steps: 12, steps per second: 155, episode reward: 6.307, mean reward: 0.526 [0.447, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.276, 10.100], loss: 0.003825, mae: 0.066841, mean_q: -0.239698
 15166/100000: episode: 161, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 1.166, mean reward: 0.194 [0.068, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.096, 10.177], loss: 0.004618, mae: 0.072303, mean_q: -0.360128
 15172/100000: episode: 162, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 0.850, mean reward: 0.142 [0.038, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.131, 10.138], loss: 0.003658, mae: 0.068790, mean_q: -0.226102
 15184/100000: episode: 163, duration: 0.076s, episode steps: 12, steps per second: 159, episode reward: 4.543, mean reward: 0.379 [0.206, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.233, 10.100], loss: 0.003820, mae: 0.067223, mean_q: -0.334037
 15191/100000: episode: 164, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 0.700, mean reward: 0.100 [0.009, 0.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.171], loss: 0.003540, mae: 0.061927, mean_q: -0.299781
 15203/100000: episode: 165, duration: 0.068s, episode steps: 12, steps per second: 175, episode reward: 5.066, mean reward: 0.422 [0.355, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.204, 10.100], loss: 0.003384, mae: 0.057205, mean_q: -0.354365
 15215/100000: episode: 166, duration: 0.079s, episode steps: 12, steps per second: 151, episode reward: 5.278, mean reward: 0.440 [0.353, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.849, 10.100], loss: 0.003636, mae: 0.061212, mean_q: -0.264452
 15222/100000: episode: 167, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 0.748, mean reward: 0.107 [0.059, 0.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.100], loss: 0.004568, mae: 0.069029, mean_q: -0.332946
 15230/100000: episode: 168, duration: 0.050s, episode steps: 8, steps per second: 162, episode reward: 1.445, mean reward: 0.181 [0.049, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.100], loss: 0.004675, mae: 0.070215, mean_q: -0.232875
 15238/100000: episode: 169, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 2.291, mean reward: 0.286 [0.153, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.390, 10.100], loss: 0.003981, mae: 0.064703, mean_q: -0.185687
 15246/100000: episode: 170, duration: 0.057s, episode steps: 8, steps per second: 140, episode reward: 1.113, mean reward: 0.139 [0.069, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.198, 10.100], loss: 0.004211, mae: 0.069268, mean_q: -0.224592
 15256/100000: episode: 171, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 2.498, mean reward: 0.250 [0.114, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.124, 10.100], loss: 0.003390, mae: 0.058199, mean_q: -0.400198
 15263/100000: episode: 172, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 0.506, mean reward: 0.072 [0.017, 0.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.241, 10.100], loss: 0.003897, mae: 0.062959, mean_q: -0.325787
 15277/100000: episode: 173, duration: 0.087s, episode steps: 14, steps per second: 160, episode reward: 4.035, mean reward: 0.288 [0.239, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.260, 10.100], loss: 0.003892, mae: 0.063519, mean_q: -0.270394
 15284/100000: episode: 174, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 0.523, mean reward: 0.075 [0.040, 0.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.100], loss: 0.004088, mae: 0.066919, mean_q: -0.292136
 15291/100000: episode: 175, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 0.522, mean reward: 0.075 [0.029, 0.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.105, 10.100], loss: 0.003368, mae: 0.062711, mean_q: -0.250187
 15303/100000: episode: 176, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 4.764, mean reward: 0.397 [0.298, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.375, 10.100], loss: 0.004289, mae: 0.067702, mean_q: -0.314664
 15317/100000: episode: 177, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 3.400, mean reward: 0.243 [0.049, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.838, 10.117], loss: 0.004134, mae: 0.066501, mean_q: -0.287936
 15361/100000: episode: 178, duration: 0.251s, episode steps: 44, steps per second: 175, episode reward: 10.135, mean reward: 0.230 [0.084, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-1.796, 10.100], loss: 0.004178, mae: 0.066708, mean_q: -0.268487
 15373/100000: episode: 179, duration: 0.086s, episode steps: 12, steps per second: 140, episode reward: 5.813, mean reward: 0.484 [0.404, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.344, 10.100], loss: 0.004513, mae: 0.070559, mean_q: -0.207841
 15417/100000: episode: 180, duration: 0.251s, episode steps: 44, steps per second: 175, episode reward: 10.221, mean reward: 0.232 [0.002, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.084, 10.128], loss: 0.004155, mae: 0.065235, mean_q: -0.304908
 15429/100000: episode: 181, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 5.636, mean reward: 0.470 [0.263, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.161, 10.100], loss: 0.003723, mae: 0.063092, mean_q: -0.277911
 15437/100000: episode: 182, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 1.358, mean reward: 0.170 [0.139, 0.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.603, 10.100], loss: 0.004707, mae: 0.071237, mean_q: -0.189105
 15447/100000: episode: 183, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 2.677, mean reward: 0.268 [0.172, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.112, 10.100], loss: 0.004783, mae: 0.070214, mean_q: -0.254772
 15459/100000: episode: 184, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 5.695, mean reward: 0.475 [0.385, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.575, 10.100], loss: 0.005027, mae: 0.071454, mean_q: -0.186041
 15467/100000: episode: 185, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 1.180, mean reward: 0.148 [0.048, 0.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.247, 10.147], loss: 0.004083, mae: 0.065307, mean_q: -0.310513
 15477/100000: episode: 186, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 2.483, mean reward: 0.248 [0.141, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.282, 10.100], loss: 0.005315, mae: 0.074470, mean_q: -0.214230
 15521/100000: episode: 187, duration: 0.256s, episode steps: 44, steps per second: 172, episode reward: 13.095, mean reward: 0.298 [0.049, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.246, 10.242], loss: 0.004505, mae: 0.067775, mean_q: -0.237930
 15527/100000: episode: 188, duration: 0.042s, episode steps: 6, steps per second: 143, episode reward: 1.390, mean reward: 0.232 [0.185, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.256], loss: 0.005011, mae: 0.075804, mean_q: -0.125361
 15534/100000: episode: 189, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 0.792, mean reward: 0.113 [0.032, 0.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.037, 10.133], loss: 0.005108, mae: 0.071307, mean_q: -0.195565
 15544/100000: episode: 190, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 2.550, mean reward: 0.255 [0.201, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.324, 10.100], loss: 0.004855, mae: 0.073239, mean_q: -0.156916
 15550/100000: episode: 191, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 1.252, mean reward: 0.209 [0.064, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.974, 10.162], loss: 0.004921, mae: 0.072879, mean_q: -0.261935
 15558/100000: episode: 192, duration: 0.044s, episode steps: 8, steps per second: 184, episode reward: 1.856, mean reward: 0.232 [0.160, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.204, 10.100], loss: 0.004453, mae: 0.069110, mean_q: -0.157950
 15564/100000: episode: 193, duration: 0.044s, episode steps: 6, steps per second: 135, episode reward: 0.997, mean reward: 0.166 [0.104, 0.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.177], loss: 0.005204, mae: 0.073131, mean_q: -0.188268
 15571/100000: episode: 194, duration: 0.050s, episode steps: 7, steps per second: 139, episode reward: 0.981, mean reward: 0.140 [0.056, 0.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.699, 10.100], loss: 0.004222, mae: 0.070605, mean_q: -0.143261
 15579/100000: episode: 195, duration: 0.057s, episode steps: 8, steps per second: 141, episode reward: 2.402, mean reward: 0.300 [0.215, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.466, 10.100], loss: 0.005128, mae: 0.073967, mean_q: -0.128491
 15585/100000: episode: 196, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 1.057, mean reward: 0.176 [0.072, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.050, 10.121], loss: 0.004631, mae: 0.072113, mean_q: -0.156268
 15599/100000: episode: 197, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 4.731, mean reward: 0.338 [0.221, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.242, 10.100], loss: 0.004538, mae: 0.069463, mean_q: -0.252054
 15613/100000: episode: 198, duration: 0.097s, episode steps: 14, steps per second: 144, episode reward: 3.541, mean reward: 0.253 [0.185, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.350, 10.100], loss: 0.005321, mae: 0.074514, mean_q: -0.275288
 15657/100000: episode: 199, duration: 0.246s, episode steps: 44, steps per second: 179, episode reward: 11.380, mean reward: 0.259 [0.049, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.407, 10.100], loss: 0.004891, mae: 0.073625, mean_q: -0.137282
 15671/100000: episode: 200, duration: 0.093s, episode steps: 14, steps per second: 150, episode reward: 4.142, mean reward: 0.296 [0.226, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.177, 10.100], loss: 0.005277, mae: 0.077136, mean_q: -0.071089
 15681/100000: episode: 201, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 2.456, mean reward: 0.246 [0.196, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.748, 10.100], loss: 0.005751, mae: 0.078012, mean_q: -0.231320
 15689/100000: episode: 202, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 1.997, mean reward: 0.250 [0.202, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.221, 10.100], loss: 0.007387, mae: 0.090315, mean_q: -0.242101
 15695/100000: episode: 203, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 0.810, mean reward: 0.135 [0.039, 0.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.354, 10.126], loss: 0.006329, mae: 0.091537, mean_q: -0.165121
 15703/100000: episode: 204, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 0.893, mean reward: 0.112 [0.028, 0.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.087, 10.149], loss: 0.005045, mae: 0.077627, mean_q: -0.152127
 15709/100000: episode: 205, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 1.017, mean reward: 0.170 [0.035, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.512, 10.135], loss: 0.005547, mae: 0.075713, mean_q: -0.275727
 15723/100000: episode: 206, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 3.199, mean reward: 0.228 [0.176, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.824, 10.100], loss: 0.005451, mae: 0.082045, mean_q: -0.251080
 15729/100000: episode: 207, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 0.902, mean reward: 0.150 [0.100, 0.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.535, 10.127], loss: 0.004704, mae: 0.073339, mean_q: -0.159683
 15773/100000: episode: 208, duration: 0.274s, episode steps: 44, steps per second: 160, episode reward: 10.411, mean reward: 0.237 [0.101, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.465, 10.100], loss: 0.004735, mae: 0.071316, mean_q: -0.224172
 15785/100000: episode: 209, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 5.037, mean reward: 0.420 [0.360, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.280, 10.100], loss: 0.004059, mae: 0.066405, mean_q: -0.176607
 15791/100000: episode: 210, duration: 0.044s, episode steps: 6, steps per second: 138, episode reward: 1.398, mean reward: 0.233 [0.073, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.814, 10.174], loss: 0.003258, mae: 0.059122, mean_q: -0.258132
 15799/100000: episode: 211, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 1.736, mean reward: 0.217 [0.172, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.130, 10.100], loss: 0.004241, mae: 0.067667, mean_q: -0.211484
 15813/100000: episode: 212, duration: 0.088s, episode steps: 14, steps per second: 159, episode reward: 5.020, mean reward: 0.359 [0.225, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.454, 10.100], loss: 0.005112, mae: 0.072814, mean_q: -0.178608
 15819/100000: episode: 213, duration: 0.040s, episode steps: 6, steps per second: 152, episode reward: 1.210, mean reward: 0.202 [0.062, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.172], loss: 0.004598, mae: 0.069616, mean_q: -0.052528
 15827/100000: episode: 214, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 1.547, mean reward: 0.193 [0.072, 0.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.136], loss: 0.005041, mae: 0.076150, mean_q: -0.015778
 15833/100000: episode: 215, duration: 0.042s, episode steps: 6, steps per second: 143, episode reward: 0.905, mean reward: 0.151 [0.124, 0.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.043, 10.153], loss: 0.004100, mae: 0.064560, mean_q: -0.180327
 15841/100000: episode: 216, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 1.982, mean reward: 0.248 [0.191, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.716, 10.100], loss: 0.005299, mae: 0.075207, mean_q: -0.134523
 15848/100000: episode: 217, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 1.277, mean reward: 0.182 [0.074, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.157], loss: 0.004096, mae: 0.069152, mean_q: -0.115717
 15862/100000: episode: 218, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 3.235, mean reward: 0.231 [0.165, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.670, 10.100], loss: 0.005499, mae: 0.077734, mean_q: -0.087200
 15870/100000: episode: 219, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 1.387, mean reward: 0.173 [0.072, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.111, 10.100], loss: 0.004460, mae: 0.073645, mean_q: -0.134296
 15876/100000: episode: 220, duration: 0.045s, episode steps: 6, steps per second: 133, episode reward: 1.106, mean reward: 0.184 [0.105, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.100], loss: 0.005010, mae: 0.082751, mean_q: 0.018590
 15886/100000: episode: 221, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 1.951, mean reward: 0.195 [0.118, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.271, 10.100], loss: 0.004235, mae: 0.070683, mean_q: -0.189707
 15894/100000: episode: 222, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 2.185, mean reward: 0.273 [0.174, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.513, 10.100], loss: 0.004829, mae: 0.073619, mean_q: -0.159790
 15901/100000: episode: 223, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 1.086, mean reward: 0.155 [0.067, 0.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.181], loss: 0.004266, mae: 0.067786, mean_q: -0.336134
 15909/100000: episode: 224, duration: 0.055s, episode steps: 8, steps per second: 146, episode reward: 1.390, mean reward: 0.174 [0.067, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.045, 10.100], loss: 0.004657, mae: 0.067401, mean_q: -0.252520
 15923/100000: episode: 225, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 4.551, mean reward: 0.325 [0.240, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.527, 10.100], loss: 0.003873, mae: 0.064165, mean_q: -0.199752
 15933/100000: episode: 226, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 2.116, mean reward: 0.212 [0.132, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.362, 10.100], loss: 0.004428, mae: 0.068940, mean_q: -0.146235
 15977/100000: episode: 227, duration: 0.278s, episode steps: 44, steps per second: 158, episode reward: 11.963, mean reward: 0.272 [0.030, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.965, 10.100], loss: 0.004640, mae: 0.069867, mean_q: -0.205099
 15987/100000: episode: 228, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 2.109, mean reward: 0.211 [0.147, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.185, 10.100], loss: 0.004971, mae: 0.073011, mean_q: -0.159869
 16031/100000: episode: 229, duration: 0.266s, episode steps: 44, steps per second: 166, episode reward: 14.171, mean reward: 0.322 [0.220, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-1.445, 10.100], loss: 0.004358, mae: 0.067900, mean_q: -0.201251
 16045/100000: episode: 230, duration: 0.096s, episode steps: 14, steps per second: 146, episode reward: 4.340, mean reward: 0.310 [0.195, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.133, 10.100], loss: 0.003984, mae: 0.064188, mean_q: -0.169405
 16051/100000: episode: 231, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 0.438, mean reward: 0.073 [0.034, 0.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.658, 10.133], loss: 0.004661, mae: 0.070865, mean_q: -0.098610
 16059/100000: episode: 232, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 1.397, mean reward: 0.175 [0.075, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.120, 10.100], loss: 0.004124, mae: 0.067754, mean_q: -0.096280
 16067/100000: episode: 233, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 1.820, mean reward: 0.227 [0.048, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-1.090, 10.100], loss: 0.003901, mae: 0.061769, mean_q: -0.148330
 16077/100000: episode: 234, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 2.921, mean reward: 0.292 [0.212, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.195, 10.100], loss: 0.004728, mae: 0.073670, mean_q: -0.124995
 16091/100000: episode: 235, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 4.240, mean reward: 0.303 [0.227, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.241, 10.100], loss: 0.004803, mae: 0.072953, mean_q: -0.072467
 16099/100000: episode: 236, duration: 0.057s, episode steps: 8, steps per second: 140, episode reward: 1.449, mean reward: 0.181 [0.157, 0.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.242, 10.100], loss: 0.005258, mae: 0.074972, mean_q: -0.139726
 16109/100000: episode: 237, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 1.902, mean reward: 0.190 [0.064, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.036, 10.100], loss: 0.004219, mae: 0.068048, mean_q: -0.216306
 16116/100000: episode: 238, duration: 0.049s, episode steps: 7, steps per second: 143, episode reward: 1.245, mean reward: 0.178 [0.033, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.058, 10.115], loss: 0.004642, mae: 0.068570, mean_q: -0.106342
 16130/100000: episode: 239, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 3.373, mean reward: 0.241 [0.084, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.436, 10.100], loss: 0.004434, mae: 0.069218, mean_q: -0.071441
[Info] 200-TH LEVEL FOUND: 0.6002505421638489, Considering 10/90 traces
 16144/100000: episode: 240, duration: 4.395s, episode steps: 14, steps per second: 3, episode reward: 3.567, mean reward: 0.255 [0.155, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.247, 10.100], loss: 0.004634, mae: 0.072624, mean_q: -0.026449
 16156/100000: episode: 241, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 6.052, mean reward: 0.504 [0.381, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.460, 10.100], loss: 0.004659, mae: 0.072985, mean_q: -0.059925
 16168/100000: episode: 242, duration: 0.087s, episode steps: 12, steps per second: 139, episode reward: 4.702, mean reward: 0.392 [0.328, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.237, 10.100], loss: 0.004757, mae: 0.072985, mean_q: -0.114884
 16180/100000: episode: 243, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 6.101, mean reward: 0.508 [0.378, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.434, 10.100], loss: 0.005200, mae: 0.075655, mean_q: -0.081132
 16192/100000: episode: 244, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 5.471, mean reward: 0.456 [0.371, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.364, 10.100], loss: 0.007826, mae: 0.087687, mean_q: -0.130112
 16204/100000: episode: 245, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 7.194, mean reward: 0.599 [0.556, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.516, 10.100], loss: 0.010827, mae: 0.103985, mean_q: -0.098957
 16216/100000: episode: 246, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 5.591, mean reward: 0.466 [0.318, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.955, 10.100], loss: 0.006301, mae: 0.090671, mean_q: -0.073832
 16228/100000: episode: 247, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 5.448, mean reward: 0.454 [0.368, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.397, 10.100], loss: 0.006533, mae: 0.084497, mean_q: -0.127684
 16240/100000: episode: 248, duration: 0.065s, episode steps: 12, steps per second: 183, episode reward: 6.334, mean reward: 0.528 [0.438, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.406, 10.100], loss: 0.005552, mae: 0.082524, mean_q: -0.143978
 16252/100000: episode: 249, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 5.173, mean reward: 0.431 [0.323, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.224, 10.100], loss: 0.005637, mae: 0.083553, mean_q: -0.063161
 16264/100000: episode: 250, duration: 0.079s, episode steps: 12, steps per second: 152, episode reward: 4.442, mean reward: 0.370 [0.263, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.316, 10.100], loss: 0.004586, mae: 0.073867, mean_q: -0.110034
 16276/100000: episode: 251, duration: 0.083s, episode steps: 12, steps per second: 145, episode reward: 6.263, mean reward: 0.522 [0.450, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.340, 10.100], loss: 0.004830, mae: 0.074000, mean_q: -0.121485
 16288/100000: episode: 252, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 6.004, mean reward: 0.500 [0.419, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.482, 10.100], loss: 0.005728, mae: 0.081606, mean_q: -0.096022
 16300/100000: episode: 253, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 5.200, mean reward: 0.433 [0.342, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.372, 10.100], loss: 0.005016, mae: 0.073154, mean_q: -0.119865
 16312/100000: episode: 254, duration: 0.079s, episode steps: 12, steps per second: 152, episode reward: 4.568, mean reward: 0.381 [0.297, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.309, 10.100], loss: 0.004711, mae: 0.071317, mean_q: -0.101143
 16324/100000: episode: 255, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 5.192, mean reward: 0.433 [0.341, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.319, 10.100], loss: 0.004647, mae: 0.072469, mean_q: -0.054049
 16336/100000: episode: 256, duration: 0.077s, episode steps: 12, steps per second: 155, episode reward: 6.311, mean reward: 0.526 [0.459, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.535, 10.100], loss: 0.004554, mae: 0.073075, mean_q: -0.065388
 16348/100000: episode: 257, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 5.215, mean reward: 0.435 [0.329, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.221, 10.100], loss: 0.004669, mae: 0.071443, mean_q: -0.094783
 16360/100000: episode: 258, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 6.032, mean reward: 0.503 [0.410, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.719, 10.100], loss: 0.005597, mae: 0.074655, mean_q: -0.149923
 16372/100000: episode: 259, duration: 0.077s, episode steps: 12, steps per second: 157, episode reward: 5.190, mean reward: 0.432 [0.333, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.401, 10.100], loss: 0.006104, mae: 0.085688, mean_q: -0.037650
 16384/100000: episode: 260, duration: 0.079s, episode steps: 12, steps per second: 153, episode reward: 5.436, mean reward: 0.453 [0.393, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.614, 10.100], loss: 0.005400, mae: 0.084081, mean_q: -0.079191
 16396/100000: episode: 261, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 5.935, mean reward: 0.495 [0.414, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.306, 10.100], loss: 0.004451, mae: 0.075756, mean_q: -0.028888
 16408/100000: episode: 262, duration: 0.084s, episode steps: 12, steps per second: 143, episode reward: 4.698, mean reward: 0.391 [0.301, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.468, 10.100], loss: 0.004777, mae: 0.078409, mean_q: -0.008087
 16420/100000: episode: 263, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 4.841, mean reward: 0.403 [0.364, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.297, 10.100], loss: 0.004891, mae: 0.076839, mean_q: -0.023374
 16432/100000: episode: 264, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 5.695, mean reward: 0.475 [0.307, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.359, 10.100], loss: 0.005028, mae: 0.075536, mean_q: -0.134472
 16444/100000: episode: 265, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 5.185, mean reward: 0.432 [0.306, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.140, 10.100], loss: 0.004102, mae: 0.066703, mean_q: -0.087744
 16456/100000: episode: 266, duration: 0.081s, episode steps: 12, steps per second: 148, episode reward: 5.294, mean reward: 0.441 [0.300, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.276, 10.100], loss: 0.004986, mae: 0.076746, mean_q: 0.009538
 16468/100000: episode: 267, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 5.435, mean reward: 0.453 [0.392, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.334, 10.100], loss: 0.004113, mae: 0.067425, mean_q: -0.024942
 16480/100000: episode: 268, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 4.923, mean reward: 0.410 [0.315, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.384, 10.100], loss: 0.004198, mae: 0.068854, mean_q: -0.069806
 16492/100000: episode: 269, duration: 0.068s, episode steps: 12, steps per second: 178, episode reward: 5.457, mean reward: 0.455 [0.362, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.350, 10.100], loss: 0.004687, mae: 0.073821, mean_q: 0.025395
 16504/100000: episode: 270, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 5.769, mean reward: 0.481 [0.396, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.813, 10.100], loss: 0.004549, mae: 0.072606, mean_q: -0.039425
 16516/100000: episode: 271, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 6.174, mean reward: 0.514 [0.406, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.315, 10.100], loss: 0.004077, mae: 0.066899, mean_q: -0.139097
 16528/100000: episode: 272, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 5.573, mean reward: 0.464 [0.311, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.314, 10.100], loss: 0.004093, mae: 0.068511, mean_q: 0.036700
 16540/100000: episode: 273, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 5.914, mean reward: 0.493 [0.413, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.409, 10.100], loss: 0.004228, mae: 0.068769, mean_q: -0.097731
 16552/100000: episode: 274, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 5.506, mean reward: 0.459 [0.378, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.730, 10.100], loss: 0.004409, mae: 0.069554, mean_q: 0.032911
 16564/100000: episode: 275, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 5.095, mean reward: 0.425 [0.300, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.314, 10.100], loss: 0.003782, mae: 0.065507, mean_q: -0.012209
 16576/100000: episode: 276, duration: 0.080s, episode steps: 12, steps per second: 150, episode reward: 5.153, mean reward: 0.429 [0.327, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.538, 10.100], loss: 0.005060, mae: 0.075954, mean_q: 0.017146
 16588/100000: episode: 277, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 6.376, mean reward: 0.531 [0.459, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.774, 10.100], loss: 0.004434, mae: 0.067682, mean_q: -0.115395
 16600/100000: episode: 278, duration: 0.080s, episode steps: 12, steps per second: 151, episode reward: 4.971, mean reward: 0.414 [0.287, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.234, 10.100], loss: 0.004024, mae: 0.063488, mean_q: -0.062232
 16612/100000: episode: 279, duration: 0.099s, episode steps: 12, steps per second: 122, episode reward: 5.379, mean reward: 0.448 [0.354, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.160, 10.100], loss: 0.003884, mae: 0.065159, mean_q: -0.068852
 16624/100000: episode: 280, duration: 0.119s, episode steps: 12, steps per second: 100, episode reward: 6.122, mean reward: 0.510 [0.475, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.347, 10.100], loss: 0.005183, mae: 0.076719, mean_q: 0.046978
 16636/100000: episode: 281, duration: 0.103s, episode steps: 12, steps per second: 117, episode reward: 5.667, mean reward: 0.472 [0.381, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.361, 10.100], loss: 0.005268, mae: 0.075852, mean_q: -0.057891
 16648/100000: episode: 282, duration: 0.098s, episode steps: 12, steps per second: 122, episode reward: 5.193, mean reward: 0.433 [0.377, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.334, 10.100], loss: 0.003834, mae: 0.066669, mean_q: -0.120925
 16660/100000: episode: 283, duration: 0.083s, episode steps: 12, steps per second: 144, episode reward: 6.006, mean reward: 0.501 [0.414, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.752, 10.100], loss: 0.005517, mae: 0.077204, mean_q: 0.037250
 16672/100000: episode: 284, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 6.533, mean reward: 0.544 [0.502, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.416, 10.100], loss: 0.004520, mae: 0.069910, mean_q: 0.006446
 16684/100000: episode: 285, duration: 0.085s, episode steps: 12, steps per second: 142, episode reward: 4.720, mean reward: 0.393 [0.300, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.271, 10.100], loss: 0.004574, mae: 0.071210, mean_q: 0.005782
 16696/100000: episode: 286, duration: 0.084s, episode steps: 12, steps per second: 143, episode reward: 5.622, mean reward: 0.469 [0.379, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.323, 10.100], loss: 0.004398, mae: 0.068981, mean_q: 0.033697
 16708/100000: episode: 287, duration: 0.080s, episode steps: 12, steps per second: 151, episode reward: 4.865, mean reward: 0.405 [0.385, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.374, 10.100], loss: 0.004574, mae: 0.072644, mean_q: 0.064536
 16720/100000: episode: 288, duration: 0.081s, episode steps: 12, steps per second: 149, episode reward: 6.051, mean reward: 0.504 [0.427, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.400, 10.100], loss: 0.004886, mae: 0.074663, mean_q: 0.078675
 16732/100000: episode: 289, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 6.763, mean reward: 0.564 [0.463, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.427, 10.100], loss: 0.003810, mae: 0.061825, mean_q: -0.022750
 16744/100000: episode: 290, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 3.869, mean reward: 0.322 [0.185, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.572, 10.100], loss: 0.004399, mae: 0.068773, mean_q: -0.043760
 16756/100000: episode: 291, duration: 0.081s, episode steps: 12, steps per second: 148, episode reward: 5.808, mean reward: 0.484 [0.374, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.652, 10.100], loss: 0.003785, mae: 0.065546, mean_q: -0.091640
 16768/100000: episode: 292, duration: 0.079s, episode steps: 12, steps per second: 151, episode reward: 5.228, mean reward: 0.436 [0.347, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.485, 10.100], loss: 0.003821, mae: 0.065326, mean_q: 0.063713
 16780/100000: episode: 293, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 5.563, mean reward: 0.464 [0.432, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.630, 10.100], loss: 0.004362, mae: 0.071227, mean_q: 0.094838
 16792/100000: episode: 294, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 4.973, mean reward: 0.414 [0.270, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.164, 10.100], loss: 0.004530, mae: 0.072012, mean_q: 0.065435
 16804/100000: episode: 295, duration: 0.111s, episode steps: 12, steps per second: 108, episode reward: 5.631, mean reward: 0.469 [0.356, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.341, 10.100], loss: 0.004935, mae: 0.074129, mean_q: 0.056051
 16816/100000: episode: 296, duration: 0.190s, episode steps: 12, steps per second: 63, episode reward: 5.628, mean reward: 0.469 [0.416, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.346, 10.100], loss: 0.004201, mae: 0.064735, mean_q: -0.049845
 16828/100000: episode: 297, duration: 0.107s, episode steps: 12, steps per second: 112, episode reward: 5.801, mean reward: 0.483 [0.413, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.382, 10.100], loss: 0.004753, mae: 0.072008, mean_q: -0.034263
[Info] FALSIFICATION!
 16837/100000: episode: 298, duration: 0.072s, episode steps: 9, steps per second: 125, episode reward: 14.110, mean reward: 1.568 [0.484, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.266, 10.082], loss: 0.004549, mae: 0.072404, mean_q: 0.036565
 16937/100000: episode: 299, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: -14.856, mean reward: -0.149 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.154, 10.098], loss: 0.021336, mae: 0.093691, mean_q: 0.025546
 17037/100000: episode: 300, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: -18.564, mean reward: -0.186 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.613, 10.098], loss: 0.004121, mae: 0.067173, mean_q: 0.057844
 17137/100000: episode: 301, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.515, mean reward: -0.165 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.923, 10.144], loss: 0.004166, mae: 0.067443, mean_q: 0.051869
 17237/100000: episode: 302, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -14.746, mean reward: -0.147 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.070, 10.520], loss: 0.035745, mae: 0.099644, mean_q: 0.028448
 17337/100000: episode: 303, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -12.953, mean reward: -0.130 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.725, 10.098], loss: 0.007482, mae: 0.085716, mean_q: 0.007778
 17437/100000: episode: 304, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.865, mean reward: -0.189 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.596, 10.221], loss: 0.003873, mae: 0.064809, mean_q: 0.050826
 17537/100000: episode: 305, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.001, mean reward: -0.180 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.897, 10.250], loss: 0.018896, mae: 0.079280, mean_q: 0.036064
 17637/100000: episode: 306, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.469, mean reward: -0.165 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.491, 10.098], loss: 0.005618, mae: 0.075410, mean_q: 0.052623
 17737/100000: episode: 307, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.127, mean reward: -0.191 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.733, 10.098], loss: 0.035527, mae: 0.100983, mean_q: 0.025379
 17837/100000: episode: 308, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.597, mean reward: -0.186 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.509, 10.234], loss: 0.018362, mae: 0.074876, mean_q: 0.035663
 17937/100000: episode: 309, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -16.021, mean reward: -0.160 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.531, 10.098], loss: 0.018233, mae: 0.073904, mean_q: 0.024724
 18037/100000: episode: 310, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.119, mean reward: -0.181 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.156, 10.142], loss: 0.019611, mae: 0.083962, mean_q: 0.053424
 18137/100000: episode: 311, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.674, mean reward: -0.187 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.499, 10.238], loss: 0.019826, mae: 0.084802, mean_q: 0.049003
 18237/100000: episode: 312, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -14.189, mean reward: -0.142 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.323, 10.176], loss: 0.019630, mae: 0.084342, mean_q: 0.060198
 18337/100000: episode: 313, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.581, mean reward: -0.156 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.995, 10.193], loss: 0.004153, mae: 0.065877, mean_q: 0.045619
 18437/100000: episode: 314, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -12.564, mean reward: -0.126 [-1.000, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.187, 10.098], loss: 0.003517, mae: 0.061946, mean_q: 0.034846
 18537/100000: episode: 315, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.800, mean reward: -0.188 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.748, 10.165], loss: 0.033042, mae: 0.089244, mean_q: 0.028318
 18637/100000: episode: 316, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.131, mean reward: -0.161 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.701, 10.098], loss: 0.032795, mae: 0.085037, mean_q: 0.054919
 18737/100000: episode: 317, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.731, mean reward: -0.167 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.064, 10.103], loss: 0.031454, mae: 0.076513, mean_q: 0.045197
 18837/100000: episode: 318, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.384, mean reward: -0.184 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.712, 10.150], loss: 0.031143, mae: 0.076135, mean_q: 0.050878
 18937/100000: episode: 319, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.988, mean reward: -0.160 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.947, 10.098], loss: 0.003702, mae: 0.061876, mean_q: 0.038208
 19037/100000: episode: 320, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.850, mean reward: -0.188 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.123, 10.098], loss: 0.017891, mae: 0.072158, mean_q: 0.043416
 19137/100000: episode: 321, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -20.723, mean reward: -0.207 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.622, 10.151], loss: 0.005875, mae: 0.073576, mean_q: 0.058983
 19237/100000: episode: 322, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -13.435, mean reward: -0.134 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.748, 10.098], loss: 0.005363, mae: 0.070784, mean_q: 0.052431
 19337/100000: episode: 323, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.822, mean reward: -0.178 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.573, 10.098], loss: 0.018329, mae: 0.074442, mean_q: 0.023970
 19437/100000: episode: 324, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.391, mean reward: -0.194 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.659, 10.098], loss: 0.018870, mae: 0.077616, mean_q: 0.050412
 19537/100000: episode: 325, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -18.763, mean reward: -0.188 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.226, 10.098], loss: 0.003938, mae: 0.064077, mean_q: 0.052650
 19637/100000: episode: 326, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -18.403, mean reward: -0.184 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.131, 10.278], loss: 0.003701, mae: 0.063264, mean_q: 0.045903
 19737/100000: episode: 327, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -20.337, mean reward: -0.203 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.375, 10.158], loss: 0.020189, mae: 0.079074, mean_q: 0.035893
 19837/100000: episode: 328, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -19.937, mean reward: -0.199 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.268, 10.098], loss: 0.004553, mae: 0.070538, mean_q: 0.062736
 19937/100000: episode: 329, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.300, mean reward: -0.163 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.570, 10.395], loss: 0.018966, mae: 0.076102, mean_q: -0.008540
 20037/100000: episode: 330, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -19.910, mean reward: -0.199 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.323, 10.142], loss: 0.004020, mae: 0.064434, mean_q: -0.001812
 20137/100000: episode: 331, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.792, mean reward: -0.188 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.586, 10.177], loss: 0.003512, mae: 0.061042, mean_q: -0.005277
 20237/100000: episode: 332, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.142, mean reward: -0.181 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.986, 10.166], loss: 0.018408, mae: 0.075363, mean_q: -0.035561
 20337/100000: episode: 333, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.191, mean reward: -0.182 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.404, 10.115], loss: 0.003349, mae: 0.059893, mean_q: -0.041633
 20437/100000: episode: 334, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -18.424, mean reward: -0.184 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.061, 10.226], loss: 0.017739, mae: 0.069662, mean_q: -0.091428
 20537/100000: episode: 335, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -20.533, mean reward: -0.205 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.937, 10.158], loss: 0.003991, mae: 0.064952, mean_q: -0.077171
 20637/100000: episode: 336, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -18.279, mean reward: -0.183 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.734, 10.274], loss: 0.002918, mae: 0.055391, mean_q: -0.108202
 20737/100000: episode: 337, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.264, mean reward: -0.163 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.442, 10.098], loss: 0.003360, mae: 0.058625, mean_q: -0.130362
 20837/100000: episode: 338, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.776, mean reward: -0.158 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.173, 10.380], loss: 0.003442, mae: 0.060469, mean_q: -0.108763
 20937/100000: episode: 339, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -19.749, mean reward: -0.197 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.430, 10.187], loss: 0.017473, mae: 0.067836, mean_q: -0.161626
 21037/100000: episode: 340, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.566, mean reward: -0.176 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.486, 10.098], loss: 0.018593, mae: 0.072828, mean_q: -0.133881
 21137/100000: episode: 341, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -19.588, mean reward: -0.196 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.548, 10.098], loss: 0.003116, mae: 0.055479, mean_q: -0.157003
 21237/100000: episode: 342, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.577, mean reward: -0.166 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.447, 10.098], loss: 0.032314, mae: 0.077414, mean_q: -0.174136
 21337/100000: episode: 343, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -15.061, mean reward: -0.151 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.090, 10.098], loss: 0.005716, mae: 0.073961, mean_q: -0.215237
 21437/100000: episode: 344, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -15.403, mean reward: -0.154 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.399, 10.098], loss: 0.031578, mae: 0.076997, mean_q: -0.290386
 21537/100000: episode: 345, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -13.222, mean reward: -0.132 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.878, 10.355], loss: 0.003106, mae: 0.057182, mean_q: -0.287335
 21637/100000: episode: 346, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.458, mean reward: -0.185 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.194, 10.108], loss: 0.016729, mae: 0.060649, mean_q: -0.296075
 21737/100000: episode: 347, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -11.525, mean reward: -0.115 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.767, 10.566], loss: 0.030013, mae: 0.069470, mean_q: -0.269264
 21837/100000: episode: 348, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.077, mean reward: -0.181 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.850, 10.260], loss: 0.003096, mae: 0.055055, mean_q: -0.348955
 21937/100000: episode: 349, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -8.514, mean reward: -0.085 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.848, 10.098], loss: 0.002837, mae: 0.053980, mean_q: -0.314049
 22037/100000: episode: 350, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.726, mean reward: -0.197 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.109, 10.182], loss: 0.002818, mae: 0.052738, mean_q: -0.332439
 22137/100000: episode: 351, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -19.738, mean reward: -0.197 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.402, 10.194], loss: 0.002553, mae: 0.051443, mean_q: -0.323767
 22237/100000: episode: 352, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -18.538, mean reward: -0.185 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.686, 10.098], loss: 0.002732, mae: 0.055071, mean_q: -0.339321
 22337/100000: episode: 353, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.013, mean reward: -0.170 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.376, 10.098], loss: 0.002611, mae: 0.051741, mean_q: -0.298114
 22437/100000: episode: 354, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.913, mean reward: -0.189 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.223, 10.248], loss: 0.002538, mae: 0.051325, mean_q: -0.347087
 22537/100000: episode: 355, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.962, mean reward: -0.180 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.923, 10.428], loss: 0.002638, mae: 0.051168, mean_q: -0.318792
 22637/100000: episode: 356, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.333, mean reward: -0.193 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.705, 10.333], loss: 0.003403, mae: 0.058100, mean_q: -0.326389
 22737/100000: episode: 357, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.520, mean reward: -0.155 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.290, 10.289], loss: 0.004711, mae: 0.066897, mean_q: -0.310601
 22837/100000: episode: 358, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -19.256, mean reward: -0.193 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.277, 10.098], loss: 0.002682, mae: 0.052898, mean_q: -0.323067
 22937/100000: episode: 359, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.874, mean reward: -0.179 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.950, 10.149], loss: 0.002669, mae: 0.051516, mean_q: -0.361415
 23037/100000: episode: 360, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -21.061, mean reward: -0.211 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.116, 10.098], loss: 0.002560, mae: 0.050462, mean_q: -0.350982
 23137/100000: episode: 361, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.089, mean reward: -0.181 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.876, 10.098], loss: 0.002532, mae: 0.051153, mean_q: -0.304773
 23237/100000: episode: 362, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.440, mean reward: -0.174 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.362, 10.098], loss: 0.002804, mae: 0.053406, mean_q: -0.336272
 23337/100000: episode: 363, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.936, mean reward: -0.169 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.201, 10.394], loss: 0.002548, mae: 0.050626, mean_q: -0.328954
 23437/100000: episode: 364, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -14.467, mean reward: -0.145 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.075, 10.139], loss: 0.003021, mae: 0.057388, mean_q: -0.297519
 23537/100000: episode: 365, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.430, mean reward: -0.174 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.345, 10.098], loss: 0.002430, mae: 0.049899, mean_q: -0.322669
 23637/100000: episode: 366, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -18.477, mean reward: -0.185 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.544, 10.098], loss: 0.002499, mae: 0.050364, mean_q: -0.322153
 23737/100000: episode: 367, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.964, mean reward: -0.200 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.205, 10.133], loss: 0.002616, mae: 0.051357, mean_q: -0.344962
 23837/100000: episode: 368, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -13.515, mean reward: -0.135 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.948, 10.130], loss: 0.005168, mae: 0.068205, mean_q: -0.318120
 23937/100000: episode: 369, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -12.772, mean reward: -0.128 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.679, 10.098], loss: 0.002837, mae: 0.055546, mean_q: -0.325933
 24037/100000: episode: 370, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -15.720, mean reward: -0.157 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.624, 10.098], loss: 0.002642, mae: 0.051926, mean_q: -0.323814
 24137/100000: episode: 371, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.994, mean reward: -0.180 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.102, 10.188], loss: 0.002787, mae: 0.054706, mean_q: -0.320927
 24237/100000: episode: 372, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.694, mean reward: -0.137 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.992, 10.098], loss: 0.002634, mae: 0.053207, mean_q: -0.316511
 24337/100000: episode: 373, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -21.370, mean reward: -0.214 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.839, 10.098], loss: 0.002687, mae: 0.052121, mean_q: -0.323314
 24437/100000: episode: 374, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.706, mean reward: -0.157 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.790, 10.244], loss: 0.004439, mae: 0.065549, mean_q: -0.390859
 24537/100000: episode: 375, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.317, mean reward: -0.183 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.627, 10.098], loss: 0.002673, mae: 0.053329, mean_q: -0.335243
 24637/100000: episode: 376, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -16.813, mean reward: -0.168 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.256, 10.098], loss: 0.002616, mae: 0.051776, mean_q: -0.298433
 24737/100000: episode: 377, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.836, mean reward: -0.178 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.440, 10.143], loss: 0.004687, mae: 0.065408, mean_q: -0.324755
 24837/100000: episode: 378, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -16.935, mean reward: -0.169 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.008, 10.344], loss: 0.003016, mae: 0.057532, mean_q: -0.321417
 24937/100000: episode: 379, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -20.936, mean reward: -0.209 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.951, 10.166], loss: 0.004122, mae: 0.061967, mean_q: -0.348230
 25037/100000: episode: 380, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.114, mean reward: -0.151 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.788, 10.098], loss: 0.002550, mae: 0.051382, mean_q: -0.328632
 25137/100000: episode: 381, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -20.640, mean reward: -0.206 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.653, 10.098], loss: 0.002804, mae: 0.053015, mean_q: -0.337572
 25237/100000: episode: 382, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.397, mean reward: -0.154 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.726, 10.332], loss: 0.002423, mae: 0.049986, mean_q: -0.348723
 25337/100000: episode: 383, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.760, mean reward: -0.198 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.419, 10.224], loss: 0.002650, mae: 0.052926, mean_q: -0.331344
 25437/100000: episode: 384, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -20.100, mean reward: -0.201 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.794, 10.310], loss: 0.002425, mae: 0.049699, mean_q: -0.319818
 25537/100000: episode: 385, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -14.431, mean reward: -0.144 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.047, 10.312], loss: 0.002428, mae: 0.049886, mean_q: -0.347190
 25637/100000: episode: 386, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.902, mean reward: -0.179 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.023, 10.098], loss: 0.002592, mae: 0.050612, mean_q: -0.303681
 25737/100000: episode: 387, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.886, mean reward: -0.179 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.549, 10.252], loss: 0.002696, mae: 0.053717, mean_q: -0.316511
 25837/100000: episode: 388, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.263, mean reward: -0.183 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.908, 10.171], loss: 0.002759, mae: 0.053051, mean_q: -0.335909
 25937/100000: episode: 389, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -18.640, mean reward: -0.186 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.854, 10.099], loss: 0.002624, mae: 0.052083, mean_q: -0.340216
 26037/100000: episode: 390, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.928, mean reward: -0.179 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.411, 10.201], loss: 0.002624, mae: 0.051200, mean_q: -0.326091
 26137/100000: episode: 391, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.173, mean reward: -0.162 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.200, 10.098], loss: 0.002678, mae: 0.052654, mean_q: -0.303158
 26237/100000: episode: 392, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.844, mean reward: -0.178 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.952, 10.177], loss: 0.002774, mae: 0.052346, mean_q: -0.318606
 26337/100000: episode: 393, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -18.530, mean reward: -0.185 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.242, 10.098], loss: 0.002466, mae: 0.050138, mean_q: -0.337691
 26437/100000: episode: 394, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -13.749, mean reward: -0.137 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.222, 10.113], loss: 0.002379, mae: 0.048996, mean_q: -0.305245
 26537/100000: episode: 395, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.796, mean reward: -0.178 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.265, 10.116], loss: 0.002533, mae: 0.050587, mean_q: -0.342797
 26637/100000: episode: 396, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.629, mean reward: -0.156 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.520, 10.098], loss: 0.002332, mae: 0.048059, mean_q: -0.350810
 26737/100000: episode: 397, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.239, mean reward: -0.172 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.298, 10.223], loss: 0.002411, mae: 0.049864, mean_q: -0.310166
[Info] 100-TH LEVEL FOUND: 0.579475998878479, Considering 10/90 traces
 26837/100000: episode: 398, duration: 4.345s, episode steps: 100, steps per second: 23, episode reward: -18.067, mean reward: -0.181 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.976, 10.204], loss: 0.002554, mae: 0.051269, mean_q: -0.311099
 26853/100000: episode: 399, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 5.074, mean reward: 0.317 [0.177, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-1.222, 10.561], loss: 0.002429, mae: 0.050838, mean_q: -0.365764
 26873/100000: episode: 400, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 6.968, mean reward: 0.348 [0.200, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.056, 10.428], loss: 0.002380, mae: 0.048987, mean_q: -0.310252
 26891/100000: episode: 401, duration: 0.085s, episode steps: 18, steps per second: 212, episode reward: 5.962, mean reward: 0.331 [0.270, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.552, 10.100], loss: 0.002576, mae: 0.050188, mean_q: -0.331470
 26909/100000: episode: 402, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 5.126, mean reward: 0.285 [0.161, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.147, 10.457], loss: 0.002357, mae: 0.049055, mean_q: -0.331982
 26953/100000: episode: 403, duration: 0.222s, episode steps: 44, steps per second: 198, episode reward: 12.536, mean reward: 0.285 [0.148, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.111, 10.100], loss: 0.002391, mae: 0.050588, mean_q: -0.325404
 26973/100000: episode: 404, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 4.465, mean reward: 0.223 [0.146, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.421], loss: 0.002642, mae: 0.052342, mean_q: -0.345225
 26990/100000: episode: 405, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 3.786, mean reward: 0.223 [0.154, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.415, 10.384], loss: 0.003006, mae: 0.057121, mean_q: -0.224548
 27008/100000: episode: 406, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 7.158, mean reward: 0.398 [0.352, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.981, 10.100], loss: 0.002393, mae: 0.051280, mean_q: -0.303067
 27025/100000: episode: 407, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 5.143, mean reward: 0.303 [0.235, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.779, 10.337], loss: 0.002322, mae: 0.050474, mean_q: -0.280669
 27045/100000: episode: 408, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 7.765, mean reward: 0.388 [0.251, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.299, 10.475], loss: 0.003001, mae: 0.058037, mean_q: -0.266820
 27063/100000: episode: 409, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 4.570, mean reward: 0.254 [0.182, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.465, 10.421], loss: 0.002149, mae: 0.046447, mean_q: -0.376587
 27083/100000: episode: 410, duration: 0.098s, episode steps: 20, steps per second: 205, episode reward: 4.964, mean reward: 0.248 [0.145, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.351], loss: 0.002419, mae: 0.049787, mean_q: -0.288221
 27127/100000: episode: 411, duration: 0.211s, episode steps: 44, steps per second: 208, episode reward: 12.415, mean reward: 0.282 [0.117, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.699, 10.100], loss: 0.002341, mae: 0.047982, mean_q: -0.307390
 27145/100000: episode: 412, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 6.368, mean reward: 0.354 [0.304, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.351, 10.100], loss: 0.002486, mae: 0.049882, mean_q: -0.259109
 27156/100000: episode: 413, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 3.768, mean reward: 0.343 [0.275, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.323, 10.100], loss: 0.002303, mae: 0.049643, mean_q: -0.275754
 27167/100000: episode: 414, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 3.453, mean reward: 0.314 [0.235, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.416, 10.100], loss: 0.002174, mae: 0.048868, mean_q: -0.241918
 27184/100000: episode: 415, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 4.441, mean reward: 0.261 [0.130, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.050, 10.295], loss: 0.002602, mae: 0.052238, mean_q: -0.237898
 27202/100000: episode: 416, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 3.652, mean reward: 0.203 [0.119, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.403], loss: 0.002536, mae: 0.050763, mean_q: -0.277321
 27219/100000: episode: 417, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 2.407, mean reward: 0.142 [0.023, 0.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.193], loss: 0.002444, mae: 0.049503, mean_q: -0.254216
 27237/100000: episode: 418, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 3.667, mean reward: 0.204 [0.038, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.142, 10.316], loss: 0.002657, mae: 0.054291, mean_q: -0.207356
 27257/100000: episode: 419, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 6.895, mean reward: 0.345 [0.233, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.498], loss: 0.002217, mae: 0.048880, mean_q: -0.279145
 27275/100000: episode: 420, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 7.228, mean reward: 0.402 [0.324, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.342, 10.100], loss: 0.003077, mae: 0.056864, mean_q: -0.204075
 27291/100000: episode: 421, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 4.163, mean reward: 0.260 [0.149, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.319, 10.374], loss: 0.024915, mae: 0.119042, mean_q: -0.297221
 27303/100000: episode: 422, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 3.339, mean reward: 0.278 [0.236, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.604, 10.375], loss: 0.012893, mae: 0.101930, mean_q: -0.346471
 27321/100000: episode: 423, duration: 0.099s, episode steps: 18, steps per second: 183, episode reward: 5.254, mean reward: 0.292 [0.079, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.167, 10.209], loss: 0.010246, mae: 0.084464, mean_q: -0.181695
 27339/100000: episode: 424, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 3.220, mean reward: 0.179 [0.053, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.392, 10.434], loss: 0.005832, mae: 0.070070, mean_q: -0.279136
 27351/100000: episode: 425, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 2.954, mean reward: 0.246 [0.132, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.300], loss: 0.003629, mae: 0.065702, mean_q: -0.190400
 27395/100000: episode: 426, duration: 0.220s, episode steps: 44, steps per second: 200, episode reward: 9.651, mean reward: 0.219 [0.045, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.111, 10.100], loss: 0.002677, mae: 0.053755, mean_q: -0.265181
 27413/100000: episode: 427, duration: 0.100s, episode steps: 18, steps per second: 179, episode reward: 5.103, mean reward: 0.284 [0.113, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.170, 10.406], loss: 0.002469, mae: 0.053689, mean_q: -0.187312
 27425/100000: episode: 428, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 3.772, mean reward: 0.314 [0.240, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.452], loss: 0.002674, mae: 0.056302, mean_q: -0.149391
 27441/100000: episode: 429, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 4.033, mean reward: 0.252 [0.083, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-1.049, 10.352], loss: 0.002982, mae: 0.056783, mean_q: -0.294044
 27459/100000: episode: 430, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 6.393, mean reward: 0.355 [0.281, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.364, 10.100], loss: 0.002703, mae: 0.053012, mean_q: -0.254307
 27471/100000: episode: 431, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 4.025, mean reward: 0.335 [0.226, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-1.575, 10.344], loss: 0.002854, mae: 0.055093, mean_q: -0.234634
 27489/100000: episode: 432, duration: 0.090s, episode steps: 18, steps per second: 199, episode reward: 7.318, mean reward: 0.407 [0.348, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.289, 10.100], loss: 0.002453, mae: 0.052852, mean_q: -0.250651
 27533/100000: episode: 433, duration: 0.229s, episode steps: 44, steps per second: 192, episode reward: 12.750, mean reward: 0.290 [0.044, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.214, 10.100], loss: 0.002606, mae: 0.053324, mean_q: -0.182894
 27551/100000: episode: 434, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 6.553, mean reward: 0.364 [0.236, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.337, 10.100], loss: 0.002373, mae: 0.050629, mean_q: -0.227994
 27568/100000: episode: 435, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 2.490, mean reward: 0.146 [0.044, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.232], loss: 0.002868, mae: 0.054678, mean_q: -0.154745
 27586/100000: episode: 436, duration: 0.086s, episode steps: 18, steps per second: 210, episode reward: 3.696, mean reward: 0.205 [0.086, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.218, 10.153], loss: 0.002979, mae: 0.058007, mean_q: -0.177921
 27597/100000: episode: 437, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 3.584, mean reward: 0.326 [0.257, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.423, 10.100], loss: 0.002315, mae: 0.050354, mean_q: -0.249561
 27614/100000: episode: 438, duration: 0.084s, episode steps: 17, steps per second: 204, episode reward: 3.984, mean reward: 0.234 [0.138, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.320], loss: 0.002500, mae: 0.052532, mean_q: -0.084089
 27632/100000: episode: 439, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 4.499, mean reward: 0.250 [0.097, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.252], loss: 0.002723, mae: 0.053817, mean_q: -0.127000
 27643/100000: episode: 440, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 3.585, mean reward: 0.326 [0.258, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.296, 10.100], loss: 0.002277, mae: 0.047675, mean_q: -0.158634
 27655/100000: episode: 441, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 4.710, mean reward: 0.393 [0.334, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.335, 10.457], loss: 0.002665, mae: 0.052829, mean_q: -0.151066
 27673/100000: episode: 442, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 5.477, mean reward: 0.304 [0.208, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-2.109, 10.353], loss: 0.002764, mae: 0.052431, mean_q: -0.207847
 27691/100000: episode: 443, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 4.312, mean reward: 0.240 [0.078, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.129, 10.100], loss: 0.002956, mae: 0.055909, mean_q: -0.167252
 27707/100000: episode: 444, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 3.247, mean reward: 0.203 [0.087, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.269, 10.363], loss: 0.002968, mae: 0.056281, mean_q: -0.205139
 27725/100000: episode: 445, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 4.812, mean reward: 0.267 [0.200, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.363], loss: 0.002897, mae: 0.055561, mean_q: -0.137481
 27741/100000: episode: 446, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 3.481, mean reward: 0.218 [0.142, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.280], loss: 0.002635, mae: 0.053255, mean_q: -0.189132
 27753/100000: episode: 447, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 3.660, mean reward: 0.305 [0.229, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.047, 10.286], loss: 0.003120, mae: 0.059654, mean_q: -0.123220
 27771/100000: episode: 448, duration: 0.087s, episode steps: 18, steps per second: 206, episode reward: 6.320, mean reward: 0.351 [0.146, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.629, 10.497], loss: 0.002568, mae: 0.054172, mean_q: -0.266958
 27782/100000: episode: 449, duration: 0.073s, episode steps: 11, steps per second: 150, episode reward: 3.654, mean reward: 0.332 [0.269, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.466, 10.100], loss: 0.002629, mae: 0.053860, mean_q: -0.200512
 27800/100000: episode: 450, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 5.549, mean reward: 0.308 [0.074, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.290, 10.366], loss: 0.002381, mae: 0.052398, mean_q: -0.163339
 27818/100000: episode: 451, duration: 0.085s, episode steps: 18, steps per second: 212, episode reward: 5.100, mean reward: 0.283 [0.160, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.170, 10.356], loss: 0.002600, mae: 0.052526, mean_q: -0.191032
 27838/100000: episode: 452, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 4.989, mean reward: 0.249 [0.182, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.819, 10.337], loss: 0.002551, mae: 0.052392, mean_q: -0.200243
 27858/100000: episode: 453, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 4.875, mean reward: 0.244 [0.170, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.328], loss: 0.002549, mae: 0.052166, mean_q: -0.131193
 27878/100000: episode: 454, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 4.786, mean reward: 0.239 [0.182, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.276, 10.388], loss: 0.002958, mae: 0.055590, mean_q: -0.131515
 27922/100000: episode: 455, duration: 0.246s, episode steps: 44, steps per second: 179, episode reward: 16.971, mean reward: 0.386 [0.178, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.665, 10.100], loss: 0.002676, mae: 0.052828, mean_q: -0.129497
 27939/100000: episode: 456, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 4.732, mean reward: 0.278 [0.172, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.287, 10.330], loss: 0.002590, mae: 0.052155, mean_q: -0.198974
 27950/100000: episode: 457, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 3.376, mean reward: 0.307 [0.275, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.292, 10.100], loss: 0.003026, mae: 0.057464, mean_q: -0.048094
 27968/100000: episode: 458, duration: 0.086s, episode steps: 18, steps per second: 210, episode reward: 5.203, mean reward: 0.289 [0.163, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.569, 10.422], loss: 0.002549, mae: 0.052685, mean_q: -0.153718
 27988/100000: episode: 459, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 5.420, mean reward: 0.271 [0.181, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.385], loss: 0.002575, mae: 0.051118, mean_q: -0.209823
 28008/100000: episode: 460, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 4.714, mean reward: 0.236 [0.068, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.078, 10.266], loss: 0.003045, mae: 0.057099, mean_q: -0.045155
 28026/100000: episode: 461, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 5.662, mean reward: 0.315 [0.205, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.361], loss: 0.002647, mae: 0.053235, mean_q: -0.083500
 28046/100000: episode: 462, duration: 0.102s, episode steps: 20, steps per second: 195, episode reward: 5.631, mean reward: 0.282 [0.157, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.130, 10.314], loss: 0.002679, mae: 0.053078, mean_q: -0.151422
 28057/100000: episode: 463, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 3.657, mean reward: 0.332 [0.286, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.367, 10.100], loss: 0.002666, mae: 0.052713, mean_q: -0.178178
 28077/100000: episode: 464, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 3.586, mean reward: 0.179 [0.014, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-1.088, 10.100], loss: 0.002902, mae: 0.055951, mean_q: -0.096437
 28088/100000: episode: 465, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 3.218, mean reward: 0.293 [0.200, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.437, 10.100], loss: 0.003960, mae: 0.064397, mean_q: -0.109508
 28105/100000: episode: 466, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 3.205, mean reward: 0.189 [0.004, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.442, 10.129], loss: 0.003065, mae: 0.055120, mean_q: -0.128170
 28116/100000: episode: 467, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 3.173, mean reward: 0.288 [0.200, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.191, 10.100], loss: 0.002850, mae: 0.054974, mean_q: -0.010846
 28127/100000: episode: 468, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 3.919, mean reward: 0.356 [0.294, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.535, 10.100], loss: 0.002513, mae: 0.051374, mean_q: -0.129460
 28147/100000: episode: 469, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 4.042, mean reward: 0.202 [0.042, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.816, 10.163], loss: 0.002402, mae: 0.050568, mean_q: -0.104018
 28164/100000: episode: 470, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 4.546, mean reward: 0.267 [0.176, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.483], loss: 0.002572, mae: 0.051500, mean_q: -0.181615
 28182/100000: episode: 471, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 3.717, mean reward: 0.206 [0.134, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.546, 10.332], loss: 0.002860, mae: 0.055467, mean_q: -0.016902
 28200/100000: episode: 472, duration: 0.086s, episode steps: 18, steps per second: 210, episode reward: 4.315, mean reward: 0.240 [0.034, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.852, 10.144], loss: 0.002775, mae: 0.055722, mean_q: -0.048058
 28212/100000: episode: 473, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 4.540, mean reward: 0.378 [0.249, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.121, 10.455], loss: 0.002772, mae: 0.057433, mean_q: 0.013072
 28232/100000: episode: 474, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 4.082, mean reward: 0.204 [0.057, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.580, 10.237], loss: 0.003045, mae: 0.058294, mean_q: -0.036317
 28276/100000: episode: 475, duration: 0.234s, episode steps: 44, steps per second: 188, episode reward: 14.840, mean reward: 0.337 [0.237, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.838, 10.100], loss: 0.002945, mae: 0.056282, mean_q: -0.082650
 28294/100000: episode: 476, duration: 0.086s, episode steps: 18, steps per second: 209, episode reward: 3.554, mean reward: 0.197 [0.047, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.249], loss: 0.002969, mae: 0.057101, mean_q: -0.011519
 28314/100000: episode: 477, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 5.845, mean reward: 0.292 [0.191, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.097, 10.417], loss: 0.002569, mae: 0.053133, mean_q: -0.108648
 28332/100000: episode: 478, duration: 0.086s, episode steps: 18, steps per second: 210, episode reward: 5.265, mean reward: 0.292 [0.156, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.328], loss: 0.002672, mae: 0.055284, mean_q: -0.041866
 28350/100000: episode: 479, duration: 0.085s, episode steps: 18, steps per second: 211, episode reward: 2.351, mean reward: 0.131 [0.029, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.240], loss: 0.002614, mae: 0.053968, mean_q: -0.028864
 28368/100000: episode: 480, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 6.778, mean reward: 0.377 [0.335, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.723, 10.449], loss: 0.002918, mae: 0.055984, mean_q: -0.089270
 28380/100000: episode: 481, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 4.685, mean reward: 0.390 [0.308, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.248, 10.536], loss: 0.002803, mae: 0.056484, mean_q: -0.012015
 28392/100000: episode: 482, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 3.174, mean reward: 0.265 [0.186, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-1.465, 10.373], loss: 0.002726, mae: 0.055002, mean_q: -0.106488
 28410/100000: episode: 483, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 5.218, mean reward: 0.290 [0.250, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.711, 10.100], loss: 0.003465, mae: 0.062882, mean_q: -0.060554
 28430/100000: episode: 484, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 6.138, mean reward: 0.307 [0.164, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.843, 10.442], loss: 0.002865, mae: 0.055735, mean_q: -0.052101
 28448/100000: episode: 485, duration: 0.086s, episode steps: 18, steps per second: 209, episode reward: 7.352, mean reward: 0.408 [0.310, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.311, 10.100], loss: 0.003188, mae: 0.058837, mean_q: -0.000872
 28468/100000: episode: 486, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 2.849, mean reward: 0.142 [0.029, 0.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.155], loss: 0.017474, mae: 0.101465, mean_q: 0.040455
 28512/100000: episode: 487, duration: 0.223s, episode steps: 44, steps per second: 197, episode reward: 15.191, mean reward: 0.345 [0.158, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.922, 10.100], loss: 0.004881, mae: 0.070899, mean_q: -0.029879
[Info] 200-TH LEVEL FOUND: 0.7782884240150452, Considering 10/90 traces
 28556/100000: episode: 488, duration: 4.028s, episode steps: 44, steps per second: 11, episode reward: 12.583, mean reward: 0.286 [0.127, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.205, 10.100], loss: 0.003080, mae: 0.058826, mean_q: -0.028113
 28564/100000: episode: 489, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 2.830, mean reward: 0.354 [0.316, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.396, 10.100], loss: 0.003348, mae: 0.063222, mean_q: -0.049028
 28578/100000: episode: 490, duration: 0.082s, episode steps: 14, steps per second: 172, episode reward: 4.796, mean reward: 0.343 [0.229, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.304, 10.100], loss: 0.002997, mae: 0.057650, mean_q: -0.104750
 28593/100000: episode: 491, duration: 0.071s, episode steps: 15, steps per second: 210, episode reward: 6.748, mean reward: 0.450 [0.357, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.594, 10.100], loss: 0.003206, mae: 0.059953, mean_q: -0.046600
 28601/100000: episode: 492, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 2.891, mean reward: 0.361 [0.317, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.399, 10.100], loss: 0.002922, mae: 0.057313, mean_q: -0.046520
 28614/100000: episode: 493, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 4.314, mean reward: 0.332 [0.226, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.258, 10.100], loss: 0.002780, mae: 0.057555, mean_q: 0.039724
 28628/100000: episode: 494, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 5.714, mean reward: 0.408 [0.338, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.885, 10.100], loss: 0.002940, mae: 0.056759, mean_q: -0.006814
 28641/100000: episode: 495, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 5.299, mean reward: 0.408 [0.366, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.235, 10.100], loss: 0.002942, mae: 0.057607, mean_q: 0.007892
 28648/100000: episode: 496, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 2.942, mean reward: 0.420 [0.383, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.327, 10.100], loss: 0.003070, mae: 0.059083, mean_q: 0.051790
 28656/100000: episode: 497, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 2.801, mean reward: 0.350 [0.337, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.527, 10.100], loss: 0.003158, mae: 0.059670, mean_q: 0.027609
 28664/100000: episode: 498, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 2.688, mean reward: 0.336 [0.292, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.322, 10.100], loss: 0.003621, mae: 0.062831, mean_q: 0.072430
 28671/100000: episode: 499, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 2.860, mean reward: 0.409 [0.379, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.367, 10.100], loss: 0.003140, mae: 0.058709, mean_q: 0.072699
 28679/100000: episode: 500, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 3.198, mean reward: 0.400 [0.304, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.436, 10.100], loss: 0.003077, mae: 0.059322, mean_q: -0.028931
 28686/100000: episode: 501, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 3.051, mean reward: 0.436 [0.341, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.287, 10.100], loss: 0.001942, mae: 0.046729, mean_q: -0.130065
 28701/100000: episode: 502, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 4.648, mean reward: 0.310 [0.199, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.404, 10.100], loss: 0.002528, mae: 0.054685, mean_q: -0.070774
 28708/100000: episode: 503, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 2.967, mean reward: 0.424 [0.272, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.288, 10.100], loss: 0.002803, mae: 0.055467, mean_q: -0.019269
 28722/100000: episode: 504, duration: 0.067s, episode steps: 14, steps per second: 208, episode reward: 6.637, mean reward: 0.474 [0.386, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.362, 10.100], loss: 0.003355, mae: 0.061378, mean_q: 0.060360
 28729/100000: episode: 505, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 2.376, mean reward: 0.339 [0.285, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.339, 10.100], loss: 0.003037, mae: 0.060066, mean_q: 0.018337
 28743/100000: episode: 506, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 6.226, mean reward: 0.445 [0.388, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.653, 10.100], loss: 0.003664, mae: 0.062761, mean_q: -0.036698
 28758/100000: episode: 507, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 6.834, mean reward: 0.456 [0.347, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.703, 10.100], loss: 0.003250, mae: 0.062510, mean_q: 0.067910
 28773/100000: episode: 508, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 5.858, mean reward: 0.391 [0.290, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.399, 10.100], loss: 0.002852, mae: 0.054723, mean_q: -0.032429
 28788/100000: episode: 509, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 6.195, mean reward: 0.413 [0.362, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.383, 10.100], loss: 0.003104, mae: 0.059157, mean_q: 0.053648
 28796/100000: episode: 510, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 3.094, mean reward: 0.387 [0.342, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.398, 10.100], loss: 0.002072, mae: 0.048809, mean_q: -0.014154
 28811/100000: episode: 511, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 6.050, mean reward: 0.403 [0.338, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.356, 10.100], loss: 0.002825, mae: 0.056211, mean_q: 0.050312
 28819/100000: episode: 512, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 3.806, mean reward: 0.476 [0.381, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.520, 10.100], loss: 0.002717, mae: 0.055995, mean_q: 0.016675
 28834/100000: episode: 513, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 5.113, mean reward: 0.341 [0.290, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.383, 10.100], loss: 0.002778, mae: 0.056307, mean_q: -0.057191
 28841/100000: episode: 514, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 2.417, mean reward: 0.345 [0.307, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.498, 10.100], loss: 0.002860, mae: 0.055640, mean_q: -0.058998
 28849/100000: episode: 515, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 3.056, mean reward: 0.382 [0.309, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.332, 10.100], loss: 0.003341, mae: 0.061643, mean_q: 0.077423
 28864/100000: episode: 516, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 5.310, mean reward: 0.354 [0.281, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.690, 10.100], loss: 0.002883, mae: 0.057701, mean_q: 0.123627
 28871/100000: episode: 517, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 2.601, mean reward: 0.372 [0.302, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.500, 10.100], loss: 0.002837, mae: 0.055051, mean_q: -0.036235
 28886/100000: episode: 518, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 6.454, mean reward: 0.430 [0.312, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.864, 10.100], loss: 0.002669, mae: 0.054466, mean_q: 0.035773
 28891/100000: episode: 519, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 2.053, mean reward: 0.411 [0.394, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.364, 10.100], loss: 0.003413, mae: 0.063323, mean_q: 0.022210
 28899/100000: episode: 520, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 3.337, mean reward: 0.417 [0.354, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.473, 10.100], loss: 0.003029, mae: 0.056177, mean_q: -0.030980
 28907/100000: episode: 521, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 3.126, mean reward: 0.391 [0.312, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.336, 10.100], loss: 0.002893, mae: 0.056667, mean_q: 0.062107
 28922/100000: episode: 522, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 4.446, mean reward: 0.296 [0.230, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.365, 10.100], loss: 0.003085, mae: 0.059708, mean_q: 0.023565
 28929/100000: episode: 523, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 2.557, mean reward: 0.365 [0.270, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.467, 10.100], loss: 0.002698, mae: 0.054643, mean_q: -0.011136
 28942/100000: episode: 524, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 5.014, mean reward: 0.386 [0.343, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.347, 10.100], loss: 0.002906, mae: 0.056443, mean_q: 0.045959
 28957/100000: episode: 525, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 5.273, mean reward: 0.352 [0.239, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.426, 10.100], loss: 0.003267, mae: 0.060292, mean_q: 0.075688
 28965/100000: episode: 526, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 2.614, mean reward: 0.327 [0.285, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.529, 10.100], loss: 0.002850, mae: 0.058404, mean_q: 0.056027
 28979/100000: episode: 527, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 6.296, mean reward: 0.450 [0.389, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.053, 10.100], loss: 0.002893, mae: 0.056509, mean_q: 0.038686
 28987/100000: episode: 528, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 3.267, mean reward: 0.408 [0.331, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.355, 10.100], loss: 0.003341, mae: 0.059897, mean_q: 0.085505
 29001/100000: episode: 529, duration: 0.092s, episode steps: 14, steps per second: 152, episode reward: 5.536, mean reward: 0.395 [0.335, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.418, 10.100], loss: 0.002752, mae: 0.055408, mean_q: 0.033544
 29008/100000: episode: 530, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 2.609, mean reward: 0.373 [0.319, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.326, 10.100], loss: 0.003031, mae: 0.056894, mean_q: 0.097710
 29016/100000: episode: 531, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 3.366, mean reward: 0.421 [0.370, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.330, 10.100], loss: 0.003461, mae: 0.061594, mean_q: 0.138726
 29023/100000: episode: 532, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 2.063, mean reward: 0.295 [0.263, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.459, 10.100], loss: 0.003300, mae: 0.062477, mean_q: 0.121538
 29036/100000: episode: 533, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 4.599, mean reward: 0.354 [0.294, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.429, 10.100], loss: 0.002719, mae: 0.056213, mean_q: 0.127651
 29044/100000: episode: 534, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 2.909, mean reward: 0.364 [0.337, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.358, 10.100], loss: 0.002938, mae: 0.057764, mean_q: 0.094947
 29058/100000: episode: 535, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 6.215, mean reward: 0.444 [0.383, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.392, 10.100], loss: 0.003000, mae: 0.057238, mean_q: 0.080556
 29065/100000: episode: 536, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 3.097, mean reward: 0.442 [0.329, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.294, 10.100], loss: 0.002432, mae: 0.052057, mean_q: -0.015505
 29079/100000: episode: 537, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 6.174, mean reward: 0.441 [0.361, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.747, 10.100], loss: 0.003212, mae: 0.059658, mean_q: 0.053267
 29086/100000: episode: 538, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 2.470, mean reward: 0.353 [0.276, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.424, 10.100], loss: 0.003390, mae: 0.063976, mean_q: 0.152482
 29091/100000: episode: 539, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 1.975, mean reward: 0.395 [0.356, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.348, 10.100], loss: 0.002786, mae: 0.056419, mean_q: 0.098942
 29106/100000: episode: 540, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 6.237, mean reward: 0.416 [0.365, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.367, 10.100], loss: 0.002529, mae: 0.053781, mean_q: 0.048791
 29121/100000: episode: 541, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 6.170, mean reward: 0.411 [0.236, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.305, 10.100], loss: 0.003060, mae: 0.058317, mean_q: 0.108113
 29129/100000: episode: 542, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 3.184, mean reward: 0.398 [0.353, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.343, 10.100], loss: 0.002856, mae: 0.056358, mean_q: 0.039492
 29136/100000: episode: 543, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 2.671, mean reward: 0.382 [0.345, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.480, 10.100], loss: 0.002463, mae: 0.052913, mean_q: 0.106923
 29150/100000: episode: 544, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 6.218, mean reward: 0.444 [0.340, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.552, 10.100], loss: 0.002665, mae: 0.056050, mean_q: 0.124165
 29164/100000: episode: 545, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 5.066, mean reward: 0.362 [0.267, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.216, 10.100], loss: 0.003385, mae: 0.061215, mean_q: 0.156526
 29179/100000: episode: 546, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 6.510, mean reward: 0.434 [0.375, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.278, 10.100], loss: 0.003368, mae: 0.061508, mean_q: 0.164481
 29184/100000: episode: 547, duration: 0.038s, episode steps: 5, steps per second: 131, episode reward: 2.157, mean reward: 0.431 [0.351, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.349, 10.100], loss: 0.002904, mae: 0.057199, mean_q: 0.197269
 29191/100000: episode: 548, duration: 0.052s, episode steps: 7, steps per second: 135, episode reward: 2.630, mean reward: 0.376 [0.272, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.239, 10.100], loss: 0.002589, mae: 0.053649, mean_q: 0.114906
 29206/100000: episode: 549, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 5.719, mean reward: 0.381 [0.340, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.410, 10.100], loss: 0.003109, mae: 0.059168, mean_q: 0.174197
 29220/100000: episode: 550, duration: 0.086s, episode steps: 14, steps per second: 164, episode reward: 6.233, mean reward: 0.445 [0.375, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.290, 10.100], loss: 0.002751, mae: 0.056947, mean_q: 0.118242
 29234/100000: episode: 551, duration: 0.067s, episode steps: 14, steps per second: 209, episode reward: 6.168, mean reward: 0.441 [0.353, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.352, 10.100], loss: 0.002819, mae: 0.054757, mean_q: 0.070194
 29242/100000: episode: 552, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 2.952, mean reward: 0.369 [0.296, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.500, 10.100], loss: 0.002742, mae: 0.054930, mean_q: 0.128374
 29257/100000: episode: 553, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 6.283, mean reward: 0.419 [0.356, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.550, 10.100], loss: 0.003182, mae: 0.060140, mean_q: 0.096399
 29262/100000: episode: 554, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: 1.772, mean reward: 0.354 [0.303, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.356, 10.100], loss: 0.002614, mae: 0.054189, mean_q: 0.181389
 29269/100000: episode: 555, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 2.458, mean reward: 0.351 [0.278, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.387, 10.100], loss: 0.003107, mae: 0.061927, mean_q: 0.090492
 29276/100000: episode: 556, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 2.391, mean reward: 0.342 [0.305, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.427, 10.100], loss: 0.002979, mae: 0.056982, mean_q: 0.121900
 29283/100000: episode: 557, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 1.997, mean reward: 0.285 [0.255, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.351, 10.100], loss: 0.003727, mae: 0.061550, mean_q: -0.078240
 29297/100000: episode: 558, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 5.652, mean reward: 0.404 [0.357, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.450, 10.100], loss: 0.003123, mae: 0.058059, mean_q: 0.094550
 29305/100000: episode: 559, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 2.790, mean reward: 0.349 [0.324, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.457, 10.100], loss: 0.003399, mae: 0.059525, mean_q: 0.171801
 29319/100000: episode: 560, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 5.929, mean reward: 0.423 [0.345, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.508, 10.100], loss: 0.003131, mae: 0.061221, mean_q: 0.168744
 29324/100000: episode: 561, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 1.802, mean reward: 0.360 [0.295, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.550, 10.100], loss: 0.002587, mae: 0.054401, mean_q: 0.061176
 29329/100000: episode: 562, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 1.968, mean reward: 0.394 [0.346, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-1.229, 10.100], loss: 0.003636, mae: 0.066436, mean_q: 0.258888
 29344/100000: episode: 563, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 6.108, mean reward: 0.407 [0.253, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.277, 10.100], loss: 0.002719, mae: 0.057831, mean_q: 0.175918
 29359/100000: episode: 564, duration: 0.072s, episode steps: 15, steps per second: 209, episode reward: 5.262, mean reward: 0.351 [0.229, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.242, 10.100], loss: 0.002823, mae: 0.056542, mean_q: 0.096621
 29364/100000: episode: 565, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 1.936, mean reward: 0.387 [0.273, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.382, 10.100], loss: 0.003088, mae: 0.060381, mean_q: 0.190428
 29371/100000: episode: 566, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 3.525, mean reward: 0.504 [0.384, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.470, 10.100], loss: 0.002877, mae: 0.056002, mean_q: 0.183208
 29376/100000: episode: 567, duration: 0.035s, episode steps: 5, steps per second: 145, episode reward: 1.830, mean reward: 0.366 [0.309, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.357, 10.100], loss: 0.003053, mae: 0.054676, mean_q: 0.106180
 29391/100000: episode: 568, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 6.318, mean reward: 0.421 [0.329, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.304, 10.100], loss: 0.003104, mae: 0.059677, mean_q: 0.165643
 29406/100000: episode: 569, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 5.969, mean reward: 0.398 [0.300, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.315, 10.100], loss: 0.002714, mae: 0.053357, mean_q: 0.102294
 29413/100000: episode: 570, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 2.284, mean reward: 0.326 [0.294, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.332, 10.100], loss: 0.003366, mae: 0.061577, mean_q: 0.189121
 29421/100000: episode: 571, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 3.697, mean reward: 0.462 [0.349, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.566, 10.100], loss: 0.003229, mae: 0.061941, mean_q: 0.123622
 29428/100000: episode: 572, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 2.958, mean reward: 0.423 [0.361, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.371, 10.100], loss: 0.003746, mae: 0.065059, mean_q: 0.270690
 29443/100000: episode: 573, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 4.606, mean reward: 0.307 [0.187, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.275, 10.100], loss: 0.003425, mae: 0.061746, mean_q: 0.110041
 29456/100000: episode: 574, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 4.829, mean reward: 0.371 [0.318, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.680, 10.100], loss: 0.003569, mae: 0.063339, mean_q: 0.184133
 29463/100000: episode: 575, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 3.110, mean reward: 0.444 [0.341, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-1.638, 10.100], loss: 0.002706, mae: 0.055243, mean_q: 0.117256
 29477/100000: episode: 576, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 6.320, mean reward: 0.451 [0.393, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.391, 10.100], loss: 0.003173, mae: 0.061753, mean_q: 0.159111
 29484/100000: episode: 577, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 2.448, mean reward: 0.350 [0.303, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.429, 10.100], loss: 0.002406, mae: 0.051404, mean_q: 0.161701
[Info] 300-TH LEVEL FOUND: 0.9132055044174194, Considering 10/90 traces
 29491/100000: episode: 578, duration: 3.885s, episode steps: 7, steps per second: 2, episode reward: 3.152, mean reward: 0.450 [0.414, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.432, 10.100], loss: 0.003685, mae: 0.063797, mean_q: 0.249310
 29502/100000: episode: 579, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 4.917, mean reward: 0.447 [0.382, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.357, 10.100], loss: 0.003337, mae: 0.064521, mean_q: 0.232960
 29511/100000: episode: 580, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 3.742, mean reward: 0.416 [0.385, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.379, 10.100], loss: 0.002810, mae: 0.056148, mean_q: 0.101305
 29521/100000: episode: 581, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 3.963, mean reward: 0.396 [0.357, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.564, 10.100], loss: 0.003170, mae: 0.059137, mean_q: 0.202559
 29531/100000: episode: 582, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 4.713, mean reward: 0.471 [0.390, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.468, 10.100], loss: 0.003443, mae: 0.060665, mean_q: 0.269267
 29538/100000: episode: 583, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 3.929, mean reward: 0.561 [0.500, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.510, 10.100], loss: 0.002614, mae: 0.054477, mean_q: 0.221244
 29548/100000: episode: 584, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 4.418, mean reward: 0.442 [0.371, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.260, 10.100], loss: 0.003597, mae: 0.063829, mean_q: 0.233312
 29558/100000: episode: 585, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 4.144, mean reward: 0.414 [0.266, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.325, 10.100], loss: 0.003061, mae: 0.059734, mean_q: 0.239753
 29566/100000: episode: 586, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 4.140, mean reward: 0.518 [0.423, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.442, 10.100], loss: 0.004011, mae: 0.069447, mean_q: 0.165916
 29576/100000: episode: 587, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 5.149, mean reward: 0.515 [0.430, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.527, 10.100], loss: 0.002868, mae: 0.058620, mean_q: 0.221351
 29586/100000: episode: 588, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 3.719, mean reward: 0.372 [0.310, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.288, 10.100], loss: 0.003115, mae: 0.059244, mean_q: 0.162718
 29597/100000: episode: 589, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 4.638, mean reward: 0.422 [0.255, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.180, 10.100], loss: 0.003800, mae: 0.064764, mean_q: 0.218567
 29608/100000: episode: 590, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 4.707, mean reward: 0.428 [0.320, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.207, 10.100], loss: 0.003720, mae: 0.063216, mean_q: 0.096820
 29619/100000: episode: 591, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 4.997, mean reward: 0.454 [0.334, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.459, 10.100], loss: 0.003760, mae: 0.066428, mean_q: 0.191940
 29629/100000: episode: 592, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 3.596, mean reward: 0.360 [0.264, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.261, 10.100], loss: 0.004073, mae: 0.069631, mean_q: 0.232683
 29637/100000: episode: 593, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 3.305, mean reward: 0.413 [0.357, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.733, 10.100], loss: 0.003181, mae: 0.059757, mean_q: 0.291290
 29646/100000: episode: 594, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 4.580, mean reward: 0.509 [0.467, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.311, 10.100], loss: 0.003073, mae: 0.060725, mean_q: 0.231533
 29657/100000: episode: 595, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 4.629, mean reward: 0.421 [0.398, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.816, 10.100], loss: 0.003016, mae: 0.056808, mean_q: 0.214972
 29666/100000: episode: 596, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 4.081, mean reward: 0.453 [0.365, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.282, 10.100], loss: 0.003532, mae: 0.064672, mean_q: 0.250192
 29675/100000: episode: 597, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 3.698, mean reward: 0.411 [0.371, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.505, 10.100], loss: 0.002984, mae: 0.057372, mean_q: 0.253146
 29686/100000: episode: 598, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 5.262, mean reward: 0.478 [0.390, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.591, 10.100], loss: 0.003617, mae: 0.063866, mean_q: 0.175123
[Info] FALSIFICATION!
 29687/100000: episode: 599, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.053, 9.911], loss: 0.003954, mae: 0.064389, mean_q: 0.223273
 29787/100000: episode: 600, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.392, mean reward: -0.164 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.106, 10.098], loss: 0.016517, mae: 0.064996, mean_q: 0.218318
 29887/100000: episode: 601, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.943, mean reward: -0.189 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.013, 10.215], loss: 0.018091, mae: 0.076562, mean_q: 0.239283
 29987/100000: episode: 602, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -12.512, mean reward: -0.125 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.453, 10.098], loss: 0.003008, mae: 0.058588, mean_q: 0.199538
 30087/100000: episode: 603, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -18.575, mean reward: -0.186 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.792, 10.288], loss: 0.003001, mae: 0.059289, mean_q: 0.219315
 30187/100000: episode: 604, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.191, mean reward: -0.182 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.229, 10.235], loss: 0.018471, mae: 0.074532, mean_q: 0.225151
 30287/100000: episode: 605, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.610, mean reward: -0.176 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.704, 10.098], loss: 0.004058, mae: 0.065561, mean_q: 0.234829
 30387/100000: episode: 606, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -19.390, mean reward: -0.194 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.646, 10.098], loss: 0.002923, mae: 0.058198, mean_q: 0.244418
 30487/100000: episode: 607, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -20.459, mean reward: -0.205 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.906, 10.098], loss: 0.003051, mae: 0.058726, mean_q: 0.206264
 30587/100000: episode: 608, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -19.115, mean reward: -0.191 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.295, 10.226], loss: 0.017162, mae: 0.070513, mean_q: 0.228147
 30687/100000: episode: 609, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -19.132, mean reward: -0.191 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.536, 10.098], loss: 0.029862, mae: 0.074450, mean_q: 0.227353
 30787/100000: episode: 610, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -18.676, mean reward: -0.187 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.594, 10.201], loss: 0.029835, mae: 0.074669, mean_q: 0.226954
 30887/100000: episode: 611, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.871, mean reward: -0.159 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.498, 10.254], loss: 0.003154, mae: 0.060815, mean_q: 0.210191
 30987/100000: episode: 612, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -10.654, mean reward: -0.107 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.674, 10.098], loss: 0.002838, mae: 0.057791, mean_q: 0.237141
 31087/100000: episode: 613, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.325, mean reward: -0.173 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.407, 10.098], loss: 0.029618, mae: 0.072869, mean_q: 0.205771
 31187/100000: episode: 614, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.142, mean reward: -0.161 [-1.000, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.873, 10.098], loss: 0.017262, mae: 0.074779, mean_q: 0.227624
 31287/100000: episode: 615, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.202, mean reward: -0.172 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.519, 10.098], loss: 0.016987, mae: 0.070700, mean_q: 0.247797
 31387/100000: episode: 616, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.364, mean reward: -0.174 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.015, 10.194], loss: 0.017043, mae: 0.073004, mean_q: 0.254329
 31487/100000: episode: 617, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.433, mean reward: -0.194 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.809, 10.197], loss: 0.019159, mae: 0.080036, mean_q: 0.278301
 31587/100000: episode: 618, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.011, mean reward: -0.180 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.441, 10.190], loss: 0.018574, mae: 0.077292, mean_q: 0.230024
 31687/100000: episode: 619, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -8.015, mean reward: -0.080 [-1.000, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.804, 10.098], loss: 0.043286, mae: 0.086279, mean_q: 0.254704
 31787/100000: episode: 620, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.128, mean reward: -0.171 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.684, 10.098], loss: 0.003689, mae: 0.061644, mean_q: 0.199583
 31887/100000: episode: 621, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.101, mean reward: -0.181 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.005, 10.182], loss: 0.031871, mae: 0.082419, mean_q: 0.198553
 31987/100000: episode: 622, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.601, mean reward: -0.196 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.268, 10.160], loss: 0.017255, mae: 0.076179, mean_q: 0.161288
 32087/100000: episode: 623, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -13.348, mean reward: -0.133 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.808, 10.333], loss: 0.029469, mae: 0.073320, mean_q: 0.166953
 32187/100000: episode: 624, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -19.305, mean reward: -0.193 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.634, 10.261], loss: 0.003266, mae: 0.061340, mean_q: 0.157466
 32287/100000: episode: 625, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -19.557, mean reward: -0.196 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.669, 10.227], loss: 0.042574, mae: 0.081354, mean_q: 0.137585
 32387/100000: episode: 626, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -18.492, mean reward: -0.185 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.926, 10.098], loss: 0.030265, mae: 0.076278, mean_q: 0.104063
 32487/100000: episode: 627, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.639, mean reward: -0.166 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.427, 10.098], loss: 0.017099, mae: 0.071577, mean_q: 0.108475
 32587/100000: episode: 628, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -14.064, mean reward: -0.141 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.970, 10.098], loss: 0.003343, mae: 0.060274, mean_q: 0.059222
 32687/100000: episode: 629, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.637, mean reward: -0.176 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.272, 10.270], loss: 0.003004, mae: 0.057683, mean_q: 0.071963
 32787/100000: episode: 630, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.536, mean reward: -0.175 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.827, 10.180], loss: 0.016269, mae: 0.065179, mean_q: 0.040658
 32887/100000: episode: 631, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -17.987, mean reward: -0.180 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.583, 10.098], loss: 0.015948, mae: 0.063040, mean_q: 0.006339
 32987/100000: episode: 632, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.054, mean reward: -0.161 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.190, 10.098], loss: 0.002826, mae: 0.054925, mean_q: -0.004762
 33087/100000: episode: 633, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.896, mean reward: -0.179 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.105, 10.168], loss: 0.016692, mae: 0.068257, mean_q: -0.012140
 33187/100000: episode: 634, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.671, mean reward: -0.177 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.128, 10.234], loss: 0.016200, mae: 0.062462, mean_q: -0.018444
 33287/100000: episode: 635, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -12.677, mean reward: -0.127 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.225, 10.098], loss: 0.002935, mae: 0.056115, mean_q: -0.049101
 33387/100000: episode: 636, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.026, mean reward: -0.180 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.910, 10.098], loss: 0.042865, mae: 0.077656, mean_q: -0.053221
 33487/100000: episode: 637, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -15.020, mean reward: -0.150 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.665, 10.098], loss: 0.029732, mae: 0.074499, mean_q: -0.093490
 33587/100000: episode: 638, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.166, mean reward: -0.182 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.108, 10.098], loss: 0.002827, mae: 0.054586, mean_q: -0.114260
 33687/100000: episode: 639, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -10.058, mean reward: -0.101 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.372, 10.098], loss: 0.015872, mae: 0.059548, mean_q: -0.104533
 33787/100000: episode: 640, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -18.483, mean reward: -0.185 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.616, 10.098], loss: 0.015814, mae: 0.059492, mean_q: -0.152550
 33887/100000: episode: 641, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.365, mean reward: -0.164 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.767, 10.259], loss: 0.016326, mae: 0.064095, mean_q: -0.172430
 33987/100000: episode: 642, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.123, mean reward: -0.171 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.850, 10.098], loss: 0.002843, mae: 0.053710, mean_q: -0.159280
 34087/100000: episode: 643, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -17.399, mean reward: -0.174 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.651, 10.200], loss: 0.002859, mae: 0.054360, mean_q: -0.219600
 34187/100000: episode: 644, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -14.289, mean reward: -0.143 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.495, 10.224], loss: 0.028529, mae: 0.065615, mean_q: -0.219505
 34287/100000: episode: 645, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.493, mean reward: -0.175 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.204, 10.144], loss: 0.003986, mae: 0.062847, mean_q: -0.227670
 34387/100000: episode: 646, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.988, mean reward: -0.170 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.005, 10.098], loss: 0.019139, mae: 0.079937, mean_q: -0.263633
 34487/100000: episode: 647, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.186, mean reward: -0.172 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.044, 10.098], loss: 0.002952, mae: 0.055139, mean_q: -0.284032
 34587/100000: episode: 648, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.824, mean reward: -0.188 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.834, 10.195], loss: 0.017896, mae: 0.072973, mean_q: -0.285655
 34687/100000: episode: 649, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -17.599, mean reward: -0.176 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.217, 10.098], loss: 0.002698, mae: 0.053432, mean_q: -0.314235
 34787/100000: episode: 650, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -20.830, mean reward: -0.208 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.808, 10.170], loss: 0.002635, mae: 0.051588, mean_q: -0.343426
 34887/100000: episode: 651, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.141, mean reward: -0.171 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.011, 10.381], loss: 0.002392, mae: 0.048940, mean_q: -0.328084
 34987/100000: episode: 652, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -15.016, mean reward: -0.150 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.989, 10.098], loss: 0.002472, mae: 0.049675, mean_q: -0.293784
 35087/100000: episode: 653, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.591, mean reward: -0.186 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.556, 10.098], loss: 0.002502, mae: 0.050637, mean_q: -0.311108
 35187/100000: episode: 654, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.644, mean reward: -0.186 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.321, 10.098], loss: 0.002582, mae: 0.050784, mean_q: -0.324911
 35287/100000: episode: 655, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.262, mean reward: -0.173 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.191, 10.098], loss: 0.002453, mae: 0.049316, mean_q: -0.355353
 35387/100000: episode: 656, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.605, mean reward: -0.146 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.733, 10.098], loss: 0.002679, mae: 0.051441, mean_q: -0.310192
 35487/100000: episode: 657, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -18.744, mean reward: -0.187 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.665, 10.098], loss: 0.002636, mae: 0.051347, mean_q: -0.335309
 35587/100000: episode: 658, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -20.274, mean reward: -0.203 [-1.000, 0.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.953, 10.236], loss: 0.002548, mae: 0.051155, mean_q: -0.302981
 35687/100000: episode: 659, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.487, mean reward: -0.145 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.749, 10.341], loss: 0.003422, mae: 0.059366, mean_q: -0.303111
 35787/100000: episode: 660, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.844, mean reward: -0.148 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.326, 10.399], loss: 0.002593, mae: 0.051447, mean_q: -0.305516
 35887/100000: episode: 661, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -15.303, mean reward: -0.153 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.284, 10.202], loss: 0.002435, mae: 0.048666, mean_q: -0.329173
 35987/100000: episode: 662, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.771, mean reward: -0.178 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.579, 10.140], loss: 0.002524, mae: 0.049769, mean_q: -0.310750
 36087/100000: episode: 663, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -19.518, mean reward: -0.195 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.806, 10.173], loss: 0.002305, mae: 0.047942, mean_q: -0.317723
 36187/100000: episode: 664, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -13.579, mean reward: -0.136 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.912, 10.098], loss: 0.002366, mae: 0.048456, mean_q: -0.330944
 36287/100000: episode: 665, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.331, mean reward: -0.193 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.755, 10.098], loss: 0.002357, mae: 0.049170, mean_q: -0.318297
 36387/100000: episode: 666, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -15.107, mean reward: -0.151 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.655, 10.098], loss: 0.002471, mae: 0.050496, mean_q: -0.316648
 36487/100000: episode: 667, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -12.146, mean reward: -0.121 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.594, 10.404], loss: 0.002540, mae: 0.050021, mean_q: -0.336922
 36587/100000: episode: 668, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -19.883, mean reward: -0.199 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.681, 10.123], loss: 0.002428, mae: 0.049483, mean_q: -0.286269
 36687/100000: episode: 669, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -19.030, mean reward: -0.190 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.716, 10.215], loss: 0.002352, mae: 0.048144, mean_q: -0.335650
 36787/100000: episode: 670, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.318, mean reward: -0.183 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.134, 10.112], loss: 0.002583, mae: 0.051153, mean_q: -0.304711
 36887/100000: episode: 671, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -13.525, mean reward: -0.135 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.140, 10.098], loss: 0.002551, mae: 0.050699, mean_q: -0.305425
 36987/100000: episode: 672, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -15.306, mean reward: -0.153 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.508, 10.098], loss: 0.006018, mae: 0.069469, mean_q: -0.328295
 37087/100000: episode: 673, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.276, mean reward: -0.183 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.552, 10.276], loss: 0.002636, mae: 0.053019, mean_q: -0.314537
 37187/100000: episode: 674, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.554, mean reward: -0.176 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.237, 10.300], loss: 0.002518, mae: 0.050797, mean_q: -0.317575
 37287/100000: episode: 675, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: -17.473, mean reward: -0.175 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.160, 10.098], loss: 0.002718, mae: 0.052885, mean_q: -0.313652
 37387/100000: episode: 676, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -12.136, mean reward: -0.121 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.573, 10.319], loss: 0.002591, mae: 0.051867, mean_q: -0.307768
 37487/100000: episode: 677, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -17.316, mean reward: -0.173 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.742, 10.098], loss: 0.002590, mae: 0.051255, mean_q: -0.294296
 37587/100000: episode: 678, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.524, mean reward: -0.155 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.282, 10.098], loss: 0.002382, mae: 0.048639, mean_q: -0.285509
 37687/100000: episode: 679, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -13.661, mean reward: -0.137 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.834, 10.247], loss: 0.002502, mae: 0.049417, mean_q: -0.314973
 37787/100000: episode: 680, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -14.947, mean reward: -0.149 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.779, 10.250], loss: 0.002642, mae: 0.051740, mean_q: -0.309488
 37887/100000: episode: 681, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -16.459, mean reward: -0.165 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.975, 10.116], loss: 0.002387, mae: 0.048265, mean_q: -0.317231
 37987/100000: episode: 682, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.600, mean reward: -0.196 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.719, 10.098], loss: 0.002481, mae: 0.049714, mean_q: -0.301235
 38087/100000: episode: 683, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -14.184, mean reward: -0.142 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.320, 10.182], loss: 0.002561, mae: 0.050228, mean_q: -0.328800
 38187/100000: episode: 684, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.386, mean reward: -0.174 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.254, 10.121], loss: 0.002598, mae: 0.050460, mean_q: -0.300866
 38287/100000: episode: 685, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -17.821, mean reward: -0.178 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.192, 10.098], loss: 0.002826, mae: 0.052816, mean_q: -0.297347
 38387/100000: episode: 686, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.421, mean reward: -0.154 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.496, 10.291], loss: 0.002444, mae: 0.049998, mean_q: -0.306010
 38487/100000: episode: 687, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.189, mean reward: -0.182 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.235, 10.098], loss: 0.002293, mae: 0.048367, mean_q: -0.339632
 38587/100000: episode: 688, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -12.339, mean reward: -0.123 [-1.000, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.854, 10.417], loss: 0.002555, mae: 0.050670, mean_q: -0.285991
 38687/100000: episode: 689, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.190, mean reward: -0.172 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.553, 10.231], loss: 0.002386, mae: 0.049412, mean_q: -0.292897
 38787/100000: episode: 690, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.786, mean reward: -0.178 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.896, 10.150], loss: 0.002478, mae: 0.049451, mean_q: -0.327288
 38887/100000: episode: 691, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.448, mean reward: -0.164 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.113, 10.327], loss: 0.002466, mae: 0.049189, mean_q: -0.322897
 38987/100000: episode: 692, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.078, mean reward: -0.181 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.560, 10.098], loss: 0.006363, mae: 0.068399, mean_q: -0.332149
 39087/100000: episode: 693, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.041, mean reward: -0.180 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.105, 10.098], loss: 0.004465, mae: 0.064812, mean_q: -0.332123
 39187/100000: episode: 694, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.055, mean reward: -0.171 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.999, 10.098], loss: 0.002531, mae: 0.050833, mean_q: -0.329987
 39287/100000: episode: 695, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -14.139, mean reward: -0.141 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.760, 10.293], loss: 0.002501, mae: 0.050198, mean_q: -0.292269
 39387/100000: episode: 696, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.444, mean reward: -0.174 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.627, 10.098], loss: 0.002279, mae: 0.047114, mean_q: -0.335328
 39487/100000: episode: 697, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -13.385, mean reward: -0.134 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.850, 10.292], loss: 0.002270, mae: 0.047288, mean_q: -0.326010
 39587/100000: episode: 698, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -13.864, mean reward: -0.139 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.880, 10.098], loss: 0.002320, mae: 0.047709, mean_q: -0.305155
[Info] 100-TH LEVEL FOUND: 0.6035686731338501, Considering 10/90 traces
 39687/100000: episode: 699, duration: 4.318s, episode steps: 100, steps per second: 23, episode reward: -16.826, mean reward: -0.168 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.793, 10.241], loss: 0.002410, mae: 0.048272, mean_q: -0.320396
 39703/100000: episode: 700, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 4.503, mean reward: 0.281 [0.176, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.324], loss: 0.002277, mae: 0.049683, mean_q: -0.261909
 39715/100000: episode: 701, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 3.697, mean reward: 0.308 [0.208, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.167, 10.464], loss: 0.003203, mae: 0.058108, mean_q: -0.281218
[Info] FALSIFICATION!
 39741/100000: episode: 702, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 22.006, mean reward: 0.846 [0.329, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.475, 10.719], loss: 0.002739, mae: 0.050912, mean_q: -0.348039
 39841/100000: episode: 703, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -21.252, mean reward: -0.213 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.385, 10.186], loss: 0.017882, mae: 0.070756, mean_q: -0.289566
 39941/100000: episode: 704, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.535, mean reward: -0.165 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.286, 10.098], loss: 0.031855, mae: 0.079717, mean_q: -0.298015
 40041/100000: episode: 705, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.511, mean reward: -0.195 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.783, 10.145], loss: 0.002640, mae: 0.051615, mean_q: -0.307489
 40141/100000: episode: 706, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.107, mean reward: -0.171 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.909, 10.167], loss: 0.027420, mae: 0.058654, mean_q: -0.318209
 40241/100000: episode: 707, duration: 0.480s, episode steps: 100, steps per second: 209, episode reward: -15.294, mean reward: -0.153 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.987, 10.320], loss: 0.015832, mae: 0.060550, mean_q: -0.287856
 40341/100000: episode: 708, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -12.868, mean reward: -0.129 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.469, 10.442], loss: 0.002659, mae: 0.051058, mean_q: -0.308463
 40441/100000: episode: 709, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -9.504, mean reward: -0.095 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.399, 10.384], loss: 0.002351, mae: 0.048034, mean_q: -0.302135
 40541/100000: episode: 710, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -19.758, mean reward: -0.198 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.701, 10.098], loss: 0.016374, mae: 0.065060, mean_q: -0.259416
 40641/100000: episode: 711, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.336, mean reward: -0.173 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.528, 10.205], loss: 0.002286, mae: 0.046547, mean_q: -0.331683
 40741/100000: episode: 712, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -19.158, mean reward: -0.192 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.522, 10.127], loss: 0.002623, mae: 0.051134, mean_q: -0.279636
 40841/100000: episode: 713, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.849, mean reward: -0.178 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.099, 10.224], loss: 0.002499, mae: 0.049910, mean_q: -0.289257
 40941/100000: episode: 714, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.333, mean reward: -0.183 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.098, 10.146], loss: 0.002550, mae: 0.050538, mean_q: -0.281547
 41041/100000: episode: 715, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.693, mean reward: -0.177 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.975, 10.098], loss: 0.002301, mae: 0.047815, mean_q: -0.292346
 41141/100000: episode: 716, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.731, mean reward: -0.157 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.900, 10.098], loss: 0.002458, mae: 0.049669, mean_q: -0.291859
 41241/100000: episode: 717, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.047, mean reward: -0.170 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.748, 10.135], loss: 0.003322, mae: 0.059058, mean_q: -0.346039
 41341/100000: episode: 718, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -15.428, mean reward: -0.154 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.627, 10.098], loss: 0.002416, mae: 0.049636, mean_q: -0.304563
 41441/100000: episode: 719, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.009, mean reward: -0.170 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.560, 10.098], loss: 0.002368, mae: 0.047457, mean_q: -0.304745
 41541/100000: episode: 720, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -19.836, mean reward: -0.198 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.817, 10.098], loss: 0.002388, mae: 0.048717, mean_q: -0.290525
 41641/100000: episode: 721, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -19.383, mean reward: -0.194 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.803, 10.098], loss: 0.016214, mae: 0.059251, mean_q: -0.326396
 41741/100000: episode: 722, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.381, mean reward: -0.194 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.513, 10.275], loss: 0.030178, mae: 0.071993, mean_q: -0.303108
 41841/100000: episode: 723, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -15.607, mean reward: -0.156 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.727, 10.098], loss: 0.027299, mae: 0.062167, mean_q: -0.323674
 41941/100000: episode: 724, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -12.767, mean reward: -0.128 [-1.000, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.702, 10.098], loss: 0.031072, mae: 0.087502, mean_q: -0.297176
 42041/100000: episode: 725, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.141, mean reward: -0.151 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.312, 10.149], loss: 0.019010, mae: 0.079854, mean_q: -0.329428
 42141/100000: episode: 726, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -15.554, mean reward: -0.156 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.577, 10.098], loss: 0.002442, mae: 0.049935, mean_q: -0.311376
 42241/100000: episode: 727, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -13.404, mean reward: -0.134 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.593, 10.346], loss: 0.014760, mae: 0.055239, mean_q: -0.295149
 42341/100000: episode: 728, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -15.957, mean reward: -0.160 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.613, 10.098], loss: 0.002890, mae: 0.053043, mean_q: -0.291468
 42441/100000: episode: 729, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.139, mean reward: -0.171 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.261, 10.135], loss: 0.002522, mae: 0.050160, mean_q: -0.293486
 42541/100000: episode: 730, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -18.211, mean reward: -0.182 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.825, 10.098], loss: 0.002386, mae: 0.048729, mean_q: -0.309241
 42641/100000: episode: 731, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.948, mean reward: -0.189 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.806, 10.098], loss: 0.015008, mae: 0.058134, mean_q: -0.281252
 42741/100000: episode: 732, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.643, mean reward: -0.166 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.850, 10.139], loss: 0.015293, mae: 0.060778, mean_q: -0.290644
 42841/100000: episode: 733, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -13.231, mean reward: -0.132 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.923, 10.450], loss: 0.002384, mae: 0.048616, mean_q: -0.266857
 42941/100000: episode: 734, duration: 0.740s, episode steps: 100, steps per second: 135, episode reward: -16.671, mean reward: -0.167 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.279, 10.182], loss: 0.002403, mae: 0.048927, mean_q: -0.301240
 43041/100000: episode: 735, duration: 1.135s, episode steps: 100, steps per second: 88, episode reward: -19.615, mean reward: -0.196 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.400, 10.205], loss: 0.014971, mae: 0.056225, mean_q: -0.324425
 43141/100000: episode: 736, duration: 1.266s, episode steps: 100, steps per second: 79, episode reward: -13.895, mean reward: -0.139 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.837, 10.113], loss: 0.002536, mae: 0.050258, mean_q: -0.300634
 43241/100000: episode: 737, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: -5.551, mean reward: -0.056 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.127, 10.336], loss: 0.015598, mae: 0.062141, mean_q: -0.304753
 43341/100000: episode: 738, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: -18.226, mean reward: -0.182 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.911, 10.341], loss: 0.002407, mae: 0.048367, mean_q: -0.320472
 43441/100000: episode: 739, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -19.266, mean reward: -0.193 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.555, 10.098], loss: 0.002516, mae: 0.050177, mean_q: -0.281711
 43541/100000: episode: 740, duration: 0.643s, episode steps: 100, steps per second: 155, episode reward: -17.537, mean reward: -0.175 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.793, 10.190], loss: 0.002632, mae: 0.051031, mean_q: -0.303212
 43641/100000: episode: 741, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: -16.031, mean reward: -0.160 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.385, 10.224], loss: 0.002512, mae: 0.050493, mean_q: -0.286116
 43741/100000: episode: 742, duration: 0.668s, episode steps: 100, steps per second: 150, episode reward: -15.731, mean reward: -0.157 [-1.000, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.722, 10.148], loss: 0.002481, mae: 0.049001, mean_q: -0.300064
 43841/100000: episode: 743, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.983, mean reward: -0.170 [-1.000, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.945, 10.098], loss: 0.002363, mae: 0.048827, mean_q: -0.278601
 43941/100000: episode: 744, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: -18.601, mean reward: -0.186 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.852, 10.217], loss: 0.002694, mae: 0.051668, mean_q: -0.270015
 44041/100000: episode: 745, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: -11.866, mean reward: -0.119 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.690, 10.270], loss: 0.005196, mae: 0.068143, mean_q: -0.293321
 44141/100000: episode: 746, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: -16.205, mean reward: -0.162 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.860, 10.098], loss: 0.003676, mae: 0.058774, mean_q: -0.287704
 44241/100000: episode: 747, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: -17.748, mean reward: -0.177 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.638, 10.235], loss: 0.015660, mae: 0.061428, mean_q: -0.284341
 44341/100000: episode: 748, duration: 0.881s, episode steps: 100, steps per second: 113, episode reward: -18.003, mean reward: -0.180 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.017, 10.363], loss: 0.002928, mae: 0.054400, mean_q: -0.278080
 44441/100000: episode: 749, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: -17.956, mean reward: -0.180 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.540, 10.217], loss: 0.015398, mae: 0.062309, mean_q: -0.305419
 44541/100000: episode: 750, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: -16.444, mean reward: -0.164 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.743, 10.098], loss: 0.028892, mae: 0.066766, mean_q: -0.279406
 44641/100000: episode: 751, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: -13.108, mean reward: -0.131 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.513, 10.098], loss: 0.003204, mae: 0.056159, mean_q: -0.299189
 44741/100000: episode: 752, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: -14.958, mean reward: -0.150 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.251, 10.245], loss: 0.002704, mae: 0.051568, mean_q: -0.333696
 44841/100000: episode: 753, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: -15.898, mean reward: -0.159 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.627, 10.098], loss: 0.002651, mae: 0.051236, mean_q: -0.305886
 44941/100000: episode: 754, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: -15.440, mean reward: -0.154 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.239, 10.112], loss: 0.002738, mae: 0.051920, mean_q: -0.325380
 45041/100000: episode: 755, duration: 0.652s, episode steps: 100, steps per second: 153, episode reward: -17.996, mean reward: -0.180 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.209, 10.118], loss: 0.002806, mae: 0.052068, mean_q: -0.327510
 45141/100000: episode: 756, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -13.715, mean reward: -0.137 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.623, 10.118], loss: 0.002886, mae: 0.053800, mean_q: -0.291058
 45241/100000: episode: 757, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -19.567, mean reward: -0.196 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.650, 10.162], loss: 0.002615, mae: 0.050801, mean_q: -0.315240
 45341/100000: episode: 758, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -18.458, mean reward: -0.185 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.648, 10.207], loss: 0.002729, mae: 0.051327, mean_q: -0.322170
 45441/100000: episode: 759, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: -19.375, mean reward: -0.194 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.389, 10.098], loss: 0.002880, mae: 0.052940, mean_q: -0.315547
 45541/100000: episode: 760, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.679, mean reward: -0.187 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.528, 10.110], loss: 0.002751, mae: 0.051700, mean_q: -0.307913
 45641/100000: episode: 761, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: -14.338, mean reward: -0.143 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.013, 10.136], loss: 0.002826, mae: 0.052986, mean_q: -0.285695
 45741/100000: episode: 762, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: -20.197, mean reward: -0.202 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.313, 10.098], loss: 0.003826, mae: 0.061326, mean_q: -0.294549
 45841/100000: episode: 763, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -18.931, mean reward: -0.189 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.287, 10.098], loss: 0.003053, mae: 0.056864, mean_q: -0.310379
 45941/100000: episode: 764, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -15.561, mean reward: -0.156 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.094, 10.204], loss: 0.003509, mae: 0.058638, mean_q: -0.286799
 46041/100000: episode: 765, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: -14.851, mean reward: -0.149 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.507, 10.137], loss: 0.002846, mae: 0.053243, mean_q: -0.305128
 46141/100000: episode: 766, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -13.581, mean reward: -0.136 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.808, 10.098], loss: 0.002767, mae: 0.052656, mean_q: -0.302530
 46241/100000: episode: 767, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: -13.863, mean reward: -0.139 [-1.000, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.486, 10.098], loss: 0.002804, mae: 0.053201, mean_q: -0.296195
 46341/100000: episode: 768, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: -15.410, mean reward: -0.154 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.225, 10.210], loss: 0.003011, mae: 0.055535, mean_q: -0.284398
 46441/100000: episode: 769, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: -18.052, mean reward: -0.181 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.914, 10.273], loss: 0.002853, mae: 0.052313, mean_q: -0.321425
 46541/100000: episode: 770, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: -16.533, mean reward: -0.165 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.491, 10.098], loss: 0.002712, mae: 0.051026, mean_q: -0.336511
 46641/100000: episode: 771, duration: 1.117s, episode steps: 100, steps per second: 90, episode reward: -18.430, mean reward: -0.184 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.192, 10.098], loss: 0.002751, mae: 0.052082, mean_q: -0.302665
 46741/100000: episode: 772, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: -14.500, mean reward: -0.145 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.471, 10.457], loss: 0.002637, mae: 0.051334, mean_q: -0.313530
 46841/100000: episode: 773, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: -14.986, mean reward: -0.150 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.172, 10.202], loss: 0.002901, mae: 0.053289, mean_q: -0.300999
 46941/100000: episode: 774, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: -17.214, mean reward: -0.172 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.719, 10.138], loss: 0.005418, mae: 0.072058, mean_q: -0.275157
 47041/100000: episode: 775, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: -14.560, mean reward: -0.146 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.682, 10.098], loss: 0.002908, mae: 0.053641, mean_q: -0.309680
 47141/100000: episode: 776, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: -10.966, mean reward: -0.110 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.513, 10.098], loss: 0.002988, mae: 0.054046, mean_q: -0.307819
 47241/100000: episode: 777, duration: 0.697s, episode steps: 100, steps per second: 143, episode reward: -18.331, mean reward: -0.183 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.697, 10.136], loss: 0.004992, mae: 0.066301, mean_q: -0.297144
 47341/100000: episode: 778, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: -17.253, mean reward: -0.173 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.326, 10.276], loss: 0.002990, mae: 0.055444, mean_q: -0.306452
 47441/100000: episode: 779, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: -19.458, mean reward: -0.195 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.714, 10.344], loss: 0.002828, mae: 0.052943, mean_q: -0.326961
 47541/100000: episode: 780, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: -16.626, mean reward: -0.166 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.720, 10.204], loss: 0.003167, mae: 0.055649, mean_q: -0.300278
 47641/100000: episode: 781, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: -12.575, mean reward: -0.126 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.893, 10.128], loss: 0.002624, mae: 0.050729, mean_q: -0.333391
 47741/100000: episode: 782, duration: 0.647s, episode steps: 100, steps per second: 155, episode reward: -18.710, mean reward: -0.187 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.511, 10.291], loss: 0.002745, mae: 0.051965, mean_q: -0.317180
 47841/100000: episode: 783, duration: 0.608s, episode steps: 100, steps per second: 165, episode reward: -18.987, mean reward: -0.190 [-1.000, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.562, 10.104], loss: 0.002839, mae: 0.053001, mean_q: -0.284701
 47941/100000: episode: 784, duration: 0.668s, episode steps: 100, steps per second: 150, episode reward: -16.861, mean reward: -0.169 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.586, 10.098], loss: 0.002824, mae: 0.053864, mean_q: -0.319424
 48041/100000: episode: 785, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -8.097, mean reward: -0.081 [-1.000, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.322, 10.098], loss: 0.002886, mae: 0.055085, mean_q: -0.276760
 48141/100000: episode: 786, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: -14.759, mean reward: -0.148 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.388, 10.098], loss: 0.002679, mae: 0.050686, mean_q: -0.336530
 48241/100000: episode: 787, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: -19.043, mean reward: -0.190 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.343, 10.273], loss: 0.002586, mae: 0.050924, mean_q: -0.306496
 48341/100000: episode: 788, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -11.208, mean reward: -0.112 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.668, 10.475], loss: 0.002800, mae: 0.052184, mean_q: -0.329141
 48441/100000: episode: 789, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: -12.868, mean reward: -0.129 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.850, 10.262], loss: 0.002628, mae: 0.050300, mean_q: -0.329023
 48541/100000: episode: 790, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: -15.833, mean reward: -0.158 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.734, 10.098], loss: 0.002656, mae: 0.052599, mean_q: -0.282228
 48641/100000: episode: 791, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: -15.192, mean reward: -0.152 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.955, 10.098], loss: 0.002848, mae: 0.053848, mean_q: -0.287014
 48741/100000: episode: 792, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: -11.342, mean reward: -0.113 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.355, 10.161], loss: 0.002679, mae: 0.051690, mean_q: -0.304257
 48841/100000: episode: 793, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -13.371, mean reward: -0.134 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.352, 10.153], loss: 0.003067, mae: 0.056548, mean_q: -0.328977
 48941/100000: episode: 794, duration: 0.697s, episode steps: 100, steps per second: 143, episode reward: -20.166, mean reward: -0.202 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.043, 10.176], loss: 0.002567, mae: 0.050864, mean_q: -0.308542
 49041/100000: episode: 795, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: -18.522, mean reward: -0.185 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.745, 10.230], loss: 0.002859, mae: 0.053268, mean_q: -0.316921
 49141/100000: episode: 796, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -17.517, mean reward: -0.175 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.911, 10.262], loss: 0.002432, mae: 0.049461, mean_q: -0.325613
 49241/100000: episode: 797, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: -17.031, mean reward: -0.170 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.982, 10.174], loss: 0.002646, mae: 0.051435, mean_q: -0.328158
 49341/100000: episode: 798, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: -17.419, mean reward: -0.174 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.086, 10.282], loss: 0.002664, mae: 0.051819, mean_q: -0.301395
 49441/100000: episode: 799, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: -17.991, mean reward: -0.180 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.949, 10.309], loss: 0.003656, mae: 0.058717, mean_q: -0.286772
 49541/100000: episode: 800, duration: 0.635s, episode steps: 100, steps per second: 158, episode reward: -18.441, mean reward: -0.184 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.756, 10.098], loss: 0.005460, mae: 0.070008, mean_q: -0.308969
 49641/100000: episode: 801, duration: 0.682s, episode steps: 100, steps per second: 147, episode reward: -17.267, mean reward: -0.173 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.464, 10.098], loss: 0.002934, mae: 0.055040, mean_q: -0.292646
[Info] 100-TH LEVEL FOUND: 0.658231258392334, Considering 10/90 traces
 49741/100000: episode: 802, duration: 5.470s, episode steps: 100, steps per second: 18, episode reward: -18.864, mean reward: -0.189 [-1.000, 0.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.133, 10.344], loss: 0.002714, mae: 0.053385, mean_q: -0.282536
 49747/100000: episode: 803, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 2.233, mean reward: 0.372 [0.308, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.493], loss: 0.002745, mae: 0.053713, mean_q: -0.221761
 49782/100000: episode: 804, duration: 0.223s, episode steps: 35, steps per second: 157, episode reward: 11.330, mean reward: 0.324 [0.157, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.777, 10.454], loss: 0.002450, mae: 0.050251, mean_q: -0.242826
 49810/100000: episode: 805, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 8.519, mean reward: 0.304 [0.171, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.302, 10.319], loss: 0.002517, mae: 0.050675, mean_q: -0.256715
 49838/100000: episode: 806, duration: 0.181s, episode steps: 28, steps per second: 155, episode reward: 5.882, mean reward: 0.210 [0.088, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.035, 10.320], loss: 0.002593, mae: 0.050636, mean_q: -0.298502
 49844/100000: episode: 807, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 2.398, mean reward: 0.400 [0.313, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.545], loss: 0.002908, mae: 0.057025, mean_q: -0.261904
 49862/100000: episode: 808, duration: 0.126s, episode steps: 18, steps per second: 142, episode reward: 5.341, mean reward: 0.297 [0.066, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.267], loss: 0.002037, mae: 0.045464, mean_q: -0.333860
 49897/100000: episode: 809, duration: 0.221s, episode steps: 35, steps per second: 158, episode reward: 8.735, mean reward: 0.250 [0.081, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.967, 10.271], loss: 0.002591, mae: 0.050266, mean_q: -0.276464
 49933/100000: episode: 810, duration: 0.215s, episode steps: 36, steps per second: 168, episode reward: 11.187, mean reward: 0.311 [0.114, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.685, 10.411], loss: 0.002638, mae: 0.052118, mean_q: -0.286268
 49961/100000: episode: 811, duration: 0.173s, episode steps: 28, steps per second: 162, episode reward: 9.659, mean reward: 0.345 [0.215, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.259, 10.548], loss: 0.002986, mae: 0.054393, mean_q: -0.253585
 49989/100000: episode: 812, duration: 0.174s, episode steps: 28, steps per second: 161, episode reward: 7.879, mean reward: 0.281 [0.213, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.443, 10.322], loss: 0.002694, mae: 0.052073, mean_q: -0.306419
 50017/100000: episode: 813, duration: 0.170s, episode steps: 28, steps per second: 165, episode reward: 10.503, mean reward: 0.375 [0.234, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.606, 10.422], loss: 0.002909, mae: 0.054868, mean_q: -0.261711
 50035/100000: episode: 814, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 6.315, mean reward: 0.351 [0.241, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.477], loss: 0.003065, mae: 0.057050, mean_q: -0.196438
 50072/100000: episode: 815, duration: 0.237s, episode steps: 37, steps per second: 156, episode reward: 7.906, mean reward: 0.214 [0.039, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.082, 10.100], loss: 0.002731, mae: 0.052660, mean_q: -0.267746
 50109/100000: episode: 816, duration: 0.219s, episode steps: 37, steps per second: 169, episode reward: 12.412, mean reward: 0.335 [0.201, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.068, 10.328], loss: 0.003639, mae: 0.061814, mean_q: -0.254939
 50144/100000: episode: 817, duration: 0.211s, episode steps: 35, steps per second: 166, episode reward: 9.349, mean reward: 0.267 [0.184, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.035, 10.339], loss: 0.002782, mae: 0.054411, mean_q: -0.233376
 50179/100000: episode: 818, duration: 0.217s, episode steps: 35, steps per second: 162, episode reward: 12.047, mean reward: 0.344 [0.143, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.084, 10.272], loss: 0.002578, mae: 0.051537, mean_q: -0.230782
 50214/100000: episode: 819, duration: 0.211s, episode steps: 35, steps per second: 166, episode reward: 10.394, mean reward: 0.297 [0.140, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.257, 10.279], loss: 0.002861, mae: 0.053064, mean_q: -0.205383
 50246/100000: episode: 820, duration: 0.198s, episode steps: 32, steps per second: 161, episode reward: 11.227, mean reward: 0.351 [0.259, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.177, 10.100], loss: 0.002890, mae: 0.053721, mean_q: -0.199950
 50255/100000: episode: 821, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 3.297, mean reward: 0.366 [0.313, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.487, 10.100], loss: 0.002037, mae: 0.045738, mean_q: -0.232785
 50283/100000: episode: 822, duration: 0.193s, episode steps: 28, steps per second: 145, episode reward: 8.525, mean reward: 0.304 [0.213, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-1.157, 10.303], loss: 0.002679, mae: 0.052301, mean_q: -0.219960
 50319/100000: episode: 823, duration: 0.221s, episode steps: 36, steps per second: 163, episode reward: 10.415, mean reward: 0.289 [0.126, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.349, 10.251], loss: 0.002556, mae: 0.050941, mean_q: -0.220396
 50355/100000: episode: 824, duration: 0.203s, episode steps: 36, steps per second: 177, episode reward: 15.311, mean reward: 0.425 [0.344, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-1.074, 10.584], loss: 0.002466, mae: 0.050071, mean_q: -0.212690
 50392/100000: episode: 825, duration: 0.252s, episode steps: 37, steps per second: 147, episode reward: 14.005, mean reward: 0.379 [0.286, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.090, 10.407], loss: 0.002721, mae: 0.052152, mean_q: -0.220477
 50398/100000: episode: 826, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 2.722, mean reward: 0.454 [0.330, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.626], loss: 0.002306, mae: 0.047814, mean_q: -0.170223
 50426/100000: episode: 827, duration: 0.204s, episode steps: 28, steps per second: 137, episode reward: 6.628, mean reward: 0.237 [0.057, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.738, 10.290], loss: 0.002696, mae: 0.052714, mean_q: -0.146784
 50435/100000: episode: 828, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 3.347, mean reward: 0.372 [0.335, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.518, 10.100], loss: 0.002875, mae: 0.053753, mean_q: -0.165304
 50488/100000: episode: 829, duration: 0.327s, episode steps: 53, steps per second: 162, episode reward: 14.204, mean reward: 0.268 [0.108, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.146, 10.100], loss: 0.002855, mae: 0.053727, mean_q: -0.167426
 50541/100000: episode: 830, duration: 0.310s, episode steps: 53, steps per second: 171, episode reward: 20.244, mean reward: 0.382 [0.213, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.652, 10.100], loss: 0.002598, mae: 0.051887, mean_q: -0.147429
 50577/100000: episode: 831, duration: 0.446s, episode steps: 36, steps per second: 81, episode reward: 9.110, mean reward: 0.253 [0.121, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.035, 10.350], loss: 0.002580, mae: 0.051836, mean_q: -0.118936
 50630/100000: episode: 832, duration: 0.683s, episode steps: 53, steps per second: 78, episode reward: 18.426, mean reward: 0.348 [0.240, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.807, 10.100], loss: 0.002653, mae: 0.053237, mean_q: -0.177873
 50683/100000: episode: 833, duration: 0.668s, episode steps: 53, steps per second: 79, episode reward: 18.584, mean reward: 0.351 [0.245, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-0.496, 10.100], loss: 0.002576, mae: 0.051734, mean_q: -0.114384
 50689/100000: episode: 834, duration: 0.048s, episode steps: 6, steps per second: 126, episode reward: 2.032, mean reward: 0.339 [0.323, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.424], loss: 0.002589, mae: 0.052964, mean_q: -0.150681
 50698/100000: episode: 835, duration: 0.070s, episode steps: 9, steps per second: 129, episode reward: 4.007, mean reward: 0.445 [0.346, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.496, 10.100], loss: 0.002252, mae: 0.048630, mean_q: -0.134020
 50751/100000: episode: 836, duration: 0.397s, episode steps: 53, steps per second: 133, episode reward: 18.916, mean reward: 0.357 [0.176, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.921, 10.100], loss: 0.002689, mae: 0.053778, mean_q: -0.063782
 50760/100000: episode: 837, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 2.545, mean reward: 0.283 [0.222, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.321, 10.100], loss: 0.002206, mae: 0.045251, mean_q: -0.216647
 50778/100000: episode: 838, duration: 0.130s, episode steps: 18, steps per second: 139, episode reward: 8.618, mean reward: 0.479 [0.327, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.404, 10.663], loss: 0.002493, mae: 0.052909, mean_q: -0.052233
 50815/100000: episode: 839, duration: 0.257s, episode steps: 37, steps per second: 144, episode reward: 9.413, mean reward: 0.254 [0.051, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.232, 10.114], loss: 0.002617, mae: 0.052074, mean_q: -0.093441
 50852/100000: episode: 840, duration: 0.242s, episode steps: 37, steps per second: 153, episode reward: 6.934, mean reward: 0.187 [0.060, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.112, 10.218], loss: 0.002672, mae: 0.052926, mean_q: -0.133922
 50870/100000: episode: 841, duration: 0.142s, episode steps: 18, steps per second: 127, episode reward: 7.641, mean reward: 0.425 [0.185, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.243, 10.396], loss: 0.002896, mae: 0.055294, mean_q: -0.036044
 50888/100000: episode: 842, duration: 0.135s, episode steps: 18, steps per second: 133, episode reward: 7.398, mean reward: 0.411 [0.268, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.618, 10.509], loss: 0.002577, mae: 0.052306, mean_q: -0.063336
 50941/100000: episode: 843, duration: 0.384s, episode steps: 53, steps per second: 138, episode reward: 24.340, mean reward: 0.459 [0.234, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-1.234, 10.100], loss: 0.003231, mae: 0.057138, mean_q: -0.044490
 50978/100000: episode: 844, duration: 0.297s, episode steps: 37, steps per second: 125, episode reward: 13.250, mean reward: 0.358 [0.259, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.878, 10.442], loss: 0.002954, mae: 0.056356, mean_q: -0.087867
 50996/100000: episode: 845, duration: 0.132s, episode steps: 18, steps per second: 136, episode reward: 5.854, mean reward: 0.325 [0.228, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.485], loss: 0.002908, mae: 0.055593, mean_q: -0.021384
 51005/100000: episode: 846, duration: 0.095s, episode steps: 9, steps per second: 95, episode reward: 2.667, mean reward: 0.296 [0.248, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.312, 10.100], loss: 0.002675, mae: 0.053708, mean_q: 0.049489
 51037/100000: episode: 847, duration: 0.222s, episode steps: 32, steps per second: 144, episode reward: 12.954, mean reward: 0.405 [0.266, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.666, 10.100], loss: 0.002729, mae: 0.054347, mean_q: -0.108270
 51043/100000: episode: 848, duration: 0.041s, episode steps: 6, steps per second: 148, episode reward: 1.954, mean reward: 0.326 [0.295, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.402], loss: 0.003339, mae: 0.058506, mean_q: 0.015193
 51078/100000: episode: 849, duration: 0.257s, episode steps: 35, steps per second: 136, episode reward: 7.232, mean reward: 0.207 [0.023, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.170, 10.109], loss: 0.002621, mae: 0.053164, mean_q: -0.016254
 51113/100000: episode: 850, duration: 0.229s, episode steps: 35, steps per second: 153, episode reward: 10.065, mean reward: 0.288 [0.163, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.042, 10.288], loss: 0.003150, mae: 0.057783, mean_q: -0.020933
 51141/100000: episode: 851, duration: 0.166s, episode steps: 28, steps per second: 169, episode reward: 10.242, mean reward: 0.366 [0.155, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.483, 10.406], loss: 0.002763, mae: 0.054366, mean_q: -0.040500
 51147/100000: episode: 852, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 2.074, mean reward: 0.346 [0.316, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.417], loss: 0.002341, mae: 0.049764, mean_q: -0.069035
 51184/100000: episode: 853, duration: 0.246s, episode steps: 37, steps per second: 150, episode reward: 9.338, mean reward: 0.252 [0.165, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.494, 10.378], loss: 0.002717, mae: 0.053595, mean_q: -0.029123
 51212/100000: episode: 854, duration: 0.154s, episode steps: 28, steps per second: 181, episode reward: 11.060, mean reward: 0.395 [0.285, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.616, 10.646], loss: 0.002569, mae: 0.052264, mean_q: 0.030582
 51230/100000: episode: 855, duration: 0.123s, episode steps: 18, steps per second: 146, episode reward: 6.164, mean reward: 0.342 [0.171, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.352], loss: 0.002716, mae: 0.054089, mean_q: -0.009822
 51239/100000: episode: 856, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 3.428, mean reward: 0.381 [0.328, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.345, 10.100], loss: 0.002079, mae: 0.047064, mean_q: -0.050871
 51248/100000: episode: 857, duration: 0.067s, episode steps: 9, steps per second: 133, episode reward: 4.004, mean reward: 0.445 [0.328, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.369, 10.100], loss: 0.002301, mae: 0.047758, mean_q: -0.050068
 51266/100000: episode: 858, duration: 0.171s, episode steps: 18, steps per second: 105, episode reward: 5.913, mean reward: 0.329 [0.172, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.847, 10.339], loss: 0.002816, mae: 0.055534, mean_q: 0.024094
 51301/100000: episode: 859, duration: 0.265s, episode steps: 35, steps per second: 132, episode reward: 11.577, mean reward: 0.331 [0.196, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.055, 10.623], loss: 0.002799, mae: 0.054057, mean_q: 0.017798
 51310/100000: episode: 860, duration: 0.063s, episode steps: 9, steps per second: 144, episode reward: 3.343, mean reward: 0.371 [0.310, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-1.688, 10.100], loss: 0.002486, mae: 0.050159, mean_q: -0.052984
 51346/100000: episode: 861, duration: 0.305s, episode steps: 36, steps per second: 118, episode reward: 10.799, mean reward: 0.300 [0.133, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.687, 10.326], loss: 0.002759, mae: 0.053298, mean_q: -0.011548
 51382/100000: episode: 862, duration: 0.237s, episode steps: 36, steps per second: 152, episode reward: 7.951, mean reward: 0.221 [0.051, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-1.000, 10.275], loss: 0.002707, mae: 0.053494, mean_q: 0.019513
 51417/100000: episode: 863, duration: 0.226s, episode steps: 35, steps per second: 155, episode reward: 12.530, mean reward: 0.358 [0.200, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.480, 10.567], loss: 0.002745, mae: 0.052629, mean_q: -0.038870
 51470/100000: episode: 864, duration: 0.340s, episode steps: 53, steps per second: 156, episode reward: 11.449, mean reward: 0.216 [0.028, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.156, 10.134], loss: 0.002782, mae: 0.054704, mean_q: 0.024447
 51523/100000: episode: 865, duration: 0.311s, episode steps: 53, steps per second: 170, episode reward: 13.298, mean reward: 0.251 [0.095, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-1.136, 10.100], loss: 0.002901, mae: 0.056660, mean_q: 0.018587
 51558/100000: episode: 866, duration: 0.223s, episode steps: 35, steps per second: 157, episode reward: 10.466, mean reward: 0.299 [0.113, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.406, 10.268], loss: 0.002908, mae: 0.056165, mean_q: 0.042818
 51593/100000: episode: 867, duration: 0.231s, episode steps: 35, steps per second: 152, episode reward: 11.236, mean reward: 0.321 [0.214, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-2.048, 10.382], loss: 0.002823, mae: 0.056222, mean_q: 0.063465
 51630/100000: episode: 868, duration: 0.243s, episode steps: 37, steps per second: 153, episode reward: 9.182, mean reward: 0.248 [0.120, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.516, 10.351], loss: 0.005854, mae: 0.070064, mean_q: 0.079973
 51683/100000: episode: 869, duration: 0.348s, episode steps: 53, steps per second: 153, episode reward: 19.933, mean reward: 0.376 [0.190, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.418, 10.100], loss: 0.003518, mae: 0.062620, mean_q: 0.055553
 51689/100000: episode: 870, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 2.291, mean reward: 0.382 [0.319, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.443, 10.487], loss: 0.003810, mae: 0.065521, mean_q: 0.003803
 51726/100000: episode: 871, duration: 0.234s, episode steps: 37, steps per second: 158, episode reward: 8.206, mean reward: 0.222 [0.028, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.629, 10.100], loss: 0.004995, mae: 0.065172, mean_q: 0.068458
 51761/100000: episode: 872, duration: 0.216s, episode steps: 35, steps per second: 162, episode reward: 9.327, mean reward: 0.266 [0.064, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.035, 10.375], loss: 0.004990, mae: 0.070178, mean_q: 0.089668
 51814/100000: episode: 873, duration: 0.355s, episode steps: 53, steps per second: 149, episode reward: 20.716, mean reward: 0.391 [0.301, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-1.258, 10.100], loss: 0.002679, mae: 0.055343, mean_q: 0.090432
 51850/100000: episode: 874, duration: 0.223s, episode steps: 36, steps per second: 161, episode reward: 10.380, mean reward: 0.288 [0.106, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.729, 10.259], loss: 0.002999, mae: 0.057345, mean_q: 0.132862
 51878/100000: episode: 875, duration: 0.184s, episode steps: 28, steps per second: 152, episode reward: 10.258, mean reward: 0.366 [0.079, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.063, 10.204], loss: 0.002954, mae: 0.056547, mean_q: 0.120653
 51906/100000: episode: 876, duration: 0.185s, episode steps: 28, steps per second: 151, episode reward: 7.151, mean reward: 0.255 [0.151, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.502, 10.371], loss: 0.002865, mae: 0.056963, mean_q: 0.126443
 51938/100000: episode: 877, duration: 0.191s, episode steps: 32, steps per second: 168, episode reward: 12.143, mean reward: 0.379 [0.250, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.344, 10.100], loss: 0.002625, mae: 0.053424, mean_q: 0.072233
 51970/100000: episode: 878, duration: 0.214s, episode steps: 32, steps per second: 149, episode reward: 11.356, mean reward: 0.355 [0.220, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.327, 10.100], loss: 0.003002, mae: 0.056507, mean_q: 0.089394
 52006/100000: episode: 879, duration: 0.251s, episode steps: 36, steps per second: 143, episode reward: 8.278, mean reward: 0.230 [0.064, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.172, 10.207], loss: 0.002849, mae: 0.057103, mean_q: 0.125141
 52041/100000: episode: 880, duration: 0.226s, episode steps: 35, steps per second: 155, episode reward: 14.305, mean reward: 0.409 [0.206, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.035, 10.496], loss: 0.003103, mae: 0.059715, mean_q: 0.129750
 52094/100000: episode: 881, duration: 0.339s, episode steps: 53, steps per second: 156, episode reward: 15.516, mean reward: 0.293 [0.137, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-1.354, 10.100], loss: 0.002865, mae: 0.056886, mean_q: 0.096784
 52147/100000: episode: 882, duration: 0.360s, episode steps: 53, steps per second: 147, episode reward: 11.271, mean reward: 0.213 [0.042, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.201, 10.124], loss: 0.002794, mae: 0.056159, mean_q: 0.128241
 52153/100000: episode: 883, duration: 0.048s, episode steps: 6, steps per second: 124, episode reward: 2.322, mean reward: 0.387 [0.329, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.508], loss: 0.002523, mae: 0.055102, mean_q: 0.217252
 52189/100000: episode: 884, duration: 0.223s, episode steps: 36, steps per second: 161, episode reward: 12.790, mean reward: 0.355 [0.239, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.554, 10.357], loss: 0.002985, mae: 0.057086, mean_q: 0.165578
 52217/100000: episode: 885, duration: 0.175s, episode steps: 28, steps per second: 160, episode reward: 7.240, mean reward: 0.259 [0.055, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.251, 10.253], loss: 0.002787, mae: 0.055433, mean_q: 0.120756
 52270/100000: episode: 886, duration: 0.335s, episode steps: 53, steps per second: 158, episode reward: 19.249, mean reward: 0.363 [0.148, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.540, 10.100], loss: 0.003104, mae: 0.059389, mean_q: 0.184585
 52288/100000: episode: 887, duration: 0.122s, episode steps: 18, steps per second: 148, episode reward: 6.371, mean reward: 0.354 [0.253, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.416], loss: 0.002342, mae: 0.051113, mean_q: 0.108730
 52325/100000: episode: 888, duration: 0.232s, episode steps: 37, steps per second: 159, episode reward: 10.234, mean reward: 0.277 [0.155, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.486, 10.195], loss: 0.002901, mae: 0.055982, mean_q: 0.213957
 52361/100000: episode: 889, duration: 0.239s, episode steps: 36, steps per second: 150, episode reward: 7.917, mean reward: 0.220 [0.057, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.342, 10.100], loss: 0.003026, mae: 0.058965, mean_q: 0.183572
 52379/100000: episode: 890, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 6.831, mean reward: 0.380 [0.286, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.282, 10.419], loss: 0.002568, mae: 0.053090, mean_q: 0.165829
 52414/100000: episode: 891, duration: 0.219s, episode steps: 35, steps per second: 160, episode reward: 12.228, mean reward: 0.349 [0.226, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.746, 10.489], loss: 0.003202, mae: 0.059703, mean_q: 0.203510
[Info] 200-TH LEVEL FOUND: 0.8794180750846863, Considering 10/90 traces
 52423/100000: episode: 892, duration: 5.036s, episode steps: 9, steps per second: 2, episode reward: 3.479, mean reward: 0.387 [0.337, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.788, 10.100], loss: 0.002557, mae: 0.053963, mean_q: 0.270505
 52463/100000: episode: 893, duration: 0.265s, episode steps: 40, steps per second: 151, episode reward: 18.272, mean reward: 0.457 [0.393, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.503, 10.100], loss: 0.002824, mae: 0.055343, mean_q: 0.181132
 52486/100000: episode: 894, duration: 0.149s, episode steps: 23, steps per second: 154, episode reward: 11.590, mean reward: 0.504 [0.375, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.751, 10.100], loss: 0.003299, mae: 0.061420, mean_q: 0.218332
 52498/100000: episode: 895, duration: 0.079s, episode steps: 12, steps per second: 152, episode reward: 5.057, mean reward: 0.421 [0.353, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.472], loss: 0.003016, mae: 0.057676, mean_q: 0.212105
 52508/100000: episode: 896, duration: 0.072s, episode steps: 10, steps per second: 140, episode reward: 3.939, mean reward: 0.394 [0.306, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.625, 10.462], loss: 0.002648, mae: 0.054591, mean_q: 0.210967
 52520/100000: episode: 897, duration: 0.083s, episode steps: 12, steps per second: 145, episode reward: 5.441, mean reward: 0.453 [0.414, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.964, 10.570], loss: 0.002961, mae: 0.057432, mean_q: 0.264294
 52543/100000: episode: 898, duration: 0.148s, episode steps: 23, steps per second: 155, episode reward: 9.523, mean reward: 0.414 [0.248, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.312, 10.100], loss: 0.002854, mae: 0.057714, mean_q: 0.223584
 52555/100000: episode: 899, duration: 0.085s, episode steps: 12, steps per second: 142, episode reward: 6.419, mean reward: 0.535 [0.496, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.579, 10.602], loss: 0.003215, mae: 0.059964, mean_q: 0.245571
 52599/100000: episode: 900, duration: 0.292s, episode steps: 44, steps per second: 151, episode reward: 17.543, mean reward: 0.399 [0.257, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.322, 10.100], loss: 0.002967, mae: 0.058292, mean_q: 0.191443
 52610/100000: episode: 901, duration: 0.074s, episode steps: 11, steps per second: 150, episode reward: 4.946, mean reward: 0.450 [0.378, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.299, 10.532], loss: 0.002804, mae: 0.054495, mean_q: 0.248443
 52621/100000: episode: 902, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 4.408, mean reward: 0.401 [0.303, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.400, 10.454], loss: 0.002916, mae: 0.056056, mean_q: 0.196172
 52632/100000: episode: 903, duration: 0.076s, episode steps: 11, steps per second: 145, episode reward: 5.530, mean reward: 0.503 [0.406, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.187, 10.462], loss: 0.003264, mae: 0.061644, mean_q: 0.276937
 52651/100000: episode: 904, duration: 0.212s, episode steps: 19, steps per second: 89, episode reward: 8.322, mean reward: 0.438 [0.344, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.401, 10.100], loss: 0.002523, mae: 0.054009, mean_q: 0.242315
 52662/100000: episode: 905, duration: 0.077s, episode steps: 11, steps per second: 142, episode reward: 4.510, mean reward: 0.410 [0.305, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.437], loss: 0.002534, mae: 0.051730, mean_q: 0.278609
 52685/100000: episode: 906, duration: 0.144s, episode steps: 23, steps per second: 160, episode reward: 8.542, mean reward: 0.371 [0.205, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.302, 10.100], loss: 0.003024, mae: 0.059990, mean_q: 0.295600
 52704/100000: episode: 907, duration: 0.119s, episode steps: 19, steps per second: 160, episode reward: 8.643, mean reward: 0.455 [0.311, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.328, 10.100], loss: 0.002804, mae: 0.055954, mean_q: 0.289384
 52715/100000: episode: 908, duration: 0.090s, episode steps: 11, steps per second: 123, episode reward: 4.589, mean reward: 0.417 [0.311, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.350, 10.483], loss: 0.002641, mae: 0.055516, mean_q: 0.206599
 52734/100000: episode: 909, duration: 0.123s, episode steps: 19, steps per second: 154, episode reward: 7.751, mean reward: 0.408 [0.336, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.488, 10.100], loss: 0.003279, mae: 0.060400, mean_q: 0.294391
 52746/100000: episode: 910, duration: 0.085s, episode steps: 12, steps per second: 142, episode reward: 5.926, mean reward: 0.494 [0.410, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.516], loss: 0.002971, mae: 0.056625, mean_q: 0.247819
 52757/100000: episode: 911, duration: 0.083s, episode steps: 11, steps per second: 133, episode reward: 4.582, mean reward: 0.417 [0.366, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.520], loss: 0.002910, mae: 0.058278, mean_q: 0.311777
 52769/100000: episode: 912, duration: 0.085s, episode steps: 12, steps per second: 142, episode reward: 4.843, mean reward: 0.404 [0.333, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.482], loss: 0.002644, mae: 0.054520, mean_q: 0.293714
 52780/100000: episode: 913, duration: 0.079s, episode steps: 11, steps per second: 139, episode reward: 6.359, mean reward: 0.578 [0.492, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.658, 10.539], loss: 0.003133, mae: 0.059289, mean_q: 0.293723
 52799/100000: episode: 914, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 9.139, mean reward: 0.481 [0.352, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.431, 10.100], loss: 0.002781, mae: 0.056656, mean_q: 0.292042
 52810/100000: episode: 915, duration: 0.079s, episode steps: 11, steps per second: 139, episode reward: 4.622, mean reward: 0.420 [0.363, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.595, 10.508], loss: 0.002589, mae: 0.053178, mean_q: 0.356737
 52820/100000: episode: 916, duration: 0.071s, episode steps: 10, steps per second: 142, episode reward: 5.264, mean reward: 0.526 [0.459, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.573], loss: 0.003742, mae: 0.066816, mean_q: 0.360340
 52839/100000: episode: 917, duration: 0.123s, episode steps: 19, steps per second: 155, episode reward: 10.292, mean reward: 0.542 [0.439, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.406, 10.100], loss: 0.003125, mae: 0.057177, mean_q: 0.315935
 52858/100000: episode: 918, duration: 0.126s, episode steps: 19, steps per second: 150, episode reward: 7.853, mean reward: 0.413 [0.322, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.342, 10.100], loss: 0.003283, mae: 0.061524, mean_q: 0.305810
 52877/100000: episode: 919, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 8.371, mean reward: 0.441 [0.318, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.242, 10.100], loss: 0.002671, mae: 0.054343, mean_q: 0.295658
 52889/100000: episode: 920, duration: 0.085s, episode steps: 12, steps per second: 142, episode reward: 5.922, mean reward: 0.493 [0.362, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.829, 10.557], loss: 0.003348, mae: 0.060292, mean_q: 0.320814
 52901/100000: episode: 921, duration: 0.077s, episode steps: 12, steps per second: 155, episode reward: 5.002, mean reward: 0.417 [0.305, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-1.104, 10.433], loss: 0.002665, mae: 0.055435, mean_q: 0.293904
 52911/100000: episode: 922, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 4.600, mean reward: 0.460 [0.409, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.450], loss: 0.003054, mae: 0.059392, mean_q: 0.266604
 52923/100000: episode: 923, duration: 0.079s, episode steps: 12, steps per second: 151, episode reward: 6.121, mean reward: 0.510 [0.407, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.415, 10.505], loss: 0.002913, mae: 0.057292, mean_q: 0.298728
 52934/100000: episode: 924, duration: 0.086s, episode steps: 11, steps per second: 127, episode reward: 5.324, mean reward: 0.484 [0.401, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.609, 10.505], loss: 0.003369, mae: 0.061246, mean_q: 0.344298
 52978/100000: episode: 925, duration: 0.301s, episode steps: 44, steps per second: 146, episode reward: 13.588, mean reward: 0.309 [0.143, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.870, 10.100], loss: 0.002904, mae: 0.057367, mean_q: 0.315424
 52997/100000: episode: 926, duration: 0.157s, episode steps: 19, steps per second: 121, episode reward: 9.716, mean reward: 0.511 [0.401, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-1.259, 10.100], loss: 0.002830, mae: 0.054848, mean_q: 0.284113
 53016/100000: episode: 927, duration: 0.130s, episode steps: 19, steps per second: 146, episode reward: 7.009, mean reward: 0.369 [0.206, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.509, 10.100], loss: 0.002579, mae: 0.055370, mean_q: 0.276965
 53056/100000: episode: 928, duration: 0.247s, episode steps: 40, steps per second: 162, episode reward: 13.125, mean reward: 0.328 [0.193, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.281, 10.100], loss: 0.003639, mae: 0.063647, mean_q: 0.355160
 53100/100000: episode: 929, duration: 0.296s, episode steps: 44, steps per second: 149, episode reward: 15.029, mean reward: 0.342 [0.209, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.542, 10.100], loss: 0.003339, mae: 0.060713, mean_q: 0.371081
 53112/100000: episode: 930, duration: 0.084s, episode steps: 12, steps per second: 142, episode reward: 4.895, mean reward: 0.408 [0.311, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.164, 10.556], loss: 0.003714, mae: 0.065705, mean_q: 0.378380
 53156/100000: episode: 931, duration: 0.280s, episode steps: 44, steps per second: 157, episode reward: 17.590, mean reward: 0.400 [0.294, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-1.786, 10.100], loss: 0.003147, mae: 0.060733, mean_q: 0.376783
 53168/100000: episode: 932, duration: 0.083s, episode steps: 12, steps per second: 144, episode reward: 5.356, mean reward: 0.446 [0.382, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.525], loss: 0.004499, mae: 0.072021, mean_q: 0.304617
 53180/100000: episode: 933, duration: 0.089s, episode steps: 12, steps per second: 135, episode reward: 5.643, mean reward: 0.470 [0.345, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.310, 10.514], loss: 0.006944, mae: 0.072851, mean_q: 0.418119
 53199/100000: episode: 934, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 6.649, mean reward: 0.350 [0.253, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.202, 10.100], loss: 0.003496, mae: 0.063915, mean_q: 0.414939
 53218/100000: episode: 935, duration: 0.132s, episode steps: 19, steps per second: 144, episode reward: 10.626, mean reward: 0.559 [0.438, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.497, 10.100], loss: 0.003614, mae: 0.066352, mean_q: 0.376556
 53230/100000: episode: 936, duration: 0.086s, episode steps: 12, steps per second: 140, episode reward: 6.532, mean reward: 0.544 [0.471, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-2.025, 10.625], loss: 0.003657, mae: 0.066758, mean_q: 0.419440
 53253/100000: episode: 937, duration: 0.142s, episode steps: 23, steps per second: 162, episode reward: 10.316, mean reward: 0.449 [0.361, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.572, 10.100], loss: 0.003251, mae: 0.061518, mean_q: 0.406583
 53293/100000: episode: 938, duration: 0.300s, episode steps: 40, steps per second: 133, episode reward: 11.092, mean reward: 0.277 [0.092, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.446, 10.100], loss: 0.004735, mae: 0.064197, mean_q: 0.384373
 53312/100000: episode: 939, duration: 0.127s, episode steps: 19, steps per second: 150, episode reward: 8.016, mean reward: 0.422 [0.276, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.249, 10.100], loss: 0.003493, mae: 0.065291, mean_q: 0.372994
 53324/100000: episode: 940, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 5.233, mean reward: 0.436 [0.304, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.929, 10.480], loss: 0.003622, mae: 0.062480, mean_q: 0.397874
 53343/100000: episode: 941, duration: 0.127s, episode steps: 19, steps per second: 149, episode reward: 10.118, mean reward: 0.533 [0.407, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.873, 10.100], loss: 0.003366, mae: 0.061626, mean_q: 0.368853
 53353/100000: episode: 942, duration: 0.067s, episode steps: 10, steps per second: 149, episode reward: 4.648, mean reward: 0.465 [0.392, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.614, 10.529], loss: 0.003209, mae: 0.061907, mean_q: 0.426652
 53393/100000: episode: 943, duration: 0.256s, episode steps: 40, steps per second: 156, episode reward: 13.586, mean reward: 0.340 [0.242, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.173, 10.100], loss: 0.003664, mae: 0.062910, mean_q: 0.415862
 53403/100000: episode: 944, duration: 0.071s, episode steps: 10, steps per second: 140, episode reward: 4.429, mean reward: 0.443 [0.399, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.487], loss: 0.006408, mae: 0.070637, mean_q: 0.455968
 53413/100000: episode: 945, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 4.562, mean reward: 0.456 [0.426, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.500], loss: 0.004168, mae: 0.071318, mean_q: 0.400918
 53425/100000: episode: 946, duration: 0.085s, episode steps: 12, steps per second: 141, episode reward: 5.401, mean reward: 0.450 [0.351, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.538], loss: 0.005469, mae: 0.068331, mean_q: 0.469630
 53436/100000: episode: 947, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 4.915, mean reward: 0.447 [0.397, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.528], loss: 0.002918, mae: 0.059726, mean_q: 0.393110
 53447/100000: episode: 948, duration: 0.093s, episode steps: 11, steps per second: 119, episode reward: 4.355, mean reward: 0.396 [0.295, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.158, 10.375], loss: 0.003869, mae: 0.063861, mean_q: 0.422080
 53458/100000: episode: 949, duration: 0.081s, episode steps: 11, steps per second: 135, episode reward: 5.458, mean reward: 0.496 [0.383, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.408, 10.480], loss: 0.002978, mae: 0.061061, mean_q: 0.449840
 53477/100000: episode: 950, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 7.848, mean reward: 0.413 [0.335, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.446, 10.100], loss: 0.002823, mae: 0.056554, mean_q: 0.343882
 53488/100000: episode: 951, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 5.185, mean reward: 0.471 [0.395, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.516], loss: 0.002868, mae: 0.057851, mean_q: 0.442503
 53499/100000: episode: 952, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 5.185, mean reward: 0.471 [0.398, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.460], loss: 0.003239, mae: 0.056475, mean_q: 0.428941
 53518/100000: episode: 953, duration: 0.123s, episode steps: 19, steps per second: 154, episode reward: 8.272, mean reward: 0.435 [0.286, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.557, 10.100], loss: 0.002966, mae: 0.058503, mean_q: 0.464338
 53537/100000: episode: 954, duration: 0.120s, episode steps: 19, steps per second: 159, episode reward: 8.481, mean reward: 0.446 [0.362, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.648, 10.100], loss: 0.003467, mae: 0.061682, mean_q: 0.418056
 53556/100000: episode: 955, duration: 0.127s, episode steps: 19, steps per second: 150, episode reward: 7.806, mean reward: 0.411 [0.298, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.170, 10.100], loss: 0.003370, mae: 0.064848, mean_q: 0.439992
 53566/100000: episode: 956, duration: 0.073s, episode steps: 10, steps per second: 137, episode reward: 4.675, mean reward: 0.468 [0.332, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.664, 10.503], loss: 0.003615, mae: 0.064653, mean_q: 0.460036
 53578/100000: episode: 957, duration: 0.082s, episode steps: 12, steps per second: 146, episode reward: 5.315, mean reward: 0.443 [0.335, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.481], loss: 0.003296, mae: 0.063133, mean_q: 0.470014
 53597/100000: episode: 958, duration: 0.127s, episode steps: 19, steps per second: 150, episode reward: 9.005, mean reward: 0.474 [0.405, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.519, 10.100], loss: 0.003612, mae: 0.063950, mean_q: 0.448666
 53609/100000: episode: 959, duration: 0.080s, episode steps: 12, steps per second: 150, episode reward: 5.263, mean reward: 0.439 [0.227, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.504], loss: 0.004099, mae: 0.066903, mean_q: 0.467160
 53619/100000: episode: 960, duration: 0.079s, episode steps: 10, steps per second: 127, episode reward: 3.962, mean reward: 0.396 [0.324, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.569], loss: 0.005813, mae: 0.067559, mean_q: 0.524380
 53663/100000: episode: 961, duration: 0.297s, episode steps: 44, steps per second: 148, episode reward: 22.300, mean reward: 0.507 [0.412, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.900, 10.100], loss: 0.005040, mae: 0.065967, mean_q: 0.448760
 53673/100000: episode: 962, duration: 0.069s, episode steps: 10, steps per second: 145, episode reward: 4.994, mean reward: 0.499 [0.427, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-1.145, 10.538], loss: 0.003176, mae: 0.062878, mean_q: 0.469028
 53696/100000: episode: 963, duration: 0.150s, episode steps: 23, steps per second: 154, episode reward: 7.539, mean reward: 0.328 [0.179, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.334, 10.100], loss: 0.002947, mae: 0.059872, mean_q: 0.489085
 53736/100000: episode: 964, duration: 0.259s, episode steps: 40, steps per second: 155, episode reward: 9.254, mean reward: 0.231 [0.034, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.641, 10.100], loss: 0.003365, mae: 0.061703, mean_q: 0.514233
 53776/100000: episode: 965, duration: 0.284s, episode steps: 40, steps per second: 141, episode reward: 7.970, mean reward: 0.199 [0.038, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.757, 10.122], loss: 0.003879, mae: 0.068610, mean_q: 0.512564
[Info] FALSIFICATION!
 53784/100000: episode: 966, duration: 0.058s, episode steps: 8, steps per second: 138, episode reward: 13.722, mean reward: 1.715 [0.415, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.275, 9.813], loss: 0.002963, mae: 0.056592, mean_q: 0.500770
 53884/100000: episode: 967, duration: 0.697s, episode steps: 100, steps per second: 144, episode reward: -18.705, mean reward: -0.187 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.730, 10.227], loss: 0.016615, mae: 0.067840, mean_q: 0.491682
 53984/100000: episode: 968, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: -17.869, mean reward: -0.179 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.921, 10.098], loss: 0.003630, mae: 0.066389, mean_q: 0.479096
 54084/100000: episode: 969, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: -17.544, mean reward: -0.175 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.026, 10.250], loss: 0.003312, mae: 0.063319, mean_q: 0.520861
 54184/100000: episode: 970, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: -17.742, mean reward: -0.177 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.745, 10.330], loss: 0.002840, mae: 0.058041, mean_q: 0.499856
 54284/100000: episode: 971, duration: 0.738s, episode steps: 100, steps per second: 135, episode reward: -12.742, mean reward: -0.127 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.592, 10.098], loss: 0.002918, mae: 0.058570, mean_q: 0.491024
 54384/100000: episode: 972, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: -17.676, mean reward: -0.177 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.680, 10.245], loss: 0.029492, mae: 0.077841, mean_q: 0.500196
 54484/100000: episode: 973, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: -19.551, mean reward: -0.196 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.488, 10.098], loss: 0.003137, mae: 0.060833, mean_q: 0.510570
 54584/100000: episode: 974, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: -17.894, mean reward: -0.179 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.489, 10.351], loss: 0.015700, mae: 0.065583, mean_q: 0.500837
 54684/100000: episode: 975, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: -17.226, mean reward: -0.172 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.829, 10.144], loss: 0.015615, mae: 0.064824, mean_q: 0.472726
 54784/100000: episode: 976, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: -12.894, mean reward: -0.129 [-1.000, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.877, 10.317], loss: 0.002759, mae: 0.056782, mean_q: 0.450030
 54884/100000: episode: 977, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: -18.038, mean reward: -0.180 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.230, 10.343], loss: 0.002756, mae: 0.056511, mean_q: 0.436983
 54984/100000: episode: 978, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -17.610, mean reward: -0.176 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.513, 10.320], loss: 0.002796, mae: 0.056833, mean_q: 0.412578
 55084/100000: episode: 979, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -10.793, mean reward: -0.108 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.776, 10.098], loss: 0.002841, mae: 0.057052, mean_q: 0.414261
 55184/100000: episode: 980, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.304, mean reward: -0.163 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.292, 10.134], loss: 0.002758, mae: 0.055782, mean_q: 0.381615
 55284/100000: episode: 981, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: -19.480, mean reward: -0.195 [-1.000, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.440, 10.170], loss: 0.015768, mae: 0.064084, mean_q: 0.382319
 55384/100000: episode: 982, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.532, mean reward: -0.185 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.034, 10.173], loss: 0.002920, mae: 0.057372, mean_q: 0.347855
 55484/100000: episode: 983, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.540, mean reward: -0.165 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.133, 10.356], loss: 0.016419, mae: 0.066095, mean_q: 0.352555
 55584/100000: episode: 984, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.890, mean reward: -0.159 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.369, 10.272], loss: 0.017989, mae: 0.075121, mean_q: 0.352201
 55684/100000: episode: 985, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -17.801, mean reward: -0.178 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.778, 10.098], loss: 0.003317, mae: 0.061769, mean_q: 0.323233
 55784/100000: episode: 986, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.395, mean reward: -0.174 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.578, 10.098], loss: 0.016290, mae: 0.066971, mean_q: 0.281977
 55884/100000: episode: 987, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -11.370, mean reward: -0.114 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.602, 10.454], loss: 0.003132, mae: 0.058902, mean_q: 0.247834
 55984/100000: episode: 988, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.600, mean reward: -0.166 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.580, 10.245], loss: 0.029065, mae: 0.075812, mean_q: 0.249691
 56084/100000: episode: 989, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -12.246, mean reward: -0.122 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.534, 10.098], loss: 0.027668, mae: 0.065460, mean_q: 0.252906
 56184/100000: episode: 990, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.282, mean reward: -0.153 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.711, 10.342], loss: 0.003770, mae: 0.063497, mean_q: 0.181402
 56284/100000: episode: 991, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.493, mean reward: -0.175 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.963, 10.103], loss: 0.002770, mae: 0.055059, mean_q: 0.160953
 56384/100000: episode: 992, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.903, mean reward: -0.149 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.713, 10.098], loss: 0.015867, mae: 0.064548, mean_q: 0.163913
 56484/100000: episode: 993, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.887, mean reward: -0.179 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.241, 10.253], loss: 0.002797, mae: 0.054874, mean_q: 0.144654
 56584/100000: episode: 994, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: -19.387, mean reward: -0.194 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.810, 10.098], loss: 0.015527, mae: 0.061814, mean_q: 0.143551
 56684/100000: episode: 995, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: -19.349, mean reward: -0.193 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.548, 10.154], loss: 0.002873, mae: 0.055740, mean_q: 0.120928
 56784/100000: episode: 996, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: -8.676, mean reward: -0.087 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.832, 10.380], loss: 0.015939, mae: 0.065857, mean_q: 0.101020
 56884/100000: episode: 997, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: -16.272, mean reward: -0.163 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.085, 10.360], loss: 0.003286, mae: 0.058386, mean_q: 0.078338
 56984/100000: episode: 998, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -11.650, mean reward: -0.117 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.679, 10.098], loss: 0.002716, mae: 0.053920, mean_q: 0.083504
 57084/100000: episode: 999, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -15.046, mean reward: -0.150 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.939, 10.098], loss: 0.015549, mae: 0.061703, mean_q: 0.066256
 57184/100000: episode: 1000, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: -12.447, mean reward: -0.124 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.485, 10.098], loss: 0.015884, mae: 0.063946, mean_q: 0.021115
 57284/100000: episode: 1001, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: -12.444, mean reward: -0.124 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.665, 10.098], loss: 0.002640, mae: 0.052687, mean_q: 0.019637
 57384/100000: episode: 1002, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -18.119, mean reward: -0.181 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.670, 10.098], loss: 0.015468, mae: 0.060234, mean_q: 0.009486
 57484/100000: episode: 1003, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: -17.439, mean reward: -0.174 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.180, 10.118], loss: 0.015887, mae: 0.062248, mean_q: -0.048185
 57584/100000: episode: 1004, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -12.945, mean reward: -0.129 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.789, 10.098], loss: 0.004778, mae: 0.063641, mean_q: -0.072505
 57684/100000: episode: 1005, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.836, mean reward: -0.158 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.620, 10.223], loss: 0.018130, mae: 0.074643, mean_q: -0.109374
 57784/100000: episode: 1006, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -17.883, mean reward: -0.179 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.471, 10.142], loss: 0.015155, mae: 0.059221, mean_q: -0.060136
 57884/100000: episode: 1007, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: -16.492, mean reward: -0.165 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.495, 10.150], loss: 0.002423, mae: 0.049894, mean_q: -0.124341
 57984/100000: episode: 1008, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: -17.226, mean reward: -0.172 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.525, 10.098], loss: 0.002477, mae: 0.050385, mean_q: -0.131759
 58084/100000: episode: 1009, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: -15.006, mean reward: -0.150 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.280, 10.098], loss: 0.015672, mae: 0.060478, mean_q: -0.193133
 58184/100000: episode: 1010, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: -19.410, mean reward: -0.194 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.315, 10.216], loss: 0.003134, mae: 0.055820, mean_q: -0.186696
 58284/100000: episode: 1011, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -16.657, mean reward: -0.167 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.783, 10.099], loss: 0.015610, mae: 0.061046, mean_q: -0.231132
 58384/100000: episode: 1012, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: -19.869, mean reward: -0.199 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.256, 10.101], loss: 0.002811, mae: 0.052640, mean_q: -0.205619
 58484/100000: episode: 1013, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.603, mean reward: -0.196 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.526, 10.098], loss: 0.015424, mae: 0.059150, mean_q: -0.243202
 58584/100000: episode: 1014, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -17.122, mean reward: -0.171 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.171, 10.098], loss: 0.028494, mae: 0.070293, mean_q: -0.272784
 58684/100000: episode: 1015, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.677, mean reward: -0.157 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.879, 10.098], loss: 0.002378, mae: 0.048404, mean_q: -0.315424
 58784/100000: episode: 1016, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.247, mean reward: -0.142 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.300, 10.428], loss: 0.002498, mae: 0.049289, mean_q: -0.318794
 58884/100000: episode: 1017, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -13.456, mean reward: -0.135 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.581, 10.098], loss: 0.002372, mae: 0.048195, mean_q: -0.306523
 58984/100000: episode: 1018, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -16.859, mean reward: -0.169 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.498, 10.098], loss: 0.002494, mae: 0.048967, mean_q: -0.289105
 59084/100000: episode: 1019, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -18.061, mean reward: -0.181 [-1.000, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.744, 10.098], loss: 0.002387, mae: 0.048212, mean_q: -0.289481
 59184/100000: episode: 1020, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -16.768, mean reward: -0.168 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.821, 10.109], loss: 0.002530, mae: 0.050273, mean_q: -0.310318
 59284/100000: episode: 1021, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -14.459, mean reward: -0.145 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.015, 10.098], loss: 0.002387, mae: 0.049639, mean_q: -0.282505
 59384/100000: episode: 1022, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: -17.761, mean reward: -0.178 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.685, 10.098], loss: 0.002259, mae: 0.047087, mean_q: -0.321766
 59484/100000: episode: 1023, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: -12.328, mean reward: -0.123 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.520, 10.552], loss: 0.002333, mae: 0.047465, mean_q: -0.304706
 59584/100000: episode: 1024, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.826, mean reward: -0.188 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.101, 10.098], loss: 0.002496, mae: 0.049206, mean_q: -0.277404
 59684/100000: episode: 1025, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.041, mean reward: -0.180 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.191, 10.098], loss: 0.002310, mae: 0.047421, mean_q: -0.298572
 59784/100000: episode: 1026, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: -17.543, mean reward: -0.175 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.424, 10.098], loss: 0.002482, mae: 0.049478, mean_q: -0.260603
 59884/100000: episode: 1027, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.801, mean reward: -0.178 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.318, 10.189], loss: 0.002328, mae: 0.048244, mean_q: -0.327233
 59984/100000: episode: 1028, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -19.616, mean reward: -0.196 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.116, 10.222], loss: 0.002323, mae: 0.047872, mean_q: -0.310785
 60084/100000: episode: 1029, duration: 0.772s, episode steps: 100, steps per second: 130, episode reward: -17.700, mean reward: -0.177 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.610, 10.242], loss: 0.002491, mae: 0.049317, mean_q: -0.294414
 60184/100000: episode: 1030, duration: 0.740s, episode steps: 100, steps per second: 135, episode reward: -18.595, mean reward: -0.186 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.192, 10.254], loss: 0.002414, mae: 0.047976, mean_q: -0.298648
 60284/100000: episode: 1031, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: -19.088, mean reward: -0.191 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.884, 10.237], loss: 0.002412, mae: 0.048428, mean_q: -0.278563
 60384/100000: episode: 1032, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: -21.341, mean reward: -0.213 [-1.000, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.005, 10.202], loss: 0.002322, mae: 0.047951, mean_q: -0.299849
 60484/100000: episode: 1033, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.720, mean reward: -0.187 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.869, 10.187], loss: 0.002545, mae: 0.049750, mean_q: -0.311135
 60584/100000: episode: 1034, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: -8.046, mean reward: -0.080 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.433, 10.098], loss: 0.002384, mae: 0.049286, mean_q: -0.312516
 60684/100000: episode: 1035, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: -18.496, mean reward: -0.185 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.327, 10.315], loss: 0.002521, mae: 0.050801, mean_q: -0.291562
 60784/100000: episode: 1036, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: -19.924, mean reward: -0.199 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.131, 10.201], loss: 0.002291, mae: 0.046704, mean_q: -0.323624
 60884/100000: episode: 1037, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: -13.146, mean reward: -0.131 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.507, 10.098], loss: 0.002290, mae: 0.047754, mean_q: -0.307404
 60984/100000: episode: 1038, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -17.211, mean reward: -0.172 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.325, 10.098], loss: 0.002314, mae: 0.049517, mean_q: -0.312753
 61084/100000: episode: 1039, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -17.938, mean reward: -0.179 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.618, 10.177], loss: 0.002367, mae: 0.047734, mean_q: -0.278936
 61184/100000: episode: 1040, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -16.453, mean reward: -0.165 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.444, 10.098], loss: 0.004612, mae: 0.064325, mean_q: -0.321499
 61284/100000: episode: 1041, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: -18.290, mean reward: -0.183 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.856, 10.124], loss: 0.002395, mae: 0.048569, mean_q: -0.317125
 61384/100000: episode: 1042, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: -19.725, mean reward: -0.197 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.445, 10.098], loss: 0.002271, mae: 0.047595, mean_q: -0.316415
 61484/100000: episode: 1043, duration: 0.689s, episode steps: 100, steps per second: 145, episode reward: -16.425, mean reward: -0.164 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.236, 10.369], loss: 0.004862, mae: 0.065976, mean_q: -0.277842
 61584/100000: episode: 1044, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: -18.107, mean reward: -0.181 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.574, 10.166], loss: 0.002475, mae: 0.049462, mean_q: -0.279105
 61684/100000: episode: 1045, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -16.122, mean reward: -0.161 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.899, 10.098], loss: 0.002439, mae: 0.048398, mean_q: -0.349881
 61784/100000: episode: 1046, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: -16.124, mean reward: -0.161 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.330, 10.098], loss: 0.002236, mae: 0.046230, mean_q: -0.317105
 61884/100000: episode: 1047, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: -17.121, mean reward: -0.171 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.999, 10.098], loss: 0.002513, mae: 0.049840, mean_q: -0.294077
 61984/100000: episode: 1048, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: -18.487, mean reward: -0.185 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.719, 10.274], loss: 0.002296, mae: 0.046694, mean_q: -0.331683
 62084/100000: episode: 1049, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.102, mean reward: -0.171 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.474, 10.300], loss: 0.002222, mae: 0.046460, mean_q: -0.295165
 62184/100000: episode: 1050, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: -15.802, mean reward: -0.158 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.495, 10.131], loss: 0.002414, mae: 0.047411, mean_q: -0.353896
 62284/100000: episode: 1051, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -18.664, mean reward: -0.187 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.053, 10.098], loss: 0.002418, mae: 0.048643, mean_q: -0.320744
 62384/100000: episode: 1052, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -17.703, mean reward: -0.177 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.654, 10.263], loss: 0.002262, mae: 0.046731, mean_q: -0.304066
 62484/100000: episode: 1053, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.325, mean reward: -0.173 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.732, 10.098], loss: 0.002453, mae: 0.049464, mean_q: -0.337248
 62584/100000: episode: 1054, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -16.820, mean reward: -0.168 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.370, 10.098], loss: 0.002520, mae: 0.048845, mean_q: -0.330507
 62684/100000: episode: 1055, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -14.717, mean reward: -0.147 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.827, 10.098], loss: 0.002342, mae: 0.046271, mean_q: -0.355891
 62784/100000: episode: 1056, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.339, mean reward: -0.163 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.662, 10.175], loss: 0.002305, mae: 0.046628, mean_q: -0.337375
 62884/100000: episode: 1057, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -12.120, mean reward: -0.121 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.824, 10.363], loss: 0.002578, mae: 0.050054, mean_q: -0.311017
 62984/100000: episode: 1058, duration: 0.593s, episode steps: 100, steps per second: 168, episode reward: -19.711, mean reward: -0.197 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.917, 10.201], loss: 0.003090, mae: 0.054016, mean_q: -0.320213
 63084/100000: episode: 1059, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: -18.340, mean reward: -0.183 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.477, 10.228], loss: 0.005137, mae: 0.064619, mean_q: -0.326380
 63184/100000: episode: 1060, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: -14.886, mean reward: -0.149 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.653, 10.145], loss: 0.002849, mae: 0.053937, mean_q: -0.307618
 63284/100000: episode: 1061, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.876, mean reward: -0.179 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.183, 10.098], loss: 0.002432, mae: 0.047664, mean_q: -0.374539
 63384/100000: episode: 1062, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.742, mean reward: -0.137 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.889, 10.098], loss: 0.002366, mae: 0.047909, mean_q: -0.297737
 63484/100000: episode: 1063, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -15.139, mean reward: -0.151 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.548, 10.098], loss: 0.002314, mae: 0.047055, mean_q: -0.324911
 63584/100000: episode: 1064, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.378, mean reward: -0.154 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.451, 10.098], loss: 0.002421, mae: 0.048477, mean_q: -0.330009
 63684/100000: episode: 1065, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -20.199, mean reward: -0.202 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.518, 10.153], loss: 0.002247, mae: 0.047422, mean_q: -0.316078
[Info] 100-TH LEVEL FOUND: 0.6372981071472168, Considering 10/90 traces
 63784/100000: episode: 1066, duration: 4.824s, episode steps: 100, steps per second: 21, episode reward: -18.898, mean reward: -0.189 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.892, 10.098], loss: 0.002312, mae: 0.046985, mean_q: -0.302765
 63814/100000: episode: 1067, duration: 0.169s, episode steps: 30, steps per second: 177, episode reward: 9.459, mean reward: 0.315 [0.237, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.095, 10.385], loss: 0.002517, mae: 0.048921, mean_q: -0.307433
 63847/100000: episode: 1068, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 8.506, mean reward: 0.258 [0.104, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.297, 10.340], loss: 0.002412, mae: 0.049168, mean_q: -0.311468
 63853/100000: episode: 1069, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 2.215, mean reward: 0.369 [0.313, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.514], loss: 0.002819, mae: 0.055166, mean_q: -0.234964
 63886/100000: episode: 1070, duration: 0.202s, episode steps: 33, steps per second: 163, episode reward: 14.207, mean reward: 0.431 [0.243, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.923, 10.475], loss: 0.002385, mae: 0.047749, mean_q: -0.324341
 63895/100000: episode: 1071, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 3.185, mean reward: 0.354 [0.284, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.364, 10.100], loss: 0.002272, mae: 0.044827, mean_q: -0.308398
 63907/100000: episode: 1072, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 4.102, mean reward: 0.342 [0.257, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.305, 10.100], loss: 0.002177, mae: 0.044831, mean_q: -0.381324
 63916/100000: episode: 1073, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 3.054, mean reward: 0.339 [0.311, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.425, 10.100], loss: 0.002215, mae: 0.045540, mean_q: -0.328910
 63952/100000: episode: 1074, duration: 0.205s, episode steps: 36, steps per second: 175, episode reward: 10.234, mean reward: 0.284 [0.082, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.066, 10.100], loss: 0.002196, mae: 0.045773, mean_q: -0.317941
[Info] FALSIFICATION!
 63959/100000: episode: 1075, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 12.244, mean reward: 1.749 [0.292, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.222, 10.098], loss: 0.002412, mae: 0.048165, mean_q: -0.269012
 64059/100000: episode: 1076, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.607, mean reward: -0.176 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.779, 10.170], loss: 0.018839, mae: 0.066787, mean_q: -0.301892
 64159/100000: episode: 1077, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.228, mean reward: -0.182 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.446, 10.251], loss: 0.002702, mae: 0.051539, mean_q: -0.296194
 64259/100000: episode: 1078, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.846, mean reward: -0.178 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.177, 10.098], loss: 0.017922, mae: 0.067433, mean_q: -0.299457
 64359/100000: episode: 1079, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.529, mean reward: -0.185 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.490, 10.228], loss: 0.016883, mae: 0.060186, mean_q: -0.300391
 64459/100000: episode: 1080, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.836, mean reward: -0.198 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.026, 10.118], loss: 0.016256, mae: 0.059333, mean_q: -0.311607
 64559/100000: episode: 1081, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -17.318, mean reward: -0.173 [-1.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.253, 10.307], loss: 0.016932, mae: 0.060303, mean_q: -0.290602
 64659/100000: episode: 1082, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.859, mean reward: -0.189 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.224, 10.098], loss: 0.002634, mae: 0.051210, mean_q: -0.301957
 64759/100000: episode: 1083, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.679, mean reward: -0.187 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.455, 10.133], loss: 0.032118, mae: 0.078603, mean_q: -0.310316
 64859/100000: episode: 1084, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.068, mean reward: -0.151 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.279, 10.135], loss: 0.031000, mae: 0.078974, mean_q: -0.287658
 64959/100000: episode: 1085, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.374, mean reward: -0.164 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.119, 10.286], loss: 0.016830, mae: 0.065613, mean_q: -0.252262
 65059/100000: episode: 1086, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -14.680, mean reward: -0.147 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.863, 10.290], loss: 0.016285, mae: 0.061203, mean_q: -0.295424
 65159/100000: episode: 1087, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.257, mean reward: -0.173 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.982, 10.198], loss: 0.002828, mae: 0.052408, mean_q: -0.302640
 65259/100000: episode: 1088, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -13.138, mean reward: -0.131 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.299, 10.098], loss: 0.002774, mae: 0.051610, mean_q: -0.294746
 65359/100000: episode: 1089, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -20.541, mean reward: -0.205 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.809, 10.098], loss: 0.002601, mae: 0.049336, mean_q: -0.311670
 65459/100000: episode: 1090, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -14.810, mean reward: -0.148 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.569, 10.098], loss: 0.017422, mae: 0.066931, mean_q: -0.289987
 65559/100000: episode: 1091, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -13.941, mean reward: -0.139 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.115, 10.183], loss: 0.002923, mae: 0.053427, mean_q: -0.275079
 65659/100000: episode: 1092, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -18.405, mean reward: -0.184 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.300, 10.098], loss: 0.002690, mae: 0.050263, mean_q: -0.271673
 65759/100000: episode: 1093, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.976, mean reward: -0.160 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.588, 10.098], loss: 0.030824, mae: 0.072519, mean_q: -0.302513
 65859/100000: episode: 1094, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.562, mean reward: -0.186 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.685, 10.098], loss: 0.002925, mae: 0.053021, mean_q: -0.288942
 65959/100000: episode: 1095, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: -17.299, mean reward: -0.173 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.951, 10.098], loss: 0.016696, mae: 0.061478, mean_q: -0.269554
 66059/100000: episode: 1096, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.499, mean reward: -0.195 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.170, 10.138], loss: 0.016432, mae: 0.062303, mean_q: -0.279377
 66159/100000: episode: 1097, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.075, mean reward: -0.201 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.482, 10.197], loss: 0.003320, mae: 0.058945, mean_q: -0.308411
 66259/100000: episode: 1098, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -15.652, mean reward: -0.157 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.251, 10.368], loss: 0.016710, mae: 0.059772, mean_q: -0.290468
 66359/100000: episode: 1099, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: -18.217, mean reward: -0.182 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.468, 10.187], loss: 0.016432, mae: 0.060649, mean_q: -0.272190
 66459/100000: episode: 1100, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.053, mean reward: -0.171 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.746, 10.201], loss: 0.002861, mae: 0.052544, mean_q: -0.294554
 66559/100000: episode: 1101, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: -18.259, mean reward: -0.183 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.947, 10.116], loss: 0.002543, mae: 0.050469, mean_q: -0.285841
 66659/100000: episode: 1102, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -16.153, mean reward: -0.162 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.951, 10.210], loss: 0.029552, mae: 0.064717, mean_q: -0.272682
 66759/100000: episode: 1103, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -14.569, mean reward: -0.146 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.871, 10.246], loss: 0.018430, mae: 0.075276, mean_q: -0.288585
 66859/100000: episode: 1104, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -19.672, mean reward: -0.197 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.885, 10.180], loss: 0.004857, mae: 0.067266, mean_q: -0.276819
 66959/100000: episode: 1105, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: -16.761, mean reward: -0.168 [-1.000, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.643, 10.204], loss: 0.002764, mae: 0.052182, mean_q: -0.288677
 67059/100000: episode: 1106, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -17.864, mean reward: -0.179 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.612, 10.098], loss: 0.002642, mae: 0.051706, mean_q: -0.278255
 67159/100000: episode: 1107, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -10.840, mean reward: -0.108 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.942, 10.344], loss: 0.002619, mae: 0.049978, mean_q: -0.284041
 67259/100000: episode: 1108, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.763, mean reward: -0.148 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.964, 10.128], loss: 0.002567, mae: 0.050293, mean_q: -0.299823
 67359/100000: episode: 1109, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.842, mean reward: -0.198 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.646, 10.098], loss: 0.002459, mae: 0.048771, mean_q: -0.294290
 67459/100000: episode: 1110, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.938, mean reward: -0.189 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.991, 10.180], loss: 0.030649, mae: 0.073895, mean_q: -0.300308
 67559/100000: episode: 1111, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.073, mean reward: -0.181 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.213, 10.098], loss: 0.015840, mae: 0.058204, mean_q: -0.292118
 67659/100000: episode: 1112, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.698, mean reward: -0.167 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.841, 10.098], loss: 0.017106, mae: 0.066652, mean_q: -0.282568
 67759/100000: episode: 1113, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -13.340, mean reward: -0.133 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.133, 10.491], loss: 0.002887, mae: 0.052301, mean_q: -0.310024
 67859/100000: episode: 1114, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: -17.999, mean reward: -0.180 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.114, 10.400], loss: 0.002741, mae: 0.050098, mean_q: -0.304365
 67959/100000: episode: 1115, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: -17.945, mean reward: -0.179 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.967, 10.165], loss: 0.017051, mae: 0.064816, mean_q: -0.300733
 68059/100000: episode: 1116, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -17.762, mean reward: -0.178 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.765, 10.176], loss: 0.002700, mae: 0.050505, mean_q: -0.335368
 68159/100000: episode: 1117, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.533, mean reward: -0.175 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.093, 10.098], loss: 0.017374, mae: 0.067143, mean_q: -0.271964
 68259/100000: episode: 1118, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.370, mean reward: -0.194 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.805, 10.230], loss: 0.002752, mae: 0.050928, mean_q: -0.302058
 68359/100000: episode: 1119, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -13.434, mean reward: -0.134 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.040, 10.324], loss: 0.002775, mae: 0.052096, mean_q: -0.265304
 68459/100000: episode: 1120, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.841, mean reward: -0.178 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.193, 10.200], loss: 0.043598, mae: 0.083711, mean_q: -0.302221
 68559/100000: episode: 1121, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -15.380, mean reward: -0.154 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.829, 10.098], loss: 0.003229, mae: 0.056802, mean_q: -0.268941
 68659/100000: episode: 1122, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.771, mean reward: -0.198 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.328, 10.158], loss: 0.017325, mae: 0.073019, mean_q: -0.282494
 68759/100000: episode: 1123, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -14.508, mean reward: -0.145 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.237, 10.098], loss: 0.015959, mae: 0.061390, mean_q: -0.282902
 68859/100000: episode: 1124, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -10.578, mean reward: -0.106 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.538, 10.432], loss: 0.015903, mae: 0.057771, mean_q: -0.304459
 68959/100000: episode: 1125, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.318, mean reward: -0.173 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.515, 10.412], loss: 0.003329, mae: 0.057183, mean_q: -0.321056
 69059/100000: episode: 1126, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -14.191, mean reward: -0.142 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.803, 10.197], loss: 0.002634, mae: 0.050514, mean_q: -0.321073
 69159/100000: episode: 1127, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -20.039, mean reward: -0.200 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.822, 10.098], loss: 0.002435, mae: 0.049963, mean_q: -0.337673
 69259/100000: episode: 1128, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: -17.726, mean reward: -0.177 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.978, 10.283], loss: 0.002694, mae: 0.051050, mean_q: -0.310827
 69359/100000: episode: 1129, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -19.566, mean reward: -0.196 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.941, 10.098], loss: 0.002728, mae: 0.052431, mean_q: -0.281953
 69459/100000: episode: 1130, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -16.609, mean reward: -0.166 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.686, 10.098], loss: 0.002421, mae: 0.047789, mean_q: -0.348949
 69559/100000: episode: 1131, duration: 0.766s, episode steps: 100, steps per second: 131, episode reward: -15.300, mean reward: -0.153 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.145, 10.098], loss: 0.002587, mae: 0.050628, mean_q: -0.326766
 69659/100000: episode: 1132, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -17.400, mean reward: -0.174 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.685, 10.098], loss: 0.002632, mae: 0.051785, mean_q: -0.303999
 69759/100000: episode: 1133, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.351, mean reward: -0.174 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.031, 10.098], loss: 0.002682, mae: 0.051175, mean_q: -0.341789
 69859/100000: episode: 1134, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: -13.913, mean reward: -0.139 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.748, 10.255], loss: 0.002320, mae: 0.048263, mean_q: -0.332899
 69959/100000: episode: 1135, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -15.476, mean reward: -0.155 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.745, 10.155], loss: 0.002630, mae: 0.050363, mean_q: -0.317993
 70059/100000: episode: 1136, duration: 0.652s, episode steps: 100, steps per second: 153, episode reward: -14.412, mean reward: -0.144 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.139, 10.098], loss: 0.002602, mae: 0.051547, mean_q: -0.275819
 70159/100000: episode: 1137, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -15.740, mean reward: -0.157 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.938, 10.098], loss: 0.002505, mae: 0.049196, mean_q: -0.333630
 70259/100000: episode: 1138, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: -20.135, mean reward: -0.201 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.999, 10.098], loss: 0.002512, mae: 0.049613, mean_q: -0.319472
 70359/100000: episode: 1139, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: -14.998, mean reward: -0.150 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.308, 10.258], loss: 0.002393, mae: 0.048915, mean_q: -0.356552
 70459/100000: episode: 1140, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: -16.928, mean reward: -0.169 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.832, 10.301], loss: 0.002559, mae: 0.050412, mean_q: -0.305653
 70559/100000: episode: 1141, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -8.504, mean reward: -0.085 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.435, 10.098], loss: 0.002510, mae: 0.049319, mean_q: -0.304565
 70659/100000: episode: 1142, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.264, mean reward: -0.183 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.680, 10.098], loss: 0.002533, mae: 0.050565, mean_q: -0.320358
 70759/100000: episode: 1143, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.639, mean reward: -0.176 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.844, 10.345], loss: 0.004246, mae: 0.064156, mean_q: -0.300918
 70859/100000: episode: 1144, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -13.583, mean reward: -0.136 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.258, 10.098], loss: 0.002651, mae: 0.051431, mean_q: -0.286858
 70959/100000: episode: 1145, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -17.186, mean reward: -0.172 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.968, 10.127], loss: 0.002773, mae: 0.051102, mean_q: -0.329960
 71059/100000: episode: 1146, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.069, mean reward: -0.161 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.898, 10.098], loss: 0.002629, mae: 0.050672, mean_q: -0.279399
 71159/100000: episode: 1147, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -19.151, mean reward: -0.192 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.906, 10.098], loss: 0.002456, mae: 0.048429, mean_q: -0.312587
 71259/100000: episode: 1148, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -9.498, mean reward: -0.095 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.321, 10.098], loss: 0.002509, mae: 0.048903, mean_q: -0.314212
 71359/100000: episode: 1149, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.283, mean reward: -0.183 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.452, 10.098], loss: 0.002843, mae: 0.052302, mean_q: -0.301269
 71459/100000: episode: 1150, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -12.220, mean reward: -0.122 [-1.000, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.818, 10.326], loss: 0.002504, mae: 0.049123, mean_q: -0.302564
 71559/100000: episode: 1151, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -13.433, mean reward: -0.134 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.152, 10.136], loss: 0.002492, mae: 0.048423, mean_q: -0.310414
 71659/100000: episode: 1152, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.370, mean reward: -0.194 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.345, 10.145], loss: 0.002582, mae: 0.050180, mean_q: -0.279827
 71759/100000: episode: 1153, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.855, mean reward: -0.159 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.578, 10.290], loss: 0.006263, mae: 0.069233, mean_q: -0.305484
 71859/100000: episode: 1154, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.566, mean reward: -0.166 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.252, 10.169], loss: 0.002505, mae: 0.050625, mean_q: -0.325296
 71959/100000: episode: 1155, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -19.542, mean reward: -0.195 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.763, 10.127], loss: 0.002630, mae: 0.050655, mean_q: -0.311195
 72059/100000: episode: 1156, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.140, mean reward: -0.161 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.602, 10.194], loss: 0.002761, mae: 0.053268, mean_q: -0.307628
 72159/100000: episode: 1157, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.786, mean reward: -0.188 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.096, 10.148], loss: 0.002664, mae: 0.051269, mean_q: -0.288717
 72259/100000: episode: 1158, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.911, mean reward: -0.149 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.416, 10.254], loss: 0.002605, mae: 0.050540, mean_q: -0.274278
 72359/100000: episode: 1159, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.739, mean reward: -0.167 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.335, 10.098], loss: 0.002474, mae: 0.049044, mean_q: -0.310578
 72459/100000: episode: 1160, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.697, mean reward: -0.157 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.754, 10.105], loss: 0.002524, mae: 0.049297, mean_q: -0.309255
 72559/100000: episode: 1161, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -13.187, mean reward: -0.132 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.282, 10.480], loss: 0.002567, mae: 0.049171, mean_q: -0.304162
 72659/100000: episode: 1162, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.146, mean reward: -0.171 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.165, 10.098], loss: 0.002454, mae: 0.049205, mean_q: -0.285943
 72759/100000: episode: 1163, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.221, mean reward: -0.192 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.357, 10.098], loss: 0.002378, mae: 0.047961, mean_q: -0.318430
 72859/100000: episode: 1164, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -19.223, mean reward: -0.192 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.924, 10.098], loss: 0.002649, mae: 0.050649, mean_q: -0.301160
 72959/100000: episode: 1165, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.777, mean reward: -0.198 [-1.000, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.643, 10.230], loss: 0.002350, mae: 0.047498, mean_q: -0.319349
 73059/100000: episode: 1166, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.782, mean reward: -0.178 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.386, 10.164], loss: 0.002550, mae: 0.049901, mean_q: -0.317550
 73159/100000: episode: 1167, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -15.286, mean reward: -0.153 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.921, 10.098], loss: 0.002549, mae: 0.049154, mean_q: -0.308090
 73259/100000: episode: 1168, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.996, mean reward: -0.180 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.904, 10.098], loss: 0.002464, mae: 0.048888, mean_q: -0.312193
 73359/100000: episode: 1169, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.341, mean reward: -0.183 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.621, 10.098], loss: 0.002326, mae: 0.047360, mean_q: -0.298388
 73459/100000: episode: 1170, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -12.139, mean reward: -0.121 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.815, 10.372], loss: 0.002428, mae: 0.049017, mean_q: -0.297288
 73559/100000: episode: 1171, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.392, mean reward: -0.184 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.044, 10.098], loss: 0.002426, mae: 0.048538, mean_q: -0.282434
 73659/100000: episode: 1172, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -16.763, mean reward: -0.168 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.415, 10.109], loss: 0.002514, mae: 0.049737, mean_q: -0.327900
 73759/100000: episode: 1173, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.508, mean reward: -0.185 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.779, 10.098], loss: 0.002347, mae: 0.048842, mean_q: -0.298971
 73859/100000: episode: 1174, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -14.929, mean reward: -0.149 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.342, 10.098], loss: 0.002777, mae: 0.053075, mean_q: -0.282193
[Info] 100-TH LEVEL FOUND: 0.5850074291229248, Considering 10/90 traces
 73959/100000: episode: 1175, duration: 4.555s, episode steps: 100, steps per second: 22, episode reward: -17.118, mean reward: -0.171 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.680, 10.118], loss: 0.002456, mae: 0.049141, mean_q: -0.328032
 74014/100000: episode: 1176, duration: 0.298s, episode steps: 55, steps per second: 185, episode reward: 8.866, mean reward: 0.161 [0.028, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-1.163, 10.100], loss: 0.002277, mae: 0.046571, mean_q: -0.349250
 74015/100000: episode: 1177, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.444, mean reward: 0.444 [0.444, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.278], loss: 0.004253, mae: 0.063709, mean_q: -0.157560
 74048/100000: episode: 1178, duration: 0.188s, episode steps: 33, steps per second: 176, episode reward: 12.761, mean reward: 0.387 [0.281, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.962, 10.100], loss: 0.002495, mae: 0.049802, mean_q: -0.253504
 74051/100000: episode: 1179, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 1.058, mean reward: 0.353 [0.338, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.391], loss: 0.002110, mae: 0.045550, mean_q: -0.387373
 74081/100000: episode: 1180, duration: 0.152s, episode steps: 30, steps per second: 197, episode reward: 9.053, mean reward: 0.302 [0.160, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.226, 10.100], loss: 0.002332, mae: 0.046200, mean_q: -0.316348
 74088/100000: episode: 1181, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 2.715, mean reward: 0.388 [0.342, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.973, 10.100], loss: 0.002429, mae: 0.046857, mean_q: -0.324411
 74091/100000: episode: 1182, duration: 0.023s, episode steps: 3, steps per second: 130, episode reward: 0.906, mean reward: 0.302 [0.275, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.415], loss: 0.001817, mae: 0.043907, mean_q: -0.307863
 74125/100000: episode: 1183, duration: 0.174s, episode steps: 34, steps per second: 196, episode reward: 12.828, mean reward: 0.377 [0.242, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.484, 10.100], loss: 0.002533, mae: 0.049903, mean_q: -0.277681
 74180/100000: episode: 1184, duration: 0.292s, episode steps: 55, steps per second: 188, episode reward: 11.247, mean reward: 0.204 [0.101, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.471, 10.100], loss: 0.002429, mae: 0.049251, mean_q: -0.275915
 74187/100000: episode: 1185, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 2.231, mean reward: 0.319 [0.270, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.198, 10.100], loss: 0.002947, mae: 0.055863, mean_q: -0.221515
 74220/100000: episode: 1186, duration: 0.184s, episode steps: 33, steps per second: 179, episode reward: 8.826, mean reward: 0.267 [0.094, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.232, 10.100], loss: 0.002457, mae: 0.050167, mean_q: -0.312142
 74221/100000: episode: 1187, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.368, mean reward: 0.368 [0.368, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.035, 10.326], loss: 0.001691, mae: 0.038810, mean_q: -0.375737
 74290/100000: episode: 1188, duration: 0.372s, episode steps: 69, steps per second: 185, episode reward: 11.857, mean reward: 0.172 [0.009, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.753 [-0.405, 10.103], loss: 0.002462, mae: 0.050133, mean_q: -0.263275
 74345/100000: episode: 1189, duration: 0.283s, episode steps: 55, steps per second: 194, episode reward: 11.983, mean reward: 0.218 [0.021, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.701, 10.160], loss: 0.002587, mae: 0.050581, mean_q: -0.235250
 74352/100000: episode: 1190, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 2.611, mean reward: 0.373 [0.258, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.297, 10.100], loss: 0.002238, mae: 0.048397, mean_q: -0.192313
 74382/100000: episode: 1191, duration: 0.171s, episode steps: 30, steps per second: 175, episode reward: 10.117, mean reward: 0.337 [0.197, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.221, 10.100], loss: 0.002485, mae: 0.049153, mean_q: -0.267483
 74448/100000: episode: 1192, duration: 0.368s, episode steps: 66, steps per second: 179, episode reward: 8.893, mean reward: 0.135 [0.007, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.779 [-0.073, 10.212], loss: 0.002803, mae: 0.053390, mean_q: -0.180800
 74481/100000: episode: 1193, duration: 0.198s, episode steps: 33, steps per second: 166, episode reward: 7.138, mean reward: 0.216 [0.092, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.205, 10.100], loss: 0.002608, mae: 0.051598, mean_q: -0.266937
 74536/100000: episode: 1194, duration: 0.293s, episode steps: 55, steps per second: 188, episode reward: 15.448, mean reward: 0.281 [0.033, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-0.603, 10.142], loss: 0.002467, mae: 0.049456, mean_q: -0.235388
 74566/100000: episode: 1195, duration: 0.151s, episode steps: 30, steps per second: 199, episode reward: 9.973, mean reward: 0.332 [0.226, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.931, 10.100], loss: 0.002918, mae: 0.054875, mean_q: -0.231511
 74600/100000: episode: 1196, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 7.713, mean reward: 0.227 [0.038, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.035, 10.258], loss: 0.002573, mae: 0.051339, mean_q: -0.269517
 74642/100000: episode: 1197, duration: 0.231s, episode steps: 42, steps per second: 182, episode reward: 12.191, mean reward: 0.290 [0.181, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.901, 10.100], loss: 0.002559, mae: 0.051257, mean_q: -0.230580
 74645/100000: episode: 1198, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 1.201, mean reward: 0.400 [0.302, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.554, 10.302], loss: 0.002240, mae: 0.047320, mean_q: -0.214380
 74678/100000: episode: 1199, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 7.475, mean reward: 0.227 [0.124, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.192, 10.100], loss: 0.002583, mae: 0.049787, mean_q: -0.181798
 74679/100000: episode: 1200, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.405, mean reward: 0.405 [0.405, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.291], loss: 0.001282, mae: 0.040273, mean_q: -0.031962
 74712/100000: episode: 1201, duration: 0.195s, episode steps: 33, steps per second: 169, episode reward: 9.408, mean reward: 0.285 [0.123, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.886, 10.100], loss: 0.002438, mae: 0.049476, mean_q: -0.246327
 74742/100000: episode: 1202, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 12.243, mean reward: 0.408 [0.287, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.521, 10.100], loss: 0.002798, mae: 0.053119, mean_q: -0.156449
 74797/100000: episode: 1203, duration: 0.302s, episode steps: 55, steps per second: 182, episode reward: 15.722, mean reward: 0.286 [0.143, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.866 [-0.453, 10.100], loss: 0.002669, mae: 0.051839, mean_q: -0.190976
 74839/100000: episode: 1204, duration: 0.215s, episode steps: 42, steps per second: 195, episode reward: 5.579, mean reward: 0.133 [0.020, 0.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.184, 10.104], loss: 0.002416, mae: 0.048315, mean_q: -0.206593
 74894/100000: episode: 1205, duration: 0.312s, episode steps: 55, steps per second: 176, episode reward: 12.952, mean reward: 0.235 [0.011, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.051, 10.100], loss: 0.004980, mae: 0.066293, mean_q: -0.133841
 74960/100000: episode: 1206, duration: 0.377s, episode steps: 66, steps per second: 175, episode reward: 15.029, mean reward: 0.228 [0.034, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.764 [-1.376, 10.100], loss: 0.003510, mae: 0.060595, mean_q: -0.159712
 74994/100000: episode: 1207, duration: 0.206s, episode steps: 34, steps per second: 165, episode reward: 10.414, mean reward: 0.306 [0.101, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.277, 10.100], loss: 0.002625, mae: 0.053018, mean_q: -0.123354
 74997/100000: episode: 1208, duration: 0.027s, episode steps: 3, steps per second: 112, episode reward: 1.104, mean reward: 0.368 [0.315, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-1.210, 10.453], loss: 0.002787, mae: 0.054973, mean_q: -0.066268
 75063/100000: episode: 1209, duration: 0.372s, episode steps: 66, steps per second: 178, episode reward: 11.979, mean reward: 0.182 [0.037, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.781 [-0.744, 10.185], loss: 0.002705, mae: 0.053278, mean_q: -0.125086
 75097/100000: episode: 1210, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 10.746, mean reward: 0.316 [0.240, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.350, 10.100], loss: 0.002594, mae: 0.051500, mean_q: -0.160205
 75152/100000: episode: 1211, duration: 0.319s, episode steps: 55, steps per second: 173, episode reward: 9.751, mean reward: 0.177 [0.015, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.153, 10.191], loss: 0.002715, mae: 0.054079, mean_q: -0.107395
 75207/100000: episode: 1212, duration: 0.334s, episode steps: 55, steps per second: 164, episode reward: 13.501, mean reward: 0.245 [0.065, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.516, 10.100], loss: 0.002970, mae: 0.055713, mean_q: -0.137256
 75241/100000: episode: 1213, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 8.073, mean reward: 0.237 [0.074, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.372, 10.100], loss: 0.002729, mae: 0.053198, mean_q: -0.105150
 75296/100000: episode: 1214, duration: 0.298s, episode steps: 55, steps per second: 185, episode reward: 9.994, mean reward: 0.182 [0.048, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.537, 10.246], loss: 0.002719, mae: 0.052299, mean_q: -0.104279
 75303/100000: episode: 1215, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 2.699, mean reward: 0.386 [0.359, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.329, 10.100], loss: 0.002182, mae: 0.045819, mean_q: -0.161382
 75337/100000: episode: 1216, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 10.723, mean reward: 0.315 [0.149, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.211, 10.100], loss: 0.002735, mae: 0.052999, mean_q: -0.121871
 75406/100000: episode: 1217, duration: 0.380s, episode steps: 69, steps per second: 181, episode reward: 14.435, mean reward: 0.209 [0.045, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.745 [-0.962, 10.100], loss: 0.002647, mae: 0.053531, mean_q: -0.050715
 75409/100000: episode: 1218, duration: 0.027s, episode steps: 3, steps per second: 110, episode reward: 1.021, mean reward: 0.340 [0.289, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.591, 10.384], loss: 0.002912, mae: 0.058992, mean_q: -0.016898
 75451/100000: episode: 1219, duration: 0.234s, episode steps: 42, steps per second: 179, episode reward: 13.490, mean reward: 0.321 [0.187, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.222, 10.100], loss: 0.002623, mae: 0.051802, mean_q: -0.089186
 75520/100000: episode: 1220, duration: 0.391s, episode steps: 69, steps per second: 177, episode reward: 21.588, mean reward: 0.313 [0.099, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.747 [-0.688, 10.100], loss: 0.002783, mae: 0.054881, mean_q: -0.045987
 75523/100000: episode: 1221, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 1.066, mean reward: 0.355 [0.336, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.457], loss: 0.002107, mae: 0.044059, mean_q: -0.131107
 75556/100000: episode: 1222, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 7.631, mean reward: 0.231 [0.050, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.577, 10.155], loss: 0.002836, mae: 0.055674, mean_q: -0.034911
 75559/100000: episode: 1223, duration: 0.024s, episode steps: 3, steps per second: 123, episode reward: 0.955, mean reward: 0.318 [0.283, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.435], loss: 0.002724, mae: 0.050645, mean_q: -0.124672
 75566/100000: episode: 1224, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 2.116, mean reward: 0.302 [0.240, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.310, 10.100], loss: 0.002549, mae: 0.050972, mean_q: -0.104741
 75599/100000: episode: 1225, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 15.514, mean reward: 0.470 [0.336, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.328, 10.100], loss: 0.002709, mae: 0.053737, mean_q: -0.016977
 75629/100000: episode: 1226, duration: 0.169s, episode steps: 30, steps per second: 177, episode reward: 6.139, mean reward: 0.205 [0.025, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.427, 10.100], loss: 0.002592, mae: 0.051829, mean_q: 0.015158
 75663/100000: episode: 1227, duration: 0.185s, episode steps: 34, steps per second: 183, episode reward: 8.865, mean reward: 0.261 [0.101, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-1.500, 10.100], loss: 0.002783, mae: 0.054103, mean_q: -0.029052
 75696/100000: episode: 1228, duration: 0.198s, episode steps: 33, steps per second: 167, episode reward: 10.580, mean reward: 0.321 [0.172, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.754, 10.100], loss: 0.002653, mae: 0.051926, mean_q: -0.067546
 75762/100000: episode: 1229, duration: 0.376s, episode steps: 66, steps per second: 175, episode reward: 9.380, mean reward: 0.142 [0.024, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.776 [-0.315, 10.176], loss: 0.002804, mae: 0.055116, mean_q: -0.031316
 75765/100000: episode: 1230, duration: 0.026s, episode steps: 3, steps per second: 117, episode reward: 0.944, mean reward: 0.315 [0.304, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.367], loss: 0.002708, mae: 0.055008, mean_q: 0.059355
 75772/100000: episode: 1231, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 2.773, mean reward: 0.396 [0.276, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.382, 10.100], loss: 0.002698, mae: 0.052939, mean_q: -0.104065
 75773/100000: episode: 1232, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.454, mean reward: 0.454 [0.454, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.297], loss: 0.003847, mae: 0.061642, mean_q: 0.106103
 75807/100000: episode: 1233, duration: 0.203s, episode steps: 34, steps per second: 167, episode reward: 13.184, mean reward: 0.388 [0.282, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.805, 10.100], loss: 0.002680, mae: 0.053586, mean_q: 0.013668
 75876/100000: episode: 1234, duration: 0.372s, episode steps: 69, steps per second: 186, episode reward: 15.503, mean reward: 0.225 [0.017, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.757 [-0.159, 10.100], loss: 0.002779, mae: 0.054110, mean_q: -0.021705
 75931/100000: episode: 1235, duration: 0.312s, episode steps: 55, steps per second: 176, episode reward: 16.484, mean reward: 0.300 [0.065, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.549, 10.100], loss: 0.002763, mae: 0.052483, mean_q: -0.028210
 75997/100000: episode: 1236, duration: 0.400s, episode steps: 66, steps per second: 165, episode reward: 13.231, mean reward: 0.200 [0.027, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.780 [-0.578, 10.168], loss: 0.002752, mae: 0.053886, mean_q: -0.007751
 76066/100000: episode: 1237, duration: 0.383s, episode steps: 69, steps per second: 180, episode reward: 14.181, mean reward: 0.206 [0.043, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.736 [-1.219, 10.100], loss: 0.002773, mae: 0.054954, mean_q: 0.026349
 76135/100000: episode: 1238, duration: 0.410s, episode steps: 69, steps per second: 168, episode reward: 14.238, mean reward: 0.206 [0.016, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.750 [-1.223, 10.192], loss: 0.002832, mae: 0.055436, mean_q: -0.004442
 76201/100000: episode: 1239, duration: 0.348s, episode steps: 66, steps per second: 190, episode reward: 11.408, mean reward: 0.173 [0.031, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.778 [-0.692, 10.100], loss: 0.002821, mae: 0.055147, mean_q: 0.034855
 76270/100000: episode: 1240, duration: 0.380s, episode steps: 69, steps per second: 182, episode reward: 14.373, mean reward: 0.208 [0.026, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.748 [-0.197, 10.187], loss: 0.002893, mae: 0.055961, mean_q: 0.064701
 76271/100000: episode: 1241, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.369, mean reward: 0.369 [0.369, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.035, 10.335], loss: 0.002321, mae: 0.048501, mean_q: 0.082620
 76274/100000: episode: 1242, duration: 0.027s, episode steps: 3, steps per second: 113, episode reward: 0.918, mean reward: 0.306 [0.295, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.377], loss: 0.002256, mae: 0.051547, mean_q: 0.149984
 76304/100000: episode: 1243, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 8.622, mean reward: 0.287 [0.177, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-1.124, 10.100], loss: 0.002981, mae: 0.057096, mean_q: 0.077271
 76370/100000: episode: 1244, duration: 0.362s, episode steps: 66, steps per second: 182, episode reward: 10.861, mean reward: 0.165 [0.017, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.775 [-0.178, 10.100], loss: 0.002919, mae: 0.056304, mean_q: 0.069830
 76400/100000: episode: 1245, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 7.480, mean reward: 0.249 [0.110, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.886, 10.100], loss: 0.002691, mae: 0.055112, mean_q: 0.082698
 76466/100000: episode: 1246, duration: 0.334s, episode steps: 66, steps per second: 197, episode reward: 13.340, mean reward: 0.202 [0.049, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.770 [-0.488, 10.100], loss: 0.002848, mae: 0.055561, mean_q: 0.098874
 76508/100000: episode: 1247, duration: 0.235s, episode steps: 42, steps per second: 179, episode reward: 8.285, mean reward: 0.197 [0.051, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.874, 10.204], loss: 0.002875, mae: 0.057710, mean_q: 0.072214
 76563/100000: episode: 1248, duration: 0.313s, episode steps: 55, steps per second: 176, episode reward: 14.019, mean reward: 0.255 [0.130, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-0.264, 10.100], loss: 0.002605, mae: 0.053946, mean_q: 0.069647
 76566/100000: episode: 1249, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 1.038, mean reward: 0.346 [0.325, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.035, 10.477], loss: 0.002685, mae: 0.055175, mean_q: 0.122111
 76573/100000: episode: 1250, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 2.622, mean reward: 0.375 [0.311, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.356, 10.100], loss: 0.002428, mae: 0.052065, mean_q: 0.128012
 76615/100000: episode: 1251, duration: 0.225s, episode steps: 42, steps per second: 187, episode reward: 11.590, mean reward: 0.276 [0.121, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.639, 10.100], loss: 0.002613, mae: 0.054088, mean_q: 0.114829
 76622/100000: episode: 1252, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 2.003, mean reward: 0.286 [0.248, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.241, 10.100], loss: 0.002587, mae: 0.053404, mean_q: 0.163071
 76623/100000: episode: 1253, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.409, mean reward: 0.409 [0.409, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.035, 10.348], loss: 0.001405, mae: 0.040860, mean_q: 0.137932
 76624/100000: episode: 1254, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.502, mean reward: 0.502 [0.502, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.272], loss: 0.003967, mae: 0.068619, mean_q: 0.127477
 76627/100000: episode: 1255, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 1.256, mean reward: 0.419 [0.369, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.372, 10.443], loss: 0.002404, mae: 0.052755, mean_q: 0.164967
 76669/100000: episode: 1256, duration: 0.228s, episode steps: 42, steps per second: 184, episode reward: 7.952, mean reward: 0.189 [0.009, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.118, 10.106], loss: 0.002809, mae: 0.056633, mean_q: 0.114004
 76699/100000: episode: 1257, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 7.248, mean reward: 0.242 [0.140, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.814, 10.100], loss: 0.002860, mae: 0.056842, mean_q: 0.144840
 76754/100000: episode: 1258, duration: 0.296s, episode steps: 55, steps per second: 186, episode reward: 14.363, mean reward: 0.261 [0.029, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.172, 10.123], loss: 0.002614, mae: 0.054119, mean_q: 0.120421
 76823/100000: episode: 1259, duration: 0.363s, episode steps: 69, steps per second: 190, episode reward: 17.157, mean reward: 0.249 [0.039, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.742 [-0.894, 10.100], loss: 0.003026, mae: 0.059104, mean_q: 0.170341
 76856/100000: episode: 1260, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 12.438, mean reward: 0.377 [0.275, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.595, 10.100], loss: 0.002826, mae: 0.056563, mean_q: 0.180778
 76863/100000: episode: 1261, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 2.738, mean reward: 0.391 [0.329, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.293, 10.100], loss: 0.002959, mae: 0.057301, mean_q: 0.171131
 76896/100000: episode: 1262, duration: 0.188s, episode steps: 33, steps per second: 176, episode reward: 8.681, mean reward: 0.263 [0.041, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.566, 10.100], loss: 0.002553, mae: 0.054157, mean_q: 0.151628
 76903/100000: episode: 1263, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 2.405, mean reward: 0.344 [0.321, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.249, 10.100], loss: 0.003063, mae: 0.061605, mean_q: 0.171265
 76936/100000: episode: 1264, duration: 0.186s, episode steps: 33, steps per second: 178, episode reward: 6.498, mean reward: 0.197 [0.042, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.555, 10.100], loss: 0.002926, mae: 0.057111, mean_q: 0.176102
[Info] 200-TH LEVEL FOUND: 0.867562472820282, Considering 10/90 traces
 76970/100000: episode: 1265, duration: 4.211s, episode steps: 34, steps per second: 8, episode reward: 11.869, mean reward: 0.349 [0.170, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.208, 10.100], loss: 0.002750, mae: 0.055185, mean_q: 0.129833
 76992/100000: episode: 1266, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 10.082, mean reward: 0.458 [0.338, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.558, 10.100], loss: 0.003045, mae: 0.059556, mean_q: 0.174090
 77011/100000: episode: 1267, duration: 0.111s, episode steps: 19, steps per second: 172, episode reward: 8.737, mean reward: 0.460 [0.381, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.361, 10.100], loss: 0.003174, mae: 0.059820, mean_q: 0.236208
 77058/100000: episode: 1268, duration: 0.239s, episode steps: 47, steps per second: 196, episode reward: 20.561, mean reward: 0.437 [0.257, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-1.776, 10.100], loss: 0.003018, mae: 0.057536, mean_q: 0.167797
 77078/100000: episode: 1269, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 5.967, mean reward: 0.298 [0.137, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.932, 10.100], loss: 0.003138, mae: 0.058057, mean_q: 0.168725
 77099/100000: episode: 1270, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 7.005, mean reward: 0.334 [0.286, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.289, 10.100], loss: 0.002997, mae: 0.056619, mean_q: 0.210972
 77119/100000: episode: 1271, duration: 0.134s, episode steps: 20, steps per second: 150, episode reward: 6.846, mean reward: 0.342 [0.264, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.126, 10.100], loss: 0.002815, mae: 0.055849, mean_q: 0.236982
 77139/100000: episode: 1272, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 7.040, mean reward: 0.352 [0.294, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.753, 10.100], loss: 0.003188, mae: 0.061362, mean_q: 0.267713
 77160/100000: episode: 1273, duration: 0.103s, episode steps: 21, steps per second: 203, episode reward: 7.694, mean reward: 0.366 [0.264, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.249, 10.100], loss: 0.002948, mae: 0.057908, mean_q: 0.220084
 77187/100000: episode: 1274, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 11.517, mean reward: 0.427 [0.291, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.167, 10.100], loss: 0.002752, mae: 0.055301, mean_q: 0.194482
 77216/100000: episode: 1275, duration: 0.155s, episode steps: 29, steps per second: 188, episode reward: 9.861, mean reward: 0.340 [0.257, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.327, 10.100], loss: 0.003013, mae: 0.059383, mean_q: 0.242143
 77263/100000: episode: 1276, duration: 0.238s, episode steps: 47, steps per second: 198, episode reward: 19.453, mean reward: 0.414 [0.297, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-1.260, 10.100], loss: 0.002846, mae: 0.058084, mean_q: 0.210053
 77310/100000: episode: 1277, duration: 0.240s, episode steps: 47, steps per second: 196, episode reward: 11.621, mean reward: 0.247 [0.053, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.518, 10.100], loss: 0.005036, mae: 0.069207, mean_q: 0.252961
 77332/100000: episode: 1278, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 9.203, mean reward: 0.418 [0.327, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.373, 10.100], loss: 0.007904, mae: 0.074914, mean_q: 0.272947
 77379/100000: episode: 1279, duration: 0.243s, episode steps: 47, steps per second: 193, episode reward: 13.702, mean reward: 0.292 [0.216, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.309, 10.100], loss: 0.005335, mae: 0.072976, mean_q: 0.261502
 77404/100000: episode: 1280, duration: 0.150s, episode steps: 25, steps per second: 166, episode reward: 10.756, mean reward: 0.430 [0.346, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.349, 10.100], loss: 0.007259, mae: 0.072447, mean_q: 0.223119
 77430/100000: episode: 1281, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 9.239, mean reward: 0.355 [0.217, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.580, 10.100], loss: 0.003568, mae: 0.064374, mean_q: 0.278845
 77456/100000: episode: 1282, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 10.235, mean reward: 0.394 [0.294, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.317, 10.100], loss: 0.003101, mae: 0.061049, mean_q: 0.291633
 77482/100000: episode: 1283, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 11.992, mean reward: 0.461 [0.322, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.253, 10.100], loss: 0.003367, mae: 0.062097, mean_q: 0.299547
 77511/100000: episode: 1284, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 10.233, mean reward: 0.353 [0.248, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.208, 10.100], loss: 0.003095, mae: 0.060603, mean_q: 0.272213
 77537/100000: episode: 1285, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 12.707, mean reward: 0.489 [0.375, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.745, 10.100], loss: 0.003081, mae: 0.060568, mean_q: 0.291831
 77559/100000: episode: 1286, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 8.772, mean reward: 0.399 [0.304, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.751, 10.100], loss: 0.003470, mae: 0.063302, mean_q: 0.302048
 77588/100000: episode: 1287, duration: 0.146s, episode steps: 29, steps per second: 198, episode reward: 11.603, mean reward: 0.400 [0.285, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.608, 10.100], loss: 0.004001, mae: 0.065368, mean_q: 0.309835
 77610/100000: episode: 1288, duration: 0.125s, episode steps: 22, steps per second: 175, episode reward: 7.067, mean reward: 0.321 [0.198, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.134, 10.100], loss: 0.003739, mae: 0.062517, mean_q: 0.271780
 77657/100000: episode: 1289, duration: 0.267s, episode steps: 47, steps per second: 176, episode reward: 11.447, mean reward: 0.244 [0.041, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.459, 10.100], loss: 0.003970, mae: 0.065638, mean_q: 0.308799
 77676/100000: episode: 1290, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 7.482, mean reward: 0.394 [0.290, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.143, 10.100], loss: 0.003899, mae: 0.065538, mean_q: 0.344359
 77696/100000: episode: 1291, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 7.655, mean reward: 0.383 [0.325, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.194, 10.100], loss: 0.004689, mae: 0.067079, mean_q: 0.338842
 77716/100000: episode: 1292, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 5.948, mean reward: 0.297 [0.211, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-1.025, 10.100], loss: 0.006360, mae: 0.076523, mean_q: 0.322146
 77743/100000: episode: 1293, duration: 0.144s, episode steps: 27, steps per second: 188, episode reward: 8.422, mean reward: 0.312 [0.163, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.388, 10.100], loss: 0.004986, mae: 0.071495, mean_q: 0.302962
 77769/100000: episode: 1294, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 9.556, mean reward: 0.368 [0.149, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-1.748, 10.100], loss: 0.003259, mae: 0.061970, mean_q: 0.363465
 77788/100000: episode: 1295, duration: 0.112s, episode steps: 19, steps per second: 169, episode reward: 7.320, mean reward: 0.385 [0.305, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.363, 10.100], loss: 0.003053, mae: 0.059429, mean_q: 0.337827
 77809/100000: episode: 1296, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 5.355, mean reward: 0.255 [0.055, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.090, 10.100], loss: 0.003263, mae: 0.061485, mean_q: 0.356258
 77836/100000: episode: 1297, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 13.343, mean reward: 0.494 [0.382, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.480, 10.100], loss: 0.003049, mae: 0.060856, mean_q: 0.374106
 77857/100000: episode: 1298, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 8.546, mean reward: 0.407 [0.331, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.275, 10.100], loss: 0.003099, mae: 0.059871, mean_q: 0.381262
 77876/100000: episode: 1299, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 5.655, mean reward: 0.298 [0.214, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.100], loss: 0.003107, mae: 0.061231, mean_q: 0.415895
 77923/100000: episode: 1300, duration: 0.245s, episode steps: 47, steps per second: 192, episode reward: 12.207, mean reward: 0.260 [0.050, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-1.053, 10.188], loss: 0.003234, mae: 0.061212, mean_q: 0.383629
 77944/100000: episode: 1301, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 6.244, mean reward: 0.297 [0.111, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.089, 10.100], loss: 0.003549, mae: 0.064787, mean_q: 0.388145
 77991/100000: episode: 1302, duration: 0.257s, episode steps: 47, steps per second: 183, episode reward: 16.033, mean reward: 0.341 [0.194, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-1.224, 10.100], loss: 0.003411, mae: 0.062786, mean_q: 0.374063
 78016/100000: episode: 1303, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 9.717, mean reward: 0.389 [0.169, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-1.284, 10.100], loss: 0.003378, mae: 0.063496, mean_q: 0.392000
 78038/100000: episode: 1304, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 8.597, mean reward: 0.391 [0.276, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.505, 10.100], loss: 0.003391, mae: 0.064044, mean_q: 0.419698
 78059/100000: episode: 1305, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 10.115, mean reward: 0.482 [0.307, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.704, 10.100], loss: 0.003238, mae: 0.061762, mean_q: 0.400470
 78078/100000: episode: 1306, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 6.644, mean reward: 0.350 [0.214, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.245, 10.100], loss: 0.003235, mae: 0.061275, mean_q: 0.401040
 78103/100000: episode: 1307, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 9.735, mean reward: 0.389 [0.290, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.311, 10.100], loss: 0.003388, mae: 0.063621, mean_q: 0.412264
 78125/100000: episode: 1308, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 7.195, mean reward: 0.327 [0.233, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.301, 10.100], loss: 0.003248, mae: 0.063276, mean_q: 0.439543
 78150/100000: episode: 1309, duration: 0.125s, episode steps: 25, steps per second: 201, episode reward: 8.111, mean reward: 0.324 [0.130, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.138, 10.100], loss: 0.003470, mae: 0.062372, mean_q: 0.423118
 78171/100000: episode: 1310, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 6.327, mean reward: 0.301 [0.133, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.250, 10.100], loss: 0.003422, mae: 0.064509, mean_q: 0.431686
 78197/100000: episode: 1311, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 7.873, mean reward: 0.303 [0.186, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.231, 10.100], loss: 0.003374, mae: 0.063169, mean_q: 0.418088
 78223/100000: episode: 1312, duration: 0.141s, episode steps: 26, steps per second: 185, episode reward: 7.274, mean reward: 0.280 [0.062, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.419, 10.100], loss: 0.002970, mae: 0.060194, mean_q: 0.411496
 78243/100000: episode: 1313, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 7.018, mean reward: 0.351 [0.267, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.370, 10.100], loss: 0.003283, mae: 0.062249, mean_q: 0.469205
 78263/100000: episode: 1314, duration: 0.100s, episode steps: 20, steps per second: 199, episode reward: 7.501, mean reward: 0.375 [0.322, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.139, 10.100], loss: 0.003078, mae: 0.060869, mean_q: 0.464175
 78285/100000: episode: 1315, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 9.560, mean reward: 0.435 [0.301, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.429, 10.100], loss: 0.003106, mae: 0.060076, mean_q: 0.426204
 78332/100000: episode: 1316, duration: 0.240s, episode steps: 47, steps per second: 196, episode reward: 17.041, mean reward: 0.363 [0.211, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.620, 10.100], loss: 0.003053, mae: 0.060479, mean_q: 0.453400
 78352/100000: episode: 1317, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 8.055, mean reward: 0.403 [0.324, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.314, 10.100], loss: 0.002700, mae: 0.056249, mean_q: 0.493725
 78374/100000: episode: 1318, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 8.417, mean reward: 0.383 [0.322, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.363, 10.100], loss: 0.002995, mae: 0.061205, mean_q: 0.445310
 78400/100000: episode: 1319, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 10.767, mean reward: 0.414 [0.296, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.223, 10.100], loss: 0.002754, mae: 0.058679, mean_q: 0.475903
 78427/100000: episode: 1320, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 9.907, mean reward: 0.367 [0.218, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.580, 10.100], loss: 0.002954, mae: 0.059013, mean_q: 0.500275
 78448/100000: episode: 1321, duration: 0.126s, episode steps: 21, steps per second: 166, episode reward: 7.753, mean reward: 0.369 [0.202, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.413, 10.100], loss: 0.003313, mae: 0.061635, mean_q: 0.497475
 78469/100000: episode: 1322, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 5.680, mean reward: 0.270 [0.145, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.867, 10.100], loss: 0.003968, mae: 0.068489, mean_q: 0.493594
 78498/100000: episode: 1323, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 12.520, mean reward: 0.432 [0.334, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.368, 10.100], loss: 0.003231, mae: 0.061225, mean_q: 0.492260
 78525/100000: episode: 1324, duration: 0.175s, episode steps: 27, steps per second: 154, episode reward: 12.415, mean reward: 0.460 [0.367, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.362, 10.100], loss: 0.003121, mae: 0.059965, mean_q: 0.491385
 78572/100000: episode: 1325, duration: 0.262s, episode steps: 47, steps per second: 179, episode reward: 14.534, mean reward: 0.309 [0.201, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.199, 10.100], loss: 0.003602, mae: 0.065619, mean_q: 0.499242
 78619/100000: episode: 1326, duration: 0.246s, episode steps: 47, steps per second: 191, episode reward: 13.119, mean reward: 0.279 [0.062, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.318, 10.100], loss: 0.003502, mae: 0.064589, mean_q: 0.514309
 78639/100000: episode: 1327, duration: 0.127s, episode steps: 20, steps per second: 157, episode reward: 6.925, mean reward: 0.346 [0.240, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.641, 10.100], loss: 0.003060, mae: 0.061092, mean_q: 0.513849
 78665/100000: episode: 1328, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 11.861, mean reward: 0.456 [0.219, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.805, 10.100], loss: 0.003243, mae: 0.063164, mean_q: 0.533864
 78687/100000: episode: 1329, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 6.797, mean reward: 0.309 [0.157, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.279, 10.100], loss: 0.003333, mae: 0.065016, mean_q: 0.522292
 78706/100000: episode: 1330, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 7.433, mean reward: 0.391 [0.308, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.397, 10.100], loss: 0.003307, mae: 0.061463, mean_q: 0.509276
 78727/100000: episode: 1331, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 7.534, mean reward: 0.359 [0.212, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.209, 10.100], loss: 0.003400, mae: 0.065243, mean_q: 0.534864
 78747/100000: episode: 1332, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 6.683, mean reward: 0.334 [0.180, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.242, 10.100], loss: 0.003060, mae: 0.061795, mean_q: 0.545126
 78768/100000: episode: 1333, duration: 0.120s, episode steps: 21, steps per second: 174, episode reward: 11.067, mean reward: 0.527 [0.394, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.493, 10.100], loss: 0.002953, mae: 0.060525, mean_q: 0.552391
 78815/100000: episode: 1334, duration: 0.255s, episode steps: 47, steps per second: 184, episode reward: 14.098, mean reward: 0.300 [0.106, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.563, 10.100], loss: 0.003209, mae: 0.062236, mean_q: 0.558193
 78835/100000: episode: 1335, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 7.819, mean reward: 0.391 [0.281, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.455, 10.100], loss: 0.003010, mae: 0.061348, mean_q: 0.543095
 78856/100000: episode: 1336, duration: 0.144s, episode steps: 21, steps per second: 145, episode reward: 9.136, mean reward: 0.435 [0.333, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.810, 10.100], loss: 0.003497, mae: 0.064305, mean_q: 0.562608
 78885/100000: episode: 1337, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 9.874, mean reward: 0.340 [0.230, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.133, 10.100], loss: 0.003180, mae: 0.062699, mean_q: 0.565081
 78906/100000: episode: 1338, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 8.229, mean reward: 0.392 [0.310, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.195, 10.100], loss: 0.003285, mae: 0.063195, mean_q: 0.549210
 78927/100000: episode: 1339, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 10.316, mean reward: 0.491 [0.397, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.416, 10.100], loss: 0.003002, mae: 0.060938, mean_q: 0.569357
 78974/100000: episode: 1340, duration: 0.243s, episode steps: 47, steps per second: 193, episode reward: 16.589, mean reward: 0.353 [0.225, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.361, 10.100], loss: 0.003160, mae: 0.062197, mean_q: 0.572727
 78993/100000: episode: 1341, duration: 0.121s, episode steps: 19, steps per second: 158, episode reward: 7.326, mean reward: 0.386 [0.285, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.466, 10.100], loss: 0.003240, mae: 0.063575, mean_q: 0.563777
 79040/100000: episode: 1342, duration: 0.267s, episode steps: 47, steps per second: 176, episode reward: 23.136, mean reward: 0.492 [0.338, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.431, 10.100], loss: 0.003334, mae: 0.063072, mean_q: 0.562700
 79069/100000: episode: 1343, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 10.959, mean reward: 0.378 [0.151, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.521, 10.100], loss: 0.003437, mae: 0.063956, mean_q: 0.564786
 79089/100000: episode: 1344, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 7.484, mean reward: 0.374 [0.159, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.108, 10.100], loss: 0.004089, mae: 0.072992, mean_q: 0.575333
 79108/100000: episode: 1345, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 8.158, mean reward: 0.429 [0.330, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.498, 10.100], loss: 0.003538, mae: 0.065769, mean_q: 0.552891
 79134/100000: episode: 1346, duration: 0.132s, episode steps: 26, steps per second: 196, episode reward: 11.440, mean reward: 0.440 [0.311, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.225, 10.100], loss: 0.003346, mae: 0.064464, mean_q: 0.580484
 79159/100000: episode: 1347, duration: 0.154s, episode steps: 25, steps per second: 162, episode reward: 9.115, mean reward: 0.365 [0.246, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.866, 10.100], loss: 0.003429, mae: 0.065371, mean_q: 0.567491
 79179/100000: episode: 1348, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 6.789, mean reward: 0.339 [0.219, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.339, 10.100], loss: 0.003167, mae: 0.061486, mean_q: 0.561580
 79205/100000: episode: 1349, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 9.597, mean reward: 0.369 [0.280, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.962, 10.100], loss: 0.003306, mae: 0.064020, mean_q: 0.574069
 79231/100000: episode: 1350, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 9.431, mean reward: 0.363 [0.099, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.110, 10.100], loss: 0.003434, mae: 0.064125, mean_q: 0.574307
 79258/100000: episode: 1351, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 9.503, mean reward: 0.352 [0.194, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.306, 10.100], loss: 0.003374, mae: 0.064228, mean_q: 0.578996
 79284/100000: episode: 1352, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 11.653, mean reward: 0.448 [0.357, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.312, 10.100], loss: 0.003228, mae: 0.061132, mean_q: 0.597987
 79331/100000: episode: 1353, duration: 0.269s, episode steps: 47, steps per second: 175, episode reward: 15.365, mean reward: 0.327 [0.027, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.180, 10.108], loss: 0.003139, mae: 0.060689, mean_q: 0.587335
 79352/100000: episode: 1354, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 8.788, mean reward: 0.418 [0.366, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.228, 10.100], loss: 0.003030, mae: 0.063666, mean_q: 0.605890
[Info] 300-TH LEVEL FOUND: 1.0064780712127686, Considering 10/90 traces
 79373/100000: episode: 1355, duration: 4.150s, episode steps: 21, steps per second: 5, episode reward: 7.091, mean reward: 0.338 [0.180, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.299, 10.100], loss: 0.002941, mae: 0.058404, mean_q: 0.580174
 79415/100000: episode: 1356, duration: 0.228s, episode steps: 42, steps per second: 184, episode reward: 19.640, mean reward: 0.468 [0.362, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-1.701, 10.100], loss: 0.003080, mae: 0.060425, mean_q: 0.586031
 79429/100000: episode: 1357, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 7.972, mean reward: 0.569 [0.466, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.206, 10.100], loss: 0.003314, mae: 0.062834, mean_q: 0.591084
 79447/100000: episode: 1358, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 7.671, mean reward: 0.426 [0.307, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.336, 10.100], loss: 0.003163, mae: 0.062441, mean_q: 0.596529
 79490/100000: episode: 1359, duration: 0.238s, episode steps: 43, steps per second: 181, episode reward: 16.813, mean reward: 0.391 [0.315, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-1.289, 10.100], loss: 0.003249, mae: 0.062908, mean_q: 0.597055
 79504/100000: episode: 1360, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 6.608, mean reward: 0.472 [0.398, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.323, 10.100], loss: 0.003082, mae: 0.062452, mean_q: 0.591309
 79518/100000: episode: 1361, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 7.875, mean reward: 0.562 [0.459, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.412, 10.100], loss: 0.003695, mae: 0.069367, mean_q: 0.604504
 79561/100000: episode: 1362, duration: 0.222s, episode steps: 43, steps per second: 193, episode reward: 15.092, mean reward: 0.351 [0.214, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.701, 10.100], loss: 0.003276, mae: 0.063172, mean_q: 0.607351
 79584/100000: episode: 1363, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 9.151, mean reward: 0.398 [0.259, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.266, 10.100], loss: 0.003573, mae: 0.065848, mean_q: 0.610115
 79598/100000: episode: 1364, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 7.387, mean reward: 0.528 [0.472, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.740, 10.100], loss: 0.003366, mae: 0.062637, mean_q: 0.601743
 79640/100000: episode: 1365, duration: 0.236s, episode steps: 42, steps per second: 178, episode reward: 18.921, mean reward: 0.451 [0.382, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.706, 10.100], loss: 0.003422, mae: 0.064064, mean_q: 0.604734
 79663/100000: episode: 1366, duration: 0.142s, episode steps: 23, steps per second: 162, episode reward: 9.313, mean reward: 0.405 [0.200, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.210, 10.100], loss: 0.002931, mae: 0.060978, mean_q: 0.605037
 79677/100000: episode: 1367, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 5.421, mean reward: 0.387 [0.297, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.318, 10.100], loss: 0.003549, mae: 0.065133, mean_q: 0.610846
 79719/100000: episode: 1368, duration: 0.218s, episode steps: 42, steps per second: 192, episode reward: 15.015, mean reward: 0.357 [0.157, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.814, 10.100], loss: 0.003640, mae: 0.064901, mean_q: 0.606246
 79737/100000: episode: 1369, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 7.325, mean reward: 0.407 [0.343, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.285, 10.100], loss: 0.003429, mae: 0.065331, mean_q: 0.601162
 79755/100000: episode: 1370, duration: 0.118s, episode steps: 18, steps per second: 152, episode reward: 6.351, mean reward: 0.353 [0.300, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.221, 10.100], loss: 0.003276, mae: 0.061724, mean_q: 0.619099
 79774/100000: episode: 1371, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 9.806, mean reward: 0.516 [0.399, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.401, 10.100], loss: 0.003274, mae: 0.061895, mean_q: 0.605943
 79816/100000: episode: 1372, duration: 0.238s, episode steps: 42, steps per second: 176, episode reward: 13.301, mean reward: 0.317 [0.192, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.188, 10.100], loss: 0.003085, mae: 0.059860, mean_q: 0.615410
 79835/100000: episode: 1373, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 7.253, mean reward: 0.382 [0.293, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.337, 10.100], loss: 0.003173, mae: 0.062454, mean_q: 0.612368
 79858/100000: episode: 1374, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 11.064, mean reward: 0.481 [0.417, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.368, 10.100], loss: 0.003201, mae: 0.062944, mean_q: 0.613626
 79876/100000: episode: 1375, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 8.152, mean reward: 0.453 [0.309, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.423, 10.100], loss: 0.002769, mae: 0.058549, mean_q: 0.605802
 79899/100000: episode: 1376, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 10.687, mean reward: 0.465 [0.356, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.091, 10.100], loss: 0.003417, mae: 0.062061, mean_q: 0.616673
 79917/100000: episode: 1377, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 7.794, mean reward: 0.433 [0.372, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.475, 10.100], loss: 0.003392, mae: 0.063355, mean_q: 0.620570
 79960/100000: episode: 1378, duration: 0.233s, episode steps: 43, steps per second: 185, episode reward: 15.373, mean reward: 0.358 [0.174, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.516, 10.100], loss: 0.003276, mae: 0.062775, mean_q: 0.627601
 79974/100000: episode: 1379, duration: 0.085s, episode steps: 14, steps per second: 166, episode reward: 7.890, mean reward: 0.564 [0.451, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.411, 10.100], loss: 0.002979, mae: 0.059927, mean_q: 0.626527
 79997/100000: episode: 1380, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 10.174, mean reward: 0.442 [0.295, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.391, 10.100], loss: 0.003475, mae: 0.064988, mean_q: 0.627260
 80020/100000: episode: 1381, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 8.830, mean reward: 0.384 [0.225, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.381, 10.100], loss: 0.003371, mae: 0.063213, mean_q: 0.623147
 80039/100000: episode: 1382, duration: 0.122s, episode steps: 19, steps per second: 156, episode reward: 8.365, mean reward: 0.440 [0.224, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.190, 10.100], loss: 0.003370, mae: 0.063289, mean_q: 0.630670
 80053/100000: episode: 1383, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 7.418, mean reward: 0.530 [0.433, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.944, 10.100], loss: 0.003455, mae: 0.065837, mean_q: 0.631320
 80095/100000: episode: 1384, duration: 0.233s, episode steps: 42, steps per second: 181, episode reward: 11.115, mean reward: 0.265 [0.083, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.380, 10.100], loss: 0.003394, mae: 0.063304, mean_q: 0.631067
 80113/100000: episode: 1385, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 9.192, mean reward: 0.511 [0.277, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.173, 10.100], loss: 0.003136, mae: 0.061260, mean_q: 0.656187
 80127/100000: episode: 1386, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 6.477, mean reward: 0.463 [0.375, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.556, 10.100], loss: 0.003221, mae: 0.064481, mean_q: 0.631895
 80145/100000: episode: 1387, duration: 0.113s, episode steps: 18, steps per second: 159, episode reward: 6.617, mean reward: 0.368 [0.287, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.375, 10.100], loss: 0.003499, mae: 0.063747, mean_q: 0.630208
 80163/100000: episode: 1388, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 9.711, mean reward: 0.539 [0.440, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-1.268, 10.100], loss: 0.003207, mae: 0.061726, mean_q: 0.663572
 80181/100000: episode: 1389, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 8.643, mean reward: 0.480 [0.315, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.755, 10.100], loss: 0.003576, mae: 0.064772, mean_q: 0.636582
 80204/100000: episode: 1390, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 12.868, mean reward: 0.559 [0.505, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.438, 10.100], loss: 0.003298, mae: 0.062649, mean_q: 0.632060
 80247/100000: episode: 1391, duration: 0.224s, episode steps: 43, steps per second: 192, episode reward: 17.256, mean reward: 0.401 [0.124, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.169, 10.100], loss: 0.003038, mae: 0.058980, mean_q: 0.647495
 80265/100000: episode: 1392, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 8.793, mean reward: 0.488 [0.406, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.339, 10.100], loss: 0.003413, mae: 0.062964, mean_q: 0.661543
 80307/100000: episode: 1393, duration: 0.238s, episode steps: 42, steps per second: 176, episode reward: 16.927, mean reward: 0.403 [0.219, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.360, 10.100], loss: 0.003552, mae: 0.065929, mean_q: 0.642936
 80325/100000: episode: 1394, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 9.210, mean reward: 0.512 [0.410, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.308, 10.100], loss: 0.003372, mae: 0.062037, mean_q: 0.651487
 80339/100000: episode: 1395, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 6.735, mean reward: 0.481 [0.414, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.372, 10.100], loss: 0.003189, mae: 0.061161, mean_q: 0.655084
 80362/100000: episode: 1396, duration: 0.135s, episode steps: 23, steps per second: 170, episode reward: 8.508, mean reward: 0.370 [0.083, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.201, 10.100], loss: 0.003262, mae: 0.062293, mean_q: 0.649738
 80385/100000: episode: 1397, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 8.830, mean reward: 0.384 [0.283, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.373, 10.100], loss: 0.003445, mae: 0.063257, mean_q: 0.662411
 80399/100000: episode: 1398, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 7.582, mean reward: 0.542 [0.476, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.572, 10.100], loss: 0.004500, mae: 0.071507, mean_q: 0.642983
 80422/100000: episode: 1399, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 10.065, mean reward: 0.438 [0.362, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.537, 10.100], loss: 0.003824, mae: 0.067536, mean_q: 0.651980
 80441/100000: episode: 1400, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 6.898, mean reward: 0.363 [0.251, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.310, 10.100], loss: 0.003253, mae: 0.062633, mean_q: 0.654585
 80459/100000: episode: 1401, duration: 0.115s, episode steps: 18, steps per second: 156, episode reward: 7.462, mean reward: 0.415 [0.285, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.248, 10.100], loss: 0.003048, mae: 0.061002, mean_q: 0.655680
 80478/100000: episode: 1402, duration: 0.116s, episode steps: 19, steps per second: 163, episode reward: 7.877, mean reward: 0.415 [0.364, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.274, 10.100], loss: 0.003917, mae: 0.069681, mean_q: 0.666355
 80521/100000: episode: 1403, duration: 0.221s, episode steps: 43, steps per second: 194, episode reward: 15.972, mean reward: 0.371 [0.136, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.898, 10.100], loss: 0.003226, mae: 0.061273, mean_q: 0.656518
 80539/100000: episode: 1404, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 6.841, mean reward: 0.380 [0.227, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.708, 10.100], loss: 0.003341, mae: 0.062293, mean_q: 0.664842
 80582/100000: episode: 1405, duration: 0.243s, episode steps: 43, steps per second: 177, episode reward: 13.383, mean reward: 0.311 [0.065, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.415, 10.100], loss: 0.003190, mae: 0.061681, mean_q: 0.662621
 80600/100000: episode: 1406, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 9.085, mean reward: 0.505 [0.339, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.831, 10.100], loss: 0.003192, mae: 0.062517, mean_q: 0.670007
 80619/100000: episode: 1407, duration: 0.119s, episode steps: 19, steps per second: 160, episode reward: 9.582, mean reward: 0.504 [0.459, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.775, 10.100], loss: 0.003024, mae: 0.060739, mean_q: 0.667345
 80637/100000: episode: 1408, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 7.191, mean reward: 0.399 [0.240, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.273, 10.100], loss: 0.003226, mae: 0.061812, mean_q: 0.671604
 80655/100000: episode: 1409, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 8.176, mean reward: 0.454 [0.402, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.671, 10.100], loss: 0.003245, mae: 0.060623, mean_q: 0.672473
 80678/100000: episode: 1410, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 10.202, mean reward: 0.444 [0.274, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.696, 10.100], loss: 0.003089, mae: 0.061017, mean_q: 0.676294
 80701/100000: episode: 1411, duration: 0.138s, episode steps: 23, steps per second: 167, episode reward: 11.081, mean reward: 0.482 [0.315, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.664, 10.100], loss: 0.003017, mae: 0.059655, mean_q: 0.676295
 80720/100000: episode: 1412, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 7.906, mean reward: 0.416 [0.312, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.453, 10.100], loss: 0.003411, mae: 0.064014, mean_q: 0.674846
 80734/100000: episode: 1413, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 7.742, mean reward: 0.553 [0.400, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.465, 10.100], loss: 0.003518, mae: 0.063733, mean_q: 0.670425
 80753/100000: episode: 1414, duration: 0.115s, episode steps: 19, steps per second: 166, episode reward: 8.175, mean reward: 0.430 [0.346, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.319, 10.100], loss: 0.003178, mae: 0.062231, mean_q: 0.680032
 80771/100000: episode: 1415, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 8.670, mean reward: 0.482 [0.378, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.416, 10.100], loss: 0.003018, mae: 0.060043, mean_q: 0.682125
 80790/100000: episode: 1416, duration: 0.116s, episode steps: 19, steps per second: 163, episode reward: 9.437, mean reward: 0.497 [0.435, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.592, 10.100], loss: 0.003781, mae: 0.067406, mean_q: 0.666084
 80832/100000: episode: 1417, duration: 0.236s, episode steps: 42, steps per second: 178, episode reward: 12.997, mean reward: 0.309 [0.027, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.890, 10.128], loss: 0.003054, mae: 0.060633, mean_q: 0.684489
 80874/100000: episode: 1418, duration: 0.232s, episode steps: 42, steps per second: 181, episode reward: 11.340, mean reward: 0.270 [0.062, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-1.661, 10.100], loss: 0.003735, mae: 0.067719, mean_q: 0.686321
 80916/100000: episode: 1419, duration: 0.251s, episode steps: 42, steps per second: 168, episode reward: 15.822, mean reward: 0.377 [0.251, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.476, 10.100], loss: 0.002971, mae: 0.060456, mean_q: 0.679229
 80939/100000: episode: 1420, duration: 0.134s, episode steps: 23, steps per second: 172, episode reward: 11.835, mean reward: 0.515 [0.311, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.205, 10.100], loss: 0.003265, mae: 0.062865, mean_q: 0.694032
 80981/100000: episode: 1421, duration: 0.245s, episode steps: 42, steps per second: 172, episode reward: 14.362, mean reward: 0.342 [0.114, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.707, 10.100], loss: 0.003122, mae: 0.060238, mean_q: 0.689232
 81004/100000: episode: 1422, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 10.552, mean reward: 0.459 [0.367, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.209, 10.100], loss: 0.002975, mae: 0.058575, mean_q: 0.685457
 81046/100000: episode: 1423, duration: 0.239s, episode steps: 42, steps per second: 176, episode reward: 19.566, mean reward: 0.466 [0.345, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.492, 10.100], loss: 0.003201, mae: 0.060678, mean_q: 0.689866
 81089/100000: episode: 1424, duration: 0.243s, episode steps: 43, steps per second: 177, episode reward: 16.681, mean reward: 0.388 [0.134, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-1.209, 10.100], loss: 0.003421, mae: 0.064076, mean_q: 0.702186
 81112/100000: episode: 1425, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 9.988, mean reward: 0.434 [0.262, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.251, 10.100], loss: 0.003839, mae: 0.068640, mean_q: 0.707212
 81135/100000: episode: 1426, duration: 0.135s, episode steps: 23, steps per second: 170, episode reward: 9.035, mean reward: 0.393 [0.234, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.708, 10.100], loss: 0.003397, mae: 0.063060, mean_q: 0.714811
 81149/100000: episode: 1427, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 6.799, mean reward: 0.486 [0.444, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.445, 10.100], loss: 0.003221, mae: 0.062094, mean_q: 0.716368
 81168/100000: episode: 1428, duration: 0.113s, episode steps: 19, steps per second: 169, episode reward: 7.938, mean reward: 0.418 [0.308, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.837, 10.100], loss: 0.003523, mae: 0.065041, mean_q: 0.699454
 81186/100000: episode: 1429, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 8.867, mean reward: 0.493 [0.350, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.356, 10.100], loss: 0.003125, mae: 0.059935, mean_q: 0.712925
 81204/100000: episode: 1430, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 6.869, mean reward: 0.382 [0.284, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.356, 10.100], loss: 0.002958, mae: 0.060149, mean_q: 0.737730
 81222/100000: episode: 1431, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 8.567, mean reward: 0.476 [0.388, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.419, 10.100], loss: 0.003036, mae: 0.059699, mean_q: 0.707302
[Info] FALSIFICATION!
 81223/100000: episode: 1432, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [0.000, 9.695], loss: 0.002638, mae: 0.058567, mean_q: 0.713732
 81323/100000: episode: 1433, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.631, mean reward: -0.196 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.889, 10.141], loss: 0.016185, mae: 0.068928, mean_q: 0.701997
 81423/100000: episode: 1434, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.477, mean reward: -0.175 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.508, 10.098], loss: 0.016866, mae: 0.073611, mean_q: 0.686927
 81523/100000: episode: 1435, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -12.578, mean reward: -0.126 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.328, 10.098], loss: 0.016926, mae: 0.072916, mean_q: 0.680706
 81623/100000: episode: 1436, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.997, mean reward: -0.160 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.082, 10.245], loss: 0.003172, mae: 0.060754, mean_q: 0.663237
 81723/100000: episode: 1437, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.219, mean reward: -0.182 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.929, 10.098], loss: 0.018166, mae: 0.074892, mean_q: 0.639002
 81823/100000: episode: 1438, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.842, mean reward: -0.178 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.634, 10.098], loss: 0.017193, mae: 0.074896, mean_q: 0.625500
 81923/100000: episode: 1439, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.288, mean reward: -0.163 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.889, 10.098], loss: 0.002951, mae: 0.059181, mean_q: 0.601525
 82023/100000: episode: 1440, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.751, mean reward: -0.188 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.226, 10.138], loss: 0.017773, mae: 0.073717, mean_q: 0.591032
 82123/100000: episode: 1441, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.299, mean reward: -0.143 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.861, 10.098], loss: 0.003742, mae: 0.063315, mean_q: 0.548198
 82223/100000: episode: 1442, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -17.111, mean reward: -0.171 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.773, 10.098], loss: 0.005428, mae: 0.067634, mean_q: 0.540333
 82323/100000: episode: 1443, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.619, mean reward: -0.186 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.128, 10.098], loss: 0.016603, mae: 0.071798, mean_q: 0.524107
 82423/100000: episode: 1444, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -13.887, mean reward: -0.139 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.922, 10.500], loss: 0.003596, mae: 0.061658, mean_q: 0.475762
 82523/100000: episode: 1445, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -13.310, mean reward: -0.133 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.505, 10.165], loss: 0.003160, mae: 0.060550, mean_q: 0.471139
 82623/100000: episode: 1446, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.343, mean reward: -0.163 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.541, 10.098], loss: 0.029247, mae: 0.077799, mean_q: 0.452523
 82723/100000: episode: 1447, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -14.453, mean reward: -0.145 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.935, 10.098], loss: 0.028702, mae: 0.072413, mean_q: 0.428053
 82823/100000: episode: 1448, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -20.807, mean reward: -0.208 [-1.000, 0.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.755, 10.098], loss: 0.002937, mae: 0.057615, mean_q: 0.404563
 82923/100000: episode: 1449, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.303, mean reward: -0.193 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.925, 10.116], loss: 0.015812, mae: 0.064677, mean_q: 0.409432
 83023/100000: episode: 1450, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -16.933, mean reward: -0.169 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.997, 10.165], loss: 0.002846, mae: 0.056198, mean_q: 0.367370
 83123/100000: episode: 1451, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.094, mean reward: -0.161 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.083, 10.098], loss: 0.002748, mae: 0.055876, mean_q: 0.351960
 83223/100000: episode: 1452, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -12.486, mean reward: -0.125 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.048, 10.201], loss: 0.016034, mae: 0.065871, mean_q: 0.336894
 83323/100000: episode: 1453, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.562, mean reward: -0.196 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.668, 10.108], loss: 0.015701, mae: 0.064025, mean_q: 0.314882
 83423/100000: episode: 1454, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.977, mean reward: -0.160 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.363, 10.165], loss: 0.006974, mae: 0.078329, mean_q: 0.301927
 83523/100000: episode: 1455, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.049, mean reward: -0.170 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.321, 10.203], loss: 0.003691, mae: 0.060769, mean_q: 0.240914
 83623/100000: episode: 1456, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.424, mean reward: -0.164 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.789, 10.159], loss: 0.015909, mae: 0.066491, mean_q: 0.227660
 83723/100000: episode: 1457, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -20.325, mean reward: -0.203 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.263, 10.098], loss: 0.002629, mae: 0.053716, mean_q: 0.246983
 83823/100000: episode: 1458, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -14.652, mean reward: -0.147 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.420, 10.098], loss: 0.028599, mae: 0.069971, mean_q: 0.226880
 83923/100000: episode: 1459, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.898, mean reward: -0.189 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.152, 10.098], loss: 0.002737, mae: 0.055289, mean_q: 0.193266
 84023/100000: episode: 1460, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.039, mean reward: -0.190 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.276, 10.103], loss: 0.003323, mae: 0.059514, mean_q: 0.152268
 84123/100000: episode: 1461, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.159, mean reward: -0.172 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.082, 10.319], loss: 0.028259, mae: 0.067477, mean_q: 0.144765
 84223/100000: episode: 1462, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.652, mean reward: -0.187 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.723, 10.188], loss: 0.002625, mae: 0.052324, mean_q: 0.112322
 84323/100000: episode: 1463, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -20.484, mean reward: -0.205 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.771, 10.114], loss: 0.003172, mae: 0.057235, mean_q: 0.085322
 84423/100000: episode: 1464, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.314, mean reward: -0.183 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.618, 10.098], loss: 0.028046, mae: 0.065618, mean_q: 0.060955
 84523/100000: episode: 1465, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.850, mean reward: -0.168 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.897, 10.150], loss: 0.015661, mae: 0.062198, mean_q: 0.048711
 84623/100000: episode: 1466, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -11.633, mean reward: -0.116 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.553, 10.098], loss: 0.002624, mae: 0.052253, mean_q: 0.010610
 84723/100000: episode: 1467, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.942, mean reward: -0.199 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.412, 10.098], loss: 0.002633, mae: 0.052682, mean_q: 0.005979
 84823/100000: episode: 1468, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.985, mean reward: -0.170 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.163, 10.098], loss: 0.002659, mae: 0.051826, mean_q: -0.023085
 84923/100000: episode: 1469, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -18.553, mean reward: -0.186 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.331, 10.135], loss: 0.002619, mae: 0.051717, mean_q: -0.018168
 85023/100000: episode: 1470, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.346, mean reward: -0.163 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.066, 10.228], loss: 0.015495, mae: 0.059462, mean_q: -0.040619
 85123/100000: episode: 1471, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.518, mean reward: -0.185 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.493, 10.172], loss: 0.002432, mae: 0.049898, mean_q: -0.089432
 85223/100000: episode: 1472, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.401, mean reward: -0.194 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.921, 10.098], loss: 0.016155, mae: 0.064085, mean_q: -0.115860
 85323/100000: episode: 1473, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -16.741, mean reward: -0.167 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.713, 10.264], loss: 0.002490, mae: 0.050111, mean_q: -0.129154
 85423/100000: episode: 1474, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -7.651, mean reward: -0.077 [-1.000, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.110, 10.615], loss: 0.002495, mae: 0.050082, mean_q: -0.113181
 85523/100000: episode: 1475, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -18.795, mean reward: -0.188 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.240, 10.098], loss: 0.002459, mae: 0.049425, mean_q: -0.166869
 85623/100000: episode: 1476, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -13.201, mean reward: -0.132 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.534, 10.098], loss: 0.002381, mae: 0.047668, mean_q: -0.251431
 85723/100000: episode: 1477, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.407, mean reward: -0.184 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.400, 10.098], loss: 0.029597, mae: 0.071296, mean_q: -0.202016
 85823/100000: episode: 1478, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.191, mean reward: -0.152 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.650, 10.328], loss: 0.002531, mae: 0.049577, mean_q: -0.221122
 85923/100000: episode: 1479, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.816, mean reward: -0.188 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.691, 10.214], loss: 0.015452, mae: 0.057816, mean_q: -0.263828
 86023/100000: episode: 1480, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.159, mean reward: -0.172 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.827, 10.212], loss: 0.015180, mae: 0.056153, mean_q: -0.262388
 86123/100000: episode: 1481, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.296, mean reward: -0.193 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.800, 10.098], loss: 0.002326, mae: 0.047322, mean_q: -0.313641
 86223/100000: episode: 1482, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -10.156, mean reward: -0.102 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.875, 10.357], loss: 0.002406, mae: 0.048028, mean_q: -0.322276
 86323/100000: episode: 1483, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.817, mean reward: -0.198 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.897, 10.137], loss: 0.002595, mae: 0.049723, mean_q: -0.303318
 86423/100000: episode: 1484, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.009, mean reward: -0.160 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.666, 10.098], loss: 0.002373, mae: 0.047763, mean_q: -0.330563
 86523/100000: episode: 1485, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.228, mean reward: -0.172 [-1.000, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.376, 10.254], loss: 0.002566, mae: 0.048461, mean_q: -0.349309
 86623/100000: episode: 1486, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.386, mean reward: -0.184 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.610, 10.098], loss: 0.002325, mae: 0.046645, mean_q: -0.343590
 86723/100000: episode: 1487, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -16.011, mean reward: -0.160 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.622, 10.098], loss: 0.002489, mae: 0.049021, mean_q: -0.325606
 86823/100000: episode: 1488, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.880, mean reward: -0.189 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.201, 10.209], loss: 0.002660, mae: 0.050286, mean_q: -0.305700
 86923/100000: episode: 1489, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.250, mean reward: -0.153 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.615, 10.360], loss: 0.002430, mae: 0.049203, mean_q: -0.335599
 87023/100000: episode: 1490, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.119, mean reward: -0.171 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.196, 10.098], loss: 0.002446, mae: 0.049239, mean_q: -0.302654
 87123/100000: episode: 1491, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.443, mean reward: -0.194 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.377, 10.194], loss: 0.002350, mae: 0.047158, mean_q: -0.324248
 87223/100000: episode: 1492, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.786, mean reward: -0.178 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.338, 10.098], loss: 0.002464, mae: 0.048189, mean_q: -0.328161
 87323/100000: episode: 1493, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -20.059, mean reward: -0.201 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.134, 10.098], loss: 0.002404, mae: 0.048775, mean_q: -0.317708
 87423/100000: episode: 1494, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -12.612, mean reward: -0.126 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.929, 10.098], loss: 0.002323, mae: 0.047432, mean_q: -0.317765
 87523/100000: episode: 1495, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.448, mean reward: -0.174 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.778, 10.098], loss: 0.002461, mae: 0.048133, mean_q: -0.303541
 87623/100000: episode: 1496, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -18.044, mean reward: -0.180 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.436, 10.101], loss: 0.002468, mae: 0.049969, mean_q: -0.308394
 87723/100000: episode: 1497, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.027, mean reward: -0.150 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.533, 10.098], loss: 0.002445, mae: 0.048491, mean_q: -0.299271
 87823/100000: episode: 1498, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -19.991, mean reward: -0.200 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.598, 10.110], loss: 0.002460, mae: 0.048528, mean_q: -0.288657
 87923/100000: episode: 1499, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.906, mean reward: -0.189 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.062, 10.098], loss: 0.002565, mae: 0.049801, mean_q: -0.313082
 88023/100000: episode: 1500, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -19.976, mean reward: -0.200 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.361, 10.245], loss: 0.002496, mae: 0.049548, mean_q: -0.314346
 88123/100000: episode: 1501, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.457, mean reward: -0.185 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.626, 10.098], loss: 0.002515, mae: 0.049480, mean_q: -0.301509
 88223/100000: episode: 1502, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -17.432, mean reward: -0.174 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.710, 10.259], loss: 0.002359, mae: 0.047921, mean_q: -0.327406
 88323/100000: episode: 1503, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -18.523, mean reward: -0.185 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.800, 10.318], loss: 0.002511, mae: 0.049125, mean_q: -0.353333
 88423/100000: episode: 1504, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.303, mean reward: -0.163 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.120, 10.098], loss: 0.002511, mae: 0.049093, mean_q: -0.318486
 88523/100000: episode: 1505, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -14.984, mean reward: -0.150 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.797, 10.098], loss: 0.002393, mae: 0.048579, mean_q: -0.340669
 88623/100000: episode: 1506, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.459, mean reward: -0.155 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.025, 10.185], loss: 0.002427, mae: 0.048088, mean_q: -0.347650
 88723/100000: episode: 1507, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.114, mean reward: -0.151 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.153, 10.098], loss: 0.002494, mae: 0.050070, mean_q: -0.317420
 88823/100000: episode: 1508, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.188, mean reward: -0.182 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.113, 10.098], loss: 0.002582, mae: 0.050542, mean_q: -0.313555
 88923/100000: episode: 1509, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.259, mean reward: -0.173 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.141, 10.200], loss: 0.002422, mae: 0.049507, mean_q: -0.326497
 89023/100000: episode: 1510, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.990, mean reward: -0.180 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.580, 10.098], loss: 0.002712, mae: 0.053880, mean_q: -0.295668
 89123/100000: episode: 1511, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -13.317, mean reward: -0.133 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.096, 10.098], loss: 0.002271, mae: 0.046318, mean_q: -0.333194
 89223/100000: episode: 1512, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.686, mean reward: -0.177 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.560, 10.098], loss: 0.002485, mae: 0.048796, mean_q: -0.325185
 89323/100000: episode: 1513, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.341, mean reward: -0.163 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.332, 10.167], loss: 0.002373, mae: 0.047172, mean_q: -0.346605
 89423/100000: episode: 1514, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.877, mean reward: -0.169 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.168, 10.098], loss: 0.002445, mae: 0.047832, mean_q: -0.324430
 89523/100000: episode: 1515, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.208, mean reward: -0.162 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.638, 10.098], loss: 0.002288, mae: 0.046862, mean_q: -0.344095
 89623/100000: episode: 1516, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -15.129, mean reward: -0.151 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.597, 10.098], loss: 0.002355, mae: 0.047980, mean_q: -0.320776
 89723/100000: episode: 1517, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.062, mean reward: -0.171 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.678, 10.290], loss: 0.002199, mae: 0.045824, mean_q: -0.330407
 89823/100000: episode: 1518, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.372, mean reward: -0.204 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.019, 10.098], loss: 0.002265, mae: 0.046883, mean_q: -0.314330
 89923/100000: episode: 1519, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.997, mean reward: -0.190 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.456, 10.103], loss: 0.002398, mae: 0.048013, mean_q: -0.339271
 90023/100000: episode: 1520, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.220, mean reward: -0.182 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.848, 10.260], loss: 0.002264, mae: 0.047304, mean_q: -0.310390
 90123/100000: episode: 1521, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.227, mean reward: -0.182 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.969, 10.098], loss: 0.002177, mae: 0.045500, mean_q: -0.334822
 90223/100000: episode: 1522, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -12.869, mean reward: -0.129 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.308, 10.257], loss: 0.002351, mae: 0.048050, mean_q: -0.339097
 90323/100000: episode: 1523, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.927, mean reward: -0.179 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.152, 10.098], loss: 0.002807, mae: 0.052435, mean_q: -0.327832
 90423/100000: episode: 1524, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.324, mean reward: -0.183 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.700, 10.194], loss: 0.004035, mae: 0.060849, mean_q: -0.318373
 90523/100000: episode: 1525, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -20.993, mean reward: -0.210 [-1.000, 0.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.064, 10.210], loss: 0.002391, mae: 0.049369, mean_q: -0.330259
 90623/100000: episode: 1526, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -20.050, mean reward: -0.200 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.459, 10.098], loss: 0.002152, mae: 0.045938, mean_q: -0.320621
 90723/100000: episode: 1527, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -16.969, mean reward: -0.170 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.927, 10.098], loss: 0.002048, mae: 0.044886, mean_q: -0.337356
 90823/100000: episode: 1528, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.645, mean reward: -0.176 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.768, 10.231], loss: 0.002209, mae: 0.046594, mean_q: -0.324528
 90923/100000: episode: 1529, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.691, mean reward: -0.157 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.323, 10.104], loss: 0.002337, mae: 0.047495, mean_q: -0.302178
 91023/100000: episode: 1530, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -15.731, mean reward: -0.157 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.098, 10.098], loss: 0.002225, mae: 0.046963, mean_q: -0.335868
 91123/100000: episode: 1531, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.601, mean reward: -0.156 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.055, 10.098], loss: 0.002371, mae: 0.047352, mean_q: -0.327825
[Info] 100-TH LEVEL FOUND: 0.5890268683433533, Considering 10/90 traces
 91223/100000: episode: 1532, duration: 4.565s, episode steps: 100, steps per second: 22, episode reward: -15.805, mean reward: -0.158 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.228, 10.510], loss: 0.002244, mae: 0.046447, mean_q: -0.341909
 91253/100000: episode: 1533, duration: 0.170s, episode steps: 30, steps per second: 177, episode reward: 9.971, mean reward: 0.332 [0.259, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.528, 10.467], loss: 0.002371, mae: 0.048452, mean_q: -0.290157
 91268/100000: episode: 1534, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 5.274, mean reward: 0.352 [0.319, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.134, 10.100], loss: 0.002393, mae: 0.048071, mean_q: -0.269198
 91340/100000: episode: 1535, duration: 0.388s, episode steps: 72, steps per second: 185, episode reward: 13.837, mean reward: 0.192 [0.015, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.709 [-1.004, 10.100], loss: 0.002187, mae: 0.045939, mean_q: -0.358643
 91353/100000: episode: 1536, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 3.590, mean reward: 0.276 [0.205, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.218, 10.100], loss: 0.002107, mae: 0.046047, mean_q: -0.336121
 91372/100000: episode: 1537, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 5.332, mean reward: 0.281 [0.152, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.163, 10.341], loss: 0.002492, mae: 0.048839, mean_q: -0.317016
 91391/100000: episode: 1538, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 7.552, mean reward: 0.397 [0.305, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.783, 10.439], loss: 0.002223, mae: 0.046923, mean_q: -0.314885
 91404/100000: episode: 1539, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 4.157, mean reward: 0.320 [0.238, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.541, 10.100], loss: 0.002566, mae: 0.051088, mean_q: -0.212240
 91436/100000: episode: 1540, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 10.330, mean reward: 0.323 [0.179, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.035, 10.516], loss: 0.002414, mae: 0.048601, mean_q: -0.235815
 91484/100000: episode: 1541, duration: 0.280s, episode steps: 48, steps per second: 171, episode reward: 15.437, mean reward: 0.322 [0.063, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-1.330, 10.100], loss: 0.002577, mae: 0.049513, mean_q: -0.299654
 91512/100000: episode: 1542, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 7.720, mean reward: 0.276 [0.079, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.257, 10.410], loss: 0.002592, mae: 0.051300, mean_q: -0.186048
 91544/100000: episode: 1543, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 11.068, mean reward: 0.346 [0.244, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.935, 10.406], loss: 0.002414, mae: 0.048826, mean_q: -0.243035
 91572/100000: episode: 1544, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 7.157, mean reward: 0.256 [0.133, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.467, 10.347], loss: 0.002379, mae: 0.050264, mean_q: -0.233760
 91620/100000: episode: 1545, duration: 0.265s, episode steps: 48, steps per second: 181, episode reward: 13.927, mean reward: 0.290 [0.093, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-1.430, 10.100], loss: 0.002748, mae: 0.052721, mean_q: -0.237233
 91692/100000: episode: 1546, duration: 0.393s, episode steps: 72, steps per second: 183, episode reward: 18.125, mean reward: 0.252 [0.002, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.704 [-1.134, 10.100], loss: 0.002692, mae: 0.052947, mean_q: -0.239271
 91711/100000: episode: 1547, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 7.288, mean reward: 0.384 [0.200, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.408, 10.100], loss: 0.002340, mae: 0.048825, mean_q: -0.302451
 91783/100000: episode: 1548, duration: 0.398s, episode steps: 72, steps per second: 181, episode reward: 13.821, mean reward: 0.192 [0.023, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.714 [-0.804, 10.246], loss: 0.002654, mae: 0.050882, mean_q: -0.250264
 91831/100000: episode: 1549, duration: 0.263s, episode steps: 48, steps per second: 183, episode reward: 14.002, mean reward: 0.292 [0.014, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.655, 10.100], loss: 0.002703, mae: 0.052221, mean_q: -0.228956
 91850/100000: episode: 1550, duration: 0.101s, episode steps: 19, steps per second: 187, episode reward: 8.006, mean reward: 0.421 [0.251, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.586], loss: 0.002869, mae: 0.055474, mean_q: -0.215128
 91869/100000: episode: 1551, duration: 0.112s, episode steps: 19, steps per second: 169, episode reward: 6.435, mean reward: 0.339 [0.257, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.236, 10.100], loss: 0.002435, mae: 0.049671, mean_q: -0.264356
 91901/100000: episode: 1552, duration: 0.181s, episode steps: 32, steps per second: 176, episode reward: 10.250, mean reward: 0.320 [0.037, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.321, 10.279], loss: 0.002629, mae: 0.051932, mean_q: -0.150177
 91973/100000: episode: 1553, duration: 0.389s, episode steps: 72, steps per second: 185, episode reward: 27.229, mean reward: 0.378 [0.191, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.711 [-1.412, 10.479], loss: 0.002521, mae: 0.050383, mean_q: -0.214385
 91985/100000: episode: 1554, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 4.039, mean reward: 0.337 [0.264, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.390, 10.100], loss: 0.002539, mae: 0.050743, mean_q: -0.152352
 91997/100000: episode: 1555, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 3.779, mean reward: 0.315 [0.243, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.248, 10.100], loss: 0.002546, mae: 0.049898, mean_q: -0.164838
 92027/100000: episode: 1556, duration: 0.160s, episode steps: 30, steps per second: 187, episode reward: 11.553, mean reward: 0.385 [0.280, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.786, 10.453], loss: 0.002402, mae: 0.049558, mean_q: -0.114000
 92046/100000: episode: 1557, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 6.845, mean reward: 0.360 [0.230, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.510], loss: 0.002554, mae: 0.049789, mean_q: -0.257547
 92076/100000: episode: 1558, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 7.581, mean reward: 0.253 [0.090, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.106, 10.188], loss: 0.002547, mae: 0.051958, mean_q: -0.175732
 92148/100000: episode: 1559, duration: 0.400s, episode steps: 72, steps per second: 180, episode reward: 12.568, mean reward: 0.175 [0.018, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.711 [-0.409, 10.100], loss: 0.002829, mae: 0.053373, mean_q: -0.181903
 92178/100000: episode: 1560, duration: 0.185s, episode steps: 30, steps per second: 162, episode reward: 8.469, mean reward: 0.282 [0.190, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.530, 10.395], loss: 0.002315, mae: 0.047353, mean_q: -0.216552
 92191/100000: episode: 1561, duration: 0.085s, episode steps: 13, steps per second: 153, episode reward: 4.373, mean reward: 0.336 [0.261, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.323, 10.100], loss: 0.003147, mae: 0.058538, mean_q: -0.102132
 92263/100000: episode: 1562, duration: 0.395s, episode steps: 72, steps per second: 182, episode reward: 13.718, mean reward: 0.191 [0.028, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.716 [-0.603, 10.124], loss: 0.002493, mae: 0.050572, mean_q: -0.139070
 92295/100000: episode: 1563, duration: 0.176s, episode steps: 32, steps per second: 181, episode reward: 6.461, mean reward: 0.202 [0.081, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.035, 10.220], loss: 0.002809, mae: 0.054716, mean_q: -0.144194
 92314/100000: episode: 1564, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 5.838, mean reward: 0.307 [0.193, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.754, 10.100], loss: 0.002608, mae: 0.050446, mean_q: -0.147259
 92326/100000: episode: 1565, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 3.293, mean reward: 0.274 [0.222, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.283, 10.100], loss: 0.002806, mae: 0.053727, mean_q: -0.166464
 92374/100000: episode: 1566, duration: 0.255s, episode steps: 48, steps per second: 188, episode reward: 16.464, mean reward: 0.343 [0.190, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.934, 10.100], loss: 0.003007, mae: 0.056653, mean_q: -0.126054
 92446/100000: episode: 1567, duration: 0.402s, episode steps: 72, steps per second: 179, episode reward: 14.791, mean reward: 0.205 [0.036, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.713 [-0.193, 10.100], loss: 0.002714, mae: 0.053535, mean_q: -0.125289
 92476/100000: episode: 1568, duration: 0.172s, episode steps: 30, steps per second: 174, episode reward: 7.648, mean reward: 0.255 [0.155, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.688, 10.293], loss: 0.002955, mae: 0.056009, mean_q: -0.067810
 92506/100000: episode: 1569, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 9.646, mean reward: 0.322 [0.202, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.035, 10.517], loss: 0.002683, mae: 0.052821, mean_q: -0.084137
 92578/100000: episode: 1570, duration: 0.382s, episode steps: 72, steps per second: 188, episode reward: 10.350, mean reward: 0.144 [0.013, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.713 [-1.317, 10.100], loss: 0.002751, mae: 0.053722, mean_q: -0.104791
 92597/100000: episode: 1571, duration: 0.096s, episode steps: 19, steps per second: 199, episode reward: 6.098, mean reward: 0.321 [0.256, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.472, 10.100], loss: 0.002522, mae: 0.050975, mean_q: -0.037237
 92610/100000: episode: 1572, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 4.013, mean reward: 0.309 [0.229, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.273, 10.100], loss: 0.003039, mae: 0.056035, mean_q: -0.094597
 92638/100000: episode: 1573, duration: 0.141s, episode steps: 28, steps per second: 198, episode reward: 4.500, mean reward: 0.161 [0.094, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.241], loss: 0.002805, mae: 0.053942, mean_q: -0.093336
 92653/100000: episode: 1574, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 5.139, mean reward: 0.343 [0.282, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.304, 10.100], loss: 0.002563, mae: 0.053514, mean_q: -0.039939
 92672/100000: episode: 1575, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 4.870, mean reward: 0.256 [0.096, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.065, 10.403], loss: 0.002974, mae: 0.056645, mean_q: 0.014077
 92687/100000: episode: 1576, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 5.803, mean reward: 0.387 [0.325, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.208, 10.100], loss: 0.003016, mae: 0.054975, mean_q: -0.096791
 92706/100000: episode: 1577, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 4.502, mean reward: 0.237 [0.138, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.262, 10.388], loss: 0.002807, mae: 0.052833, mean_q: -0.161096
 92718/100000: episode: 1578, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 4.387, mean reward: 0.366 [0.237, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.176, 10.100], loss: 0.002682, mae: 0.051488, mean_q: -0.100230
 92737/100000: episode: 1579, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 5.858, mean reward: 0.308 [0.240, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.295, 10.100], loss: 0.002940, mae: 0.053964, mean_q: -0.116375
 92749/100000: episode: 1580, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 3.617, mean reward: 0.301 [0.188, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.224, 10.100], loss: 0.002879, mae: 0.054303, mean_q: -0.139982
 92762/100000: episode: 1581, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 3.489, mean reward: 0.268 [0.044, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.148], loss: 0.002463, mae: 0.049576, mean_q: -0.080220
 92775/100000: episode: 1582, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 3.448, mean reward: 0.265 [0.227, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.267, 10.100], loss: 0.003109, mae: 0.059689, mean_q: 0.018120
 92805/100000: episode: 1583, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 10.618, mean reward: 0.354 [0.236, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.439, 10.424], loss: 0.002785, mae: 0.053473, mean_q: -0.045565
 92835/100000: episode: 1584, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 8.203, mean reward: 0.273 [0.147, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.035, 10.363], loss: 0.002852, mae: 0.054908, mean_q: -0.057968
 92883/100000: episode: 1585, duration: 0.246s, episode steps: 48, steps per second: 195, episode reward: 12.344, mean reward: 0.257 [0.076, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-1.070, 10.145], loss: 0.003051, mae: 0.055683, mean_q: -0.067113
 92898/100000: episode: 1586, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 5.595, mean reward: 0.373 [0.243, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.279, 10.100], loss: 0.003314, mae: 0.059952, mean_q: -0.004201
 92917/100000: episode: 1587, duration: 0.121s, episode steps: 19, steps per second: 156, episode reward: 5.093, mean reward: 0.268 [0.204, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.911, 10.100], loss: 0.003469, mae: 0.059645, mean_q: -0.039099
 92932/100000: episode: 1588, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 6.759, mean reward: 0.451 [0.347, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.509, 10.100], loss: 0.002626, mae: 0.053097, mean_q: -0.031638
 92945/100000: episode: 1589, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 4.114, mean reward: 0.316 [0.235, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.322, 10.100], loss: 0.002760, mae: 0.055707, mean_q: -0.085805
 92958/100000: episode: 1590, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 4.366, mean reward: 0.336 [0.284, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.311, 10.100], loss: 0.002228, mae: 0.046431, mean_q: -0.136600
 92973/100000: episode: 1591, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 3.940, mean reward: 0.263 [0.181, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.628, 10.100], loss: 0.003019, mae: 0.055114, mean_q: 0.043032
 93021/100000: episode: 1592, duration: 0.260s, episode steps: 48, steps per second: 185, episode reward: 14.361, mean reward: 0.299 [0.177, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.312, 10.100], loss: 0.002986, mae: 0.056750, mean_q: -0.008559
 93069/100000: episode: 1593, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 10.787, mean reward: 0.225 [0.035, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.222, 10.100], loss: 0.003183, mae: 0.057911, mean_q: 0.005377
 93081/100000: episode: 1594, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 4.029, mean reward: 0.336 [0.280, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.353, 10.100], loss: 0.002983, mae: 0.059144, mean_q: 0.040239
 93094/100000: episode: 1595, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 4.073, mean reward: 0.313 [0.224, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.255, 10.100], loss: 0.003509, mae: 0.061667, mean_q: -0.108360
 93107/100000: episode: 1596, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 4.699, mean reward: 0.361 [0.278, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.245, 10.100], loss: 0.002895, mae: 0.055763, mean_q: -0.023218
 93126/100000: episode: 1597, duration: 0.123s, episode steps: 19, steps per second: 154, episode reward: 6.858, mean reward: 0.361 [0.259, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-1.092, 10.100], loss: 0.002900, mae: 0.056708, mean_q: 0.027442
 93141/100000: episode: 1598, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 5.815, mean reward: 0.388 [0.255, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.324, 10.100], loss: 0.002513, mae: 0.051023, mean_q: -0.057032
 93153/100000: episode: 1599, duration: 0.061s, episode steps: 12, steps per second: 195, episode reward: 3.926, mean reward: 0.327 [0.223, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.388, 10.100], loss: 0.002730, mae: 0.054605, mean_q: 0.026745
 93225/100000: episode: 1600, duration: 0.398s, episode steps: 72, steps per second: 181, episode reward: 18.998, mean reward: 0.264 [0.035, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.705 [-0.341, 10.100], loss: 0.003131, mae: 0.057797, mean_q: 0.006870
 93237/100000: episode: 1601, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 3.808, mean reward: 0.317 [0.213, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.346, 10.100], loss: 0.003033, mae: 0.059737, mean_q: 0.115896
 93256/100000: episode: 1602, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 6.599, mean reward: 0.347 [0.194, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.414, 10.100], loss: 0.003719, mae: 0.068613, mean_q: -0.020658
 93286/100000: episode: 1603, duration: 0.175s, episode steps: 30, steps per second: 171, episode reward: 9.282, mean reward: 0.309 [0.219, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-1.661, 10.329], loss: 0.012628, mae: 0.094895, mean_q: 0.082206
 93305/100000: episode: 1604, duration: 0.095s, episode steps: 19, steps per second: 201, episode reward: 7.437, mean reward: 0.391 [0.268, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.527], loss: 0.012176, mae: 0.091199, mean_q: 0.073763
 93353/100000: episode: 1605, duration: 0.264s, episode steps: 48, steps per second: 182, episode reward: 13.047, mean reward: 0.272 [0.153, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.301, 10.100], loss: 0.003666, mae: 0.063972, mean_q: 0.053971
 93383/100000: episode: 1606, duration: 0.168s, episode steps: 30, steps per second: 178, episode reward: 8.971, mean reward: 0.299 [0.222, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.081, 10.343], loss: 0.003128, mae: 0.059866, mean_q: 0.010755
 93411/100000: episode: 1607, duration: 0.157s, episode steps: 28, steps per second: 179, episode reward: 3.191, mean reward: 0.114 [0.026, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.219, 10.101], loss: 0.003454, mae: 0.063650, mean_q: 0.102825
 93426/100000: episode: 1608, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 3.416, mean reward: 0.228 [0.067, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.100], loss: 0.003093, mae: 0.058283, mean_q: 0.069584
 93454/100000: episode: 1609, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 7.684, mean reward: 0.274 [0.186, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.427, 10.438], loss: 0.003139, mae: 0.058111, mean_q: 0.112338
 93473/100000: episode: 1610, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 7.248, mean reward: 0.381 [0.186, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.597, 10.100], loss: 0.003117, mae: 0.058181, mean_q: 0.045374
 93492/100000: episode: 1611, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 3.953, mean reward: 0.208 [0.152, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.305], loss: 0.002866, mae: 0.056857, mean_q: 0.033901
 93564/100000: episode: 1612, duration: 0.387s, episode steps: 72, steps per second: 186, episode reward: 26.163, mean reward: 0.363 [0.200, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.705 [-0.899, 10.379], loss: 0.002840, mae: 0.055835, mean_q: 0.086823
 93577/100000: episode: 1613, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 3.929, mean reward: 0.302 [0.268, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.322, 10.100], loss: 0.003471, mae: 0.062021, mean_q: 0.137743
 93609/100000: episode: 1614, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 13.381, mean reward: 0.418 [0.310, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.336, 10.525], loss: 0.002865, mae: 0.056101, mean_q: 0.075763
 93628/100000: episode: 1615, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 6.379, mean reward: 0.336 [0.244, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.346, 10.456], loss: 0.003424, mae: 0.062552, mean_q: 0.107339
 93658/100000: episode: 1616, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 7.541, mean reward: 0.251 [0.123, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.035, 10.274], loss: 0.003213, mae: 0.058977, mean_q: 0.091705
 93677/100000: episode: 1617, duration: 0.117s, episode steps: 19, steps per second: 163, episode reward: 5.209, mean reward: 0.274 [0.191, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.584, 10.100], loss: 0.002946, mae: 0.055186, mean_q: 0.158802
 93707/100000: episode: 1618, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 8.682, mean reward: 0.289 [0.211, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.481, 10.359], loss: 0.003241, mae: 0.057834, mean_q: 0.048525
 93779/100000: episode: 1619, duration: 0.403s, episode steps: 72, steps per second: 179, episode reward: 16.583, mean reward: 0.230 [0.013, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.700 [-1.258, 10.169], loss: 0.003243, mae: 0.058882, mean_q: 0.135405
 93807/100000: episode: 1620, duration: 0.178s, episode steps: 28, steps per second: 157, episode reward: 5.729, mean reward: 0.205 [0.075, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.052, 10.216], loss: 0.003655, mae: 0.063993, mean_q: 0.160281
 93837/100000: episode: 1621, duration: 0.181s, episode steps: 30, steps per second: 166, episode reward: 6.855, mean reward: 0.229 [0.081, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.024, 10.182], loss: 0.003215, mae: 0.059322, mean_q: 0.109614
[Info] 200-TH LEVEL FOUND: 0.9058840870857239, Considering 12/88 traces
 93850/100000: episode: 1622, duration: 4.166s, episode steps: 13, steps per second: 3, episode reward: 3.565, mean reward: 0.274 [0.239, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.145, 10.100], loss: 0.002940, mae: 0.055872, mean_q: 0.109700
 93922/100000: episode: 1623, duration: 0.388s, episode steps: 72, steps per second: 186, episode reward: 21.555, mean reward: 0.299 [0.174, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.711 [-0.398, 10.403], loss: 0.003029, mae: 0.058225, mean_q: 0.173147
 93994/100000: episode: 1624, duration: 0.397s, episode steps: 72, steps per second: 181, episode reward: 17.634, mean reward: 0.245 [0.119, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.705 [-0.353, 10.184], loss: 0.003338, mae: 0.061259, mean_q: 0.162735
 94066/100000: episode: 1625, duration: 0.402s, episode steps: 72, steps per second: 179, episode reward: 14.466, mean reward: 0.201 [0.026, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.708 [-1.135, 10.116], loss: 0.003246, mae: 0.060014, mean_q: 0.159455
 94138/100000: episode: 1626, duration: 0.379s, episode steps: 72, steps per second: 190, episode reward: 14.265, mean reward: 0.198 [0.026, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.720 [-0.213, 10.179], loss: 0.003336, mae: 0.060860, mean_q: 0.178601
 94210/100000: episode: 1627, duration: 0.391s, episode steps: 72, steps per second: 184, episode reward: 16.185, mean reward: 0.225 [0.018, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.710 [-0.722, 10.176], loss: 0.003212, mae: 0.059829, mean_q: 0.192714
 94282/100000: episode: 1628, duration: 0.399s, episode steps: 72, steps per second: 180, episode reward: 21.220, mean reward: 0.295 [0.171, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.708 [-1.008, 10.261], loss: 0.003175, mae: 0.059946, mean_q: 0.187687
 94354/100000: episode: 1629, duration: 0.439s, episode steps: 72, steps per second: 164, episode reward: 18.576, mean reward: 0.258 [0.104, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.717 [-1.267, 10.100], loss: 0.003135, mae: 0.059850, mean_q: 0.227535
 94426/100000: episode: 1630, duration: 0.412s, episode steps: 72, steps per second: 175, episode reward: 19.812, mean reward: 0.275 [0.111, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.711 [-1.119, 10.351], loss: 0.003111, mae: 0.059203, mean_q: 0.216192
 94498/100000: episode: 1631, duration: 0.385s, episode steps: 72, steps per second: 187, episode reward: 19.427, mean reward: 0.270 [0.058, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.710 [-2.360, 10.162], loss: 0.003045, mae: 0.059589, mean_q: 0.245859
 94570/100000: episode: 1632, duration: 0.382s, episode steps: 72, steps per second: 189, episode reward: 15.504, mean reward: 0.215 [0.048, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.707 [-1.741, 10.112], loss: 0.002935, mae: 0.057474, mean_q: 0.230348
 94642/100000: episode: 1633, duration: 0.386s, episode steps: 72, steps per second: 187, episode reward: 15.255, mean reward: 0.212 [0.016, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.718 [-0.765, 10.100], loss: 0.003021, mae: 0.059039, mean_q: 0.265620
 94714/100000: episode: 1634, duration: 0.406s, episode steps: 72, steps per second: 177, episode reward: 20.897, mean reward: 0.290 [0.028, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.704 [-0.270, 10.100], loss: 0.003082, mae: 0.059299, mean_q: 0.280462
 94786/100000: episode: 1635, duration: 0.373s, episode steps: 72, steps per second: 193, episode reward: 16.013, mean reward: 0.222 [0.041, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.710 [-0.634, 10.256], loss: 0.003238, mae: 0.062330, mean_q: 0.311348
 94858/100000: episode: 1636, duration: 0.407s, episode steps: 72, steps per second: 177, episode reward: 19.435, mean reward: 0.270 [0.099, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.707 [-0.844, 10.231], loss: 0.003332, mae: 0.061884, mean_q: 0.298993
 94930/100000: episode: 1637, duration: 0.402s, episode steps: 72, steps per second: 179, episode reward: 14.861, mean reward: 0.206 [0.016, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.710 [-0.869, 10.100], loss: 0.003240, mae: 0.061617, mean_q: 0.326487
 95002/100000: episode: 1638, duration: 0.377s, episode steps: 72, steps per second: 191, episode reward: 28.771, mean reward: 0.400 [0.169, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.710 [-1.000, 10.534], loss: 0.003150, mae: 0.060616, mean_q: 0.325391
 95074/100000: episode: 1639, duration: 0.420s, episode steps: 72, steps per second: 171, episode reward: 18.167, mean reward: 0.252 [0.100, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.726 [-1.499, 10.339], loss: 0.003287, mae: 0.061488, mean_q: 0.347358
 95146/100000: episode: 1640, duration: 0.390s, episode steps: 72, steps per second: 185, episode reward: 21.546, mean reward: 0.299 [0.086, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.704 [-0.487, 10.112], loss: 0.003226, mae: 0.060875, mean_q: 0.353655
 95218/100000: episode: 1641, duration: 0.395s, episode steps: 72, steps per second: 182, episode reward: 14.440, mean reward: 0.201 [0.009, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.712 [-0.747, 10.100], loss: 0.003259, mae: 0.061482, mean_q: 0.350768
 95290/100000: episode: 1642, duration: 0.402s, episode steps: 72, steps per second: 179, episode reward: 9.841, mean reward: 0.137 [0.006, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.727 [-0.937, 10.379], loss: 0.003072, mae: 0.060584, mean_q: 0.389342
 95362/100000: episode: 1643, duration: 0.384s, episode steps: 72, steps per second: 187, episode reward: 16.246, mean reward: 0.226 [0.023, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.708 [-0.617, 10.304], loss: 0.003513, mae: 0.065031, mean_q: 0.379168
 95434/100000: episode: 1644, duration: 0.397s, episode steps: 72, steps per second: 181, episode reward: 18.284, mean reward: 0.254 [0.078, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.707 [-0.956, 10.219], loss: 0.003388, mae: 0.063547, mean_q: 0.394603
 95506/100000: episode: 1645, duration: 0.391s, episode steps: 72, steps per second: 184, episode reward: 11.977, mean reward: 0.166 [0.014, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.715 [-0.154, 10.150], loss: 0.003328, mae: 0.063581, mean_q: 0.423079
 95578/100000: episode: 1646, duration: 0.413s, episode steps: 72, steps per second: 174, episode reward: 18.354, mean reward: 0.255 [0.103, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.717 [-1.085, 10.327], loss: 0.003325, mae: 0.062374, mean_q: 0.433955
 95650/100000: episode: 1647, duration: 0.396s, episode steps: 72, steps per second: 182, episode reward: 18.739, mean reward: 0.260 [0.038, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.707 [-0.606, 10.163], loss: 0.003364, mae: 0.063857, mean_q: 0.440281
 95722/100000: episode: 1648, duration: 0.395s, episode steps: 72, steps per second: 182, episode reward: 17.390, mean reward: 0.242 [0.020, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.696 [-0.572, 10.100], loss: 0.003332, mae: 0.062959, mean_q: 0.451268
 95794/100000: episode: 1649, duration: 0.394s, episode steps: 72, steps per second: 183, episode reward: 15.554, mean reward: 0.216 [0.040, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.711 [-1.046, 10.100], loss: 0.003110, mae: 0.061479, mean_q: 0.458937
 95866/100000: episode: 1650, duration: 0.399s, episode steps: 72, steps per second: 181, episode reward: 13.162, mean reward: 0.183 [0.025, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.715 [-1.611, 10.115], loss: 0.003421, mae: 0.064297, mean_q: 0.472978
 95938/100000: episode: 1651, duration: 0.386s, episode steps: 72, steps per second: 187, episode reward: 13.042, mean reward: 0.181 [0.001, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.706 [-0.857, 10.199], loss: 0.005602, mae: 0.069362, mean_q: 0.480964
 96010/100000: episode: 1652, duration: 0.387s, episode steps: 72, steps per second: 186, episode reward: 15.272, mean reward: 0.212 [0.008, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.704 [-0.832, 10.100], loss: 0.004267, mae: 0.065928, mean_q: 0.485403
 96082/100000: episode: 1653, duration: 0.398s, episode steps: 72, steps per second: 181, episode reward: 15.491, mean reward: 0.215 [0.054, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.706 [-0.797, 10.100], loss: 0.003339, mae: 0.063948, mean_q: 0.502957
 96154/100000: episode: 1654, duration: 0.380s, episode steps: 72, steps per second: 190, episode reward: 15.219, mean reward: 0.211 [0.017, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.707 [-1.358, 10.100], loss: 0.003347, mae: 0.063578, mean_q: 0.496851
 96226/100000: episode: 1655, duration: 0.388s, episode steps: 72, steps per second: 185, episode reward: 13.030, mean reward: 0.181 [0.023, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.709 [-1.039, 10.167], loss: 0.003525, mae: 0.065334, mean_q: 0.493222
 96298/100000: episode: 1656, duration: 0.384s, episode steps: 72, steps per second: 188, episode reward: 11.615, mean reward: 0.161 [0.017, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.706 [-1.013, 10.100], loss: 0.003549, mae: 0.064725, mean_q: 0.496969
 96370/100000: episode: 1657, duration: 0.422s, episode steps: 72, steps per second: 171, episode reward: 16.095, mean reward: 0.224 [0.027, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.710 [-0.867, 10.160], loss: 0.003316, mae: 0.063625, mean_q: 0.499838
 96442/100000: episode: 1658, duration: 0.404s, episode steps: 72, steps per second: 178, episode reward: 14.670, mean reward: 0.204 [0.005, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.708 [-0.304, 10.277], loss: 0.003208, mae: 0.062879, mean_q: 0.497216
 96514/100000: episode: 1659, duration: 0.419s, episode steps: 72, steps per second: 172, episode reward: 20.904, mean reward: 0.290 [0.035, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.707 [-0.944, 10.309], loss: 0.003308, mae: 0.063395, mean_q: 0.497397
 96586/100000: episode: 1660, duration: 0.398s, episode steps: 72, steps per second: 181, episode reward: 16.533, mean reward: 0.230 [0.019, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.713 [-0.706, 10.100], loss: 0.003376, mae: 0.064946, mean_q: 0.495372
 96658/100000: episode: 1661, duration: 0.375s, episode steps: 72, steps per second: 192, episode reward: 23.533, mean reward: 0.327 [0.124, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.714 [-0.542, 10.432], loss: 0.003254, mae: 0.062640, mean_q: 0.485572
 96730/100000: episode: 1662, duration: 0.388s, episode steps: 72, steps per second: 185, episode reward: 14.264, mean reward: 0.198 [0.011, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.719 [-0.745, 10.149], loss: 0.003373, mae: 0.064292, mean_q: 0.491711
 96802/100000: episode: 1663, duration: 0.376s, episode steps: 72, steps per second: 191, episode reward: 16.052, mean reward: 0.223 [0.047, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.709 [-1.966, 10.343], loss: 0.003472, mae: 0.065078, mean_q: 0.493059
 96874/100000: episode: 1664, duration: 0.384s, episode steps: 72, steps per second: 188, episode reward: 16.408, mean reward: 0.228 [0.082, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.709 [-1.634, 10.100], loss: 0.003483, mae: 0.065831, mean_q: 0.487164
 96946/100000: episode: 1665, duration: 0.387s, episode steps: 72, steps per second: 186, episode reward: 12.805, mean reward: 0.178 [0.012, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.717 [-1.210, 10.178], loss: 0.003505, mae: 0.064796, mean_q: 0.489694
 97018/100000: episode: 1666, duration: 0.388s, episode steps: 72, steps per second: 185, episode reward: 24.580, mean reward: 0.341 [0.155, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.712 [-0.769, 10.503], loss: 0.003465, mae: 0.064411, mean_q: 0.487721
 97090/100000: episode: 1667, duration: 0.374s, episode steps: 72, steps per second: 192, episode reward: 13.218, mean reward: 0.184 [0.017, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.722 [-0.461, 10.322], loss: 0.003123, mae: 0.061977, mean_q: 0.484506
 97162/100000: episode: 1668, duration: 0.374s, episode steps: 72, steps per second: 193, episode reward: 13.743, mean reward: 0.191 [0.026, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.720 [-0.652, 10.282], loss: 0.003366, mae: 0.064440, mean_q: 0.484849
 97234/100000: episode: 1669, duration: 0.396s, episode steps: 72, steps per second: 182, episode reward: 13.624, mean reward: 0.189 [0.008, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.718 [-0.765, 10.100], loss: 0.003450, mae: 0.064222, mean_q: 0.482932
 97306/100000: episode: 1670, duration: 0.386s, episode steps: 72, steps per second: 187, episode reward: 13.697, mean reward: 0.190 [0.013, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.708 [-0.264, 10.171], loss: 0.003718, mae: 0.067353, mean_q: 0.476566
 97378/100000: episode: 1671, duration: 0.383s, episode steps: 72, steps per second: 188, episode reward: 16.756, mean reward: 0.233 [0.029, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.705 [-0.243, 10.100], loss: 0.003459, mae: 0.064738, mean_q: 0.478682
 97450/100000: episode: 1672, duration: 0.406s, episode steps: 72, steps per second: 177, episode reward: 13.836, mean reward: 0.192 [0.030, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.701 [-0.952, 10.100], loss: 0.003546, mae: 0.066795, mean_q: 0.481875
 97522/100000: episode: 1673, duration: 0.391s, episode steps: 72, steps per second: 184, episode reward: 17.479, mean reward: 0.243 [0.028, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.712 [-0.687, 10.404], loss: 0.003486, mae: 0.064171, mean_q: 0.471318
 97594/100000: episode: 1674, duration: 0.372s, episode steps: 72, steps per second: 193, episode reward: 11.809, mean reward: 0.164 [0.031, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.722 [-1.556, 10.100], loss: 0.003232, mae: 0.062885, mean_q: 0.478205
 97666/100000: episode: 1675, duration: 0.397s, episode steps: 72, steps per second: 181, episode reward: 13.770, mean reward: 0.191 [0.015, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.723 [-0.326, 10.162], loss: 0.003403, mae: 0.064438, mean_q: 0.478268
 97738/100000: episode: 1676, duration: 0.374s, episode steps: 72, steps per second: 193, episode reward: 14.758, mean reward: 0.205 [0.026, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.707 [-0.638, 10.100], loss: 0.003518, mae: 0.065208, mean_q: 0.473688
 97810/100000: episode: 1677, duration: 0.399s, episode steps: 72, steps per second: 180, episode reward: 14.563, mean reward: 0.202 [0.017, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.708 [-1.175, 10.153], loss: 0.003375, mae: 0.065200, mean_q: 0.472261
 97882/100000: episode: 1678, duration: 0.385s, episode steps: 72, steps per second: 187, episode reward: 23.324, mean reward: 0.324 [0.150, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.714 [-0.735, 10.461], loss: 0.003509, mae: 0.065797, mean_q: 0.460589
 97954/100000: episode: 1679, duration: 0.379s, episode steps: 72, steps per second: 190, episode reward: 14.466, mean reward: 0.201 [0.026, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.713 [-1.237, 10.100], loss: 0.003446, mae: 0.064307, mean_q: 0.461546
 98026/100000: episode: 1680, duration: 0.385s, episode steps: 72, steps per second: 187, episode reward: 14.277, mean reward: 0.198 [0.011, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.709 [-0.516, 10.232], loss: 0.003553, mae: 0.066115, mean_q: 0.469797
 98098/100000: episode: 1681, duration: 0.411s, episode steps: 72, steps per second: 175, episode reward: 29.534, mean reward: 0.410 [0.230, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.702 [-0.494, 10.605], loss: 0.003429, mae: 0.065124, mean_q: 0.462250
 98170/100000: episode: 1682, duration: 0.368s, episode steps: 72, steps per second: 196, episode reward: 15.898, mean reward: 0.221 [0.019, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.712 [-1.316, 10.100], loss: 0.003514, mae: 0.065772, mean_q: 0.462839
 98242/100000: episode: 1683, duration: 0.406s, episode steps: 72, steps per second: 177, episode reward: 15.772, mean reward: 0.219 [0.045, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.714 [-1.027, 10.119], loss: 0.003532, mae: 0.065130, mean_q: 0.463634
 98314/100000: episode: 1684, duration: 0.396s, episode steps: 72, steps per second: 182, episode reward: 15.703, mean reward: 0.218 [0.024, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.707 [-0.288, 10.155], loss: 0.003398, mae: 0.064647, mean_q: 0.460398
 98386/100000: episode: 1685, duration: 0.388s, episode steps: 72, steps per second: 185, episode reward: 16.660, mean reward: 0.231 [0.051, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.695 [-0.298, 10.100], loss: 0.003272, mae: 0.063301, mean_q: 0.462187
 98458/100000: episode: 1686, duration: 0.382s, episode steps: 72, steps per second: 188, episode reward: 16.346, mean reward: 0.227 [0.012, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.702 [-0.423, 10.100], loss: 0.003395, mae: 0.063642, mean_q: 0.449436
 98530/100000: episode: 1687, duration: 0.368s, episode steps: 72, steps per second: 196, episode reward: 13.927, mean reward: 0.193 [0.033, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.713 [-1.583, 10.100], loss: 0.003276, mae: 0.063197, mean_q: 0.455068
 98602/100000: episode: 1688, duration: 0.404s, episode steps: 72, steps per second: 178, episode reward: 11.462, mean reward: 0.159 [0.021, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.713 [-0.214, 10.100], loss: 0.003149, mae: 0.062224, mean_q: 0.453163
 98674/100000: episode: 1689, duration: 0.386s, episode steps: 72, steps per second: 187, episode reward: 25.797, mean reward: 0.358 [0.101, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.710 [-0.691, 10.454], loss: 0.003301, mae: 0.063747, mean_q: 0.444864
 98746/100000: episode: 1690, duration: 0.408s, episode steps: 72, steps per second: 176, episode reward: 15.269, mean reward: 0.212 [0.021, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.712 [-0.952, 10.100], loss: 0.003300, mae: 0.064211, mean_q: 0.454752
 98818/100000: episode: 1691, duration: 0.379s, episode steps: 72, steps per second: 190, episode reward: 18.452, mean reward: 0.256 [0.022, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.710 [-0.796, 10.100], loss: 0.003463, mae: 0.065508, mean_q: 0.445782
 98890/100000: episode: 1692, duration: 0.386s, episode steps: 72, steps per second: 187, episode reward: 14.827, mean reward: 0.206 [0.055, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.706 [-0.964, 10.142], loss: 0.003333, mae: 0.063760, mean_q: 0.445888
 98962/100000: episode: 1693, duration: 0.385s, episode steps: 72, steps per second: 187, episode reward: 14.471, mean reward: 0.201 [0.018, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.713 [-1.465, 10.100], loss: 0.003170, mae: 0.062585, mean_q: 0.442613
 99034/100000: episode: 1694, duration: 0.407s, episode steps: 72, steps per second: 177, episode reward: 15.858, mean reward: 0.220 [0.006, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.714 [-1.866, 10.144], loss: 0.003563, mae: 0.066705, mean_q: 0.447093
 99106/100000: episode: 1695, duration: 0.396s, episode steps: 72, steps per second: 182, episode reward: 14.124, mean reward: 0.196 [0.026, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.716 [-0.583, 10.100], loss: 0.003411, mae: 0.064769, mean_q: 0.446977
 99178/100000: episode: 1696, duration: 0.390s, episode steps: 72, steps per second: 185, episode reward: 25.274, mean reward: 0.351 [0.205, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.724 [-0.454, 10.708], loss: 0.003602, mae: 0.066131, mean_q: 0.452213
 99250/100000: episode: 1697, duration: 0.373s, episode steps: 72, steps per second: 193, episode reward: 15.685, mean reward: 0.218 [0.047, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.717 [-0.343, 10.100], loss: 0.003329, mae: 0.063620, mean_q: 0.439985
 99322/100000: episode: 1698, duration: 0.389s, episode steps: 72, steps per second: 185, episode reward: 17.641, mean reward: 0.245 [0.036, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.709 [-0.812, 10.100], loss: 0.003335, mae: 0.063771, mean_q: 0.456195
 99394/100000: episode: 1699, duration: 0.409s, episode steps: 72, steps per second: 176, episode reward: 23.509, mean reward: 0.327 [0.213, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.711 [-0.914, 10.371], loss: 0.003218, mae: 0.062840, mean_q: 0.451308
 99466/100000: episode: 1700, duration: 0.398s, episode steps: 72, steps per second: 181, episode reward: 19.505, mean reward: 0.271 [0.061, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.713 [-0.286, 10.432], loss: 0.003184, mae: 0.062781, mean_q: 0.449642
 99538/100000: episode: 1701, duration: 0.394s, episode steps: 72, steps per second: 183, episode reward: 14.528, mean reward: 0.202 [0.052, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.708 [-0.870, 10.231], loss: 0.003450, mae: 0.065691, mean_q: 0.455127
 99610/100000: episode: 1702, duration: 0.393s, episode steps: 72, steps per second: 183, episode reward: 19.021, mean reward: 0.264 [0.058, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.704 [-0.825, 10.205], loss: 0.003299, mae: 0.062689, mean_q: 0.457518
 99682/100000: episode: 1703, duration: 0.395s, episode steps: 72, steps per second: 182, episode reward: 15.423, mean reward: 0.214 [0.042, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.715 [-1.289, 10.294], loss: 0.003315, mae: 0.064249, mean_q: 0.442277
 99754/100000: episode: 1704, duration: 0.386s, episode steps: 72, steps per second: 186, episode reward: 16.046, mean reward: 0.223 [0.051, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.720 [-0.989, 10.399], loss: 0.003061, mae: 0.061278, mean_q: 0.446477
 99826/100000: episode: 1705, duration: 0.377s, episode steps: 72, steps per second: 191, episode reward: 16.311, mean reward: 0.227 [0.030, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.715 [-1.602, 10.261], loss: 0.003467, mae: 0.065504, mean_q: 0.445525
 99898/100000: episode: 1706, duration: 0.360s, episode steps: 72, steps per second: 200, episode reward: 21.473, mean reward: 0.298 [0.097, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.719 [-0.523, 10.202], loss: 0.003471, mae: 0.065067, mean_q: 0.450187
 99970/100000: episode: 1707, duration: 0.363s, episode steps: 72, steps per second: 198, episode reward: 15.356, mean reward: 0.213 [0.018, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.720 [-0.396, 10.100], loss: 0.003505, mae: 0.065934, mean_q: 0.446023
done, took 634.097 seconds
[Info] End Importance Splitting. Falsification occurred 6 times.
