Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.171s, episode steps: 100, steps per second: 585, episode reward: -16.887, mean reward: -0.169 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.616, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.074s, episode steps: 100, steps per second: 1359, episode reward: -18.703, mean reward: -0.187 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.551, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.094s, episode steps: 100, steps per second: 1068, episode reward: -19.058, mean reward: -0.191 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.423, 10.143], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.061s, episode steps: 100, steps per second: 1638, episode reward: -17.991, mean reward: -0.180 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.264, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.076s, episode steps: 100, steps per second: 1314, episode reward: -13.732, mean reward: -0.137 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.718, 10.369], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.238s, episode steps: 100, steps per second: 81, episode reward: -16.805, mean reward: -0.168 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.845, 10.258], loss: 0.049210, mae: 0.221087, mean_q: -0.040275
   700/100000: episode: 7, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -10.850, mean reward: -0.108 [-1.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.680, 10.420], loss: 0.014221, mae: 0.111067, mean_q: -0.213521
   800/100000: episode: 8, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -17.920, mean reward: -0.179 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.948, 10.098], loss: 0.011354, mae: 0.098088, mean_q: -0.267319
   900/100000: episode: 9, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.846, mean reward: -0.168 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.819, 10.141], loss: 0.009464, mae: 0.089018, mean_q: -0.307115
  1000/100000: episode: 10, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.303, mean reward: -0.143 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.957, 10.204], loss: 0.008478, mae: 0.085524, mean_q: -0.321647
  1100/100000: episode: 11, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -16.918, mean reward: -0.169 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.673, 10.136], loss: 0.008148, mae: 0.080968, mean_q: -0.352267
  1200/100000: episode: 12, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -13.324, mean reward: -0.133 [-1.000, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.784, 10.098], loss: 0.008174, mae: 0.080421, mean_q: -0.340843
  1300/100000: episode: 13, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -11.066, mean reward: -0.111 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.820, 10.098], loss: 0.009005, mae: 0.085833, mean_q: -0.309221
  1400/100000: episode: 14, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -19.690, mean reward: -0.197 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.141, 10.117], loss: 0.008150, mae: 0.080399, mean_q: -0.318005
  1500/100000: episode: 15, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.966, mean reward: -0.160 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.768, 10.098], loss: 0.007942, mae: 0.080367, mean_q: -0.312068
  1600/100000: episode: 16, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.046, mean reward: -0.170 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.333, 10.320], loss: 0.008378, mae: 0.082574, mean_q: -0.292071
  1700/100000: episode: 17, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -13.981, mean reward: -0.140 [-1.000, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.728, 10.098], loss: 0.007017, mae: 0.076177, mean_q: -0.321452
  1800/100000: episode: 18, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.246, mean reward: -0.152 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.893, 10.098], loss: 0.006351, mae: 0.073717, mean_q: -0.290615
  1900/100000: episode: 19, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.271, mean reward: -0.153 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.590, 10.407], loss: 0.006530, mae: 0.072539, mean_q: -0.328917
  2000/100000: episode: 20, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.352, mean reward: -0.174 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.359, 10.098], loss: 0.007101, mae: 0.075852, mean_q: -0.280833
  2100/100000: episode: 21, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.876, mean reward: -0.169 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.489, 10.098], loss: 0.007184, mae: 0.076731, mean_q: -0.320404
  2200/100000: episode: 22, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -19.048, mean reward: -0.190 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.676, 10.098], loss: 0.005912, mae: 0.070018, mean_q: -0.344277
  2300/100000: episode: 23, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.473, mean reward: -0.155 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.868, 10.412], loss: 0.006443, mae: 0.072761, mean_q: -0.309373
  2400/100000: episode: 24, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.129, mean reward: -0.181 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.630, 10.098], loss: 0.007295, mae: 0.078555, mean_q: -0.307676
  2500/100000: episode: 25, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.959, mean reward: -0.190 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.975, 10.127], loss: 0.006166, mae: 0.072117, mean_q: -0.298988
  2600/100000: episode: 26, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.328, mean reward: -0.173 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.993, 10.098], loss: 0.006047, mae: 0.072054, mean_q: -0.340239
  2700/100000: episode: 27, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.429, mean reward: -0.174 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.525, 10.366], loss: 0.006361, mae: 0.073102, mean_q: -0.294888
  2800/100000: episode: 28, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.338, mean reward: -0.193 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.788, 10.172], loss: 0.005683, mae: 0.070295, mean_q: -0.323253
  2900/100000: episode: 29, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.011, mean reward: -0.160 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.269, 10.098], loss: 0.005683, mae: 0.069032, mean_q: -0.310412
  3000/100000: episode: 30, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.517, mean reward: -0.185 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.682, 10.108], loss: 0.006635, mae: 0.074949, mean_q: -0.310350
  3100/100000: episode: 31, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -16.933, mean reward: -0.169 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.786, 10.098], loss: 0.005388, mae: 0.067276, mean_q: -0.304377
  3200/100000: episode: 32, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -18.285, mean reward: -0.183 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.091, 10.135], loss: 0.006022, mae: 0.068684, mean_q: -0.361145
  3300/100000: episode: 33, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.216, mean reward: -0.172 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.742, 10.098], loss: 0.006303, mae: 0.071966, mean_q: -0.299631
  3400/100000: episode: 34, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.209, mean reward: -0.192 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.397, 10.098], loss: 0.005828, mae: 0.070997, mean_q: -0.286751
  3500/100000: episode: 35, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.064, mean reward: -0.181 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.728, 10.118], loss: 0.004802, mae: 0.065150, mean_q: -0.336097
  3600/100000: episode: 36, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.012, mean reward: -0.170 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.832, 10.206], loss: 0.006178, mae: 0.071430, mean_q: -0.310688
  3700/100000: episode: 37, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -15.901, mean reward: -0.159 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.116, 10.511], loss: 0.005716, mae: 0.068336, mean_q: -0.304698
  3800/100000: episode: 38, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.151, mean reward: -0.152 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.478, 10.490], loss: 0.005295, mae: 0.068420, mean_q: -0.317068
  3900/100000: episode: 39, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.111, mean reward: -0.181 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.174, 10.292], loss: 0.005850, mae: 0.069453, mean_q: -0.339834
  4000/100000: episode: 40, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.317, mean reward: -0.163 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.485, 10.098], loss: 0.005489, mae: 0.068611, mean_q: -0.329045
  4100/100000: episode: 41, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.442, mean reward: -0.184 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.427, 10.125], loss: 0.005501, mae: 0.068341, mean_q: -0.327748
  4200/100000: episode: 42, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -16.584, mean reward: -0.166 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.509, 10.098], loss: 0.005405, mae: 0.066965, mean_q: -0.302352
  4300/100000: episode: 43, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.704, mean reward: -0.167 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.125, 10.098], loss: 0.005507, mae: 0.068530, mean_q: -0.329348
  4400/100000: episode: 44, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.535, mean reward: -0.185 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.939, 10.098], loss: 0.005892, mae: 0.069273, mean_q: -0.323305
  4500/100000: episode: 45, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -18.338, mean reward: -0.183 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.051, 10.108], loss: 0.004986, mae: 0.067070, mean_q: -0.307440
  4600/100000: episode: 46, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.965, mean reward: -0.190 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.775, 10.098], loss: 0.005349, mae: 0.068244, mean_q: -0.282049
  4700/100000: episode: 47, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.007, mean reward: -0.190 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.431, 10.164], loss: 0.004975, mae: 0.065486, mean_q: -0.340147
  4800/100000: episode: 48, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -8.832, mean reward: -0.088 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.707, 10.098], loss: 0.004958, mae: 0.065342, mean_q: -0.347408
  4900/100000: episode: 49, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.108, mean reward: -0.161 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.235, 10.138], loss: 0.005107, mae: 0.068359, mean_q: -0.334408
  5000/100000: episode: 50, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.109, mean reward: -0.171 [-1.000, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.866, 10.098], loss: 0.005886, mae: 0.068335, mean_q: -0.318707
  5100/100000: episode: 51, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.961, mean reward: -0.180 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.813, 10.291], loss: 0.004847, mae: 0.064045, mean_q: -0.339851
  5200/100000: episode: 52, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -10.637, mean reward: -0.106 [-1.000, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.623, 10.475], loss: 0.005141, mae: 0.065023, mean_q: -0.321211
  5300/100000: episode: 53, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -11.029, mean reward: -0.110 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.044, 10.098], loss: 0.005304, mae: 0.068527, mean_q: -0.296820
  5400/100000: episode: 54, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.205, mean reward: -0.182 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.862, 10.098], loss: 0.004488, mae: 0.065251, mean_q: -0.276966
  5500/100000: episode: 55, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -20.968, mean reward: -0.210 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.609, 10.260], loss: 0.007280, mae: 0.073918, mean_q: -0.306263
  5600/100000: episode: 56, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -6.369, mean reward: -0.064 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.275, 10.482], loss: 0.005678, mae: 0.069593, mean_q: -0.297966
  5700/100000: episode: 57, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -20.373, mean reward: -0.204 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.570, 10.208], loss: 0.004874, mae: 0.066484, mean_q: -0.318625
  5800/100000: episode: 58, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.777, mean reward: -0.188 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.970, 10.146], loss: 0.005544, mae: 0.071593, mean_q: -0.278154
  5900/100000: episode: 59, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.633, mean reward: -0.166 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.709, 10.109], loss: 0.005022, mae: 0.067914, mean_q: -0.302125
  6000/100000: episode: 60, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.210, mean reward: -0.152 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.499, 10.098], loss: 0.004378, mae: 0.065638, mean_q: -0.281390
  6100/100000: episode: 61, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -19.203, mean reward: -0.192 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.119, 10.158], loss: 0.005820, mae: 0.071182, mean_q: -0.299669
  6200/100000: episode: 62, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.578, mean reward: -0.176 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.764, 10.264], loss: 0.005355, mae: 0.069742, mean_q: -0.321379
  6300/100000: episode: 63, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.896, mean reward: -0.179 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.396, 10.098], loss: 0.005215, mae: 0.066006, mean_q: -0.373743
  6400/100000: episode: 64, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: -14.378, mean reward: -0.144 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.305, 10.376], loss: 0.005454, mae: 0.071341, mean_q: -0.303447
  6500/100000: episode: 65, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.625, mean reward: -0.146 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.040, 10.098], loss: 0.004910, mae: 0.068840, mean_q: -0.295184
  6600/100000: episode: 66, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -21.163, mean reward: -0.212 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.766, 10.185], loss: 0.004558, mae: 0.064904, mean_q: -0.340534
  6700/100000: episode: 67, duration: 0.519s, episode steps: 100, steps per second: 192, episode reward: -16.491, mean reward: -0.165 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.492, 10.499], loss: 0.004458, mae: 0.064689, mean_q: -0.317955
  6800/100000: episode: 68, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.350, mean reward: -0.154 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.860, 10.211], loss: 0.005645, mae: 0.070483, mean_q: -0.282816
  6900/100000: episode: 69, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.908, mean reward: -0.169 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.237, 10.181], loss: 0.005290, mae: 0.068589, mean_q: -0.306286
  7000/100000: episode: 70, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.635, mean reward: -0.166 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.260, 10.298], loss: 0.004144, mae: 0.062943, mean_q: -0.301689
  7100/100000: episode: 71, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -20.424, mean reward: -0.204 [-1.000, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.166, 10.179], loss: 0.004575, mae: 0.065360, mean_q: -0.308086
  7200/100000: episode: 72, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.196, mean reward: -0.182 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.476, 10.107], loss: 0.004225, mae: 0.062309, mean_q: -0.328196
  7300/100000: episode: 73, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.731, mean reward: -0.167 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.012, 10.404], loss: 0.004425, mae: 0.063674, mean_q: -0.319048
  7400/100000: episode: 74, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -12.667, mean reward: -0.127 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.557, 10.098], loss: 0.004068, mae: 0.062134, mean_q: -0.320333
  7500/100000: episode: 75, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.747, mean reward: -0.187 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.263, 10.098], loss: 0.004804, mae: 0.065977, mean_q: -0.306908
  7600/100000: episode: 76, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -14.021, mean reward: -0.140 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.223, 10.098], loss: 0.005514, mae: 0.067515, mean_q: -0.318323
  7700/100000: episode: 77, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.188, mean reward: -0.192 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.740, 10.180], loss: 0.004399, mae: 0.062614, mean_q: -0.330409
  7800/100000: episode: 78, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -14.648, mean reward: -0.146 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.265, 10.098], loss: 0.004432, mae: 0.063927, mean_q: -0.322191
  7900/100000: episode: 79, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -18.370, mean reward: -0.184 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.793, 10.098], loss: 0.005584, mae: 0.067022, mean_q: -0.323301
  8000/100000: episode: 80, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -18.604, mean reward: -0.186 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.709, 10.098], loss: 0.005064, mae: 0.065103, mean_q: -0.343139
  8100/100000: episode: 81, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -16.166, mean reward: -0.162 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.365, 10.098], loss: 0.004832, mae: 0.067015, mean_q: -0.299612
  8200/100000: episode: 82, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -14.798, mean reward: -0.148 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.926, 10.098], loss: 0.004296, mae: 0.062588, mean_q: -0.312868
  8300/100000: episode: 83, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -16.651, mean reward: -0.167 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.778, 10.323], loss: 0.004033, mae: 0.062793, mean_q: -0.308248
  8400/100000: episode: 84, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.570, mean reward: -0.176 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.794, 10.098], loss: 0.004170, mae: 0.061671, mean_q: -0.331359
  8500/100000: episode: 85, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -18.140, mean reward: -0.181 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.825, 10.098], loss: 0.004358, mae: 0.063924, mean_q: -0.301755
  8600/100000: episode: 86, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -18.060, mean reward: -0.181 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.884, 10.098], loss: 0.004215, mae: 0.062967, mean_q: -0.302371
  8700/100000: episode: 87, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.298, mean reward: -0.203 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.809, 10.189], loss: 0.005311, mae: 0.067942, mean_q: -0.299028
  8800/100000: episode: 88, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.849, mean reward: -0.178 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.217, 10.187], loss: 0.005784, mae: 0.067816, mean_q: -0.328034
  8900/100000: episode: 89, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.903, mean reward: -0.179 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.947, 10.224], loss: 0.004812, mae: 0.065322, mean_q: -0.316402
  9000/100000: episode: 90, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.343, mean reward: -0.193 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.583, 10.098], loss: 0.004532, mae: 0.063351, mean_q: -0.368739
  9100/100000: episode: 91, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.375, mean reward: -0.174 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.967, 10.226], loss: 0.004376, mae: 0.061734, mean_q: -0.338688
  9200/100000: episode: 92, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.478, mean reward: -0.205 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.199, 10.098], loss: 0.005400, mae: 0.064835, mean_q: -0.314880
  9300/100000: episode: 93, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.287, mean reward: -0.153 [-1.000, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.401, 10.300], loss: 0.004365, mae: 0.060495, mean_q: -0.342601
  9400/100000: episode: 94, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -19.371, mean reward: -0.194 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.647, 10.137], loss: 0.003865, mae: 0.062158, mean_q: -0.299289
  9500/100000: episode: 95, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -19.065, mean reward: -0.191 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.247, 10.192], loss: 0.003891, mae: 0.060505, mean_q: -0.320722
  9600/100000: episode: 96, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -15.173, mean reward: -0.152 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.897, 10.255], loss: 0.004047, mae: 0.061363, mean_q: -0.301841
  9700/100000: episode: 97, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -14.718, mean reward: -0.147 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.952, 10.467], loss: 0.003859, mae: 0.059772, mean_q: -0.358339
  9800/100000: episode: 98, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.753, mean reward: -0.178 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.310, 10.098], loss: 0.004628, mae: 0.062068, mean_q: -0.333965
  9900/100000: episode: 99, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -14.327, mean reward: -0.143 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.321, 10.135], loss: 0.004426, mae: 0.063273, mean_q: -0.296688
 10000/100000: episode: 100, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -19.945, mean reward: -0.199 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.297, 10.098], loss: 0.003656, mae: 0.059581, mean_q: -0.288633
 10100/100000: episode: 101, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -18.622, mean reward: -0.186 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.742, 10.098], loss: 0.003503, mae: 0.061208, mean_q: -0.316732
 10200/100000: episode: 102, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -19.568, mean reward: -0.196 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.119, 10.098], loss: 0.003970, mae: 0.061158, mean_q: -0.289549
 10300/100000: episode: 103, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.620, mean reward: -0.176 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.395, 10.208], loss: 0.003452, mae: 0.058348, mean_q: -0.343993
 10400/100000: episode: 104, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.913, mean reward: -0.169 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.070, 10.161], loss: 0.004083, mae: 0.062323, mean_q: -0.329974
 10500/100000: episode: 105, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.691, mean reward: -0.187 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.731, 10.098], loss: 0.004065, mae: 0.062116, mean_q: -0.336450
 10600/100000: episode: 106, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -16.652, mean reward: -0.167 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.577, 10.098], loss: 0.003712, mae: 0.059926, mean_q: -0.311132
 10700/100000: episode: 107, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -18.690, mean reward: -0.187 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.600, 10.159], loss: 0.003163, mae: 0.056307, mean_q: -0.342646
 10800/100000: episode: 108, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -15.252, mean reward: -0.153 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.424, 10.171], loss: 0.004130, mae: 0.059444, mean_q: -0.343749
 10900/100000: episode: 109, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.124, mean reward: -0.171 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.267, 10.098], loss: 0.004954, mae: 0.065098, mean_q: -0.330988
 11000/100000: episode: 110, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -17.313, mean reward: -0.173 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.449, 10.098], loss: 0.003100, mae: 0.056506, mean_q: -0.332583
 11100/100000: episode: 111, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.414, mean reward: -0.164 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.067, 10.098], loss: 0.003343, mae: 0.058621, mean_q: -0.334624
 11200/100000: episode: 112, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -20.688, mean reward: -0.207 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.966, 10.100], loss: 0.003423, mae: 0.060207, mean_q: -0.307465
 11300/100000: episode: 113, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -9.016, mean reward: -0.090 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.421, 10.127], loss: 0.004687, mae: 0.064249, mean_q: -0.310927
 11400/100000: episode: 114, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -14.307, mean reward: -0.143 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.002, 10.098], loss: 0.003709, mae: 0.060415, mean_q: -0.336078
 11500/100000: episode: 115, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.723, mean reward: -0.147 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.334, 10.098], loss: 0.003679, mae: 0.060055, mean_q: -0.331680
 11600/100000: episode: 116, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.434, mean reward: -0.164 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.291, 10.388], loss: 0.003768, mae: 0.059714, mean_q: -0.340828
 11700/100000: episode: 117, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -19.037, mean reward: -0.190 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.611, 10.135], loss: 0.003840, mae: 0.060836, mean_q: -0.301234
 11800/100000: episode: 118, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.780, mean reward: -0.178 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.506, 10.249], loss: 0.003269, mae: 0.057383, mean_q: -0.341612
 11900/100000: episode: 119, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.000, mean reward: -0.170 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.983, 10.098], loss: 0.003628, mae: 0.058404, mean_q: -0.357784
 12000/100000: episode: 120, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -9.572, mean reward: -0.096 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.414, 10.098], loss: 0.003520, mae: 0.059952, mean_q: -0.298821
 12100/100000: episode: 121, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -10.927, mean reward: -0.109 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.901, 10.334], loss: 0.008181, mae: 0.075702, mean_q: -0.318968
 12200/100000: episode: 122, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -11.471, mean reward: -0.115 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.320, 10.217], loss: 0.004137, mae: 0.063784, mean_q: -0.327322
 12300/100000: episode: 123, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -14.123, mean reward: -0.141 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.062, 10.249], loss: 0.003659, mae: 0.061479, mean_q: -0.299940
 12400/100000: episode: 124, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -12.127, mean reward: -0.121 [-1.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.306, 10.393], loss: 0.003389, mae: 0.060480, mean_q: -0.323029
 12500/100000: episode: 125, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.826, mean reward: -0.148 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.953, 10.246], loss: 0.003890, mae: 0.064084, mean_q: -0.326051
 12600/100000: episode: 126, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.046, mean reward: -0.180 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.118, 10.351], loss: 0.003322, mae: 0.059448, mean_q: -0.310225
 12700/100000: episode: 127, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.856, mean reward: -0.169 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.571, 10.313], loss: 0.003383, mae: 0.060080, mean_q: -0.291019
 12800/100000: episode: 128, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -16.970, mean reward: -0.170 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.022, 10.276], loss: 0.003558, mae: 0.061766, mean_q: -0.305159
 12900/100000: episode: 129, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.471, mean reward: -0.175 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.792, 10.334], loss: 0.003712, mae: 0.062086, mean_q: -0.271936
 13000/100000: episode: 130, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -17.129, mean reward: -0.171 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.294, 10.101], loss: 0.003495, mae: 0.059436, mean_q: -0.323298
 13100/100000: episode: 131, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.871, mean reward: -0.179 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.139, 10.098], loss: 0.005606, mae: 0.067672, mean_q: -0.305980
 13200/100000: episode: 132, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.965, mean reward: -0.190 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.206, 10.105], loss: 0.003652, mae: 0.061970, mean_q: -0.328621
 13300/100000: episode: 133, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.792, mean reward: -0.178 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.624, 10.335], loss: 0.003363, mae: 0.059630, mean_q: -0.302178
 13400/100000: episode: 134, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -12.112, mean reward: -0.121 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.032, 10.098], loss: 0.003643, mae: 0.059789, mean_q: -0.306202
 13500/100000: episode: 135, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.380, mean reward: -0.164 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.423, 10.108], loss: 0.005937, mae: 0.070980, mean_q: -0.308478
 13600/100000: episode: 136, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.251, mean reward: -0.163 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.471, 10.098], loss: 0.003478, mae: 0.060283, mean_q: -0.309878
 13700/100000: episode: 137, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.327, mean reward: -0.193 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.772, 10.098], loss: 0.003363, mae: 0.059517, mean_q: -0.334290
 13800/100000: episode: 138, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.164, mean reward: -0.192 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.768, 10.098], loss: 0.003483, mae: 0.060319, mean_q: -0.307765
 13900/100000: episode: 139, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.574, mean reward: -0.186 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.826, 10.098], loss: 0.003620, mae: 0.062621, mean_q: -0.341074
 14000/100000: episode: 140, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -19.543, mean reward: -0.195 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.435, 10.221], loss: 0.003655, mae: 0.061084, mean_q: -0.334909
 14100/100000: episode: 141, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.281, mean reward: -0.173 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.754, 10.098], loss: 0.004081, mae: 0.065779, mean_q: -0.293425
 14200/100000: episode: 142, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.977, mean reward: -0.180 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.181, 10.098], loss: 0.003739, mae: 0.064964, mean_q: -0.295250
 14300/100000: episode: 143, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -17.332, mean reward: -0.173 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.668, 10.098], loss: 0.003534, mae: 0.060691, mean_q: -0.353966
 14400/100000: episode: 144, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -7.592, mean reward: -0.076 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.871, 10.465], loss: 0.003690, mae: 0.062083, mean_q: -0.314971
 14500/100000: episode: 145, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.559, mean reward: -0.186 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.711, 10.361], loss: 0.003998, mae: 0.065306, mean_q: -0.292861
 14600/100000: episode: 146, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.034, mean reward: -0.180 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.018, 10.155], loss: 0.005022, mae: 0.069355, mean_q: -0.314634
 14700/100000: episode: 147, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.748, mean reward: -0.177 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.629, 10.266], loss: 0.004340, mae: 0.063634, mean_q: -0.378671
 14800/100000: episode: 148, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.428, mean reward: -0.164 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.112, 10.098], loss: 0.003463, mae: 0.059749, mean_q: -0.313440
 14900/100000: episode: 149, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.210, mean reward: -0.162 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.567, 10.098], loss: 0.003482, mae: 0.059702, mean_q: -0.309409
[Info] 100-TH LEVEL FOUND: 0.420746386051178, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.343s, episode steps: 100, steps per second: 23, episode reward: -18.064, mean reward: -0.181 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.817, 10.098], loss: 0.004044, mae: 0.064465, mean_q: -0.278659
 15031/100000: episode: 151, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 7.524, mean reward: 0.243 [0.097, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.035, 10.220], loss: 0.003667, mae: 0.063160, mean_q: -0.284264
 15060/100000: episode: 152, duration: 0.135s, episode steps: 29, steps per second: 215, episode reward: 5.958, mean reward: 0.205 [0.025, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.257], loss: 0.003502, mae: 0.059914, mean_q: -0.266962
 15095/100000: episode: 153, duration: 0.179s, episode steps: 35, steps per second: 195, episode reward: 11.261, mean reward: 0.322 [0.182, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.814, 10.100], loss: 0.003452, mae: 0.060423, mean_q: -0.305192
 15166/100000: episode: 154, duration: 0.373s, episode steps: 71, steps per second: 190, episode reward: 12.440, mean reward: 0.175 [0.032, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.727 [-0.484, 10.191], loss: 0.003530, mae: 0.061060, mean_q: -0.300233
 15202/100000: episode: 155, duration: 0.182s, episode steps: 36, steps per second: 197, episode reward: 8.118, mean reward: 0.226 [0.017, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.127, 10.149], loss: 0.005429, mae: 0.067885, mean_q: -0.339867
 15238/100000: episode: 156, duration: 0.189s, episode steps: 36, steps per second: 191, episode reward: 11.009, mean reward: 0.306 [0.124, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.325, 10.278], loss: 0.004112, mae: 0.067196, mean_q: -0.269469
 15265/100000: episode: 157, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 6.694, mean reward: 0.248 [0.162, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.389, 10.283], loss: 0.004650, mae: 0.067104, mean_q: -0.296087
 15300/100000: episode: 158, duration: 0.178s, episode steps: 35, steps per second: 196, episode reward: 8.740, mean reward: 0.250 [0.053, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.106, 10.100], loss: 0.003695, mae: 0.062276, mean_q: -0.273952
 15336/100000: episode: 159, duration: 0.194s, episode steps: 36, steps per second: 186, episode reward: 9.972, mean reward: 0.277 [0.166, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.444, 10.259], loss: 0.003595, mae: 0.063350, mean_q: -0.187319
 15407/100000: episode: 160, duration: 0.356s, episode steps: 71, steps per second: 200, episode reward: 16.262, mean reward: 0.229 [0.061, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.719 [-0.286, 10.100], loss: 0.003508, mae: 0.061267, mean_q: -0.233775
 15434/100000: episode: 161, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 9.953, mean reward: 0.369 [0.265, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.061, 10.551], loss: 0.003418, mae: 0.060723, mean_q: -0.242232
 15463/100000: episode: 162, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 8.803, mean reward: 0.304 [0.182, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.483, 10.264], loss: 0.003998, mae: 0.065516, mean_q: -0.219039
 15495/100000: episode: 163, duration: 0.158s, episode steps: 32, steps per second: 202, episode reward: 10.553, mean reward: 0.330 [0.209, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.035, 10.511], loss: 0.003974, mae: 0.065532, mean_q: -0.233348
 15530/100000: episode: 164, duration: 0.183s, episode steps: 35, steps per second: 192, episode reward: 8.049, mean reward: 0.230 [0.055, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.893, 10.134], loss: 0.004518, mae: 0.069432, mean_q: -0.133512
 15562/100000: episode: 165, duration: 0.166s, episode steps: 32, steps per second: 193, episode reward: 8.029, mean reward: 0.251 [0.120, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.784, 10.288], loss: 0.004634, mae: 0.068980, mean_q: -0.279374
 15598/100000: episode: 166, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 12.405, mean reward: 0.345 [0.263, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.469, 10.468], loss: 0.004562, mae: 0.068126, mean_q: -0.202440
 15669/100000: episode: 167, duration: 0.358s, episode steps: 71, steps per second: 199, episode reward: 12.034, mean reward: 0.169 [0.012, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.731 [-1.102, 10.197], loss: 0.004010, mae: 0.065487, mean_q: -0.192052
 15740/100000: episode: 168, duration: 0.347s, episode steps: 71, steps per second: 204, episode reward: 12.195, mean reward: 0.172 [0.022, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.727 [-0.964, 10.196], loss: 0.004702, mae: 0.068630, mean_q: -0.197068
 15771/100000: episode: 169, duration: 0.166s, episode steps: 31, steps per second: 186, episode reward: 7.544, mean reward: 0.243 [0.114, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-1.637, 10.307], loss: 0.005693, mae: 0.070730, mean_q: -0.203657
 15807/100000: episode: 170, duration: 0.176s, episode steps: 36, steps per second: 205, episode reward: 7.235, mean reward: 0.201 [0.049, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.617, 10.235], loss: 0.006298, mae: 0.073834, mean_q: -0.198124
 15842/100000: episode: 171, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 15.614, mean reward: 0.446 [0.287, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.702, 10.100], loss: 0.003839, mae: 0.066512, mean_q: -0.215031
 15869/100000: episode: 172, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 6.593, mean reward: 0.244 [0.177, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.607, 10.328], loss: 0.004595, mae: 0.071512, mean_q: -0.149957
 15940/100000: episode: 173, duration: 0.353s, episode steps: 71, steps per second: 201, episode reward: 9.219, mean reward: 0.130 [0.006, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.735 [-0.474, 10.100], loss: 0.004273, mae: 0.066380, mean_q: -0.197264
 15971/100000: episode: 174, duration: 0.181s, episode steps: 31, steps per second: 172, episode reward: 6.934, mean reward: 0.224 [0.078, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.875, 10.304], loss: 0.005414, mae: 0.073709, mean_q: -0.162671
 16007/100000: episode: 175, duration: 0.202s, episode steps: 36, steps per second: 179, episode reward: 7.643, mean reward: 0.212 [0.078, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.065, 10.210], loss: 0.004015, mae: 0.065999, mean_q: -0.196779
 16038/100000: episode: 176, duration: 0.156s, episode steps: 31, steps per second: 198, episode reward: 5.506, mean reward: 0.178 [0.054, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.035, 10.245], loss: 0.004170, mae: 0.066389, mean_q: -0.172543
 16065/100000: episode: 177, duration: 0.145s, episode steps: 27, steps per second: 187, episode reward: 6.343, mean reward: 0.235 [0.091, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.131, 10.214], loss: 0.004557, mae: 0.070715, mean_q: -0.134243
 16096/100000: episode: 178, duration: 0.152s, episode steps: 31, steps per second: 204, episode reward: 11.863, mean reward: 0.383 [0.250, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.531, 10.553], loss: 0.003966, mae: 0.066478, mean_q: -0.094710
 16123/100000: episode: 179, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 5.617, mean reward: 0.208 [0.101, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.786, 10.193], loss: 0.004452, mae: 0.071347, mean_q: -0.058835
 16154/100000: episode: 180, duration: 0.154s, episode steps: 31, steps per second: 201, episode reward: 6.938, mean reward: 0.224 [0.049, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-1.473, 10.141], loss: 0.004652, mae: 0.072307, mean_q: -0.090260
 16185/100000: episode: 181, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 9.217, mean reward: 0.297 [0.161, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.465, 10.441], loss: 0.004256, mae: 0.069504, mean_q: -0.105629
 16212/100000: episode: 182, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 6.995, mean reward: 0.259 [0.162, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.406], loss: 0.004663, mae: 0.068177, mean_q: -0.118708
 16239/100000: episode: 183, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 9.967, mean reward: 0.369 [0.194, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.233, 10.442], loss: 0.004386, mae: 0.068279, mean_q: -0.114816
 16275/100000: episode: 184, duration: 0.180s, episode steps: 36, steps per second: 201, episode reward: 10.480, mean reward: 0.291 [0.139, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.515, 10.315], loss: 0.004763, mae: 0.071167, mean_q: -0.072032
 16347/100000: episode: 185, duration: 0.369s, episode steps: 72, steps per second: 195, episode reward: 13.254, mean reward: 0.184 [0.013, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.728 [-1.292, 10.224], loss: 0.004128, mae: 0.067462, mean_q: -0.067890
 16418/100000: episode: 186, duration: 0.379s, episode steps: 71, steps per second: 187, episode reward: 17.138, mean reward: 0.241 [0.033, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.740 [-1.310, 10.618], loss: 0.004028, mae: 0.066047, mean_q: -0.084886
 16445/100000: episode: 187, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 8.207, mean reward: 0.304 [0.234, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.552, 10.397], loss: 0.004806, mae: 0.073158, mean_q: -0.084412
 16516/100000: episode: 188, duration: 0.364s, episode steps: 71, steps per second: 195, episode reward: 13.623, mean reward: 0.192 [0.031, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.743 [-0.775, 10.198], loss: 0.004852, mae: 0.070038, mean_q: -0.086244
 16543/100000: episode: 189, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 6.763, mean reward: 0.250 [0.171, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.410], loss: 0.004816, mae: 0.071032, mean_q: -0.085523
 16578/100000: episode: 190, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 7.332, mean reward: 0.209 [0.029, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.304, 10.137], loss: 0.005024, mae: 0.075411, mean_q: -0.023589
 16605/100000: episode: 191, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 8.537, mean reward: 0.316 [0.194, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.706, 10.308], loss: 0.004166, mae: 0.068425, mean_q: 0.009561
 16641/100000: episode: 192, duration: 0.181s, episode steps: 36, steps per second: 198, episode reward: 10.363, mean reward: 0.288 [0.172, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.147, 10.309], loss: 0.004129, mae: 0.067268, mean_q: -0.110779
 16668/100000: episode: 193, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 4.155, mean reward: 0.154 [0.054, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.138, 10.163], loss: 0.004186, mae: 0.068432, mean_q: -0.031665
 16739/100000: episode: 194, duration: 0.374s, episode steps: 71, steps per second: 190, episode reward: 10.897, mean reward: 0.153 [0.022, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.726 [-0.123, 10.105], loss: 0.004194, mae: 0.067733, mean_q: -0.000935
 16766/100000: episode: 195, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 9.155, mean reward: 0.339 [0.182, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.035, 10.281], loss: 0.004440, mae: 0.070095, mean_q: -0.017977
 16838/100000: episode: 196, duration: 0.354s, episode steps: 72, steps per second: 203, episode reward: 12.759, mean reward: 0.177 [0.006, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.710 [-0.897, 10.265], loss: 0.004482, mae: 0.070582, mean_q: -0.012743
 16874/100000: episode: 197, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 10.478, mean reward: 0.291 [0.177, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.817, 10.425], loss: 0.004417, mae: 0.068389, mean_q: -0.002637
 16909/100000: episode: 198, duration: 0.186s, episode steps: 35, steps per second: 189, episode reward: 9.872, mean reward: 0.282 [0.143, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.833, 10.100], loss: 0.004490, mae: 0.070275, mean_q: 0.005029
 16938/100000: episode: 199, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 6.491, mean reward: 0.224 [0.135, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.377, 10.304], loss: 0.004462, mae: 0.069937, mean_q: -0.034020
 16969/100000: episode: 200, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 5.637, mean reward: 0.182 [0.045, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.877, 10.327], loss: 0.005092, mae: 0.075375, mean_q: 0.040669
 16996/100000: episode: 201, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 6.974, mean reward: 0.258 [0.123, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.454, 10.208], loss: 0.004962, mae: 0.070758, mean_q: -0.021139
 17067/100000: episode: 202, duration: 0.347s, episode steps: 71, steps per second: 204, episode reward: 10.504, mean reward: 0.148 [0.021, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.738 [-0.372, 10.356], loss: 0.004665, mae: 0.071612, mean_q: 0.041721
 17098/100000: episode: 203, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 7.350, mean reward: 0.237 [0.118, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-1.138, 10.202], loss: 0.007207, mae: 0.082232, mean_q: 0.006208
 17127/100000: episode: 204, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 7.321, mean reward: 0.252 [0.104, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.744, 10.244], loss: 0.004211, mae: 0.067687, mean_q: -0.002355
 17159/100000: episode: 205, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 10.863, mean reward: 0.339 [0.243, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.105, 10.417], loss: 0.004229, mae: 0.068984, mean_q: -0.028084
 17186/100000: episode: 206, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 7.364, mean reward: 0.273 [0.201, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.035, 10.460], loss: 0.004554, mae: 0.071362, mean_q: 0.017518
 17213/100000: episode: 207, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 4.393, mean reward: 0.163 [0.021, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.179, 10.213], loss: 0.004407, mae: 0.070677, mean_q: 0.064140
 17285/100000: episode: 208, duration: 0.364s, episode steps: 72, steps per second: 198, episode reward: 13.072, mean reward: 0.182 [0.016, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.720 [-1.310, 10.100], loss: 0.004453, mae: 0.071952, mean_q: 0.081654
 17317/100000: episode: 209, duration: 0.157s, episode steps: 32, steps per second: 204, episode reward: 9.476, mean reward: 0.296 [0.079, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.374], loss: 0.004375, mae: 0.069353, mean_q: 0.025255
 17353/100000: episode: 210, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 5.990, mean reward: 0.166 [0.042, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.242, 10.144], loss: 0.004362, mae: 0.068910, mean_q: 0.015295
 17389/100000: episode: 211, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 8.820, mean reward: 0.245 [0.132, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.965, 10.325], loss: 0.004143, mae: 0.068135, mean_q: 0.079895
 17460/100000: episode: 212, duration: 0.376s, episode steps: 71, steps per second: 189, episode reward: 14.353, mean reward: 0.202 [0.019, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.734 [-0.184, 10.100], loss: 0.004125, mae: 0.068913, mean_q: 0.077951
 17487/100000: episode: 213, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 8.622, mean reward: 0.319 [0.190, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.381], loss: 0.005053, mae: 0.077770, mean_q: 0.095554
 17558/100000: episode: 214, duration: 0.340s, episode steps: 71, steps per second: 209, episode reward: 11.306, mean reward: 0.159 [0.013, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.732 [-0.885, 10.262], loss: 0.004450, mae: 0.070648, mean_q: 0.056078
 17589/100000: episode: 215, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 12.535, mean reward: 0.404 [0.262, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.716, 10.380], loss: 0.004671, mae: 0.071675, mean_q: 0.042750
 17661/100000: episode: 216, duration: 0.362s, episode steps: 72, steps per second: 199, episode reward: 11.662, mean reward: 0.162 [0.021, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.714 [-0.238, 10.100], loss: 0.004188, mae: 0.069095, mean_q: 0.091755
 17697/100000: episode: 217, duration: 0.189s, episode steps: 36, steps per second: 191, episode reward: 9.343, mean reward: 0.260 [0.092, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.275, 10.325], loss: 0.004087, mae: 0.068090, mean_q: 0.096090
 17724/100000: episode: 218, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 6.313, mean reward: 0.234 [0.091, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.430], loss: 0.003827, mae: 0.065984, mean_q: 0.042533
 17755/100000: episode: 219, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 4.759, mean reward: 0.154 [0.023, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.081, 10.100], loss: 0.004285, mae: 0.069008, mean_q: 0.110118
 17784/100000: episode: 220, duration: 0.151s, episode steps: 29, steps per second: 193, episode reward: 6.906, mean reward: 0.238 [0.048, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.502, 10.185], loss: 0.004161, mae: 0.070996, mean_q: 0.138144
 17820/100000: episode: 221, duration: 0.186s, episode steps: 36, steps per second: 193, episode reward: 6.880, mean reward: 0.191 [0.037, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.441, 10.212], loss: 0.004097, mae: 0.069259, mean_q: 0.125183
 17847/100000: episode: 222, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 5.773, mean reward: 0.214 [0.017, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.810, 10.134], loss: 0.003954, mae: 0.068263, mean_q: 0.112578
 17918/100000: episode: 223, duration: 0.352s, episode steps: 71, steps per second: 202, episode reward: 12.353, mean reward: 0.174 [0.008, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.726 [-0.311, 10.273], loss: 0.004177, mae: 0.068532, mean_q: 0.105591
 17989/100000: episode: 224, duration: 0.377s, episode steps: 71, steps per second: 189, episode reward: 8.785, mean reward: 0.124 [0.019, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.721 [-0.586, 10.132], loss: 0.004240, mae: 0.070144, mean_q: 0.170753
 18018/100000: episode: 225, duration: 0.138s, episode steps: 29, steps per second: 211, episode reward: 7.102, mean reward: 0.245 [0.091, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.021, 10.269], loss: 0.004266, mae: 0.070282, mean_q: 0.137088
 18050/100000: episode: 226, duration: 0.166s, episode steps: 32, steps per second: 192, episode reward: 6.232, mean reward: 0.195 [0.056, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.355, 10.142], loss: 0.004771, mae: 0.074776, mean_q: 0.149046
 18079/100000: episode: 227, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 7.741, mean reward: 0.267 [0.144, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.035, 10.291], loss: 0.004238, mae: 0.070202, mean_q: 0.167573
 18110/100000: episode: 228, duration: 0.153s, episode steps: 31, steps per second: 202, episode reward: 7.510, mean reward: 0.242 [0.112, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-1.072, 10.264], loss: 0.004156, mae: 0.069593, mean_q: 0.181419
 18137/100000: episode: 229, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 3.531, mean reward: 0.131 [0.038, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.458, 10.100], loss: 0.005132, mae: 0.076903, mean_q: 0.188901
 18209/100000: episode: 230, duration: 0.362s, episode steps: 72, steps per second: 199, episode reward: 11.922, mean reward: 0.166 [0.020, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.724 [-0.746, 10.318], loss: 0.004631, mae: 0.072184, mean_q: 0.149851
 18241/100000: episode: 231, duration: 0.156s, episode steps: 32, steps per second: 205, episode reward: 8.308, mean reward: 0.260 [0.089, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.035, 10.253], loss: 0.004824, mae: 0.074680, mean_q: 0.184010
 18313/100000: episode: 232, duration: 0.369s, episode steps: 72, steps per second: 195, episode reward: 12.385, mean reward: 0.172 [0.018, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.728 [-0.086, 10.311], loss: 0.004414, mae: 0.071718, mean_q: 0.186314
 18340/100000: episode: 233, duration: 0.146s, episode steps: 27, steps per second: 186, episode reward: 7.410, mean reward: 0.274 [0.178, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.441], loss: 0.004073, mae: 0.068961, mean_q: 0.246836
 18367/100000: episode: 234, duration: 0.151s, episode steps: 27, steps per second: 178, episode reward: 8.330, mean reward: 0.309 [0.131, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.433, 10.338], loss: 0.004615, mae: 0.073147, mean_q: 0.217285
 18396/100000: episode: 235, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 5.722, mean reward: 0.197 [0.058, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.125, 10.189], loss: 0.004604, mae: 0.072447, mean_q: 0.226613
 18423/100000: episode: 236, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 7.583, mean reward: 0.281 [0.173, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.575, 10.382], loss: 0.004762, mae: 0.072528, mean_q: 0.228892
 18454/100000: episode: 237, duration: 0.155s, episode steps: 31, steps per second: 200, episode reward: 8.321, mean reward: 0.268 [0.112, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.395, 10.218], loss: 0.007502, mae: 0.082029, mean_q: 0.227367
 18490/100000: episode: 238, duration: 0.195s, episode steps: 36, steps per second: 185, episode reward: 12.386, mean reward: 0.344 [0.219, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.546, 10.415], loss: 0.007541, mae: 0.078903, mean_q: 0.184202
 18561/100000: episode: 239, duration: 0.360s, episode steps: 71, steps per second: 197, episode reward: 17.078, mean reward: 0.241 [0.069, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.730 [-0.304, 10.100], loss: 0.005432, mae: 0.078061, mean_q: 0.243167
[Info] 200-TH LEVEL FOUND: 0.6346079111099243, Considering 10/90 traces
 18588/100000: episode: 240, duration: 3.960s, episode steps: 27, steps per second: 7, episode reward: 7.328, mean reward: 0.271 [0.166, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.053, 10.277], loss: 0.004031, mae: 0.069152, mean_q: 0.249930
 18616/100000: episode: 241, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 6.675, mean reward: 0.238 [0.170, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.557, 10.300], loss: 0.004318, mae: 0.072778, mean_q: 0.262170
 18636/100000: episode: 242, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 7.004, mean reward: 0.350 [0.277, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-1.050, 10.440], loss: 0.005403, mae: 0.081095, mean_q: 0.257498
 18661/100000: episode: 243, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 7.831, mean reward: 0.313 [0.140, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.032, 10.272], loss: 0.004492, mae: 0.073806, mean_q: 0.260812
 18681/100000: episode: 244, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 8.795, mean reward: 0.440 [0.314, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.663, 10.580], loss: 0.003863, mae: 0.068550, mean_q: 0.239623
 18712/100000: episode: 245, duration: 0.160s, episode steps: 31, steps per second: 193, episode reward: 11.754, mean reward: 0.379 [0.219, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.344, 10.463], loss: 0.004120, mae: 0.069661, mean_q: 0.240120
 18739/100000: episode: 246, duration: 0.139s, episode steps: 27, steps per second: 195, episode reward: 9.011, mean reward: 0.334 [0.215, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.393, 10.467], loss: 0.004475, mae: 0.074012, mean_q: 0.281474
 18770/100000: episode: 247, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 8.924, mean reward: 0.288 [0.122, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.794, 10.220], loss: 0.004431, mae: 0.073416, mean_q: 0.272306
 18795/100000: episode: 248, duration: 0.143s, episode steps: 25, steps per second: 174, episode reward: 10.662, mean reward: 0.426 [0.335, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.230, 10.527], loss: 0.004312, mae: 0.071819, mean_q: 0.286988
 18818/100000: episode: 249, duration: 0.121s, episode steps: 23, steps per second: 191, episode reward: 7.363, mean reward: 0.320 [0.221, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.747, 10.460], loss: 0.004329, mae: 0.072520, mean_q: 0.323296
 18845/100000: episode: 250, duration: 0.139s, episode steps: 27, steps per second: 195, episode reward: 9.177, mean reward: 0.340 [0.211, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.482], loss: 0.004112, mae: 0.070275, mean_q: 0.285646
 18865/100000: episode: 251, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 8.209, mean reward: 0.410 [0.330, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.643], loss: 0.004303, mae: 0.070968, mean_q: 0.258592
 18890/100000: episode: 252, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 5.377, mean reward: 0.215 [0.144, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.570, 10.274], loss: 0.003972, mae: 0.069086, mean_q: 0.308098
 18912/100000: episode: 253, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 6.563, mean reward: 0.298 [0.106, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.959, 10.232], loss: 0.004514, mae: 0.073960, mean_q: 0.323414
 18934/100000: episode: 254, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 10.659, mean reward: 0.485 [0.369, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.612, 10.644], loss: 0.004087, mae: 0.070256, mean_q: 0.327017
 18954/100000: episode: 255, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 8.119, mean reward: 0.406 [0.281, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.408, 10.564], loss: 0.004059, mae: 0.070113, mean_q: 0.322654
 18977/100000: episode: 256, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 7.188, mean reward: 0.313 [0.222, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.254, 10.354], loss: 0.004269, mae: 0.071293, mean_q: 0.291561
 19002/100000: episode: 257, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 6.551, mean reward: 0.262 [0.102, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.349, 10.154], loss: 0.004748, mae: 0.075412, mean_q: 0.315285
 19033/100000: episode: 258, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 8.872, mean reward: 0.286 [0.054, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.553, 10.117], loss: 0.004512, mae: 0.075138, mean_q: 0.328122
 19051/100000: episode: 259, duration: 0.086s, episode steps: 18, steps per second: 210, episode reward: 8.659, mean reward: 0.481 [0.337, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.352, 10.100], loss: 0.004083, mae: 0.069627, mean_q: 0.293333
 19079/100000: episode: 260, duration: 0.141s, episode steps: 28, steps per second: 199, episode reward: 9.950, mean reward: 0.355 [0.216, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.472], loss: 0.005012, mae: 0.076816, mean_q: 0.292517
 19107/100000: episode: 261, duration: 0.143s, episode steps: 28, steps per second: 195, episode reward: 6.693, mean reward: 0.239 [0.034, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.585, 10.167], loss: 0.009480, mae: 0.086386, mean_q: 0.346633
 19130/100000: episode: 262, duration: 0.134s, episode steps: 23, steps per second: 171, episode reward: 8.586, mean reward: 0.373 [0.279, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.473], loss: 0.005335, mae: 0.077692, mean_q: 0.347624
 19158/100000: episode: 263, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 11.402, mean reward: 0.407 [0.309, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.469, 10.456], loss: 0.004890, mae: 0.073521, mean_q: 0.378639
 19183/100000: episode: 264, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 9.840, mean reward: 0.394 [0.237, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.588, 10.444], loss: 0.005465, mae: 0.079812, mean_q: 0.346885
 19214/100000: episode: 265, duration: 0.152s, episode steps: 31, steps per second: 204, episode reward: 13.490, mean reward: 0.435 [0.349, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-1.084, 10.642], loss: 0.004752, mae: 0.076563, mean_q: 0.359979
 19245/100000: episode: 266, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 8.620, mean reward: 0.278 [0.017, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.563, 10.145], loss: 0.004540, mae: 0.072639, mean_q: 0.364040
 19270/100000: episode: 267, duration: 0.144s, episode steps: 25, steps per second: 174, episode reward: 7.445, mean reward: 0.298 [0.195, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.462, 10.475], loss: 0.005135, mae: 0.078531, mean_q: 0.377968
 19290/100000: episode: 268, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 9.165, mean reward: 0.458 [0.271, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.626], loss: 0.004594, mae: 0.075399, mean_q: 0.381633
 19315/100000: episode: 269, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 6.834, mean reward: 0.273 [0.141, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.629, 10.222], loss: 0.005226, mae: 0.079635, mean_q: 0.386144
 19343/100000: episode: 270, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 10.815, mean reward: 0.386 [0.248, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.055, 10.436], loss: 0.005167, mae: 0.076885, mean_q: 0.387241
 19364/100000: episode: 271, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 7.753, mean reward: 0.369 [0.260, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-1.020, 10.364], loss: 0.004441, mae: 0.074952, mean_q: 0.391882
 19392/100000: episode: 272, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 7.871, mean reward: 0.281 [0.068, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-1.692, 10.215], loss: 0.004769, mae: 0.075134, mean_q: 0.400229
 19417/100000: episode: 273, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 6.577, mean reward: 0.263 [0.080, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.350, 10.115], loss: 0.004037, mae: 0.069956, mean_q: 0.397992
 19442/100000: episode: 274, duration: 0.135s, episode steps: 25, steps per second: 186, episode reward: 6.989, mean reward: 0.280 [0.093, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.224], loss: 0.004456, mae: 0.072172, mean_q: 0.430253
 19464/100000: episode: 275, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 9.123, mean reward: 0.415 [0.302, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.605, 10.390], loss: 0.004596, mae: 0.073481, mean_q: 0.424893
 19482/100000: episode: 276, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 7.742, mean reward: 0.430 [0.335, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.367, 10.100], loss: 0.005489, mae: 0.078736, mean_q: 0.410655
 19507/100000: episode: 277, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 6.748, mean reward: 0.270 [0.100, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.162, 10.278], loss: 0.005319, mae: 0.080817, mean_q: 0.425660
 19528/100000: episode: 278, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 8.022, mean reward: 0.382 [0.286, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.417], loss: 0.004797, mae: 0.077550, mean_q: 0.414883
 19551/100000: episode: 279, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 7.870, mean reward: 0.342 [0.143, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.622, 10.352], loss: 0.004927, mae: 0.075778, mean_q: 0.431319
 19582/100000: episode: 280, duration: 0.180s, episode steps: 31, steps per second: 173, episode reward: 10.376, mean reward: 0.335 [0.252, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.392, 10.478], loss: 0.004720, mae: 0.075411, mean_q: 0.437420
 19603/100000: episode: 281, duration: 0.100s, episode steps: 21, steps per second: 209, episode reward: 9.188, mean reward: 0.438 [0.275, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.576], loss: 0.003790, mae: 0.067728, mean_q: 0.465668
 19621/100000: episode: 282, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 9.726, mean reward: 0.540 [0.410, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.363, 10.100], loss: 0.004185, mae: 0.071027, mean_q: 0.431617
 19641/100000: episode: 283, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 7.600, mean reward: 0.380 [0.240, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.949, 10.486], loss: 0.004497, mae: 0.073776, mean_q: 0.445273
 19663/100000: episode: 284, duration: 0.110s, episode steps: 22, steps per second: 199, episode reward: 10.188, mean reward: 0.463 [0.378, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.499, 10.503], loss: 0.004540, mae: 0.074653, mean_q: 0.439501
 19686/100000: episode: 285, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 5.978, mean reward: 0.260 [0.120, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.264], loss: 0.004335, mae: 0.071552, mean_q: 0.472654
 19713/100000: episode: 286, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 5.076, mean reward: 0.188 [0.067, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.035, 10.110], loss: 0.004536, mae: 0.073694, mean_q: 0.488103
 19733/100000: episode: 287, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 8.751, mean reward: 0.438 [0.294, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.442, 10.562], loss: 0.004333, mae: 0.071472, mean_q: 0.490862
 19755/100000: episode: 288, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 9.435, mean reward: 0.429 [0.319, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.310, 10.399], loss: 0.004480, mae: 0.073670, mean_q: 0.485718
 19777/100000: episode: 289, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 7.915, mean reward: 0.360 [0.272, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.470], loss: 0.003784, mae: 0.067587, mean_q: 0.468638
 19798/100000: episode: 290, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 9.792, mean reward: 0.466 [0.348, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.856, 10.577], loss: 0.004173, mae: 0.070684, mean_q: 0.493739
 19823/100000: episode: 291, duration: 0.118s, episode steps: 25, steps per second: 213, episode reward: 8.147, mean reward: 0.326 [0.230, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.405, 10.406], loss: 0.004324, mae: 0.071383, mean_q: 0.494671
 19850/100000: episode: 292, duration: 0.144s, episode steps: 27, steps per second: 188, episode reward: 7.103, mean reward: 0.263 [0.008, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.446, 10.254], loss: 0.004277, mae: 0.072084, mean_q: 0.490522
 19871/100000: episode: 293, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 8.258, mean reward: 0.393 [0.261, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.512], loss: 0.005510, mae: 0.080484, mean_q: 0.478095
 19892/100000: episode: 294, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 9.641, mean reward: 0.459 [0.384, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.632, 10.551], loss: 0.004169, mae: 0.072240, mean_q: 0.496485
 19913/100000: episode: 295, duration: 0.103s, episode steps: 21, steps per second: 205, episode reward: 10.738, mean reward: 0.511 [0.398, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.430, 10.625], loss: 0.003910, mae: 0.069232, mean_q: 0.509394
 19940/100000: episode: 296, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 7.213, mean reward: 0.267 [0.142, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.286, 10.296], loss: 0.004754, mae: 0.077547, mean_q: 0.502760
 19958/100000: episode: 297, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 8.960, mean reward: 0.498 [0.393, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.300, 10.100], loss: 0.004380, mae: 0.073375, mean_q: 0.505204
 19985/100000: episode: 298, duration: 0.152s, episode steps: 27, steps per second: 177, episode reward: 7.178, mean reward: 0.266 [0.120, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.966, 10.197], loss: 0.004872, mae: 0.077917, mean_q: 0.507989
 20016/100000: episode: 299, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 11.396, mean reward: 0.368 [0.222, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.515, 10.381], loss: 0.004241, mae: 0.071539, mean_q: 0.501824
 20047/100000: episode: 300, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 13.530, mean reward: 0.436 [0.348, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.176, 10.667], loss: 0.004230, mae: 0.072594, mean_q: 0.506634
 20069/100000: episode: 301, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 5.761, mean reward: 0.262 [0.204, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.152, 10.316], loss: 0.004453, mae: 0.073818, mean_q: 0.516696
 20094/100000: episode: 302, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 8.889, mean reward: 0.356 [0.265, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.362, 10.453], loss: 0.004385, mae: 0.073677, mean_q: 0.509242
 20116/100000: episode: 303, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 7.524, mean reward: 0.342 [0.265, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.488, 10.401], loss: 0.003749, mae: 0.067842, mean_q: 0.507225
 20137/100000: episode: 304, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 7.938, mean reward: 0.378 [0.172, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.442, 10.274], loss: 0.004207, mae: 0.071360, mean_q: 0.529440
 20165/100000: episode: 305, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 7.799, mean reward: 0.279 [0.213, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-1.008, 10.397], loss: 0.004303, mae: 0.073836, mean_q: 0.511626
 20190/100000: episode: 306, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 10.018, mean reward: 0.401 [0.197, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.388], loss: 0.005624, mae: 0.083608, mean_q: 0.513310
 20211/100000: episode: 307, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 9.491, mean reward: 0.452 [0.340, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.062, 10.601], loss: 0.004363, mae: 0.072216, mean_q: 0.520534
 20239/100000: episode: 308, duration: 0.139s, episode steps: 28, steps per second: 202, episode reward: 8.087, mean reward: 0.289 [0.150, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.450, 10.338], loss: 0.004314, mae: 0.073529, mean_q: 0.515767
 20257/100000: episode: 309, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 8.658, mean reward: 0.481 [0.431, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.297, 10.100], loss: 0.004598, mae: 0.074139, mean_q: 0.509911
 20282/100000: episode: 310, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 7.799, mean reward: 0.312 [0.124, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.053, 10.253], loss: 0.004872, mae: 0.078955, mean_q: 0.512302
 20302/100000: episode: 311, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 8.946, mean reward: 0.447 [0.341, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.298, 10.629], loss: 0.004661, mae: 0.073676, mean_q: 0.523334
 20327/100000: episode: 312, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 9.223, mean reward: 0.369 [0.239, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.106, 10.412], loss: 0.005018, mae: 0.078223, mean_q: 0.517525
 20347/100000: episode: 313, duration: 0.113s, episode steps: 20, steps per second: 178, episode reward: 9.220, mean reward: 0.461 [0.303, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.541], loss: 0.004145, mae: 0.072645, mean_q: 0.508423
 20375/100000: episode: 314, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 10.193, mean reward: 0.364 [0.229, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.512, 10.429], loss: 0.004910, mae: 0.078865, mean_q: 0.522821
 20406/100000: episode: 315, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 9.287, mean reward: 0.300 [0.138, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.247, 10.369], loss: 0.004179, mae: 0.071871, mean_q: 0.515806
 20428/100000: episode: 316, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 8.428, mean reward: 0.383 [0.300, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.403], loss: 0.004580, mae: 0.074992, mean_q: 0.521289
 20451/100000: episode: 317, duration: 0.111s, episode steps: 23, steps per second: 208, episode reward: 7.610, mean reward: 0.331 [0.237, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.436], loss: 0.003945, mae: 0.067912, mean_q: 0.515178
 20469/100000: episode: 318, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 8.088, mean reward: 0.449 [0.323, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.343, 10.100], loss: 0.005148, mae: 0.081440, mean_q: 0.536367
 20494/100000: episode: 319, duration: 0.120s, episode steps: 25, steps per second: 208, episode reward: 8.258, mean reward: 0.330 [0.254, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.865, 10.408], loss: 0.004011, mae: 0.068726, mean_q: 0.519792
 20516/100000: episode: 320, duration: 0.107s, episode steps: 22, steps per second: 205, episode reward: 7.564, mean reward: 0.344 [0.281, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.271, 10.400], loss: 0.004174, mae: 0.070607, mean_q: 0.530627
 20541/100000: episode: 321, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 8.940, mean reward: 0.358 [0.207, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.251, 10.339], loss: 0.003893, mae: 0.067901, mean_q: 0.529917
 20564/100000: episode: 322, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 6.765, mean reward: 0.294 [0.102, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.287], loss: 0.004272, mae: 0.070524, mean_q: 0.536108
 20595/100000: episode: 323, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 7.297, mean reward: 0.235 [0.058, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.467, 10.221], loss: 0.004451, mae: 0.073152, mean_q: 0.528556
 20616/100000: episode: 324, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 8.687, mean reward: 0.414 [0.317, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.477], loss: 0.004033, mae: 0.069147, mean_q: 0.534387
 20643/100000: episode: 325, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 6.956, mean reward: 0.258 [0.047, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.100], loss: 0.003803, mae: 0.068245, mean_q: 0.533560
 20674/100000: episode: 326, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 11.073, mean reward: 0.357 [0.164, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.035, 10.390], loss: 0.004143, mae: 0.071163, mean_q: 0.547129
 20697/100000: episode: 327, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 8.933, mean reward: 0.388 [0.311, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.886, 10.501], loss: 0.004104, mae: 0.070057, mean_q: 0.556118
 20718/100000: episode: 328, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 7.632, mean reward: 0.363 [0.248, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.371], loss: 0.004765, mae: 0.077344, mean_q: 0.529967
 20743/100000: episode: 329, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 7.934, mean reward: 0.317 [0.206, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.062, 10.385], loss: 0.003958, mae: 0.067835, mean_q: 0.543016
[Info] 300-TH LEVEL FOUND: 0.9634602665901184, Considering 10/90 traces
 20770/100000: episode: 330, duration: 4.003s, episode steps: 27, steps per second: 7, episode reward: 9.447, mean reward: 0.350 [0.229, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.939, 10.441], loss: 0.004481, mae: 0.074820, mean_q: 0.530135
 20796/100000: episode: 331, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 10.125, mean reward: 0.389 [0.261, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-1.127, 10.100], loss: 0.005464, mae: 0.082603, mean_q: 0.539731
 20822/100000: episode: 332, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 11.214, mean reward: 0.431 [0.271, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.233, 10.100], loss: 0.004213, mae: 0.071367, mean_q: 0.550750
 20848/100000: episode: 333, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 9.070, mean reward: 0.349 [0.055, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.764, 10.100], loss: 0.004736, mae: 0.076287, mean_q: 0.542398
 20874/100000: episode: 334, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 7.205, mean reward: 0.277 [0.101, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.100], loss: 0.005233, mae: 0.080254, mean_q: 0.555789
 20900/100000: episode: 335, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 9.594, mean reward: 0.369 [0.297, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.519, 10.100], loss: 0.004430, mae: 0.073609, mean_q: 0.536561
 20926/100000: episode: 336, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 9.765, mean reward: 0.376 [0.218, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.035, 10.100], loss: 0.003929, mae: 0.068289, mean_q: 0.551381
 20952/100000: episode: 337, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 9.549, mean reward: 0.367 [0.242, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.230, 10.100], loss: 0.004312, mae: 0.071569, mean_q: 0.553025
 20978/100000: episode: 338, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 8.560, mean reward: 0.329 [0.229, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.157, 10.100], loss: 0.004528, mae: 0.073146, mean_q: 0.556084
 21004/100000: episode: 339, duration: 0.139s, episode steps: 26, steps per second: 188, episode reward: 11.482, mean reward: 0.442 [0.362, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.026, 10.100], loss: 0.004608, mae: 0.075130, mean_q: 0.549408
 21012/100000: episode: 340, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 4.086, mean reward: 0.511 [0.460, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.164, 10.608], loss: 0.003865, mae: 0.070125, mean_q: 0.539226
 21020/100000: episode: 341, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 4.320, mean reward: 0.540 [0.458, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.658], loss: 0.004088, mae: 0.069682, mean_q: 0.592472
 21046/100000: episode: 342, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 10.836, mean reward: 0.417 [0.321, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.212, 10.100], loss: 0.006330, mae: 0.088015, mean_q: 0.549533
 21072/100000: episode: 343, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 10.266, mean reward: 0.395 [0.285, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.255, 10.100], loss: 0.004368, mae: 0.072645, mean_q: 0.550179
 21098/100000: episode: 344, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 10.595, mean reward: 0.407 [0.174, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.372, 10.100], loss: 0.004051, mae: 0.070559, mean_q: 0.562128
 21124/100000: episode: 345, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 11.564, mean reward: 0.445 [0.259, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.310, 10.100], loss: 0.003740, mae: 0.067438, mean_q: 0.564519
[Info] FALSIFICATION!
 21131/100000: episode: 346, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 13.120, mean reward: 1.874 [0.456, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.011, 10.798], loss: 0.002986, mae: 0.060854, mean_q: 0.563104
 21231/100000: episode: 347, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.288, mean reward: -0.163 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.922, 10.098], loss: 0.018671, mae: 0.080657, mean_q: 0.551125
 21331/100000: episode: 348, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.845, mean reward: -0.168 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.024, 10.098], loss: 0.018933, mae: 0.087628, mean_q: 0.536242
 21431/100000: episode: 349, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -14.919, mean reward: -0.149 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.429, 10.176], loss: 0.017825, mae: 0.081020, mean_q: 0.508670
 21531/100000: episode: 350, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -11.275, mean reward: -0.113 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.237, 10.350], loss: 0.018623, mae: 0.084097, mean_q: 0.516914
 21631/100000: episode: 351, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -20.546, mean reward: -0.205 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.524, 10.098], loss: 0.030711, mae: 0.086279, mean_q: 0.495751
 21731/100000: episode: 352, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -19.246, mean reward: -0.192 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.628, 10.172], loss: 0.004904, mae: 0.069091, mean_q: 0.473625
 21831/100000: episode: 353, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -16.750, mean reward: -0.168 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.159, 10.098], loss: 0.003902, mae: 0.068098, mean_q: 0.452509
 21931/100000: episode: 354, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.100, mean reward: -0.141 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.852, 10.098], loss: 0.003794, mae: 0.066725, mean_q: 0.442644
 22031/100000: episode: 355, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.006, mean reward: -0.150 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.120, 10.125], loss: 0.004190, mae: 0.069958, mean_q: 0.439728
 22131/100000: episode: 356, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -15.273, mean reward: -0.153 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.409, 10.098], loss: 0.004196, mae: 0.069663, mean_q: 0.425434
 22231/100000: episode: 357, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.177, mean reward: -0.182 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.240, 10.098], loss: 0.003990, mae: 0.066836, mean_q: 0.396890
 22331/100000: episode: 358, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.844, mean reward: -0.188 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.084, 10.152], loss: 0.043951, mae: 0.089388, mean_q: 0.377771
 22431/100000: episode: 359, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -17.896, mean reward: -0.179 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.125, 10.098], loss: 0.004454, mae: 0.069197, mean_q: 0.357853
 22531/100000: episode: 360, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -19.724, mean reward: -0.197 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.896, 10.098], loss: 0.003860, mae: 0.065832, mean_q: 0.358597
 22631/100000: episode: 361, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -18.742, mean reward: -0.187 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.777, 10.222], loss: 0.004626, mae: 0.071369, mean_q: 0.327894
 22731/100000: episode: 362, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.497, mean reward: -0.175 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.426, 10.148], loss: 0.018161, mae: 0.080414, mean_q: 0.340215
 22831/100000: episode: 363, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.668, mean reward: -0.167 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.008, 10.098], loss: 0.004293, mae: 0.069679, mean_q: 0.325889
 22931/100000: episode: 364, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -18.526, mean reward: -0.185 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.582, 10.252], loss: 0.003629, mae: 0.064100, mean_q: 0.279076
 23031/100000: episode: 365, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -13.543, mean reward: -0.135 [-1.000, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.089, 10.260], loss: 0.030040, mae: 0.078985, mean_q: 0.295168
 23131/100000: episode: 366, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.715, mean reward: -0.177 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.843, 10.341], loss: 0.043787, mae: 0.094753, mean_q: 0.280283
 23231/100000: episode: 367, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -15.080, mean reward: -0.151 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.433, 10.098], loss: 0.003546, mae: 0.062428, mean_q: 0.250751
 23331/100000: episode: 368, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.038, mean reward: -0.160 [-1.000, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.758, 10.374], loss: 0.018223, mae: 0.081873, mean_q: 0.213930
 23431/100000: episode: 369, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -12.232, mean reward: -0.122 [-1.000, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.912, 10.098], loss: 0.003878, mae: 0.063644, mean_q: 0.212612
 23531/100000: episode: 370, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.377, mean reward: -0.184 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.634, 10.218], loss: 0.017744, mae: 0.076127, mean_q: 0.219938
 23631/100000: episode: 371, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -15.933, mean reward: -0.159 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.783, 10.098], loss: 0.017866, mae: 0.077415, mean_q: 0.170400
 23731/100000: episode: 372, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.788, mean reward: -0.198 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.735, 10.247], loss: 0.017794, mae: 0.077803, mean_q: 0.158070
 23831/100000: episode: 373, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -15.213, mean reward: -0.152 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.365, 10.098], loss: 0.003295, mae: 0.059969, mean_q: 0.133309
 23931/100000: episode: 374, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -14.269, mean reward: -0.143 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.231, 10.172], loss: 0.029923, mae: 0.079195, mean_q: 0.118090
 24031/100000: episode: 375, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -18.190, mean reward: -0.182 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.483, 10.230], loss: 0.016548, mae: 0.070940, mean_q: 0.099947
 24131/100000: episode: 376, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -18.147, mean reward: -0.181 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.471, 10.141], loss: 0.003839, mae: 0.062494, mean_q: 0.085049
 24231/100000: episode: 377, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -8.087, mean reward: -0.081 [-1.000, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.394, 10.267], loss: 0.003365, mae: 0.059875, mean_q: 0.069213
 24331/100000: episode: 378, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -17.681, mean reward: -0.177 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.911, 10.314], loss: 0.017636, mae: 0.076677, mean_q: 0.038256
 24431/100000: episode: 379, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.977, mean reward: -0.170 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.021, 10.212], loss: 0.003750, mae: 0.061183, mean_q: 0.034740
 24531/100000: episode: 380, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.811, mean reward: -0.178 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.442, 10.098], loss: 0.003434, mae: 0.060346, mean_q: 0.030284
 24631/100000: episode: 381, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.491, mean reward: -0.175 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.727, 10.098], loss: 0.003978, mae: 0.062666, mean_q: 0.007988
 24731/100000: episode: 382, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.622, mean reward: -0.186 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.972, 10.285], loss: 0.003892, mae: 0.062783, mean_q: -0.048543
 24831/100000: episode: 383, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.499, mean reward: -0.165 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.057, 10.127], loss: 0.016240, mae: 0.066915, mean_q: -0.077524
 24931/100000: episode: 384, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.322, mean reward: -0.163 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.775, 10.191], loss: 0.003724, mae: 0.061416, mean_q: -0.074978
 25031/100000: episode: 385, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -9.246, mean reward: -0.092 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.783, 10.350], loss: 0.015744, mae: 0.064278, mean_q: -0.094770
 25131/100000: episode: 386, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -19.180, mean reward: -0.192 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.351, 10.251], loss: 0.017959, mae: 0.080351, mean_q: -0.099150
 25231/100000: episode: 387, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.341, mean reward: -0.143 [-1.000, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.951, 10.098], loss: 0.017384, mae: 0.075830, mean_q: -0.140997
 25331/100000: episode: 388, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.481, mean reward: -0.165 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.307, 10.285], loss: 0.003905, mae: 0.060853, mean_q: -0.147018
 25431/100000: episode: 389, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.614, mean reward: -0.166 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.559, 10.098], loss: 0.003272, mae: 0.058863, mean_q: -0.206074
 25531/100000: episode: 390, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.617, mean reward: -0.186 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.601, 10.103], loss: 0.003251, mae: 0.058170, mean_q: -0.208743
 25631/100000: episode: 391, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.080, mean reward: -0.161 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.552, 10.218], loss: 0.031323, mae: 0.082651, mean_q: -0.211846
 25731/100000: episode: 392, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -15.559, mean reward: -0.156 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.617, 10.160], loss: 0.003497, mae: 0.061650, mean_q: -0.234152
 25831/100000: episode: 393, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.474, mean reward: -0.175 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.601, 10.098], loss: 0.016864, mae: 0.070587, mean_q: -0.256532
 25931/100000: episode: 394, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -19.449, mean reward: -0.194 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.048, 10.098], loss: 0.003230, mae: 0.057783, mean_q: -0.253214
 26031/100000: episode: 395, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -11.713, mean reward: -0.117 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.779, 10.098], loss: 0.029165, mae: 0.080839, mean_q: -0.279074
 26131/100000: episode: 396, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.225, mean reward: -0.182 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.979, 10.237], loss: 0.003007, mae: 0.055350, mean_q: -0.300407
 26231/100000: episode: 397, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -12.164, mean reward: -0.122 [-1.000, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.909, 10.251], loss: 0.003008, mae: 0.055559, mean_q: -0.314348
 26331/100000: episode: 398, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -13.879, mean reward: -0.139 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.520, 10.358], loss: 0.003511, mae: 0.059255, mean_q: -0.297013
 26431/100000: episode: 399, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.517, mean reward: -0.175 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.060, 10.159], loss: 0.003101, mae: 0.056047, mean_q: -0.302617
 26531/100000: episode: 400, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -18.698, mean reward: -0.187 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.497, 10.098], loss: 0.003176, mae: 0.055329, mean_q: -0.311164
 26631/100000: episode: 401, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.641, mean reward: -0.166 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.611, 10.098], loss: 0.004220, mae: 0.062649, mean_q: -0.272787
 26731/100000: episode: 402, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -14.907, mean reward: -0.149 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.547, 10.273], loss: 0.002761, mae: 0.053201, mean_q: -0.307143
 26831/100000: episode: 403, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.374, mean reward: -0.154 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.295, 10.098], loss: 0.003850, mae: 0.056939, mean_q: -0.309173
 26931/100000: episode: 404, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.194, mean reward: -0.182 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.553, 10.167], loss: 0.005393, mae: 0.063900, mean_q: -0.286947
 27031/100000: episode: 405, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -19.420, mean reward: -0.194 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.066, 10.098], loss: 0.002907, mae: 0.054783, mean_q: -0.331176
 27131/100000: episode: 406, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.661, mean reward: -0.187 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.103, 10.291], loss: 0.002868, mae: 0.053994, mean_q: -0.322597
 27231/100000: episode: 407, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -16.739, mean reward: -0.167 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.945, 10.098], loss: 0.002832, mae: 0.053682, mean_q: -0.275554
 27331/100000: episode: 408, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.939, mean reward: -0.179 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.581, 10.290], loss: 0.002810, mae: 0.053416, mean_q: -0.306073
 27431/100000: episode: 409, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.594, mean reward: -0.156 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.625, 10.317], loss: 0.002864, mae: 0.053450, mean_q: -0.322481
 27531/100000: episode: 410, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.931, mean reward: -0.179 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.521, 10.152], loss: 0.002999, mae: 0.055966, mean_q: -0.293540
 27631/100000: episode: 411, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.467, mean reward: -0.155 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.250, 10.297], loss: 0.003532, mae: 0.058354, mean_q: -0.312621
 27731/100000: episode: 412, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.255, mean reward: -0.183 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.615, 10.169], loss: 0.003228, mae: 0.057131, mean_q: -0.327640
 27831/100000: episode: 413, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.753, mean reward: -0.168 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.775, 10.131], loss: 0.002791, mae: 0.054203, mean_q: -0.280095
 27931/100000: episode: 414, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.487, mean reward: -0.175 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.522, 10.432], loss: 0.003143, mae: 0.055877, mean_q: -0.297312
 28031/100000: episode: 415, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.915, mean reward: -0.189 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.553, 10.134], loss: 0.002983, mae: 0.055699, mean_q: -0.307696
 28131/100000: episode: 416, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: -16.430, mean reward: -0.164 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.551, 10.098], loss: 0.002700, mae: 0.052748, mean_q: -0.326671
 28231/100000: episode: 417, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.849, mean reward: -0.198 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.752, 10.098], loss: 0.003250, mae: 0.057820, mean_q: -0.323140
 28331/100000: episode: 418, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.146, mean reward: -0.171 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.185, 10.251], loss: 0.002654, mae: 0.052619, mean_q: -0.280833
 28431/100000: episode: 419, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.251, mean reward: -0.143 [-1.000, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.678, 10.098], loss: 0.002829, mae: 0.053984, mean_q: -0.308815
 28531/100000: episode: 420, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -14.293, mean reward: -0.143 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.826, 10.098], loss: 0.002744, mae: 0.054340, mean_q: -0.316082
 28631/100000: episode: 421, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.791, mean reward: -0.188 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.720, 10.098], loss: 0.002917, mae: 0.054448, mean_q: -0.296541
 28731/100000: episode: 422, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -19.027, mean reward: -0.190 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.119, 10.120], loss: 0.003017, mae: 0.056137, mean_q: -0.301725
 28831/100000: episode: 423, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -15.766, mean reward: -0.158 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.773, 10.222], loss: 0.002874, mae: 0.055528, mean_q: -0.281048
 28931/100000: episode: 424, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.242, mean reward: -0.182 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.300, 10.098], loss: 0.002826, mae: 0.053826, mean_q: -0.299687
 29031/100000: episode: 425, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.594, mean reward: -0.146 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.170, 10.098], loss: 0.004189, mae: 0.063315, mean_q: -0.299132
 29131/100000: episode: 426, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -19.605, mean reward: -0.196 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.482, 10.104], loss: 0.004208, mae: 0.062173, mean_q: -0.307242
 29231/100000: episode: 427, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.014, mean reward: -0.150 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.584, 10.098], loss: 0.002942, mae: 0.055064, mean_q: -0.309618
 29331/100000: episode: 428, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.149, mean reward: -0.181 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.017, 10.273], loss: 0.002903, mae: 0.053364, mean_q: -0.317491
 29431/100000: episode: 429, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.100, mean reward: -0.161 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.271, 10.150], loss: 0.002925, mae: 0.055252, mean_q: -0.331119
 29531/100000: episode: 430, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.786, mean reward: -0.178 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.683, 10.112], loss: 0.003050, mae: 0.055830, mean_q: -0.292070
 29631/100000: episode: 431, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.785, mean reward: -0.198 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.867, 10.098], loss: 0.002798, mae: 0.054488, mean_q: -0.293455
 29731/100000: episode: 432, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -10.793, mean reward: -0.108 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.354, 10.120], loss: 0.002727, mae: 0.053778, mean_q: -0.293127
 29831/100000: episode: 433, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.291, mean reward: -0.193 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.826, 10.098], loss: 0.003197, mae: 0.058095, mean_q: -0.308451
 29931/100000: episode: 434, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.724, mean reward: -0.157 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.118, 10.098], loss: 0.002826, mae: 0.054265, mean_q: -0.312212
 30031/100000: episode: 435, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.259, mean reward: -0.183 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.692, 10.300], loss: 0.002519, mae: 0.050923, mean_q: -0.328313
 30131/100000: episode: 436, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.533, mean reward: -0.165 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.793, 10.098], loss: 0.002980, mae: 0.055864, mean_q: -0.290202
 30231/100000: episode: 437, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -12.356, mean reward: -0.124 [-1.000, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.991, 10.580], loss: 0.002655, mae: 0.052341, mean_q: -0.325731
 30331/100000: episode: 438, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -19.208, mean reward: -0.192 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.966, 10.208], loss: 0.002662, mae: 0.052834, mean_q: -0.307407
 30431/100000: episode: 439, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -15.845, mean reward: -0.158 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.492, 10.098], loss: 0.002785, mae: 0.053784, mean_q: -0.314007
 30531/100000: episode: 440, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -12.189, mean reward: -0.122 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.210, 10.098], loss: 0.002844, mae: 0.053869, mean_q: -0.344426
 30631/100000: episode: 441, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -20.408, mean reward: -0.204 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.631, 10.135], loss: 0.002887, mae: 0.054912, mean_q: -0.301674
 30731/100000: episode: 442, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -14.939, mean reward: -0.149 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.607, 10.098], loss: 0.005155, mae: 0.063547, mean_q: -0.330870
 30831/100000: episode: 443, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.516, mean reward: -0.165 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.213, 10.288], loss: 0.003620, mae: 0.059822, mean_q: -0.334406
 30931/100000: episode: 444, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.485, mean reward: -0.145 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.728, 10.338], loss: 0.002971, mae: 0.055217, mean_q: -0.321669
 31031/100000: episode: 445, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.436, mean reward: -0.184 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.361, 10.306], loss: 0.002687, mae: 0.053312, mean_q: -0.305906
[Info] 100-TH LEVEL FOUND: 0.5842239856719971, Considering 10/90 traces
 31131/100000: episode: 446, duration: 4.312s, episode steps: 100, steps per second: 23, episode reward: -15.688, mean reward: -0.157 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.576, 10.389], loss: 0.003002, mae: 0.056549, mean_q: -0.295648
 31166/100000: episode: 447, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 9.778, mean reward: 0.279 [0.148, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.660, 10.100], loss: 0.002628, mae: 0.052958, mean_q: -0.319339
 31196/100000: episode: 448, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 6.176, mean reward: 0.206 [0.021, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.680, 10.100], loss: 0.002672, mae: 0.052390, mean_q: -0.315797
 31233/100000: episode: 449, duration: 0.190s, episode steps: 37, steps per second: 195, episode reward: 13.903, mean reward: 0.376 [0.244, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.405, 10.100], loss: 0.002775, mae: 0.053969, mean_q: -0.306967
 31260/100000: episode: 450, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 8.283, mean reward: 0.307 [0.174, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.934, 10.100], loss: 0.002799, mae: 0.053380, mean_q: -0.310751
 31292/100000: episode: 451, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 9.509, mean reward: 0.297 [0.138, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.461, 10.100], loss: 0.002723, mae: 0.053672, mean_q: -0.312775
 31329/100000: episode: 452, duration: 0.193s, episode steps: 37, steps per second: 191, episode reward: 12.453, mean reward: 0.337 [0.238, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.205, 10.100], loss: 0.002861, mae: 0.055492, mean_q: -0.275393
 31359/100000: episode: 453, duration: 0.152s, episode steps: 30, steps per second: 197, episode reward: 7.515, mean reward: 0.250 [0.086, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.325, 10.100], loss: 0.002951, mae: 0.056767, mean_q: -0.238497
 31394/100000: episode: 454, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 11.919, mean reward: 0.341 [0.206, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.414, 10.100], loss: 0.002884, mae: 0.053544, mean_q: -0.303367
 31431/100000: episode: 455, duration: 0.192s, episode steps: 37, steps per second: 193, episode reward: 10.806, mean reward: 0.292 [0.099, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.237, 10.100], loss: 0.002873, mae: 0.054999, mean_q: -0.285389
 31452/100000: episode: 456, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 4.966, mean reward: 0.236 [0.055, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.218, 10.100], loss: 0.002924, mae: 0.052842, mean_q: -0.338160
 31479/100000: episode: 457, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 5.984, mean reward: 0.222 [0.130, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.978, 10.100], loss: 0.003138, mae: 0.056253, mean_q: -0.215954
 31498/100000: episode: 458, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 6.761, mean reward: 0.356 [0.307, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.300, 10.100], loss: 0.003333, mae: 0.056256, mean_q: -0.190432
 31526/100000: episode: 459, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 8.385, mean reward: 0.299 [0.214, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.489, 10.100], loss: 0.002816, mae: 0.054536, mean_q: -0.232847
 31554/100000: episode: 460, duration: 0.152s, episode steps: 28, steps per second: 185, episode reward: 8.348, mean reward: 0.298 [0.153, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.347, 10.100], loss: 0.003500, mae: 0.057727, mean_q: -0.293720
 31581/100000: episode: 461, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 8.781, mean reward: 0.325 [0.167, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.900, 10.100], loss: 0.003555, mae: 0.060229, mean_q: -0.173185
 31613/100000: episode: 462, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 9.302, mean reward: 0.291 [0.196, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.655, 10.100], loss: 0.003513, mae: 0.058233, mean_q: -0.246048
 31643/100000: episode: 463, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 10.686, mean reward: 0.356 [0.264, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-1.085, 10.100], loss: 0.003374, mae: 0.059334, mean_q: -0.255367
 31671/100000: episode: 464, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 9.696, mean reward: 0.346 [0.200, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.633, 10.100], loss: 0.006409, mae: 0.069273, mean_q: -0.259102
 31706/100000: episode: 465, duration: 0.167s, episode steps: 35, steps per second: 210, episode reward: 8.818, mean reward: 0.252 [0.134, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.143, 10.100], loss: 0.003323, mae: 0.056305, mean_q: -0.236022
 31733/100000: episode: 466, duration: 0.139s, episode steps: 27, steps per second: 195, episode reward: 8.533, mean reward: 0.316 [0.126, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.398, 10.100], loss: 0.002549, mae: 0.052427, mean_q: -0.206922
 31765/100000: episode: 467, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 7.690, mean reward: 0.240 [0.090, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.038, 10.100], loss: 0.003272, mae: 0.058526, mean_q: -0.212300
 31800/100000: episode: 468, duration: 0.182s, episode steps: 35, steps per second: 193, episode reward: 10.466, mean reward: 0.299 [0.211, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.472, 10.100], loss: 0.003485, mae: 0.059900, mean_q: -0.184895
 31829/100000: episode: 469, duration: 0.144s, episode steps: 29, steps per second: 202, episode reward: 10.181, mean reward: 0.351 [0.194, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.351, 10.100], loss: 0.002768, mae: 0.052828, mean_q: -0.203513
 31848/100000: episode: 470, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 4.169, mean reward: 0.219 [0.036, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.157, 10.100], loss: 0.002663, mae: 0.049687, mean_q: -0.244257
 31876/100000: episode: 471, duration: 0.135s, episode steps: 28, steps per second: 207, episode reward: 5.507, mean reward: 0.197 [0.117, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.130, 10.100], loss: 0.002699, mae: 0.053394, mean_q: -0.178769
 31903/100000: episode: 472, duration: 0.140s, episode steps: 27, steps per second: 192, episode reward: 6.483, mean reward: 0.240 [0.147, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.035, 10.100], loss: 0.002810, mae: 0.054574, mean_q: -0.166285
 31924/100000: episode: 473, duration: 0.103s, episode steps: 21, steps per second: 203, episode reward: 5.347, mean reward: 0.255 [0.154, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.770, 10.100], loss: 0.003217, mae: 0.059176, mean_q: -0.076380
 31943/100000: episode: 474, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 5.471, mean reward: 0.288 [0.223, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.505, 10.100], loss: 0.003406, mae: 0.058945, mean_q: -0.135030
 31970/100000: episode: 475, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 9.947, mean reward: 0.368 [0.160, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.487, 10.100], loss: 0.002778, mae: 0.055664, mean_q: -0.131207
 31999/100000: episode: 476, duration: 0.148s, episode steps: 29, steps per second: 195, episode reward: 8.528, mean reward: 0.294 [0.116, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.235, 10.100], loss: 0.002918, mae: 0.055651, mean_q: -0.133315
 32026/100000: episode: 477, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 7.560, mean reward: 0.280 [0.190, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.647, 10.100], loss: 0.002837, mae: 0.055378, mean_q: -0.146327
 32061/100000: episode: 478, duration: 0.171s, episode steps: 35, steps per second: 204, episode reward: 10.135, mean reward: 0.290 [0.192, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-1.302, 10.100], loss: 0.003249, mae: 0.057977, mean_q: -0.094494
 32082/100000: episode: 479, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 6.106, mean reward: 0.291 [0.201, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.121, 10.100], loss: 0.002880, mae: 0.056135, mean_q: -0.128944
 32112/100000: episode: 480, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 8.175, mean reward: 0.273 [0.159, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.579, 10.100], loss: 0.002869, mae: 0.056179, mean_q: -0.088268
 32140/100000: episode: 481, duration: 0.139s, episode steps: 28, steps per second: 201, episode reward: 9.865, mean reward: 0.352 [0.195, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.516, 10.100], loss: 0.003619, mae: 0.062343, mean_q: -0.137427
 32168/100000: episode: 482, duration: 0.146s, episode steps: 28, steps per second: 191, episode reward: 6.506, mean reward: 0.232 [0.107, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.287, 10.100], loss: 0.003321, mae: 0.060764, mean_q: -0.067995
 32189/100000: episode: 483, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 5.378, mean reward: 0.256 [0.115, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.738, 10.100], loss: 0.002742, mae: 0.052867, mean_q: -0.110718
 32218/100000: episode: 484, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 7.704, mean reward: 0.266 [0.136, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.397, 10.100], loss: 0.003017, mae: 0.056434, mean_q: -0.127659
 32245/100000: episode: 485, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 9.657, mean reward: 0.358 [0.185, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.520, 10.100], loss: 0.003190, mae: 0.058246, mean_q: -0.070872
 32274/100000: episode: 486, duration: 0.147s, episode steps: 29, steps per second: 198, episode reward: 8.569, mean reward: 0.295 [0.230, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.120, 10.100], loss: 0.002966, mae: 0.054670, mean_q: -0.177173
 32311/100000: episode: 487, duration: 0.205s, episode steps: 37, steps per second: 181, episode reward: 12.128, mean reward: 0.328 [0.162, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.159, 10.100], loss: 0.002902, mae: 0.056097, mean_q: -0.066695
 32332/100000: episode: 488, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 4.163, mean reward: 0.198 [0.136, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.880, 10.100], loss: 0.003222, mae: 0.057977, mean_q: -0.108889
 32359/100000: episode: 489, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 11.928, mean reward: 0.442 [0.256, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.503, 10.100], loss: 0.002746, mae: 0.051858, mean_q: -0.137184
 32386/100000: episode: 490, duration: 0.131s, episode steps: 27, steps per second: 207, episode reward: 11.092, mean reward: 0.411 [0.205, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-1.702, 10.100], loss: 0.003398, mae: 0.057643, mean_q: -0.082092
 32414/100000: episode: 491, duration: 0.142s, episode steps: 28, steps per second: 198, episode reward: 6.423, mean reward: 0.229 [0.166, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.106, 10.100], loss: 0.002858, mae: 0.053827, mean_q: -0.081086
 32435/100000: episode: 492, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 3.893, mean reward: 0.185 [0.057, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.405, 10.100], loss: 0.002858, mae: 0.056011, mean_q: -0.089979
 32456/100000: episode: 493, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 7.173, mean reward: 0.342 [0.250, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.148, 10.100], loss: 0.003545, mae: 0.059742, mean_q: -0.102410
 32483/100000: episode: 494, duration: 0.140s, episode steps: 27, steps per second: 192, episode reward: 7.982, mean reward: 0.296 [0.147, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.112, 10.100], loss: 0.003064, mae: 0.057045, mean_q: -0.109842
 32504/100000: episode: 495, duration: 0.118s, episode steps: 21, steps per second: 179, episode reward: 5.083, mean reward: 0.242 [0.084, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.728, 10.100], loss: 0.003392, mae: 0.061340, mean_q: -0.047772
 32523/100000: episode: 496, duration: 0.090s, episode steps: 19, steps per second: 210, episode reward: 5.355, mean reward: 0.282 [0.074, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.971, 10.108], loss: 0.003289, mae: 0.059596, mean_q: -0.050022
 32550/100000: episode: 497, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 5.785, mean reward: 0.214 [0.077, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.250, 10.105], loss: 0.002927, mae: 0.055847, mean_q: -0.068678
 32578/100000: episode: 498, duration: 0.134s, episode steps: 28, steps per second: 209, episode reward: 6.701, mean reward: 0.239 [0.159, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.353, 10.100], loss: 0.003259, mae: 0.058918, mean_q: -0.069140
 32607/100000: episode: 499, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 5.714, mean reward: 0.197 [0.139, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.051, 10.100], loss: 0.002910, mae: 0.057097, mean_q: -0.001777
 32644/100000: episode: 500, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 12.531, mean reward: 0.339 [0.156, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.592, 10.100], loss: 0.003333, mae: 0.060101, mean_q: -0.055458
 32665/100000: episode: 501, duration: 0.105s, episode steps: 21, steps per second: 201, episode reward: 4.893, mean reward: 0.233 [0.158, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.094, 10.100], loss: 0.003499, mae: 0.061657, mean_q: 0.014489
 32702/100000: episode: 502, duration: 0.188s, episode steps: 37, steps per second: 197, episode reward: 9.665, mean reward: 0.261 [0.048, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.888, 10.100], loss: 0.002933, mae: 0.055654, mean_q: -0.081583
 32737/100000: episode: 503, duration: 0.179s, episode steps: 35, steps per second: 195, episode reward: 6.146, mean reward: 0.176 [0.053, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.222, 10.100], loss: 0.003197, mae: 0.058630, mean_q: -0.012416
 32756/100000: episode: 504, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 6.088, mean reward: 0.320 [0.214, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.228, 10.100], loss: 0.003280, mae: 0.060617, mean_q: -0.010127
 32788/100000: episode: 505, duration: 0.155s, episode steps: 32, steps per second: 206, episode reward: 6.649, mean reward: 0.208 [0.058, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.455, 10.100], loss: 0.003206, mae: 0.058949, mean_q: -0.095334
 32809/100000: episode: 506, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 4.029, mean reward: 0.192 [0.073, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.100], loss: 0.003245, mae: 0.058089, mean_q: 0.016992
 32830/100000: episode: 507, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 5.731, mean reward: 0.273 [0.184, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.003, 10.100], loss: 0.003017, mae: 0.055355, mean_q: -0.060806
 32865/100000: episode: 508, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 12.971, mean reward: 0.371 [0.102, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.440, 10.100], loss: 0.002920, mae: 0.056516, mean_q: -0.015347
 32894/100000: episode: 509, duration: 0.169s, episode steps: 29, steps per second: 172, episode reward: 8.847, mean reward: 0.305 [0.185, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.984, 10.100], loss: 0.003672, mae: 0.062367, mean_q: 0.035044
 32931/100000: episode: 510, duration: 0.181s, episode steps: 37, steps per second: 204, episode reward: 14.695, mean reward: 0.397 [0.136, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.545, 10.100], loss: 0.003749, mae: 0.062729, mean_q: 0.003970
 32959/100000: episode: 511, duration: 0.153s, episode steps: 28, steps per second: 184, episode reward: 7.135, mean reward: 0.255 [0.088, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.693, 10.100], loss: 0.006761, mae: 0.073134, mean_q: 0.054128
 32986/100000: episode: 512, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 7.271, mean reward: 0.269 [0.106, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.933, 10.100], loss: 0.004231, mae: 0.067553, mean_q: 0.035063
 33015/100000: episode: 513, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 6.495, mean reward: 0.224 [0.121, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.101, 10.100], loss: 0.004048, mae: 0.066442, mean_q: 0.056262
 33052/100000: episode: 514, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 11.321, mean reward: 0.306 [0.197, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.344, 10.100], loss: 0.003567, mae: 0.062638, mean_q: 0.014475
 33087/100000: episode: 515, duration: 0.205s, episode steps: 35, steps per second: 171, episode reward: 9.224, mean reward: 0.264 [0.072, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.441, 10.100], loss: 0.003482, mae: 0.061787, mean_q: -0.001395
 33114/100000: episode: 516, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 6.525, mean reward: 0.242 [0.094, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.057, 10.100], loss: 0.003400, mae: 0.062106, mean_q: 0.010405
 33149/100000: episode: 517, duration: 0.195s, episode steps: 35, steps per second: 179, episode reward: 7.174, mean reward: 0.205 [0.121, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.419, 10.100], loss: 0.003501, mae: 0.063487, mean_q: 0.087749
 33176/100000: episode: 518, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 10.588, mean reward: 0.392 [0.235, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.355, 10.100], loss: 0.003204, mae: 0.059364, mean_q: 0.067922
 33206/100000: episode: 519, duration: 0.145s, episode steps: 30, steps per second: 207, episode reward: 9.301, mean reward: 0.310 [0.193, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.171, 10.100], loss: 0.003032, mae: 0.058622, mean_q: 0.074894
 33233/100000: episode: 520, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 9.567, mean reward: 0.354 [0.202, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.919, 10.100], loss: 0.003156, mae: 0.059648, mean_q: 0.060144
 33254/100000: episode: 521, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 5.488, mean reward: 0.261 [0.181, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.752, 10.100], loss: 0.003157, mae: 0.059370, mean_q: 0.038372
 33275/100000: episode: 522, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 5.729, mean reward: 0.273 [0.170, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.152, 10.100], loss: 0.003565, mae: 0.062841, mean_q: 0.089497
 33294/100000: episode: 523, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 5.849, mean reward: 0.308 [0.212, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.055, 10.100], loss: 0.003703, mae: 0.064756, mean_q: 0.057969
 33331/100000: episode: 524, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 12.845, mean reward: 0.347 [0.184, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.845, 10.100], loss: 0.004425, mae: 0.070224, mean_q: 0.100697
 33363/100000: episode: 525, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 10.564, mean reward: 0.330 [0.218, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.055, 10.100], loss: 0.005233, mae: 0.070312, mean_q: 0.097601
 33390/100000: episode: 526, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 7.447, mean reward: 0.276 [0.131, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-1.056, 10.100], loss: 0.011008, mae: 0.080720, mean_q: 0.116042
 33417/100000: episode: 527, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 8.159, mean reward: 0.302 [0.189, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.402, 10.100], loss: 0.005614, mae: 0.072446, mean_q: 0.071057
 33449/100000: episode: 528, duration: 0.171s, episode steps: 32, steps per second: 188, episode reward: 12.218, mean reward: 0.382 [0.220, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.617, 10.100], loss: 0.004068, mae: 0.063798, mean_q: 0.082874
 33479/100000: episode: 529, duration: 0.151s, episode steps: 30, steps per second: 199, episode reward: 8.565, mean reward: 0.285 [0.214, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.508, 10.100], loss: 0.003909, mae: 0.066847, mean_q: 0.124868
 33506/100000: episode: 530, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 6.599, mean reward: 0.244 [0.122, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.142, 10.100], loss: 0.003963, mae: 0.066280, mean_q: 0.109098
 33525/100000: episode: 531, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 5.347, mean reward: 0.281 [0.136, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.831, 10.100], loss: 0.003416, mae: 0.060808, mean_q: 0.099594
 33555/100000: episode: 532, duration: 0.149s, episode steps: 30, steps per second: 202, episode reward: 7.384, mean reward: 0.246 [0.133, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.225, 10.100], loss: 0.003472, mae: 0.061292, mean_q: 0.137283
 33587/100000: episode: 533, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 7.368, mean reward: 0.230 [0.100, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.774, 10.100], loss: 0.003444, mae: 0.063841, mean_q: 0.133146
 33619/100000: episode: 534, duration: 0.183s, episode steps: 32, steps per second: 175, episode reward: 9.898, mean reward: 0.309 [0.135, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.354, 10.100], loss: 0.003909, mae: 0.062191, mean_q: 0.113516
 33640/100000: episode: 535, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 4.103, mean reward: 0.195 [0.126, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.525, 10.100], loss: 0.003670, mae: 0.062173, mean_q: 0.092311
[Info] 200-TH LEVEL FOUND: 0.7493202686309814, Considering 10/90 traces
 33668/100000: episode: 536, duration: 4.031s, episode steps: 28, steps per second: 7, episode reward: 7.547, mean reward: 0.270 [0.153, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.910, 10.100], loss: 0.003179, mae: 0.060488, mean_q: 0.144063
 33682/100000: episode: 537, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 5.371, mean reward: 0.384 [0.335, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.503, 10.100], loss: 0.003413, mae: 0.060927, mean_q: 0.126242
 33697/100000: episode: 538, duration: 0.076s, episode steps: 15, steps per second: 199, episode reward: 6.215, mean reward: 0.414 [0.358, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.390, 10.100], loss: 0.003956, mae: 0.066191, mean_q: 0.126272
 33709/100000: episode: 539, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 4.457, mean reward: 0.371 [0.293, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.682, 10.100], loss: 0.003938, mae: 0.061076, mean_q: 0.153355
 33732/100000: episode: 540, duration: 0.113s, episode steps: 23, steps per second: 204, episode reward: 11.093, mean reward: 0.482 [0.303, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.359, 10.100], loss: 0.003748, mae: 0.064058, mean_q: 0.143933
 33754/100000: episode: 541, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 10.093, mean reward: 0.459 [0.304, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.533, 10.100], loss: 0.003189, mae: 0.060398, mean_q: 0.157353
 33767/100000: episode: 542, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 4.940, mean reward: 0.380 [0.276, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.277, 10.100], loss: 0.003505, mae: 0.064652, mean_q: 0.209883
 33782/100000: episode: 543, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 5.036, mean reward: 0.336 [0.242, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.168, 10.100], loss: 0.003579, mae: 0.063611, mean_q: 0.133016
 33796/100000: episode: 544, duration: 0.072s, episode steps: 14, steps per second: 193, episode reward: 6.485, mean reward: 0.463 [0.367, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.466, 10.100], loss: 0.002968, mae: 0.058213, mean_q: 0.198310
 33818/100000: episode: 545, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 9.598, mean reward: 0.436 [0.341, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.409, 10.100], loss: 0.003621, mae: 0.062053, mean_q: 0.180700
 33832/100000: episode: 546, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 5.583, mean reward: 0.399 [0.325, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.427, 10.100], loss: 0.003196, mae: 0.059209, mean_q: 0.158022
 33844/100000: episode: 547, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 5.743, mean reward: 0.479 [0.390, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.338, 10.100], loss: 0.003371, mae: 0.059288, mean_q: 0.186790
 33867/100000: episode: 548, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 11.810, mean reward: 0.513 [0.369, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.504, 10.100], loss: 0.003529, mae: 0.065093, mean_q: 0.189806
 33882/100000: episode: 549, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 5.845, mean reward: 0.390 [0.218, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.270, 10.100], loss: 0.003898, mae: 0.067033, mean_q: 0.216100
 33901/100000: episode: 550, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 9.766, mean reward: 0.514 [0.366, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.533, 10.100], loss: 0.004496, mae: 0.070458, mean_q: 0.224706
 33920/100000: episode: 551, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 7.941, mean reward: 0.418 [0.282, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.278, 10.100], loss: 0.003643, mae: 0.062318, mean_q: 0.180317
 33936/100000: episode: 552, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 6.310, mean reward: 0.394 [0.313, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.199, 10.100], loss: 0.003270, mae: 0.061759, mean_q: 0.186766
 33958/100000: episode: 553, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 8.852, mean reward: 0.402 [0.321, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.407, 10.100], loss: 0.003849, mae: 0.064435, mean_q: 0.262452
 33974/100000: episode: 554, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 7.814, mean reward: 0.488 [0.447, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.305, 10.100], loss: 0.003678, mae: 0.060522, mean_q: 0.197756
 33986/100000: episode: 555, duration: 0.058s, episode steps: 12, steps per second: 205, episode reward: 5.661, mean reward: 0.472 [0.431, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.289, 10.100], loss: 0.002711, mae: 0.054470, mean_q: 0.268605
 34000/100000: episode: 556, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 5.421, mean reward: 0.387 [0.309, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.937, 10.100], loss: 0.003531, mae: 0.062589, mean_q: 0.237728
 34023/100000: episode: 557, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 9.451, mean reward: 0.411 [0.346, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.236, 10.100], loss: 0.003582, mae: 0.062702, mean_q: 0.258467
 34046/100000: episode: 558, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 8.552, mean reward: 0.372 [0.272, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.405, 10.100], loss: 0.003169, mae: 0.059898, mean_q: 0.191292
 34068/100000: episode: 559, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 11.164, mean reward: 0.507 [0.401, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.355, 10.100], loss: 0.003337, mae: 0.060625, mean_q: 0.223203
 34096/100000: episode: 560, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 11.601, mean reward: 0.414 [0.258, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.914, 10.100], loss: 0.003309, mae: 0.062424, mean_q: 0.231020
 34111/100000: episode: 561, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 5.292, mean reward: 0.353 [0.295, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.310, 10.100], loss: 0.004545, mae: 0.073426, mean_q: 0.279842
 34133/100000: episode: 562, duration: 0.133s, episode steps: 22, steps per second: 165, episode reward: 8.204, mean reward: 0.373 [0.269, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.413, 10.100], loss: 0.003607, mae: 0.063705, mean_q: 0.313000
 34146/100000: episode: 563, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 5.414, mean reward: 0.416 [0.358, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.426, 10.100], loss: 0.003281, mae: 0.059309, mean_q: 0.231149
 34160/100000: episode: 564, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 5.191, mean reward: 0.371 [0.286, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.238, 10.100], loss: 0.003753, mae: 0.067447, mean_q: 0.293890
 34181/100000: episode: 565, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 8.061, mean reward: 0.384 [0.297, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.395, 10.100], loss: 0.003378, mae: 0.062986, mean_q: 0.247511
 34204/100000: episode: 566, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 9.075, mean reward: 0.395 [0.282, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.780, 10.100], loss: 0.003712, mae: 0.063278, mean_q: 0.241390
 34223/100000: episode: 567, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 6.274, mean reward: 0.330 [0.215, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.368, 10.100], loss: 0.003782, mae: 0.065548, mean_q: 0.227228
 34235/100000: episode: 568, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 5.534, mean reward: 0.461 [0.407, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.364, 10.100], loss: 0.003436, mae: 0.061864, mean_q: 0.274018
 34258/100000: episode: 569, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 9.886, mean reward: 0.430 [0.281, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.803, 10.100], loss: 0.003936, mae: 0.066906, mean_q: 0.314752
 34280/100000: episode: 570, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 8.240, mean reward: 0.375 [0.274, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.277, 10.100], loss: 0.006489, mae: 0.077411, mean_q: 0.300099
 34299/100000: episode: 571, duration: 0.112s, episode steps: 19, steps per second: 169, episode reward: 7.794, mean reward: 0.410 [0.265, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.541, 10.100], loss: 0.004565, mae: 0.070848, mean_q: 0.219403
 34318/100000: episode: 572, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 9.855, mean reward: 0.519 [0.326, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.509, 10.100], loss: 0.004550, mae: 0.068421, mean_q: 0.277075
 34346/100000: episode: 573, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 9.160, mean reward: 0.327 [0.237, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.330, 10.100], loss: 0.003733, mae: 0.065445, mean_q: 0.268437
 34374/100000: episode: 574, duration: 0.131s, episode steps: 28, steps per second: 214, episode reward: 10.263, mean reward: 0.367 [0.119, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.989, 10.100], loss: 0.003497, mae: 0.061782, mean_q: 0.260451
 34395/100000: episode: 575, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 10.509, mean reward: 0.500 [0.389, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.480, 10.100], loss: 0.003826, mae: 0.066110, mean_q: 0.315949
 34411/100000: episode: 576, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 5.576, mean reward: 0.349 [0.242, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.833, 10.100], loss: 0.003852, mae: 0.066078, mean_q: 0.354012
 34433/100000: episode: 577, duration: 0.111s, episode steps: 22, steps per second: 199, episode reward: 9.708, mean reward: 0.441 [0.366, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.468, 10.100], loss: 0.003557, mae: 0.062220, mean_q: 0.317509
 34455/100000: episode: 578, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 8.471, mean reward: 0.385 [0.319, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.309, 10.100], loss: 0.003734, mae: 0.065736, mean_q: 0.333158
 34476/100000: episode: 579, duration: 0.104s, episode steps: 21, steps per second: 201, episode reward: 7.465, mean reward: 0.355 [0.233, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.502, 10.100], loss: 0.003867, mae: 0.065351, mean_q: 0.283021
 34495/100000: episode: 580, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 7.492, mean reward: 0.394 [0.285, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.433, 10.100], loss: 0.004434, mae: 0.073407, mean_q: 0.377688
 34523/100000: episode: 581, duration: 0.137s, episode steps: 28, steps per second: 204, episode reward: 14.257, mean reward: 0.509 [0.419, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.628, 10.100], loss: 0.004007, mae: 0.065356, mean_q: 0.338415
 34542/100000: episode: 582, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 6.732, mean reward: 0.354 [0.291, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.281, 10.100], loss: 0.003186, mae: 0.060493, mean_q: 0.360800
 34570/100000: episode: 583, duration: 0.143s, episode steps: 28, steps per second: 195, episode reward: 11.099, mean reward: 0.396 [0.285, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.500, 10.100], loss: 0.003742, mae: 0.065011, mean_q: 0.382965
 34592/100000: episode: 584, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 9.785, mean reward: 0.445 [0.298, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.550, 10.100], loss: 0.003311, mae: 0.061080, mean_q: 0.292949
 34611/100000: episode: 585, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 8.062, mean reward: 0.424 [0.352, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.379, 10.100], loss: 0.004116, mae: 0.065540, mean_q: 0.376049
 34634/100000: episode: 586, duration: 0.108s, episode steps: 23, steps per second: 213, episode reward: 9.936, mean reward: 0.432 [0.334, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.157, 10.100], loss: 0.003697, mae: 0.066194, mean_q: 0.335045
 34655/100000: episode: 587, duration: 0.100s, episode steps: 21, steps per second: 210, episode reward: 7.055, mean reward: 0.336 [0.209, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.252, 10.100], loss: 0.003572, mae: 0.064811, mean_q: 0.395904
 34678/100000: episode: 588, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 9.424, mean reward: 0.410 [0.362, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.204, 10.100], loss: 0.003641, mae: 0.063787, mean_q: 0.400775
 34693/100000: episode: 589, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 6.702, mean reward: 0.447 [0.349, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.458, 10.100], loss: 0.003693, mae: 0.063186, mean_q: 0.407942
 34721/100000: episode: 590, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 12.751, mean reward: 0.455 [0.310, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-1.050, 10.100], loss: 0.003864, mae: 0.066737, mean_q: 0.377174
 34737/100000: episode: 591, duration: 0.083s, episode steps: 16, steps per second: 194, episode reward: 6.329, mean reward: 0.396 [0.268, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.393, 10.100], loss: 0.003352, mae: 0.059587, mean_q: 0.367643
 34758/100000: episode: 592, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 7.510, mean reward: 0.358 [0.211, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.340, 10.100], loss: 0.003487, mae: 0.061908, mean_q: 0.419785
 34777/100000: episode: 593, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 9.052, mean reward: 0.476 [0.254, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.306, 10.100], loss: 0.003644, mae: 0.064859, mean_q: 0.377555
 34790/100000: episode: 594, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 5.699, mean reward: 0.438 [0.369, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.697, 10.100], loss: 0.004271, mae: 0.066569, mean_q: 0.409209
 34802/100000: episode: 595, duration: 0.062s, episode steps: 12, steps per second: 192, episode reward: 5.319, mean reward: 0.443 [0.392, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.415, 10.100], loss: 0.003126, mae: 0.062017, mean_q: 0.501028
 34824/100000: episode: 596, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 9.209, mean reward: 0.419 [0.214, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.149, 10.100], loss: 0.004341, mae: 0.070098, mean_q: 0.425118
 34846/100000: episode: 597, duration: 0.111s, episode steps: 22, steps per second: 199, episode reward: 9.974, mean reward: 0.453 [0.368, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.249, 10.100], loss: 0.003691, mae: 0.064844, mean_q: 0.420582
 34865/100000: episode: 598, duration: 0.097s, episode steps: 19, steps per second: 197, episode reward: 6.520, mean reward: 0.343 [0.232, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.669, 10.100], loss: 0.004121, mae: 0.066136, mean_q: 0.410166
 34893/100000: episode: 599, duration: 0.135s, episode steps: 28, steps per second: 207, episode reward: 10.600, mean reward: 0.379 [0.222, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.315, 10.100], loss: 0.004645, mae: 0.067796, mean_q: 0.438239
 34921/100000: episode: 600, duration: 0.136s, episode steps: 28, steps per second: 206, episode reward: 14.705, mean reward: 0.525 [0.416, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.668, 10.100], loss: 0.003612, mae: 0.064374, mean_q: 0.440430
 34943/100000: episode: 601, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 8.340, mean reward: 0.379 [0.270, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.099, 10.100], loss: 0.003312, mae: 0.060448, mean_q: 0.452473
 34956/100000: episode: 602, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 3.981, mean reward: 0.306 [0.220, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.347, 10.100], loss: 0.003736, mae: 0.063912, mean_q: 0.437640
 34975/100000: episode: 603, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 8.133, mean reward: 0.428 [0.287, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.660, 10.100], loss: 0.003595, mae: 0.063635, mean_q: 0.412083
 34990/100000: episode: 604, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 5.076, mean reward: 0.338 [0.270, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.381, 10.100], loss: 0.003587, mae: 0.061854, mean_q: 0.411263
 35005/100000: episode: 605, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 6.612, mean reward: 0.441 [0.395, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.630, 10.100], loss: 0.003834, mae: 0.065794, mean_q: 0.408482
 35033/100000: episode: 606, duration: 0.149s, episode steps: 28, steps per second: 187, episode reward: 11.811, mean reward: 0.422 [0.320, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.404, 10.100], loss: 0.003862, mae: 0.066877, mean_q: 0.474142
 35055/100000: episode: 607, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 8.263, mean reward: 0.376 [0.317, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.255, 10.100], loss: 0.004380, mae: 0.070837, mean_q: 0.479634
 35077/100000: episode: 608, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 8.628, mean reward: 0.392 [0.295, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.527, 10.100], loss: 0.004480, mae: 0.073036, mean_q: 0.514857
 35096/100000: episode: 609, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 7.306, mean reward: 0.385 [0.279, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.081, 10.100], loss: 0.003997, mae: 0.067675, mean_q: 0.504794
 35109/100000: episode: 610, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 5.220, mean reward: 0.402 [0.293, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.372, 10.100], loss: 0.003541, mae: 0.064812, mean_q: 0.425825
 35125/100000: episode: 611, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 6.333, mean reward: 0.396 [0.285, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.440, 10.100], loss: 0.004005, mae: 0.067106, mean_q: 0.492057
 35148/100000: episode: 612, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 11.355, mean reward: 0.494 [0.373, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.517, 10.100], loss: 0.003782, mae: 0.064706, mean_q: 0.488643
 35167/100000: episode: 613, duration: 0.094s, episode steps: 19, steps per second: 202, episode reward: 7.682, mean reward: 0.404 [0.338, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.250, 10.100], loss: 0.004557, mae: 0.072626, mean_q: 0.501528
 35186/100000: episode: 614, duration: 0.102s, episode steps: 19, steps per second: 185, episode reward: 7.109, mean reward: 0.374 [0.276, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.814, 10.100], loss: 0.004148, mae: 0.071779, mean_q: 0.538054
 35199/100000: episode: 615, duration: 0.063s, episode steps: 13, steps per second: 206, episode reward: 6.414, mean reward: 0.493 [0.390, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.563, 10.100], loss: 0.004393, mae: 0.075053, mean_q: 0.500729
 35227/100000: episode: 616, duration: 0.141s, episode steps: 28, steps per second: 198, episode reward: 10.984, mean reward: 0.392 [0.209, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.366, 10.100], loss: 0.004067, mae: 0.065735, mean_q: 0.496604
 35243/100000: episode: 617, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 6.092, mean reward: 0.381 [0.259, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.195, 10.100], loss: 0.003461, mae: 0.063980, mean_q: 0.504223
 35256/100000: episode: 618, duration: 0.063s, episode steps: 13, steps per second: 206, episode reward: 5.172, mean reward: 0.398 [0.320, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.488, 10.100], loss: 0.003794, mae: 0.063507, mean_q: 0.537894
 35268/100000: episode: 619, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 4.569, mean reward: 0.381 [0.342, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.322, 10.100], loss: 0.003344, mae: 0.062446, mean_q: 0.542815
 35282/100000: episode: 620, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 5.097, mean reward: 0.364 [0.293, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.483, 10.100], loss: 0.003646, mae: 0.067709, mean_q: 0.582344
 35303/100000: episode: 621, duration: 0.110s, episode steps: 21, steps per second: 190, episode reward: 8.113, mean reward: 0.386 [0.277, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.859, 10.100], loss: 0.004104, mae: 0.069042, mean_q: 0.520131
 35315/100000: episode: 622, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 4.764, mean reward: 0.397 [0.333, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.284, 10.100], loss: 0.004238, mae: 0.071263, mean_q: 0.548361
 35328/100000: episode: 623, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 5.867, mean reward: 0.451 [0.383, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.492, 10.100], loss: 0.004559, mae: 0.074776, mean_q: 0.502370
 35349/100000: episode: 624, duration: 0.103s, episode steps: 21, steps per second: 205, episode reward: 8.273, mean reward: 0.394 [0.286, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.242, 10.100], loss: 0.004170, mae: 0.071088, mean_q: 0.525839
 35362/100000: episode: 625, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 6.256, mean reward: 0.481 [0.365, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.313, 10.100], loss: 0.003449, mae: 0.063256, mean_q: 0.539666
[Info] 300-TH LEVEL FOUND: 0.9325236082077026, Considering 10/90 traces
 35377/100000: episode: 626, duration: 3.899s, episode steps: 15, steps per second: 4, episode reward: 6.118, mean reward: 0.408 [0.289, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.480, 10.100], loss: 0.003999, mae: 0.066746, mean_q: 0.546509
 35396/100000: episode: 627, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 9.590, mean reward: 0.505 [0.435, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.332, 10.100], loss: 0.004116, mae: 0.068670, mean_q: 0.538869
 35417/100000: episode: 628, duration: 0.099s, episode steps: 21, steps per second: 212, episode reward: 9.478, mean reward: 0.451 [0.402, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.047, 10.100], loss: 0.003873, mae: 0.066231, mean_q: 0.537290
 35429/100000: episode: 629, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 6.571, mean reward: 0.548 [0.504, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.464, 10.100], loss: 0.003575, mae: 0.065707, mean_q: 0.563350
 35450/100000: episode: 630, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 8.866, mean reward: 0.422 [0.351, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.417, 10.100], loss: 0.004788, mae: 0.074359, mean_q: 0.527518
[Info] FALSIFICATION!
 35452/100000: episode: 631, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 10.589, mean reward: 5.294 [0.589, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.209, 9.864], loss: 0.004685, mae: 0.071219, mean_q: 0.451855
 35552/100000: episode: 632, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -19.027, mean reward: -0.190 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.484, 10.098], loss: 0.003755, mae: 0.065951, mean_q: 0.566235
 35652/100000: episode: 633, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.301, mean reward: -0.163 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.396, 10.163], loss: 0.029892, mae: 0.077075, mean_q: 0.548143
 35752/100000: episode: 634, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.813, mean reward: -0.188 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.753, 10.166], loss: 0.018681, mae: 0.083452, mean_q: 0.576055
 35852/100000: episode: 635, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -10.346, mean reward: -0.103 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.788, 10.098], loss: 0.017835, mae: 0.075049, mean_q: 0.566782
 35952/100000: episode: 636, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.112, mean reward: -0.191 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.552, 10.098], loss: 0.004266, mae: 0.067273, mean_q: 0.555057
 36052/100000: episode: 637, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -18.351, mean reward: -0.184 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.704, 10.210], loss: 0.043307, mae: 0.090261, mean_q: 0.549541
 36152/100000: episode: 638, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.802, mean reward: -0.178 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.616, 10.132], loss: 0.003832, mae: 0.065610, mean_q: 0.514849
 36252/100000: episode: 639, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.281, mean reward: -0.183 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.513, 10.098], loss: 0.017675, mae: 0.078616, mean_q: 0.522383
 36352/100000: episode: 640, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.436, mean reward: -0.184 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.750, 10.098], loss: 0.005523, mae: 0.070093, mean_q: 0.473889
 36452/100000: episode: 641, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.842, mean reward: -0.168 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.980, 10.098], loss: 0.003926, mae: 0.066164, mean_q: 0.460491
 36552/100000: episode: 642, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.560, mean reward: -0.166 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.815, 10.202], loss: 0.004146, mae: 0.064918, mean_q: 0.461325
 36652/100000: episode: 643, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.389, mean reward: -0.184 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.709, 10.098], loss: 0.018269, mae: 0.080160, mean_q: 0.429921
 36752/100000: episode: 644, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.541, mean reward: -0.145 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.108, 10.287], loss: 0.004494, mae: 0.066516, mean_q: 0.424444
 36852/100000: episode: 645, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.116, mean reward: -0.171 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.621, 10.174], loss: 0.016825, mae: 0.072312, mean_q: 0.405496
 36952/100000: episode: 646, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -19.732, mean reward: -0.197 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.603, 10.166], loss: 0.003791, mae: 0.065715, mean_q: 0.391068
 37052/100000: episode: 647, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -14.437, mean reward: -0.144 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.696, 10.135], loss: 0.016485, mae: 0.069305, mean_q: 0.346189
 37152/100000: episode: 648, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -11.649, mean reward: -0.116 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.225, 10.098], loss: 0.005536, mae: 0.073090, mean_q: 0.346524
 37252/100000: episode: 649, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.271, mean reward: -0.163 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.874, 10.099], loss: 0.017184, mae: 0.074127, mean_q: 0.298498
 37352/100000: episode: 650, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.352, mean reward: -0.174 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.393, 10.161], loss: 0.007305, mae: 0.074590, mean_q: 0.319004
 37452/100000: episode: 651, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -17.358, mean reward: -0.174 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.678, 10.098], loss: 0.004328, mae: 0.065332, mean_q: 0.297053
 37552/100000: episode: 652, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.878, mean reward: -0.159 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.881, 10.361], loss: 0.030920, mae: 0.079580, mean_q: 0.281916
 37652/100000: episode: 653, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.447, mean reward: -0.194 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.192, 10.098], loss: 0.017002, mae: 0.073784, mean_q: 0.279780
 37752/100000: episode: 654, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.146, mean reward: -0.191 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.710, 10.135], loss: 0.003807, mae: 0.065340, mean_q: 0.240983
 37852/100000: episode: 655, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.918, mean reward: -0.199 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.069, 10.146], loss: 0.003707, mae: 0.062513, mean_q: 0.219895
 37952/100000: episode: 656, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -11.887, mean reward: -0.119 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.586, 10.493], loss: 0.016346, mae: 0.066366, mean_q: 0.209524
 38052/100000: episode: 657, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -15.635, mean reward: -0.156 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.859, 10.098], loss: 0.003851, mae: 0.064218, mean_q: 0.196692
 38152/100000: episode: 658, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.574, mean reward: -0.166 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.511, 10.098], loss: 0.003614, mae: 0.061417, mean_q: 0.171691
 38252/100000: episode: 659, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.723, mean reward: -0.197 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.674, 10.098], loss: 0.003347, mae: 0.058757, mean_q: 0.128108
 38352/100000: episode: 660, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -19.342, mean reward: -0.193 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.211, 10.098], loss: 0.003509, mae: 0.061135, mean_q: 0.126133
 38452/100000: episode: 661, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.201, mean reward: -0.172 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.195, 10.484], loss: 0.016874, mae: 0.070744, mean_q: 0.088102
 38552/100000: episode: 662, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.377, mean reward: -0.194 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.692, 10.239], loss: 0.003348, mae: 0.060200, mean_q: 0.097521
 38652/100000: episode: 663, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -14.805, mean reward: -0.148 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.510, 10.142], loss: 0.017098, mae: 0.073896, mean_q: 0.065920
 38752/100000: episode: 664, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -14.052, mean reward: -0.141 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.766, 10.098], loss: 0.017125, mae: 0.073635, mean_q: 0.063365
 38852/100000: episode: 665, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.957, mean reward: -0.180 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.652, 10.098], loss: 0.016931, mae: 0.069879, mean_q: 0.006160
 38952/100000: episode: 666, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -12.094, mean reward: -0.121 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.575, 10.476], loss: 0.003948, mae: 0.061586, mean_q: -0.011457
 39052/100000: episode: 667, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -11.931, mean reward: -0.119 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.089, 10.129], loss: 0.041877, mae: 0.077409, mean_q: -0.011518
 39152/100000: episode: 668, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.744, mean reward: -0.167 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.455, 10.291], loss: 0.004110, mae: 0.064473, mean_q: -0.030281
 39252/100000: episode: 669, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -14.713, mean reward: -0.147 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.623, 10.098], loss: 0.003345, mae: 0.058961, mean_q: -0.021270
 39352/100000: episode: 670, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.817, mean reward: -0.198 [-1.000, 0.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.810, 10.221], loss: 0.029947, mae: 0.077279, mean_q: -0.066608
 39452/100000: episode: 671, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.398, mean reward: -0.144 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.718, 10.423], loss: 0.003559, mae: 0.062826, mean_q: -0.110624
 39552/100000: episode: 672, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -16.461, mean reward: -0.165 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.788, 10.098], loss: 0.003970, mae: 0.060369, mean_q: -0.118573
 39652/100000: episode: 673, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.562, mean reward: -0.146 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.411, 10.098], loss: 0.016256, mae: 0.065732, mean_q: -0.096865
 39752/100000: episode: 674, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -14.827, mean reward: -0.148 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.344, 10.327], loss: 0.003973, mae: 0.060412, mean_q: -0.180430
 39852/100000: episode: 675, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.416, mean reward: -0.144 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.975, 10.098], loss: 0.015849, mae: 0.061313, mean_q: -0.183208
 39952/100000: episode: 676, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -8.784, mean reward: -0.088 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.591, 10.399], loss: 0.016215, mae: 0.065911, mean_q: -0.183853
 40052/100000: episode: 677, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.688, mean reward: -0.187 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.226, 10.098], loss: 0.002969, mae: 0.055405, mean_q: -0.225425
 40152/100000: episode: 678, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.105, mean reward: -0.161 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.182, 10.098], loss: 0.002808, mae: 0.054022, mean_q: -0.249889
 40252/100000: episode: 679, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -18.986, mean reward: -0.190 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.209, 10.104], loss: 0.018156, mae: 0.072385, mean_q: -0.213789
 40352/100000: episode: 680, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -13.893, mean reward: -0.139 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.235, 10.360], loss: 0.002910, mae: 0.055135, mean_q: -0.280890
 40452/100000: episode: 681, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -15.467, mean reward: -0.155 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.818, 10.098], loss: 0.002801, mae: 0.054042, mean_q: -0.307668
 40552/100000: episode: 682, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -18.343, mean reward: -0.183 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.405, 10.098], loss: 0.003766, mae: 0.058042, mean_q: -0.289298
 40652/100000: episode: 683, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.304, mean reward: -0.153 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.247, 10.358], loss: 0.002848, mae: 0.054321, mean_q: -0.303273
 40752/100000: episode: 684, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.913, mean reward: -0.179 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.596, 10.342], loss: 0.002712, mae: 0.053522, mean_q: -0.283645
 40852/100000: episode: 685, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -16.500, mean reward: -0.165 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.533, 10.105], loss: 0.002695, mae: 0.053350, mean_q: -0.322798
 40952/100000: episode: 686, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.580, mean reward: -0.166 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.835, 10.132], loss: 0.002768, mae: 0.053867, mean_q: -0.340397
 41052/100000: episode: 687, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -15.999, mean reward: -0.160 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.783, 10.227], loss: 0.002633, mae: 0.052042, mean_q: -0.308069
 41152/100000: episode: 688, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -9.410, mean reward: -0.094 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.254, 10.098], loss: 0.002770, mae: 0.053174, mean_q: -0.313090
 41252/100000: episode: 689, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -17.359, mean reward: -0.174 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.147, 10.098], loss: 0.002724, mae: 0.053324, mean_q: -0.295401
 41352/100000: episode: 690, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.393, mean reward: -0.164 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.722, 10.098], loss: 0.002666, mae: 0.053327, mean_q: -0.270715
 41452/100000: episode: 691, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.432, mean reward: -0.144 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.159, 10.098], loss: 0.002689, mae: 0.052860, mean_q: -0.279998
 41552/100000: episode: 692, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -15.107, mean reward: -0.151 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.882, 10.128], loss: 0.002819, mae: 0.054238, mean_q: -0.292291
 41652/100000: episode: 693, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.031, mean reward: -0.170 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.269, 10.098], loss: 0.002726, mae: 0.055490, mean_q: -0.275662
 41752/100000: episode: 694, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.230, mean reward: -0.172 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.264, 10.352], loss: 0.002786, mae: 0.054801, mean_q: -0.302161
 41852/100000: episode: 695, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.956, mean reward: -0.160 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.527, 10.117], loss: 0.002622, mae: 0.052235, mean_q: -0.301266
 41952/100000: episode: 696, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.794, mean reward: -0.158 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.605, 10.128], loss: 0.003033, mae: 0.055319, mean_q: -0.309496
 42052/100000: episode: 697, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.526, mean reward: -0.195 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.061, 10.098], loss: 0.002688, mae: 0.053193, mean_q: -0.278780
 42152/100000: episode: 698, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -15.341, mean reward: -0.153 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.483, 10.098], loss: 0.002748, mae: 0.054141, mean_q: -0.313432
 42252/100000: episode: 699, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.766, mean reward: -0.158 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.022, 10.240], loss: 0.003025, mae: 0.057265, mean_q: -0.309484
 42352/100000: episode: 700, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -12.832, mean reward: -0.128 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.561, 10.098], loss: 0.002542, mae: 0.051568, mean_q: -0.295906
 42452/100000: episode: 701, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.701, mean reward: -0.187 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.722, 10.197], loss: 0.002651, mae: 0.052525, mean_q: -0.296345
 42552/100000: episode: 702, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.530, mean reward: -0.165 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.504, 10.149], loss: 0.002717, mae: 0.053643, mean_q: -0.278244
 42652/100000: episode: 703, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.297, mean reward: -0.163 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.554, 10.145], loss: 0.002706, mae: 0.053750, mean_q: -0.311999
 42752/100000: episode: 704, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.827, mean reward: -0.168 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.766, 10.311], loss: 0.002639, mae: 0.052501, mean_q: -0.302447
 42852/100000: episode: 705, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -13.250, mean reward: -0.133 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.202, 10.111], loss: 0.002869, mae: 0.054198, mean_q: -0.301728
 42952/100000: episode: 706, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.732, mean reward: -0.157 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.529, 10.098], loss: 0.002874, mae: 0.053892, mean_q: -0.358987
 43052/100000: episode: 707, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.884, mean reward: -0.169 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.330, 10.262], loss: 0.002719, mae: 0.052985, mean_q: -0.288945
 43152/100000: episode: 708, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.993, mean reward: -0.170 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.823, 10.098], loss: 0.002689, mae: 0.051740, mean_q: -0.302215
 43252/100000: episode: 709, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.239, mean reward: -0.182 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.263, 10.248], loss: 0.002976, mae: 0.056123, mean_q: -0.301363
 43352/100000: episode: 710, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.638, mean reward: -0.186 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.187, 10.098], loss: 0.002848, mae: 0.055457, mean_q: -0.307123
 43452/100000: episode: 711, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.190, mean reward: -0.182 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.596, 10.299], loss: 0.002697, mae: 0.053386, mean_q: -0.282154
 43552/100000: episode: 712, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -13.544, mean reward: -0.135 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.851, 10.098], loss: 0.002747, mae: 0.054097, mean_q: -0.260173
 43652/100000: episode: 713, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.173, mean reward: -0.182 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.651, 10.123], loss: 0.002666, mae: 0.052579, mean_q: -0.302016
 43752/100000: episode: 714, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.612, mean reward: -0.186 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.979, 10.165], loss: 0.002784, mae: 0.052756, mean_q: -0.302005
 43852/100000: episode: 715, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -14.614, mean reward: -0.146 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.223, 10.225], loss: 0.002959, mae: 0.056965, mean_q: -0.276396
 43952/100000: episode: 716, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -20.376, mean reward: -0.204 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.747, 10.114], loss: 0.002882, mae: 0.054665, mean_q: -0.304934
 44052/100000: episode: 717, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -10.858, mean reward: -0.109 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.535, 10.098], loss: 0.004521, mae: 0.060863, mean_q: -0.272333
 44152/100000: episode: 718, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.795, mean reward: -0.178 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.279, 10.118], loss: 0.004253, mae: 0.064218, mean_q: -0.301520
 44252/100000: episode: 719, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.898, mean reward: -0.189 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.605, 10.098], loss: 0.002573, mae: 0.052134, mean_q: -0.311964
 44352/100000: episode: 720, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.140, mean reward: -0.191 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.443, 10.098], loss: 0.002619, mae: 0.052264, mean_q: -0.281904
 44452/100000: episode: 721, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.506, mean reward: -0.185 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.115, 10.222], loss: 0.002722, mae: 0.053565, mean_q: -0.325648
 44552/100000: episode: 722, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -17.608, mean reward: -0.176 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.675, 10.465], loss: 0.002743, mae: 0.052957, mean_q: -0.283164
 44652/100000: episode: 723, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.311, mean reward: -0.183 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.814, 10.098], loss: 0.002689, mae: 0.053105, mean_q: -0.296423
 44752/100000: episode: 724, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.506, mean reward: -0.175 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.165, 10.098], loss: 0.002806, mae: 0.054550, mean_q: -0.297409
 44852/100000: episode: 725, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.382, mean reward: -0.164 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.418, 10.313], loss: 0.002616, mae: 0.052362, mean_q: -0.311964
 44952/100000: episode: 726, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -12.766, mean reward: -0.128 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.873, 10.268], loss: 0.002646, mae: 0.051591, mean_q: -0.317030
 45052/100000: episode: 727, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -17.078, mean reward: -0.171 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.260, 10.160], loss: 0.002732, mae: 0.053462, mean_q: -0.288794
 45152/100000: episode: 728, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.402, mean reward: -0.184 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.552, 10.208], loss: 0.002704, mae: 0.052368, mean_q: -0.281399
 45252/100000: episode: 729, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.053, mean reward: -0.171 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.747, 10.098], loss: 0.002782, mae: 0.054081, mean_q: -0.315637
 45352/100000: episode: 730, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -9.750, mean reward: -0.098 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.759, 10.459], loss: 0.002674, mae: 0.052038, mean_q: -0.323494
[Info] 100-TH LEVEL FOUND: 0.52088463306427, Considering 10/90 traces
 45452/100000: episode: 731, duration: 4.307s, episode steps: 100, steps per second: 23, episode reward: -17.805, mean reward: -0.178 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.817, 10.098], loss: 0.004990, mae: 0.062716, mean_q: -0.304990
 45465/100000: episode: 732, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 4.660, mean reward: 0.358 [0.276, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.549], loss: 0.003167, mae: 0.059952, mean_q: -0.317552
 45491/100000: episode: 733, duration: 0.156s, episode steps: 26, steps per second: 167, episode reward: 10.336, mean reward: 0.398 [0.264, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.656, 10.400], loss: 0.002688, mae: 0.055651, mean_q: -0.286250
 45519/100000: episode: 734, duration: 0.166s, episode steps: 28, steps per second: 169, episode reward: 8.671, mean reward: 0.310 [0.169, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.713, 10.379], loss: 0.002604, mae: 0.052505, mean_q: -0.317477
 45549/100000: episode: 735, duration: 0.171s, episode steps: 30, steps per second: 176, episode reward: 10.357, mean reward: 0.345 [0.208, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.387, 10.100], loss: 0.002819, mae: 0.053536, mean_q: -0.291853
 45575/100000: episode: 736, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 8.545, mean reward: 0.329 [0.169, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.091, 10.271], loss: 0.002482, mae: 0.050430, mean_q: -0.326824
 45601/100000: episode: 737, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 10.785, mean reward: 0.415 [0.286, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.815, 10.550], loss: 0.002529, mae: 0.052089, mean_q: -0.304261
 45638/100000: episode: 738, duration: 0.205s, episode steps: 37, steps per second: 181, episode reward: 7.293, mean reward: 0.197 [0.099, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.777, 10.100], loss: 0.002645, mae: 0.053315, mean_q: -0.236121
 45651/100000: episode: 739, duration: 0.083s, episode steps: 13, steps per second: 156, episode reward: 5.188, mean reward: 0.399 [0.288, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.565], loss: 0.003226, mae: 0.056255, mean_q: -0.314919
 45704/100000: episode: 740, duration: 0.291s, episode steps: 53, steps per second: 182, episode reward: 20.560, mean reward: 0.388 [0.278, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.154, 10.100], loss: 0.002799, mae: 0.054679, mean_q: -0.263731
 45734/100000: episode: 741, duration: 0.160s, episode steps: 30, steps per second: 188, episode reward: 9.184, mean reward: 0.306 [0.191, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.424, 10.100], loss: 0.003311, mae: 0.059359, mean_q: -0.265794
 45764/100000: episode: 742, duration: 0.180s, episode steps: 30, steps per second: 167, episode reward: 9.325, mean reward: 0.311 [0.158, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.142, 10.100], loss: 0.002721, mae: 0.054683, mean_q: -0.278391
 45817/100000: episode: 743, duration: 0.303s, episode steps: 53, steps per second: 175, episode reward: 15.154, mean reward: 0.286 [0.028, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.942, 10.100], loss: 0.002830, mae: 0.053647, mean_q: -0.251910
 45843/100000: episode: 744, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 10.949, mean reward: 0.421 [0.258, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.357, 10.532], loss: 0.002615, mae: 0.050466, mean_q: -0.266834
 45856/100000: episode: 745, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 5.053, mean reward: 0.389 [0.242, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.571], loss: 0.002880, mae: 0.055263, mean_q: -0.166926
 45869/100000: episode: 746, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 4.572, mean reward: 0.352 [0.252, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.736, 10.464], loss: 0.003324, mae: 0.057490, mean_q: -0.248401
 45882/100000: episode: 747, duration: 0.063s, episode steps: 13, steps per second: 206, episode reward: 4.163, mean reward: 0.320 [0.256, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.752, 10.436], loss: 0.003056, mae: 0.055937, mean_q: -0.144681
 45910/100000: episode: 748, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 14.494, mean reward: 0.518 [0.333, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.035, 10.711], loss: 0.003071, mae: 0.056236, mean_q: -0.225545
 45923/100000: episode: 749, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 4.837, mean reward: 0.372 [0.255, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.483], loss: 0.002522, mae: 0.052770, mean_q: -0.142967
 45958/100000: episode: 750, duration: 0.178s, episode steps: 35, steps per second: 196, episode reward: 11.402, mean reward: 0.326 [0.214, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.452, 10.100], loss: 0.002585, mae: 0.052040, mean_q: -0.217398
 45988/100000: episode: 751, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 8.972, mean reward: 0.299 [0.236, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.376, 10.100], loss: 0.002928, mae: 0.055157, mean_q: -0.223433
 46023/100000: episode: 752, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 11.234, mean reward: 0.321 [0.192, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.938, 10.100], loss: 0.003027, mae: 0.057784, mean_q: -0.172607
 46060/100000: episode: 753, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 8.027, mean reward: 0.217 [0.071, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-1.106, 10.100], loss: 0.003292, mae: 0.060259, mean_q: -0.128725
 46090/100000: episode: 754, duration: 0.154s, episode steps: 30, steps per second: 194, episode reward: 6.513, mean reward: 0.217 [0.071, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.627, 10.100], loss: 0.003309, mae: 0.057503, mean_q: -0.218380
 46143/100000: episode: 755, duration: 0.277s, episode steps: 53, steps per second: 192, episode reward: 16.634, mean reward: 0.314 [0.018, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-1.811, 10.131], loss: 0.003164, mae: 0.059107, mean_q: -0.131848
 46148/100000: episode: 756, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 1.559, mean reward: 0.312 [0.251, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.503], loss: 0.003791, mae: 0.062517, mean_q: -0.066027
 46161/100000: episode: 757, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 5.245, mean reward: 0.403 [0.333, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.411], loss: 0.003265, mae: 0.059325, mean_q: -0.154784
 46191/100000: episode: 758, duration: 0.171s, episode steps: 30, steps per second: 175, episode reward: 7.018, mean reward: 0.234 [0.066, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.331, 10.164], loss: 0.003323, mae: 0.059021, mean_q: -0.131907
 46219/100000: episode: 759, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 15.750, mean reward: 0.562 [0.349, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.445, 10.693], loss: 0.002985, mae: 0.056038, mean_q: -0.207060
 46272/100000: episode: 760, duration: 0.275s, episode steps: 53, steps per second: 193, episode reward: 19.628, mean reward: 0.370 [0.106, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.818, 10.146], loss: 0.003053, mae: 0.055649, mean_q: -0.168803
 46285/100000: episode: 761, duration: 0.074s, episode steps: 13, steps per second: 177, episode reward: 5.157, mean reward: 0.397 [0.338, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.552], loss: 0.003379, mae: 0.060406, mean_q: -0.086461
 46298/100000: episode: 762, duration: 0.078s, episode steps: 13, steps per second: 168, episode reward: 6.076, mean reward: 0.467 [0.316, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.431], loss: 0.003029, mae: 0.056578, mean_q: -0.173285
 46333/100000: episode: 763, duration: 0.199s, episode steps: 35, steps per second: 175, episode reward: 10.636, mean reward: 0.304 [0.168, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.306, 10.100], loss: 0.002873, mae: 0.055656, mean_q: -0.159597
 46338/100000: episode: 764, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 1.744, mean reward: 0.349 [0.304, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.478], loss: 0.002445, mae: 0.049195, mean_q: -0.166455
 46351/100000: episode: 765, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 3.950, mean reward: 0.304 [0.211, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.353, 10.492], loss: 0.002849, mae: 0.056636, mean_q: -0.106097
 46388/100000: episode: 766, duration: 0.225s, episode steps: 37, steps per second: 164, episode reward: 8.938, mean reward: 0.242 [0.143, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.397, 10.100], loss: 0.002745, mae: 0.053020, mean_q: -0.149696
 46418/100000: episode: 767, duration: 0.192s, episode steps: 30, steps per second: 156, episode reward: 9.905, mean reward: 0.330 [0.180, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.318, 10.100], loss: 0.003094, mae: 0.057023, mean_q: -0.085946
 46431/100000: episode: 768, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 3.827, mean reward: 0.294 [0.242, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.299], loss: 0.003143, mae: 0.055296, mean_q: -0.089731
 46461/100000: episode: 769, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 7.925, mean reward: 0.264 [0.161, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.188, 10.100], loss: 0.002812, mae: 0.053720, mean_q: -0.125229
 46491/100000: episode: 770, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 8.624, mean reward: 0.287 [0.174, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.149, 10.100], loss: 0.002920, mae: 0.055229, mean_q: -0.156715
 46526/100000: episode: 771, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 10.746, mean reward: 0.307 [0.211, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.236, 10.100], loss: 0.003020, mae: 0.056286, mean_q: -0.128185
 46563/100000: episode: 772, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 10.142, mean reward: 0.274 [0.166, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.711, 10.100], loss: 0.003236, mae: 0.056802, mean_q: -0.104271
 46593/100000: episode: 773, duration: 0.173s, episode steps: 30, steps per second: 173, episode reward: 10.426, mean reward: 0.348 [0.143, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.761, 10.100], loss: 0.003111, mae: 0.057422, mean_q: -0.126697
 46628/100000: episode: 774, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 9.751, mean reward: 0.279 [0.126, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.345, 10.100], loss: 0.003627, mae: 0.062538, mean_q: -0.063024
 46633/100000: episode: 775, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 2.474, mean reward: 0.495 [0.403, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.035, 10.651], loss: 0.003179, mae: 0.059017, mean_q: -0.059475
 46686/100000: episode: 776, duration: 0.295s, episode steps: 53, steps per second: 180, episode reward: 17.443, mean reward: 0.329 [0.203, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.300, 10.100], loss: 0.003150, mae: 0.057495, mean_q: -0.117547
 46699/100000: episode: 777, duration: 0.087s, episode steps: 13, steps per second: 149, episode reward: 5.020, mean reward: 0.386 [0.247, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.593], loss: 0.003359, mae: 0.060661, mean_q: -0.095212
 46734/100000: episode: 778, duration: 0.205s, episode steps: 35, steps per second: 170, episode reward: 6.665, mean reward: 0.190 [0.049, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.306, 10.189], loss: 0.003208, mae: 0.061115, mean_q: -0.067099
 46747/100000: episode: 779, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 4.844, mean reward: 0.373 [0.319, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.510], loss: 0.003138, mae: 0.058907, mean_q: 0.002852
 46773/100000: episode: 780, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 9.627, mean reward: 0.370 [0.244, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.535, 10.415], loss: 0.002923, mae: 0.056270, mean_q: -0.094185
 46803/100000: episode: 781, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 4.708, mean reward: 0.157 [0.028, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.078, 10.100], loss: 0.003049, mae: 0.057819, mean_q: -0.014400
 46856/100000: episode: 782, duration: 0.285s, episode steps: 53, steps per second: 186, episode reward: 19.540, mean reward: 0.369 [0.237, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-0.608, 10.100], loss: 0.003482, mae: 0.060906, mean_q: -0.057484
 46886/100000: episode: 783, duration: 0.157s, episode steps: 30, steps per second: 192, episode reward: 5.364, mean reward: 0.179 [0.031, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.085, 10.100], loss: 0.003718, mae: 0.064189, mean_q: -0.027115
 46912/100000: episode: 784, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 8.627, mean reward: 0.332 [0.237, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.035, 10.420], loss: 0.002944, mae: 0.057477, mean_q: -0.049238
 46925/100000: episode: 785, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 5.573, mean reward: 0.429 [0.265, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.599], loss: 0.003224, mae: 0.059356, mean_q: -0.016349
 46951/100000: episode: 786, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 9.365, mean reward: 0.360 [0.250, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.357, 10.492], loss: 0.002900, mae: 0.058448, mean_q: -0.047288
 46986/100000: episode: 787, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 11.441, mean reward: 0.327 [0.172, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.564, 10.100], loss: 0.003135, mae: 0.058334, mean_q: -0.012496
 46999/100000: episode: 788, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 3.892, mean reward: 0.299 [0.185, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.445], loss: 0.003074, mae: 0.056393, mean_q: -0.027078
 47012/100000: episode: 789, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 4.814, mean reward: 0.370 [0.259, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.250, 10.425], loss: 0.003015, mae: 0.056604, mean_q: 0.009489
 47047/100000: episode: 790, duration: 0.202s, episode steps: 35, steps per second: 173, episode reward: 8.634, mean reward: 0.247 [0.100, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.498, 10.100], loss: 0.003651, mae: 0.063627, mean_q: 0.024941
 47077/100000: episode: 791, duration: 0.160s, episode steps: 30, steps per second: 188, episode reward: 8.003, mean reward: 0.267 [0.086, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.174, 10.100], loss: 0.003273, mae: 0.060042, mean_q: -0.022003
 47130/100000: episode: 792, duration: 0.281s, episode steps: 53, steps per second: 189, episode reward: 14.879, mean reward: 0.281 [0.057, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-1.104, 10.100], loss: 0.003278, mae: 0.060331, mean_q: 0.013473
 47160/100000: episode: 793, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 9.805, mean reward: 0.327 [0.168, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.841, 10.100], loss: 0.003507, mae: 0.061754, mean_q: -0.055962
 47173/100000: episode: 794, duration: 0.072s, episode steps: 13, steps per second: 179, episode reward: 5.096, mean reward: 0.392 [0.307, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.481], loss: 0.003606, mae: 0.064077, mean_q: -0.005047
 47226/100000: episode: 795, duration: 0.265s, episode steps: 53, steps per second: 200, episode reward: 13.209, mean reward: 0.249 [0.086, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.597, 10.100], loss: 0.003152, mae: 0.058756, mean_q: 0.027474
 47254/100000: episode: 796, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 7.143, mean reward: 0.255 [0.100, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.821, 10.177], loss: 0.003163, mae: 0.058109, mean_q: 0.021379
 47307/100000: episode: 797, duration: 0.292s, episode steps: 53, steps per second: 182, episode reward: 17.200, mean reward: 0.325 [0.218, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.777, 10.100], loss: 0.003138, mae: 0.058863, mean_q: 0.044017
 47360/100000: episode: 798, duration: 0.290s, episode steps: 53, steps per second: 183, episode reward: 11.567, mean reward: 0.218 [0.026, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.404, 10.200], loss: 0.003318, mae: 0.059869, mean_q: 0.047913
 47388/100000: episode: 799, duration: 0.141s, episode steps: 28, steps per second: 198, episode reward: 7.849, mean reward: 0.280 [0.210, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.304, 10.401], loss: 0.003319, mae: 0.060986, mean_q: 0.064551
 47401/100000: episode: 800, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 5.259, mean reward: 0.405 [0.236, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.767, 10.537], loss: 0.002917, mae: 0.056887, mean_q: 0.106835
 47431/100000: episode: 801, duration: 0.173s, episode steps: 30, steps per second: 173, episode reward: 5.798, mean reward: 0.193 [0.115, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.313, 10.100], loss: 0.003129, mae: 0.059415, mean_q: 0.037172
 47444/100000: episode: 802, duration: 0.086s, episode steps: 13, steps per second: 150, episode reward: 4.189, mean reward: 0.322 [0.258, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.500], loss: 0.003316, mae: 0.060960, mean_q: 0.070739
 47479/100000: episode: 803, duration: 0.196s, episode steps: 35, steps per second: 178, episode reward: 10.763, mean reward: 0.308 [0.183, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.206, 10.100], loss: 0.003700, mae: 0.063963, mean_q: 0.057646
 47492/100000: episode: 804, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 4.903, mean reward: 0.377 [0.287, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.544], loss: 0.003609, mae: 0.064417, mean_q: 0.036504
 47520/100000: episode: 805, duration: 0.161s, episode steps: 28, steps per second: 173, episode reward: 9.258, mean reward: 0.331 [0.137, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.178, 10.439], loss: 0.003470, mae: 0.061173, mean_q: 0.056665
 47550/100000: episode: 806, duration: 0.176s, episode steps: 30, steps per second: 170, episode reward: 8.212, mean reward: 0.274 [0.166, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.328, 10.100], loss: 0.002989, mae: 0.057060, mean_q: 0.038911
 47576/100000: episode: 807, duration: 0.156s, episode steps: 26, steps per second: 166, episode reward: 11.181, mean reward: 0.430 [0.343, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.528, 10.531], loss: 0.003084, mae: 0.057750, mean_q: 0.085526
 47606/100000: episode: 808, duration: 0.195s, episode steps: 30, steps per second: 154, episode reward: 10.187, mean reward: 0.340 [0.259, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.469, 10.100], loss: 0.003303, mae: 0.061610, mean_q: 0.115685
 47659/100000: episode: 809, duration: 0.301s, episode steps: 53, steps per second: 176, episode reward: 10.745, mean reward: 0.203 [0.013, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.177, 10.100], loss: 0.003335, mae: 0.059371, mean_q: 0.094243
 47685/100000: episode: 810, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 8.568, mean reward: 0.330 [0.166, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.890, 10.246], loss: 0.002992, mae: 0.059202, mean_q: 0.131497
 47698/100000: episode: 811, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 4.018, mean reward: 0.309 [0.224, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.454], loss: 0.002931, mae: 0.057719, mean_q: 0.087773
 47703/100000: episode: 812, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 1.488, mean reward: 0.298 [0.256, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.400], loss: 0.003686, mae: 0.060932, mean_q: 0.029893
 47738/100000: episode: 813, duration: 0.202s, episode steps: 35, steps per second: 173, episode reward: 11.356, mean reward: 0.324 [0.153, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.418, 10.100], loss: 0.003127, mae: 0.059004, mean_q: 0.129971
 47751/100000: episode: 814, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 4.213, mean reward: 0.324 [0.213, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.442], loss: 0.003320, mae: 0.060633, mean_q: 0.071015
 47777/100000: episode: 815, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 10.933, mean reward: 0.420 [0.289, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.710, 10.476], loss: 0.003507, mae: 0.062536, mean_q: 0.122877
 47790/100000: episode: 816, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 5.561, mean reward: 0.428 [0.273, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.619, 10.479], loss: 0.002754, mae: 0.055542, mean_q: 0.103622
 47827/100000: episode: 817, duration: 0.221s, episode steps: 37, steps per second: 168, episode reward: 12.253, mean reward: 0.331 [0.165, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.113, 10.100], loss: 0.002978, mae: 0.056569, mean_q: 0.110225
 47880/100000: episode: 818, duration: 0.311s, episode steps: 53, steps per second: 170, episode reward: 23.690, mean reward: 0.447 [0.318, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.846 [-1.160, 10.100], loss: 0.003404, mae: 0.062075, mean_q: 0.164633
 47893/100000: episode: 819, duration: 0.085s, episode steps: 13, steps per second: 152, episode reward: 4.973, mean reward: 0.383 [0.267, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-1.808, 10.370], loss: 0.003071, mae: 0.059975, mean_q: 0.138372
 47923/100000: episode: 820, duration: 0.180s, episode steps: 30, steps per second: 166, episode reward: 8.013, mean reward: 0.267 [0.117, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.035, 10.100], loss: 0.002961, mae: 0.057556, mean_q: 0.133943
[Info] 200-TH LEVEL FOUND: 0.940218448638916, Considering 10/90 traces
 47960/100000: episode: 821, duration: 4.110s, episode steps: 37, steps per second: 9, episode reward: 10.166, mean reward: 0.275 [0.171, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.928, 10.100], loss: 0.003349, mae: 0.060376, mean_q: 0.123581
 47969/100000: episode: 822, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 3.585, mean reward: 0.398 [0.341, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.505], loss: 0.004326, mae: 0.073000, mean_q: 0.153717
 47975/100000: episode: 823, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 2.193, mean reward: 0.365 [0.308, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.445], loss: 0.003478, mae: 0.063016, mean_q: 0.159833
 47988/100000: episode: 824, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 5.767, mean reward: 0.444 [0.304, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.194, 10.516], loss: 0.003271, mae: 0.060721, mean_q: 0.112770
 48002/100000: episode: 825, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 6.185, mean reward: 0.442 [0.266, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.626, 10.100], loss: 0.002972, mae: 0.057660, mean_q: 0.137781
 48015/100000: episode: 826, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 5.673, mean reward: 0.436 [0.366, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.984, 10.440], loss: 0.003487, mae: 0.061357, mean_q: 0.216220
 48024/100000: episode: 827, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 3.238, mean reward: 0.360 [0.288, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.489], loss: 0.002488, mae: 0.050684, mean_q: 0.046922
 48031/100000: episode: 828, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 2.725, mean reward: 0.389 [0.341, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.514], loss: 0.004049, mae: 0.067064, mean_q: 0.145063
 48044/100000: episode: 829, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 6.279, mean reward: 0.483 [0.450, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.549], loss: 0.003379, mae: 0.063649, mean_q: 0.207440
 48057/100000: episode: 830, duration: 0.085s, episode steps: 13, steps per second: 152, episode reward: 6.370, mean reward: 0.490 [0.386, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.687], loss: 0.002960, mae: 0.058443, mean_q: 0.211785
 48066/100000: episode: 831, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 3.598, mean reward: 0.400 [0.351, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.601], loss: 0.003035, mae: 0.058401, mean_q: 0.163692
 48079/100000: episode: 832, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 5.830, mean reward: 0.448 [0.391, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.053, 10.466], loss: 0.003922, mae: 0.066886, mean_q: 0.215070
 48086/100000: episode: 833, duration: 0.043s, episode steps: 7, steps per second: 161, episode reward: 2.555, mean reward: 0.365 [0.283, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.268, 10.488], loss: 0.003948, mae: 0.067883, mean_q: 0.195622
 48100/100000: episode: 834, duration: 0.089s, episode steps: 14, steps per second: 158, episode reward: 6.879, mean reward: 0.491 [0.425, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.662], loss: 0.003117, mae: 0.060234, mean_q: 0.250724
 48114/100000: episode: 835, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 7.065, mean reward: 0.505 [0.438, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.305, 10.100], loss: 0.003361, mae: 0.059192, mean_q: 0.200244
 48128/100000: episode: 836, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 6.624, mean reward: 0.473 [0.409, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.494, 10.100], loss: 0.003279, mae: 0.062439, mean_q: 0.210741
[Info] FALSIFICATION!
 48143/100000: episode: 837, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 18.387, mean reward: 1.226 [0.474, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.014, 10.467], loss: 0.004045, mae: 0.067668, mean_q: 0.240413
 48243/100000: episode: 838, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.261, mean reward: -0.183 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.005, 10.143], loss: 0.003184, mae: 0.060008, mean_q: 0.219742
 48343/100000: episode: 839, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.543, mean reward: -0.155 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.887, 10.098], loss: 0.018386, mae: 0.080965, mean_q: 0.203108
 48443/100000: episode: 840, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.930, mean reward: -0.179 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.696, 10.098], loss: 0.016970, mae: 0.075533, mean_q: 0.203100
 48543/100000: episode: 841, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -13.884, mean reward: -0.139 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.018, 10.098], loss: 0.031070, mae: 0.087866, mean_q: 0.229307
 48643/100000: episode: 842, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -17.772, mean reward: -0.178 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.560, 10.249], loss: 0.003211, mae: 0.060436, mean_q: 0.184207
 48743/100000: episode: 843, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.952, mean reward: -0.190 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.723, 10.195], loss: 0.003099, mae: 0.058984, mean_q: 0.203719
 48843/100000: episode: 844, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -13.543, mean reward: -0.135 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.391, 10.098], loss: 0.017403, mae: 0.075635, mean_q: 0.203094
 48943/100000: episode: 845, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.131, mean reward: -0.171 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.989, 10.098], loss: 0.016041, mae: 0.066625, mean_q: 0.210844
 49043/100000: episode: 846, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.115, mean reward: -0.171 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.737, 10.296], loss: 0.003418, mae: 0.062107, mean_q: 0.211531
 49143/100000: episode: 847, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -20.022, mean reward: -0.200 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.522, 10.196], loss: 0.003103, mae: 0.059576, mean_q: 0.192308
 49243/100000: episode: 848, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.652, mean reward: -0.167 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.086, 10.286], loss: 0.016834, mae: 0.072767, mean_q: 0.224866
 49343/100000: episode: 849, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.927, mean reward: -0.159 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.492, 10.098], loss: 0.016743, mae: 0.070241, mean_q: 0.204549
 49443/100000: episode: 850, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.113, mean reward: -0.151 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.858, 10.303], loss: 0.003085, mae: 0.058858, mean_q: 0.200133
 49543/100000: episode: 851, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.218, mean reward: -0.172 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.871, 10.098], loss: 0.003029, mae: 0.057621, mean_q: 0.212375
 49643/100000: episode: 852, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -7.629, mean reward: -0.076 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.697, 10.098], loss: 0.003230, mae: 0.059993, mean_q: 0.182958
 49743/100000: episode: 853, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.542, mean reward: -0.185 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.712, 10.098], loss: 0.016649, mae: 0.070067, mean_q: 0.213902
 49843/100000: episode: 854, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -20.927, mean reward: -0.209 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-2.223, 10.218], loss: 0.041841, mae: 0.084299, mean_q: 0.208886
 49943/100000: episode: 855, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.623, mean reward: -0.176 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.598, 10.281], loss: 0.028722, mae: 0.079773, mean_q: 0.226925
 50043/100000: episode: 856, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.429, mean reward: -0.174 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.978, 10.098], loss: 0.003375, mae: 0.061145, mean_q: 0.202476
 50143/100000: episode: 857, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -21.000, mean reward: -0.210 [-1.000, 0.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.908, 10.173], loss: 0.016186, mae: 0.068888, mean_q: 0.200476
 50243/100000: episode: 858, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -15.426, mean reward: -0.154 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.591, 10.098], loss: 0.003191, mae: 0.059926, mean_q: 0.220154
 50343/100000: episode: 859, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -12.846, mean reward: -0.128 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.054, 10.098], loss: 0.003330, mae: 0.062236, mean_q: 0.198469
 50443/100000: episode: 860, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.745, mean reward: -0.187 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.208, 10.098], loss: 0.016213, mae: 0.066787, mean_q: 0.193504
 50543/100000: episode: 861, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -19.770, mean reward: -0.198 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.210, 10.112], loss: 0.016797, mae: 0.071047, mean_q: 0.174828
 50643/100000: episode: 862, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.728, mean reward: -0.177 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.271, 10.134], loss: 0.042143, mae: 0.087305, mean_q: 0.136668
 50743/100000: episode: 863, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.199, mean reward: -0.182 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.799, 10.098], loss: 0.006057, mae: 0.072152, mean_q: 0.139025
 50843/100000: episode: 864, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.372, mean reward: -0.154 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.094, 10.098], loss: 0.010293, mae: 0.084173, mean_q: 0.092803
 50943/100000: episode: 865, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -16.554, mean reward: -0.166 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.271, 10.162], loss: 0.003315, mae: 0.061473, mean_q: 0.086009
 51043/100000: episode: 866, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.147, mean reward: -0.181 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.780, 10.112], loss: 0.003229, mae: 0.059354, mean_q: 0.054318
 51143/100000: episode: 867, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.636, mean reward: -0.166 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.841, 10.263], loss: 0.003138, mae: 0.058498, mean_q: 0.041122
 51243/100000: episode: 868, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.279, mean reward: -0.173 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.569, 10.098], loss: 0.002928, mae: 0.057155, mean_q: 0.047622
 51343/100000: episode: 869, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -17.375, mean reward: -0.174 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.439, 10.273], loss: 0.016374, mae: 0.069983, mean_q: 0.039288
 51443/100000: episode: 870, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.989, mean reward: -0.170 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.133, 10.098], loss: 0.029078, mae: 0.074418, mean_q: 0.006276
 51543/100000: episode: 871, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -20.369, mean reward: -0.204 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.675, 10.137], loss: 0.042182, mae: 0.087347, mean_q: -0.036677
 51643/100000: episode: 872, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.844, mean reward: -0.168 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.124, 10.375], loss: 0.015320, mae: 0.060844, mean_q: -0.017233
 51743/100000: episode: 873, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.971, mean reward: -0.170 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.692, 10.171], loss: 0.016243, mae: 0.069144, mean_q: -0.033905
 51843/100000: episode: 874, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -12.349, mean reward: -0.123 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.973, 10.305], loss: 0.015748, mae: 0.064387, mean_q: -0.065447
 51943/100000: episode: 875, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -15.726, mean reward: -0.157 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.871, 10.177], loss: 0.003009, mae: 0.055577, mean_q: -0.107191
 52043/100000: episode: 876, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -13.928, mean reward: -0.139 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.941, 10.333], loss: 0.002918, mae: 0.055404, mean_q: -0.097675
 52143/100000: episode: 877, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.870, mean reward: -0.149 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.178, 10.098], loss: 0.016426, mae: 0.067703, mean_q: -0.125821
 52243/100000: episode: 878, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -18.660, mean reward: -0.187 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.695, 10.109], loss: 0.028750, mae: 0.072274, mean_q: -0.146303
 52343/100000: episode: 879, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -13.371, mean reward: -0.134 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.514, 10.098], loss: 0.003148, mae: 0.056620, mean_q: -0.181367
 52443/100000: episode: 880, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.350, mean reward: -0.163 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.370, 10.227], loss: 0.002919, mae: 0.054850, mean_q: -0.204890
 52543/100000: episode: 881, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -13.752, mean reward: -0.138 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.111, 10.203], loss: 0.015534, mae: 0.060485, mean_q: -0.198070
 52643/100000: episode: 882, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.712, mean reward: -0.187 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.811, 10.098], loss: 0.029272, mae: 0.073179, mean_q: -0.201567
 52743/100000: episode: 883, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.747, mean reward: -0.187 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.734, 10.164], loss: 0.002797, mae: 0.053283, mean_q: -0.246283
 52843/100000: episode: 884, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.655, mean reward: -0.157 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.594, 10.116], loss: 0.002769, mae: 0.053564, mean_q: -0.273364
 52943/100000: episode: 885, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.527, mean reward: -0.175 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.606, 10.162], loss: 0.002716, mae: 0.053045, mean_q: -0.226881
 53043/100000: episode: 886, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.975, mean reward: -0.190 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.189, 10.300], loss: 0.015662, mae: 0.062245, mean_q: -0.282853
 53143/100000: episode: 887, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -10.828, mean reward: -0.108 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.773, 10.098], loss: 0.002773, mae: 0.052631, mean_q: -0.320324
 53243/100000: episode: 888, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.906, mean reward: -0.159 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.567, 10.136], loss: 0.002640, mae: 0.051580, mean_q: -0.345492
 53343/100000: episode: 889, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.811, mean reward: -0.168 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.567, 10.098], loss: 0.002754, mae: 0.053768, mean_q: -0.293447
 53443/100000: episode: 890, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -14.124, mean reward: -0.141 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.856, 10.105], loss: 0.002724, mae: 0.052455, mean_q: -0.308500
 53543/100000: episode: 891, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -14.032, mean reward: -0.140 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.915, 10.098], loss: 0.002799, mae: 0.053127, mean_q: -0.302476
 53643/100000: episode: 892, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.435, mean reward: -0.144 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.085, 10.098], loss: 0.003012, mae: 0.054492, mean_q: -0.319471
 53743/100000: episode: 893, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -21.018, mean reward: -0.210 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.359, 10.130], loss: 0.002813, mae: 0.053238, mean_q: -0.322048
 53843/100000: episode: 894, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -14.633, mean reward: -0.146 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.171, 10.230], loss: 0.002904, mae: 0.054324, mean_q: -0.313978
 53943/100000: episode: 895, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.992, mean reward: -0.170 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.336, 10.098], loss: 0.002866, mae: 0.053926, mean_q: -0.339206
 54043/100000: episode: 896, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.516, mean reward: -0.175 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.052, 10.288], loss: 0.007276, mae: 0.070037, mean_q: -0.299369
 54143/100000: episode: 897, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.870, mean reward: -0.159 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.964, 10.098], loss: 0.006095, mae: 0.076117, mean_q: -0.294653
 54243/100000: episode: 898, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -13.547, mean reward: -0.135 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.595, 10.098], loss: 0.003594, mae: 0.060753, mean_q: -0.288514
 54343/100000: episode: 899, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -16.485, mean reward: -0.165 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.770, 10.189], loss: 0.002839, mae: 0.053831, mean_q: -0.337848
 54443/100000: episode: 900, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.614, mean reward: -0.176 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.741, 10.098], loss: 0.002709, mae: 0.052185, mean_q: -0.332652
 54543/100000: episode: 901, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -18.527, mean reward: -0.185 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.474, 10.125], loss: 0.002821, mae: 0.052658, mean_q: -0.327460
 54643/100000: episode: 902, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -21.197, mean reward: -0.212 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.382, 10.098], loss: 0.002883, mae: 0.053543, mean_q: -0.290219
 54743/100000: episode: 903, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.034, mean reward: -0.180 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.278, 10.098], loss: 0.002801, mae: 0.052876, mean_q: -0.320921
 54843/100000: episode: 904, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -16.243, mean reward: -0.162 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.786, 10.334], loss: 0.002747, mae: 0.052267, mean_q: -0.312096
 54943/100000: episode: 905, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.703, mean reward: -0.177 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.559, 10.098], loss: 0.002854, mae: 0.053326, mean_q: -0.342659
 55043/100000: episode: 906, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -11.268, mean reward: -0.113 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.829, 10.345], loss: 0.002904, mae: 0.053586, mean_q: -0.338746
 55143/100000: episode: 907, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.411, mean reward: -0.194 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.718, 10.098], loss: 0.002821, mae: 0.053278, mean_q: -0.308637
 55243/100000: episode: 908, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -13.961, mean reward: -0.140 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.416, 10.240], loss: 0.002838, mae: 0.053778, mean_q: -0.302005
 55343/100000: episode: 909, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -12.455, mean reward: -0.125 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.615, 10.098], loss: 0.003057, mae: 0.055597, mean_q: -0.309230
 55443/100000: episode: 910, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.367, mean reward: -0.184 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.320, 10.098], loss: 0.003188, mae: 0.056502, mean_q: -0.290783
 55543/100000: episode: 911, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -15.297, mean reward: -0.153 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.846, 10.098], loss: 0.002962, mae: 0.054792, mean_q: -0.302107
 55643/100000: episode: 912, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.589, mean reward: -0.166 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.940, 10.098], loss: 0.002907, mae: 0.054122, mean_q: -0.292867
 55743/100000: episode: 913, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -15.549, mean reward: -0.155 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.819, 10.098], loss: 0.002767, mae: 0.051474, mean_q: -0.328526
 55843/100000: episode: 914, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.609, mean reward: -0.176 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.454, 10.098], loss: 0.002870, mae: 0.053971, mean_q: -0.269450
 55943/100000: episode: 915, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.861, mean reward: -0.189 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.327, 10.152], loss: 0.003141, mae: 0.055338, mean_q: -0.281638
 56043/100000: episode: 916, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.618, mean reward: -0.176 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.735, 10.098], loss: 0.002953, mae: 0.054319, mean_q: -0.316214
 56143/100000: episode: 917, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.004, mean reward: -0.160 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.659, 10.171], loss: 0.002608, mae: 0.051266, mean_q: -0.291567
 56243/100000: episode: 918, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -12.561, mean reward: -0.126 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.929, 10.301], loss: 0.002687, mae: 0.051004, mean_q: -0.296561
 56343/100000: episode: 919, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.874, mean reward: -0.179 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.888, 10.098], loss: 0.002825, mae: 0.053510, mean_q: -0.323299
 56443/100000: episode: 920, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.102, mean reward: -0.131 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.431, 10.411], loss: 0.002948, mae: 0.053592, mean_q: -0.292529
 56543/100000: episode: 921, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.438, mean reward: -0.144 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.703, 10.191], loss: 0.003083, mae: 0.055782, mean_q: -0.272349
 56643/100000: episode: 922, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -15.131, mean reward: -0.151 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.539, 10.388], loss: 0.003145, mae: 0.056588, mean_q: -0.290097
 56743/100000: episode: 923, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -11.573, mean reward: -0.116 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.478, 10.098], loss: 0.004120, mae: 0.057505, mean_q: -0.314371
 56843/100000: episode: 924, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.369, mean reward: -0.164 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.719, 10.098], loss: 0.008215, mae: 0.078609, mean_q: -0.278204
 56943/100000: episode: 925, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -14.456, mean reward: -0.145 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.218, 10.098], loss: 0.007024, mae: 0.072434, mean_q: -0.282160
 57043/100000: episode: 926, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.024, mean reward: -0.180 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.265, 10.111], loss: 0.002788, mae: 0.053109, mean_q: -0.285074
 57143/100000: episode: 927, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.595, mean reward: -0.166 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.981, 10.098], loss: 0.002866, mae: 0.053211, mean_q: -0.331200
 57243/100000: episode: 928, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.651, mean reward: -0.177 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.756, 10.330], loss: 0.002726, mae: 0.052041, mean_q: -0.264625
 57343/100000: episode: 929, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.681, mean reward: -0.157 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.148, 10.098], loss: 0.002658, mae: 0.051647, mean_q: -0.285679
 57443/100000: episode: 930, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.812, mean reward: -0.148 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.562, 10.386], loss: 0.002833, mae: 0.052581, mean_q: -0.295514
 57543/100000: episode: 931, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.027, mean reward: -0.180 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.673, 10.155], loss: 0.002501, mae: 0.049394, mean_q: -0.327014
 57643/100000: episode: 932, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -18.953, mean reward: -0.190 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.244, 10.366], loss: 0.002767, mae: 0.051752, mean_q: -0.319515
 57743/100000: episode: 933, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -14.863, mean reward: -0.149 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.269, 10.249], loss: 0.002843, mae: 0.053030, mean_q: -0.285446
 57843/100000: episode: 934, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -15.834, mean reward: -0.158 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.537, 10.098], loss: 0.002792, mae: 0.051901, mean_q: -0.326155
 57943/100000: episode: 935, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.126, mean reward: -0.181 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.863, 10.235], loss: 0.002612, mae: 0.052159, mean_q: -0.284927
 58043/100000: episode: 936, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -10.659, mean reward: -0.107 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.844, 10.288], loss: 0.002841, mae: 0.053036, mean_q: -0.311044
[Info] 100-TH LEVEL FOUND: 0.6139882802963257, Considering 10/90 traces
 58143/100000: episode: 937, duration: 4.345s, episode steps: 100, steps per second: 23, episode reward: -17.930, mean reward: -0.179 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.242, 10.098], loss: 0.002582, mae: 0.049774, mean_q: -0.310873
 58163/100000: episode: 938, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 5.055, mean reward: 0.253 [0.103, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.190, 10.100], loss: 0.002695, mae: 0.050251, mean_q: -0.324394
 58191/100000: episode: 939, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 8.170, mean reward: 0.292 [0.144, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.849, 10.452], loss: 0.002867, mae: 0.052102, mean_q: -0.337421
 58219/100000: episode: 940, duration: 0.146s, episode steps: 28, steps per second: 191, episode reward: 7.907, mean reward: 0.282 [0.192, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.310, 10.387], loss: 0.002834, mae: 0.052524, mean_q: -0.262120
 58239/100000: episode: 941, duration: 0.129s, episode steps: 20, steps per second: 155, episode reward: 6.185, mean reward: 0.309 [0.210, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.471], loss: 0.002782, mae: 0.052883, mean_q: -0.265955
 58253/100000: episode: 942, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 4.559, mean reward: 0.326 [0.243, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.378, 10.100], loss: 0.002714, mae: 0.050914, mean_q: -0.309131
 58283/100000: episode: 943, duration: 0.152s, episode steps: 30, steps per second: 197, episode reward: 9.525, mean reward: 0.318 [0.226, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.156, 10.100], loss: 0.002837, mae: 0.052640, mean_q: -0.264421
 58302/100000: episode: 944, duration: 0.091s, episode steps: 19, steps per second: 210, episode reward: 4.974, mean reward: 0.262 [0.115, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.218, 10.100], loss: 0.002792, mae: 0.052756, mean_q: -0.237094
 58316/100000: episode: 945, duration: 0.068s, episode steps: 14, steps per second: 205, episode reward: 5.044, mean reward: 0.360 [0.293, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.969, 10.100], loss: 0.002988, mae: 0.054767, mean_q: -0.278430
 58345/100000: episode: 946, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 8.962, mean reward: 0.309 [0.103, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.243, 10.333], loss: 0.002633, mae: 0.051067, mean_q: -0.281506
 58373/100000: episode: 947, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 8.070, mean reward: 0.288 [0.172, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.836, 10.298], loss: 0.002682, mae: 0.052618, mean_q: -0.259039
 58387/100000: episode: 948, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 3.667, mean reward: 0.262 [0.096, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.204, 10.100], loss: 0.002503, mae: 0.049348, mean_q: -0.325599
 58406/100000: episode: 949, duration: 0.097s, episode steps: 19, steps per second: 197, episode reward: 5.171, mean reward: 0.272 [0.119, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.366, 10.100], loss: 0.002805, mae: 0.052672, mean_q: -0.245502
 58427/100000: episode: 950, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 7.502, mean reward: 0.357 [0.266, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.473], loss: 0.002462, mae: 0.050217, mean_q: -0.229968
 58456/100000: episode: 951, duration: 0.139s, episode steps: 29, steps per second: 208, episode reward: 8.540, mean reward: 0.294 [0.186, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.344], loss: 0.002551, mae: 0.051357, mean_q: -0.238993
 58476/100000: episode: 952, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 4.474, mean reward: 0.224 [0.137, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.348], loss: 0.002839, mae: 0.054482, mean_q: -0.251161
 58496/100000: episode: 953, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 6.018, mean reward: 0.301 [0.199, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.898, 10.330], loss: 0.002836, mae: 0.054199, mean_q: -0.138434
 58515/100000: episode: 954, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 6.852, mean reward: 0.361 [0.273, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.349, 10.100], loss: 0.002597, mae: 0.052204, mean_q: -0.178036
 58544/100000: episode: 955, duration: 0.143s, episode steps: 29, steps per second: 202, episode reward: 6.047, mean reward: 0.209 [0.041, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.529, 10.100], loss: 0.002822, mae: 0.053514, mean_q: -0.199535
 58574/100000: episode: 956, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 7.616, mean reward: 0.254 [0.067, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.082, 10.100], loss: 0.002659, mae: 0.051657, mean_q: -0.270918
 58603/100000: episode: 957, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 11.607, mean reward: 0.400 [0.211, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.124, 10.365], loss: 0.002845, mae: 0.054392, mean_q: -0.253174
 58623/100000: episode: 958, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 6.423, mean reward: 0.321 [0.221, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.374, 10.100], loss: 0.003201, mae: 0.058189, mean_q: -0.211141
 58643/100000: episode: 959, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 6.679, mean reward: 0.334 [0.249, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.408, 10.381], loss: 0.002321, mae: 0.050337, mean_q: -0.161843
 58662/100000: episode: 960, duration: 0.090s, episode steps: 19, steps per second: 210, episode reward: 4.501, mean reward: 0.237 [0.149, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.138, 10.100], loss: 0.002659, mae: 0.051275, mean_q: -0.275857
 58697/100000: episode: 961, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 14.258, mean reward: 0.407 [0.304, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.283, 10.100], loss: 0.002632, mae: 0.051850, mean_q: -0.185494
 58716/100000: episode: 962, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 6.488, mean reward: 0.341 [0.277, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.354, 10.100], loss: 0.002725, mae: 0.052505, mean_q: -0.189586
 58735/100000: episode: 963, duration: 0.099s, episode steps: 19, steps per second: 193, episode reward: 6.369, mean reward: 0.335 [0.241, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.396, 10.100], loss: 0.002798, mae: 0.053273, mean_q: -0.217938
 58755/100000: episode: 964, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 6.005, mean reward: 0.300 [0.185, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.424, 10.100], loss: 0.002698, mae: 0.052361, mean_q: -0.226700
 58787/100000: episode: 965, duration: 0.159s, episode steps: 32, steps per second: 202, episode reward: 10.589, mean reward: 0.331 [0.196, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.035, 10.324], loss: 0.002805, mae: 0.052307, mean_q: -0.241164
 58817/100000: episode: 966, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 8.224, mean reward: 0.274 [0.153, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.315, 10.100], loss: 0.002829, mae: 0.053501, mean_q: -0.258448
 58852/100000: episode: 967, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 10.234, mean reward: 0.292 [0.185, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.741, 10.100], loss: 0.002968, mae: 0.055250, mean_q: -0.141356
 58866/100000: episode: 968, duration: 0.088s, episode steps: 14, steps per second: 160, episode reward: 3.858, mean reward: 0.276 [0.189, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.205, 10.100], loss: 0.002781, mae: 0.052824, mean_q: -0.209153
 58885/100000: episode: 969, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 4.159, mean reward: 0.219 [0.101, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.931, 10.100], loss: 0.002401, mae: 0.049273, mean_q: -0.238324
 58899/100000: episode: 970, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 4.259, mean reward: 0.304 [0.224, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.125, 10.100], loss: 0.002884, mae: 0.055348, mean_q: -0.197265
 58918/100000: episode: 971, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 5.530, mean reward: 0.291 [0.060, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.129, 10.113], loss: 0.002680, mae: 0.054722, mean_q: -0.107523
 58938/100000: episode: 972, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 7.114, mean reward: 0.356 [0.295, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.977, 10.100], loss: 0.002556, mae: 0.051971, mean_q: -0.184164
 58957/100000: episode: 973, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 4.370, mean reward: 0.230 [0.139, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.597, 10.100], loss: 0.002193, mae: 0.048015, mean_q: -0.164175
 58985/100000: episode: 974, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 7.302, mean reward: 0.261 [0.152, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.951, 10.438], loss: 0.002761, mae: 0.054204, mean_q: -0.103522
 59006/100000: episode: 975, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 10.300, mean reward: 0.490 [0.306, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.378, 10.559], loss: 0.003160, mae: 0.058157, mean_q: -0.193473
 59034/100000: episode: 976, duration: 0.139s, episode steps: 28, steps per second: 201, episode reward: 7.193, mean reward: 0.257 [0.129, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.035, 10.327], loss: 0.004015, mae: 0.065695, mean_q: -0.111800
 59064/100000: episode: 977, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 10.738, mean reward: 0.358 [0.226, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.602, 10.100], loss: 0.003428, mae: 0.060849, mean_q: -0.185997
 59084/100000: episode: 978, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 7.420, mean reward: 0.371 [0.252, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.212, 10.499], loss: 0.002610, mae: 0.052693, mean_q: -0.161115
 59119/100000: episode: 979, duration: 0.174s, episode steps: 35, steps per second: 201, episode reward: 14.467, mean reward: 0.413 [0.211, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.675, 10.100], loss: 0.003016, mae: 0.056449, mean_q: -0.078993
 59138/100000: episode: 980, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 4.308, mean reward: 0.227 [0.086, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.368, 10.100], loss: 0.002604, mae: 0.052081, mean_q: -0.080024
 59152/100000: episode: 981, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 3.374, mean reward: 0.241 [0.169, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.294, 10.100], loss: 0.002937, mae: 0.056033, mean_q: -0.092886
 59171/100000: episode: 982, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 5.469, mean reward: 0.288 [0.207, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.869, 10.100], loss: 0.002893, mae: 0.055287, mean_q: -0.136906
 59206/100000: episode: 983, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 8.565, mean reward: 0.245 [0.007, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.706, 10.100], loss: 0.002897, mae: 0.054994, mean_q: -0.119282
 59227/100000: episode: 984, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 6.578, mean reward: 0.313 [0.256, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.429], loss: 0.002942, mae: 0.055816, mean_q: -0.117841
 59247/100000: episode: 985, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 6.256, mean reward: 0.313 [0.223, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.428, 10.100], loss: 0.003178, mae: 0.059023, mean_q: -0.044293
 59279/100000: episode: 986, duration: 0.162s, episode steps: 32, steps per second: 198, episode reward: 11.896, mean reward: 0.372 [0.236, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.450, 10.521], loss: 0.003019, mae: 0.057255, mean_q: -0.072091
 59309/100000: episode: 987, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 9.642, mean reward: 0.321 [0.236, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.178, 10.100], loss: 0.002595, mae: 0.051905, mean_q: -0.058896
 59339/100000: episode: 988, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 7.208, mean reward: 0.240 [0.099, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.559, 10.100], loss: 0.002760, mae: 0.054465, mean_q: -0.095860
 59371/100000: episode: 989, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 13.055, mean reward: 0.408 [0.315, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.172, 10.606], loss: 0.003063, mae: 0.056893, mean_q: -0.073734
 59391/100000: episode: 990, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 4.633, mean reward: 0.232 [0.132, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.810, 10.237], loss: 0.003075, mae: 0.058248, mean_q: -0.089081
 59419/100000: episode: 991, duration: 0.137s, episode steps: 28, steps per second: 204, episode reward: 8.753, mean reward: 0.313 [0.164, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.391, 10.296], loss: 0.003050, mae: 0.056041, mean_q: -0.111109
 59447/100000: episode: 992, duration: 0.132s, episode steps: 28, steps per second: 212, episode reward: 9.414, mean reward: 0.336 [0.225, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.222, 10.407], loss: 0.002829, mae: 0.056161, mean_q: -0.025333
 59467/100000: episode: 993, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 6.402, mean reward: 0.320 [0.228, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.532, 10.430], loss: 0.003592, mae: 0.061952, mean_q: -0.037424
 59486/100000: episode: 994, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 5.687, mean reward: 0.299 [0.221, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.344, 10.100], loss: 0.002761, mae: 0.055901, mean_q: -0.079977
 59516/100000: episode: 995, duration: 0.153s, episode steps: 30, steps per second: 197, episode reward: 8.636, mean reward: 0.288 [0.155, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.610, 10.100], loss: 0.002885, mae: 0.053608, mean_q: -0.116247
 59535/100000: episode: 996, duration: 0.097s, episode steps: 19, steps per second: 197, episode reward: 4.441, mean reward: 0.234 [0.104, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.199, 10.100], loss: 0.002907, mae: 0.053790, mean_q: -0.157214
 59565/100000: episode: 997, duration: 0.150s, episode steps: 30, steps per second: 199, episode reward: 6.185, mean reward: 0.206 [0.072, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.496, 10.100], loss: 0.002908, mae: 0.056165, mean_q: -0.004455
 59595/100000: episode: 998, duration: 0.160s, episode steps: 30, steps per second: 188, episode reward: 9.025, mean reward: 0.301 [0.156, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.163, 10.100], loss: 0.002910, mae: 0.054733, mean_q: -0.056458
 59616/100000: episode: 999, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 8.467, mean reward: 0.403 [0.320, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.251, 10.421], loss: 0.002997, mae: 0.057438, mean_q: 0.018596
 59630/100000: episode: 1000, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 3.890, mean reward: 0.278 [0.208, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.104, 10.100], loss: 0.003038, mae: 0.057417, mean_q: 0.011033
 59650/100000: episode: 1001, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 4.673, mean reward: 0.234 [0.120, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.234, 10.100], loss: 0.002781, mae: 0.054114, mean_q: -0.093316
 59671/100000: episode: 1002, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 8.737, mean reward: 0.416 [0.309, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.282, 10.558], loss: 0.002987, mae: 0.057576, mean_q: -0.068247
 59690/100000: episode: 1003, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 4.677, mean reward: 0.246 [0.110, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.631, 10.100], loss: 0.002843, mae: 0.055434, mean_q: 0.038242
 59709/100000: episode: 1004, duration: 0.097s, episode steps: 19, steps per second: 197, episode reward: 6.882, mean reward: 0.362 [0.286, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.337, 10.100], loss: 0.002406, mae: 0.051044, mean_q: -0.064665
 59723/100000: episode: 1005, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 4.341, mean reward: 0.310 [0.259, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.269, 10.100], loss: 0.002728, mae: 0.054509, mean_q: -0.013757
 59744/100000: episode: 1006, duration: 0.104s, episode steps: 21, steps per second: 203, episode reward: 8.790, mean reward: 0.419 [0.326, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.553], loss: 0.002756, mae: 0.054402, mean_q: -0.024575
 59776/100000: episode: 1007, duration: 0.159s, episode steps: 32, steps per second: 202, episode reward: 12.411, mean reward: 0.388 [0.259, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.545, 10.446], loss: 0.003322, mae: 0.061132, mean_q: 0.062174
 59796/100000: episode: 1008, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 7.466, mean reward: 0.373 [0.219, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.914, 10.325], loss: 0.002812, mae: 0.057599, mean_q: 0.011620
 59826/100000: episode: 1009, duration: 0.147s, episode steps: 30, steps per second: 204, episode reward: 7.042, mean reward: 0.235 [0.128, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.463, 10.100], loss: 0.003282, mae: 0.059786, mean_q: 0.071610
 59846/100000: episode: 1010, duration: 0.124s, episode steps: 20, steps per second: 161, episode reward: 7.718, mean reward: 0.386 [0.214, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.401, 10.100], loss: 0.002566, mae: 0.052542, mean_q: -0.065961
 59866/100000: episode: 1011, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 6.184, mean reward: 0.309 [0.235, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.266, 10.435], loss: 0.002849, mae: 0.056054, mean_q: 0.030995
 59896/100000: episode: 1012, duration: 0.166s, episode steps: 30, steps per second: 180, episode reward: 9.130, mean reward: 0.304 [0.227, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.247, 10.100], loss: 0.002911, mae: 0.055289, mean_q: 0.002536
 59931/100000: episode: 1013, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 9.465, mean reward: 0.270 [0.122, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.153, 10.100], loss: 0.003108, mae: 0.058123, mean_q: 0.031092
 59959/100000: episode: 1014, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 9.928, mean reward: 0.355 [0.230, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.406, 10.457], loss: 0.002897, mae: 0.055684, mean_q: -0.026893
 59994/100000: episode: 1015, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 10.736, mean reward: 0.307 [0.126, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.798, 10.100], loss: 0.002708, mae: 0.054680, mean_q: 0.047035
 60014/100000: episode: 1016, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 6.200, mean reward: 0.310 [0.203, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.793, 10.100], loss: 0.003045, mae: 0.058430, mean_q: 0.090749
 60042/100000: episode: 1017, duration: 0.135s, episode steps: 28, steps per second: 207, episode reward: 7.547, mean reward: 0.270 [0.143, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.476, 10.322], loss: 0.002879, mae: 0.055801, mean_q: 0.032596
 60074/100000: episode: 1018, duration: 0.157s, episode steps: 32, steps per second: 204, episode reward: 13.077, mean reward: 0.409 [0.287, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.259, 10.583], loss: 0.003616, mae: 0.063866, mean_q: 0.044021
 60088/100000: episode: 1019, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 4.090, mean reward: 0.292 [0.179, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.285, 10.100], loss: 0.002697, mae: 0.055753, mean_q: 0.018870
 60120/100000: episode: 1020, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 12.242, mean reward: 0.383 [0.250, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.448, 10.361], loss: 0.002798, mae: 0.056329, mean_q: 0.069608
 60139/100000: episode: 1021, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 6.162, mean reward: 0.324 [0.234, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.367, 10.100], loss: 0.002956, mae: 0.057385, mean_q: 0.059265
 60158/100000: episode: 1022, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 5.348, mean reward: 0.281 [0.208, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.370, 10.100], loss: 0.002586, mae: 0.052866, mean_q: 0.093097
 60190/100000: episode: 1023, duration: 0.149s, episode steps: 32, steps per second: 214, episode reward: 12.548, mean reward: 0.392 [0.259, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.831, 10.547], loss: 0.002965, mae: 0.056526, mean_q: 0.111756
 60209/100000: episode: 1024, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 5.893, mean reward: 0.310 [0.242, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.445, 10.100], loss: 0.002405, mae: 0.052773, mean_q: 0.151763
 60229/100000: episode: 1025, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 5.941, mean reward: 0.297 [0.220, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.519, 10.444], loss: 0.002824, mae: 0.054590, mean_q: 0.068815
 60261/100000: episode: 1026, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 11.900, mean reward: 0.372 [0.270, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.452, 10.563], loss: 0.002903, mae: 0.057283, mean_q: 0.067487
[Info] 200-TH LEVEL FOUND: 0.8253436088562012, Considering 10/90 traces
 60290/100000: episode: 1027, duration: 4.026s, episode steps: 29, steps per second: 7, episode reward: 8.016, mean reward: 0.276 [0.140, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.108, 10.246], loss: 0.003169, mae: 0.058883, mean_q: 0.088281
 60307/100000: episode: 1028, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 6.944, mean reward: 0.408 [0.343, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.603], loss: 0.003117, mae: 0.058950, mean_q: 0.170915
 60336/100000: episode: 1029, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 11.162, mean reward: 0.385 [0.220, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.220, 10.100], loss: 0.003298, mae: 0.060979, mean_q: 0.104685
 60353/100000: episode: 1030, duration: 0.100s, episode steps: 17, steps per second: 169, episode reward: 7.682, mean reward: 0.452 [0.365, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.531, 10.548], loss: 0.003052, mae: 0.058449, mean_q: 0.148687
 60370/100000: episode: 1031, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 7.055, mean reward: 0.415 [0.324, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.323, 10.100], loss: 0.003094, mae: 0.060783, mean_q: 0.110767
 60385/100000: episode: 1032, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 6.325, mean reward: 0.422 [0.342, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.441, 10.516], loss: 0.002888, mae: 0.056441, mean_q: 0.119046
 60400/100000: episode: 1033, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 5.684, mean reward: 0.379 [0.271, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.454], loss: 0.002706, mae: 0.053657, mean_q: 0.121760
 60409/100000: episode: 1034, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 3.566, mean reward: 0.396 [0.335, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.358, 10.100], loss: 0.003354, mae: 0.061385, mean_q: 0.151708
 60420/100000: episode: 1035, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 4.190, mean reward: 0.381 [0.294, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.411, 10.100], loss: 0.002922, mae: 0.056136, mean_q: 0.165554
 60449/100000: episode: 1036, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 11.413, mean reward: 0.394 [0.140, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.228, 10.100], loss: 0.002728, mae: 0.055258, mean_q: 0.174131
 60464/100000: episode: 1037, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 6.965, mean reward: 0.464 [0.378, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.592, 10.577], loss: 0.003460, mae: 0.062833, mean_q: 0.121803
 60481/100000: episode: 1038, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 7.081, mean reward: 0.417 [0.336, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.661, 10.619], loss: 0.002967, mae: 0.058832, mean_q: 0.109861
 60494/100000: episode: 1039, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 5.477, mean reward: 0.421 [0.321, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.150, 10.435], loss: 0.003279, mae: 0.062307, mean_q: 0.189704
 60511/100000: episode: 1040, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 6.553, mean reward: 0.385 [0.273, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.375, 10.100], loss: 0.003088, mae: 0.057703, mean_q: 0.159101
 60524/100000: episode: 1041, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 5.768, mean reward: 0.444 [0.390, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.913, 10.534], loss: 0.003077, mae: 0.056057, mean_q: 0.152133
 60553/100000: episode: 1042, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 13.752, mean reward: 0.474 [0.366, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.623, 10.100], loss: 0.003225, mae: 0.060408, mean_q: 0.174257
 60562/100000: episode: 1043, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 3.279, mean reward: 0.364 [0.297, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.736, 10.100], loss: 0.003870, mae: 0.066521, mean_q: 0.162175
 60577/100000: episode: 1044, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 6.190, mean reward: 0.413 [0.284, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.067, 10.423], loss: 0.003329, mae: 0.059832, mean_q: 0.152302
 60586/100000: episode: 1045, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 2.987, mean reward: 0.332 [0.286, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.408, 10.100], loss: 0.002248, mae: 0.050522, mean_q: 0.148062
 60595/100000: episode: 1046, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 3.297, mean reward: 0.366 [0.299, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.436, 10.100], loss: 0.003717, mae: 0.064793, mean_q: 0.212759
 60606/100000: episode: 1047, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 3.874, mean reward: 0.352 [0.294, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.436, 10.100], loss: 0.002689, mae: 0.054904, mean_q: 0.233747
 60627/100000: episode: 1048, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 7.952, mean reward: 0.379 [0.204, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.383], loss: 0.002642, mae: 0.053009, mean_q: 0.153492
 60656/100000: episode: 1049, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 12.061, mean reward: 0.416 [0.274, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.382, 10.100], loss: 0.002908, mae: 0.057246, mean_q: 0.181945
 60671/100000: episode: 1050, duration: 0.072s, episode steps: 15, steps per second: 209, episode reward: 5.642, mean reward: 0.376 [0.267, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.909, 10.587], loss: 0.003242, mae: 0.060060, mean_q: 0.216817
 60680/100000: episode: 1051, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 3.827, mean reward: 0.425 [0.301, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.438, 10.100], loss: 0.003545, mae: 0.061804, mean_q: 0.119839
 60689/100000: episode: 1052, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 2.666, mean reward: 0.296 [0.210, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.312, 10.100], loss: 0.003232, mae: 0.060572, mean_q: 0.175796
 60701/100000: episode: 1053, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 4.544, mean reward: 0.379 [0.328, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.230, 10.100], loss: 0.002671, mae: 0.054431, mean_q: 0.091069
 60716/100000: episode: 1054, duration: 0.073s, episode steps: 15, steps per second: 206, episode reward: 6.183, mean reward: 0.412 [0.316, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.472], loss: 0.003098, mae: 0.057939, mean_q: 0.137020
 60733/100000: episode: 1055, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 8.642, mean reward: 0.508 [0.472, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.632], loss: 0.002889, mae: 0.056081, mean_q: 0.272745
 60754/100000: episode: 1056, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 8.120, mean reward: 0.387 [0.281, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.244, 10.402], loss: 0.003236, mae: 0.060401, mean_q: 0.210702
 60771/100000: episode: 1057, duration: 0.082s, episode steps: 17, steps per second: 206, episode reward: 6.461, mean reward: 0.380 [0.291, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.432], loss: 0.002740, mae: 0.056123, mean_q: 0.186879
 60800/100000: episode: 1058, duration: 0.146s, episode steps: 29, steps per second: 198, episode reward: 6.823, mean reward: 0.235 [0.033, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.317, 10.100], loss: 0.003107, mae: 0.059547, mean_q: 0.242181
 60817/100000: episode: 1059, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 7.438, mean reward: 0.438 [0.380, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.331, 10.100], loss: 0.002925, mae: 0.056407, mean_q: 0.147127
 60834/100000: episode: 1060, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 7.045, mean reward: 0.414 [0.349, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.302, 10.100], loss: 0.002758, mae: 0.055445, mean_q: 0.253059
 60849/100000: episode: 1061, duration: 0.073s, episode steps: 15, steps per second: 204, episode reward: 6.049, mean reward: 0.403 [0.332, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.788, 10.420], loss: 0.003284, mae: 0.059014, mean_q: 0.143281
 60858/100000: episode: 1062, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 3.501, mean reward: 0.389 [0.349, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.346, 10.100], loss: 0.003076, mae: 0.058028, mean_q: 0.165654
 60871/100000: episode: 1063, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 5.398, mean reward: 0.415 [0.322, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.478], loss: 0.002975, mae: 0.058413, mean_q: 0.253329
 60892/100000: episode: 1064, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 8.986, mean reward: 0.428 [0.315, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-1.869, 10.482], loss: 0.002567, mae: 0.054562, mean_q: 0.197556
 60901/100000: episode: 1065, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 3.226, mean reward: 0.358 [0.304, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.680, 10.100], loss: 0.002688, mae: 0.054643, mean_q: 0.283673
 60912/100000: episode: 1066, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 4.218, mean reward: 0.383 [0.362, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.252, 10.100], loss: 0.002554, mae: 0.051057, mean_q: 0.112512
 60941/100000: episode: 1067, duration: 0.159s, episode steps: 29, steps per second: 183, episode reward: 11.141, mean reward: 0.384 [0.214, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.530, 10.100], loss: 0.003119, mae: 0.059384, mean_q: 0.267219
 60962/100000: episode: 1068, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 8.200, mean reward: 0.390 [0.299, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.313, 10.417], loss: 0.003075, mae: 0.057914, mean_q: 0.288179
 60973/100000: episode: 1069, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 4.000, mean reward: 0.364 [0.234, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.372, 10.100], loss: 0.003222, mae: 0.061758, mean_q: 0.240299
 60990/100000: episode: 1070, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 7.290, mean reward: 0.429 [0.350, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.946, 10.440], loss: 0.002899, mae: 0.058285, mean_q: 0.271985
 61001/100000: episode: 1071, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 4.397, mean reward: 0.400 [0.358, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.352, 10.100], loss: 0.003049, mae: 0.059229, mean_q: 0.294905
 61030/100000: episode: 1072, duration: 0.152s, episode steps: 29, steps per second: 190, episode reward: 11.633, mean reward: 0.401 [0.206, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.298, 10.100], loss: 0.003138, mae: 0.059600, mean_q: 0.265428
 61039/100000: episode: 1073, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 2.941, mean reward: 0.327 [0.255, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.276, 10.100], loss: 0.003380, mae: 0.062029, mean_q: 0.336685
 61068/100000: episode: 1074, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 13.584, mean reward: 0.468 [0.375, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.480, 10.100], loss: 0.003209, mae: 0.060223, mean_q: 0.304437
 61083/100000: episode: 1075, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 5.905, mean reward: 0.394 [0.326, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.502], loss: 0.003080, mae: 0.059132, mean_q: 0.245271
 61112/100000: episode: 1076, duration: 0.150s, episode steps: 29, steps per second: 194, episode reward: 10.056, mean reward: 0.347 [0.128, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.139, 10.100], loss: 0.003541, mae: 0.062935, mean_q: 0.290624
 61123/100000: episode: 1077, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 4.688, mean reward: 0.426 [0.293, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.790, 10.100], loss: 0.004029, mae: 0.069731, mean_q: 0.346932
 61144/100000: episode: 1078, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 8.012, mean reward: 0.382 [0.177, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.429], loss: 0.003044, mae: 0.060743, mean_q: 0.314831
 61173/100000: episode: 1079, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 12.825, mean reward: 0.442 [0.260, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.231, 10.100], loss: 0.003227, mae: 0.059931, mean_q: 0.250874
 61186/100000: episode: 1080, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 6.219, mean reward: 0.478 [0.440, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.609], loss: 0.003383, mae: 0.062802, mean_q: 0.280700
 61215/100000: episode: 1081, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 8.533, mean reward: 0.294 [0.027, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.430, 10.177], loss: 0.003115, mae: 0.059012, mean_q: 0.270232
 61230/100000: episode: 1082, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 5.707, mean reward: 0.380 [0.326, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.490], loss: 0.003605, mae: 0.064748, mean_q: 0.326106
 61239/100000: episode: 1083, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 3.056, mean reward: 0.340 [0.317, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.415, 10.100], loss: 0.003083, mae: 0.057894, mean_q: 0.357005
 61256/100000: episode: 1084, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 6.518, mean reward: 0.383 [0.295, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.599, 10.100], loss: 0.003232, mae: 0.060323, mean_q: 0.333470
 61273/100000: episode: 1085, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 7.784, mean reward: 0.458 [0.377, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.114, 10.610], loss: 0.003267, mae: 0.060322, mean_q: 0.367928
 61284/100000: episode: 1086, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 3.913, mean reward: 0.356 [0.234, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.703, 10.100], loss: 0.003011, mae: 0.057201, mean_q: 0.258927
 61313/100000: episode: 1087, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 10.572, mean reward: 0.365 [0.199, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.138, 10.100], loss: 0.002976, mae: 0.056656, mean_q: 0.282180
 61322/100000: episode: 1088, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 3.954, mean reward: 0.439 [0.313, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.480, 10.100], loss: 0.003018, mae: 0.058615, mean_q: 0.307648
 61331/100000: episode: 1089, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 3.766, mean reward: 0.418 [0.326, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.441, 10.100], loss: 0.003432, mae: 0.063263, mean_q: 0.403564
 61360/100000: episode: 1090, duration: 0.140s, episode steps: 29, steps per second: 207, episode reward: 10.067, mean reward: 0.347 [0.206, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.151, 10.100], loss: 0.002912, mae: 0.057541, mean_q: 0.327524
 61369/100000: episode: 1091, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 3.256, mean reward: 0.362 [0.281, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.882, 10.100], loss: 0.004124, mae: 0.071541, mean_q: 0.355041
 61384/100000: episode: 1092, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 6.027, mean reward: 0.402 [0.309, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.126, 10.478], loss: 0.003356, mae: 0.062030, mean_q: 0.374288
 61413/100000: episode: 1093, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 9.377, mean reward: 0.323 [0.086, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.753, 10.100], loss: 0.003270, mae: 0.061280, mean_q: 0.358980
 61430/100000: episode: 1094, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 5.471, mean reward: 0.322 [0.156, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.324, 10.100], loss: 0.003022, mae: 0.058765, mean_q: 0.355164
 61439/100000: episode: 1095, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 3.370, mean reward: 0.374 [0.310, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.286, 10.100], loss: 0.002849, mae: 0.058567, mean_q: 0.393399
 61468/100000: episode: 1096, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 12.369, mean reward: 0.427 [0.299, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.389, 10.100], loss: 0.002807, mae: 0.056539, mean_q: 0.341553
 61485/100000: episode: 1097, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 6.236, mean reward: 0.367 [0.225, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.542, 10.100], loss: 0.002993, mae: 0.058255, mean_q: 0.383564
 61497/100000: episode: 1098, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 4.350, mean reward: 0.362 [0.335, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.249, 10.100], loss: 0.003386, mae: 0.063284, mean_q: 0.367165
 61509/100000: episode: 1099, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 5.075, mean reward: 0.423 [0.377, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.354, 10.100], loss: 0.002453, mae: 0.052498, mean_q: 0.360190
 61518/100000: episode: 1100, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 3.346, mean reward: 0.372 [0.284, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.494, 10.100], loss: 0.003125, mae: 0.055340, mean_q: 0.396568
 61531/100000: episode: 1101, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 6.537, mean reward: 0.503 [0.455, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.668, 10.575], loss: 0.003437, mae: 0.063953, mean_q: 0.379645
 61548/100000: episode: 1102, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 7.687, mean reward: 0.452 [0.347, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.744, 10.100], loss: 0.003051, mae: 0.060298, mean_q: 0.419838
 61569/100000: episode: 1103, duration: 0.100s, episode steps: 21, steps per second: 209, episode reward: 9.750, mean reward: 0.464 [0.307, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.379, 10.454], loss: 0.003197, mae: 0.058804, mean_q: 0.331234
 61584/100000: episode: 1104, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 5.851, mean reward: 0.390 [0.326, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.750, 10.407], loss: 0.003651, mae: 0.066434, mean_q: 0.372071
[Info] FALSIFICATION!
 61585/100000: episode: 1105, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.004, 9.484], loss: 0.005567, mae: 0.078996, mean_q: 0.245856
 61685/100000: episode: 1106, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.691, mean reward: -0.167 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.072, 10.098], loss: 0.017879, mae: 0.075132, mean_q: 0.373027
 61785/100000: episode: 1107, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.962, mean reward: -0.190 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.477, 10.098], loss: 0.003050, mae: 0.059612, mean_q: 0.366962
 61885/100000: episode: 1108, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.010, mean reward: -0.140 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.540, 10.098], loss: 0.018657, mae: 0.080383, mean_q: 0.381259
 61985/100000: episode: 1109, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -17.456, mean reward: -0.175 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.885, 10.098], loss: 0.018348, mae: 0.076910, mean_q: 0.368998
 62085/100000: episode: 1110, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.068, mean reward: -0.181 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.780, 10.299], loss: 0.003018, mae: 0.058567, mean_q: 0.366142
 62185/100000: episode: 1111, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.814, mean reward: -0.178 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.586, 10.305], loss: 0.017569, mae: 0.073866, mean_q: 0.392391
 62285/100000: episode: 1112, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.694, mean reward: -0.147 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.717, 10.266], loss: 0.005570, mae: 0.067558, mean_q: 0.385230
 62385/100000: episode: 1113, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.379, mean reward: -0.154 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.608, 10.176], loss: 0.017485, mae: 0.071692, mean_q: 0.389582
 62485/100000: episode: 1114, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -16.106, mean reward: -0.161 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.299, 10.416], loss: 0.003299, mae: 0.061028, mean_q: 0.365577
 62585/100000: episode: 1115, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -14.871, mean reward: -0.149 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.941, 10.284], loss: 0.018028, mae: 0.075262, mean_q: 0.356781
 62685/100000: episode: 1116, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -18.545, mean reward: -0.185 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.264, 10.098], loss: 0.017864, mae: 0.073298, mean_q: 0.379598
 62785/100000: episode: 1117, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.941, mean reward: -0.149 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.694, 10.235], loss: 0.003767, mae: 0.064263, mean_q: 0.379119
 62885/100000: episode: 1118, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.043, mean reward: -0.140 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.519, 10.098], loss: 0.003449, mae: 0.062945, mean_q: 0.364986
 62985/100000: episode: 1119, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.075, mean reward: -0.151 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.383, 10.098], loss: 0.003414, mae: 0.062194, mean_q: 0.385024
 63085/100000: episode: 1120, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -13.339, mean reward: -0.133 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.720, 10.098], loss: 0.003221, mae: 0.060691, mean_q: 0.361858
 63185/100000: episode: 1121, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.104, mean reward: -0.191 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.072, 10.154], loss: 0.030406, mae: 0.075304, mean_q: 0.358837
 63285/100000: episode: 1122, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.206, mean reward: -0.152 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.914, 10.199], loss: 0.003775, mae: 0.062621, mean_q: 0.336427
 63385/100000: episode: 1123, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.388, mean reward: -0.144 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.224, 10.098], loss: 0.056898, mae: 0.088956, mean_q: 0.338709
 63485/100000: episode: 1124, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -11.639, mean reward: -0.116 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.791, 10.306], loss: 0.003275, mae: 0.060250, mean_q: 0.314165
 63585/100000: episode: 1125, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.185, mean reward: -0.172 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.202, 10.098], loss: 0.003173, mae: 0.060010, mean_q: 0.276351
 63685/100000: episode: 1126, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -17.606, mean reward: -0.176 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.342, 10.098], loss: 0.017068, mae: 0.070218, mean_q: 0.276944
 63785/100000: episode: 1127, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -16.945, mean reward: -0.169 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.642, 10.098], loss: 0.031915, mae: 0.086450, mean_q: 0.249126
 63885/100000: episode: 1128, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.394, mean reward: -0.164 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.027, 10.098], loss: 0.003209, mae: 0.061386, mean_q: 0.196969
 63985/100000: episode: 1129, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -18.475, mean reward: -0.185 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.345, 10.098], loss: 0.003133, mae: 0.059107, mean_q: 0.233972
 64085/100000: episode: 1130, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -14.237, mean reward: -0.142 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.672, 10.215], loss: 0.002945, mae: 0.057210, mean_q: 0.171777
 64185/100000: episode: 1131, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -14.121, mean reward: -0.141 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.752, 10.160], loss: 0.017271, mae: 0.069954, mean_q: 0.180736
 64285/100000: episode: 1132, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.840, mean reward: -0.188 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.782, 10.098], loss: 0.003071, mae: 0.057365, mean_q: 0.116625
 64385/100000: episode: 1133, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -20.158, mean reward: -0.202 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.469, 10.098], loss: 0.016982, mae: 0.066992, mean_q: 0.141708
 64485/100000: episode: 1134, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.217, mean reward: -0.152 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.147, 10.098], loss: 0.002894, mae: 0.055528, mean_q: 0.067994
 64585/100000: episode: 1135, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.840, mean reward: -0.188 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.127, 10.098], loss: 0.016901, mae: 0.067268, mean_q: 0.057563
 64685/100000: episode: 1136, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -16.225, mean reward: -0.162 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.941, 10.098], loss: 0.003295, mae: 0.058990, mean_q: 0.043910
 64785/100000: episode: 1137, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.362, mean reward: -0.164 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.342, 10.173], loss: 0.002965, mae: 0.055720, mean_q: 0.038995
 64885/100000: episode: 1138, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -16.155, mean reward: -0.162 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.573, 10.250], loss: 0.016912, mae: 0.066518, mean_q: 0.027058
 64985/100000: episode: 1139, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -4.841, mean reward: -0.048 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.320, 10.425], loss: 0.003508, mae: 0.061567, mean_q: 0.004260
 65085/100000: episode: 1140, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -18.964, mean reward: -0.190 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.836, 10.098], loss: 0.003333, mae: 0.059524, mean_q: -0.020228
 65185/100000: episode: 1141, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.125, mean reward: -0.181 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.889, 10.098], loss: 0.002782, mae: 0.053272, mean_q: -0.046283
 65285/100000: episode: 1142, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -12.799, mean reward: -0.128 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.545, 10.207], loss: 0.003091, mae: 0.056996, mean_q: -0.053664
 65385/100000: episode: 1143, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -15.738, mean reward: -0.157 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.169, 10.198], loss: 0.003011, mae: 0.056141, mean_q: -0.068289
 65485/100000: episode: 1144, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.831, mean reward: -0.148 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.107, 10.258], loss: 0.003043, mae: 0.056217, mean_q: -0.101854
 65585/100000: episode: 1145, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.019, mean reward: -0.190 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.789, 10.098], loss: 0.016656, mae: 0.063695, mean_q: -0.076857
 65685/100000: episode: 1146, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.127, mean reward: -0.171 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.933, 10.225], loss: 0.017378, mae: 0.068155, mean_q: -0.109735
 65785/100000: episode: 1147, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.322, mean reward: -0.173 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.934, 10.195], loss: 0.031036, mae: 0.076017, mean_q: -0.127095
 65885/100000: episode: 1148, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.766, mean reward: -0.188 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.720, 10.098], loss: 0.003009, mae: 0.055894, mean_q: -0.130300
 65985/100000: episode: 1149, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -18.912, mean reward: -0.189 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.105, 10.098], loss: 0.002967, mae: 0.055214, mean_q: -0.195476
 66085/100000: episode: 1150, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.098, mean reward: -0.181 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.027, 10.098], loss: 0.002882, mae: 0.053528, mean_q: -0.211801
 66185/100000: episode: 1151, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.221, mean reward: -0.172 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.070, 10.107], loss: 0.030689, mae: 0.072780, mean_q: -0.238403
 66285/100000: episode: 1152, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.075, mean reward: -0.181 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.345, 10.098], loss: 0.017096, mae: 0.067523, mean_q: -0.243698
 66385/100000: episode: 1153, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.463, mean reward: -0.195 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.720, 10.098], loss: 0.003041, mae: 0.055341, mean_q: -0.245633
 66485/100000: episode: 1154, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -17.299, mean reward: -0.173 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.252, 10.098], loss: 0.002761, mae: 0.052461, mean_q: -0.287680
 66585/100000: episode: 1155, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.992, mean reward: -0.170 [-1.000, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.973, 10.098], loss: 0.002763, mae: 0.052231, mean_q: -0.301924
 66685/100000: episode: 1156, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.234, mean reward: -0.172 [-1.000, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.194, 10.098], loss: 0.002853, mae: 0.052836, mean_q: -0.308414
 66785/100000: episode: 1157, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.021, mean reward: -0.170 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.994, 10.098], loss: 0.002934, mae: 0.054006, mean_q: -0.315720
 66885/100000: episode: 1158, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.184, mean reward: -0.152 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.501, 10.098], loss: 0.002891, mae: 0.052744, mean_q: -0.328344
 66985/100000: episode: 1159, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -20.029, mean reward: -0.200 [-1.000, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.778, 10.201], loss: 0.003001, mae: 0.054383, mean_q: -0.318957
 67085/100000: episode: 1160, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.431, mean reward: -0.184 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.833, 10.243], loss: 0.002991, mae: 0.054333, mean_q: -0.299013
 67185/100000: episode: 1161, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -18.177, mean reward: -0.182 [-1.000, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.126, 10.098], loss: 0.002868, mae: 0.054252, mean_q: -0.290694
 67285/100000: episode: 1162, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -11.649, mean reward: -0.116 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.330, 10.098], loss: 0.003236, mae: 0.056605, mean_q: -0.332504
 67385/100000: episode: 1163, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.640, mean reward: -0.166 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.050, 10.158], loss: 0.003715, mae: 0.060855, mean_q: -0.297404
 67485/100000: episode: 1164, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.298, mean reward: -0.193 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.406, 10.098], loss: 0.003049, mae: 0.053887, mean_q: -0.320321
 67585/100000: episode: 1165, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.176, mean reward: -0.182 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.465, 10.098], loss: 0.002916, mae: 0.052836, mean_q: -0.329628
 67685/100000: episode: 1166, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -15.933, mean reward: -0.159 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.966, 10.098], loss: 0.002796, mae: 0.051779, mean_q: -0.333582
 67785/100000: episode: 1167, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -16.955, mean reward: -0.170 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.982, 10.183], loss: 0.002897, mae: 0.053674, mean_q: -0.307720
 67885/100000: episode: 1168, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.513, mean reward: -0.185 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.478, 10.173], loss: 0.002790, mae: 0.052861, mean_q: -0.327334
 67985/100000: episode: 1169, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.349, mean reward: -0.173 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.309, 10.169], loss: 0.002867, mae: 0.052156, mean_q: -0.316456
 68085/100000: episode: 1170, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -15.709, mean reward: -0.157 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.617, 10.429], loss: 0.002881, mae: 0.054031, mean_q: -0.329194
 68185/100000: episode: 1171, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -20.463, mean reward: -0.205 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.848, 10.344], loss: 0.003084, mae: 0.054810, mean_q: -0.331150
 68285/100000: episode: 1172, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -18.236, mean reward: -0.182 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.948, 10.098], loss: 0.002903, mae: 0.053551, mean_q: -0.276561
 68385/100000: episode: 1173, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.857, mean reward: -0.189 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.386, 10.098], loss: 0.002932, mae: 0.054086, mean_q: -0.309365
 68485/100000: episode: 1174, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -18.366, mean reward: -0.184 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.037, 10.098], loss: 0.002752, mae: 0.051061, mean_q: -0.348066
 68585/100000: episode: 1175, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.218, mean reward: -0.172 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.939, 10.159], loss: 0.003153, mae: 0.055276, mean_q: -0.303099
 68685/100000: episode: 1176, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.742, mean reward: -0.197 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.553, 10.098], loss: 0.004612, mae: 0.065150, mean_q: -0.326500
 68785/100000: episode: 1177, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -13.961, mean reward: -0.140 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.748, 10.098], loss: 0.003108, mae: 0.055431, mean_q: -0.312499
 68885/100000: episode: 1178, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -17.206, mean reward: -0.172 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.283, 10.131], loss: 0.002901, mae: 0.052540, mean_q: -0.300701
 68985/100000: episode: 1179, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.170, mean reward: -0.162 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.950, 10.338], loss: 0.002882, mae: 0.052804, mean_q: -0.303972
 69085/100000: episode: 1180, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.134, mean reward: -0.191 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.016, 10.331], loss: 0.002712, mae: 0.051528, mean_q: -0.300535
 69185/100000: episode: 1181, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.142, mean reward: -0.161 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.495, 10.218], loss: 0.002957, mae: 0.053698, mean_q: -0.305124
 69285/100000: episode: 1182, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -11.561, mean reward: -0.116 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.294, 10.350], loss: 0.002651, mae: 0.049979, mean_q: -0.358164
 69385/100000: episode: 1183, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.976, mean reward: -0.200 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.411, 10.098], loss: 0.003021, mae: 0.055098, mean_q: -0.306200
 69485/100000: episode: 1184, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -13.374, mean reward: -0.134 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.325, 10.253], loss: 0.002836, mae: 0.053001, mean_q: -0.292348
 69585/100000: episode: 1185, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.747, mean reward: -0.167 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.912, 10.098], loss: 0.002851, mae: 0.053352, mean_q: -0.288755
 69685/100000: episode: 1186, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.912, mean reward: -0.179 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.662, 10.238], loss: 0.002913, mae: 0.052843, mean_q: -0.320648
 69785/100000: episode: 1187, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.173, mean reward: -0.172 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.409, 10.098], loss: 0.002813, mae: 0.051662, mean_q: -0.309881
 69885/100000: episode: 1188, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.551, mean reward: -0.146 [-1.000, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.367, 10.098], loss: 0.002913, mae: 0.054534, mean_q: -0.281080
 69985/100000: episode: 1189, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -20.540, mean reward: -0.205 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.620, 10.098], loss: 0.002971, mae: 0.053773, mean_q: -0.315316
 70085/100000: episode: 1190, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -10.053, mean reward: -0.101 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.928, 10.420], loss: 0.002615, mae: 0.050902, mean_q: -0.317196
 70185/100000: episode: 1191, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -19.966, mean reward: -0.200 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.509, 10.206], loss: 0.002755, mae: 0.051495, mean_q: -0.337316
 70285/100000: episode: 1192, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.950, mean reward: -0.189 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.280, 10.184], loss: 0.003855, mae: 0.059814, mean_q: -0.305382
 70385/100000: episode: 1193, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -15.417, mean reward: -0.154 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.791, 10.135], loss: 0.002826, mae: 0.053324, mean_q: -0.313790
 70485/100000: episode: 1194, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.664, mean reward: -0.187 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.168, 10.258], loss: 0.002864, mae: 0.052539, mean_q: -0.314091
 70585/100000: episode: 1195, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.709, mean reward: -0.187 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.552, 10.098], loss: 0.002799, mae: 0.052680, mean_q: -0.297214
 70685/100000: episode: 1196, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.322, mean reward: -0.183 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.602, 10.286], loss: 0.002742, mae: 0.051993, mean_q: -0.318539
 70785/100000: episode: 1197, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.565, mean reward: -0.196 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.745, 10.113], loss: 0.002601, mae: 0.050695, mean_q: -0.308315
 70885/100000: episode: 1198, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.873, mean reward: -0.169 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.777, 10.138], loss: 0.002599, mae: 0.050539, mean_q: -0.332722
 70985/100000: episode: 1199, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.203, mean reward: -0.182 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.550, 10.098], loss: 0.002718, mae: 0.051523, mean_q: -0.362606
 71085/100000: episode: 1200, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -14.272, mean reward: -0.143 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.222, 10.098], loss: 0.002945, mae: 0.054130, mean_q: -0.336673
 71185/100000: episode: 1201, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -17.912, mean reward: -0.179 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.887, 10.098], loss: 0.002903, mae: 0.053022, mean_q: -0.343539
 71285/100000: episode: 1202, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.650, mean reward: -0.176 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.721, 10.357], loss: 0.002668, mae: 0.050820, mean_q: -0.345735
 71385/100000: episode: 1203, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -12.664, mean reward: -0.127 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.623, 10.098], loss: 0.002879, mae: 0.052707, mean_q: -0.342868
 71485/100000: episode: 1204, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.240, mean reward: -0.162 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.391, 10.098], loss: 0.002808, mae: 0.052478, mean_q: -0.331302
[Info] 100-TH LEVEL FOUND: 0.5713690519332886, Considering 10/90 traces
 71585/100000: episode: 1205, duration: 4.373s, episode steps: 100, steps per second: 23, episode reward: -20.778, mean reward: -0.208 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.375, 10.187], loss: 0.002823, mae: 0.053795, mean_q: -0.314366
 71611/100000: episode: 1206, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 7.781, mean reward: 0.299 [0.221, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.496, 10.429], loss: 0.002862, mae: 0.052417, mean_q: -0.302584
 71620/100000: episode: 1207, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 3.436, mean reward: 0.382 [0.293, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.472, 10.100], loss: 0.002277, mae: 0.049907, mean_q: -0.220452
 71645/100000: episode: 1208, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 5.699, mean reward: 0.228 [0.090, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.694, 10.288], loss: 0.002728, mae: 0.052189, mean_q: -0.246359
 71670/100000: episode: 1209, duration: 0.129s, episode steps: 25, steps per second: 193, episode reward: 8.012, mean reward: 0.320 [0.243, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.562, 10.429], loss: 0.002416, mae: 0.048891, mean_q: -0.315260
 71689/100000: episode: 1210, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 4.463, mean reward: 0.235 [0.182, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.306], loss: 0.002995, mae: 0.055230, mean_q: -0.310597
 71708/100000: episode: 1211, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 4.318, mean reward: 0.227 [0.130, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.641, 10.266], loss: 0.003153, mae: 0.055569, mean_q: -0.321142
 71723/100000: episode: 1212, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 5.703, mean reward: 0.380 [0.272, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.546, 10.324], loss: 0.002366, mae: 0.047608, mean_q: -0.274220
 71762/100000: episode: 1213, duration: 0.207s, episode steps: 39, steps per second: 188, episode reward: 9.767, mean reward: 0.250 [0.006, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.184, 10.174], loss: 0.002982, mae: 0.055336, mean_q: -0.274611
 71788/100000: episode: 1214, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 8.254, mean reward: 0.317 [0.208, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.110, 10.485], loss: 0.002674, mae: 0.049515, mean_q: -0.288356
 71823/100000: episode: 1215, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 14.276, mean reward: 0.408 [0.274, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.206, 10.573], loss: 0.002673, mae: 0.051951, mean_q: -0.279872
 71832/100000: episode: 1216, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 3.385, mean reward: 0.376 [0.279, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.323, 10.100], loss: 0.002319, mae: 0.050446, mean_q: -0.301222
 71871/100000: episode: 1217, duration: 0.208s, episode steps: 39, steps per second: 188, episode reward: 9.769, mean reward: 0.250 [0.069, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.080, 10.100], loss: 0.003010, mae: 0.054278, mean_q: -0.239689
 71884/100000: episode: 1218, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 3.258, mean reward: 0.251 [0.142, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.399, 10.239], loss: 0.002411, mae: 0.050193, mean_q: -0.253917
 71897/100000: episode: 1219, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 3.774, mean reward: 0.290 [0.220, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.361], loss: 0.002690, mae: 0.052173, mean_q: -0.260359
 71912/100000: episode: 1220, duration: 0.072s, episode steps: 15, steps per second: 209, episode reward: 4.848, mean reward: 0.323 [0.257, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.551], loss: 0.003264, mae: 0.058632, mean_q: -0.271691
 71921/100000: episode: 1221, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 3.117, mean reward: 0.346 [0.281, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.272, 10.100], loss: 0.002790, mae: 0.051100, mean_q: -0.241444
 71930/100000: episode: 1222, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 3.271, mean reward: 0.363 [0.282, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.372, 10.100], loss: 0.002841, mae: 0.052444, mean_q: -0.313706
 71945/100000: episode: 1223, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 4.243, mean reward: 0.283 [0.127, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.349], loss: 0.002275, mae: 0.047474, mean_q: -0.350087
 71964/100000: episode: 1224, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 5.287, mean reward: 0.278 [0.206, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.221, 10.342], loss: 0.002778, mae: 0.055832, mean_q: -0.262384
 71990/100000: episode: 1225, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 7.781, mean reward: 0.299 [0.130, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.297], loss: 0.002710, mae: 0.052999, mean_q: -0.252899
 72003/100000: episode: 1226, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 3.213, mean reward: 0.247 [0.146, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.376, 10.281], loss: 0.002626, mae: 0.051200, mean_q: -0.263996
 72012/100000: episode: 1227, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 2.924, mean reward: 0.325 [0.291, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.387, 10.100], loss: 0.002443, mae: 0.047847, mean_q: -0.280117
 72037/100000: episode: 1228, duration: 0.136s, episode steps: 25, steps per second: 183, episode reward: 6.483, mean reward: 0.259 [0.152, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.368], loss: 0.003004, mae: 0.053686, mean_q: -0.269523
 72076/100000: episode: 1229, duration: 0.191s, episode steps: 39, steps per second: 204, episode reward: 7.525, mean reward: 0.193 [0.029, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-1.281, 10.130], loss: 0.002794, mae: 0.052845, mean_q: -0.207469
 72102/100000: episode: 1230, duration: 0.141s, episode steps: 26, steps per second: 185, episode reward: 8.523, mean reward: 0.328 [0.222, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.382], loss: 0.002513, mae: 0.052143, mean_q: -0.213815
 72128/100000: episode: 1231, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 8.574, mean reward: 0.330 [0.187, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.394, 10.371], loss: 0.003727, mae: 0.060199, mean_q: -0.204654
 72167/100000: episode: 1232, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 6.796, mean reward: 0.174 [0.030, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-1.493, 10.100], loss: 0.013030, mae: 0.089480, mean_q: -0.246475
 72192/100000: episode: 1233, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 6.258, mean reward: 0.250 [0.168, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.077, 10.414], loss: 0.012388, mae: 0.089302, mean_q: -0.200853
 72217/100000: episode: 1234, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 8.490, mean reward: 0.340 [0.236, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.501, 10.372], loss: 0.004440, mae: 0.070188, mean_q: -0.204161
 72236/100000: episode: 1235, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 5.782, mean reward: 0.304 [0.218, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.455, 10.489], loss: 0.003083, mae: 0.056108, mean_q: -0.246476
 72249/100000: episode: 1236, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 3.626, mean reward: 0.279 [0.205, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.378], loss: 0.002423, mae: 0.051746, mean_q: -0.174447
 72275/100000: episode: 1237, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 9.840, mean reward: 0.378 [0.286, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.543], loss: 0.002532, mae: 0.052228, mean_q: -0.199372
 72293/100000: episode: 1238, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 5.557, mean reward: 0.309 [0.203, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.900, 10.388], loss: 0.002574, mae: 0.052972, mean_q: -0.221320
 72302/100000: episode: 1239, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 2.634, mean reward: 0.293 [0.245, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.269, 10.100], loss: 0.002652, mae: 0.053764, mean_q: -0.240783
 72311/100000: episode: 1240, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 2.555, mean reward: 0.284 [0.198, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.331, 10.100], loss: 0.002867, mae: 0.054258, mean_q: -0.167715
 72326/100000: episode: 1241, duration: 0.081s, episode steps: 15, steps per second: 184, episode reward: 4.225, mean reward: 0.282 [0.200, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.350], loss: 0.003212, mae: 0.058084, mean_q: -0.170768
 72344/100000: episode: 1242, duration: 0.086s, episode steps: 18, steps per second: 210, episode reward: 5.617, mean reward: 0.312 [0.195, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.364], loss: 0.002709, mae: 0.053400, mean_q: -0.210564
 72362/100000: episode: 1243, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 4.983, mean reward: 0.277 [0.167, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.151], loss: 0.003194, mae: 0.057613, mean_q: -0.153682
 72380/100000: episode: 1244, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 5.647, mean reward: 0.314 [0.218, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.875, 10.433], loss: 0.003104, mae: 0.058038, mean_q: -0.182506
 72393/100000: episode: 1245, duration: 0.076s, episode steps: 13, steps per second: 170, episode reward: 3.138, mean reward: 0.241 [0.148, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.557, 10.266], loss: 0.002914, mae: 0.057709, mean_q: -0.233111
 72432/100000: episode: 1246, duration: 0.188s, episode steps: 39, steps per second: 207, episode reward: 11.566, mean reward: 0.297 [0.121, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.361, 10.100], loss: 0.002896, mae: 0.056536, mean_q: -0.192160
 72445/100000: episode: 1247, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 3.763, mean reward: 0.289 [0.116, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.283], loss: 0.002989, mae: 0.054238, mean_q: -0.163905
 72454/100000: episode: 1248, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 3.044, mean reward: 0.338 [0.234, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.333, 10.100], loss: 0.002595, mae: 0.054271, mean_q: -0.207837
 72473/100000: episode: 1249, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 6.118, mean reward: 0.322 [0.256, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.200, 10.334], loss: 0.003265, mae: 0.057344, mean_q: -0.235158
 72499/100000: episode: 1250, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 8.995, mean reward: 0.346 [0.251, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.553, 10.476], loss: 0.002780, mae: 0.054684, mean_q: -0.105064
 72518/100000: episode: 1251, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 5.135, mean reward: 0.270 [0.199, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.313, 10.269], loss: 0.002871, mae: 0.053193, mean_q: -0.226669
 72553/100000: episode: 1252, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 8.274, mean reward: 0.236 [0.029, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.701, 10.227], loss: 0.002659, mae: 0.052809, mean_q: -0.167450
 72562/100000: episode: 1253, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 2.888, mean reward: 0.321 [0.237, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.376, 10.100], loss: 0.002228, mae: 0.047582, mean_q: -0.285093
 72601/100000: episode: 1254, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 9.806, mean reward: 0.251 [0.077, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.440, 10.100], loss: 0.003121, mae: 0.057400, mean_q: -0.132399
 72614/100000: episode: 1255, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 4.494, mean reward: 0.346 [0.251, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.431, 10.336], loss: 0.002534, mae: 0.052305, mean_q: -0.099029
 72653/100000: episode: 1256, duration: 0.192s, episode steps: 39, steps per second: 203, episode reward: 8.467, mean reward: 0.217 [0.035, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-1.246, 10.100], loss: 0.002935, mae: 0.056148, mean_q: -0.097918
 72671/100000: episode: 1257, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 5.076, mean reward: 0.282 [0.136, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.130, 10.336], loss: 0.003017, mae: 0.054512, mean_q: -0.112986
 72680/100000: episode: 1258, duration: 0.069s, episode steps: 9, steps per second: 131, episode reward: 2.416, mean reward: 0.268 [0.206, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.592, 10.100], loss: 0.003108, mae: 0.057749, mean_q: -0.087762
 72699/100000: episode: 1259, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 6.240, mean reward: 0.328 [0.263, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-1.065, 10.502], loss: 0.002859, mae: 0.055380, mean_q: -0.057211
 72725/100000: episode: 1260, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 7.181, mean reward: 0.276 [0.069, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.173], loss: 0.002778, mae: 0.053437, mean_q: -0.159354
 72738/100000: episode: 1261, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 3.178, mean reward: 0.244 [0.199, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.908, 10.363], loss: 0.003317, mae: 0.057434, mean_q: -0.119445
 72777/100000: episode: 1262, duration: 0.198s, episode steps: 39, steps per second: 197, episode reward: 11.910, mean reward: 0.305 [0.107, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.699, 10.100], loss: 0.003033, mae: 0.055864, mean_q: -0.100798
 72796/100000: episode: 1263, duration: 0.092s, episode steps: 19, steps per second: 207, episode reward: 7.038, mean reward: 0.370 [0.242, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.341, 10.415], loss: 0.002581, mae: 0.051501, mean_q: -0.189270
 72805/100000: episode: 1264, duration: 0.059s, episode steps: 9, steps per second: 154, episode reward: 2.099, mean reward: 0.233 [0.139, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.676, 10.100], loss: 0.002460, mae: 0.051014, mean_q: -0.143979
 72818/100000: episode: 1265, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 4.449, mean reward: 0.342 [0.294, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.124, 10.394], loss: 0.003020, mae: 0.057224, mean_q: -0.089984
 72827/100000: episode: 1266, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 2.179, mean reward: 0.242 [0.181, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.351, 10.100], loss: 0.002693, mae: 0.052776, mean_q: -0.069480
 72845/100000: episode: 1267, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 4.772, mean reward: 0.265 [0.178, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.366], loss: 0.002877, mae: 0.054926, mean_q: -0.105451
 72870/100000: episode: 1268, duration: 0.121s, episode steps: 25, steps per second: 206, episode reward: 4.635, mean reward: 0.185 [0.075, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.568, 10.187], loss: 0.002522, mae: 0.052002, mean_q: -0.117392
 72895/100000: episode: 1269, duration: 0.139s, episode steps: 25, steps per second: 179, episode reward: 6.153, mean reward: 0.246 [0.074, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.086, 10.192], loss: 0.002809, mae: 0.054818, mean_q: -0.110830
 72920/100000: episode: 1270, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 7.233, mean reward: 0.289 [0.175, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.292, 10.359], loss: 0.002813, mae: 0.053545, mean_q: -0.081625
 72929/100000: episode: 1271, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 2.572, mean reward: 0.286 [0.241, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.236, 10.100], loss: 0.003802, mae: 0.062931, mean_q: -0.043426
 72942/100000: episode: 1272, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 5.195, mean reward: 0.400 [0.304, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.316, 10.455], loss: 0.003388, mae: 0.058531, mean_q: -0.086540
 72968/100000: episode: 1273, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 9.691, mean reward: 0.373 [0.278, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.451, 10.488], loss: 0.002940, mae: 0.055678, mean_q: -0.060961
 72983/100000: episode: 1274, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 4.131, mean reward: 0.275 [0.146, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.391], loss: 0.002662, mae: 0.052477, mean_q: -0.080615
 72996/100000: episode: 1275, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 3.283, mean reward: 0.253 [0.164, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.251, 10.307], loss: 0.003174, mae: 0.056441, mean_q: -0.037895
 73011/100000: episode: 1276, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 4.721, mean reward: 0.315 [0.233, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-1.181, 10.397], loss: 0.003010, mae: 0.055097, mean_q: -0.120683
 73036/100000: episode: 1277, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 8.394, mean reward: 0.336 [0.212, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.428, 10.461], loss: 0.003064, mae: 0.056566, mean_q: -0.007639
 73062/100000: episode: 1278, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 8.561, mean reward: 0.329 [0.080, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.095, 10.375], loss: 0.003037, mae: 0.056781, mean_q: 0.007659
 73087/100000: episode: 1279, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 7.529, mean reward: 0.301 [0.147, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.291, 10.521], loss: 0.003002, mae: 0.056417, mean_q: -0.046121
 73122/100000: episode: 1280, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 9.899, mean reward: 0.283 [0.052, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.059, 10.134], loss: 0.002468, mae: 0.050817, mean_q: -0.093306
 73141/100000: episode: 1281, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 7.011, mean reward: 0.369 [0.261, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.080, 10.363], loss: 0.002894, mae: 0.055267, mean_q: -0.080792
 73176/100000: episode: 1282, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 13.525, mean reward: 0.386 [0.185, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.789, 10.386], loss: 0.002883, mae: 0.055551, mean_q: -0.046928
 73215/100000: episode: 1283, duration: 0.210s, episode steps: 39, steps per second: 185, episode reward: 10.284, mean reward: 0.264 [0.069, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.398, 10.100], loss: 0.002811, mae: 0.055150, mean_q: -0.007026
 73234/100000: episode: 1284, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 5.252, mean reward: 0.276 [0.161, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.328, 10.382], loss: 0.003243, mae: 0.059112, mean_q: 0.017777
 73269/100000: episode: 1285, duration: 0.173s, episode steps: 35, steps per second: 203, episode reward: 16.991, mean reward: 0.485 [0.355, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.379, 10.548], loss: 0.002843, mae: 0.055102, mean_q: -0.046999
 73304/100000: episode: 1286, duration: 0.174s, episode steps: 35, steps per second: 201, episode reward: 16.905, mean reward: 0.483 [0.292, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.066, 10.493], loss: 0.002945, mae: 0.056591, mean_q: 0.015501
 73343/100000: episode: 1287, duration: 0.203s, episode steps: 39, steps per second: 192, episode reward: 7.638, mean reward: 0.196 [0.023, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.933, 10.102], loss: 0.003104, mae: 0.057551, mean_q: -0.005672
 73378/100000: episode: 1288, duration: 0.180s, episode steps: 35, steps per second: 195, episode reward: 16.234, mean reward: 0.464 [0.276, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.160, 10.554], loss: 0.003043, mae: 0.058011, mean_q: 0.085933
 73413/100000: episode: 1289, duration: 0.172s, episode steps: 35, steps per second: 204, episode reward: 11.809, mean reward: 0.337 [0.250, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.144, 10.344], loss: 0.002832, mae: 0.055051, mean_q: 0.015101
 73428/100000: episode: 1290, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 4.630, mean reward: 0.309 [0.207, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-1.206, 10.361], loss: 0.003134, mae: 0.057307, mean_q: 0.078902
 73454/100000: episode: 1291, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 8.402, mean reward: 0.323 [0.235, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.382, 10.376], loss: 0.002865, mae: 0.057652, mean_q: 0.033966
 73467/100000: episode: 1292, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 4.441, mean reward: 0.342 [0.257, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.493], loss: 0.002779, mae: 0.055174, mean_q: 0.059345
 73502/100000: episode: 1293, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 14.816, mean reward: 0.423 [0.216, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.633, 10.615], loss: 0.002831, mae: 0.055095, mean_q: -0.010283
 73511/100000: episode: 1294, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 2.585, mean reward: 0.287 [0.243, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.416, 10.100], loss: 0.002272, mae: 0.048234, mean_q: 0.086494
[Info] 200-TH LEVEL FOUND: 0.7907519340515137, Considering 10/90 traces
 73524/100000: episode: 1295, duration: 3.895s, episode steps: 13, steps per second: 3, episode reward: 3.075, mean reward: 0.237 [0.127, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.092, 10.297], loss: 0.003041, mae: 0.053417, mean_q: -0.025329
 73556/100000: episode: 1296, duration: 0.158s, episode steps: 32, steps per second: 202, episode reward: 14.637, mean reward: 0.457 [0.383, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.555, 10.547], loss: 0.002810, mae: 0.053913, mean_q: -0.002142
 73589/100000: episode: 1297, duration: 0.160s, episode steps: 33, steps per second: 206, episode reward: 15.762, mean reward: 0.478 [0.307, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.710, 10.404], loss: 0.003201, mae: 0.059574, mean_q: 0.071831
 73613/100000: episode: 1298, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 6.847, mean reward: 0.285 [0.123, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.220], loss: 0.003400, mae: 0.059741, mean_q: 0.069225
 73641/100000: episode: 1299, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 13.791, mean reward: 0.493 [0.357, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.479], loss: 0.003118, mae: 0.058276, mean_q: 0.056373
 73674/100000: episode: 1300, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 14.997, mean reward: 0.454 [0.259, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.053, 10.406], loss: 0.002987, mae: 0.058136, mean_q: 0.100940
 73702/100000: episode: 1301, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 11.472, mean reward: 0.410 [0.356, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.081, 10.434], loss: 0.003118, mae: 0.058694, mean_q: 0.085367
 73733/100000: episode: 1302, duration: 0.154s, episode steps: 31, steps per second: 201, episode reward: 11.220, mean reward: 0.362 [0.184, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.310, 10.303], loss: 0.003100, mae: 0.059133, mean_q: 0.122097
 73738/100000: episode: 1303, duration: 0.034s, episode steps: 5, steps per second: 149, episode reward: 2.049, mean reward: 0.410 [0.328, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.686, 10.478], loss: 0.002342, mae: 0.051897, mean_q: 0.102922
 73771/100000: episode: 1304, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 9.539, mean reward: 0.289 [0.119, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.035, 10.365], loss: 0.003046, mae: 0.058198, mean_q: 0.112224
 73803/100000: episode: 1305, duration: 0.162s, episode steps: 32, steps per second: 198, episode reward: 11.317, mean reward: 0.354 [0.222, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.754, 10.504], loss: 0.002870, mae: 0.057003, mean_q: 0.085566
 73827/100000: episode: 1306, duration: 0.129s, episode steps: 24, steps per second: 187, episode reward: 9.701, mean reward: 0.404 [0.220, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.603, 10.480], loss: 0.003211, mae: 0.059208, mean_q: 0.110344
 73858/100000: episode: 1307, duration: 0.185s, episode steps: 31, steps per second: 167, episode reward: 15.041, mean reward: 0.485 [0.298, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.416, 10.410], loss: 0.002970, mae: 0.058511, mean_q: 0.095799
 73889/100000: episode: 1308, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 12.998, mean reward: 0.419 [0.168, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.035, 10.308], loss: 0.003370, mae: 0.061983, mean_q: 0.118248
 73922/100000: episode: 1309, duration: 0.187s, episode steps: 33, steps per second: 177, episode reward: 12.650, mean reward: 0.383 [0.232, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.130, 10.329], loss: 0.003151, mae: 0.060441, mean_q: 0.080459
 73953/100000: episode: 1310, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 16.461, mean reward: 0.531 [0.255, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.280, 10.370], loss: 0.003532, mae: 0.063050, mean_q: 0.130522
 73958/100000: episode: 1311, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 2.206, mean reward: 0.441 [0.384, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.421], loss: 0.003019, mae: 0.057330, mean_q: 0.086426
 73982/100000: episode: 1312, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 10.487, mean reward: 0.437 [0.350, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-1.137, 10.594], loss: 0.002976, mae: 0.057843, mean_q: 0.123115
 74013/100000: episode: 1313, duration: 0.169s, episode steps: 31, steps per second: 184, episode reward: 10.424, mean reward: 0.336 [0.214, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.035, 10.435], loss: 0.003312, mae: 0.061101, mean_q: 0.149487
 74046/100000: episode: 1314, duration: 0.171s, episode steps: 33, steps per second: 192, episode reward: 13.125, mean reward: 0.398 [0.285, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.529, 10.346], loss: 0.002857, mae: 0.057049, mean_q: 0.128043
[Info] FALSIFICATION!
 74064/100000: episode: 1315, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 19.046, mean reward: 1.058 [0.396, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.068, 9.941], loss: 0.002924, mae: 0.057531, mean_q: 0.190204
 74164/100000: episode: 1316, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -13.582, mean reward: -0.136 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.155, 10.459], loss: 0.016600, mae: 0.070924, mean_q: 0.176830
 74264/100000: episode: 1317, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.378, mean reward: -0.184 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.773, 10.098], loss: 0.003184, mae: 0.059600, mean_q: 0.156685
 74364/100000: episode: 1318, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -16.432, mean reward: -0.164 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.261, 10.098], loss: 0.003119, mae: 0.058892, mean_q: 0.138354
 74464/100000: episode: 1319, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -9.821, mean reward: -0.098 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.085, 10.446], loss: 0.029951, mae: 0.076945, mean_q: 0.179415
 74564/100000: episode: 1320, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.896, mean reward: -0.179 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.656, 10.271], loss: 0.029597, mae: 0.078443, mean_q: 0.177508
 74664/100000: episode: 1321, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -16.932, mean reward: -0.169 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.818, 10.106], loss: 0.016189, mae: 0.067899, mean_q: 0.169036
 74764/100000: episode: 1322, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -19.659, mean reward: -0.197 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.816, 10.098], loss: 0.015702, mae: 0.061882, mean_q: 0.153538
 74864/100000: episode: 1323, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.418, mean reward: -0.174 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.177, 10.137], loss: 0.003224, mae: 0.060073, mean_q: 0.165579
 74964/100000: episode: 1324, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.701, mean reward: -0.187 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.297, 10.098], loss: 0.029794, mae: 0.079388, mean_q: 0.170680
 75064/100000: episode: 1325, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -19.969, mean reward: -0.200 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.493, 10.136], loss: 0.016011, mae: 0.066678, mean_q: 0.184016
 75164/100000: episode: 1326, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -19.906, mean reward: -0.199 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.939, 10.191], loss: 0.003806, mae: 0.061131, mean_q: 0.151787
 75264/100000: episode: 1327, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.396, mean reward: -0.194 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.250, 10.183], loss: 0.018886, mae: 0.077079, mean_q: 0.145420
 75364/100000: episode: 1328, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.266, mean reward: -0.193 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.655, 10.099], loss: 0.015844, mae: 0.064714, mean_q: 0.142406
 75464/100000: episode: 1329, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -14.346, mean reward: -0.143 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.789, 10.371], loss: 0.003292, mae: 0.060070, mean_q: 0.151480
 75564/100000: episode: 1330, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.641, mean reward: -0.186 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.465, 10.098], loss: 0.003087, mae: 0.058502, mean_q: 0.163207
 75664/100000: episode: 1331, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.036, mean reward: -0.140 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.680, 10.098], loss: 0.002788, mae: 0.055344, mean_q: 0.152330
 75764/100000: episode: 1332, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -13.401, mean reward: -0.134 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.876, 10.098], loss: 0.003295, mae: 0.061154, mean_q: 0.155358
 75864/100000: episode: 1333, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.909, mean reward: -0.169 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.669, 10.269], loss: 0.003035, mae: 0.057736, mean_q: 0.147762
 75964/100000: episode: 1334, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -15.594, mean reward: -0.156 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.253, 10.354], loss: 0.003089, mae: 0.058045, mean_q: 0.161015
 76064/100000: episode: 1335, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -13.939, mean reward: -0.139 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.537, 10.123], loss: 0.002838, mae: 0.056352, mean_q: 0.163381
 76164/100000: episode: 1336, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.728, mean reward: -0.157 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.832, 10.098], loss: 0.015900, mae: 0.065154, mean_q: 0.186322
 76264/100000: episode: 1337, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -15.502, mean reward: -0.155 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.886, 10.308], loss: 0.015861, mae: 0.065604, mean_q: 0.193085
 76364/100000: episode: 1338, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -19.499, mean reward: -0.195 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.249, 10.098], loss: 0.002983, mae: 0.058323, mean_q: 0.190825
 76464/100000: episode: 1339, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.266, mean reward: -0.153 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.608, 10.098], loss: 0.015778, mae: 0.063781, mean_q: 0.156126
 76564/100000: episode: 1340, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.842, mean reward: -0.148 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.514, 10.150], loss: 0.003011, mae: 0.057743, mean_q: 0.146967
 76664/100000: episode: 1341, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -10.896, mean reward: -0.109 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.021, 10.440], loss: 0.003561, mae: 0.061669, mean_q: 0.164299
 76764/100000: episode: 1342, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.189, mean reward: -0.172 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.443, 10.098], loss: 0.003423, mae: 0.058983, mean_q: 0.093471
 76864/100000: episode: 1343, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -21.070, mean reward: -0.211 [-1.000, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.942, 10.220], loss: 0.015927, mae: 0.064582, mean_q: 0.084306
 76964/100000: episode: 1344, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.028, mean reward: -0.170 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.907, 10.145], loss: 0.002639, mae: 0.053973, mean_q: 0.087523
 77064/100000: episode: 1345, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -18.982, mean reward: -0.190 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.339, 10.098], loss: 0.002750, mae: 0.054234, mean_q: 0.050159
 77164/100000: episode: 1346, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -15.843, mean reward: -0.158 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.520, 10.098], loss: 0.002726, mae: 0.054302, mean_q: 0.058016
 77264/100000: episode: 1347, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -17.490, mean reward: -0.175 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.062, 10.116], loss: 0.028968, mae: 0.072525, mean_q: 0.043535
 77364/100000: episode: 1348, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.257, mean reward: -0.143 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.863, 10.285], loss: 0.002899, mae: 0.055862, mean_q: 0.003018
 77464/100000: episode: 1349, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.857, mean reward: -0.179 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.772, 10.098], loss: 0.002739, mae: 0.054001, mean_q: 0.010296
 77564/100000: episode: 1350, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.511, mean reward: -0.175 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.760, 10.098], loss: 0.015342, mae: 0.058351, mean_q: -0.055702
 77664/100000: episode: 1351, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.530, mean reward: -0.155 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.105, 10.179], loss: 0.003083, mae: 0.056376, mean_q: -0.047039
 77764/100000: episode: 1352, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -20.000, mean reward: -0.200 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.301, 10.098], loss: 0.015071, mae: 0.055219, mean_q: -0.085057
 77864/100000: episode: 1353, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.656, mean reward: -0.177 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.171, 10.128], loss: 0.002646, mae: 0.052107, mean_q: -0.104943
 77964/100000: episode: 1354, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -12.984, mean reward: -0.130 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.100, 10.098], loss: 0.016032, mae: 0.063416, mean_q: -0.093558
 78064/100000: episode: 1355, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.964, mean reward: -0.190 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.140, 10.206], loss: 0.015716, mae: 0.062468, mean_q: -0.081154
 78164/100000: episode: 1356, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -19.193, mean reward: -0.192 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.195, 10.098], loss: 0.015737, mae: 0.061002, mean_q: -0.138543
 78264/100000: episode: 1357, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.724, mean reward: -0.167 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.533, 10.098], loss: 0.002722, mae: 0.052467, mean_q: -0.166049
 78364/100000: episode: 1358, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.697, mean reward: -0.187 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.886, 10.098], loss: 0.002733, mae: 0.052399, mean_q: -0.183940
 78464/100000: episode: 1359, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -18.925, mean reward: -0.189 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.342, 10.098], loss: 0.003165, mae: 0.054687, mean_q: -0.189479
 78564/100000: episode: 1360, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -12.205, mean reward: -0.122 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.927, 10.098], loss: 0.004998, mae: 0.064782, mean_q: -0.214016
 78664/100000: episode: 1361, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.054, mean reward: -0.191 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.921, 10.098], loss: 0.002815, mae: 0.052834, mean_q: -0.258420
 78764/100000: episode: 1362, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.298, mean reward: -0.163 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.490, 10.098], loss: 0.027605, mae: 0.065287, mean_q: -0.228916
 78864/100000: episode: 1363, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.993, mean reward: -0.170 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.174, 10.098], loss: 0.015710, mae: 0.061228, mean_q: -0.285621
 78964/100000: episode: 1364, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.459, mean reward: -0.195 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.916, 10.208], loss: 0.015628, mae: 0.059314, mean_q: -0.328738
 79064/100000: episode: 1365, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -20.073, mean reward: -0.201 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.533, 10.183], loss: 0.015367, mae: 0.056802, mean_q: -0.329498
 79164/100000: episode: 1366, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.888, mean reward: -0.179 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.287, 10.171], loss: 0.002860, mae: 0.049911, mean_q: -0.319477
 79264/100000: episode: 1367, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.576, mean reward: -0.166 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.638, 10.098], loss: 0.002834, mae: 0.054212, mean_q: -0.344766
 79364/100000: episode: 1368, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.152, mean reward: -0.152 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.293, 10.232], loss: 0.002441, mae: 0.049377, mean_q: -0.274585
 79464/100000: episode: 1369, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.221, mean reward: -0.182 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.512, 10.098], loss: 0.002651, mae: 0.051303, mean_q: -0.300463
 79564/100000: episode: 1370, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.973, mean reward: -0.180 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.045, 10.098], loss: 0.002522, mae: 0.049886, mean_q: -0.279474
 79664/100000: episode: 1371, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.891, mean reward: -0.169 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.514, 10.098], loss: 0.002658, mae: 0.051404, mean_q: -0.309309
 79764/100000: episode: 1372, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -16.865, mean reward: -0.169 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.602, 10.297], loss: 0.002560, mae: 0.049871, mean_q: -0.304525
 79864/100000: episode: 1373, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.308, mean reward: -0.183 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.715, 10.197], loss: 0.002732, mae: 0.051903, mean_q: -0.317861
 79964/100000: episode: 1374, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.947, mean reward: -0.139 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.394, 10.098], loss: 0.002658, mae: 0.050635, mean_q: -0.293981
 80064/100000: episode: 1375, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -18.227, mean reward: -0.182 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.535, 10.098], loss: 0.002412, mae: 0.047442, mean_q: -0.356045
 80164/100000: episode: 1376, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.351, mean reward: -0.144 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.503, 10.198], loss: 0.002515, mae: 0.048826, mean_q: -0.340264
 80264/100000: episode: 1377, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.741, mean reward: -0.167 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.900, 10.098], loss: 0.002748, mae: 0.051831, mean_q: -0.295605
 80364/100000: episode: 1378, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.473, mean reward: -0.175 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.806, 10.181], loss: 0.002744, mae: 0.052367, mean_q: -0.313760
 80464/100000: episode: 1379, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.298, mean reward: -0.193 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.159, 10.127], loss: 0.002774, mae: 0.051680, mean_q: -0.312037
 80564/100000: episode: 1380, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.683, mean reward: -0.167 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.531, 10.098], loss: 0.002611, mae: 0.049659, mean_q: -0.338176
 80664/100000: episode: 1381, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.004, mean reward: -0.160 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.463, 10.098], loss: 0.002648, mae: 0.051430, mean_q: -0.290471
 80764/100000: episode: 1382, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -13.584, mean reward: -0.136 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.271, 10.300], loss: 0.002393, mae: 0.049044, mean_q: -0.324482
 80864/100000: episode: 1383, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -17.573, mean reward: -0.176 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.084, 10.098], loss: 0.002519, mae: 0.049959, mean_q: -0.313433
 80964/100000: episode: 1384, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.837, mean reward: -0.178 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.700, 10.098], loss: 0.002596, mae: 0.050711, mean_q: -0.288882
 81064/100000: episode: 1385, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -19.317, mean reward: -0.193 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.117, 10.121], loss: 0.002905, mae: 0.054137, mean_q: -0.286316
 81164/100000: episode: 1386, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.428, mean reward: -0.174 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.471, 10.098], loss: 0.002451, mae: 0.048556, mean_q: -0.345834
 81264/100000: episode: 1387, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -15.518, mean reward: -0.155 [-1.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.130, 10.210], loss: 0.002503, mae: 0.048828, mean_q: -0.313594
 81364/100000: episode: 1388, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.861, mean reward: -0.189 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.811, 10.098], loss: 0.002606, mae: 0.050788, mean_q: -0.329681
 81464/100000: episode: 1389, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -18.948, mean reward: -0.189 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.244, 10.098], loss: 0.002696, mae: 0.051682, mean_q: -0.309044
 81564/100000: episode: 1390, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -17.840, mean reward: -0.178 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.465, 10.210], loss: 0.002698, mae: 0.052064, mean_q: -0.343598
 81664/100000: episode: 1391, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.869, mean reward: -0.159 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.348, 10.098], loss: 0.002721, mae: 0.052634, mean_q: -0.339807
 81764/100000: episode: 1392, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -12.364, mean reward: -0.124 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.139, 10.098], loss: 0.002727, mae: 0.050762, mean_q: -0.344326
 81864/100000: episode: 1393, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.965, mean reward: -0.170 [-1.000, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.811, 10.200], loss: 0.002811, mae: 0.053763, mean_q: -0.287665
 81964/100000: episode: 1394, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -13.310, mean reward: -0.133 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.126, 10.390], loss: 0.002799, mae: 0.051714, mean_q: -0.312417
 82064/100000: episode: 1395, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -8.587, mean reward: -0.086 [-1.000, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.537, 10.098], loss: 0.002883, mae: 0.052296, mean_q: -0.322286
 82164/100000: episode: 1396, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.472, mean reward: -0.185 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.633, 10.147], loss: 0.002681, mae: 0.051237, mean_q: -0.310812
 82264/100000: episode: 1397, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.881, mean reward: -0.189 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.875, 10.098], loss: 0.002704, mae: 0.050894, mean_q: -0.319435
 82364/100000: episode: 1398, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.450, mean reward: -0.184 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.179, 10.098], loss: 0.002800, mae: 0.051719, mean_q: -0.326054
 82464/100000: episode: 1399, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.981, mean reward: -0.170 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.620, 10.358], loss: 0.002702, mae: 0.051227, mean_q: -0.315347
 82564/100000: episode: 1400, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -19.638, mean reward: -0.196 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.808, 10.185], loss: 0.002660, mae: 0.050397, mean_q: -0.324395
 82664/100000: episode: 1401, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.984, mean reward: -0.170 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.213, 10.379], loss: 0.002648, mae: 0.051310, mean_q: -0.342791
 82764/100000: episode: 1402, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -18.437, mean reward: -0.184 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.956, 10.098], loss: 0.002706, mae: 0.051624, mean_q: -0.326386
 82864/100000: episode: 1403, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.332, mean reward: -0.173 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.323, 10.120], loss: 0.002814, mae: 0.052492, mean_q: -0.286957
 82964/100000: episode: 1404, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -18.990, mean reward: -0.190 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.970, 10.098], loss: 0.002689, mae: 0.051330, mean_q: -0.302348
 83064/100000: episode: 1405, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.921, mean reward: -0.179 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.713, 10.200], loss: 0.006358, mae: 0.064861, mean_q: -0.328640
 83164/100000: episode: 1406, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.001, mean reward: -0.150 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.206, 10.098], loss: 0.002870, mae: 0.053976, mean_q: -0.323404
 83264/100000: episode: 1407, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.998, mean reward: -0.170 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.104, 10.279], loss: 0.002810, mae: 0.052096, mean_q: -0.316710
 83364/100000: episode: 1408, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -12.280, mean reward: -0.123 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.194, 10.427], loss: 0.002663, mae: 0.051286, mean_q: -0.323506
 83464/100000: episode: 1409, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -19.256, mean reward: -0.193 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.845, 10.117], loss: 0.002812, mae: 0.052032, mean_q: -0.315042
 83564/100000: episode: 1410, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -19.406, mean reward: -0.194 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-2.055, 10.098], loss: 0.002828, mae: 0.052482, mean_q: -0.280371
 83664/100000: episode: 1411, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.620, mean reward: -0.156 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.017, 10.098], loss: 0.002658, mae: 0.051351, mean_q: -0.302712
 83764/100000: episode: 1412, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -15.018, mean reward: -0.150 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.399, 10.098], loss: 0.002735, mae: 0.051425, mean_q: -0.328837
 83864/100000: episode: 1413, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.759, mean reward: -0.168 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.035, 10.319], loss: 0.002555, mae: 0.049261, mean_q: -0.362719
 83964/100000: episode: 1414, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.101, mean reward: -0.191 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.748, 10.137], loss: 0.002694, mae: 0.050891, mean_q: -0.307796
[Info] 100-TH LEVEL FOUND: 0.5969676971435547, Considering 10/90 traces
 84064/100000: episode: 1415, duration: 4.323s, episode steps: 100, steps per second: 23, episode reward: -12.823, mean reward: -0.128 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.659, 10.213], loss: 0.002725, mae: 0.050378, mean_q: -0.352496
 84095/100000: episode: 1416, duration: 0.155s, episode steps: 31, steps per second: 200, episode reward: 7.387, mean reward: 0.238 [0.119, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.151, 10.100], loss: 0.002510, mae: 0.049383, mean_q: -0.333982
 84111/100000: episode: 1417, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 4.240, mean reward: 0.265 [0.177, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.247, 10.316], loss: 0.002644, mae: 0.049124, mean_q: -0.287925
 84146/100000: episode: 1418, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 9.423, mean reward: 0.269 [0.086, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.745, 10.100], loss: 0.002901, mae: 0.053338, mean_q: -0.215185
 84181/100000: episode: 1419, duration: 0.179s, episode steps: 35, steps per second: 196, episode reward: 6.900, mean reward: 0.197 [0.043, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-1.489, 10.117], loss: 0.002782, mae: 0.051217, mean_q: -0.307801
 84194/100000: episode: 1420, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 4.774, mean reward: 0.367 [0.235, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.356, 10.100], loss: 0.002248, mae: 0.047903, mean_q: -0.298454
 84211/100000: episode: 1421, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 4.484, mean reward: 0.264 [0.188, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.320], loss: 0.002744, mae: 0.050916, mean_q: -0.302769
 84242/100000: episode: 1422, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 6.726, mean reward: 0.217 [0.047, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.120, 10.100], loss: 0.002309, mae: 0.047462, mean_q: -0.302774
 84277/100000: episode: 1423, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 12.537, mean reward: 0.358 [0.154, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.864, 10.100], loss: 0.002928, mae: 0.054620, mean_q: -0.308176
 84294/100000: episode: 1424, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 5.687, mean reward: 0.335 [0.246, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.333, 10.457], loss: 0.002518, mae: 0.050439, mean_q: -0.267379
 84320/100000: episode: 1425, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 8.274, mean reward: 0.318 [0.243, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.602, 10.453], loss: 0.002738, mae: 0.053142, mean_q: -0.243608
 84346/100000: episode: 1426, duration: 0.125s, episode steps: 26, steps per second: 209, episode reward: 9.283, mean reward: 0.357 [0.280, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.404], loss: 0.003022, mae: 0.054703, mean_q: -0.262841
 84354/100000: episode: 1427, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 2.334, mean reward: 0.292 [0.230, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.310, 10.100], loss: 0.003346, mae: 0.058993, mean_q: -0.135853
 84371/100000: episode: 1428, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 6.359, mean reward: 0.374 [0.194, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.058, 10.575], loss: 0.002831, mae: 0.053589, mean_q: -0.284652
 84379/100000: episode: 1429, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 1.902, mean reward: 0.238 [0.143, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-1.128, 10.100], loss: 0.002674, mae: 0.050899, mean_q: -0.308388
 84414/100000: episode: 1430, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 8.755, mean reward: 0.250 [0.098, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-1.064, 10.100], loss: 0.002744, mae: 0.051774, mean_q: -0.253843
 84431/100000: episode: 1431, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 4.903, mean reward: 0.288 [0.210, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.181, 10.393], loss: 0.002939, mae: 0.057074, mean_q: -0.216635
 84466/100000: episode: 1432, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 11.661, mean reward: 0.333 [0.227, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-1.230, 10.100], loss: 0.002987, mae: 0.053888, mean_q: -0.273163
 84476/100000: episode: 1433, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 2.984, mean reward: 0.298 [0.255, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.568, 10.100], loss: 0.002663, mae: 0.051924, mean_q: -0.270902
 84489/100000: episode: 1434, duration: 0.072s, episode steps: 13, steps per second: 179, episode reward: 4.414, mean reward: 0.340 [0.239, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.264, 10.100], loss: 0.002734, mae: 0.053858, mean_q: -0.168901
 84502/100000: episode: 1435, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 4.476, mean reward: 0.344 [0.257, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.555, 10.100], loss: 0.002883, mae: 0.052484, mean_q: -0.301744
 84510/100000: episode: 1436, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 2.583, mean reward: 0.323 [0.231, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.491], loss: 0.003015, mae: 0.052812, mean_q: -0.342554
 84536/100000: episode: 1437, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 11.431, mean reward: 0.440 [0.304, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.063, 10.547], loss: 0.003366, mae: 0.058995, mean_q: -0.222022
 84546/100000: episode: 1438, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 2.624, mean reward: 0.262 [0.196, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.335, 10.100], loss: 0.002835, mae: 0.054877, mean_q: -0.199981
 84562/100000: episode: 1439, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 4.581, mean reward: 0.286 [0.210, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.143, 10.425], loss: 0.003016, mae: 0.056993, mean_q: -0.191339
 84570/100000: episode: 1440, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 2.727, mean reward: 0.341 [0.223, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.517], loss: 0.002987, mae: 0.054001, mean_q: -0.252843
 84578/100000: episode: 1441, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 1.978, mean reward: 0.247 [0.208, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.295, 10.100], loss: 0.003180, mae: 0.053952, mean_q: -0.287193
 84595/100000: episode: 1442, duration: 0.086s, episode steps: 17, steps per second: 199, episode reward: 5.103, mean reward: 0.300 [0.206, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.268, 10.403], loss: 0.002532, mae: 0.051606, mean_q: -0.192963
 84630/100000: episode: 1443, duration: 0.175s, episode steps: 35, steps per second: 201, episode reward: 9.792, mean reward: 0.280 [0.185, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.252, 10.100], loss: 0.002676, mae: 0.052365, mean_q: -0.244287
 84643/100000: episode: 1444, duration: 0.076s, episode steps: 13, steps per second: 170, episode reward: 3.190, mean reward: 0.245 [0.129, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.267, 10.100], loss: 0.002879, mae: 0.054415, mean_q: -0.146687
 84656/100000: episode: 1445, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 4.595, mean reward: 0.353 [0.271, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.249, 10.100], loss: 0.003316, mae: 0.059141, mean_q: -0.214901
 84672/100000: episode: 1446, duration: 0.100s, episode steps: 16, steps per second: 160, episode reward: 4.518, mean reward: 0.282 [0.087, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.238, 10.271], loss: 0.002494, mae: 0.049094, mean_q: -0.231489
 84703/100000: episode: 1447, duration: 0.187s, episode steps: 31, steps per second: 165, episode reward: 5.616, mean reward: 0.181 [0.011, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.997, 10.168], loss: 0.002600, mae: 0.051188, mean_q: -0.227862
 84734/100000: episode: 1448, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 6.439, mean reward: 0.208 [0.121, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.321, 10.100], loss: 0.002512, mae: 0.050152, mean_q: -0.220732
 84744/100000: episode: 1449, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 3.325, mean reward: 0.332 [0.253, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.199, 10.100], loss: 0.003095, mae: 0.056863, mean_q: -0.141764
 84779/100000: episode: 1450, duration: 0.170s, episode steps: 35, steps per second: 206, episode reward: 11.240, mean reward: 0.321 [0.153, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.192, 10.100], loss: 0.003150, mae: 0.055888, mean_q: -0.165697
 84814/100000: episode: 1451, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 13.009, mean reward: 0.372 [0.295, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.564, 10.100], loss: 0.002979, mae: 0.055083, mean_q: -0.217979
 84822/100000: episode: 1452, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 3.032, mean reward: 0.379 [0.265, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.588], loss: 0.003067, mae: 0.052208, mean_q: -0.168550
 84848/100000: episode: 1453, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 8.797, mean reward: 0.338 [0.216, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.362], loss: 0.002663, mae: 0.053074, mean_q: -0.186536
 84864/100000: episode: 1454, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 4.546, mean reward: 0.284 [0.131, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.271], loss: 0.002865, mae: 0.053921, mean_q: -0.183898
 84882/100000: episode: 1455, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 6.297, mean reward: 0.350 [0.183, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.232, 10.513], loss: 0.002426, mae: 0.050533, mean_q: -0.159720
 84890/100000: episode: 1456, duration: 0.053s, episode steps: 8, steps per second: 152, episode reward: 2.363, mean reward: 0.295 [0.207, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.307, 10.100], loss: 0.003438, mae: 0.060239, mean_q: -0.140680
 84903/100000: episode: 1457, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 3.194, mean reward: 0.246 [0.173, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.340, 10.100], loss: 0.002518, mae: 0.049344, mean_q: -0.196792
 84920/100000: episode: 1458, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 4.189, mean reward: 0.246 [0.152, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.471, 10.294], loss: 0.002861, mae: 0.055580, mean_q: -0.101154
 84955/100000: episode: 1459, duration: 0.176s, episode steps: 35, steps per second: 198, episode reward: 9.814, mean reward: 0.280 [0.034, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.059, 10.100], loss: 0.002547, mae: 0.052319, mean_q: -0.166376
 84968/100000: episode: 1460, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 3.700, mean reward: 0.285 [0.237, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.142, 10.100], loss: 0.002519, mae: 0.050285, mean_q: -0.208606
 85003/100000: episode: 1461, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 11.112, mean reward: 0.317 [0.153, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.541, 10.100], loss: 0.002694, mae: 0.052749, mean_q: -0.153932
 85021/100000: episode: 1462, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 6.444, mean reward: 0.358 [0.200, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.075, 10.469], loss: 0.003048, mae: 0.055175, mean_q: -0.194138
 85029/100000: episode: 1463, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 2.024, mean reward: 0.253 [0.212, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.384], loss: 0.003861, mae: 0.061444, mean_q: -0.123969
 85055/100000: episode: 1464, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 7.579, mean reward: 0.292 [0.195, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.824, 10.389], loss: 0.003460, mae: 0.060274, mean_q: -0.177038
 85063/100000: episode: 1465, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 2.104, mean reward: 0.263 [0.192, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.351], loss: 0.003904, mae: 0.066655, mean_q: -0.102443
 85098/100000: episode: 1466, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 6.620, mean reward: 0.189 [0.042, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.340, 10.204], loss: 0.008162, mae: 0.077107, mean_q: -0.098609
 85106/100000: episode: 1467, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 1.838, mean reward: 0.230 [0.167, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.521, 10.100], loss: 0.016549, mae: 0.097891, mean_q: -0.025963
 85114/100000: episode: 1468, duration: 0.041s, episode steps: 8, steps per second: 198, episode reward: 2.571, mean reward: 0.321 [0.192, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.560], loss: 0.006746, mae: 0.075334, mean_q: -0.180937
 85130/100000: episode: 1469, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 4.530, mean reward: 0.283 [0.218, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.156, 10.419], loss: 0.003315, mae: 0.061429, mean_q: -0.090526
 85138/100000: episode: 1470, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 2.126, mean reward: 0.266 [0.216, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.340, 10.100], loss: 0.003733, mae: 0.065767, mean_q: -0.083316
 85169/100000: episode: 1471, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 8.349, mean reward: 0.269 [0.130, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.264, 10.100], loss: 0.003145, mae: 0.060154, mean_q: -0.155114
 85200/100000: episode: 1472, duration: 0.155s, episode steps: 31, steps per second: 201, episode reward: 11.350, mean reward: 0.366 [0.264, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.325, 10.100], loss: 0.002712, mae: 0.053766, mean_q: -0.139906
 85231/100000: episode: 1473, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 6.921, mean reward: 0.223 [0.037, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.322, 10.263], loss: 0.002856, mae: 0.054777, mean_q: -0.088166
 85248/100000: episode: 1474, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 5.755, mean reward: 0.339 [0.226, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.670, 10.443], loss: 0.002788, mae: 0.051737, mean_q: -0.170796
 85256/100000: episode: 1475, duration: 0.057s, episode steps: 8, steps per second: 140, episode reward: 2.053, mean reward: 0.257 [0.214, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.224, 10.100], loss: 0.002449, mae: 0.052884, mean_q: -0.108859
 85273/100000: episode: 1476, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 5.512, mean reward: 0.324 [0.218, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.282, 10.484], loss: 0.002925, mae: 0.053722, mean_q: -0.096780
 85281/100000: episode: 1477, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 2.360, mean reward: 0.295 [0.231, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.320, 10.100], loss: 0.002673, mae: 0.050875, mean_q: -0.157494
 85297/100000: episode: 1478, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 5.736, mean reward: 0.358 [0.302, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.603, 10.393], loss: 0.003128, mae: 0.058427, mean_q: 0.019297
 85313/100000: episode: 1479, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 4.128, mean reward: 0.258 [0.186, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.333], loss: 0.002754, mae: 0.055204, mean_q: -0.133843
 85339/100000: episode: 1480, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 10.055, mean reward: 0.387 [0.300, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.766, 10.269], loss: 0.002732, mae: 0.053958, mean_q: -0.107819
 85357/100000: episode: 1481, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 5.214, mean reward: 0.290 [0.144, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.828, 10.241], loss: 0.003240, mae: 0.056909, mean_q: -0.032228
 85388/100000: episode: 1482, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 10.831, mean reward: 0.349 [0.220, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.924, 10.100], loss: 0.002850, mae: 0.055233, mean_q: -0.125151
 85406/100000: episode: 1483, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 6.039, mean reward: 0.335 [0.285, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.063, 10.468], loss: 0.003413, mae: 0.060165, mean_q: -0.099177
 85414/100000: episode: 1484, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 3.062, mean reward: 0.383 [0.266, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.563, 10.488], loss: 0.003036, mae: 0.055792, mean_q: -0.045741
 85445/100000: episode: 1485, duration: 0.169s, episode steps: 31, steps per second: 184, episode reward: 7.864, mean reward: 0.254 [0.077, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.765, 10.100], loss: 0.003248, mae: 0.058766, mean_q: -0.070891
 85455/100000: episode: 1486, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 3.041, mean reward: 0.304 [0.238, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.297, 10.100], loss: 0.002980, mae: 0.056492, mean_q: -0.035953
 85481/100000: episode: 1487, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 7.337, mean reward: 0.282 [0.168, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.690, 10.327], loss: 0.002969, mae: 0.055591, mean_q: -0.040270
 85507/100000: episode: 1488, duration: 0.142s, episode steps: 26, steps per second: 184, episode reward: 6.383, mean reward: 0.245 [0.105, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.215, 10.247], loss: 0.003094, mae: 0.054712, mean_q: -0.055003
 85520/100000: episode: 1489, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 4.706, mean reward: 0.362 [0.258, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.395, 10.100], loss: 0.002529, mae: 0.051300, mean_q: -0.113573
 85536/100000: episode: 1490, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 4.429, mean reward: 0.277 [0.109, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.161, 10.234], loss: 0.003427, mae: 0.058553, mean_q: -0.041165
 85544/100000: episode: 1491, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 2.394, mean reward: 0.299 [0.198, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.375, 10.100], loss: 0.004622, mae: 0.070706, mean_q: 0.062504
 85570/100000: episode: 1492, duration: 0.123s, episode steps: 26, steps per second: 212, episode reward: 7.436, mean reward: 0.286 [0.223, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.970, 10.408], loss: 0.003923, mae: 0.063527, mean_q: -0.055165
 85601/100000: episode: 1493, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 5.251, mean reward: 0.169 [0.061, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.035, 10.289], loss: 0.002612, mae: 0.052286, mean_q: -0.083080
 85617/100000: episode: 1494, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 6.122, mean reward: 0.383 [0.318, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.734, 10.405], loss: 0.003321, mae: 0.059871, mean_q: 0.007538
 85643/100000: episode: 1495, duration: 0.142s, episode steps: 26, steps per second: 182, episode reward: 8.599, mean reward: 0.331 [0.235, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-2.053, 10.326], loss: 0.003141, mae: 0.057706, mean_q: -0.052355
 85674/100000: episode: 1496, duration: 0.152s, episode steps: 31, steps per second: 204, episode reward: 10.046, mean reward: 0.324 [0.208, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.633, 10.100], loss: 0.003082, mae: 0.056893, mean_q: -0.018853
 85682/100000: episode: 1497, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 2.191, mean reward: 0.274 [0.215, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-1.198, 10.100], loss: 0.003012, mae: 0.057144, mean_q: 0.035656
 85698/100000: episode: 1498, duration: 0.080s, episode steps: 16, steps per second: 201, episode reward: 4.393, mean reward: 0.275 [0.215, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.156, 10.372], loss: 0.003455, mae: 0.059245, mean_q: -0.018981
 85715/100000: episode: 1499, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 5.458, mean reward: 0.321 [0.250, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.429], loss: 0.003370, mae: 0.059024, mean_q: 0.043213
 85725/100000: episode: 1500, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 3.148, mean reward: 0.315 [0.179, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.349, 10.100], loss: 0.003087, mae: 0.053536, mean_q: -0.054135
 85735/100000: episode: 1501, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 2.909, mean reward: 0.291 [0.271, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.239, 10.100], loss: 0.002880, mae: 0.055672, mean_q: 0.026227
 85745/100000: episode: 1502, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 3.240, mean reward: 0.324 [0.286, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.416, 10.100], loss: 0.002662, mae: 0.052839, mean_q: -0.022802
 85776/100000: episode: 1503, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 5.987, mean reward: 0.193 [0.002, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.623, 10.163], loss: 0.003094, mae: 0.056052, mean_q: -0.034802
 85794/100000: episode: 1504, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 5.671, mean reward: 0.315 [0.222, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.120, 10.489], loss: 0.003205, mae: 0.056323, mean_q: -0.006113
[Info] 200-TH LEVEL FOUND: 0.7485326528549194, Considering 10/90 traces
 85804/100000: episode: 1505, duration: 3.845s, episode steps: 10, steps per second: 3, episode reward: 3.213, mean reward: 0.321 [0.255, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.453, 10.100], loss: 0.002862, mae: 0.055762, mean_q: -0.041170
 85808/100000: episode: 1506, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 1.397, mean reward: 0.349 [0.315, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.374], loss: 0.003107, mae: 0.058251, mean_q: -0.027818
 85821/100000: episode: 1507, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 5.419, mean reward: 0.417 [0.322, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-1.564, 10.542], loss: 0.002741, mae: 0.053957, mean_q: -0.094821
 85835/100000: episode: 1508, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 6.719, mean reward: 0.480 [0.347, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.302, 10.605], loss: 0.002905, mae: 0.054053, mean_q: 0.004292
 85839/100000: episode: 1509, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 1.400, mean reward: 0.350 [0.295, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.360], loss: 0.002573, mae: 0.051534, mean_q: 0.101338
 85852/100000: episode: 1510, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 5.797, mean reward: 0.446 [0.333, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.548], loss: 0.003207, mae: 0.057615, mean_q: 0.102971
 85865/100000: episode: 1511, duration: 0.074s, episode steps: 13, steps per second: 177, episode reward: 5.266, mean reward: 0.405 [0.360, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.332, 10.465], loss: 0.002691, mae: 0.055072, mean_q: 0.046542
 85869/100000: episode: 1512, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 1.627, mean reward: 0.407 [0.337, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.337, 10.422], loss: 0.002693, mae: 0.053181, mean_q: -0.025909
 85883/100000: episode: 1513, duration: 0.068s, episode steps: 14, steps per second: 204, episode reward: 6.240, mean reward: 0.446 [0.375, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.245, 10.546], loss: 0.002388, mae: 0.051006, mean_q: -0.016404
 85890/100000: episode: 1514, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 2.424, mean reward: 0.346 [0.262, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.438], loss: 0.003082, mae: 0.058374, mean_q: -0.002319
 85894/100000: episode: 1515, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 1.761, mean reward: 0.440 [0.399, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.576], loss: 0.002716, mae: 0.050071, mean_q: -0.085748
 85913/100000: episode: 1516, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 8.272, mean reward: 0.435 [0.291, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.038, 10.473], loss: 0.002974, mae: 0.055186, mean_q: 0.001076
 85923/100000: episode: 1517, duration: 0.067s, episode steps: 10, steps per second: 149, episode reward: 4.073, mean reward: 0.407 [0.346, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.479], loss: 0.003164, mae: 0.055787, mean_q: -0.038432
 85930/100000: episode: 1518, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 2.379, mean reward: 0.340 [0.292, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.388], loss: 0.003071, mae: 0.054663, mean_q: 0.133626
 85937/100000: episode: 1519, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 2.980, mean reward: 0.426 [0.411, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.492], loss: 0.002507, mae: 0.052635, mean_q: -0.034607
 85944/100000: episode: 1520, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 2.677, mean reward: 0.382 [0.329, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.137, 10.471], loss: 0.003247, mae: 0.057615, mean_q: -0.070469
 85948/100000: episode: 1521, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 1.482, mean reward: 0.370 [0.299, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.035, 10.515], loss: 0.003738, mae: 0.063520, mean_q: 0.090440
 85967/100000: episode: 1522, duration: 0.104s, episode steps: 19, steps per second: 184, episode reward: 8.252, mean reward: 0.434 [0.366, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.761, 10.456], loss: 0.003026, mae: 0.056879, mean_q: 0.039509
 85974/100000: episode: 1523, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 2.511, mean reward: 0.359 [0.325, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.246, 10.460], loss: 0.003450, mae: 0.060619, mean_q: 0.096097
 85988/100000: episode: 1524, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 5.184, mean reward: 0.370 [0.305, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.038, 10.472], loss: 0.003139, mae: 0.057097, mean_q: 0.013732
 85992/100000: episode: 1525, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 1.748, mean reward: 0.437 [0.424, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.582, 10.539], loss: 0.002821, mae: 0.056324, mean_q: 0.122618
 85999/100000: episode: 1526, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 2.880, mean reward: 0.411 [0.343, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.610], loss: 0.004123, mae: 0.068022, mean_q: 0.120761
 86006/100000: episode: 1527, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 2.365, mean reward: 0.338 [0.288, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.436], loss: 0.002992, mae: 0.055522, mean_q: -0.034855
 86013/100000: episode: 1528, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 2.766, mean reward: 0.395 [0.361, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.264, 10.452], loss: 0.003990, mae: 0.063453, mean_q: 0.001477
 86027/100000: episode: 1529, duration: 0.090s, episode steps: 14, steps per second: 155, episode reward: 6.899, mean reward: 0.493 [0.398, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.533], loss: 0.003133, mae: 0.057605, mean_q: -0.098406
 86031/100000: episode: 1530, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 1.386, mean reward: 0.347 [0.312, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.419], loss: 0.003648, mae: 0.062226, mean_q: 0.012006
 86035/100000: episode: 1531, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 1.444, mean reward: 0.361 [0.327, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.460], loss: 0.003556, mae: 0.069047, mean_q: 0.159193
 86049/100000: episode: 1532, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 5.803, mean reward: 0.415 [0.349, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.926, 10.557], loss: 0.003098, mae: 0.057081, mean_q: 0.011185
 86068/100000: episode: 1533, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 9.728, mean reward: 0.512 [0.420, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.114, 10.629], loss: 0.003580, mae: 0.061107, mean_q: 0.104775
 86078/100000: episode: 1534, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 4.705, mean reward: 0.471 [0.397, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.592], loss: 0.003201, mae: 0.057602, mean_q: 0.038403
 86092/100000: episode: 1535, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 5.480, mean reward: 0.391 [0.326, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.465], loss: 0.003335, mae: 0.059199, mean_q: 0.021590
 86111/100000: episode: 1536, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 7.755, mean reward: 0.408 [0.278, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.702, 10.407], loss: 0.002861, mae: 0.055949, mean_q: 0.076890
 86125/100000: episode: 1537, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 4.037, mean reward: 0.288 [0.236, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.350], loss: 0.003051, mae: 0.056430, mean_q: 0.080669
 86129/100000: episode: 1538, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 1.435, mean reward: 0.359 [0.336, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.473], loss: 0.003556, mae: 0.062803, mean_q: 0.125515
 86135/100000: episode: 1539, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 2.299, mean reward: 0.383 [0.326, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.483], loss: 0.003965, mae: 0.064370, mean_q: 0.198646
 86141/100000: episode: 1540, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 2.700, mean reward: 0.450 [0.366, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.601], loss: 0.003262, mae: 0.060099, mean_q: 0.058874
 86145/100000: episode: 1541, duration: 0.025s, episode steps: 4, steps per second: 161, episode reward: 1.607, mean reward: 0.402 [0.370, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.529], loss: 0.003170, mae: 0.056649, mean_q: 0.086576
 86152/100000: episode: 1542, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 2.615, mean reward: 0.374 [0.303, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.432], loss: 0.003599, mae: 0.062874, mean_q: 0.072127
 86165/100000: episode: 1543, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 5.414, mean reward: 0.416 [0.308, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.541], loss: 0.002795, mae: 0.054018, mean_q: 0.130155
 86172/100000: episode: 1544, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 3.234, mean reward: 0.462 [0.369, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.215, 10.626], loss: 0.002695, mae: 0.052479, mean_q: 0.083306
 86178/100000: episode: 1545, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 2.163, mean reward: 0.360 [0.323, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.452], loss: 0.002939, mae: 0.056815, mean_q: -0.008182
 86179/100000: episode: 1546, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.429, mean reward: 0.429 [0.429, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.526], loss: 0.003134, mae: 0.063975, mean_q: 0.250128
 86183/100000: episode: 1547, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 1.609, mean reward: 0.402 [0.355, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.452, 10.541], loss: 0.003629, mae: 0.058553, mean_q: 0.030211
 86197/100000: episode: 1548, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 5.184, mean reward: 0.370 [0.321, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.324, 10.395], loss: 0.003157, mae: 0.056542, mean_q: 0.074778
 86204/100000: episode: 1549, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 2.766, mean reward: 0.395 [0.329, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.499], loss: 0.003361, mae: 0.062244, mean_q: 0.214127
 86208/100000: episode: 1550, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 1.630, mean reward: 0.407 [0.381, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.491], loss: 0.002889, mae: 0.056100, mean_q: -0.017093
 86218/100000: episode: 1551, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 3.910, mean reward: 0.391 [0.342, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.536, 10.471], loss: 0.003787, mae: 0.063262, mean_q: 0.073343
 86237/100000: episode: 1552, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 9.053, mean reward: 0.476 [0.446, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.616], loss: 0.003285, mae: 0.060153, mean_q: 0.093988
 86250/100000: episode: 1553, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 4.638, mean reward: 0.357 [0.294, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.482], loss: 0.002870, mae: 0.056294, mean_q: 0.054257
 86260/100000: episode: 1554, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 3.664, mean reward: 0.366 [0.308, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.498], loss: 0.003081, mae: 0.059806, mean_q: 0.084088
 86264/100000: episode: 1555, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 1.512, mean reward: 0.378 [0.341, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.489], loss: 0.003124, mae: 0.057339, mean_q: 0.076770
 86271/100000: episode: 1556, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 2.241, mean reward: 0.320 [0.265, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.394], loss: 0.004246, mae: 0.067599, mean_q: 0.128831
 86278/100000: episode: 1557, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 2.619, mean reward: 0.374 [0.329, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.487], loss: 0.003811, mae: 0.064691, mean_q: 0.224853
 86284/100000: episode: 1558, duration: 0.036s, episode steps: 6, steps per second: 169, episode reward: 2.006, mean reward: 0.334 [0.297, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.453], loss: 0.003459, mae: 0.061947, mean_q: 0.140829
 86285/100000: episode: 1559, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.417, mean reward: 0.417 [0.417, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.726, 10.501], loss: 0.003724, mae: 0.063012, mean_q: 0.205029
 86304/100000: episode: 1560, duration: 0.098s, episode steps: 19, steps per second: 195, episode reward: 9.250, mean reward: 0.487 [0.397, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.285, 10.390], loss: 0.003635, mae: 0.061998, mean_q: 0.130467
 86318/100000: episode: 1561, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 5.937, mean reward: 0.424 [0.351, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.503], loss: 0.003867, mae: 0.063546, mean_q: 0.107059
 86331/100000: episode: 1562, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 4.406, mean reward: 0.339 [0.238, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.375, 10.576], loss: 0.003170, mae: 0.059156, mean_q: 0.148438
 86338/100000: episode: 1563, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 2.341, mean reward: 0.334 [0.214, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.256], loss: 0.003134, mae: 0.059195, mean_q: 0.062446
 86352/100000: episode: 1564, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 5.562, mean reward: 0.397 [0.291, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.333, 10.443], loss: 0.002806, mae: 0.054109, mean_q: 0.099964
 86356/100000: episode: 1565, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 1.342, mean reward: 0.336 [0.303, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.454], loss: 0.003188, mae: 0.058918, mean_q: 0.041869
 86375/100000: episode: 1566, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 9.428, mean reward: 0.496 [0.462, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.929, 10.615], loss: 0.003694, mae: 0.062248, mean_q: 0.154118
 86388/100000: episode: 1567, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 5.155, mean reward: 0.397 [0.240, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.413, 10.507], loss: 0.003228, mae: 0.059269, mean_q: 0.146790
 86401/100000: episode: 1568, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 4.350, mean reward: 0.335 [0.268, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.457], loss: 0.002913, mae: 0.057549, mean_q: 0.164025
 86414/100000: episode: 1569, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 4.410, mean reward: 0.339 [0.188, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.487], loss: 0.002564, mae: 0.054859, mean_q: 0.154663
 86428/100000: episode: 1570, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 4.422, mean reward: 0.316 [0.096, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.336], loss: 0.003435, mae: 0.058919, mean_q: 0.137559
 86442/100000: episode: 1571, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 5.606, mean reward: 0.400 [0.357, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.552], loss: 0.003604, mae: 0.062648, mean_q: 0.189282
 86452/100000: episode: 1572, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 4.148, mean reward: 0.415 [0.367, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.519], loss: 0.002911, mae: 0.056939, mean_q: 0.167849
 86462/100000: episode: 1573, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 3.957, mean reward: 0.396 [0.347, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.491], loss: 0.003839, mae: 0.061998, mean_q: 0.110138
 86475/100000: episode: 1574, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 4.638, mean reward: 0.357 [0.189, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.455], loss: 0.003647, mae: 0.064126, mean_q: 0.196375
 86479/100000: episode: 1575, duration: 0.029s, episode steps: 4, steps per second: 136, episode reward: 1.785, mean reward: 0.446 [0.404, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.555], loss: 0.003539, mae: 0.061613, mean_q: 0.158614
 86489/100000: episode: 1576, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 3.620, mean reward: 0.362 [0.325, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.490], loss: 0.003801, mae: 0.064937, mean_q: 0.190271
 86508/100000: episode: 1577, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 7.816, mean reward: 0.411 [0.271, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.142, 10.441], loss: 0.003506, mae: 0.062405, mean_q: 0.169581
 86515/100000: episode: 1578, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 2.607, mean reward: 0.372 [0.335, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.549], loss: 0.003553, mae: 0.059934, mean_q: 0.180261
 86529/100000: episode: 1579, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 4.703, mean reward: 0.336 [0.290, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.420], loss: 0.003093, mae: 0.057442, mean_q: 0.156600
 86542/100000: episode: 1580, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 4.394, mean reward: 0.338 [0.246, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.299, 10.463], loss: 0.004075, mae: 0.068143, mean_q: 0.185108
 86549/100000: episode: 1581, duration: 0.055s, episode steps: 7, steps per second: 128, episode reward: 2.370, mean reward: 0.339 [0.279, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.479], loss: 0.003836, mae: 0.061460, mean_q: 0.086725
 86559/100000: episode: 1582, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 4.208, mean reward: 0.421 [0.375, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.548], loss: 0.003990, mae: 0.066408, mean_q: 0.108752
 86560/100000: episode: 1583, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.444, mean reward: 0.444 [0.444, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.375 [-0.035, 10.534], loss: 0.004540, mae: 0.068019, mean_q: 0.116392
 86561/100000: episode: 1584, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.324, mean reward: 0.324 [0.324, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.405], loss: 0.003132, mae: 0.055629, mean_q: 0.107899
 86568/100000: episode: 1585, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 2.791, mean reward: 0.399 [0.351, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.464], loss: 0.003452, mae: 0.060833, mean_q: 0.140419
 86575/100000: episode: 1586, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 2.676, mean reward: 0.382 [0.325, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.328, 10.467], loss: 0.002951, mae: 0.057426, mean_q: 0.239634
 86594/100000: episode: 1587, duration: 0.094s, episode steps: 19, steps per second: 202, episode reward: 9.331, mean reward: 0.491 [0.370, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.423], loss: 0.003430, mae: 0.060720, mean_q: 0.166155
 86607/100000: episode: 1588, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 4.623, mean reward: 0.356 [0.186, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.132, 10.593], loss: 0.002801, mae: 0.056220, mean_q: 0.166938
 86626/100000: episode: 1589, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 9.164, mean reward: 0.482 [0.384, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.565], loss: 0.003049, mae: 0.057246, mean_q: 0.190503
 86645/100000: episode: 1590, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 8.046, mean reward: 0.423 [0.258, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.045, 10.406], loss: 0.003660, mae: 0.061174, mean_q: 0.258060
 86651/100000: episode: 1591, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 2.798, mean reward: 0.466 [0.397, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.595], loss: 0.003922, mae: 0.065058, mean_q: 0.151703
 86665/100000: episode: 1592, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 3.956, mean reward: 0.283 [0.225, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.411], loss: 0.003805, mae: 0.062870, mean_q: 0.137557
 86684/100000: episode: 1593, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 6.811, mean reward: 0.358 [0.161, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.418], loss: 0.003551, mae: 0.063691, mean_q: 0.143611
 86685/100000: episode: 1594, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.371, mean reward: 0.371 [0.371, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.469], loss: 0.002914, mae: 0.054534, mean_q: 0.435546
[Info] 300-TH LEVEL FOUND: 0.8521786332130432, Considering 10/90 traces
 86698/100000: episode: 1595, duration: 3.897s, episode steps: 13, steps per second: 3, episode reward: 4.758, mean reward: 0.366 [0.262, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.531], loss: 0.002737, mae: 0.056494, mean_q: 0.189720
 86706/100000: episode: 1596, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 4.045, mean reward: 0.506 [0.446, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.101, 10.550], loss: 0.002818, mae: 0.054564, mean_q: 0.135932
 86714/100000: episode: 1597, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 3.206, mean reward: 0.401 [0.350, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.526], loss: 0.003274, mae: 0.059746, mean_q: 0.274804
 86730/100000: episode: 1598, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 6.879, mean reward: 0.430 [0.362, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.545], loss: 0.003103, mae: 0.055289, mean_q: 0.141113
 86741/100000: episode: 1599, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 4.885, mean reward: 0.444 [0.368, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.115, 10.525], loss: 0.003394, mae: 0.060895, mean_q: 0.174235
 86756/100000: episode: 1600, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 8.002, mean reward: 0.533 [0.436, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.641], loss: 0.003588, mae: 0.062436, mean_q: 0.200958
 86765/100000: episode: 1601, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 3.766, mean reward: 0.418 [0.368, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.423], loss: 0.003652, mae: 0.061503, mean_q: 0.204370
 86769/100000: episode: 1602, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 2.030, mean reward: 0.508 [0.491, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.610], loss: 0.003828, mae: 0.063209, mean_q: 0.185152
 86773/100000: episode: 1603, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 2.240, mean reward: 0.560 [0.521, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.683], loss: 0.004065, mae: 0.066427, mean_q: 0.234951
 86781/100000: episode: 1604, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 3.387, mean reward: 0.423 [0.364, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.469], loss: 0.003422, mae: 0.060581, mean_q: 0.206574
 86793/100000: episode: 1605, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 5.770, mean reward: 0.481 [0.322, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.887, 10.561], loss: 0.003811, mae: 0.062466, mean_q: 0.264303
 86802/100000: episode: 1606, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 3.478, mean reward: 0.386 [0.259, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.464], loss: 0.003492, mae: 0.058668, mean_q: 0.201955
 86814/100000: episode: 1607, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 5.628, mean reward: 0.469 [0.428, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.501], loss: 0.003712, mae: 0.064619, mean_q: 0.241721
 86829/100000: episode: 1608, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 8.341, mean reward: 0.556 [0.490, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.561], loss: 0.003740, mae: 0.063785, mean_q: 0.220373
 86838/100000: episode: 1609, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 3.998, mean reward: 0.444 [0.393, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.550], loss: 0.003074, mae: 0.055376, mean_q: 0.239552
 86853/100000: episode: 1610, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 8.917, mean reward: 0.594 [0.523, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.780, 10.726], loss: 0.003293, mae: 0.060988, mean_q: 0.265346
 86864/100000: episode: 1611, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 4.555, mean reward: 0.414 [0.359, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.497], loss: 0.003713, mae: 0.062170, mean_q: 0.244403
 86879/100000: episode: 1612, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 5.773, mean reward: 0.385 [0.171, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.116, 10.272], loss: 0.003798, mae: 0.063900, mean_q: 0.196008
 86894/100000: episode: 1613, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 7.194, mean reward: 0.480 [0.381, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.203, 10.568], loss: 0.003965, mae: 0.068214, mean_q: 0.325162
 86898/100000: episode: 1614, duration: 0.032s, episode steps: 4, steps per second: 126, episode reward: 2.138, mean reward: 0.534 [0.485, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.588], loss: 0.003978, mae: 0.067129, mean_q: 0.275648
 86907/100000: episode: 1615, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 4.246, mean reward: 0.472 [0.412, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.077, 10.684], loss: 0.003613, mae: 0.062329, mean_q: 0.322108
 86919/100000: episode: 1616, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 4.564, mean reward: 0.380 [0.310, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.209, 10.457], loss: 0.003780, mae: 0.060538, mean_q: 0.170017
 86928/100000: episode: 1617, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 4.746, mean reward: 0.527 [0.490, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.635], loss: 0.003256, mae: 0.059775, mean_q: 0.285721
 86943/100000: episode: 1618, duration: 0.073s, episode steps: 15, steps per second: 206, episode reward: 5.683, mean reward: 0.379 [0.226, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.386], loss: 0.003647, mae: 0.062214, mean_q: 0.278815
 86955/100000: episode: 1619, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 5.821, mean reward: 0.485 [0.418, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.596], loss: 0.003006, mae: 0.057570, mean_q: 0.268889
 86963/100000: episode: 1620, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 3.659, mean reward: 0.457 [0.391, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.562], loss: 0.003734, mae: 0.064695, mean_q: 0.265938
 86975/100000: episode: 1621, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 5.433, mean reward: 0.453 [0.384, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.542], loss: 0.003148, mae: 0.059564, mean_q: 0.288919
 86990/100000: episode: 1622, duration: 0.072s, episode steps: 15, steps per second: 208, episode reward: 5.406, mean reward: 0.360 [0.216, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.361], loss: 0.003871, mae: 0.062215, mean_q: 0.226408
 86998/100000: episode: 1623, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 3.238, mean reward: 0.405 [0.381, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.477], loss: 0.003646, mae: 0.065343, mean_q: 0.282980
 87002/100000: episode: 1624, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 2.106, mean reward: 0.527 [0.492, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.427, 10.633], loss: 0.003628, mae: 0.063879, mean_q: 0.160531
 87017/100000: episode: 1625, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 7.792, mean reward: 0.519 [0.441, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.547], loss: 0.003463, mae: 0.061380, mean_q: 0.324709
 87026/100000: episode: 1626, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 4.944, mean reward: 0.549 [0.467, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.184, 10.568], loss: 0.003129, mae: 0.059547, mean_q: 0.271925
 87041/100000: episode: 1627, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 7.374, mean reward: 0.492 [0.421, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.089, 10.565], loss: 0.003621, mae: 0.064185, mean_q: 0.326589
 87056/100000: episode: 1628, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 6.563, mean reward: 0.438 [0.366, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.523, 10.433], loss: 0.003718, mae: 0.062949, mean_q: 0.312255
 87060/100000: episode: 1629, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 1.968, mean reward: 0.492 [0.459, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.608, 10.507], loss: 0.003002, mae: 0.056301, mean_q: 0.333188
 87075/100000: episode: 1630, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 5.980, mean reward: 0.399 [0.327, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.407], loss: 0.003660, mae: 0.062048, mean_q: 0.280261
 87087/100000: episode: 1631, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 5.754, mean reward: 0.479 [0.392, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.112, 10.567], loss: 0.004018, mae: 0.065744, mean_q: 0.308207
 87102/100000: episode: 1632, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 6.692, mean reward: 0.446 [0.202, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.396, 10.416], loss: 0.003095, mae: 0.057009, mean_q: 0.316724
 87114/100000: episode: 1633, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 5.682, mean reward: 0.474 [0.315, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.118, 10.401], loss: 0.003125, mae: 0.057137, mean_q: 0.246029
 87123/100000: episode: 1634, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 4.063, mean reward: 0.451 [0.424, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.563], loss: 0.003186, mae: 0.060370, mean_q: 0.269806
 87138/100000: episode: 1635, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 7.611, mean reward: 0.507 [0.454, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.093, 10.591], loss: 0.003536, mae: 0.062751, mean_q: 0.286575
[Info] FALSIFICATION!
 87147/100000: episode: 1636, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 14.660, mean reward: 1.629 [0.530, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.014, 10.833], loss: 0.004244, mae: 0.066251, mean_q: 0.277692
 87247/100000: episode: 1637, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.772, mean reward: -0.148 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.489, 10.255], loss: 0.003738, mae: 0.064087, mean_q: 0.301109
 87347/100000: episode: 1638, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.284, mean reward: -0.173 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.844, 10.108], loss: 0.004870, mae: 0.069470, mean_q: 0.356050
 87447/100000: episode: 1639, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.646, mean reward: -0.196 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.845, 10.223], loss: 0.018284, mae: 0.080017, mean_q: 0.312107
 87547/100000: episode: 1640, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -19.946, mean reward: -0.199 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.550, 10.164], loss: 0.017237, mae: 0.071541, mean_q: 0.308828
 87647/100000: episode: 1641, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.707, mean reward: -0.157 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.896, 10.239], loss: 0.016537, mae: 0.065330, mean_q: 0.317857
 87747/100000: episode: 1642, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.910, mean reward: -0.179 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.416, 10.159], loss: 0.004573, mae: 0.070705, mean_q: 0.289308
 87847/100000: episode: 1643, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.029, mean reward: -0.190 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.047, 10.098], loss: 0.003468, mae: 0.061996, mean_q: 0.297119
 87947/100000: episode: 1644, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -13.674, mean reward: -0.137 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.111, 10.301], loss: 0.003710, mae: 0.062768, mean_q: 0.285294
 88047/100000: episode: 1645, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.744, mean reward: -0.177 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.706, 10.187], loss: 0.030635, mae: 0.078372, mean_q: 0.320737
 88147/100000: episode: 1646, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.100, mean reward: -0.191 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.069, 10.100], loss: 0.003938, mae: 0.065313, mean_q: 0.297535
 88247/100000: episode: 1647, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -9.677, mean reward: -0.097 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.788, 10.098], loss: 0.030515, mae: 0.077861, mean_q: 0.322062
 88347/100000: episode: 1648, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.226, mean reward: -0.172 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.570, 10.114], loss: 0.003407, mae: 0.060823, mean_q: 0.299300
 88447/100000: episode: 1649, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.153, mean reward: -0.152 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.483, 10.261], loss: 0.003301, mae: 0.059894, mean_q: 0.309882
 88547/100000: episode: 1650, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.605, mean reward: -0.166 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.129, 10.098], loss: 0.003590, mae: 0.062407, mean_q: 0.305488
 88647/100000: episode: 1651, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -18.792, mean reward: -0.188 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.024, 10.115], loss: 0.004369, mae: 0.065443, mean_q: 0.284879
 88747/100000: episode: 1652, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.127, mean reward: -0.181 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.741, 10.127], loss: 0.005754, mae: 0.071418, mean_q: 0.305416
 88847/100000: episode: 1653, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.465, mean reward: -0.145 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.951, 10.098], loss: 0.004162, mae: 0.065410, mean_q: 0.316535
 88947/100000: episode: 1654, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.553, mean reward: -0.186 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.830, 10.098], loss: 0.019788, mae: 0.079830, mean_q: 0.294455
 89047/100000: episode: 1655, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -18.240, mean reward: -0.182 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.207, 10.161], loss: 0.018977, mae: 0.078278, mean_q: 0.268631
 89147/100000: episode: 1656, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.617, mean reward: -0.176 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.008, 10.129], loss: 0.017177, mae: 0.071819, mean_q: 0.255005
 89247/100000: episode: 1657, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -10.296, mean reward: -0.103 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.857, 10.098], loss: 0.017241, mae: 0.069720, mean_q: 0.221948
 89347/100000: episode: 1658, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -8.054, mean reward: -0.081 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.756, 10.387], loss: 0.003183, mae: 0.059453, mean_q: 0.214054
 89447/100000: episode: 1659, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.222, mean reward: -0.162 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.215, 10.098], loss: 0.003202, mae: 0.058190, mean_q: 0.189073
 89547/100000: episode: 1660, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -21.058, mean reward: -0.211 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.292, 10.135], loss: 0.016434, mae: 0.064042, mean_q: 0.183970
 89647/100000: episode: 1661, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.105, mean reward: -0.171 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.606, 10.272], loss: 0.003697, mae: 0.061718, mean_q: 0.166019
 89747/100000: episode: 1662, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.417, mean reward: -0.184 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.410, 10.261], loss: 0.030568, mae: 0.076350, mean_q: 0.162896
 89847/100000: episode: 1663, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -19.436, mean reward: -0.194 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.197, 10.098], loss: 0.003395, mae: 0.060015, mean_q: 0.154156
 89947/100000: episode: 1664, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.860, mean reward: -0.189 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.669, 10.098], loss: 0.003355, mae: 0.059874, mean_q: 0.117815
 90047/100000: episode: 1665, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -20.179, mean reward: -0.202 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.362, 10.303], loss: 0.003097, mae: 0.057057, mean_q: 0.064868
 90147/100000: episode: 1666, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -15.427, mean reward: -0.154 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.938, 10.120], loss: 0.016523, mae: 0.064962, mean_q: 0.078726
 90247/100000: episode: 1667, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.059, mean reward: -0.181 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.946, 10.098], loss: 0.016684, mae: 0.066640, mean_q: 0.082756
 90347/100000: episode: 1668, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.257, mean reward: -0.163 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.512, 10.272], loss: 0.003253, mae: 0.058054, mean_q: 0.057191
 90447/100000: episode: 1669, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -20.081, mean reward: -0.201 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.276, 10.244], loss: 0.002982, mae: 0.055628, mean_q: 0.016426
 90547/100000: episode: 1670, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.772, mean reward: -0.188 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.638, 10.209], loss: 0.003018, mae: 0.055580, mean_q: 0.010954
 90647/100000: episode: 1671, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -18.367, mean reward: -0.184 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.339, 10.224], loss: 0.016809, mae: 0.066870, mean_q: 0.002115
 90747/100000: episode: 1672, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.272, mean reward: -0.193 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.725, 10.200], loss: 0.029098, mae: 0.066128, mean_q: -0.062553
 90847/100000: episode: 1673, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.677, mean reward: -0.187 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.378, 10.148], loss: 0.018018, mae: 0.073792, mean_q: -0.015910
 90947/100000: episode: 1674, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -12.108, mean reward: -0.121 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.635, 10.133], loss: 0.016836, mae: 0.063001, mean_q: -0.080898
 91047/100000: episode: 1675, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -16.032, mean reward: -0.160 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.633, 10.336], loss: 0.003410, mae: 0.056872, mean_q: -0.086018
 91147/100000: episode: 1676, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -14.257, mean reward: -0.143 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.579, 10.331], loss: 0.003250, mae: 0.054140, mean_q: -0.119465
 91247/100000: episode: 1677, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.472, mean reward: -0.165 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.414, 10.098], loss: 0.016779, mae: 0.063842, mean_q: -0.141289
 91347/100000: episode: 1678, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.555, mean reward: -0.186 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.223, 10.098], loss: 0.003083, mae: 0.053684, mean_q: -0.186171
 91447/100000: episode: 1679, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -18.926, mean reward: -0.189 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.292, 10.098], loss: 0.002994, mae: 0.053250, mean_q: -0.185992
 91547/100000: episode: 1680, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.162, mean reward: -0.182 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.682, 10.118], loss: 0.018241, mae: 0.069468, mean_q: -0.202217
 91647/100000: episode: 1681, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.536, mean reward: -0.165 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.949, 10.098], loss: 0.003010, mae: 0.053883, mean_q: -0.218239
 91747/100000: episode: 1682, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.746, mean reward: -0.197 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.759, 10.098], loss: 0.002789, mae: 0.052268, mean_q: -0.224699
 91847/100000: episode: 1683, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -15.422, mean reward: -0.154 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.078, 10.210], loss: 0.002894, mae: 0.052469, mean_q: -0.265921
 91947/100000: episode: 1684, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -17.310, mean reward: -0.173 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.748, 10.244], loss: 0.002724, mae: 0.051106, mean_q: -0.285256
 92047/100000: episode: 1685, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.164, mean reward: -0.162 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.668, 10.098], loss: 0.002684, mae: 0.050476, mean_q: -0.313390
 92147/100000: episode: 1686, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.210, mean reward: -0.192 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.885, 10.137], loss: 0.029494, mae: 0.066482, mean_q: -0.317950
 92247/100000: episode: 1687, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -19.486, mean reward: -0.195 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.633, 10.098], loss: 0.002484, mae: 0.048842, mean_q: -0.358382
 92347/100000: episode: 1688, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.572, mean reward: -0.146 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.430, 10.098], loss: 0.002439, mae: 0.048803, mean_q: -0.293837
 92447/100000: episode: 1689, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -18.866, mean reward: -0.189 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.887, 10.098], loss: 0.002390, mae: 0.048233, mean_q: -0.313843
 92547/100000: episode: 1690, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.608, mean reward: -0.156 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.432, 10.122], loss: 0.002531, mae: 0.049794, mean_q: -0.321387
 92647/100000: episode: 1691, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -17.236, mean reward: -0.172 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.621, 10.270], loss: 0.002327, mae: 0.048312, mean_q: -0.308159
 92747/100000: episode: 1692, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.316, mean reward: -0.173 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.975, 10.239], loss: 0.002501, mae: 0.049136, mean_q: -0.348161
 92847/100000: episode: 1693, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -21.015, mean reward: -0.210 [-1.000, 0.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.680, 10.137], loss: 0.002617, mae: 0.050422, mean_q: -0.298542
 92947/100000: episode: 1694, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.280, mean reward: -0.173 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.497, 10.245], loss: 0.002488, mae: 0.049170, mean_q: -0.322666
 93047/100000: episode: 1695, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -14.154, mean reward: -0.142 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.975, 10.098], loss: 0.002674, mae: 0.050551, mean_q: -0.322632
 93147/100000: episode: 1696, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.783, mean reward: -0.178 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.771, 10.145], loss: 0.002532, mae: 0.049628, mean_q: -0.330104
 93247/100000: episode: 1697, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.876, mean reward: -0.129 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.443, 10.271], loss: 0.002668, mae: 0.051698, mean_q: -0.287455
 93347/100000: episode: 1698, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.084, mean reward: -0.161 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.831, 10.306], loss: 0.002735, mae: 0.052135, mean_q: -0.310912
 93447/100000: episode: 1699, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.351, mean reward: -0.184 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.573, 10.113], loss: 0.002818, mae: 0.052928, mean_q: -0.284272
 93547/100000: episode: 1700, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.291, mean reward: -0.163 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.719, 10.427], loss: 0.002786, mae: 0.052451, mean_q: -0.320773
 93647/100000: episode: 1701, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -15.040, mean reward: -0.150 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.685, 10.126], loss: 0.002668, mae: 0.051454, mean_q: -0.305326
 93747/100000: episode: 1702, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.164, mean reward: -0.172 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.798, 10.226], loss: 0.002779, mae: 0.051426, mean_q: -0.331618
 93847/100000: episode: 1703, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.418, mean reward: -0.184 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.075, 10.250], loss: 0.002812, mae: 0.054826, mean_q: -0.325789
 93947/100000: episode: 1704, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.447, mean reward: -0.184 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.568, 10.165], loss: 0.002964, mae: 0.053769, mean_q: -0.313797
 94047/100000: episode: 1705, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.832, mean reward: -0.178 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.510, 10.098], loss: 0.002643, mae: 0.050730, mean_q: -0.344499
 94147/100000: episode: 1706, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.502, mean reward: -0.175 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.407, 10.129], loss: 0.002690, mae: 0.051575, mean_q: -0.302675
 94247/100000: episode: 1707, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -13.908, mean reward: -0.139 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.829, 10.098], loss: 0.002618, mae: 0.050373, mean_q: -0.304538
 94347/100000: episode: 1708, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.335, mean reward: -0.173 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.456, 10.218], loss: 0.002866, mae: 0.052204, mean_q: -0.326064
 94447/100000: episode: 1709, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.448, mean reward: -0.164 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.669, 10.159], loss: 0.002498, mae: 0.049369, mean_q: -0.348487
 94547/100000: episode: 1710, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -20.015, mean reward: -0.200 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.682, 10.120], loss: 0.002960, mae: 0.053653, mean_q: -0.309991
 94647/100000: episode: 1711, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -13.544, mean reward: -0.135 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.855, 10.213], loss: 0.003488, mae: 0.056186, mean_q: -0.310895
 94747/100000: episode: 1712, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -12.415, mean reward: -0.124 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.794, 10.356], loss: 0.002583, mae: 0.050305, mean_q: -0.314555
 94847/100000: episode: 1713, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -20.111, mean reward: -0.201 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.152, 10.124], loss: 0.005611, mae: 0.063542, mean_q: -0.348449
 94947/100000: episode: 1714, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.371, mean reward: -0.174 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.522, 10.304], loss: 0.003289, mae: 0.055973, mean_q: -0.310985
 95047/100000: episode: 1715, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.877, mean reward: -0.169 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.487, 10.218], loss: 0.002580, mae: 0.049848, mean_q: -0.322420
 95147/100000: episode: 1716, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.255, mean reward: -0.193 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.770, 10.098], loss: 0.002677, mae: 0.051075, mean_q: -0.323293
 95247/100000: episode: 1717, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.256, mean reward: -0.163 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.884, 10.179], loss: 0.002825, mae: 0.052313, mean_q: -0.331541
 95347/100000: episode: 1718, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.170, mean reward: -0.172 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.868, 10.098], loss: 0.002688, mae: 0.050619, mean_q: -0.319782
 95447/100000: episode: 1719, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.042, mean reward: -0.180 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.359, 10.139], loss: 0.002652, mae: 0.050316, mean_q: -0.332402
 95547/100000: episode: 1720, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.570, mean reward: -0.176 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.934, 10.210], loss: 0.002695, mae: 0.051690, mean_q: -0.302929
 95647/100000: episode: 1721, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -11.050, mean reward: -0.111 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.325, 10.098], loss: 0.002820, mae: 0.051956, mean_q: -0.300289
 95747/100000: episode: 1722, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.892, mean reward: -0.169 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.634, 10.098], loss: 0.002714, mae: 0.051348, mean_q: -0.326637
 95847/100000: episode: 1723, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.144, mean reward: -0.171 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.621, 10.098], loss: 0.002679, mae: 0.051217, mean_q: -0.325164
 95947/100000: episode: 1724, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.464, mean reward: -0.165 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.333, 10.187], loss: 0.002786, mae: 0.051748, mean_q: -0.327315
 96047/100000: episode: 1725, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.043, mean reward: -0.190 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.150, 10.098], loss: 0.002543, mae: 0.049111, mean_q: -0.346383
 96147/100000: episode: 1726, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -14.819, mean reward: -0.148 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.193, 10.365], loss: 0.002665, mae: 0.051140, mean_q: -0.324824
 96247/100000: episode: 1727, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.585, mean reward: -0.176 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.231, 10.459], loss: 0.002711, mae: 0.051872, mean_q: -0.313058
 96347/100000: episode: 1728, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -18.082, mean reward: -0.181 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.669, 10.098], loss: 0.002560, mae: 0.049865, mean_q: -0.328730
 96447/100000: episode: 1729, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -19.190, mean reward: -0.192 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.526, 10.192], loss: 0.002765, mae: 0.052542, mean_q: -0.304346
 96547/100000: episode: 1730, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.029, mean reward: -0.170 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.547, 10.098], loss: 0.002669, mae: 0.051866, mean_q: -0.350944
 96647/100000: episode: 1731, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -13.610, mean reward: -0.136 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.721, 10.203], loss: 0.002670, mae: 0.051235, mean_q: -0.353183
 96747/100000: episode: 1732, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.186, mean reward: -0.162 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.628, 10.098], loss: 0.002522, mae: 0.049552, mean_q: -0.338755
 96847/100000: episode: 1733, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -15.795, mean reward: -0.158 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.927, 10.098], loss: 0.002503, mae: 0.050344, mean_q: -0.359105
 96947/100000: episode: 1734, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.254, mean reward: -0.173 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.717, 10.324], loss: 0.002718, mae: 0.051851, mean_q: -0.308518
 97047/100000: episode: 1735, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -14.823, mean reward: -0.148 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.488, 10.431], loss: 0.002433, mae: 0.048836, mean_q: -0.304154
[Info] 100-TH LEVEL FOUND: 0.5492870807647705, Considering 10/90 traces
 97147/100000: episode: 1736, duration: 4.248s, episode steps: 100, steps per second: 24, episode reward: -14.141, mean reward: -0.141 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.793, 10.098], loss: 0.005105, mae: 0.062301, mean_q: -0.314717
 97192/100000: episode: 1737, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 15.334, mean reward: 0.341 [0.195, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-1.101, 10.100], loss: 0.003825, mae: 0.060858, mean_q: -0.348996
 97246/100000: episode: 1738, duration: 0.265s, episode steps: 54, steps per second: 204, episode reward: 16.785, mean reward: 0.311 [0.138, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.676, 10.100], loss: 0.003074, mae: 0.056803, mean_q: -0.275582
 97264/100000: episode: 1739, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 5.477, mean reward: 0.304 [0.195, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.600, 10.100], loss: 0.003201, mae: 0.057982, mean_q: -0.299797
 97309/100000: episode: 1740, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 15.734, mean reward: 0.350 [0.158, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.630, 10.100], loss: 0.002737, mae: 0.054695, mean_q: -0.252605
 97334/100000: episode: 1741, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 8.461, mean reward: 0.338 [0.201, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.452], loss: 0.002710, mae: 0.052501, mean_q: -0.280592
 97352/100000: episode: 1742, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 6.164, mean reward: 0.342 [0.208, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.301, 10.100], loss: 0.002343, mae: 0.050267, mean_q: -0.245580
 97397/100000: episode: 1743, duration: 0.229s, episode steps: 45, steps per second: 197, episode reward: 10.578, mean reward: 0.235 [0.052, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.749, 10.100], loss: 0.002949, mae: 0.054525, mean_q: -0.271493
 97444/100000: episode: 1744, duration: 0.257s, episode steps: 47, steps per second: 183, episode reward: 14.475, mean reward: 0.308 [0.136, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.436, 10.100], loss: 0.002936, mae: 0.053071, mean_q: -0.272220
 97463/100000: episode: 1745, duration: 0.093s, episode steps: 19, steps per second: 205, episode reward: 7.619, mean reward: 0.401 [0.306, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.282, 10.324], loss: 0.003094, mae: 0.057538, mean_q: -0.239029
 97492/100000: episode: 1746, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 7.914, mean reward: 0.273 [0.171, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.186, 10.100], loss: 0.002646, mae: 0.052540, mean_q: -0.271033
 97537/100000: episode: 1747, duration: 0.247s, episode steps: 45, steps per second: 182, episode reward: 13.899, mean reward: 0.309 [0.214, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-1.575, 10.100], loss: 0.002662, mae: 0.052417, mean_q: -0.262383
 97591/100000: episode: 1748, duration: 0.275s, episode steps: 54, steps per second: 196, episode reward: 22.281, mean reward: 0.413 [0.251, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-0.732, 10.100], loss: 0.002853, mae: 0.055145, mean_q: -0.215056
 97638/100000: episode: 1749, duration: 0.264s, episode steps: 47, steps per second: 178, episode reward: 8.810, mean reward: 0.187 [0.037, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.165, 10.100], loss: 0.003054, mae: 0.055768, mean_q: -0.259327
 97664/100000: episode: 1750, duration: 0.134s, episode steps: 26, steps per second: 195, episode reward: 7.956, mean reward: 0.306 [0.129, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.431, 10.100], loss: 0.003114, mae: 0.057366, mean_q: -0.148235
 97675/100000: episode: 1751, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 2.533, mean reward: 0.230 [0.185, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.305, 10.100], loss: 0.003289, mae: 0.060835, mean_q: -0.170560
 97720/100000: episode: 1752, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 12.469, mean reward: 0.277 [0.120, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.132, 10.100], loss: 0.002701, mae: 0.052992, mean_q: -0.201755
 97774/100000: episode: 1753, duration: 0.268s, episode steps: 54, steps per second: 201, episode reward: 15.266, mean reward: 0.283 [0.043, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-1.211, 10.100], loss: 0.002753, mae: 0.054218, mean_q: -0.183837
 97800/100000: episode: 1754, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 10.005, mean reward: 0.385 [0.289, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.570, 10.100], loss: 0.002425, mae: 0.050176, mean_q: -0.194070
 97825/100000: episode: 1755, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 8.749, mean reward: 0.350 [0.234, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.161, 10.336], loss: 0.002490, mae: 0.051591, mean_q: -0.162624
 97843/100000: episode: 1756, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 4.321, mean reward: 0.240 [0.138, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.840, 10.100], loss: 0.002105, mae: 0.046624, mean_q: -0.221707
 97861/100000: episode: 1757, duration: 0.086s, episode steps: 18, steps per second: 209, episode reward: 5.438, mean reward: 0.302 [0.243, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.312, 10.100], loss: 0.002769, mae: 0.053471, mean_q: -0.229637
 97886/100000: episode: 1758, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 6.336, mean reward: 0.253 [0.120, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.792, 10.384], loss: 0.002674, mae: 0.054669, mean_q: -0.157610
 97912/100000: episode: 1759, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 6.428, mean reward: 0.247 [0.071, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.096, 10.100], loss: 0.002699, mae: 0.053606, mean_q: -0.154813
 97957/100000: episode: 1760, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: 12.139, mean reward: 0.270 [0.080, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.637, 10.100], loss: 0.002715, mae: 0.052486, mean_q: -0.189544
 97975/100000: episode: 1761, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 3.883, mean reward: 0.216 [0.136, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.199, 10.100], loss: 0.002639, mae: 0.054252, mean_q: -0.070184
 98000/100000: episode: 1762, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 8.437, mean reward: 0.337 [0.275, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.555, 10.426], loss: 0.002409, mae: 0.049940, mean_q: -0.197018
 98011/100000: episode: 1763, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 3.465, mean reward: 0.315 [0.211, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.424, 10.100], loss: 0.003013, mae: 0.053971, mean_q: -0.225806
 98058/100000: episode: 1764, duration: 0.240s, episode steps: 47, steps per second: 196, episode reward: 13.898, mean reward: 0.296 [0.106, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.482, 10.100], loss: 0.002767, mae: 0.053725, mean_q: -0.162553
 98069/100000: episode: 1765, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 3.132, mean reward: 0.285 [0.241, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.276, 10.100], loss: 0.002933, mae: 0.055446, mean_q: -0.195539
 98123/100000: episode: 1766, duration: 0.274s, episode steps: 54, steps per second: 197, episode reward: 12.260, mean reward: 0.227 [0.060, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.337, 10.100], loss: 0.002571, mae: 0.051747, mean_q: -0.133743
 98170/100000: episode: 1767, duration: 0.224s, episode steps: 47, steps per second: 210, episode reward: 10.721, mean reward: 0.228 [0.076, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-1.610, 10.100], loss: 0.002566, mae: 0.050890, mean_q: -0.137727
 98224/100000: episode: 1768, duration: 0.272s, episode steps: 54, steps per second: 198, episode reward: 16.844, mean reward: 0.312 [0.085, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.657, 10.100], loss: 0.002719, mae: 0.053617, mean_q: -0.132659
 98243/100000: episode: 1769, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 9.379, mean reward: 0.494 [0.392, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.218, 10.598], loss: 0.002917, mae: 0.059140, mean_q: -0.079412
 98297/100000: episode: 1770, duration: 0.283s, episode steps: 54, steps per second: 191, episode reward: 16.348, mean reward: 0.303 [0.084, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.866 [-0.562, 10.100], loss: 0.002838, mae: 0.057521, mean_q: -0.098517
 98315/100000: episode: 1771, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 4.223, mean reward: 0.235 [0.123, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.559, 10.100], loss: 0.002589, mae: 0.052384, mean_q: -0.089936
 98360/100000: episode: 1772, duration: 0.219s, episode steps: 45, steps per second: 206, episode reward: 10.874, mean reward: 0.242 [0.023, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.120, 10.100], loss: 0.002562, mae: 0.052592, mean_q: -0.063662
 98405/100000: episode: 1773, duration: 0.242s, episode steps: 45, steps per second: 186, episode reward: 12.941, mean reward: 0.288 [0.130, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.458, 10.100], loss: 0.002833, mae: 0.055556, mean_q: -0.095205
 98459/100000: episode: 1774, duration: 0.271s, episode steps: 54, steps per second: 199, episode reward: 16.640, mean reward: 0.308 [0.217, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-0.375, 10.100], loss: 0.002790, mae: 0.053877, mean_q: -0.100668
 98504/100000: episode: 1775, duration: 0.219s, episode steps: 45, steps per second: 205, episode reward: 13.781, mean reward: 0.306 [0.150, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.658, 10.100], loss: 0.002576, mae: 0.052298, mean_q: -0.074403
 98549/100000: episode: 1776, duration: 0.250s, episode steps: 45, steps per second: 180, episode reward: 14.753, mean reward: 0.328 [0.202, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-1.122, 10.100], loss: 0.002983, mae: 0.055988, mean_q: -0.045065
 98602/100000: episode: 1777, duration: 0.282s, episode steps: 53, steps per second: 188, episode reward: 11.224, mean reward: 0.212 [0.021, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-1.009, 10.100], loss: 0.003038, mae: 0.058461, mean_q: -0.019433
 98627/100000: episode: 1778, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 8.677, mean reward: 0.347 [0.197, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.135, 10.399], loss: 0.002295, mae: 0.049956, mean_q: -0.025359
 98645/100000: episode: 1779, duration: 0.086s, episode steps: 18, steps per second: 210, episode reward: 5.793, mean reward: 0.322 [0.171, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.208, 10.100], loss: 0.002688, mae: 0.054205, mean_q: -0.086270
 98670/100000: episode: 1780, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 10.551, mean reward: 0.422 [0.308, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.210, 10.494], loss: 0.002737, mae: 0.055512, mean_q: -0.008962
 98695/100000: episode: 1781, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 5.960, mean reward: 0.238 [0.067, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.262], loss: 0.002605, mae: 0.054513, mean_q: -0.032928
 98740/100000: episode: 1782, duration: 0.212s, episode steps: 45, steps per second: 212, episode reward: 11.135, mean reward: 0.247 [0.035, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.261, 10.100], loss: 0.002761, mae: 0.054504, mean_q: -0.029155
 98787/100000: episode: 1783, duration: 0.234s, episode steps: 47, steps per second: 201, episode reward: 9.081, mean reward: 0.193 [0.056, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-1.146, 10.280], loss: 0.002917, mae: 0.056150, mean_q: -0.038912
 98812/100000: episode: 1784, duration: 0.121s, episode steps: 25, steps per second: 206, episode reward: 7.669, mean reward: 0.307 [0.206, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.401], loss: 0.003089, mae: 0.060438, mean_q: 0.058940
 98841/100000: episode: 1785, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 6.446, mean reward: 0.222 [0.089, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.610, 10.100], loss: 0.003032, mae: 0.058088, mean_q: 0.040127
 98894/100000: episode: 1786, duration: 0.274s, episode steps: 53, steps per second: 193, episode reward: 11.413, mean reward: 0.215 [0.014, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-1.180, 10.100], loss: 0.002667, mae: 0.054439, mean_q: -0.011457
 98905/100000: episode: 1787, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 3.962, mean reward: 0.360 [0.230, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.402, 10.100], loss: 0.002788, mae: 0.055474, mean_q: 0.084911
 98916/100000: episode: 1788, duration: 0.061s, episode steps: 11, steps per second: 182, episode reward: 2.803, mean reward: 0.255 [0.183, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.239, 10.100], loss: 0.003006, mae: 0.056433, mean_q: 0.014607
 98969/100000: episode: 1789, duration: 0.256s, episode steps: 53, steps per second: 207, episode reward: 14.892, mean reward: 0.281 [0.165, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-1.044, 10.100], loss: 0.002681, mae: 0.054694, mean_q: 0.045242
 98988/100000: episode: 1790, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 8.750, mean reward: 0.461 [0.352, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.409], loss: 0.002786, mae: 0.056010, mean_q: 0.015680
 99006/100000: episode: 1791, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 4.119, mean reward: 0.229 [0.142, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.138, 10.100], loss: 0.002606, mae: 0.052570, mean_q: -0.003787
 99059/100000: episode: 1792, duration: 0.266s, episode steps: 53, steps per second: 200, episode reward: 14.981, mean reward: 0.283 [0.162, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-1.066, 10.100], loss: 0.002759, mae: 0.055193, mean_q: 0.002179
 99104/100000: episode: 1793, duration: 0.244s, episode steps: 45, steps per second: 185, episode reward: 7.678, mean reward: 0.171 [0.017, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-1.793, 10.180], loss: 0.002656, mae: 0.055681, mean_q: 0.016961
 99157/100000: episode: 1794, duration: 0.276s, episode steps: 53, steps per second: 192, episode reward: 9.273, mean reward: 0.175 [0.018, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.888, 10.101], loss: 0.002749, mae: 0.054908, mean_q: 0.046394
 99176/100000: episode: 1795, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 7.328, mean reward: 0.386 [0.315, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.438], loss: 0.003221, mae: 0.058205, mean_q: 0.023773
 99187/100000: episode: 1796, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 3.766, mean reward: 0.342 [0.251, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.331, 10.100], loss: 0.002711, mae: 0.056483, mean_q: 0.067739
 99240/100000: episode: 1797, duration: 0.276s, episode steps: 53, steps per second: 192, episode reward: 11.301, mean reward: 0.213 [0.016, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-1.210, 10.279], loss: 0.002790, mae: 0.056534, mean_q: 0.060148
 99285/100000: episode: 1798, duration: 0.229s, episode steps: 45, steps per second: 196, episode reward: 8.813, mean reward: 0.196 [0.036, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.527, 10.197], loss: 0.002652, mae: 0.054809, mean_q: 0.051935
 99314/100000: episode: 1799, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 7.919, mean reward: 0.273 [0.149, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.298, 10.100], loss: 0.002574, mae: 0.054152, mean_q: 0.048388
 99343/100000: episode: 1800, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 9.774, mean reward: 0.337 [0.236, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.494, 10.100], loss: 0.003162, mae: 0.061099, mean_q: 0.078945
 99397/100000: episode: 1801, duration: 0.285s, episode steps: 54, steps per second: 189, episode reward: 11.339, mean reward: 0.210 [0.024, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.269, 10.214], loss: 0.002941, mae: 0.057610, mean_q: 0.062430
 99423/100000: episode: 1802, duration: 0.128s, episode steps: 26, steps per second: 204, episode reward: 7.676, mean reward: 0.295 [0.110, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.267, 10.100], loss: 0.002928, mae: 0.057286, mean_q: 0.046628
 99468/100000: episode: 1803, duration: 0.226s, episode steps: 45, steps per second: 199, episode reward: 10.477, mean reward: 0.233 [0.091, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.996, 10.100], loss: 0.003232, mae: 0.060514, mean_q: 0.113977
 99486/100000: episode: 1804, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 5.793, mean reward: 0.322 [0.191, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.637, 10.100], loss: 0.002862, mae: 0.057888, mean_q: 0.113256
 99504/100000: episode: 1805, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 4.400, mean reward: 0.244 [0.159, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.310, 10.100], loss: 0.002746, mae: 0.057995, mean_q: 0.160848
 99515/100000: episode: 1806, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 3.407, mean reward: 0.310 [0.269, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.265, 10.100], loss: 0.002747, mae: 0.054532, mean_q: 0.106462
 99568/100000: episode: 1807, duration: 0.274s, episode steps: 53, steps per second: 193, episode reward: 11.640, mean reward: 0.220 [0.021, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-1.226, 10.224], loss: 0.002948, mae: 0.057799, mean_q: 0.138874
 99613/100000: episode: 1808, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 9.954, mean reward: 0.221 [0.063, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.467, 10.100], loss: 0.002989, mae: 0.056040, mean_q: 0.079940
 99666/100000: episode: 1809, duration: 0.263s, episode steps: 53, steps per second: 201, episode reward: 12.423, mean reward: 0.234 [0.023, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.645, 10.301], loss: 0.002952, mae: 0.057660, mean_q: 0.136430
 99692/100000: episode: 1810, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 6.502, mean reward: 0.250 [0.108, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.119, 10.100], loss: 0.002903, mae: 0.056862, mean_q: 0.143403
 99711/100000: episode: 1811, duration: 0.112s, episode steps: 19, steps per second: 169, episode reward: 7.519, mean reward: 0.396 [0.342, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-1.223, 10.480], loss: 0.002784, mae: 0.057540, mean_q: 0.142316
 99756/100000: episode: 1812, duration: 0.221s, episode steps: 45, steps per second: 204, episode reward: 8.169, mean reward: 0.182 [0.018, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.037, 10.156], loss: 0.002984, mae: 0.058035, mean_q: 0.151654
 99781/100000: episode: 1813, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 7.772, mean reward: 0.311 [0.193, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.398], loss: 0.003068, mae: 0.059626, mean_q: 0.163897
 99806/100000: episode: 1814, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 9.030, mean reward: 0.361 [0.234, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.143, 10.435], loss: 0.002904, mae: 0.056656, mean_q: 0.171493
 99835/100000: episode: 1815, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 6.290, mean reward: 0.217 [0.117, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.302, 10.100], loss: 0.002974, mae: 0.057635, mean_q: 0.137873
 99846/100000: episode: 1816, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 3.412, mean reward: 0.310 [0.204, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.854, 10.100], loss: 0.003299, mae: 0.060494, mean_q: 0.178351
 99899/100000: episode: 1817, duration: 0.290s, episode steps: 53, steps per second: 183, episode reward: 15.432, mean reward: 0.291 [0.123, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.866 [-1.051, 10.100], loss: 0.003132, mae: 0.060117, mean_q: 0.144106
 99946/100000: episode: 1818, duration: 0.235s, episode steps: 47, steps per second: 200, episode reward: 15.106, mean reward: 0.321 [0.185, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.233, 10.100], loss: 0.002736, mae: 0.056217, mean_q: 0.214022
 99957/100000: episode: 1819, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 3.706, mean reward: 0.337 [0.265, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.374, 10.100], loss: 0.002782, mae: 0.057096, mean_q: 0.210377
done, took 578.447 seconds
[Info] End Importance Splitting. Falsification occurred 6 times.
