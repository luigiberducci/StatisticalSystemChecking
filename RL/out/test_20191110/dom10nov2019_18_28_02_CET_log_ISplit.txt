Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.174s, episode steps: 100, steps per second: 576, episode reward: -17.089, mean reward: -0.171 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.310, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.068s, episode steps: 100, steps per second: 1469, episode reward: -14.295, mean reward: -0.143 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.508, 10.396], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.059s, episode steps: 100, steps per second: 1707, episode reward: -15.942, mean reward: -0.159 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.005, 10.160], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.059s, episode steps: 100, steps per second: 1700, episode reward: -18.238, mean reward: -0.182 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.075, 10.280], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.059s, episode steps: 100, steps per second: 1705, episode reward: -18.120, mean reward: -0.181 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.643, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.227s, episode steps: 100, steps per second: 82, episode reward: -16.931, mean reward: -0.169 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.864, 10.359], loss: 0.135534, mae: 0.359561, mean_q: 0.934145
   700/100000: episode: 7, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.424, mean reward: -0.184 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.537, 10.239], loss: 0.020152, mae: 0.123705, mean_q: 0.396004
   800/100000: episode: 8, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.282, mean reward: -0.153 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.857, 10.157], loss: 0.014606, mae: 0.109300, mean_q: 0.140407
   900/100000: episode: 9, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.021, mean reward: -0.190 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.120, 10.170], loss: 0.011371, mae: 0.099297, mean_q: -0.033490
  1000/100000: episode: 10, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -17.628, mean reward: -0.176 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.701, 10.098], loss: 0.009770, mae: 0.094066, mean_q: -0.123758
  1100/100000: episode: 11, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -18.431, mean reward: -0.184 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.791, 10.376], loss: 0.008917, mae: 0.088706, mean_q: -0.216673
  1200/100000: episode: 12, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.085, mean reward: -0.161 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.475, 10.098], loss: 0.008377, mae: 0.086599, mean_q: -0.250398
  1300/100000: episode: 13, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.237, mean reward: -0.172 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.713, 10.123], loss: 0.007622, mae: 0.079396, mean_q: -0.320502
  1400/100000: episode: 14, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.100, mean reward: -0.181 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.380, 10.098], loss: 0.006604, mae: 0.076027, mean_q: -0.318174
  1500/100000: episode: 15, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -11.795, mean reward: -0.118 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.663, 10.418], loss: 0.006705, mae: 0.075959, mean_q: -0.310444
  1600/100000: episode: 16, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -12.202, mean reward: -0.122 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.884, 10.098], loss: 0.007702, mae: 0.082086, mean_q: -0.328043
  1700/100000: episode: 17, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.249, mean reward: -0.152 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.599, 10.098], loss: 0.007120, mae: 0.081386, mean_q: -0.300792
  1800/100000: episode: 18, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.968, mean reward: -0.180 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.760, 10.100], loss: 0.006196, mae: 0.074352, mean_q: -0.340317
  1900/100000: episode: 19, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.476, mean reward: -0.165 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.168, 10.268], loss: 0.005966, mae: 0.072599, mean_q: -0.324744
  2000/100000: episode: 20, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.219, mean reward: -0.162 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.234, 10.098], loss: 0.006679, mae: 0.074379, mean_q: -0.327760
  2100/100000: episode: 21, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.869, mean reward: -0.179 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.799, 10.098], loss: 0.005579, mae: 0.070983, mean_q: -0.337343
  2200/100000: episode: 22, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -14.560, mean reward: -0.146 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.497, 10.103], loss: 0.006379, mae: 0.072570, mean_q: -0.302264
  2300/100000: episode: 23, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.117, mean reward: -0.191 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.182, 10.145], loss: 0.006720, mae: 0.077256, mean_q: -0.290885
  2400/100000: episode: 24, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.362, mean reward: -0.154 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.678, 10.370], loss: 0.007230, mae: 0.076873, mean_q: -0.311141
  2500/100000: episode: 25, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.102, mean reward: -0.181 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.431, 10.132], loss: 0.006075, mae: 0.073437, mean_q: -0.339850
  2600/100000: episode: 26, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.311, mean reward: -0.183 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.190, 10.101], loss: 0.005453, mae: 0.070558, mean_q: -0.291241
  2700/100000: episode: 27, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -14.606, mean reward: -0.146 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.470, 10.251], loss: 0.005962, mae: 0.072289, mean_q: -0.323079
  2800/100000: episode: 28, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -17.698, mean reward: -0.177 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.312, 10.292], loss: 0.006139, mae: 0.072077, mean_q: -0.306686
  2900/100000: episode: 29, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.323, mean reward: -0.163 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.940, 10.098], loss: 0.005397, mae: 0.069236, mean_q: -0.329832
  3000/100000: episode: 30, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -19.047, mean reward: -0.190 [-1.000, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.727, 10.263], loss: 0.006423, mae: 0.073051, mean_q: -0.307234
  3100/100000: episode: 31, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -16.492, mean reward: -0.165 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.661, 10.281], loss: 0.004390, mae: 0.064364, mean_q: -0.320760
  3200/100000: episode: 32, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.527, mean reward: -0.185 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.745, 10.098], loss: 0.006166, mae: 0.073269, mean_q: -0.298867
  3300/100000: episode: 33, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.940, mean reward: -0.179 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.065, 10.313], loss: 0.005184, mae: 0.067033, mean_q: -0.325045
  3400/100000: episode: 34, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.529, mean reward: -0.165 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.753, 10.101], loss: 0.005535, mae: 0.070379, mean_q: -0.321662
  3500/100000: episode: 35, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -14.455, mean reward: -0.145 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.710, 10.098], loss: 0.006252, mae: 0.075946, mean_q: -0.354861
  3600/100000: episode: 36, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.807, mean reward: -0.188 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.340, 10.267], loss: 0.006160, mae: 0.072045, mean_q: -0.291020
  3700/100000: episode: 37, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.131, mean reward: -0.161 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.910, 10.219], loss: 0.004931, mae: 0.065721, mean_q: -0.327283
  3800/100000: episode: 38, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -13.506, mean reward: -0.135 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.206, 10.287], loss: 0.005970, mae: 0.072752, mean_q: -0.302158
  3900/100000: episode: 39, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -16.794, mean reward: -0.168 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.383, 10.116], loss: 0.004195, mae: 0.063692, mean_q: -0.310864
  4000/100000: episode: 40, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.766, mean reward: -0.168 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.138, 10.098], loss: 0.005362, mae: 0.068277, mean_q: -0.319375
  4100/100000: episode: 41, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.931, mean reward: -0.189 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.385, 10.157], loss: 0.005752, mae: 0.068441, mean_q: -0.293769
  4200/100000: episode: 42, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.401, mean reward: -0.184 [-1.000, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.578, 10.098], loss: 0.005162, mae: 0.068208, mean_q: -0.284165
  4300/100000: episode: 43, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.909, mean reward: -0.179 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.431, 10.137], loss: 0.004667, mae: 0.065253, mean_q: -0.321176
  4400/100000: episode: 44, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.330, mean reward: -0.183 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.150, 10.343], loss: 0.004822, mae: 0.066914, mean_q: -0.317286
  4500/100000: episode: 45, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -18.102, mean reward: -0.181 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.112, 10.098], loss: 0.004832, mae: 0.066060, mean_q: -0.313522
  4600/100000: episode: 46, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.739, mean reward: -0.177 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.538, 10.098], loss: 0.004764, mae: 0.066426, mean_q: -0.302441
  4700/100000: episode: 47, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -13.312, mean reward: -0.133 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.947, 10.107], loss: 0.005651, mae: 0.070988, mean_q: -0.305529
  4800/100000: episode: 48, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -20.191, mean reward: -0.202 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.506, 10.098], loss: 0.004888, mae: 0.066172, mean_q: -0.348031
  4900/100000: episode: 49, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -8.439, mean reward: -0.084 [-1.000, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.410, 10.098], loss: 0.006459, mae: 0.073620, mean_q: -0.322036
  5000/100000: episode: 50, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.698, mean reward: -0.177 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.426, 10.116], loss: 0.005959, mae: 0.075739, mean_q: -0.271507
  5100/100000: episode: 51, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.637, mean reward: -0.186 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.073, 10.212], loss: 0.004974, mae: 0.066327, mean_q: -0.333215
  5200/100000: episode: 52, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.333, mean reward: -0.173 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.337, 10.243], loss: 0.006370, mae: 0.073843, mean_q: -0.295930
  5300/100000: episode: 53, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.093, mean reward: -0.181 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.719, 10.098], loss: 0.004441, mae: 0.065841, mean_q: -0.306861
  5400/100000: episode: 54, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.600, mean reward: -0.196 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.843, 10.135], loss: 0.005651, mae: 0.072201, mean_q: -0.306690
  5500/100000: episode: 55, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.065, mean reward: -0.191 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.995, 10.128], loss: 0.005936, mae: 0.071141, mean_q: -0.312133
  5600/100000: episode: 56, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.637, mean reward: -0.196 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.021, 10.262], loss: 0.004927, mae: 0.069200, mean_q: -0.298494
  5700/100000: episode: 57, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.826, mean reward: -0.198 [-1.000, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.882, 10.098], loss: 0.004418, mae: 0.066748, mean_q: -0.304802
  5800/100000: episode: 58, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -13.253, mean reward: -0.133 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.621, 10.098], loss: 0.005094, mae: 0.068834, mean_q: -0.323751
  5900/100000: episode: 59, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.190, mean reward: -0.182 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.486, 10.112], loss: 0.005374, mae: 0.070540, mean_q: -0.344934
  6000/100000: episode: 60, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.978, mean reward: -0.150 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.565, 10.246], loss: 0.004640, mae: 0.065019, mean_q: -0.321299
  6100/100000: episode: 61, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -12.398, mean reward: -0.124 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.968, 10.098], loss: 0.005975, mae: 0.071965, mean_q: -0.342927
  6200/100000: episode: 62, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.616, mean reward: -0.146 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.256, 10.182], loss: 0.005098, mae: 0.067975, mean_q: -0.336276
  6300/100000: episode: 63, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -14.307, mean reward: -0.143 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.221, 10.098], loss: 0.004878, mae: 0.067009, mean_q: -0.311639
  6400/100000: episode: 64, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -16.652, mean reward: -0.167 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.537, 10.098], loss: 0.004672, mae: 0.065745, mean_q: -0.291159
  6500/100000: episode: 65, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.372, mean reward: -0.184 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.961, 10.255], loss: 0.004241, mae: 0.063389, mean_q: -0.304294
  6600/100000: episode: 66, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.441, mean reward: -0.184 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.246, 10.098], loss: 0.004595, mae: 0.064552, mean_q: -0.322321
  6700/100000: episode: 67, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -13.719, mean reward: -0.137 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.835, 10.134], loss: 0.004978, mae: 0.067897, mean_q: -0.331988
  6800/100000: episode: 68, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.479, mean reward: -0.155 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.279, 10.098], loss: 0.005706, mae: 0.069486, mean_q: -0.328764
  6900/100000: episode: 69, duration: 0.480s, episode steps: 100, steps per second: 209, episode reward: -15.599, mean reward: -0.156 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.104, 10.325], loss: 0.004975, mae: 0.068985, mean_q: -0.303040
  7000/100000: episode: 70, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.561, mean reward: -0.186 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.894, 10.253], loss: 0.004700, mae: 0.064836, mean_q: -0.323093
  7100/100000: episode: 71, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.890, mean reward: -0.149 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.991, 10.098], loss: 0.005557, mae: 0.070950, mean_q: -0.357914
  7200/100000: episode: 72, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.667, mean reward: -0.177 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.663, 10.104], loss: 0.005046, mae: 0.069013, mean_q: -0.308419
  7300/100000: episode: 73, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -8.895, mean reward: -0.089 [-1.000, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.740, 10.264], loss: 0.003785, mae: 0.061655, mean_q: -0.347392
  7400/100000: episode: 74, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.064, mean reward: -0.171 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.088, 10.187], loss: 0.003988, mae: 0.064177, mean_q: -0.341191
  7500/100000: episode: 75, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -20.227, mean reward: -0.202 [-1.000, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.764, 10.162], loss: 0.005022, mae: 0.066244, mean_q: -0.318159
  7600/100000: episode: 76, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.617, mean reward: -0.176 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.809, 10.232], loss: 0.004640, mae: 0.067731, mean_q: -0.335625
  7700/100000: episode: 77, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -18.256, mean reward: -0.183 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.466, 10.098], loss: 0.005514, mae: 0.069704, mean_q: -0.300332
  7800/100000: episode: 78, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.311, mean reward: -0.193 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.328, 10.322], loss: 0.004747, mae: 0.066153, mean_q: -0.339995
  7900/100000: episode: 79, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -20.683, mean reward: -0.207 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.074, 10.098], loss: 0.004086, mae: 0.064215, mean_q: -0.320148
  8000/100000: episode: 80, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.515, mean reward: -0.175 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.838, 10.293], loss: 0.004823, mae: 0.067906, mean_q: -0.292806
  8100/100000: episode: 81, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -10.476, mean reward: -0.105 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.372, 10.098], loss: 0.004169, mae: 0.063893, mean_q: -0.349051
  8200/100000: episode: 82, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.561, mean reward: -0.166 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.842, 10.098], loss: 0.004582, mae: 0.066971, mean_q: -0.280984
  8300/100000: episode: 83, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -17.860, mean reward: -0.179 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.306, 10.098], loss: 0.004762, mae: 0.066989, mean_q: -0.305380
  8400/100000: episode: 84, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -11.247, mean reward: -0.112 [-1.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-2.685, 10.098], loss: 0.004418, mae: 0.065232, mean_q: -0.307985
  8500/100000: episode: 85, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.179, mean reward: -0.182 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.643, 10.098], loss: 0.004593, mae: 0.066732, mean_q: -0.328241
  8600/100000: episode: 86, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.919, mean reward: -0.189 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.692, 10.098], loss: 0.004400, mae: 0.065883, mean_q: -0.328988
  8700/100000: episode: 87, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -8.919, mean reward: -0.089 [-1.000, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.881, 10.485], loss: 0.004096, mae: 0.064215, mean_q: -0.294775
  8800/100000: episode: 88, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.070, mean reward: -0.151 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.422, 10.098], loss: 0.003804, mae: 0.062218, mean_q: -0.319343
  8900/100000: episode: 89, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.128, mean reward: -0.161 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.204, 10.118], loss: 0.004807, mae: 0.066656, mean_q: -0.291508
  9000/100000: episode: 90, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.558, mean reward: -0.186 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.693, 10.142], loss: 0.005007, mae: 0.068587, mean_q: -0.345829
  9100/100000: episode: 91, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -19.919, mean reward: -0.199 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.802, 10.099], loss: 0.005116, mae: 0.069772, mean_q: -0.277924
  9200/100000: episode: 92, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -18.213, mean reward: -0.182 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.592, 10.098], loss: 0.004357, mae: 0.065641, mean_q: -0.306916
  9300/100000: episode: 93, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: -18.285, mean reward: -0.183 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.967, 10.098], loss: 0.003579, mae: 0.061654, mean_q: -0.329106
  9400/100000: episode: 94, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.300, mean reward: -0.163 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.599, 10.242], loss: 0.004536, mae: 0.065056, mean_q: -0.317780
  9500/100000: episode: 95, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -15.780, mean reward: -0.158 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.471, 10.247], loss: 0.004832, mae: 0.068592, mean_q: -0.325770
  9600/100000: episode: 96, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -12.761, mean reward: -0.128 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.378, 10.290], loss: 0.004242, mae: 0.062814, mean_q: -0.326134
  9700/100000: episode: 97, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -16.583, mean reward: -0.166 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.837, 10.246], loss: 0.005540, mae: 0.072207, mean_q: -0.297371
  9800/100000: episode: 98, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.159, mean reward: -0.172 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.744, 10.120], loss: 0.005341, mae: 0.073222, mean_q: -0.340048
  9900/100000: episode: 99, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.046, mean reward: -0.170 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.867, 10.255], loss: 0.004243, mae: 0.064261, mean_q: -0.301617
 10000/100000: episode: 100, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: -12.581, mean reward: -0.126 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.014, 10.098], loss: 0.004471, mae: 0.068736, mean_q: -0.299862
 10100/100000: episode: 101, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -14.429, mean reward: -0.144 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.797, 10.184], loss: 0.003577, mae: 0.060132, mean_q: -0.282995
 10200/100000: episode: 102, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.318, mean reward: -0.143 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.604, 10.127], loss: 0.004388, mae: 0.065482, mean_q: -0.334833
 10300/100000: episode: 103, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.568, mean reward: -0.196 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.254, 10.184], loss: 0.003777, mae: 0.062820, mean_q: -0.295851
 10400/100000: episode: 104, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.786, mean reward: -0.188 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.228, 10.249], loss: 0.004035, mae: 0.063545, mean_q: -0.302096
 10500/100000: episode: 105, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -13.038, mean reward: -0.130 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.678, 10.098], loss: 0.004378, mae: 0.064494, mean_q: -0.318085
 10600/100000: episode: 106, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.453, mean reward: -0.165 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.642, 10.098], loss: 0.004447, mae: 0.066591, mean_q: -0.281959
 10700/100000: episode: 107, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: -13.366, mean reward: -0.134 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.749, 10.098], loss: 0.003866, mae: 0.061755, mean_q: -0.316657
 10800/100000: episode: 108, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.511, mean reward: -0.195 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.348, 10.208], loss: 0.003395, mae: 0.059599, mean_q: -0.311175
 10900/100000: episode: 109, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -11.135, mean reward: -0.111 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.865, 10.482], loss: 0.004376, mae: 0.067068, mean_q: -0.282133
 11000/100000: episode: 110, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -11.367, mean reward: -0.114 [-1.000, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.273, 10.098], loss: 0.004182, mae: 0.066191, mean_q: -0.327099
 11100/100000: episode: 111, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.985, mean reward: -0.160 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.374, 10.098], loss: 0.003589, mae: 0.060484, mean_q: -0.297706
 11200/100000: episode: 112, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.848, mean reward: -0.188 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.380, 10.098], loss: 0.003818, mae: 0.062106, mean_q: -0.285746
 11300/100000: episode: 113, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.997, mean reward: -0.190 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.691, 10.157], loss: 0.003636, mae: 0.060880, mean_q: -0.302676
 11400/100000: episode: 114, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -20.313, mean reward: -0.203 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.162, 10.157], loss: 0.003787, mae: 0.063626, mean_q: -0.294970
 11500/100000: episode: 115, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.952, mean reward: -0.190 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.710, 10.297], loss: 0.003389, mae: 0.059444, mean_q: -0.342728
 11600/100000: episode: 116, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -14.683, mean reward: -0.147 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.347, 10.280], loss: 0.003885, mae: 0.063301, mean_q: -0.289689
 11700/100000: episode: 117, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -13.299, mean reward: -0.133 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.182, 10.542], loss: 0.004420, mae: 0.065891, mean_q: -0.301846
 11800/100000: episode: 118, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.033, mean reward: -0.180 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.134, 10.098], loss: 0.003768, mae: 0.061753, mean_q: -0.336212
 11900/100000: episode: 119, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.731, mean reward: -0.167 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.241, 10.163], loss: 0.004116, mae: 0.063567, mean_q: -0.318408
 12000/100000: episode: 120, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -12.380, mean reward: -0.124 [-1.000, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.328, 10.098], loss: 0.004364, mae: 0.066664, mean_q: -0.305208
 12100/100000: episode: 121, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -14.990, mean reward: -0.150 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.884, 10.098], loss: 0.003803, mae: 0.063050, mean_q: -0.291694
 12200/100000: episode: 122, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -8.724, mean reward: -0.087 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.321, 10.274], loss: 0.004692, mae: 0.069646, mean_q: -0.277350
 12300/100000: episode: 123, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.523, mean reward: -0.175 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.119, 10.098], loss: 0.004162, mae: 0.065640, mean_q: -0.297622
 12400/100000: episode: 124, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -19.699, mean reward: -0.197 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.032, 10.242], loss: 0.005145, mae: 0.071109, mean_q: -0.334394
 12500/100000: episode: 125, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -17.943, mean reward: -0.179 [-1.000, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.303, 10.115], loss: 0.004645, mae: 0.069817, mean_q: -0.307625
 12600/100000: episode: 126, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.220, mean reward: -0.192 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.875, 10.175], loss: 0.004405, mae: 0.066973, mean_q: -0.326539
 12700/100000: episode: 127, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.557, mean reward: -0.176 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.969, 10.147], loss: 0.004375, mae: 0.066905, mean_q: -0.295568
 12800/100000: episode: 128, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.741, mean reward: -0.177 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.610, 10.295], loss: 0.004073, mae: 0.065000, mean_q: -0.331190
 12900/100000: episode: 129, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.607, mean reward: -0.166 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.199, 10.158], loss: 0.004554, mae: 0.067864, mean_q: -0.299818
 13000/100000: episode: 130, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -18.739, mean reward: -0.187 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.469, 10.118], loss: 0.003787, mae: 0.062199, mean_q: -0.301718
 13100/100000: episode: 131, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.734, mean reward: -0.167 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.557, 10.184], loss: 0.004440, mae: 0.067006, mean_q: -0.289007
 13200/100000: episode: 132, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.656, mean reward: -0.187 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.973, 10.098], loss: 0.004489, mae: 0.067809, mean_q: -0.318496
 13300/100000: episode: 133, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.448, mean reward: -0.164 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.959, 10.098], loss: 0.003688, mae: 0.060712, mean_q: -0.315553
 13400/100000: episode: 134, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -13.517, mean reward: -0.135 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.613, 10.331], loss: 0.004436, mae: 0.067149, mean_q: -0.302174
 13500/100000: episode: 135, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -14.114, mean reward: -0.141 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.065, 10.213], loss: 0.003845, mae: 0.063736, mean_q: -0.301737
 13600/100000: episode: 136, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.409, mean reward: -0.164 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.174, 10.098], loss: 0.003924, mae: 0.063900, mean_q: -0.291671
 13700/100000: episode: 137, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -20.525, mean reward: -0.205 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.729, 10.098], loss: 0.003528, mae: 0.060238, mean_q: -0.341073
 13800/100000: episode: 138, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.575, mean reward: -0.186 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.433, 10.098], loss: 0.003530, mae: 0.060983, mean_q: -0.301105
 13900/100000: episode: 139, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.382, mean reward: -0.174 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.154, 10.101], loss: 0.003866, mae: 0.062536, mean_q: -0.296428
 14000/100000: episode: 140, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -20.341, mean reward: -0.203 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.414, 10.212], loss: 0.004129, mae: 0.064634, mean_q: -0.280297
 14100/100000: episode: 141, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.605, mean reward: -0.186 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.639, 10.109], loss: 0.004652, mae: 0.067051, mean_q: -0.296226
 14200/100000: episode: 142, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.657, mean reward: -0.167 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.547, 10.098], loss: 0.003525, mae: 0.061097, mean_q: -0.315457
 14300/100000: episode: 143, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.578, mean reward: -0.186 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.095, 10.299], loss: 0.004499, mae: 0.067311, mean_q: -0.324025
 14400/100000: episode: 144, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -14.527, mean reward: -0.145 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.878, 10.257], loss: 0.004834, mae: 0.067257, mean_q: -0.330174
 14500/100000: episode: 145, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -16.552, mean reward: -0.166 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.441, 10.274], loss: 0.004228, mae: 0.065569, mean_q: -0.300059
 14600/100000: episode: 146, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.893, mean reward: -0.189 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.548, 10.165], loss: 0.003730, mae: 0.062610, mean_q: -0.336651
 14700/100000: episode: 147, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.957, mean reward: -0.170 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.310, 10.098], loss: 0.003586, mae: 0.060796, mean_q: -0.307895
 14800/100000: episode: 148, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -11.674, mean reward: -0.117 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.485, 10.411], loss: 0.003653, mae: 0.061528, mean_q: -0.321592
 14900/100000: episode: 149, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -13.375, mean reward: -0.134 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.422, 10.280], loss: 0.003792, mae: 0.063785, mean_q: -0.279208
[Info] 100-TH LEVEL FOUND: 0.3807021379470825, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.336s, episode steps: 100, steps per second: 23, episode reward: -15.052, mean reward: -0.151 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.975, 10.098], loss: 0.005249, mae: 0.071124, mean_q: -0.300420
 15008/100000: episode: 151, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 2.181, mean reward: 0.273 [0.162, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.987, 10.100], loss: 0.005087, mae: 0.073504, mean_q: -0.309625
 15009/100000: episode: 152, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.275, mean reward: 0.275 [0.275, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.084, 10.100], loss: 0.005676, mae: 0.085422, mean_q: -0.021783
 15013/100000: episode: 153, duration: 0.026s, episode steps: 4, steps per second: 157, episode reward: 1.324, mean reward: 0.331 [0.281, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.342, 10.100], loss: 0.003691, mae: 0.067249, mean_q: -0.194693
 15018/100000: episode: 154, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 1.636, mean reward: 0.327 [0.297, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.375, 10.100], loss: 0.003603, mae: 0.062814, mean_q: -0.209416
 15027/100000: episode: 155, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 2.582, mean reward: 0.287 [0.223, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.353, 10.100], loss: 0.003930, mae: 0.066570, mean_q: -0.254515
 15032/100000: episode: 156, duration: 0.029s, episode steps: 5, steps per second: 170, episode reward: 1.866, mean reward: 0.373 [0.304, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.372, 10.100], loss: 0.004489, mae: 0.065767, mean_q: -0.281257
 15037/100000: episode: 157, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 1.630, mean reward: 0.326 [0.306, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.544, 10.100], loss: 0.004178, mae: 0.065868, mean_q: -0.304553
 15042/100000: episode: 158, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 1.820, mean reward: 0.364 [0.341, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.343, 10.100], loss: 0.004438, mae: 0.066900, mean_q: -0.291542
 15043/100000: episode: 159, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.328, mean reward: 0.328 [0.328, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.117, 10.100], loss: 0.010131, mae: 0.077924, mean_q: -0.381178
 15051/100000: episode: 160, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 2.746, mean reward: 0.343 [0.168, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.152, 10.100], loss: 0.003586, mae: 0.063606, mean_q: -0.279246
 15059/100000: episode: 161, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 2.396, mean reward: 0.300 [0.254, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.442, 10.100], loss: 0.004811, mae: 0.072500, mean_q: -0.353682
 15110/100000: episode: 162, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 15.655, mean reward: 0.307 [0.137, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.789, 10.434], loss: 0.006252, mae: 0.077000, mean_q: -0.279951
 15118/100000: episode: 163, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 2.976, mean reward: 0.372 [0.223, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.212, 10.100], loss: 0.003966, mae: 0.068798, mean_q: -0.264603
 15123/100000: episode: 164, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 1.534, mean reward: 0.307 [0.283, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.421, 10.100], loss: 0.004960, mae: 0.075915, mean_q: -0.311503
 15174/100000: episode: 165, duration: 0.284s, episode steps: 51, steps per second: 179, episode reward: 14.385, mean reward: 0.282 [0.156, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.354, 10.332], loss: 0.005168, mae: 0.073335, mean_q: -0.273654
 15182/100000: episode: 166, duration: 0.050s, episode steps: 8, steps per second: 162, episode reward: 2.038, mean reward: 0.255 [0.161, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.338, 10.100], loss: 0.004589, mae: 0.071998, mean_q: -0.246926
 15192/100000: episode: 167, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 3.608, mean reward: 0.361 [0.268, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.541, 10.100], loss: 0.005241, mae: 0.075184, mean_q: -0.223355
 15200/100000: episode: 168, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 3.537, mean reward: 0.442 [0.354, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.708, 10.100], loss: 0.003342, mae: 0.059315, mean_q: -0.271339
 15208/100000: episode: 169, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 3.022, mean reward: 0.378 [0.329, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.302, 10.100], loss: 0.004358, mae: 0.070193, mean_q: -0.191270
 15209/100000: episode: 170, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.268, mean reward: 0.268 [0.268, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.107, 10.100], loss: 0.003714, mae: 0.062587, mean_q: -0.096157
 15217/100000: episode: 171, duration: 0.039s, episode steps: 8, steps per second: 203, episode reward: 1.858, mean reward: 0.232 [0.138, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.057, 10.100], loss: 0.003983, mae: 0.065342, mean_q: -0.304886
 15222/100000: episode: 172, duration: 0.027s, episode steps: 5, steps per second: 189, episode reward: 1.565, mean reward: 0.313 [0.290, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.403, 10.100], loss: 0.004339, mae: 0.067959, mean_q: -0.308110
 15229/100000: episode: 173, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 2.246, mean reward: 0.321 [0.261, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-1.314, 10.100], loss: 0.004823, mae: 0.068104, mean_q: -0.312736
 15239/100000: episode: 174, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 3.068, mean reward: 0.307 [0.199, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.369, 10.100], loss: 0.004673, mae: 0.067708, mean_q: -0.211858
 15244/100000: episode: 175, duration: 0.031s, episode steps: 5, steps per second: 164, episode reward: 1.411, mean reward: 0.282 [0.245, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.411, 10.100], loss: 0.004736, mae: 0.068693, mean_q: -0.271395
 15251/100000: episode: 176, duration: 0.037s, episode steps: 7, steps per second: 192, episode reward: 1.707, mean reward: 0.244 [0.202, 0.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.185, 10.100], loss: 0.004232, mae: 0.070107, mean_q: -0.240040
 15258/100000: episode: 177, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 2.115, mean reward: 0.302 [0.196, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.138, 10.100], loss: 0.004942, mae: 0.068920, mean_q: -0.380889
 15259/100000: episode: 178, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.293, mean reward: 0.293 [0.293, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.104, 10.100], loss: 0.003128, mae: 0.057349, mean_q: -0.301619
 15267/100000: episode: 179, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 2.552, mean reward: 0.319 [0.232, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.554, 10.100], loss: 0.005195, mae: 0.075485, mean_q: -0.261002
 15277/100000: episode: 180, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 3.684, mean reward: 0.368 [0.278, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.284, 10.100], loss: 0.004710, mae: 0.066513, mean_q: -0.304841
 15286/100000: episode: 181, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 2.857, mean reward: 0.317 [0.238, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.353, 10.100], loss: 0.004827, mae: 0.068072, mean_q: -0.272022
 15290/100000: episode: 182, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 1.666, mean reward: 0.417 [0.381, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.235, 10.100], loss: 0.005008, mae: 0.075463, mean_q: -0.332341
 15298/100000: episode: 183, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 2.385, mean reward: 0.298 [0.226, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.085, 10.100], loss: 0.004003, mae: 0.067899, mean_q: -0.262362
 15307/100000: episode: 184, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 2.929, mean reward: 0.325 [0.262, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.945, 10.100], loss: 0.003898, mae: 0.066541, mean_q: -0.163383
 15315/100000: episode: 185, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 2.892, mean reward: 0.361 [0.280, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.243, 10.100], loss: 0.003678, mae: 0.061387, mean_q: -0.275582
 15324/100000: episode: 186, duration: 0.052s, episode steps: 9, steps per second: 175, episode reward: 2.395, mean reward: 0.266 [0.202, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.318, 10.100], loss: 0.003759, mae: 0.064438, mean_q: -0.229147
 15325/100000: episode: 187, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.359, mean reward: 0.359 [0.359, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.077, 10.100], loss: 0.002723, mae: 0.055856, mean_q: -0.231387
 15333/100000: episode: 188, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 2.651, mean reward: 0.331 [0.271, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.194, 10.100], loss: 0.004109, mae: 0.065791, mean_q: -0.336261
 15341/100000: episode: 189, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 2.789, mean reward: 0.349 [0.300, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.146, 10.100], loss: 0.004511, mae: 0.068218, mean_q: -0.207814
 15349/100000: episode: 190, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 2.635, mean reward: 0.329 [0.269, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.295, 10.100], loss: 0.003847, mae: 0.065683, mean_q: -0.171803
 15354/100000: episode: 191, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 1.832, mean reward: 0.366 [0.297, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.391, 10.100], loss: 0.003834, mae: 0.064591, mean_q: -0.253493
 15363/100000: episode: 192, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 2.688, mean reward: 0.299 [0.238, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-1.005, 10.100], loss: 0.005586, mae: 0.076661, mean_q: -0.183853
 15372/100000: episode: 193, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 3.094, mean reward: 0.344 [0.280, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.172, 10.100], loss: 0.004650, mae: 0.071338, mean_q: -0.156039
 15381/100000: episode: 194, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 3.349, mean reward: 0.372 [0.334, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.402, 10.100], loss: 0.004675, mae: 0.074778, mean_q: -0.183061
 15390/100000: episode: 195, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 2.728, mean reward: 0.303 [0.180, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.177, 10.100], loss: 0.004415, mae: 0.066900, mean_q: -0.255955
 15391/100000: episode: 196, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.344, mean reward: 0.344 [0.344, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.118, 10.100], loss: 0.006230, mae: 0.086760, mean_q: -0.147814
 15395/100000: episode: 197, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 1.165, mean reward: 0.291 [0.197, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.317, 10.100], loss: 0.004314, mae: 0.066426, mean_q: -0.189348
 15396/100000: episode: 198, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.318, mean reward: 0.318 [0.318, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.110, 10.100], loss: 0.006542, mae: 0.083641, mean_q: -0.285682
 15404/100000: episode: 199, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 2.862, mean reward: 0.358 [0.247, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.320, 10.100], loss: 0.004565, mae: 0.066363, mean_q: -0.341162
 15413/100000: episode: 200, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 2.832, mean reward: 0.315 [0.179, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.100], loss: 0.004513, mae: 0.068799, mean_q: -0.240625
 15421/100000: episode: 201, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 2.823, mean reward: 0.353 [0.246, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.298, 10.100], loss: 0.004309, mae: 0.068980, mean_q: -0.222229
 15429/100000: episode: 202, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 3.011, mean reward: 0.376 [0.264, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-1.492, 10.100], loss: 0.004200, mae: 0.066362, mean_q: -0.322558
 15437/100000: episode: 203, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 2.897, mean reward: 0.362 [0.306, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.621, 10.100], loss: 0.004639, mae: 0.068499, mean_q: -0.275178
 15441/100000: episode: 204, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 1.185, mean reward: 0.296 [0.241, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.231, 10.100], loss: 0.005548, mae: 0.081271, mean_q: -0.000032
 15448/100000: episode: 205, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 1.708, mean reward: 0.244 [0.142, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.203, 10.100], loss: 0.006136, mae: 0.083319, mean_q: -0.245952
 15499/100000: episode: 206, duration: 0.248s, episode steps: 51, steps per second: 206, episode reward: 14.941, mean reward: 0.293 [0.121, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.661, 10.316], loss: 0.004966, mae: 0.073605, mean_q: -0.215651
 15506/100000: episode: 207, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 2.514, mean reward: 0.359 [0.319, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.307, 10.100], loss: 0.019552, mae: 0.106933, mean_q: -0.257481
 15516/100000: episode: 208, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 5.034, mean reward: 0.503 [0.438, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.389, 10.100], loss: 0.009506, mae: 0.094965, mean_q: -0.196128
 15567/100000: episode: 209, duration: 0.261s, episode steps: 51, steps per second: 195, episode reward: 12.423, mean reward: 0.244 [0.099, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-1.038, 10.173], loss: 0.006223, mae: 0.078560, mean_q: -0.229855
 15575/100000: episode: 210, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 3.344, mean reward: 0.418 [0.364, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.310, 10.100], loss: 0.004588, mae: 0.071304, mean_q: -0.207033
 15580/100000: episode: 211, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 1.497, mean reward: 0.299 [0.280, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.435, 10.100], loss: 0.004864, mae: 0.071747, mean_q: -0.143270
 15588/100000: episode: 212, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 3.224, mean reward: 0.403 [0.371, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.338, 10.100], loss: 0.004821, mae: 0.070465, mean_q: -0.214868
 15592/100000: episode: 213, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 1.616, mean reward: 0.404 [0.390, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.381, 10.100], loss: 0.002776, mae: 0.057416, mean_q: -0.316777
 15600/100000: episode: 214, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 2.222, mean reward: 0.278 [0.198, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.193, 10.100], loss: 0.003699, mae: 0.061494, mean_q: -0.239911
 15608/100000: episode: 215, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 2.770, mean reward: 0.346 [0.244, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.308, 10.100], loss: 0.005070, mae: 0.076025, mean_q: -0.173576
 15615/100000: episode: 216, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 2.483, mean reward: 0.355 [0.331, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.187, 10.100], loss: 0.004242, mae: 0.067016, mean_q: -0.221040
 15619/100000: episode: 217, duration: 0.028s, episode steps: 4, steps per second: 142, episode reward: 1.286, mean reward: 0.321 [0.292, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.325, 10.100], loss: 0.004197, mae: 0.069221, mean_q: -0.105302
 15623/100000: episode: 218, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 1.223, mean reward: 0.306 [0.279, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.299, 10.100], loss: 0.004854, mae: 0.072636, mean_q: -0.143890
 15630/100000: episode: 219, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 2.565, mean reward: 0.366 [0.333, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.271, 10.100], loss: 0.004282, mae: 0.067744, mean_q: -0.283553
 15638/100000: episode: 220, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 1.887, mean reward: 0.236 [0.202, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.722, 10.100], loss: 0.005256, mae: 0.075849, mean_q: -0.086244
 15645/100000: episode: 221, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 2.119, mean reward: 0.303 [0.261, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.284, 10.100], loss: 0.004552, mae: 0.068021, mean_q: -0.161465
 15655/100000: episode: 222, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 3.890, mean reward: 0.389 [0.315, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.299, 10.100], loss: 0.005690, mae: 0.073221, mean_q: -0.182412
 15663/100000: episode: 223, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 3.153, mean reward: 0.394 [0.354, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.329, 10.100], loss: 0.004930, mae: 0.072194, mean_q: -0.169661
 15673/100000: episode: 224, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 2.847, mean reward: 0.285 [0.245, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.401, 10.100], loss: 0.004189, mae: 0.066632, mean_q: -0.280802
 15724/100000: episode: 225, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 12.247, mean reward: 0.240 [0.095, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.311, 10.100], loss: 0.004835, mae: 0.072685, mean_q: -0.178468
 15732/100000: episode: 226, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 2.790, mean reward: 0.349 [0.312, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.317, 10.100], loss: 0.005627, mae: 0.078960, mean_q: -0.045702
 15740/100000: episode: 227, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 2.173, mean reward: 0.272 [0.218, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.243, 10.100], loss: 0.004154, mae: 0.066974, mean_q: -0.186242
 15747/100000: episode: 228, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 2.081, mean reward: 0.297 [0.250, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.296, 10.100], loss: 0.004705, mae: 0.070267, mean_q: -0.057416
 15751/100000: episode: 229, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 1.256, mean reward: 0.314 [0.304, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.421, 10.100], loss: 0.004397, mae: 0.072527, mean_q: 0.021341
 15760/100000: episode: 230, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 2.777, mean reward: 0.309 [0.284, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.249, 10.100], loss: 0.004975, mae: 0.073131, mean_q: -0.122750
 15811/100000: episode: 231, duration: 0.272s, episode steps: 51, steps per second: 187, episode reward: 9.445, mean reward: 0.185 [0.032, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.319, 10.100], loss: 0.004609, mae: 0.068489, mean_q: -0.176327
 15821/100000: episode: 232, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 4.048, mean reward: 0.405 [0.337, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.506, 10.100], loss: 0.005365, mae: 0.076050, mean_q: -0.175113
 15831/100000: episode: 233, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 3.808, mean reward: 0.381 [0.287, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.339, 10.100], loss: 0.004615, mae: 0.070516, mean_q: -0.162176
 15840/100000: episode: 234, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 2.949, mean reward: 0.328 [0.263, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.255, 10.100], loss: 0.004168, mae: 0.070097, mean_q: -0.076165
 15847/100000: episode: 235, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 2.182, mean reward: 0.312 [0.246, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.360, 10.100], loss: 0.003648, mae: 0.060960, mean_q: -0.168509
 15855/100000: episode: 236, duration: 0.044s, episode steps: 8, steps per second: 184, episode reward: 1.517, mean reward: 0.190 [0.102, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.205, 10.100], loss: 0.003777, mae: 0.062437, mean_q: -0.201662
 15863/100000: episode: 237, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 2.422, mean reward: 0.303 [0.235, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.214, 10.100], loss: 0.004919, mae: 0.069750, mean_q: -0.252693
 15871/100000: episode: 238, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 2.814, mean reward: 0.352 [0.298, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.206, 10.100], loss: 0.004504, mae: 0.071787, mean_q: -0.202354
 15879/100000: episode: 239, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 2.926, mean reward: 0.366 [0.275, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.229, 10.100], loss: 0.004918, mae: 0.071243, mean_q: -0.158476
[Info] 200-TH LEVEL FOUND: 0.5557160973548889, Considering 10/90 traces
 15887/100000: episode: 240, duration: 3.888s, episode steps: 8, steps per second: 2, episode reward: 2.865, mean reward: 0.358 [0.234, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-1.435, 10.100], loss: 0.004731, mae: 0.069564, mean_q: -0.187283
 15888/100000: episode: 241, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.298, mean reward: 0.298 [0.298, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.371, 10.200], loss: 0.004939, mae: 0.066369, mean_q: -0.335456
 15889/100000: episode: 242, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.506, mean reward: 0.506 [0.506, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.488, 10.100], loss: 0.003148, mae: 0.057527, mean_q: 0.021886
 15890/100000: episode: 243, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.368, mean reward: 0.368 [0.368, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.346, 10.200], loss: 0.005453, mae: 0.075362, mean_q: -0.232217
 15892/100000: episode: 244, duration: 0.022s, episode steps: 2, steps per second: 92, episode reward: 0.947, mean reward: 0.474 [0.421, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.575, 10.100], loss: 0.006180, mae: 0.082487, mean_q: -0.125727
 15893/100000: episode: 245, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.314, mean reward: 0.314 [0.314, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.653, 10.200], loss: 0.004577, mae: 0.073364, mean_q: 0.027228
 15895/100000: episode: 246, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.935, mean reward: 0.467 [0.466, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.509, 10.100], loss: 0.008311, mae: 0.087735, mean_q: -0.130634
 15896/100000: episode: 247, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.360, mean reward: 0.360 [0.360, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.394, 10.200], loss: 0.003194, mae: 0.061097, mean_q: -0.091382
 15897/100000: episode: 248, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.396, mean reward: 0.396 [0.396, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.395, 10.200], loss: 0.006939, mae: 0.084757, mean_q: -0.055525
 15898/100000: episode: 249, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.413, mean reward: 0.413 [0.413, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.407, 10.100], loss: 0.005021, mae: 0.076271, mean_q: -0.239627
 15900/100000: episode: 250, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 0.754, mean reward: 0.377 [0.358, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.436, 10.100], loss: 0.004522, mae: 0.072784, mean_q: 0.009176
 15901/100000: episode: 251, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.405, mean reward: 0.405 [0.405, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.379, 10.200], loss: 0.004313, mae: 0.068117, mean_q: -0.266104
 15902/100000: episode: 252, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.447, mean reward: 0.447 [0.447, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.366, 10.200], loss: 0.008370, mae: 0.091130, mean_q: 0.131113
 15904/100000: episode: 253, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 0.664, mean reward: 0.332 [0.310, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.514, 10.100], loss: 0.007618, mae: 0.086319, mean_q: -0.279662
 15905/100000: episode: 254, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.381, mean reward: 0.381 [0.381, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.469, 10.100], loss: 0.004726, mae: 0.072958, mean_q: -0.057897
 15906/100000: episode: 255, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.344, mean reward: 0.344 [0.344, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.349, 10.200], loss: 0.004587, mae: 0.073241, mean_q: 0.241316
 15907/100000: episode: 256, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.372, mean reward: 0.372 [0.372, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.345, 10.100], loss: 0.003491, mae: 0.063538, mean_q: 0.097588
 15908/100000: episode: 257, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.458, mean reward: 0.458 [0.458, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.436, 10.200], loss: 0.006132, mae: 0.091359, mean_q: -0.083701
 15909/100000: episode: 258, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.359, mean reward: 0.359 [0.359, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.310, 10.200], loss: 0.004359, mae: 0.072277, mean_q: -0.273428
 15910/100000: episode: 259, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.553, mean reward: 0.553 [0.553, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.357, 10.200], loss: 0.005908, mae: 0.086555, mean_q: -0.155679
 15911/100000: episode: 260, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.469, mean reward: 0.469 [0.469, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.376, 10.100], loss: 0.005693, mae: 0.071842, mean_q: -0.140893
 15912/100000: episode: 261, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.431, mean reward: 0.431 [0.431, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.546, 10.100], loss: 0.006024, mae: 0.087311, mean_q: -0.072621
 15913/100000: episode: 262, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.393, mean reward: 0.393 [0.393, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.541, 10.100], loss: 0.006812, mae: 0.089229, mean_q: -0.145380
 15915/100000: episode: 263, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.773, mean reward: 0.387 [0.384, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.479, 10.100], loss: 0.006468, mae: 0.088020, mean_q: -0.164109
 15916/100000: episode: 264, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.512, mean reward: 0.512 [0.512, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.404, 10.200], loss: 0.003026, mae: 0.059973, mean_q: -0.370861
 15918/100000: episode: 265, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 0.837, mean reward: 0.419 [0.406, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.541, 10.100], loss: 0.005245, mae: 0.079717, mean_q: -0.003993
 15919/100000: episode: 266, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.406, mean reward: 0.406 [0.406, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.323, 10.100], loss: 0.002466, mae: 0.057622, mean_q: -0.255316
 15920/100000: episode: 267, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.311, mean reward: 0.311 [0.311, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.362, 10.200], loss: 0.005312, mae: 0.068248, mean_q: -0.399215
 15921/100000: episode: 268, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.437, mean reward: 0.437 [0.437, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.388, 10.200], loss: 0.003531, mae: 0.057555, mean_q: -0.472740
 15922/100000: episode: 269, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.472, mean reward: 0.472 [0.472, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.398, 10.200], loss: 0.003223, mae: 0.057088, mean_q: -0.124347
 15923/100000: episode: 270, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.388, mean reward: 0.388 [0.388, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.386, 10.200], loss: 0.002162, mae: 0.047001, mean_q: -0.243571
 15924/100000: episode: 271, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 0.387, mean reward: 0.387 [0.387, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.339, 10.200], loss: 0.003522, mae: 0.067871, mean_q: -0.079924
 15925/100000: episode: 272, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.415, mean reward: 0.415 [0.415, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.416, 10.200], loss: 0.001926, mae: 0.040955, mean_q: -0.232063
 15926/100000: episode: 273, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.441, mean reward: 0.441 [0.441, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.392, 10.200], loss: 0.003725, mae: 0.067839, mean_q: 0.118267
 15927/100000: episode: 274, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.378, mean reward: 0.378 [0.378, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.411, 10.200], loss: 0.008042, mae: 0.090815, mean_q: -0.111497
 15928/100000: episode: 275, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.526, mean reward: 0.526 [0.526, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.363, 10.100], loss: 0.003039, mae: 0.052068, mean_q: 0.039153
 15929/100000: episode: 276, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.377, mean reward: 0.377 [0.377, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.337, 10.100], loss: 0.004865, mae: 0.069711, mean_q: -0.073354
 15930/100000: episode: 277, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.356, mean reward: 0.356 [0.356, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.386, 10.200], loss: 0.006366, mae: 0.076397, mean_q: -0.627660
 15931/100000: episode: 278, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.423, mean reward: 0.423 [0.423, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.293, 10.100], loss: 0.004570, mae: 0.077185, mean_q: -0.160595
 15932/100000: episode: 279, duration: 0.009s, episode steps: 1, steps per second: 117, episode reward: 0.388, mean reward: 0.388 [0.388, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.489, 10.100], loss: 0.005214, mae: 0.081873, mean_q: 0.093819
 15933/100000: episode: 280, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 0.358, mean reward: 0.358 [0.358, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.337, 10.100], loss: 0.005470, mae: 0.082167, mean_q: -0.040388
 15934/100000: episode: 281, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.386, mean reward: 0.386 [0.386, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.378, 10.200], loss: 0.003061, mae: 0.053257, mean_q: -0.112827
 15935/100000: episode: 282, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.506, mean reward: 0.506 [0.506, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.389, 10.100], loss: 0.004150, mae: 0.077293, mean_q: 0.039838
 15936/100000: episode: 283, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.418, mean reward: 0.418 [0.418, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.366, 10.200], loss: 0.004630, mae: 0.065968, mean_q: 0.031352
 15937/100000: episode: 284, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.321, mean reward: 0.321 [0.321, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.313, 10.200], loss: 0.005111, mae: 0.075074, mean_q: 0.016116
 15938/100000: episode: 285, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 0.325, mean reward: 0.325 [0.325, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.313, 10.200], loss: 0.005327, mae: 0.085895, mean_q: 0.051891
 15939/100000: episode: 286, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 0.519, mean reward: 0.519 [0.519, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.379, 10.100], loss: 0.004312, mae: 0.071972, mean_q: 0.043491
 15940/100000: episode: 287, duration: 0.008s, episode steps: 1, steps per second: 118, episode reward: 0.471, mean reward: 0.471 [0.471, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.379, 10.200], loss: 0.004093, mae: 0.069515, mean_q: 0.007317
 15941/100000: episode: 288, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.320, mean reward: 0.320 [0.320, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.363, 10.200], loss: 0.007500, mae: 0.108165, mean_q: 0.228774
 15942/100000: episode: 289, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.537, mean reward: 0.537 [0.537, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.407, 10.200], loss: 0.004733, mae: 0.075282, mean_q: -0.035544
 15943/100000: episode: 290, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.474, mean reward: 0.474 [0.474, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.405, 10.200], loss: 0.004705, mae: 0.063944, mean_q: -0.179066
 15944/100000: episode: 291, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.438, mean reward: 0.438 [0.438, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.403, 10.200], loss: 0.004694, mae: 0.076705, mean_q: -0.011752
 15945/100000: episode: 292, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 0.442, mean reward: 0.442 [0.442, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.340, 10.200], loss: 0.003162, mae: 0.058476, mean_q: -0.478066
 15947/100000: episode: 293, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.897, mean reward: 0.449 [0.439, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.500, 10.100], loss: 0.003523, mae: 0.059607, mean_q: -0.179188
 15948/100000: episode: 294, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.465, mean reward: 0.465 [0.465, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.372, 10.200], loss: 0.004303, mae: 0.075113, mean_q: -0.065079
 15949/100000: episode: 295, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.388, mean reward: 0.388 [0.388, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.538, 10.100], loss: 0.004467, mae: 0.074894, mean_q: -0.105301
 15950/100000: episode: 296, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.415, mean reward: 0.415 [0.415, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.393, 10.200], loss: 0.003990, mae: 0.071393, mean_q: 0.008895
 15951/100000: episode: 297, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.605, mean reward: 0.605 [0.605, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-2.085, 10.200], loss: 0.002944, mae: 0.052992, mean_q: -0.513315
 15952/100000: episode: 298, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.457, mean reward: 0.457 [0.457, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.424, 10.200], loss: 0.006490, mae: 0.084092, mean_q: -0.083037
 15953/100000: episode: 299, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.407, mean reward: 0.407 [0.407, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.334, 10.100], loss: 0.005695, mae: 0.084629, mean_q: -0.023933
 15955/100000: episode: 300, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 1.085, mean reward: 0.543 [0.538, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.466, 10.100], loss: 0.003655, mae: 0.064686, mean_q: -0.143610
 15956/100000: episode: 301, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.504, mean reward: 0.504 [0.504, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.403, 10.200], loss: 0.002485, mae: 0.047636, mean_q: -0.386235
 15958/100000: episode: 302, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 0.725, mean reward: 0.363 [0.317, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.458, 10.100], loss: 0.010126, mae: 0.083388, mean_q: -0.269131
 15959/100000: episode: 303, duration: 0.009s, episode steps: 1, steps per second: 117, episode reward: 0.379, mean reward: 0.379 [0.379, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.372, 10.200], loss: 0.013407, mae: 0.099181, mean_q: -0.272910
 15960/100000: episode: 304, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.497, mean reward: 0.497 [0.497, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.385, 10.100], loss: 0.003313, mae: 0.066027, mean_q: 0.004661
 15961/100000: episode: 305, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.472, mean reward: 0.472 [0.472, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.324, 10.200], loss: 0.004647, mae: 0.077196, mean_q: 0.022728
 15962/100000: episode: 306, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.519, mean reward: 0.519 [0.519, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.372, 10.100], loss: 0.007525, mae: 0.094454, mean_q: -0.206240
 15964/100000: episode: 307, duration: 0.019s, episode steps: 2, steps per second: 103, episode reward: 0.726, mean reward: 0.363 [0.352, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.484, 10.100], loss: 0.005480, mae: 0.077280, mean_q: -0.241118
 15965/100000: episode: 308, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.512, mean reward: 0.512 [0.512, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.412, 10.100], loss: 0.007799, mae: 0.086805, mean_q: -0.244366
 15966/100000: episode: 309, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.466, mean reward: 0.466 [0.466, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.352, 10.200], loss: 0.004817, mae: 0.085019, mean_q: -0.108217
 15967/100000: episode: 310, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 0.480, mean reward: 0.480 [0.480, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.453, 10.200], loss: 0.004321, mae: 0.077520, mean_q: -0.160746
 15968/100000: episode: 311, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.403, mean reward: 0.403 [0.403, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.420, 10.200], loss: 0.003426, mae: 0.063509, mean_q: -0.178686
 15969/100000: episode: 312, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.516, mean reward: 0.516 [0.516, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.310, 10.200], loss: 0.004037, mae: 0.068561, mean_q: -0.030771
 15971/100000: episode: 313, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.818, mean reward: 0.409 [0.407, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.446, 10.100], loss: 0.002894, mae: 0.058787, mean_q: -0.251790
 15973/100000: episode: 314, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 0.716, mean reward: 0.358 [0.349, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.624, 10.100], loss: 0.005134, mae: 0.071175, mean_q: -0.137162
 15975/100000: episode: 315, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.805, mean reward: 0.403 [0.395, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.527, 10.100], loss: 0.004647, mae: 0.074869, mean_q: 0.016754
 15976/100000: episode: 316, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.470, mean reward: 0.470 [0.470, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.363, 10.200], loss: 0.004718, mae: 0.074452, mean_q: -0.015826
 15977/100000: episode: 317, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.511, mean reward: 0.511 [0.511, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.368, 10.100], loss: 0.003289, mae: 0.062046, mean_q: -0.194463
 15978/100000: episode: 318, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.395, mean reward: 0.395 [0.395, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.358, 10.200], loss: 0.006365, mae: 0.080801, mean_q: -0.082493
 15979/100000: episode: 319, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.455, mean reward: 0.455 [0.455, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.399, 10.200], loss: 0.007669, mae: 0.081690, mean_q: -0.075423
 15980/100000: episode: 320, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.556, mean reward: 0.556 [0.556, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.393, 10.100], loss: 0.004795, mae: 0.083666, mean_q: -0.197737
 15981/100000: episode: 321, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.424, mean reward: 0.424 [0.424, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.390, 10.200], loss: 0.004057, mae: 0.068859, mean_q: -0.192423
 15982/100000: episode: 322, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.437, mean reward: 0.437 [0.437, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.402, 10.200], loss: 0.005693, mae: 0.075889, mean_q: -0.323777
 15983/100000: episode: 323, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.535, mean reward: 0.535 [0.535, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.390, 10.100], loss: 0.003478, mae: 0.067180, mean_q: -0.194839
 15985/100000: episode: 324, duration: 0.013s, episode steps: 2, steps per second: 153, episode reward: 0.934, mean reward: 0.467 [0.467, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.497, 10.100], loss: 0.005692, mae: 0.078444, mean_q: -0.060860
 15986/100000: episode: 325, duration: 0.009s, episode steps: 1, steps per second: 117, episode reward: 0.421, mean reward: 0.421 [0.421, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.496, 10.100], loss: 0.006135, mae: 0.084491, mean_q: -0.117488
 15987/100000: episode: 326, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.333, mean reward: 0.333 [0.333, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.496, 10.100], loss: 0.003763, mae: 0.064340, mean_q: 0.045512
 15988/100000: episode: 327, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.389, mean reward: 0.389 [0.389, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.318, 10.100], loss: 0.006026, mae: 0.084977, mean_q: 0.146997
 15990/100000: episode: 328, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 0.999, mean reward: 0.499 [0.492, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.489, 10.100], loss: 0.003319, mae: 0.061535, mean_q: -0.071434
 15991/100000: episode: 329, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.353, mean reward: 0.353 [0.353, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.985, 10.200], loss: 0.006742, mae: 0.085166, mean_q: -0.280758
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.5557160973548889
 15992/100000: episode: 330, duration: 3.899s, episode steps: 1, steps per second: 0, episode reward: 0.457, mean reward: 0.457 [0.457, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.422, 10.200], loss: 0.004589, mae: 0.072203, mean_q: -0.263810
 16092/100000: episode: 331, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.785, mean reward: -0.168 [-1.000, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.553, 10.098], loss: 0.005405, mae: 0.076815, mean_q: -0.100936
 16192/100000: episode: 332, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -20.462, mean reward: -0.205 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.056, 10.191], loss: 0.005615, mae: 0.079317, mean_q: -0.144019
 16292/100000: episode: 333, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.190, mean reward: -0.172 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.130, 10.238], loss: 0.005655, mae: 0.075237, mean_q: -0.164827
 16392/100000: episode: 334, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -10.490, mean reward: -0.105 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.125, 10.437], loss: 0.005273, mae: 0.077928, mean_q: -0.104854
 16492/100000: episode: 335, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.205, mean reward: -0.162 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.435, 10.221], loss: 0.005528, mae: 0.077335, mean_q: -0.135421
 16592/100000: episode: 336, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.749, mean reward: -0.157 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.140, 10.098], loss: 0.004621, mae: 0.070094, mean_q: -0.117301
 16692/100000: episode: 337, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.829, mean reward: -0.188 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.900, 10.208], loss: 0.004233, mae: 0.068082, mean_q: -0.146056
 16792/100000: episode: 338, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -9.813, mean reward: -0.098 [-1.000, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.284, 10.397], loss: 0.005171, mae: 0.074231, mean_q: -0.145206
 16892/100000: episode: 339, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.828, mean reward: -0.188 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.910, 10.098], loss: 0.004748, mae: 0.072029, mean_q: -0.112411
 16992/100000: episode: 340, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -12.586, mean reward: -0.126 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.597, 10.098], loss: 0.004386, mae: 0.069250, mean_q: -0.134188
 17092/100000: episode: 341, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.724, mean reward: -0.157 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.120, 10.200], loss: 0.004630, mae: 0.070152, mean_q: -0.160379
 17192/100000: episode: 342, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.887, mean reward: -0.189 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.195, 10.098], loss: 0.003991, mae: 0.065948, mean_q: -0.139732
 17292/100000: episode: 343, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.201, mean reward: -0.182 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.049, 10.108], loss: 0.003948, mae: 0.065831, mean_q: -0.138458
 17392/100000: episode: 344, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.355, mean reward: -0.164 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.484, 10.121], loss: 0.003949, mae: 0.065379, mean_q: -0.144131
 17492/100000: episode: 345, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.403, mean reward: -0.184 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.025, 10.098], loss: 0.004154, mae: 0.066776, mean_q: -0.147255
 17592/100000: episode: 346, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.242, mean reward: -0.172 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.475, 10.274], loss: 0.004067, mae: 0.066382, mean_q: -0.128750
 17692/100000: episode: 347, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.372, mean reward: -0.174 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.210, 10.338], loss: 0.004592, mae: 0.070411, mean_q: -0.124511
 17792/100000: episode: 348, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.932, mean reward: -0.159 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.487, 10.484], loss: 0.004007, mae: 0.066880, mean_q: -0.130387
 17892/100000: episode: 349, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.233, mean reward: -0.182 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.122, 10.338], loss: 0.004061, mae: 0.066752, mean_q: -0.094912
 17992/100000: episode: 350, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.858, mean reward: -0.179 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.616, 10.098], loss: 0.006169, mae: 0.077654, mean_q: -0.139993
 18092/100000: episode: 351, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.857, mean reward: -0.179 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.559, 10.098], loss: 0.004316, mae: 0.068045, mean_q: -0.161173
 18192/100000: episode: 352, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -17.245, mean reward: -0.172 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.208, 10.113], loss: 0.004063, mae: 0.066007, mean_q: -0.134736
 18292/100000: episode: 353, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.566, mean reward: -0.156 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.773, 10.098], loss: 0.003890, mae: 0.065104, mean_q: -0.185054
 18392/100000: episode: 354, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -14.878, mean reward: -0.149 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.502, 10.098], loss: 0.004722, mae: 0.071431, mean_q: -0.109115
 18492/100000: episode: 355, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -19.145, mean reward: -0.191 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.156, 10.263], loss: 0.004005, mae: 0.066496, mean_q: -0.156371
 18592/100000: episode: 356, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.919, mean reward: -0.179 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.799, 10.098], loss: 0.004746, mae: 0.067972, mean_q: -0.130943
 18692/100000: episode: 357, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.767, mean reward: -0.198 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.233, 10.101], loss: 0.004557, mae: 0.069855, mean_q: -0.170351
 18792/100000: episode: 358, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.357, mean reward: -0.154 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.643, 10.126], loss: 0.006829, mae: 0.079955, mean_q: -0.153921
 18892/100000: episode: 359, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.791, mean reward: -0.178 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.889, 10.098], loss: 0.003991, mae: 0.066583, mean_q: -0.156968
 18992/100000: episode: 360, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.740, mean reward: -0.177 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.773, 10.335], loss: 0.003785, mae: 0.064404, mean_q: -0.138349
 19092/100000: episode: 361, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.888, mean reward: -0.199 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.559, 10.192], loss: 0.004490, mae: 0.069022, mean_q: -0.141524
 19192/100000: episode: 362, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.888, mean reward: -0.179 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.803, 10.098], loss: 0.004384, mae: 0.070048, mean_q: -0.106949
 19292/100000: episode: 363, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.360, mean reward: -0.164 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.385, 10.098], loss: 0.003832, mae: 0.065148, mean_q: -0.130055
 19392/100000: episode: 364, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -13.898, mean reward: -0.139 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.089, 10.376], loss: 0.003546, mae: 0.062007, mean_q: -0.144736
 19492/100000: episode: 365, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.162, mean reward: -0.162 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.763, 10.109], loss: 0.003682, mae: 0.063733, mean_q: -0.159130
 19592/100000: episode: 366, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -19.104, mean reward: -0.191 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.001, 10.107], loss: 0.003992, mae: 0.065821, mean_q: -0.163532
 19692/100000: episode: 367, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -18.413, mean reward: -0.184 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.800, 10.155], loss: 0.003713, mae: 0.063790, mean_q: -0.146046
 19792/100000: episode: 368, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -15.940, mean reward: -0.159 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.694, 10.098], loss: 0.003722, mae: 0.063250, mean_q: -0.132202
 19892/100000: episode: 369, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.469, mean reward: -0.165 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.602, 10.098], loss: 0.004535, mae: 0.067871, mean_q: -0.141608
 19992/100000: episode: 370, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.196, mean reward: -0.182 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.788, 10.098], loss: 0.003744, mae: 0.064536, mean_q: -0.191905
 20092/100000: episode: 371, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.541, mean reward: -0.155 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.522, 10.106], loss: 0.003482, mae: 0.061596, mean_q: -0.176108
 20192/100000: episode: 372, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.806, mean reward: -0.168 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.685, 10.098], loss: 0.003861, mae: 0.063734, mean_q: -0.203977
 20292/100000: episode: 373, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -14.913, mean reward: -0.149 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.284, 10.098], loss: 0.003399, mae: 0.060995, mean_q: -0.209014
 20392/100000: episode: 374, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.361, mean reward: -0.184 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.973, 10.156], loss: 0.004611, mae: 0.067873, mean_q: -0.246650
 20492/100000: episode: 375, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -13.346, mean reward: -0.133 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.241, 10.098], loss: 0.004202, mae: 0.067132, mean_q: -0.242540
 20592/100000: episode: 376, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -14.572, mean reward: -0.146 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.977, 10.242], loss: 0.003419, mae: 0.060639, mean_q: -0.258464
 20692/100000: episode: 377, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.801, mean reward: -0.158 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.593, 10.098], loss: 0.003252, mae: 0.058186, mean_q: -0.297029
 20792/100000: episode: 378, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.949, mean reward: -0.159 [-1.000, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.143, 10.229], loss: 0.003994, mae: 0.062858, mean_q: -0.324594
 20892/100000: episode: 379, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -14.853, mean reward: -0.149 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.662, 10.098], loss: 0.003900, mae: 0.062054, mean_q: -0.304911
 20992/100000: episode: 380, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -19.242, mean reward: -0.192 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.260, 10.294], loss: 0.002967, mae: 0.054715, mean_q: -0.378372
 21092/100000: episode: 381, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -14.392, mean reward: -0.144 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.861, 10.098], loss: 0.002929, mae: 0.055936, mean_q: -0.287609
 21192/100000: episode: 382, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.016, mean reward: -0.150 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.608, 10.421], loss: 0.003056, mae: 0.057188, mean_q: -0.283431
 21292/100000: episode: 383, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.419, mean reward: -0.184 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.414, 10.185], loss: 0.003778, mae: 0.060433, mean_q: -0.286774
 21392/100000: episode: 384, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -12.828, mean reward: -0.128 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.493, 10.245], loss: 0.003329, mae: 0.059309, mean_q: -0.298367
 21492/100000: episode: 385, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -16.204, mean reward: -0.162 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.221, 10.162], loss: 0.002925, mae: 0.055125, mean_q: -0.348599
 21592/100000: episode: 386, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.113, mean reward: -0.191 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.504, 10.098], loss: 0.002880, mae: 0.054941, mean_q: -0.359610
 21692/100000: episode: 387, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -20.156, mean reward: -0.202 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.003, 10.177], loss: 0.003250, mae: 0.059405, mean_q: -0.293689
 21792/100000: episode: 388, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -10.671, mean reward: -0.107 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.039, 10.098], loss: 0.003333, mae: 0.059249, mean_q: -0.334503
 21892/100000: episode: 389, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.519, mean reward: -0.165 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.660, 10.226], loss: 0.003395, mae: 0.060751, mean_q: -0.287497
 21992/100000: episode: 390, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -20.465, mean reward: -0.205 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.030, 10.183], loss: 0.003089, mae: 0.057199, mean_q: -0.351424
 22092/100000: episode: 391, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.793, mean reward: -0.178 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.107, 10.098], loss: 0.003055, mae: 0.057039, mean_q: -0.323115
 22192/100000: episode: 392, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.002, mean reward: -0.180 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.949, 10.098], loss: 0.003238, mae: 0.059028, mean_q: -0.317304
 22292/100000: episode: 393, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -15.061, mean reward: -0.151 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.077, 10.098], loss: 0.003060, mae: 0.058443, mean_q: -0.298155
 22392/100000: episode: 394, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -11.647, mean reward: -0.116 [-1.000, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.163, 10.098], loss: 0.003655, mae: 0.061702, mean_q: -0.336603
 22492/100000: episode: 395, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.161, mean reward: -0.172 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.423, 10.098], loss: 0.003210, mae: 0.060439, mean_q: -0.290622
 22592/100000: episode: 396, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -12.048, mean reward: -0.120 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.441, 10.407], loss: 0.003203, mae: 0.059452, mean_q: -0.265438
 22692/100000: episode: 397, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -14.362, mean reward: -0.144 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.913, 10.098], loss: 0.002926, mae: 0.056082, mean_q: -0.321975
 22792/100000: episode: 398, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -17.803, mean reward: -0.178 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.386, 10.098], loss: 0.003281, mae: 0.058853, mean_q: -0.302105
 22892/100000: episode: 399, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -15.384, mean reward: -0.154 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.442, 10.098], loss: 0.003005, mae: 0.056894, mean_q: -0.306300
 22992/100000: episode: 400, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.382, mean reward: -0.144 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.417, 10.098], loss: 0.003907, mae: 0.062286, mean_q: -0.331790
 23092/100000: episode: 401, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.191, mean reward: -0.162 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.599, 10.098], loss: 0.004887, mae: 0.068874, mean_q: -0.306321
 23192/100000: episode: 402, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -20.382, mean reward: -0.204 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.688, 10.157], loss: 0.003195, mae: 0.058709, mean_q: -0.324600
 23292/100000: episode: 403, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.290, mean reward: -0.193 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.711, 10.212], loss: 0.002979, mae: 0.057250, mean_q: -0.300594
 23392/100000: episode: 404, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.782, mean reward: -0.188 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.545, 10.098], loss: 0.003191, mae: 0.058291, mean_q: -0.334336
 23492/100000: episode: 405, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -11.489, mean reward: -0.115 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.955, 10.327], loss: 0.002966, mae: 0.056218, mean_q: -0.317253
 23592/100000: episode: 406, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.201, mean reward: -0.172 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.043, 10.098], loss: 0.002938, mae: 0.055223, mean_q: -0.321567
 23692/100000: episode: 407, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.172, mean reward: -0.192 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.118, 10.098], loss: 0.003464, mae: 0.059675, mean_q: -0.324070
 23792/100000: episode: 408, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -14.167, mean reward: -0.142 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.056, 10.098], loss: 0.003821, mae: 0.063070, mean_q: -0.318868
 23892/100000: episode: 409, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.083, mean reward: -0.161 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.479, 10.243], loss: 0.003351, mae: 0.059243, mean_q: -0.317355
 23992/100000: episode: 410, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -14.875, mean reward: -0.149 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.808, 10.098], loss: 0.002864, mae: 0.055058, mean_q: -0.291279
 24092/100000: episode: 411, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.397, mean reward: -0.194 [-1.000, 0.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.523, 10.123], loss: 0.003010, mae: 0.056880, mean_q: -0.304627
 24192/100000: episode: 412, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -15.823, mean reward: -0.158 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.340, 10.484], loss: 0.002889, mae: 0.054662, mean_q: -0.305815
 24292/100000: episode: 413, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -15.434, mean reward: -0.154 [-1.000, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.230, 10.208], loss: 0.003163, mae: 0.057879, mean_q: -0.273310
 24392/100000: episode: 414, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.067, mean reward: -0.171 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.460, 10.098], loss: 0.002882, mae: 0.054585, mean_q: -0.316710
 24492/100000: episode: 415, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.142, mean reward: -0.171 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.134, 10.226], loss: 0.002923, mae: 0.055878, mean_q: -0.288455
 24592/100000: episode: 416, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.233, mean reward: -0.162 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.690, 10.098], loss: 0.002914, mae: 0.055851, mean_q: -0.320637
 24692/100000: episode: 417, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.420, mean reward: -0.194 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.375, 10.160], loss: 0.002965, mae: 0.056007, mean_q: -0.320079
 24792/100000: episode: 418, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -13.387, mean reward: -0.134 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.748, 10.098], loss: 0.003030, mae: 0.056397, mean_q: -0.317085
 24892/100000: episode: 419, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -15.736, mean reward: -0.157 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-2.009, 10.098], loss: 0.002856, mae: 0.055175, mean_q: -0.284504
 24992/100000: episode: 420, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -17.106, mean reward: -0.171 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.701, 10.268], loss: 0.003040, mae: 0.056729, mean_q: -0.313533
 25092/100000: episode: 421, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.563, mean reward: -0.146 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.588, 10.098], loss: 0.002831, mae: 0.054314, mean_q: -0.280886
 25192/100000: episode: 422, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -15.623, mean reward: -0.156 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.067, 10.138], loss: 0.002869, mae: 0.054085, mean_q: -0.348307
 25292/100000: episode: 423, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.789, mean reward: -0.168 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.519, 10.098], loss: 0.003094, mae: 0.057205, mean_q: -0.309182
 25392/100000: episode: 424, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -18.345, mean reward: -0.183 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.563, 10.225], loss: 0.003083, mae: 0.059137, mean_q: -0.314341
 25492/100000: episode: 425, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.559, mean reward: -0.196 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.903, 10.098], loss: 0.003102, mae: 0.056537, mean_q: -0.311376
 25592/100000: episode: 426, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.561, mean reward: -0.146 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.057, 10.304], loss: 0.003363, mae: 0.060249, mean_q: -0.301577
 25692/100000: episode: 427, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -17.690, mean reward: -0.177 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.878, 10.120], loss: 0.005102, mae: 0.070222, mean_q: -0.297595
 25792/100000: episode: 428, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -17.706, mean reward: -0.177 [-1.000, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.516, 10.098], loss: 0.003162, mae: 0.056443, mean_q: -0.326844
 25892/100000: episode: 429, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.846, mean reward: -0.188 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.023, 10.098], loss: 0.003101, mae: 0.056392, mean_q: -0.285735
[Info] 100-TH LEVEL FOUND: 0.583740770816803, Considering 10/90 traces
 25992/100000: episode: 430, duration: 4.349s, episode steps: 100, steps per second: 23, episode reward: -11.834, mean reward: -0.118 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.271, 10.098], loss: 0.003044, mae: 0.056376, mean_q: -0.309972
 26012/100000: episode: 431, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 4.786, mean reward: 0.239 [0.156, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.287], loss: 0.002743, mae: 0.053598, mean_q: -0.256290
 26027/100000: episode: 432, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 4.811, mean reward: 0.321 [0.252, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.592, 10.410], loss: 0.002802, mae: 0.053638, mean_q: -0.300683
 26068/100000: episode: 433, duration: 0.208s, episode steps: 41, steps per second: 198, episode reward: 10.871, mean reward: 0.265 [0.046, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.783, 10.153], loss: 0.003190, mae: 0.056465, mean_q: -0.357219
 26088/100000: episode: 434, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 7.828, mean reward: 0.391 [0.257, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.093, 10.507], loss: 0.003265, mae: 0.059898, mean_q: -0.262683
 26129/100000: episode: 435, duration: 0.222s, episode steps: 41, steps per second: 185, episode reward: 8.625, mean reward: 0.210 [0.024, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.480, 10.100], loss: 0.002797, mae: 0.054244, mean_q: -0.318692
 26169/100000: episode: 436, duration: 0.191s, episode steps: 40, steps per second: 209, episode reward: 7.359, mean reward: 0.184 [0.045, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.504, 10.164], loss: 0.003070, mae: 0.055266, mean_q: -0.285928
 26183/100000: episode: 437, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 4.444, mean reward: 0.317 [0.238, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.925, 10.414], loss: 0.003465, mae: 0.057650, mean_q: -0.327480
 26228/100000: episode: 438, duration: 0.248s, episode steps: 45, steps per second: 182, episode reward: 8.291, mean reward: 0.184 [0.037, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.692, 10.167], loss: 0.003207, mae: 0.056601, mean_q: -0.285612
 26269/100000: episode: 439, duration: 0.203s, episode steps: 41, steps per second: 202, episode reward: 6.281, mean reward: 0.153 [0.025, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.157, 10.163], loss: 0.003123, mae: 0.055592, mean_q: -0.312048
 26314/100000: episode: 440, duration: 0.257s, episode steps: 45, steps per second: 175, episode reward: 11.811, mean reward: 0.262 [0.028, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.468, 10.100], loss: 0.003224, mae: 0.058389, mean_q: -0.240497
 26354/100000: episode: 441, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 11.281, mean reward: 0.282 [0.110, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.220, 10.359], loss: 0.002838, mae: 0.052420, mean_q: -0.301889
 26368/100000: episode: 442, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 3.787, mean reward: 0.270 [0.234, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.372], loss: 0.002593, mae: 0.051244, mean_q: -0.270365
 26382/100000: episode: 443, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 5.065, mean reward: 0.362 [0.257, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.536], loss: 0.002843, mae: 0.055281, mean_q: -0.265993
 26422/100000: episode: 444, duration: 0.197s, episode steps: 40, steps per second: 203, episode reward: 8.426, mean reward: 0.211 [0.100, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.417, 10.300], loss: 0.003259, mae: 0.058992, mean_q: -0.259955
 26467/100000: episode: 445, duration: 0.220s, episode steps: 45, steps per second: 205, episode reward: 12.717, mean reward: 0.283 [0.177, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.497, 10.320], loss: 0.002978, mae: 0.056984, mean_q: -0.225264
 26481/100000: episode: 446, duration: 0.084s, episode steps: 14, steps per second: 168, episode reward: 3.466, mean reward: 0.248 [0.192, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.449, 10.369], loss: 0.002606, mae: 0.052058, mean_q: -0.281349
 26496/100000: episode: 447, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 3.652, mean reward: 0.243 [0.175, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.370, 10.261], loss: 0.003061, mae: 0.055661, mean_q: -0.302344
 26519/100000: episode: 448, duration: 0.138s, episode steps: 23, steps per second: 166, episode reward: 7.006, mean reward: 0.305 [0.208, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.527, 10.471], loss: 0.002738, mae: 0.053618, mean_q: -0.281367
 26542/100000: episode: 449, duration: 0.108s, episode steps: 23, steps per second: 214, episode reward: 5.118, mean reward: 0.223 [0.087, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.212], loss: 0.002858, mae: 0.053402, mean_q: -0.242852
 26577/100000: episode: 450, duration: 0.171s, episode steps: 35, steps per second: 205, episode reward: 6.820, mean reward: 0.195 [0.031, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.391, 10.152], loss: 0.002697, mae: 0.053286, mean_q: -0.193134
 26620/100000: episode: 451, duration: 0.225s, episode steps: 43, steps per second: 191, episode reward: 11.279, mean reward: 0.262 [0.177, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.955, 10.358], loss: 0.002662, mae: 0.052742, mean_q: -0.254368
 26661/100000: episode: 452, duration: 0.220s, episode steps: 41, steps per second: 186, episode reward: 8.949, mean reward: 0.218 [0.103, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.937, 10.303], loss: 0.003501, mae: 0.061501, mean_q: -0.200242
 26675/100000: episode: 453, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 4.530, mean reward: 0.324 [0.194, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.444], loss: 0.002950, mae: 0.058383, mean_q: -0.070393
 26689/100000: episode: 454, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 4.726, mean reward: 0.338 [0.207, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.485], loss: 0.002639, mae: 0.052732, mean_q: -0.176674
 26724/100000: episode: 455, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 9.115, mean reward: 0.260 [0.033, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.765, 10.290], loss: 0.002668, mae: 0.051747, mean_q: -0.218217
 26738/100000: episode: 456, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 4.709, mean reward: 0.336 [0.197, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.415, 10.368], loss: 0.002752, mae: 0.051236, mean_q: -0.292247
 26753/100000: episode: 457, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 3.840, mean reward: 0.256 [0.197, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.336], loss: 0.003073, mae: 0.056237, mean_q: -0.207093
 26796/100000: episode: 458, duration: 0.213s, episode steps: 43, steps per second: 202, episode reward: 9.915, mean reward: 0.231 [0.035, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.646, 10.102], loss: 0.003454, mae: 0.062512, mean_q: -0.176976
 26819/100000: episode: 459, duration: 0.145s, episode steps: 23, steps per second: 158, episode reward: 7.517, mean reward: 0.327 [0.227, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.482], loss: 0.003567, mae: 0.060778, mean_q: -0.174622
 26834/100000: episode: 460, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 4.481, mean reward: 0.299 [0.206, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.175, 10.504], loss: 0.003178, mae: 0.057134, mean_q: -0.179079
 26857/100000: episode: 461, duration: 0.111s, episode steps: 23, steps per second: 208, episode reward: 6.021, mean reward: 0.262 [0.103, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.172, 10.277], loss: 0.003241, mae: 0.058050, mean_q: -0.163887
 26877/100000: episode: 462, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 7.551, mean reward: 0.378 [0.239, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.468, 10.436], loss: 0.003100, mae: 0.057361, mean_q: -0.125020
 26897/100000: episode: 463, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 5.437, mean reward: 0.272 [0.182, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.318], loss: 0.002847, mae: 0.056092, mean_q: -0.136845
 26912/100000: episode: 464, duration: 0.071s, episode steps: 15, steps per second: 211, episode reward: 4.143, mean reward: 0.276 [0.198, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.384], loss: 0.002612, mae: 0.052449, mean_q: -0.135930
 26957/100000: episode: 465, duration: 0.228s, episode steps: 45, steps per second: 197, episode reward: 10.508, mean reward: 0.234 [0.080, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.204, 10.333], loss: 0.003160, mae: 0.058456, mean_q: -0.185914
 26977/100000: episode: 466, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 5.796, mean reward: 0.290 [0.232, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.444], loss: 0.002722, mae: 0.054546, mean_q: -0.150044
 27020/100000: episode: 467, duration: 0.229s, episode steps: 43, steps per second: 188, episode reward: 10.299, mean reward: 0.240 [0.046, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.878, 10.259], loss: 0.003445, mae: 0.062235, mean_q: -0.178282
 27035/100000: episode: 468, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 2.544, mean reward: 0.170 [0.104, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.201], loss: 0.003319, mae: 0.060131, mean_q: -0.104935
 27078/100000: episode: 469, duration: 0.202s, episode steps: 43, steps per second: 212, episode reward: 8.833, mean reward: 0.205 [0.044, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.160, 10.193], loss: 0.003003, mae: 0.057368, mean_q: -0.111201
 27098/100000: episode: 470, duration: 0.099s, episode steps: 20, steps per second: 203, episode reward: 4.779, mean reward: 0.239 [0.105, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.233], loss: 0.003005, mae: 0.056173, mean_q: -0.139535
 27138/100000: episode: 471, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 13.412, mean reward: 0.335 [0.108, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.698, 10.534], loss: 0.003232, mae: 0.059362, mean_q: -0.129797
 27179/100000: episode: 472, duration: 0.221s, episode steps: 41, steps per second: 186, episode reward: 10.756, mean reward: 0.262 [0.171, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.084, 10.374], loss: 0.002611, mae: 0.053755, mean_q: -0.107925
 27222/100000: episode: 473, duration: 0.204s, episode steps: 43, steps per second: 211, episode reward: 8.611, mean reward: 0.200 [0.053, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.661, 10.302], loss: 0.003516, mae: 0.059694, mean_q: -0.121184
 27236/100000: episode: 474, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 2.827, mean reward: 0.202 [0.083, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.281], loss: 0.002753, mae: 0.056046, mean_q: -0.137269
 27250/100000: episode: 475, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 3.681, mean reward: 0.263 [0.169, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.335], loss: 0.003503, mae: 0.063139, mean_q: -0.173939
 27291/100000: episode: 476, duration: 0.211s, episode steps: 41, steps per second: 195, episode reward: 10.378, mean reward: 0.253 [0.069, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.054, 10.210], loss: 0.009027, mae: 0.087154, mean_q: -0.119796
 27314/100000: episode: 477, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 5.190, mean reward: 0.226 [0.041, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.638, 10.151], loss: 0.004588, mae: 0.070101, mean_q: -0.156880
 27354/100000: episode: 478, duration: 0.218s, episode steps: 40, steps per second: 184, episode reward: 13.276, mean reward: 0.332 [0.206, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.737, 10.488], loss: 0.004292, mae: 0.062968, mean_q: -0.105665
 27397/100000: episode: 479, duration: 0.214s, episode steps: 43, steps per second: 201, episode reward: 8.310, mean reward: 0.193 [0.028, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.664, 10.155], loss: 0.004040, mae: 0.064996, mean_q: -0.033416
 27440/100000: episode: 480, duration: 0.224s, episode steps: 43, steps per second: 192, episode reward: 7.685, mean reward: 0.179 [0.029, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.445, 10.164], loss: 0.003070, mae: 0.058326, mean_q: -0.059377
[Info] FALSIFICATION!
 27462/100000: episode: 481, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 20.122, mean reward: 0.915 [0.345, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.734 [-0.974, 9.458], loss: 0.003206, mae: 0.058566, mean_q: -0.047056
 27562/100000: episode: 482, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.607, mean reward: -0.166 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.758, 10.098], loss: 0.020401, mae: 0.085082, mean_q: -0.056554
 27662/100000: episode: 483, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -16.484, mean reward: -0.165 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.837, 10.098], loss: 0.046596, mae: 0.095569, mean_q: -0.090703
 27762/100000: episode: 484, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -20.190, mean reward: -0.202 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.369, 10.209], loss: 0.018286, mae: 0.076743, mean_q: -0.040258
 27862/100000: episode: 485, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.930, mean reward: -0.169 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.609, 10.098], loss: 0.030848, mae: 0.077353, mean_q: -0.084690
 27962/100000: episode: 486, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -20.008, mean reward: -0.200 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.941, 10.098], loss: 0.003857, mae: 0.064139, mean_q: -0.073748
 28062/100000: episode: 487, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -15.245, mean reward: -0.152 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.913, 10.262], loss: 0.017488, mae: 0.070643, mean_q: -0.069404
 28162/100000: episode: 488, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.882, mean reward: -0.199 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.175, 10.101], loss: 0.003321, mae: 0.059479, mean_q: -0.102322
 28262/100000: episode: 489, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.514, mean reward: -0.145 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.126, 10.259], loss: 0.017703, mae: 0.073235, mean_q: -0.061726
 28362/100000: episode: 490, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.502, mean reward: -0.185 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.286, 10.098], loss: 0.003115, mae: 0.058055, mean_q: -0.075151
 28462/100000: episode: 491, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -14.785, mean reward: -0.148 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.604, 10.098], loss: 0.017370, mae: 0.073041, mean_q: -0.038936
 28562/100000: episode: 492, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.579, mean reward: -0.166 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.849, 10.098], loss: 0.043051, mae: 0.082850, mean_q: -0.067044
 28662/100000: episode: 493, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -13.016, mean reward: -0.130 [-1.000, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.977, 10.458], loss: 0.003396, mae: 0.060708, mean_q: -0.083492
 28762/100000: episode: 494, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -12.268, mean reward: -0.123 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.651, 10.357], loss: 0.003796, mae: 0.060101, mean_q: -0.068264
 28862/100000: episode: 495, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.496, mean reward: -0.165 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.419, 10.190], loss: 0.018189, mae: 0.076325, mean_q: -0.057225
 28962/100000: episode: 496, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.651, mean reward: -0.177 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.959, 10.298], loss: 0.017480, mae: 0.072981, mean_q: -0.064507
 29062/100000: episode: 497, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -19.132, mean reward: -0.191 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.908, 10.098], loss: 0.003365, mae: 0.060319, mean_q: -0.038814
 29162/100000: episode: 498, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -13.056, mean reward: -0.131 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.810, 10.131], loss: 0.029654, mae: 0.073890, mean_q: -0.077809
 29262/100000: episode: 499, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -17.109, mean reward: -0.171 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.933, 10.185], loss: 0.030233, mae: 0.080660, mean_q: -0.032658
 29362/100000: episode: 500, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.214, mean reward: -0.162 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.398, 10.332], loss: 0.004311, mae: 0.063935, mean_q: -0.075370
 29462/100000: episode: 501, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -16.490, mean reward: -0.165 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.833, 10.156], loss: 0.003076, mae: 0.058403, mean_q: -0.086493
 29562/100000: episode: 502, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.138, mean reward: -0.181 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.582, 10.192], loss: 0.003183, mae: 0.057999, mean_q: -0.062588
 29662/100000: episode: 503, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.678, mean reward: -0.167 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.695, 10.098], loss: 0.003092, mae: 0.057778, mean_q: -0.082231
 29762/100000: episode: 504, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.068, mean reward: -0.181 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.296, 10.206], loss: 0.016630, mae: 0.067593, mean_q: -0.069066
 29862/100000: episode: 505, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -15.725, mean reward: -0.157 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.165, 10.098], loss: 0.016645, mae: 0.067379, mean_q: -0.089599
 29962/100000: episode: 506, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.145, mean reward: -0.131 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.084, 10.098], loss: 0.030051, mae: 0.077917, mean_q: -0.079336
 30062/100000: episode: 507, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -13.457, mean reward: -0.135 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.985, 10.098], loss: 0.040818, mae: 0.073658, mean_q: -0.087069
 30162/100000: episode: 508, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.829, mean reward: -0.148 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.588, 10.098], loss: 0.003304, mae: 0.060241, mean_q: -0.067429
 30262/100000: episode: 509, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.020, mean reward: -0.160 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.929, 10.330], loss: 0.002898, mae: 0.056325, mean_q: -0.090026
 30362/100000: episode: 510, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.571, mean reward: -0.176 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.109, 10.106], loss: 0.016307, mae: 0.067399, mean_q: -0.052862
 30462/100000: episode: 511, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -12.801, mean reward: -0.128 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.770, 10.477], loss: 0.003255, mae: 0.057306, mean_q: -0.062763
 30562/100000: episode: 512, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.229, mean reward: -0.172 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.432, 10.098], loss: 0.015560, mae: 0.061242, mean_q: -0.064698
 30662/100000: episode: 513, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.872, mean reward: -0.179 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.382, 10.098], loss: 0.004141, mae: 0.066399, mean_q: -0.088203
 30762/100000: episode: 514, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.370, mean reward: -0.164 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.050, 10.098], loss: 0.017188, mae: 0.073050, mean_q: -0.048882
 30862/100000: episode: 515, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -16.400, mean reward: -0.164 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.848, 10.098], loss: 0.029742, mae: 0.081458, mean_q: -0.095767
 30962/100000: episode: 516, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.930, mean reward: -0.189 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.822, 10.098], loss: 0.003140, mae: 0.058471, mean_q: -0.072154
 31062/100000: episode: 517, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -15.870, mean reward: -0.159 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.698, 10.098], loss: 0.003056, mae: 0.057354, mean_q: -0.090303
 31162/100000: episode: 518, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -17.369, mean reward: -0.174 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.682, 10.176], loss: 0.003086, mae: 0.057659, mean_q: -0.114869
 31262/100000: episode: 519, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.694, mean reward: -0.177 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.889, 10.108], loss: 0.003053, mae: 0.057315, mean_q: -0.141164
 31362/100000: episode: 520, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.110, mean reward: -0.191 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.487, 10.234], loss: 0.003372, mae: 0.060750, mean_q: -0.126043
 31462/100000: episode: 521, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.467, mean reward: -0.185 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.544, 10.100], loss: 0.002999, mae: 0.056208, mean_q: -0.153882
 31562/100000: episode: 522, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -12.156, mean reward: -0.122 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.329, 10.268], loss: 0.004567, mae: 0.068036, mean_q: -0.168750
 31662/100000: episode: 523, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -12.772, mean reward: -0.128 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.885, 10.489], loss: 0.016874, mae: 0.067423, mean_q: -0.176202
 31762/100000: episode: 524, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -18.138, mean reward: -0.181 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.259, 10.098], loss: 0.031116, mae: 0.074990, mean_q: -0.189777
 31862/100000: episode: 525, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.199, mean reward: -0.192 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.656, 10.208], loss: 0.031127, mae: 0.077977, mean_q: -0.227370
 31962/100000: episode: 526, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.532, mean reward: -0.155 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.778, 10.205], loss: 0.003470, mae: 0.061111, mean_q: -0.245598
 32062/100000: episode: 527, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.564, mean reward: -0.176 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.764, 10.139], loss: 0.016626, mae: 0.068335, mean_q: -0.249807
 32162/100000: episode: 528, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.553, mean reward: -0.186 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.895, 10.098], loss: 0.017006, mae: 0.065569, mean_q: -0.284657
 32262/100000: episode: 529, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.842, mean reward: -0.168 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.748, 10.098], loss: 0.028228, mae: 0.071770, mean_q: -0.279232
 32362/100000: episode: 530, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.863, mean reward: -0.159 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.579, 10.266], loss: 0.016388, mae: 0.068416, mean_q: -0.283721
 32462/100000: episode: 531, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.912, mean reward: -0.169 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.736, 10.098], loss: 0.016518, mae: 0.067064, mean_q: -0.316925
 32562/100000: episode: 532, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.158, mean reward: -0.172 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.975, 10.255], loss: 0.003109, mae: 0.056707, mean_q: -0.296581
 32662/100000: episode: 533, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.758, mean reward: -0.188 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.758, 10.098], loss: 0.002772, mae: 0.053383, mean_q: -0.318487
 32762/100000: episode: 534, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.363, mean reward: -0.174 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.563, 10.258], loss: 0.002925, mae: 0.054575, mean_q: -0.302712
 32862/100000: episode: 535, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -19.282, mean reward: -0.193 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.770, 10.098], loss: 0.002697, mae: 0.052642, mean_q: -0.330824
 32962/100000: episode: 536, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -10.920, mean reward: -0.109 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.938, 10.225], loss: 0.002852, mae: 0.054308, mean_q: -0.317122
 33062/100000: episode: 537, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -18.861, mean reward: -0.189 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.672, 10.103], loss: 0.002880, mae: 0.053971, mean_q: -0.337393
 33162/100000: episode: 538, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.232, mean reward: -0.192 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.283, 10.301], loss: 0.002846, mae: 0.053516, mean_q: -0.313173
 33262/100000: episode: 539, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -12.507, mean reward: -0.125 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.056, 10.098], loss: 0.002878, mae: 0.054335, mean_q: -0.289874
 33362/100000: episode: 540, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -20.316, mean reward: -0.203 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.793, 10.098], loss: 0.002931, mae: 0.054967, mean_q: -0.280321
 33462/100000: episode: 541, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.392, mean reward: -0.184 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.448, 10.226], loss: 0.002939, mae: 0.054932, mean_q: -0.336285
 33562/100000: episode: 542, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.250, mean reward: -0.173 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.071, 10.186], loss: 0.002792, mae: 0.053819, mean_q: -0.306795
 33662/100000: episode: 543, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -14.323, mean reward: -0.143 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.411, 10.386], loss: 0.005513, mae: 0.066633, mean_q: -0.332030
 33762/100000: episode: 544, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.547, mean reward: -0.185 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.027, 10.261], loss: 0.005644, mae: 0.068922, mean_q: -0.314351
 33862/100000: episode: 545, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -15.590, mean reward: -0.156 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.791, 10.098], loss: 0.006537, mae: 0.069908, mean_q: -0.293835
 33962/100000: episode: 546, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.117, mean reward: -0.171 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.824, 10.200], loss: 0.004324, mae: 0.060887, mean_q: -0.299147
 34062/100000: episode: 547, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.414, mean reward: -0.184 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.074, 10.098], loss: 0.003220, mae: 0.058535, mean_q: -0.341392
 34162/100000: episode: 548, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.495, mean reward: -0.175 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.670, 10.178], loss: 0.002989, mae: 0.055529, mean_q: -0.311186
 34262/100000: episode: 549, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -14.168, mean reward: -0.142 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.619, 10.329], loss: 0.002845, mae: 0.054463, mean_q: -0.302937
 34362/100000: episode: 550, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.809, mean reward: -0.188 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.707, 10.098], loss: 0.003124, mae: 0.056386, mean_q: -0.335186
 34462/100000: episode: 551, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -19.902, mean reward: -0.199 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.488, 10.098], loss: 0.002918, mae: 0.054444, mean_q: -0.304934
 34562/100000: episode: 552, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -14.760, mean reward: -0.148 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.738, 10.098], loss: 0.004589, mae: 0.063720, mean_q: -0.288290
 34662/100000: episode: 553, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.750, mean reward: -0.168 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.984, 10.098], loss: 0.002779, mae: 0.054206, mean_q: -0.309898
 34762/100000: episode: 554, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -19.415, mean reward: -0.194 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.910, 10.098], loss: 0.003053, mae: 0.055462, mean_q: -0.314976
 34862/100000: episode: 555, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -17.350, mean reward: -0.173 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.676, 10.098], loss: 0.003055, mae: 0.056735, mean_q: -0.328562
 34962/100000: episode: 556, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.731, mean reward: -0.177 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.127, 10.098], loss: 0.002619, mae: 0.051662, mean_q: -0.323491
 35062/100000: episode: 557, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.435, mean reward: -0.184 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.371, 10.230], loss: 0.002802, mae: 0.053516, mean_q: -0.349124
 35162/100000: episode: 558, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -18.987, mean reward: -0.190 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.276, 10.098], loss: 0.002867, mae: 0.053645, mean_q: -0.309341
 35262/100000: episode: 559, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -12.107, mean reward: -0.121 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.173, 10.450], loss: 0.002772, mae: 0.052699, mean_q: -0.314704
 35362/100000: episode: 560, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.011, mean reward: -0.160 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.148, 10.098], loss: 0.002850, mae: 0.053533, mean_q: -0.325153
 35462/100000: episode: 561, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -20.265, mean reward: -0.203 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.075, 10.124], loss: 0.002899, mae: 0.055343, mean_q: -0.294688
 35562/100000: episode: 562, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -13.790, mean reward: -0.138 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.823, 10.294], loss: 0.002770, mae: 0.053204, mean_q: -0.320133
 35662/100000: episode: 563, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.265, mean reward: -0.163 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.794, 10.159], loss: 0.003181, mae: 0.057958, mean_q: -0.300459
 35762/100000: episode: 564, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.228, mean reward: -0.182 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.059, 10.282], loss: 0.002727, mae: 0.053056, mean_q: -0.343697
 35862/100000: episode: 565, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -14.874, mean reward: -0.149 [-1.000, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.850, 10.098], loss: 0.002762, mae: 0.054117, mean_q: -0.322181
 35962/100000: episode: 566, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.001, mean reward: -0.150 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.617, 10.435], loss: 0.002943, mae: 0.055382, mean_q: -0.302117
 36062/100000: episode: 567, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -20.348, mean reward: -0.203 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.626, 10.147], loss: 0.002702, mae: 0.052604, mean_q: -0.310752
 36162/100000: episode: 568, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.779, mean reward: -0.168 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.719, 10.098], loss: 0.002726, mae: 0.053044, mean_q: -0.322622
 36262/100000: episode: 569, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.365, mean reward: -0.184 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.822, 10.166], loss: 0.003071, mae: 0.056216, mean_q: -0.318429
 36362/100000: episode: 570, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -14.450, mean reward: -0.144 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.041, 10.158], loss: 0.002924, mae: 0.054238, mean_q: -0.307711
 36462/100000: episode: 571, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.934, mean reward: -0.169 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.106, 10.098], loss: 0.002803, mae: 0.053030, mean_q: -0.318155
 36562/100000: episode: 572, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.185, mean reward: -0.132 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.577, 10.098], loss: 0.002802, mae: 0.052796, mean_q: -0.330913
 36662/100000: episode: 573, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -17.775, mean reward: -0.178 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.916, 10.098], loss: 0.004111, mae: 0.060950, mean_q: -0.334332
 36762/100000: episode: 574, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.977, mean reward: -0.190 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.576, 10.240], loss: 0.003665, mae: 0.058981, mean_q: -0.324238
 36862/100000: episode: 575, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.057, mean reward: -0.151 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.480, 10.098], loss: 0.002969, mae: 0.054868, mean_q: -0.348557
 36962/100000: episode: 576, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -14.567, mean reward: -0.146 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.685, 10.098], loss: 0.002706, mae: 0.051977, mean_q: -0.321937
 37062/100000: episode: 577, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -15.793, mean reward: -0.158 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.665, 10.198], loss: 0.002680, mae: 0.052427, mean_q: -0.330544
 37162/100000: episode: 578, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -9.833, mean reward: -0.098 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-2.858, 10.098], loss: 0.002806, mae: 0.053196, mean_q: -0.335236
 37262/100000: episode: 579, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -14.677, mean reward: -0.147 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.171, 10.098], loss: 0.002829, mae: 0.053123, mean_q: -0.307383
 37362/100000: episode: 580, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.153, mean reward: -0.172 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.781, 10.154], loss: 0.002978, mae: 0.054974, mean_q: -0.320147
[Info] 100-TH LEVEL FOUND: 0.686788022518158, Considering 10/90 traces
 37462/100000: episode: 581, duration: 4.723s, episode steps: 100, steps per second: 21, episode reward: -15.488, mean reward: -0.155 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.801, 10.098], loss: 0.003054, mae: 0.056508, mean_q: -0.302400
 37473/100000: episode: 582, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 3.448, mean reward: 0.313 [0.241, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.443, 10.100], loss: 0.002331, mae: 0.048298, mean_q: -0.361363
 37491/100000: episode: 583, duration: 0.087s, episode steps: 18, steps per second: 208, episode reward: 6.668, mean reward: 0.370 [0.246, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.265, 10.512], loss: 0.002632, mae: 0.051396, mean_q: -0.319811
 37517/100000: episode: 584, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 13.261, mean reward: 0.510 [0.380, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.507], loss: 0.003016, mae: 0.055415, mean_q: -0.310700
 37534/100000: episode: 585, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 6.559, mean reward: 0.386 [0.319, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.497], loss: 0.003192, mae: 0.057056, mean_q: -0.376666
 37544/100000: episode: 586, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 2.618, mean reward: 0.262 [0.204, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.219, 10.100], loss: 0.002548, mae: 0.050724, mean_q: -0.360162
 37570/100000: episode: 587, duration: 0.150s, episode steps: 26, steps per second: 174, episode reward: 7.131, mean reward: 0.274 [0.094, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.326, 10.322], loss: 0.003072, mae: 0.055031, mean_q: -0.321957
 37580/100000: episode: 588, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 2.664, mean reward: 0.266 [0.196, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.163, 10.100], loss: 0.002692, mae: 0.051797, mean_q: -0.237743
 37594/100000: episode: 589, duration: 0.068s, episode steps: 14, steps per second: 206, episode reward: 5.015, mean reward: 0.358 [0.225, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.134, 10.100], loss: 0.002698, mae: 0.054030, mean_q: -0.262618
 37610/100000: episode: 590, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 5.423, mean reward: 0.339 [0.294, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.379, 10.100], loss: 0.003446, mae: 0.059772, mean_q: -0.227219
 37623/100000: episode: 591, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 6.452, mean reward: 0.496 [0.419, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.576, 10.100], loss: 0.002778, mae: 0.057844, mean_q: -0.246429
 37634/100000: episode: 592, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 3.556, mean reward: 0.323 [0.182, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.296, 10.100], loss: 0.002721, mae: 0.053914, mean_q: -0.203637
 37660/100000: episode: 593, duration: 0.134s, episode steps: 26, steps per second: 193, episode reward: 8.623, mean reward: 0.332 [0.179, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.035, 10.349], loss: 0.002637, mae: 0.053250, mean_q: -0.245273
 37673/100000: episode: 594, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 4.578, mean reward: 0.352 [0.261, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.241, 10.100], loss: 0.002506, mae: 0.048080, mean_q: -0.229948
 37684/100000: episode: 595, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 3.425, mean reward: 0.311 [0.208, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.481, 10.100], loss: 0.003162, mae: 0.055353, mean_q: -0.359500
 37695/100000: episode: 596, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 3.289, mean reward: 0.299 [0.266, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.420, 10.100], loss: 0.002606, mae: 0.053389, mean_q: -0.211455
 37706/100000: episode: 597, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 3.623, mean reward: 0.329 [0.218, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.321, 10.100], loss: 0.002739, mae: 0.051819, mean_q: -0.269977
 37716/100000: episode: 598, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 3.233, mean reward: 0.323 [0.250, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.505, 10.100], loss: 0.002468, mae: 0.050868, mean_q: -0.224012
 37734/100000: episode: 599, duration: 0.085s, episode steps: 18, steps per second: 211, episode reward: 6.656, mean reward: 0.370 [0.281, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.429], loss: 0.003305, mae: 0.058497, mean_q: -0.204777
 37750/100000: episode: 600, duration: 0.079s, episode steps: 16, steps per second: 201, episode reward: 5.445, mean reward: 0.340 [0.284, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.360, 10.100], loss: 0.003250, mae: 0.057265, mean_q: -0.268123
 37770/100000: episode: 601, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 6.395, mean reward: 0.320 [0.263, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.277, 10.100], loss: 0.002545, mae: 0.052170, mean_q: -0.212940
 37786/100000: episode: 602, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 4.897, mean reward: 0.306 [0.202, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.510, 10.100], loss: 0.002694, mae: 0.054228, mean_q: -0.325963
 37796/100000: episode: 603, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 3.234, mean reward: 0.323 [0.248, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.295, 10.100], loss: 0.002721, mae: 0.053729, mean_q: -0.404896
 37814/100000: episode: 604, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 6.722, mean reward: 0.373 [0.206, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.494, 10.348], loss: 0.002770, mae: 0.053121, mean_q: -0.279304
 37834/100000: episode: 605, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 7.126, mean reward: 0.356 [0.242, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.557, 10.100], loss: 0.003223, mae: 0.055063, mean_q: -0.306902
 37845/100000: episode: 606, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 2.539, mean reward: 0.231 [0.158, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.377, 10.100], loss: 0.003414, mae: 0.060794, mean_q: -0.155039
 37871/100000: episode: 607, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 8.082, mean reward: 0.311 [0.215, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.182, 10.427], loss: 0.002650, mae: 0.051918, mean_q: -0.233643
 37881/100000: episode: 608, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 2.629, mean reward: 0.263 [0.172, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.161, 10.100], loss: 0.003357, mae: 0.060842, mean_q: -0.169262
 37895/100000: episode: 609, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 6.263, mean reward: 0.447 [0.303, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.387, 10.100], loss: 0.003078, mae: 0.055153, mean_q: -0.158041
 37911/100000: episode: 610, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 4.994, mean reward: 0.312 [0.203, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.220, 10.100], loss: 0.004389, mae: 0.066903, mean_q: -0.297524
 37931/100000: episode: 611, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 8.526, mean reward: 0.426 [0.385, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.474, 10.100], loss: 0.004065, mae: 0.067462, mean_q: -0.167419
 37957/100000: episode: 612, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 8.722, mean reward: 0.335 [0.239, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.593, 10.274], loss: 0.003417, mae: 0.059440, mean_q: -0.272412
 37973/100000: episode: 613, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 5.099, mean reward: 0.319 [0.186, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.875, 10.100], loss: 0.002812, mae: 0.055465, mean_q: -0.208819
 37983/100000: episode: 614, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 3.996, mean reward: 0.400 [0.271, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.731, 10.100], loss: 0.003009, mae: 0.056985, mean_q: -0.200131
 37996/100000: episode: 615, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 5.166, mean reward: 0.397 [0.317, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.293, 10.100], loss: 0.002837, mae: 0.056353, mean_q: -0.190921
 38022/100000: episode: 616, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 9.372, mean reward: 0.360 [0.292, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.035, 10.407], loss: 0.003120, mae: 0.054444, mean_q: -0.260590
 38036/100000: episode: 617, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 5.820, mean reward: 0.416 [0.360, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.391, 10.100], loss: 0.002634, mae: 0.052756, mean_q: -0.176104
 38054/100000: episode: 618, duration: 0.091s, episode steps: 18, steps per second: 199, episode reward: 5.923, mean reward: 0.329 [0.213, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.482], loss: 0.002624, mae: 0.052397, mean_q: -0.300651
 38064/100000: episode: 619, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 3.548, mean reward: 0.355 [0.301, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.176, 10.100], loss: 0.003044, mae: 0.056076, mean_q: -0.286620
 38074/100000: episode: 620, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 2.861, mean reward: 0.286 [0.257, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.774, 10.100], loss: 0.002785, mae: 0.053441, mean_q: -0.195983
 38094/100000: episode: 621, duration: 0.099s, episode steps: 20, steps per second: 203, episode reward: 6.654, mean reward: 0.333 [0.259, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.357, 10.100], loss: 0.003042, mae: 0.056614, mean_q: -0.251991
 38107/100000: episode: 622, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 5.650, mean reward: 0.435 [0.348, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.312, 10.100], loss: 0.003783, mae: 0.061817, mean_q: -0.167028
 38123/100000: episode: 623, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 4.511, mean reward: 0.282 [0.214, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.117, 10.100], loss: 0.003889, mae: 0.065933, mean_q: -0.154028
 38137/100000: episode: 624, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 5.354, mean reward: 0.382 [0.223, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.246, 10.100], loss: 0.003337, mae: 0.060106, mean_q: -0.130095
 38155/100000: episode: 625, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 7.752, mean reward: 0.431 [0.360, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.405, 10.545], loss: 0.002601, mae: 0.055085, mean_q: -0.153969
 38165/100000: episode: 626, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 3.700, mean reward: 0.370 [0.342, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.305, 10.100], loss: 0.003469, mae: 0.062214, mean_q: -0.216838
 38182/100000: episode: 627, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 6.429, mean reward: 0.378 [0.328, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.520], loss: 0.003106, mae: 0.060739, mean_q: -0.120343
 38195/100000: episode: 628, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 4.677, mean reward: 0.360 [0.279, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.325, 10.100], loss: 0.003209, mae: 0.058108, mean_q: -0.210260
 38205/100000: episode: 629, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 2.957, mean reward: 0.296 [0.194, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.262, 10.100], loss: 0.003126, mae: 0.058871, mean_q: -0.262508
 38219/100000: episode: 630, duration: 0.075s, episode steps: 14, steps per second: 188, episode reward: 4.485, mean reward: 0.320 [0.242, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.422, 10.100], loss: 0.003318, mae: 0.059992, mean_q: -0.067774
 38245/100000: episode: 631, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 9.638, mean reward: 0.371 [0.171, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.366], loss: 0.003027, mae: 0.057013, mean_q: -0.095674
 38262/100000: episode: 632, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 6.776, mean reward: 0.399 [0.311, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.585], loss: 0.003315, mae: 0.060191, mean_q: -0.213758
 38279/100000: episode: 633, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 5.812, mean reward: 0.342 [0.278, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.408], loss: 0.003091, mae: 0.057361, mean_q: -0.180814
 38293/100000: episode: 634, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 4.839, mean reward: 0.346 [0.247, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.242, 10.100], loss: 0.003866, mae: 0.065673, mean_q: -0.197647
 38303/100000: episode: 635, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 3.648, mean reward: 0.365 [0.311, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.823, 10.100], loss: 0.005696, mae: 0.065684, mean_q: -0.199680
 38329/100000: episode: 636, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 8.186, mean reward: 0.315 [0.165, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.738, 10.337], loss: 0.003167, mae: 0.059533, mean_q: -0.135901
 38340/100000: episode: 637, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 3.505, mean reward: 0.319 [0.185, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.336, 10.100], loss: 0.003439, mae: 0.060569, mean_q: -0.223888
 38357/100000: episode: 638, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 5.889, mean reward: 0.346 [0.273, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.664, 10.473], loss: 0.004778, mae: 0.066658, mean_q: -0.152392
 38367/100000: episode: 639, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 4.015, mean reward: 0.401 [0.290, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.452, 10.100], loss: 0.011796, mae: 0.086935, mean_q: -0.147858
 38393/100000: episode: 640, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 9.442, mean reward: 0.363 [0.204, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.242, 10.385], loss: 0.005732, mae: 0.072401, mean_q: -0.168831
 38403/100000: episode: 641, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 2.871, mean reward: 0.287 [0.215, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.175, 10.100], loss: 0.003994, mae: 0.068777, mean_q: -0.049499
 38417/100000: episode: 642, duration: 0.067s, episode steps: 14, steps per second: 208, episode reward: 4.528, mean reward: 0.323 [0.264, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.462, 10.100], loss: 0.003330, mae: 0.059376, mean_q: -0.161749
 38427/100000: episode: 643, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 3.379, mean reward: 0.338 [0.241, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.264, 10.100], loss: 0.003259, mae: 0.059926, mean_q: -0.122779
 38447/100000: episode: 644, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 6.041, mean reward: 0.302 [0.129, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.224, 10.100], loss: 0.003240, mae: 0.057116, mean_q: -0.183783
 38465/100000: episode: 645, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 6.714, mean reward: 0.373 [0.298, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.434], loss: 0.003617, mae: 0.063237, mean_q: -0.067010
 38479/100000: episode: 646, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 5.196, mean reward: 0.371 [0.314, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.299, 10.100], loss: 0.002836, mae: 0.056017, mean_q: -0.081455
 38492/100000: episode: 647, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 5.700, mean reward: 0.438 [0.380, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.500, 10.100], loss: 0.003122, mae: 0.055057, mean_q: -0.153990
 38512/100000: episode: 648, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 7.086, mean reward: 0.354 [0.209, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.205, 10.100], loss: 0.003164, mae: 0.057455, mean_q: -0.116793
 38526/100000: episode: 649, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 6.454, mean reward: 0.461 [0.385, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.744, 10.100], loss: 0.003525, mae: 0.062220, mean_q: 0.038820
 38543/100000: episode: 650, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 5.131, mean reward: 0.302 [0.213, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.387], loss: 0.003438, mae: 0.059232, mean_q: -0.141200
 38557/100000: episode: 651, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 4.944, mean reward: 0.353 [0.267, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.494, 10.100], loss: 0.002957, mae: 0.056340, mean_q: -0.061231
 38571/100000: episode: 652, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 5.064, mean reward: 0.362 [0.269, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.268, 10.100], loss: 0.002825, mae: 0.052807, mean_q: -0.088182
 38591/100000: episode: 653, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 5.738, mean reward: 0.287 [0.127, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.252, 10.100], loss: 0.003291, mae: 0.060188, mean_q: -0.079645
 38605/100000: episode: 654, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 5.487, mean reward: 0.392 [0.367, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.778, 10.100], loss: 0.002954, mae: 0.057887, mean_q: -0.084722
 38622/100000: episode: 655, duration: 0.082s, episode steps: 17, steps per second: 206, episode reward: 6.119, mean reward: 0.360 [0.246, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.411], loss: 0.003345, mae: 0.062813, mean_q: -0.074380
 38638/100000: episode: 656, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 6.421, mean reward: 0.401 [0.296, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.379, 10.100], loss: 0.003501, mae: 0.062244, mean_q: -0.021230
 38652/100000: episode: 657, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 5.675, mean reward: 0.405 [0.279, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.249, 10.100], loss: 0.002711, mae: 0.053851, mean_q: -0.109056
 38662/100000: episode: 658, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 2.605, mean reward: 0.260 [0.158, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.165, 10.100], loss: 0.003359, mae: 0.060895, mean_q: 0.004187
 38679/100000: episode: 659, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 5.481, mean reward: 0.322 [0.149, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.277], loss: 0.003975, mae: 0.065759, mean_q: -0.039932
 38690/100000: episode: 660, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 4.055, mean reward: 0.369 [0.291, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.470, 10.100], loss: 0.003850, mae: 0.064442, mean_q: -0.050302
 38707/100000: episode: 661, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 5.844, mean reward: 0.344 [0.273, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.368], loss: 0.003393, mae: 0.059789, mean_q: -0.054560
 38724/100000: episode: 662, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 6.990, mean reward: 0.411 [0.347, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.408, 10.470], loss: 0.003417, mae: 0.059866, mean_q: -0.095895
 38750/100000: episode: 663, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 7.814, mean reward: 0.301 [0.178, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.537, 10.317], loss: 0.003325, mae: 0.057415, mean_q: -0.116102
 38760/100000: episode: 664, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 4.179, mean reward: 0.418 [0.335, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.301, 10.100], loss: 0.003606, mae: 0.059402, mean_q: -0.073366
 38773/100000: episode: 665, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 5.583, mean reward: 0.429 [0.309, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.326, 10.100], loss: 0.003115, mae: 0.057699, mean_q: -0.063287
 38783/100000: episode: 666, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 3.417, mean reward: 0.342 [0.264, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.211, 10.100], loss: 0.003320, mae: 0.061242, mean_q: -0.067449
 38809/100000: episode: 667, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 5.666, mean reward: 0.218 [0.022, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.071, 10.100], loss: 0.003499, mae: 0.059135, mean_q: -0.022598
 38825/100000: episode: 668, duration: 0.096s, episode steps: 16, steps per second: 168, episode reward: 5.395, mean reward: 0.337 [0.272, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.389, 10.100], loss: 0.002948, mae: 0.055230, mean_q: -0.085917
 38851/100000: episode: 669, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 7.071, mean reward: 0.272 [0.137, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.466, 10.239], loss: 0.003033, mae: 0.057861, mean_q: -0.005324
 38877/100000: episode: 670, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 10.208, mean reward: 0.393 [0.246, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.808, 10.420], loss: 0.002822, mae: 0.055280, mean_q: -0.063061
[Info] 200-TH LEVEL FOUND: 0.8413983583450317, Considering 10/90 traces
 38888/100000: episode: 671, duration: 3.931s, episode steps: 11, steps per second: 3, episode reward: 4.229, mean reward: 0.384 [0.221, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.518, 10.100], loss: 0.003093, mae: 0.058420, mean_q: -0.007065
 38899/100000: episode: 672, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 4.339, mean reward: 0.394 [0.348, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.546], loss: 0.003171, mae: 0.056706, mean_q: 0.043970
 38915/100000: episode: 673, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 6.609, mean reward: 0.413 [0.277, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.404, 10.376], loss: 0.003701, mae: 0.061614, mean_q: 0.031129
 38931/100000: episode: 674, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 6.243, mean reward: 0.390 [0.292, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.560], loss: 0.004075, mae: 0.066274, mean_q: -0.014084
 38941/100000: episode: 675, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 4.971, mean reward: 0.497 [0.443, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.436, 10.100], loss: 0.003241, mae: 0.055706, mean_q: 0.029839
 38952/100000: episode: 676, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 5.322, mean reward: 0.484 [0.358, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.528], loss: 0.002705, mae: 0.053728, mean_q: -0.001729
 38962/100000: episode: 677, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 4.949, mean reward: 0.495 [0.456, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.429, 10.100], loss: 0.003843, mae: 0.062805, mean_q: 0.044321
 38976/100000: episode: 678, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 5.514, mean reward: 0.394 [0.339, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.074, 10.493], loss: 0.003353, mae: 0.060022, mean_q: 0.048453
 38987/100000: episode: 679, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 4.259, mean reward: 0.387 [0.251, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.431], loss: 0.002705, mae: 0.054442, mean_q: 0.004451
 38995/100000: episode: 680, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 3.303, mean reward: 0.413 [0.255, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.190, 10.100], loss: 0.002859, mae: 0.055276, mean_q: 0.009572
 39011/100000: episode: 681, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 6.632, mean reward: 0.415 [0.158, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.862, 10.301], loss: 0.002802, mae: 0.055206, mean_q: 0.029340
 39027/100000: episode: 682, duration: 0.080s, episode steps: 16, steps per second: 199, episode reward: 6.380, mean reward: 0.399 [0.325, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.147, 10.100], loss: 0.003411, mae: 0.057907, mean_q: -0.059027
 39038/100000: episode: 683, duration: 0.071s, episode steps: 11, steps per second: 156, episode reward: 5.467, mean reward: 0.497 [0.435, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-1.106, 10.454], loss: 0.002940, mae: 0.058362, mean_q: 0.011675
 39059/100000: episode: 684, duration: 0.103s, episode steps: 21, steps per second: 203, episode reward: 11.060, mean reward: 0.527 [0.444, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.583], loss: 0.003203, mae: 0.056380, mean_q: -0.066219
 39075/100000: episode: 685, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 5.914, mean reward: 0.370 [0.308, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.254, 10.100], loss: 0.003001, mae: 0.055425, mean_q: -0.047632
 39091/100000: episode: 686, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 5.730, mean reward: 0.358 [0.303, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.232, 10.482], loss: 0.003563, mae: 0.061807, mean_q: 0.128003
 39099/100000: episode: 687, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 3.461, mean reward: 0.433 [0.370, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.342, 10.100], loss: 0.003840, mae: 0.064745, mean_q: 0.094921
 39120/100000: episode: 688, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 8.951, mean reward: 0.426 [0.319, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.051, 10.586], loss: 0.003837, mae: 0.064980, mean_q: 0.066008
 39130/100000: episode: 689, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 5.619, mean reward: 0.562 [0.464, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.011, 10.100], loss: 0.004513, mae: 0.071421, mean_q: 0.063342
 39146/100000: episode: 690, duration: 0.083s, episode steps: 16, steps per second: 194, episode reward: 6.967, mean reward: 0.435 [0.355, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.549], loss: 0.010620, mae: 0.089969, mean_q: 0.003684
 39167/100000: episode: 691, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 7.409, mean reward: 0.353 [0.199, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.240, 10.359], loss: 0.004177, mae: 0.067345, mean_q: 0.003241
 39188/100000: episode: 692, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 9.857, mean reward: 0.469 [0.377, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.240, 10.500], loss: 0.004161, mae: 0.070272, mean_q: 0.060318
 39195/100000: episode: 693, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 2.977, mean reward: 0.425 [0.368, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.512, 10.100], loss: 0.003613, mae: 0.067142, mean_q: 0.094771
 39209/100000: episode: 694, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 6.540, mean reward: 0.467 [0.357, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.561], loss: 0.003963, mae: 0.067492, mean_q: 0.043533
 39216/100000: episode: 695, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 2.496, mean reward: 0.357 [0.323, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.430, 10.100], loss: 0.003798, mae: 0.061971, mean_q: 0.081973
 39232/100000: episode: 696, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 4.268, mean reward: 0.267 [0.120, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.212, 10.100], loss: 0.003650, mae: 0.063057, mean_q: 0.085746
 39243/100000: episode: 697, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 4.592, mean reward: 0.417 [0.386, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.202, 10.501], loss: 0.004066, mae: 0.063787, mean_q: 0.060396
 39253/100000: episode: 698, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 4.442, mean reward: 0.444 [0.339, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.449, 10.100], loss: 0.003541, mae: 0.065154, mean_q: 0.068682
 39261/100000: episode: 699, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 3.545, mean reward: 0.443 [0.384, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.325, 10.100], loss: 0.005359, mae: 0.070380, mean_q: 0.116597
 39268/100000: episode: 700, duration: 0.051s, episode steps: 7, steps per second: 137, episode reward: 2.617, mean reward: 0.374 [0.324, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.485, 10.100], loss: 0.009791, mae: 0.087158, mean_q: 0.082985
 39279/100000: episode: 701, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 4.456, mean reward: 0.405 [0.301, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.156, 10.387], loss: 0.008510, mae: 0.089629, mean_q: -0.029435
 39295/100000: episode: 702, duration: 0.077s, episode steps: 16, steps per second: 209, episode reward: 6.683, mean reward: 0.418 [0.330, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-1.092, 10.442], loss: 0.009495, mae: 0.089991, mean_q: 0.043376
 39311/100000: episode: 703, duration: 0.077s, episode steps: 16, steps per second: 209, episode reward: 6.456, mean reward: 0.404 [0.321, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.436, 10.100], loss: 0.005123, mae: 0.075388, mean_q: 0.092085
 39327/100000: episode: 704, duration: 0.080s, episode steps: 16, steps per second: 199, episode reward: 5.274, mean reward: 0.330 [0.245, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-1.145, 10.371], loss: 0.004468, mae: 0.070191, mean_q: 0.097334
 39335/100000: episode: 705, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 4.081, mean reward: 0.510 [0.424, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.806, 10.100], loss: 0.004679, mae: 0.071980, mean_q: 0.168488
 39346/100000: episode: 706, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 4.426, mean reward: 0.402 [0.242, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.531], loss: 0.005164, mae: 0.076762, mean_q: 0.054670
 39362/100000: episode: 707, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 5.078, mean reward: 0.317 [0.142, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.506, 10.241], loss: 0.004398, mae: 0.070851, mean_q: 0.054295
 39378/100000: episode: 708, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 7.068, mean reward: 0.442 [0.351, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.052, 10.521], loss: 0.005723, mae: 0.072879, mean_q: 0.115584
 39386/100000: episode: 709, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 3.712, mean reward: 0.464 [0.384, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-1.608, 10.100], loss: 0.007719, mae: 0.082053, mean_q: 0.072585
 39402/100000: episode: 710, duration: 0.085s, episode steps: 16, steps per second: 187, episode reward: 6.649, mean reward: 0.416 [0.359, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.286, 10.100], loss: 0.011103, mae: 0.085843, mean_q: 0.083580
 39418/100000: episode: 711, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 7.770, mean reward: 0.486 [0.416, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.352, 10.100], loss: 0.004653, mae: 0.069925, mean_q: 0.088718
 39432/100000: episode: 712, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 6.092, mean reward: 0.435 [0.309, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.126, 10.548], loss: 0.005509, mae: 0.072238, mean_q: 0.029568
 39453/100000: episode: 713, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 8.405, mean reward: 0.400 [0.291, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.467, 10.443], loss: 0.004182, mae: 0.069692, mean_q: 0.139165
 39461/100000: episode: 714, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 3.938, mean reward: 0.492 [0.469, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.300, 10.100], loss: 0.003300, mae: 0.060674, mean_q: 0.023301
 39477/100000: episode: 715, duration: 0.076s, episode steps: 16, steps per second: 209, episode reward: 5.119, mean reward: 0.320 [0.231, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.267, 10.455], loss: 0.003724, mae: 0.061580, mean_q: 0.143037
 39493/100000: episode: 716, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 7.035, mean reward: 0.440 [0.335, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-1.498, 10.533], loss: 0.003244, mae: 0.060086, mean_q: 0.115366
 39509/100000: episode: 717, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 6.301, mean reward: 0.394 [0.257, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.188, 10.100], loss: 0.003211, mae: 0.057921, mean_q: 0.083788
 39525/100000: episode: 718, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 8.013, mean reward: 0.501 [0.370, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.657], loss: 0.003272, mae: 0.059304, mean_q: 0.135939
 39532/100000: episode: 719, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 2.530, mean reward: 0.361 [0.332, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.428, 10.100], loss: 0.002840, mae: 0.057045, mean_q: 0.146651
 39548/100000: episode: 720, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 7.566, mean reward: 0.473 [0.345, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.464], loss: 0.003643, mae: 0.059854, mean_q: 0.035964
 39564/100000: episode: 721, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 6.927, mean reward: 0.433 [0.339, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.856, 10.100], loss: 0.003796, mae: 0.063731, mean_q: 0.118570
 39580/100000: episode: 722, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 5.893, mean reward: 0.368 [0.286, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.268, 10.100], loss: 0.003409, mae: 0.059121, mean_q: 0.046255
 39596/100000: episode: 723, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 6.447, mean reward: 0.403 [0.295, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-1.231, 10.388], loss: 0.003848, mae: 0.063478, mean_q: 0.203760
 39612/100000: episode: 724, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 7.416, mean reward: 0.464 [0.380, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.360, 10.100], loss: 0.003125, mae: 0.058809, mean_q: 0.160033
 39633/100000: episode: 725, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 9.037, mean reward: 0.430 [0.350, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.593, 10.486], loss: 0.003459, mae: 0.059769, mean_q: 0.067672
 39641/100000: episode: 726, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 3.406, mean reward: 0.426 [0.326, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.474, 10.100], loss: 0.003067, mae: 0.060175, mean_q: 0.138547
 39651/100000: episode: 727, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 4.777, mean reward: 0.478 [0.414, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.442, 10.100], loss: 0.004158, mae: 0.066635, mean_q: 0.249466
 39661/100000: episode: 728, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 4.658, mean reward: 0.466 [0.391, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.999, 10.100], loss: 0.003814, mae: 0.063711, mean_q: 0.183904
 39671/100000: episode: 729, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 5.014, mean reward: 0.501 [0.423, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.493, 10.100], loss: 0.003563, mae: 0.063501, mean_q: 0.154210
 39678/100000: episode: 730, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 3.181, mean reward: 0.454 [0.368, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.620, 10.100], loss: 0.004508, mae: 0.069353, mean_q: 0.194043
 39694/100000: episode: 731, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 6.326, mean reward: 0.395 [0.344, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.386, 10.100], loss: 0.003597, mae: 0.059860, mean_q: 0.067063
 39715/100000: episode: 732, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 12.439, mean reward: 0.592 [0.501, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.626], loss: 0.003525, mae: 0.061826, mean_q: 0.194030
 39731/100000: episode: 733, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 5.509, mean reward: 0.344 [0.230, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.266, 10.100], loss: 0.003882, mae: 0.066308, mean_q: 0.171165
 39747/100000: episode: 734, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 6.562, mean reward: 0.410 [0.295, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-1.561, 10.478], loss: 0.003039, mae: 0.057603, mean_q: 0.192108
 39763/100000: episode: 735, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 6.538, mean reward: 0.409 [0.298, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.356, 10.100], loss: 0.003520, mae: 0.062484, mean_q: 0.194887
 39779/100000: episode: 736, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 4.690, mean reward: 0.293 [0.159, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.339], loss: 0.003539, mae: 0.060763, mean_q: 0.173559
 39789/100000: episode: 737, duration: 0.048s, episode steps: 10, steps per second: 206, episode reward: 4.775, mean reward: 0.477 [0.414, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.363, 10.100], loss: 0.003837, mae: 0.063321, mean_q: 0.116018
 39800/100000: episode: 738, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 3.814, mean reward: 0.347 [0.195, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.348], loss: 0.003360, mae: 0.061408, mean_q: 0.231667
 39811/100000: episode: 739, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 4.649, mean reward: 0.423 [0.309, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.473, 10.399], loss: 0.002892, mae: 0.054372, mean_q: 0.057312
 39818/100000: episode: 740, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 3.069, mean reward: 0.438 [0.397, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.676, 10.100], loss: 0.003231, mae: 0.060775, mean_q: 0.200106
 39834/100000: episode: 741, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 6.521, mean reward: 0.408 [0.315, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.370, 10.100], loss: 0.003879, mae: 0.064352, mean_q: 0.252261
 39855/100000: episode: 742, duration: 0.111s, episode steps: 21, steps per second: 188, episode reward: 7.418, mean reward: 0.353 [0.183, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.392, 10.333], loss: 0.003191, mae: 0.059373, mean_q: 0.213174
 39871/100000: episode: 743, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 7.572, mean reward: 0.473 [0.340, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.356, 10.453], loss: 0.002998, mae: 0.058202, mean_q: 0.262365
 39892/100000: episode: 744, duration: 0.103s, episode steps: 21, steps per second: 205, episode reward: 9.650, mean reward: 0.460 [0.373, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.038, 10.571], loss: 0.003707, mae: 0.061701, mean_q: 0.219549
 39908/100000: episode: 745, duration: 0.085s, episode steps: 16, steps per second: 187, episode reward: 5.539, mean reward: 0.346 [0.249, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.787, 10.373], loss: 0.003631, mae: 0.061120, mean_q: 0.250796
 39915/100000: episode: 746, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 2.601, mean reward: 0.372 [0.336, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.463, 10.100], loss: 0.004428, mae: 0.073046, mean_q: 0.298193
 39931/100000: episode: 747, duration: 0.083s, episode steps: 16, steps per second: 194, episode reward: 6.863, mean reward: 0.429 [0.362, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.496], loss: 0.004418, mae: 0.069333, mean_q: 0.199406
 39938/100000: episode: 748, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 2.532, mean reward: 0.362 [0.247, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.663, 10.100], loss: 0.004428, mae: 0.069696, mean_q: 0.187449
 39949/100000: episode: 749, duration: 0.068s, episode steps: 11, steps per second: 163, episode reward: 5.123, mean reward: 0.466 [0.396, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.634], loss: 0.004088, mae: 0.066877, mean_q: 0.199091
 39963/100000: episode: 750, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 7.116, mean reward: 0.508 [0.408, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.358, 10.657], loss: 0.003484, mae: 0.061153, mean_q: 0.185155
 39970/100000: episode: 751, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 2.523, mean reward: 0.360 [0.283, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.465, 10.100], loss: 0.003670, mae: 0.063826, mean_q: 0.204373
 39986/100000: episode: 752, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 6.336, mean reward: 0.396 [0.313, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.551, 10.452], loss: 0.004088, mae: 0.067675, mean_q: 0.279264
 39993/100000: episode: 753, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 2.444, mean reward: 0.349 [0.276, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.489, 10.100], loss: 0.003213, mae: 0.060343, mean_q: 0.311401
 40007/100000: episode: 754, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 7.095, mean reward: 0.507 [0.413, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.674], loss: 0.003168, mae: 0.061516, mean_q: 0.310795
 40021/100000: episode: 755, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 5.755, mean reward: 0.411 [0.372, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-1.022, 10.521], loss: 0.003530, mae: 0.062300, mean_q: 0.245646
 40037/100000: episode: 756, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 6.366, mean reward: 0.398 [0.321, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.617, 10.475], loss: 0.003592, mae: 0.061198, mean_q: 0.187300
 40044/100000: episode: 757, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 2.909, mean reward: 0.416 [0.374, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.667, 10.100], loss: 0.003616, mae: 0.063732, mean_q: 0.166897
 40060/100000: episode: 758, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 6.383, mean reward: 0.399 [0.300, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.488], loss: 0.003544, mae: 0.059591, mean_q: 0.187146
 40076/100000: episode: 759, duration: 0.089s, episode steps: 16, steps per second: 181, episode reward: 7.092, mean reward: 0.443 [0.349, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.543], loss: 0.003286, mae: 0.059594, mean_q: 0.239560
 40092/100000: episode: 760, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 5.665, mean reward: 0.354 [0.307, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.341, 10.456], loss: 0.003797, mae: 0.063944, mean_q: 0.239974
[Info] 300-TH LEVEL FOUND: 0.9955297708511353, Considering 10/90 traces
 40103/100000: episode: 761, duration: 3.904s, episode steps: 11, steps per second: 3, episode reward: 5.382, mean reward: 0.489 [0.398, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.322, 10.518], loss: 0.003795, mae: 0.065069, mean_q: 0.265606
 40112/100000: episode: 762, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 3.657, mean reward: 0.406 [0.349, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.359, 10.405], loss: 0.003623, mae: 0.064306, mean_q: 0.249295
 40123/100000: episode: 763, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 5.544, mean reward: 0.504 [0.458, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.531, 10.491], loss: 0.004045, mae: 0.067283, mean_q: 0.290272
 40132/100000: episode: 764, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 5.154, mean reward: 0.573 [0.525, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.660], loss: 0.003566, mae: 0.063841, mean_q: 0.232073
 40143/100000: episode: 765, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 5.500, mean reward: 0.500 [0.455, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.629], loss: 0.004496, mae: 0.070023, mean_q: 0.215655
 40155/100000: episode: 766, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 5.678, mean reward: 0.473 [0.447, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.565], loss: 0.004153, mae: 0.068235, mean_q: 0.223806
 40165/100000: episode: 767, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 4.389, mean reward: 0.439 [0.334, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.696, 10.426], loss: 0.003617, mae: 0.065137, mean_q: 0.286667
 40182/100000: episode: 768, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 9.031, mean reward: 0.531 [0.446, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.595, 10.616], loss: 0.004229, mae: 0.067646, mean_q: 0.284965
 40193/100000: episode: 769, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 5.154, mean reward: 0.469 [0.390, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.163, 10.526], loss: 0.003591, mae: 0.061941, mean_q: 0.319106
 40204/100000: episode: 770, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 6.155, mean reward: 0.560 [0.516, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.637], loss: 0.004508, mae: 0.071484, mean_q: 0.300800
 40216/100000: episode: 771, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 5.675, mean reward: 0.473 [0.366, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.594], loss: 0.003534, mae: 0.061739, mean_q: 0.313676
 40226/100000: episode: 772, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 5.038, mean reward: 0.504 [0.403, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.617], loss: 0.003762, mae: 0.063548, mean_q: 0.297868
 40239/100000: episode: 773, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 6.778, mean reward: 0.521 [0.469, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.561], loss: 0.003309, mae: 0.061525, mean_q: 0.251035
 40249/100000: episode: 774, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 3.906, mean reward: 0.391 [0.234, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.481], loss: 0.003193, mae: 0.059559, mean_q: 0.313075
 40262/100000: episode: 775, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 6.104, mean reward: 0.470 [0.415, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.413, 10.599], loss: 0.004260, mae: 0.070185, mean_q: 0.313558
 40279/100000: episode: 776, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 7.063, mean reward: 0.415 [0.295, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.062, 10.471], loss: 0.003792, mae: 0.062589, mean_q: 0.339719
 40296/100000: episode: 777, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 9.099, mean reward: 0.535 [0.472, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.406, 10.671], loss: 0.003382, mae: 0.061071, mean_q: 0.283367
 40311/100000: episode: 778, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 6.930, mean reward: 0.462 [0.373, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.319, 10.551], loss: 0.003284, mae: 0.061048, mean_q: 0.335427
 40323/100000: episode: 779, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 5.815, mean reward: 0.485 [0.417, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.301, 10.501], loss: 0.003912, mae: 0.065117, mean_q: 0.354887
 40335/100000: episode: 780, duration: 0.075s, episode steps: 12, steps per second: 159, episode reward: 6.309, mean reward: 0.526 [0.422, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.527], loss: 0.003654, mae: 0.062993, mean_q: 0.321803
 40346/100000: episode: 781, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 5.193, mean reward: 0.472 [0.309, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.492], loss: 0.003413, mae: 0.061401, mean_q: 0.348088
 40358/100000: episode: 782, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 6.419, mean reward: 0.535 [0.498, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.524], loss: 0.004089, mae: 0.066022, mean_q: 0.344543
 40370/100000: episode: 783, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 5.672, mean reward: 0.473 [0.398, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.447, 10.532], loss: 0.003623, mae: 0.063700, mean_q: 0.343811
 40381/100000: episode: 784, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 4.721, mean reward: 0.429 [0.332, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.446], loss: 0.004263, mae: 0.068105, mean_q: 0.378515
 40396/100000: episode: 785, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 6.854, mean reward: 0.457 [0.381, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.560], loss: 0.003407, mae: 0.060815, mean_q: 0.281845
 40413/100000: episode: 786, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 8.907, mean reward: 0.524 [0.471, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-1.453, 10.627], loss: 0.003210, mae: 0.058644, mean_q: 0.285876
[Info] FALSIFICATION!
 40419/100000: episode: 787, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 12.786, mean reward: 2.131 [0.539, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.067, 10.745], loss: 0.002699, mae: 0.054538, mean_q: 0.293023
 40519/100000: episode: 788, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -15.656, mean reward: -0.157 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.646, 10.216], loss: 0.003736, mae: 0.064502, mean_q: 0.361792
 40619/100000: episode: 789, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.901, mean reward: -0.189 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.671, 10.098], loss: 0.003398, mae: 0.061030, mean_q: 0.324456
 40719/100000: episode: 790, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.350, mean reward: -0.173 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.210, 10.138], loss: 0.003478, mae: 0.062238, mean_q: 0.338584
 40819/100000: episode: 791, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.938, mean reward: -0.149 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.572, 10.255], loss: 0.003589, mae: 0.062863, mean_q: 0.310931
 40919/100000: episode: 792, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.185, mean reward: -0.182 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.931, 10.233], loss: 0.003570, mae: 0.062127, mean_q: 0.322445
 41019/100000: episode: 793, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.974, mean reward: -0.150 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-0.705, 10.098], loss: 0.018064, mae: 0.077203, mean_q: 0.347125
 41119/100000: episode: 794, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -18.243, mean reward: -0.182 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.594, 10.098], loss: 0.017288, mae: 0.072598, mean_q: 0.316355
 41219/100000: episode: 795, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.200, mean reward: -0.142 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.669, 10.098], loss: 0.003436, mae: 0.062223, mean_q: 0.348294
 41319/100000: episode: 796, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -16.388, mean reward: -0.164 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.758, 10.161], loss: 0.003630, mae: 0.063033, mean_q: 0.314988
 41419/100000: episode: 797, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -13.888, mean reward: -0.139 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.806, 10.411], loss: 0.003493, mae: 0.062037, mean_q: 0.319780
 41519/100000: episode: 798, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -11.470, mean reward: -0.115 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.534, 10.098], loss: 0.003528, mae: 0.062126, mean_q: 0.329861
 41619/100000: episode: 799, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.128, mean reward: -0.171 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.229, 10.098], loss: 0.031666, mae: 0.086002, mean_q: 0.343815
 41719/100000: episode: 800, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.781, mean reward: -0.168 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.237, 10.098], loss: 0.016705, mae: 0.069725, mean_q: 0.353363
 41819/100000: episode: 801, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.838, mean reward: -0.158 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.357, 10.188], loss: 0.003493, mae: 0.062678, mean_q: 0.354878
 41919/100000: episode: 802, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.951, mean reward: -0.180 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.690, 10.103], loss: 0.016309, mae: 0.066025, mean_q: 0.353022
 42019/100000: episode: 803, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.426, mean reward: -0.194 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.621, 10.101], loss: 0.030468, mae: 0.081053, mean_q: 0.368333
 42119/100000: episode: 804, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -13.087, mean reward: -0.131 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.625, 10.331], loss: 0.030304, mae: 0.080636, mean_q: 0.351553
 42219/100000: episode: 805, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.567, mean reward: -0.186 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.087, 10.098], loss: 0.003506, mae: 0.062457, mean_q: 0.355630
 42319/100000: episode: 806, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -19.088, mean reward: -0.191 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.786, 10.098], loss: 0.017980, mae: 0.080338, mean_q: 0.363838
 42419/100000: episode: 807, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -15.613, mean reward: -0.156 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.228, 10.098], loss: 0.016330, mae: 0.066498, mean_q: 0.329125
 42519/100000: episode: 808, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.271, mean reward: -0.153 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.859, 10.098], loss: 0.016450, mae: 0.066628, mean_q: 0.312877
 42619/100000: episode: 809, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.371, mean reward: -0.194 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.617, 10.122], loss: 0.003251, mae: 0.059899, mean_q: 0.272944
 42719/100000: episode: 810, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.510, mean reward: -0.175 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.768, 10.171], loss: 0.002874, mae: 0.057166, mean_q: 0.243442
 42819/100000: episode: 811, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.572, mean reward: -0.196 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.964, 10.098], loss: 0.016582, mae: 0.067847, mean_q: 0.220358
 42919/100000: episode: 812, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.344, mean reward: -0.183 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.267, 10.128], loss: 0.005220, mae: 0.069088, mean_q: 0.185155
 43019/100000: episode: 813, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.738, mean reward: -0.167 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.728, 10.106], loss: 0.004278, mae: 0.065001, mean_q: 0.175601
 43119/100000: episode: 814, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.977, mean reward: -0.190 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.578, 10.207], loss: 0.003716, mae: 0.063044, mean_q: 0.163701
 43219/100000: episode: 815, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -18.841, mean reward: -0.188 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.928, 10.118], loss: 0.029238, mae: 0.070793, mean_q: 0.150192
 43319/100000: episode: 816, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.225, mean reward: -0.182 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.472, 10.183], loss: 0.003195, mae: 0.058845, mean_q: 0.138552
 43419/100000: episode: 817, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.638, mean reward: -0.196 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.330, 10.142], loss: 0.017034, mae: 0.069936, mean_q: 0.102465
 43519/100000: episode: 818, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -18.197, mean reward: -0.182 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.096, 10.101], loss: 0.003151, mae: 0.057941, mean_q: 0.107445
 43619/100000: episode: 819, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.488, mean reward: -0.195 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.392, 10.145], loss: 0.016934, mae: 0.069657, mean_q: 0.081601
 43719/100000: episode: 820, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -15.159, mean reward: -0.152 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.515, 10.464], loss: 0.002876, mae: 0.055318, mean_q: 0.011596
 43819/100000: episode: 821, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.901, mean reward: -0.199 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.876, 10.098], loss: 0.003025, mae: 0.056748, mean_q: 0.059045
 43919/100000: episode: 822, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -13.531, mean reward: -0.135 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.375, 10.098], loss: 0.028602, mae: 0.064448, mean_q: -0.005658
 44019/100000: episode: 823, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -19.358, mean reward: -0.194 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.347, 10.098], loss: 0.003278, mae: 0.058520, mean_q: -0.014760
 44119/100000: episode: 824, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -12.104, mean reward: -0.121 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.262, 10.327], loss: 0.002746, mae: 0.053452, mean_q: -0.050134
 44219/100000: episode: 825, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.780, mean reward: -0.168 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.856, 10.098], loss: 0.015717, mae: 0.059738, mean_q: -0.036192
 44319/100000: episode: 826, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -19.367, mean reward: -0.194 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.945, 10.282], loss: 0.002579, mae: 0.052528, mean_q: -0.094523
 44419/100000: episode: 827, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.685, mean reward: -0.177 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.511, 10.098], loss: 0.042725, mae: 0.073963, mean_q: -0.079032
 44519/100000: episode: 828, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.956, mean reward: -0.180 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.399, 10.310], loss: 0.003468, mae: 0.059543, mean_q: -0.116995
 44619/100000: episode: 829, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -20.104, mean reward: -0.201 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.160, 10.130], loss: 0.002719, mae: 0.052986, mean_q: -0.116951
 44719/100000: episode: 830, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -14.566, mean reward: -0.146 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.336, 10.098], loss: 0.029212, mae: 0.068465, mean_q: -0.177078
 44819/100000: episode: 831, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.779, mean reward: -0.178 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.061, 10.154], loss: 0.008220, mae: 0.079490, mean_q: -0.187945
 44919/100000: episode: 832, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.577, mean reward: -0.176 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.598, 10.215], loss: 0.003411, mae: 0.057757, mean_q: -0.238471
 45019/100000: episode: 833, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.904, mean reward: -0.179 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.863, 10.177], loss: 0.016675, mae: 0.067191, mean_q: -0.275145
 45119/100000: episode: 834, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -12.650, mean reward: -0.126 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.831, 10.098], loss: 0.015546, mae: 0.057604, mean_q: -0.272699
 45219/100000: episode: 835, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -18.568, mean reward: -0.186 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.197, 10.098], loss: 0.029664, mae: 0.067955, mean_q: -0.253279
 45319/100000: episode: 836, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -20.491, mean reward: -0.205 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.597, 10.098], loss: 0.016219, mae: 0.062765, mean_q: -0.308015
 45419/100000: episode: 837, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.777, mean reward: -0.168 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.606, 10.287], loss: 0.028413, mae: 0.066880, mean_q: -0.317782
 45519/100000: episode: 838, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.907, mean reward: -0.169 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.797, 10.271], loss: 0.003002, mae: 0.054860, mean_q: -0.321634
 45619/100000: episode: 839, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.581, mean reward: -0.186 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.329, 10.325], loss: 0.002743, mae: 0.052713, mean_q: -0.323490
 45719/100000: episode: 840, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -15.886, mean reward: -0.159 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.787, 10.332], loss: 0.004144, mae: 0.062044, mean_q: -0.311107
 45819/100000: episode: 841, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -19.914, mean reward: -0.199 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.736, 10.116], loss: 0.002837, mae: 0.052777, mean_q: -0.353001
 45919/100000: episode: 842, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -12.744, mean reward: -0.127 [-1.000, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.363, 10.176], loss: 0.002920, mae: 0.055134, mean_q: -0.320013
 46019/100000: episode: 843, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -11.366, mean reward: -0.114 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.535, 10.389], loss: 0.002740, mae: 0.052687, mean_q: -0.302104
 46119/100000: episode: 844, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.522, mean reward: -0.175 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.359, 10.098], loss: 0.002767, mae: 0.052694, mean_q: -0.355389
 46219/100000: episode: 845, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.666, mean reward: -0.167 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.007, 10.358], loss: 0.002717, mae: 0.052533, mean_q: -0.308724
 46319/100000: episode: 846, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.770, mean reward: -0.178 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.106, 10.111], loss: 0.002624, mae: 0.050896, mean_q: -0.321008
 46419/100000: episode: 847, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.890, mean reward: -0.169 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.806, 10.098], loss: 0.002656, mae: 0.051650, mean_q: -0.303592
 46519/100000: episode: 848, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.714, mean reward: -0.157 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.989, 10.170], loss: 0.002740, mae: 0.051677, mean_q: -0.330008
 46619/100000: episode: 849, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -17.962, mean reward: -0.180 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.077, 10.098], loss: 0.002681, mae: 0.051300, mean_q: -0.308068
 46719/100000: episode: 850, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.206, mean reward: -0.162 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.273, 10.153], loss: 0.002688, mae: 0.051107, mean_q: -0.327100
 46819/100000: episode: 851, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -20.935, mean reward: -0.209 [-1.000, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.001, 10.098], loss: 0.002463, mae: 0.049334, mean_q: -0.323809
 46919/100000: episode: 852, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.818, mean reward: -0.168 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.522, 10.178], loss: 0.002709, mae: 0.052930, mean_q: -0.302559
 47019/100000: episode: 853, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.427, mean reward: -0.174 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.387, 10.150], loss: 0.002396, mae: 0.049126, mean_q: -0.330675
 47119/100000: episode: 854, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.902, mean reward: -0.179 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.111, 10.098], loss: 0.002467, mae: 0.050625, mean_q: -0.335934
 47219/100000: episode: 855, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -12.929, mean reward: -0.129 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.441, 10.220], loss: 0.002659, mae: 0.052259, mean_q: -0.324619
 47319/100000: episode: 856, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.722, mean reward: -0.187 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.606, 10.150], loss: 0.002713, mae: 0.051309, mean_q: -0.335207
 47419/100000: episode: 857, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.645, mean reward: -0.186 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.938, 10.098], loss: 0.002648, mae: 0.051323, mean_q: -0.322387
 47519/100000: episode: 858, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -18.773, mean reward: -0.188 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.980, 10.101], loss: 0.002666, mae: 0.052188, mean_q: -0.325380
 47619/100000: episode: 859, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.143, mean reward: -0.191 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.680, 10.100], loss: 0.002830, mae: 0.054764, mean_q: -0.315607
 47719/100000: episode: 860, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.236, mean reward: -0.182 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.880, 10.120], loss: 0.002661, mae: 0.050833, mean_q: -0.296276
 47819/100000: episode: 861, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -12.879, mean reward: -0.129 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.840, 10.389], loss: 0.002713, mae: 0.051112, mean_q: -0.350555
 47919/100000: episode: 862, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.044, mean reward: -0.160 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.130, 10.338], loss: 0.002668, mae: 0.050269, mean_q: -0.351113
 48019/100000: episode: 863, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -16.806, mean reward: -0.168 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.749, 10.376], loss: 0.002685, mae: 0.051047, mean_q: -0.313891
 48119/100000: episode: 864, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.638, mean reward: -0.186 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.996, 10.098], loss: 0.002454, mae: 0.048901, mean_q: -0.329766
 48219/100000: episode: 865, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.432, mean reward: -0.174 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.795, 10.157], loss: 0.002664, mae: 0.051303, mean_q: -0.356206
 48319/100000: episode: 866, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -8.268, mean reward: -0.083 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.193, 10.098], loss: 0.002645, mae: 0.051365, mean_q: -0.326645
 48419/100000: episode: 867, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -12.591, mean reward: -0.126 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.873, 10.491], loss: 0.002512, mae: 0.050266, mean_q: -0.299590
 48519/100000: episode: 868, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -15.708, mean reward: -0.157 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.249, 10.240], loss: 0.002634, mae: 0.051977, mean_q: -0.286599
 48619/100000: episode: 869, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -13.064, mean reward: -0.131 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.537, 10.098], loss: 0.002644, mae: 0.050283, mean_q: -0.319710
 48719/100000: episode: 870, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -15.470, mean reward: -0.155 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.154, 10.116], loss: 0.002487, mae: 0.049161, mean_q: -0.330044
 48819/100000: episode: 871, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -16.917, mean reward: -0.169 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.489, 10.398], loss: 0.002622, mae: 0.051183, mean_q: -0.300733
 48919/100000: episode: 872, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -7.978, mean reward: -0.080 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.474, 10.419], loss: 0.002761, mae: 0.052779, mean_q: -0.317244
 49019/100000: episode: 873, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -21.085, mean reward: -0.211 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.660, 10.098], loss: 0.002672, mae: 0.051519, mean_q: -0.336291
 49119/100000: episode: 874, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.476, mean reward: -0.185 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.949, 10.199], loss: 0.002644, mae: 0.051010, mean_q: -0.313375
 49219/100000: episode: 875, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -20.092, mean reward: -0.201 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.422, 10.105], loss: 0.002638, mae: 0.051112, mean_q: -0.291477
 49319/100000: episode: 876, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -13.105, mean reward: -0.131 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.732, 10.161], loss: 0.002628, mae: 0.051803, mean_q: -0.303693
 49419/100000: episode: 877, duration: 0.460s, episode steps: 100, steps per second: 217, episode reward: -19.340, mean reward: -0.193 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.080, 10.098], loss: 0.002553, mae: 0.050820, mean_q: -0.326486
 49519/100000: episode: 878, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.969, mean reward: -0.160 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.146, 10.098], loss: 0.002982, mae: 0.053370, mean_q: -0.315993
 49619/100000: episode: 879, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.138, mean reward: -0.171 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.467, 10.122], loss: 0.002859, mae: 0.054745, mean_q: -0.303902
 49719/100000: episode: 880, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.229, mean reward: -0.162 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.747, 10.282], loss: 0.002493, mae: 0.050074, mean_q: -0.324073
 49819/100000: episode: 881, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -14.807, mean reward: -0.148 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.247, 10.098], loss: 0.002936, mae: 0.052663, mean_q: -0.304613
 49919/100000: episode: 882, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.538, mean reward: -0.185 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.680, 10.232], loss: 0.006102, mae: 0.067777, mean_q: -0.277777
 50019/100000: episode: 883, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -12.167, mean reward: -0.122 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.749, 10.098], loss: 0.003234, mae: 0.056789, mean_q: -0.312548
 50119/100000: episode: 884, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -16.483, mean reward: -0.165 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.858, 10.199], loss: 0.003767, mae: 0.059410, mean_q: -0.320279
 50219/100000: episode: 885, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.775, mean reward: -0.178 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.499, 10.098], loss: 0.004108, mae: 0.059245, mean_q: -0.301484
 50319/100000: episode: 886, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -15.837, mean reward: -0.158 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.638, 10.231], loss: 0.002475, mae: 0.049648, mean_q: -0.297592
[Info] 100-TH LEVEL FOUND: 0.6598162651062012, Considering 10/90 traces
 50419/100000: episode: 887, duration: 4.315s, episode steps: 100, steps per second: 23, episode reward: -13.803, mean reward: -0.138 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.778, 10.098], loss: 0.002481, mae: 0.049050, mean_q: -0.300596
 50447/100000: episode: 888, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 8.579, mean reward: 0.306 [0.231, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.720, 10.100], loss: 0.002568, mae: 0.049017, mean_q: -0.283015
 50475/100000: episode: 889, duration: 0.139s, episode steps: 28, steps per second: 202, episode reward: 8.935, mean reward: 0.319 [0.113, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.979, 10.100], loss: 0.002008, mae: 0.044368, mean_q: -0.299350
 50496/100000: episode: 890, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 4.487, mean reward: 0.214 [0.031, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.159, 10.167], loss: 0.002750, mae: 0.049303, mean_q: -0.302267
 50509/100000: episode: 891, duration: 0.074s, episode steps: 13, steps per second: 177, episode reward: 5.170, mean reward: 0.398 [0.290, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.794, 10.100], loss: 0.002536, mae: 0.050819, mean_q: -0.323108
 50521/100000: episode: 892, duration: 0.064s, episode steps: 12, steps per second: 189, episode reward: 2.598, mean reward: 0.217 [0.164, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.177, 10.100], loss: 0.003475, mae: 0.057092, mean_q: -0.328331
 50530/100000: episode: 893, duration: 0.044s, episode steps: 9, steps per second: 203, episode reward: 3.219, mean reward: 0.358 [0.227, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.440, 10.100], loss: 0.002416, mae: 0.050729, mean_q: -0.295095
 50539/100000: episode: 894, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 1.832, mean reward: 0.204 [0.158, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.171, 10.100], loss: 0.002201, mae: 0.048223, mean_q: -0.255882
 50595/100000: episode: 895, duration: 0.285s, episode steps: 56, steps per second: 197, episode reward: 15.880, mean reward: 0.284 [0.077, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.849 [-0.508, 10.100], loss: 0.002475, mae: 0.049815, mean_q: -0.285833
 50651/100000: episode: 896, duration: 0.268s, episode steps: 56, steps per second: 209, episode reward: 16.294, mean reward: 0.291 [0.170, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-1.166, 10.100], loss: 0.002237, mae: 0.047915, mean_q: -0.278771
 50661/100000: episode: 897, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 2.834, mean reward: 0.283 [0.163, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.687, 10.100], loss: 0.002599, mae: 0.053148, mean_q: -0.203100
 50671/100000: episode: 898, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 2.494, mean reward: 0.249 [0.152, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.238, 10.100], loss: 0.002833, mae: 0.052897, mean_q: -0.217758
 50703/100000: episode: 899, duration: 0.168s, episode steps: 32, steps per second: 191, episode reward: 8.807, mean reward: 0.275 [0.133, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.206, 10.271], loss: 0.002446, mae: 0.050419, mean_q: -0.240038
 50715/100000: episode: 900, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 2.965, mean reward: 0.247 [0.164, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.273, 10.100], loss: 0.002524, mae: 0.050862, mean_q: -0.235505
 50747/100000: episode: 901, duration: 0.158s, episode steps: 32, steps per second: 202, episode reward: 6.310, mean reward: 0.197 [0.023, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.225, 10.112], loss: 0.002260, mae: 0.049611, mean_q: -0.186648
 50779/100000: episode: 902, duration: 0.168s, episode steps: 32, steps per second: 191, episode reward: 8.914, mean reward: 0.279 [0.156, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.308, 10.100], loss: 0.002794, mae: 0.051639, mean_q: -0.264373
 50805/100000: episode: 903, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 10.477, mean reward: 0.403 [0.176, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.208, 10.609], loss: 0.002272, mae: 0.050061, mean_q: -0.198420
 50861/100000: episode: 904, duration: 0.305s, episode steps: 56, steps per second: 184, episode reward: 10.032, mean reward: 0.179 [0.019, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.035, 10.203], loss: 0.002592, mae: 0.050436, mean_q: -0.274118
 50873/100000: episode: 905, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 3.960, mean reward: 0.330 [0.284, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.278, 10.100], loss: 0.002213, mae: 0.050014, mean_q: -0.266970
 50882/100000: episode: 906, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 2.663, mean reward: 0.296 [0.236, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.428, 10.100], loss: 0.002389, mae: 0.050814, mean_q: -0.290911
 50895/100000: episode: 907, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 4.253, mean reward: 0.327 [0.287, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.443, 10.100], loss: 0.002709, mae: 0.053439, mean_q: -0.257108
 50927/100000: episode: 908, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 10.386, mean reward: 0.325 [0.174, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.018, 10.355], loss: 0.002669, mae: 0.054109, mean_q: -0.153201
 50937/100000: episode: 909, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 2.876, mean reward: 0.288 [0.133, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-2.116, 10.100], loss: 0.002602, mae: 0.050658, mean_q: -0.156137
 50946/100000: episode: 910, duration: 0.044s, episode steps: 9, steps per second: 204, episode reward: 2.769, mean reward: 0.308 [0.217, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.385, 10.100], loss: 0.002604, mae: 0.049777, mean_q: -0.301473
 50974/100000: episode: 911, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 6.788, mean reward: 0.242 [0.115, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.243, 10.100], loss: 0.002465, mae: 0.050537, mean_q: -0.234502
 51030/100000: episode: 912, duration: 0.268s, episode steps: 56, steps per second: 209, episode reward: 12.910, mean reward: 0.231 [0.018, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.037, 10.226], loss: 0.002452, mae: 0.049989, mean_q: -0.186395
 51062/100000: episode: 913, duration: 0.155s, episode steps: 32, steps per second: 206, episode reward: 12.446, mean reward: 0.389 [0.254, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.089, 10.428], loss: 0.002620, mae: 0.051377, mean_q: -0.195457
 51090/100000: episode: 914, duration: 0.134s, episode steps: 28, steps per second: 209, episode reward: 6.135, mean reward: 0.219 [0.110, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.403, 10.100], loss: 0.002581, mae: 0.051692, mean_q: -0.162541
 51122/100000: episode: 915, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 13.294, mean reward: 0.415 [0.267, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.222, 10.100], loss: 0.002200, mae: 0.048142, mean_q: -0.177101
 51131/100000: episode: 916, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 3.357, mean reward: 0.373 [0.273, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.225, 10.100], loss: 0.003309, mae: 0.059665, mean_q: -0.076078
 51159/100000: episode: 917, duration: 0.143s, episode steps: 28, steps per second: 195, episode reward: 13.321, mean reward: 0.476 [0.370, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.184, 10.100], loss: 0.002375, mae: 0.050070, mean_q: -0.154257
 51172/100000: episode: 918, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 5.110, mean reward: 0.393 [0.261, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.343, 10.100], loss: 0.002831, mae: 0.053668, mean_q: -0.158829
 51182/100000: episode: 919, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 2.976, mean reward: 0.298 [0.223, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.378, 10.100], loss: 0.002943, mae: 0.054119, mean_q: -0.133022
 51195/100000: episode: 920, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 3.750, mean reward: 0.288 [0.198, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.199, 10.100], loss: 0.002762, mae: 0.054213, mean_q: 0.003853
 51251/100000: episode: 921, duration: 0.283s, episode steps: 56, steps per second: 198, episode reward: 11.610, mean reward: 0.207 [0.044, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-0.937, 10.360], loss: 0.002781, mae: 0.054273, mean_q: -0.128832
 51307/100000: episode: 922, duration: 0.292s, episode steps: 56, steps per second: 192, episode reward: 12.599, mean reward: 0.225 [0.043, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.632, 10.155], loss: 0.002756, mae: 0.053780, mean_q: -0.131263
 51339/100000: episode: 923, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 10.674, mean reward: 0.334 [0.118, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.106, 10.254], loss: 0.002383, mae: 0.050387, mean_q: -0.169349
 51352/100000: episode: 924, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 4.051, mean reward: 0.312 [0.262, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.059, 10.100], loss: 0.002946, mae: 0.054797, mean_q: -0.118689
 51373/100000: episode: 925, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 6.202, mean reward: 0.295 [0.191, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.047, 10.100], loss: 0.003004, mae: 0.057714, mean_q: -0.088888
 51429/100000: episode: 926, duration: 0.286s, episode steps: 56, steps per second: 196, episode reward: 21.236, mean reward: 0.379 [0.162, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.835 [-0.482, 10.100], loss: 0.002721, mae: 0.053805, mean_q: -0.163949
 51485/100000: episode: 927, duration: 0.273s, episode steps: 56, steps per second: 205, episode reward: 15.287, mean reward: 0.273 [0.078, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.643, 10.100], loss: 0.002731, mae: 0.053298, mean_q: -0.150826
 51506/100000: episode: 928, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 6.942, mean reward: 0.331 [0.248, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.191, 10.100], loss: 0.002481, mae: 0.052649, mean_q: -0.066997
 51515/100000: episode: 929, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 2.506, mean reward: 0.278 [0.233, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.236, 10.100], loss: 0.002317, mae: 0.052431, mean_q: -0.087324
 51525/100000: episode: 930, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 2.675, mean reward: 0.267 [0.214, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.421, 10.100], loss: 0.002926, mae: 0.055491, mean_q: -0.069041
 51537/100000: episode: 931, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 3.634, mean reward: 0.303 [0.216, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.196, 10.100], loss: 0.002925, mae: 0.055704, mean_q: -0.073034
 51565/100000: episode: 932, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 8.420, mean reward: 0.301 [0.179, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.252, 10.100], loss: 0.002612, mae: 0.053665, mean_q: -0.060590
 51575/100000: episode: 933, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 2.021, mean reward: 0.202 [0.137, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.292, 10.100], loss: 0.002881, mae: 0.054643, mean_q: -0.110928
 51607/100000: episode: 934, duration: 0.162s, episode steps: 32, steps per second: 198, episode reward: 10.469, mean reward: 0.327 [0.177, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.470, 10.100], loss: 0.002775, mae: 0.055244, mean_q: -0.116555
 51635/100000: episode: 935, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 7.838, mean reward: 0.280 [0.112, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.143, 10.100], loss: 0.002665, mae: 0.052352, mean_q: -0.149856
 51667/100000: episode: 936, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 11.524, mean reward: 0.360 [0.251, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.448, 10.100], loss: 0.002430, mae: 0.050553, mean_q: -0.097720
 51679/100000: episode: 937, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 3.097, mean reward: 0.258 [0.171, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.327, 10.100], loss: 0.002525, mae: 0.051792, mean_q: -0.117024
 51705/100000: episode: 938, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 9.414, mean reward: 0.362 [0.219, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.411, 10.380], loss: 0.002613, mae: 0.052589, mean_q: -0.057518
 51714/100000: episode: 939, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 2.917, mean reward: 0.324 [0.256, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.318, 10.100], loss: 0.002415, mae: 0.050981, mean_q: -0.055684
 51742/100000: episode: 940, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 9.336, mean reward: 0.333 [0.264, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.590, 10.100], loss: 0.002693, mae: 0.054538, mean_q: -0.044785
 51755/100000: episode: 941, duration: 0.078s, episode steps: 13, steps per second: 168, episode reward: 2.838, mean reward: 0.218 [0.059, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.114, 10.106], loss: 0.003445, mae: 0.059104, mean_q: -0.105469
 51767/100000: episode: 942, duration: 0.058s, episode steps: 12, steps per second: 208, episode reward: 3.787, mean reward: 0.316 [0.215, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.546, 10.100], loss: 0.002571, mae: 0.052234, mean_q: -0.044180
 51799/100000: episode: 943, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 12.608, mean reward: 0.394 [0.268, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-1.134, 10.456], loss: 0.002724, mae: 0.054426, mean_q: -0.050425
 51825/100000: episode: 944, duration: 0.126s, episode steps: 26, steps per second: 207, episode reward: 10.530, mean reward: 0.405 [0.266, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.818, 10.564], loss: 0.002728, mae: 0.055165, mean_q: -0.017237
 51857/100000: episode: 945, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 12.260, mean reward: 0.383 [0.278, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.432, 10.100], loss: 0.002699, mae: 0.053362, mean_q: -0.037343
 51885/100000: episode: 946, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 9.301, mean reward: 0.332 [0.202, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.243, 10.100], loss: 0.002958, mae: 0.055785, mean_q: -0.035540
 51917/100000: episode: 947, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 12.363, mean reward: 0.386 [0.252, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.229, 10.387], loss: 0.002843, mae: 0.054514, mean_q: -0.081617
 51926/100000: episode: 948, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 2.430, mean reward: 0.270 [0.240, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.314, 10.100], loss: 0.002993, mae: 0.059031, mean_q: 0.025750
 51958/100000: episode: 949, duration: 0.156s, episode steps: 32, steps per second: 206, episode reward: 11.463, mean reward: 0.358 [0.274, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.588, 10.452], loss: 0.002728, mae: 0.053555, mean_q: -0.105680
 51990/100000: episode: 950, duration: 0.168s, episode steps: 32, steps per second: 191, episode reward: 11.417, mean reward: 0.357 [0.246, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.229, 10.100], loss: 0.003172, mae: 0.058790, mean_q: -0.028607
 51999/100000: episode: 951, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 2.698, mean reward: 0.300 [0.232, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.524, 10.100], loss: 0.002931, mae: 0.057673, mean_q: -0.027877
 52055/100000: episode: 952, duration: 0.284s, episode steps: 56, steps per second: 197, episode reward: 9.937, mean reward: 0.177 [0.011, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-0.520, 10.276], loss: 0.002749, mae: 0.054977, mean_q: -0.017375
 52068/100000: episode: 953, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 4.513, mean reward: 0.347 [0.257, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.193, 10.100], loss: 0.002670, mae: 0.053433, mean_q: -0.125810
 52080/100000: episode: 954, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 3.134, mean reward: 0.261 [0.190, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.247, 10.100], loss: 0.002852, mae: 0.056244, mean_q: -0.013895
 52093/100000: episode: 955, duration: 0.074s, episode steps: 13, steps per second: 177, episode reward: 3.453, mean reward: 0.266 [0.157, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.218, 10.100], loss: 0.002832, mae: 0.056127, mean_q: 0.007241
 52149/100000: episode: 956, duration: 0.283s, episode steps: 56, steps per second: 198, episode reward: 12.876, mean reward: 0.230 [0.082, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-0.388, 10.100], loss: 0.002849, mae: 0.055766, mean_q: -0.008947
 52181/100000: episode: 957, duration: 0.167s, episode steps: 32, steps per second: 191, episode reward: 14.522, mean reward: 0.454 [0.316, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.052, 10.100], loss: 0.002894, mae: 0.055490, mean_q: 0.000272
 52193/100000: episode: 958, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 3.048, mean reward: 0.254 [0.157, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.036, 10.100], loss: 0.002742, mae: 0.057073, mean_q: 0.046334
 52225/100000: episode: 959, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 11.153, mean reward: 0.349 [0.205, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.198, 10.100], loss: 0.002855, mae: 0.057246, mean_q: 0.059031
 52234/100000: episode: 960, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 2.513, mean reward: 0.279 [0.173, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-1.741, 10.100], loss: 0.002281, mae: 0.050420, mean_q: 0.082398
 52246/100000: episode: 961, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 3.109, mean reward: 0.259 [0.166, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.404, 10.100], loss: 0.002814, mae: 0.053131, mean_q: -0.026334
 52258/100000: episode: 962, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 3.188, mean reward: 0.266 [0.198, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.213, 10.100], loss: 0.002976, mae: 0.057177, mean_q: 0.094373
 52286/100000: episode: 963, duration: 0.133s, episode steps: 28, steps per second: 211, episode reward: 9.240, mean reward: 0.330 [0.240, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.212, 10.100], loss: 0.002625, mae: 0.052987, mean_q: 0.026284
 52342/100000: episode: 964, duration: 0.271s, episode steps: 56, steps per second: 206, episode reward: 11.062, mean reward: 0.198 [0.051, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.247, 10.100], loss: 0.002562, mae: 0.053002, mean_q: 0.027963
 52363/100000: episode: 965, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 6.165, mean reward: 0.294 [0.216, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.776, 10.100], loss: 0.002873, mae: 0.056617, mean_q: 0.103341
 52376/100000: episode: 966, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 4.840, mean reward: 0.372 [0.295, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.772, 10.100], loss: 0.002638, mae: 0.053469, mean_q: 0.078318
 52388/100000: episode: 967, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 4.221, mean reward: 0.352 [0.239, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.479, 10.100], loss: 0.002531, mae: 0.051923, mean_q: 0.039124
 52420/100000: episode: 968, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 12.518, mean reward: 0.391 [0.261, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.301, 10.430], loss: 0.002721, mae: 0.054060, mean_q: 0.071069
 52476/100000: episode: 969, duration: 0.296s, episode steps: 56, steps per second: 189, episode reward: 12.198, mean reward: 0.218 [0.044, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-1.196, 10.100], loss: 0.003057, mae: 0.058825, mean_q: 0.088557
 52485/100000: episode: 970, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 2.517, mean reward: 0.280 [0.216, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.196, 10.100], loss: 0.002926, mae: 0.057140, mean_q: 0.096884
 52517/100000: episode: 971, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 11.175, mean reward: 0.349 [0.091, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.280, 10.100], loss: 0.003180, mae: 0.060064, mean_q: 0.069363
 52538/100000: episode: 972, duration: 0.101s, episode steps: 21, steps per second: 209, episode reward: 5.353, mean reward: 0.255 [0.176, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.225, 10.100], loss: 0.002853, mae: 0.057749, mean_q: 0.120464
 52564/100000: episode: 973, duration: 0.143s, episode steps: 26, steps per second: 181, episode reward: 7.273, mean reward: 0.280 [0.167, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.317, 10.315], loss: 0.003054, mae: 0.058840, mean_q: 0.077693
 52592/100000: episode: 974, duration: 0.155s, episode steps: 28, steps per second: 180, episode reward: 12.079, mean reward: 0.431 [0.352, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.234, 10.100], loss: 0.002708, mae: 0.056250, mean_q: 0.112259
 52605/100000: episode: 975, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 4.095, mean reward: 0.315 [0.210, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.329, 10.100], loss: 0.002934, mae: 0.056650, mean_q: 0.047691
 52615/100000: episode: 976, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 2.666, mean reward: 0.267 [0.168, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.296, 10.100], loss: 0.002679, mae: 0.054840, mean_q: 0.093477
[Info] 200-TH LEVEL FOUND: 0.8719136714935303, Considering 10/90 traces
 52636/100000: episode: 977, duration: 3.957s, episode steps: 21, steps per second: 5, episode reward: 6.245, mean reward: 0.297 [0.251, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.865, 10.100], loss: 0.002883, mae: 0.057342, mean_q: 0.110797
 52660/100000: episode: 978, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 8.000, mean reward: 0.333 [0.186, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.671, 10.100], loss: 0.002864, mae: 0.056575, mean_q: 0.105840
 52678/100000: episode: 979, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 6.785, mean reward: 0.377 [0.256, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.121, 10.100], loss: 0.002769, mae: 0.054861, mean_q: 0.089495
 52691/100000: episode: 980, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 5.324, mean reward: 0.410 [0.299, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.267, 10.100], loss: 0.002640, mae: 0.055114, mean_q: 0.139508
 52700/100000: episode: 981, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 4.482, mean reward: 0.498 [0.443, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.575], loss: 0.003157, mae: 0.059468, mean_q: 0.108885
 52712/100000: episode: 982, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 4.476, mean reward: 0.373 [0.324, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.433, 10.504], loss: 0.002893, mae: 0.058062, mean_q: 0.171421
 52725/100000: episode: 983, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 6.916, mean reward: 0.532 [0.451, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.465, 10.100], loss: 0.002426, mae: 0.052373, mean_q: 0.132538
 52749/100000: episode: 984, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 7.730, mean reward: 0.322 [0.208, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.623, 10.100], loss: 0.002953, mae: 0.059158, mean_q: 0.154648
 52771/100000: episode: 985, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 9.963, mean reward: 0.453 [0.373, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.574, 10.100], loss: 0.002529, mae: 0.053151, mean_q: 0.132556
 52784/100000: episode: 986, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 4.851, mean reward: 0.373 [0.263, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.048, 10.100], loss: 0.002769, mae: 0.056841, mean_q: 0.193476
 52796/100000: episode: 987, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 4.561, mean reward: 0.380 [0.343, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.172, 10.553], loss: 0.002576, mae: 0.054135, mean_q: 0.206135
 52818/100000: episode: 988, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 9.093, mean reward: 0.413 [0.327, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.746, 10.100], loss: 0.002930, mae: 0.058842, mean_q: 0.146159
 52835/100000: episode: 989, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 8.322, mean reward: 0.490 [0.393, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.118, 10.480], loss: 0.002495, mae: 0.053529, mean_q: 0.210687
 52844/100000: episode: 990, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 4.066, mean reward: 0.452 [0.366, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.442], loss: 0.002930, mae: 0.059726, mean_q: 0.277876
 52862/100000: episode: 991, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 6.934, mean reward: 0.385 [0.238, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.331, 10.100], loss: 0.002687, mae: 0.055590, mean_q: 0.180648
 52880/100000: episode: 992, duration: 0.090s, episode steps: 18, steps per second: 201, episode reward: 8.864, mean reward: 0.492 [0.389, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.558, 10.100], loss: 0.002821, mae: 0.055872, mean_q: 0.210972
 52904/100000: episode: 993, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 10.670, mean reward: 0.445 [0.293, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.770, 10.100], loss: 0.002628, mae: 0.055175, mean_q: 0.177422
 52926/100000: episode: 994, duration: 0.114s, episode steps: 22, steps per second: 194, episode reward: 7.229, mean reward: 0.329 [0.172, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.196, 10.100], loss: 0.003142, mae: 0.059625, mean_q: 0.170321
 52943/100000: episode: 995, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 7.932, mean reward: 0.467 [0.394, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.358, 10.525], loss: 0.002531, mae: 0.054128, mean_q: 0.213746
 52960/100000: episode: 996, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 7.656, mean reward: 0.450 [0.330, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.428, 10.440], loss: 0.002524, mae: 0.055060, mean_q: 0.107794
 52978/100000: episode: 997, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 8.545, mean reward: 0.475 [0.432, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.831, 10.100], loss: 0.002490, mae: 0.053778, mean_q: 0.205288
 53001/100000: episode: 998, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 8.522, mean reward: 0.371 [0.206, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.355, 10.100], loss: 0.002539, mae: 0.054019, mean_q: 0.144363
 53023/100000: episode: 999, duration: 0.104s, episode steps: 22, steps per second: 212, episode reward: 10.049, mean reward: 0.457 [0.365, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.341, 10.100], loss: 0.002877, mae: 0.056881, mean_q: 0.275705
 53036/100000: episode: 1000, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 5.952, mean reward: 0.458 [0.315, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.249, 10.100], loss: 0.002541, mae: 0.052173, mean_q: 0.205279
 53045/100000: episode: 1001, duration: 0.044s, episode steps: 9, steps per second: 205, episode reward: 4.306, mean reward: 0.478 [0.434, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.099, 10.600], loss: 0.003511, mae: 0.065033, mean_q: 0.312521
 53076/100000: episode: 1002, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 11.143, mean reward: 0.359 [0.137, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.156, 10.100], loss: 0.002981, mae: 0.058779, mean_q: 0.192181
 53085/100000: episode: 1003, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 4.599, mean reward: 0.511 [0.407, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.888, 10.551], loss: 0.003969, mae: 0.061454, mean_q: 0.280021
 53098/100000: episode: 1004, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 5.460, mean reward: 0.420 [0.312, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.159, 10.100], loss: 0.008606, mae: 0.076321, mean_q: 0.201970
 53121/100000: episode: 1005, duration: 0.112s, episode steps: 23, steps per second: 205, episode reward: 8.369, mean reward: 0.364 [0.297, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.497, 10.100], loss: 0.003355, mae: 0.065874, mean_q: 0.210605
 53143/100000: episode: 1006, duration: 0.111s, episode steps: 22, steps per second: 199, episode reward: 10.381, mean reward: 0.472 [0.375, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.633, 10.100], loss: 0.002889, mae: 0.058144, mean_q: 0.219772
 53171/100000: episode: 1007, duration: 0.134s, episode steps: 28, steps per second: 209, episode reward: 11.545, mean reward: 0.412 [0.305, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.540, 10.100], loss: 0.003136, mae: 0.060597, mean_q: 0.205890
 53193/100000: episode: 1008, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 7.544, mean reward: 0.343 [0.234, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.253, 10.100], loss: 0.002498, mae: 0.054945, mean_q: 0.211640
 53215/100000: episode: 1009, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 9.256, mean reward: 0.421 [0.318, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.558, 10.100], loss: 0.002754, mae: 0.056073, mean_q: 0.209648
 53246/100000: episode: 1010, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 10.946, mean reward: 0.353 [0.122, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.404, 10.100], loss: 0.002438, mae: 0.052690, mean_q: 0.161107
 53259/100000: episode: 1011, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 6.277, mean reward: 0.483 [0.311, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.235, 10.100], loss: 0.002892, mae: 0.057310, mean_q: 0.243100
[Info] FALSIFICATION!
 53264/100000: episode: 1012, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 12.020, mean reward: 2.404 [0.478, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.003, 9.695], loss: 0.002387, mae: 0.054271, mean_q: 0.232554
 53364/100000: episode: 1013, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.671, mean reward: -0.157 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.580, 10.098], loss: 0.017192, mae: 0.070540, mean_q: 0.236418
 53464/100000: episode: 1014, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -17.354, mean reward: -0.174 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.085, 10.343], loss: 0.017647, mae: 0.074811, mean_q: 0.231228
 53564/100000: episode: 1015, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -19.532, mean reward: -0.195 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.709, 10.198], loss: 0.002879, mae: 0.057404, mean_q: 0.232462
 53664/100000: episode: 1016, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -17.726, mean reward: -0.177 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.347, 10.098], loss: 0.002828, mae: 0.056706, mean_q: 0.233652
 53764/100000: episode: 1017, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.732, mean reward: -0.177 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.411, 10.124], loss: 0.016473, mae: 0.063955, mean_q: 0.243583
 53864/100000: episode: 1018, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.069, mean reward: -0.181 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.474, 10.098], loss: 0.016838, mae: 0.069024, mean_q: 0.223918
 53964/100000: episode: 1019, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -10.347, mean reward: -0.103 [-1.000, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.601, 10.411], loss: 0.030140, mae: 0.074420, mean_q: 0.217436
 54064/100000: episode: 1020, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -14.701, mean reward: -0.147 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.839, 10.098], loss: 0.003920, mae: 0.066745, mean_q: 0.250239
 54164/100000: episode: 1021, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.906, mean reward: -0.179 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.957, 10.188], loss: 0.002620, mae: 0.054191, mean_q: 0.242785
 54264/100000: episode: 1022, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -18.624, mean reward: -0.186 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.303, 10.098], loss: 0.043211, mae: 0.079428, mean_q: 0.254895
 54364/100000: episode: 1023, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.485, mean reward: -0.185 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.822, 10.217], loss: 0.004603, mae: 0.063330, mean_q: 0.218457
 54464/100000: episode: 1024, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.752, mean reward: -0.178 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.273, 10.316], loss: 0.030791, mae: 0.080434, mean_q: 0.218131
 54564/100000: episode: 1025, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -19.184, mean reward: -0.192 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.842, 10.098], loss: 0.002757, mae: 0.057051, mean_q: 0.235679
 54664/100000: episode: 1026, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -15.735, mean reward: -0.157 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.076, 10.219], loss: 0.002610, mae: 0.054969, mean_q: 0.260666
 54764/100000: episode: 1027, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.433, mean reward: -0.154 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.632, 10.098], loss: 0.029512, mae: 0.068309, mean_q: 0.247098
 54864/100000: episode: 1028, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -16.349, mean reward: -0.163 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.107, 10.098], loss: 0.030044, mae: 0.074867, mean_q: 0.238458
 54964/100000: episode: 1029, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.081, mean reward: -0.181 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.654, 10.342], loss: 0.016476, mae: 0.065924, mean_q: 0.251897
 55064/100000: episode: 1030, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -19.250, mean reward: -0.193 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.937, 10.098], loss: 0.016142, mae: 0.063366, mean_q: 0.234862
 55164/100000: episode: 1031, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -15.485, mean reward: -0.155 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.165, 10.098], loss: 0.002807, mae: 0.055616, mean_q: 0.221226
 55264/100000: episode: 1032, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.258, mean reward: -0.153 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.099, 10.098], loss: 0.002646, mae: 0.055090, mean_q: 0.221065
 55364/100000: episode: 1033, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.727, mean reward: -0.157 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.401, 10.209], loss: 0.016507, mae: 0.064994, mean_q: 0.221810
 55464/100000: episode: 1034, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.633, mean reward: -0.166 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.070, 10.233], loss: 0.002688, mae: 0.055153, mean_q: 0.195912
 55564/100000: episode: 1035, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.287, mean reward: -0.183 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.496, 10.098], loss: 0.029321, mae: 0.068394, mean_q: 0.182590
 55664/100000: episode: 1036, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -10.721, mean reward: -0.107 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-2.534, 10.392], loss: 0.002915, mae: 0.058192, mean_q: 0.181259
 55764/100000: episode: 1037, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -13.720, mean reward: -0.137 [-1.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.889, 10.282], loss: 0.017011, mae: 0.071087, mean_q: 0.143699
 55864/100000: episode: 1038, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.689, mean reward: -0.177 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.573, 10.182], loss: 0.016108, mae: 0.061488, mean_q: 0.147782
 55964/100000: episode: 1039, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -17.893, mean reward: -0.179 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.771, 10.098], loss: 0.016015, mae: 0.062133, mean_q: 0.130148
 56064/100000: episode: 1040, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -17.990, mean reward: -0.180 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.022, 10.127], loss: 0.002496, mae: 0.052820, mean_q: 0.094407
 56164/100000: episode: 1041, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -17.920, mean reward: -0.179 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.214, 10.098], loss: 0.016147, mae: 0.062159, mean_q: 0.078172
 56264/100000: episode: 1042, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.297, mean reward: -0.153 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.488, 10.252], loss: 0.015807, mae: 0.059998, mean_q: 0.045168
 56364/100000: episode: 1043, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.966, mean reward: -0.150 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.463, 10.333], loss: 0.002433, mae: 0.051202, mean_q: 0.022372
 56464/100000: episode: 1044, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -14.658, mean reward: -0.147 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.291, 10.098], loss: 0.002617, mae: 0.053303, mean_q: 0.013752
 56564/100000: episode: 1045, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.508, mean reward: -0.185 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.767, 10.352], loss: 0.002445, mae: 0.050822, mean_q: -0.015756
 56664/100000: episode: 1046, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -6.164, mean reward: -0.062 [-1.000, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.003, 10.354], loss: 0.015919, mae: 0.058655, mean_q: 0.002945
 56764/100000: episode: 1047, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -15.063, mean reward: -0.151 [-1.000, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.378, 10.364], loss: 0.016529, mae: 0.065885, mean_q: -0.008716
 56864/100000: episode: 1048, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.113, mean reward: -0.151 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.338, 10.098], loss: 0.055982, mae: 0.080290, mean_q: -0.053188
 56964/100000: episode: 1049, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.444, mean reward: -0.154 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.701, 10.236], loss: 0.016353, mae: 0.062941, mean_q: -0.048505
 57064/100000: episode: 1050, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -10.548, mean reward: -0.105 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.079, 10.470], loss: 0.002626, mae: 0.053768, mean_q: -0.075124
 57164/100000: episode: 1051, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -13.464, mean reward: -0.135 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.866, 10.199], loss: 0.002492, mae: 0.050833, mean_q: -0.101532
 57264/100000: episode: 1052, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.419, mean reward: -0.174 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.957, 10.180], loss: 0.004818, mae: 0.063112, mean_q: -0.094431
 57364/100000: episode: 1053, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -20.162, mean reward: -0.202 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.695, 10.098], loss: 0.004733, mae: 0.062500, mean_q: -0.156223
 57464/100000: episode: 1054, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.976, mean reward: -0.190 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.929, 10.242], loss: 0.016518, mae: 0.062838, mean_q: -0.141388
 57564/100000: episode: 1055, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.094, mean reward: -0.171 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.820, 10.194], loss: 0.042535, mae: 0.073181, mean_q: -0.148539
 57664/100000: episode: 1056, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.285, mean reward: -0.153 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.473, 10.304], loss: 0.002736, mae: 0.053354, mean_q: -0.138688
 57764/100000: episode: 1057, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -16.522, mean reward: -0.165 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.970, 10.126], loss: 0.028683, mae: 0.062527, mean_q: -0.194283
 57864/100000: episode: 1058, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -14.886, mean reward: -0.149 [-1.000, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.673, 10.156], loss: 0.028862, mae: 0.062644, mean_q: -0.197598
 57964/100000: episode: 1059, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.020, mean reward: -0.150 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.546, 10.098], loss: 0.015762, mae: 0.057786, mean_q: -0.273195
 58064/100000: episode: 1060, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.702, mean reward: -0.167 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.923, 10.305], loss: 0.028888, mae: 0.065460, mean_q: -0.226500
 58164/100000: episode: 1061, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.332, mean reward: -0.173 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.906, 10.098], loss: 0.002639, mae: 0.050441, mean_q: -0.303185
 58264/100000: episode: 1062, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -19.752, mean reward: -0.198 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.900, 10.098], loss: 0.015884, mae: 0.057531, mean_q: -0.307956
 58364/100000: episode: 1063, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.457, mean reward: -0.165 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.693, 10.331], loss: 0.002756, mae: 0.050743, mean_q: -0.302023
 58464/100000: episode: 1064, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -15.476, mean reward: -0.155 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.890, 10.437], loss: 0.002339, mae: 0.048323, mean_q: -0.314717
 58564/100000: episode: 1065, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -15.359, mean reward: -0.154 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.719, 10.098], loss: 0.002498, mae: 0.048408, mean_q: -0.299914
 58664/100000: episode: 1066, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.803, mean reward: -0.188 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.826, 10.098], loss: 0.002307, mae: 0.046709, mean_q: -0.335800
 58764/100000: episode: 1067, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -16.071, mean reward: -0.161 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.498, 10.098], loss: 0.002566, mae: 0.048479, mean_q: -0.301069
 58864/100000: episode: 1068, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.596, mean reward: -0.186 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.292, 10.281], loss: 0.002258, mae: 0.045900, mean_q: -0.319299
 58964/100000: episode: 1069, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.165, mean reward: -0.152 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.344, 10.267], loss: 0.002484, mae: 0.049421, mean_q: -0.295881
 59064/100000: episode: 1070, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -18.493, mean reward: -0.185 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.717, 10.098], loss: 0.002479, mae: 0.047851, mean_q: -0.324894
 59164/100000: episode: 1071, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.698, mean reward: -0.177 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.366, 10.222], loss: 0.002219, mae: 0.045724, mean_q: -0.293663
 59264/100000: episode: 1072, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -19.655, mean reward: -0.197 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.619, 10.151], loss: 0.002386, mae: 0.047880, mean_q: -0.287859
 59364/100000: episode: 1073, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.383, mean reward: -0.174 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.594, 10.210], loss: 0.002343, mae: 0.047068, mean_q: -0.302959
 59464/100000: episode: 1074, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.388, mean reward: -0.164 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.564, 10.287], loss: 0.002237, mae: 0.047455, mean_q: -0.275912
 59564/100000: episode: 1075, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.210, mean reward: -0.182 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.011, 10.199], loss: 0.002399, mae: 0.047953, mean_q: -0.332186
 59664/100000: episode: 1076, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.291, mean reward: -0.163 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.622, 10.216], loss: 0.002311, mae: 0.047558, mean_q: -0.307312
 59764/100000: episode: 1077, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.160, mean reward: -0.172 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.455, 10.154], loss: 0.002258, mae: 0.046634, mean_q: -0.309313
 59864/100000: episode: 1078, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -19.236, mean reward: -0.192 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.043, 10.177], loss: 0.003923, mae: 0.060156, mean_q: -0.332821
 59964/100000: episode: 1079, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.852, mean reward: -0.169 [-1.000, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.351, 10.098], loss: 0.002347, mae: 0.048198, mean_q: -0.283619
 60064/100000: episode: 1080, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -13.666, mean reward: -0.137 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.387, 10.098], loss: 0.004038, mae: 0.055667, mean_q: -0.306282
 60164/100000: episode: 1081, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.487, mean reward: -0.145 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.804, 10.098], loss: 0.002537, mae: 0.048651, mean_q: -0.304579
 60264/100000: episode: 1082, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -14.791, mean reward: -0.148 [-1.000, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.398, 10.216], loss: 0.002318, mae: 0.047767, mean_q: -0.307262
 60364/100000: episode: 1083, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -14.923, mean reward: -0.149 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.095, 10.158], loss: 0.002373, mae: 0.047139, mean_q: -0.322648
 60464/100000: episode: 1084, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.266, mean reward: -0.173 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.356, 10.098], loss: 0.002824, mae: 0.049603, mean_q: -0.315463
 60564/100000: episode: 1085, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.828, mean reward: -0.168 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.535, 10.139], loss: 0.002924, mae: 0.053534, mean_q: -0.311390
 60664/100000: episode: 1086, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.714, mean reward: -0.187 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.697, 10.272], loss: 0.002287, mae: 0.047393, mean_q: -0.295774
 60764/100000: episode: 1087, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -14.306, mean reward: -0.143 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.578, 10.382], loss: 0.002607, mae: 0.050254, mean_q: -0.295550
 60864/100000: episode: 1088, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.331, mean reward: -0.173 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.962, 10.117], loss: 0.002498, mae: 0.047629, mean_q: -0.307574
 60964/100000: episode: 1089, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.274, mean reward: -0.173 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.515, 10.436], loss: 0.002250, mae: 0.045799, mean_q: -0.310601
 61064/100000: episode: 1090, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -12.852, mean reward: -0.129 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.491, 10.098], loss: 0.002417, mae: 0.047088, mean_q: -0.332941
 61164/100000: episode: 1091, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -14.786, mean reward: -0.148 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.590, 10.098], loss: 0.002405, mae: 0.047313, mean_q: -0.310632
 61264/100000: episode: 1092, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.068, mean reward: -0.181 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.530, 10.098], loss: 0.002714, mae: 0.051986, mean_q: -0.305544
 61364/100000: episode: 1093, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.401, mean reward: -0.184 [-1.000, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.170, 10.098], loss: 0.002318, mae: 0.046453, mean_q: -0.341846
 61464/100000: episode: 1094, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -13.970, mean reward: -0.140 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.917, 10.253], loss: 0.002579, mae: 0.048979, mean_q: -0.314490
 61564/100000: episode: 1095, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.150, mean reward: -0.162 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.673, 10.148], loss: 0.002644, mae: 0.049476, mean_q: -0.344090
 61664/100000: episode: 1096, duration: 0.512s, episode steps: 100, steps per second: 196, episode reward: -18.555, mean reward: -0.186 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.506, 10.098], loss: 0.002488, mae: 0.049078, mean_q: -0.310646
 61764/100000: episode: 1097, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.619, mean reward: -0.156 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.621, 10.231], loss: 0.002616, mae: 0.050106, mean_q: -0.326323
 61864/100000: episode: 1098, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.588, mean reward: -0.166 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.604, 10.098], loss: 0.002579, mae: 0.048381, mean_q: -0.323381
 61964/100000: episode: 1099, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -19.356, mean reward: -0.194 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.466, 10.098], loss: 0.002471, mae: 0.048231, mean_q: -0.314153
 62064/100000: episode: 1100, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.208, mean reward: -0.152 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.029, 10.298], loss: 0.004098, mae: 0.061056, mean_q: -0.284824
 62164/100000: episode: 1101, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.263, mean reward: -0.173 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.115, 10.154], loss: 0.003382, mae: 0.055096, mean_q: -0.319963
 62264/100000: episode: 1102, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.641, mean reward: -0.166 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.629, 10.098], loss: 0.002586, mae: 0.049347, mean_q: -0.334388
 62364/100000: episode: 1103, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.186, mean reward: -0.162 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.755, 10.142], loss: 0.002470, mae: 0.048062, mean_q: -0.314026
 62464/100000: episode: 1104, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -18.336, mean reward: -0.183 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.615, 10.098], loss: 0.002659, mae: 0.049434, mean_q: -0.342579
 62564/100000: episode: 1105, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.114, mean reward: -0.151 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.922, 10.098], loss: 0.002639, mae: 0.050044, mean_q: -0.343861
 62664/100000: episode: 1106, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.289, mean reward: -0.163 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.383, 10.098], loss: 0.002625, mae: 0.050186, mean_q: -0.304945
 62764/100000: episode: 1107, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.683, mean reward: -0.177 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.807, 10.124], loss: 0.002578, mae: 0.050045, mean_q: -0.292770
 62864/100000: episode: 1108, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -15.067, mean reward: -0.151 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.985, 10.139], loss: 0.002540, mae: 0.048781, mean_q: -0.296120
 62964/100000: episode: 1109, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -14.675, mean reward: -0.147 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.785, 10.468], loss: 0.002611, mae: 0.048482, mean_q: -0.357191
 63064/100000: episode: 1110, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.050, mean reward: -0.131 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.293, 10.396], loss: 0.002531, mae: 0.048358, mean_q: -0.339421
 63164/100000: episode: 1111, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.347, mean reward: -0.173 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.905, 10.098], loss: 0.002404, mae: 0.047719, mean_q: -0.292758
[Info] 100-TH LEVEL FOUND: 0.6239284873008728, Considering 10/90 traces
 63264/100000: episode: 1112, duration: 4.302s, episode steps: 100, steps per second: 23, episode reward: -15.836, mean reward: -0.158 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.807, 10.098], loss: 0.002494, mae: 0.048774, mean_q: -0.327505
 63335/100000: episode: 1113, duration: 0.360s, episode steps: 71, steps per second: 197, episode reward: 13.471, mean reward: 0.190 [0.016, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.733 [-0.162, 10.100], loss: 0.002506, mae: 0.049721, mean_q: -0.270253
 63374/100000: episode: 1114, duration: 0.203s, episode steps: 39, steps per second: 192, episode reward: 8.560, mean reward: 0.219 [0.028, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.223, 10.100], loss: 0.002517, mae: 0.049733, mean_q: -0.273061
 63392/100000: episode: 1115, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 5.437, mean reward: 0.302 [0.189, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.230, 10.100], loss: 0.002846, mae: 0.053693, mean_q: -0.285449
 63431/100000: episode: 1116, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 7.860, mean reward: 0.202 [0.033, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.540, 10.225], loss: 0.002646, mae: 0.051359, mean_q: -0.259487
 63475/100000: episode: 1117, duration: 0.211s, episode steps: 44, steps per second: 209, episode reward: 12.213, mean reward: 0.278 [0.094, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.734, 10.221], loss: 0.003103, mae: 0.053516, mean_q: -0.279949
 63514/100000: episode: 1118, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 10.806, mean reward: 0.277 [0.150, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.214, 10.207], loss: 0.002518, mae: 0.049718, mean_q: -0.304119
 63544/100000: episode: 1119, duration: 0.155s, episode steps: 30, steps per second: 193, episode reward: 7.960, mean reward: 0.265 [0.117, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.387, 10.216], loss: 0.002862, mae: 0.052978, mean_q: -0.211585
 63588/100000: episode: 1120, duration: 0.215s, episode steps: 44, steps per second: 205, episode reward: 15.795, mean reward: 0.359 [0.028, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.230, 10.222], loss: 0.002651, mae: 0.052536, mean_q: -0.237201
 63659/100000: episode: 1121, duration: 0.375s, episode steps: 71, steps per second: 189, episode reward: 14.043, mean reward: 0.198 [0.020, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.726 [-0.654, 10.100], loss: 0.002727, mae: 0.051568, mean_q: -0.261477
 63730/100000: episode: 1122, duration: 0.354s, episode steps: 71, steps per second: 201, episode reward: 15.163, mean reward: 0.214 [0.010, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.730 [-0.633, 10.100], loss: 0.002558, mae: 0.050929, mean_q: -0.238587
 63748/100000: episode: 1123, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 7.341, mean reward: 0.408 [0.314, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.397, 10.100], loss: 0.002284, mae: 0.048113, mean_q: -0.233718
 63766/100000: episode: 1124, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 6.494, mean reward: 0.361 [0.270, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.885, 10.100], loss: 0.002934, mae: 0.052965, mean_q: -0.228880
 63810/100000: episode: 1125, duration: 0.215s, episode steps: 44, steps per second: 204, episode reward: 13.471, mean reward: 0.306 [0.159, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.225, 10.266], loss: 0.002876, mae: 0.053657, mean_q: -0.155033
 63840/100000: episode: 1126, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 8.753, mean reward: 0.292 [0.184, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.511, 10.376], loss: 0.002615, mae: 0.050127, mean_q: -0.255555
 63911/100000: episode: 1127, duration: 0.355s, episode steps: 71, steps per second: 200, episode reward: 11.502, mean reward: 0.162 [0.006, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.730 [-0.364, 10.100], loss: 0.002795, mae: 0.052771, mean_q: -0.183672
 63950/100000: episode: 1128, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 6.411, mean reward: 0.164 [0.030, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.804, 10.185], loss: 0.002974, mae: 0.054073, mean_q: -0.184535
 63988/100000: episode: 1129, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 8.789, mean reward: 0.231 [0.116, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.035, 10.290], loss: 0.003020, mae: 0.055562, mean_q: -0.203894
 64018/100000: episode: 1130, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 9.978, mean reward: 0.333 [0.152, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.656, 10.414], loss: 0.002496, mae: 0.050601, mean_q: -0.210779
 64057/100000: episode: 1131, duration: 0.208s, episode steps: 39, steps per second: 188, episode reward: 8.427, mean reward: 0.216 [0.079, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.114, 10.221], loss: 0.002825, mae: 0.052629, mean_q: -0.199550
 64095/100000: episode: 1132, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 11.652, mean reward: 0.307 [0.200, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.460, 10.332], loss: 0.002665, mae: 0.052999, mean_q: -0.166902
 64107/100000: episode: 1133, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 3.907, mean reward: 0.326 [0.302, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.308, 10.100], loss: 0.002891, mae: 0.054287, mean_q: -0.090074
 64137/100000: episode: 1134, duration: 0.175s, episode steps: 30, steps per second: 171, episode reward: 13.359, mean reward: 0.445 [0.302, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.475, 10.483], loss: 0.003142, mae: 0.055595, mean_q: -0.125352
 64149/100000: episode: 1135, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 5.114, mean reward: 0.426 [0.344, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.384, 10.100], loss: 0.002560, mae: 0.051076, mean_q: -0.129206
 64179/100000: episode: 1136, duration: 0.162s, episode steps: 30, steps per second: 186, episode reward: 8.414, mean reward: 0.280 [0.203, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.824, 10.330], loss: 0.002837, mae: 0.053703, mean_q: -0.136478
 64250/100000: episode: 1137, duration: 0.377s, episode steps: 71, steps per second: 188, episode reward: 17.892, mean reward: 0.252 [0.083, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.726 [-0.716, 10.100], loss: 0.002985, mae: 0.054001, mean_q: -0.164387
 64268/100000: episode: 1138, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 6.996, mean reward: 0.389 [0.266, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.351, 10.100], loss: 0.003279, mae: 0.058395, mean_q: -0.132423
 64298/100000: episode: 1139, duration: 0.146s, episode steps: 30, steps per second: 205, episode reward: 8.235, mean reward: 0.274 [0.158, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.059, 10.274], loss: 0.002707, mae: 0.052659, mean_q: -0.138301
 64336/100000: episode: 1140, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 10.735, mean reward: 0.282 [0.108, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.035, 10.313], loss: 0.002832, mae: 0.053497, mean_q: -0.117171
 64348/100000: episode: 1141, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 3.565, mean reward: 0.297 [0.242, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.317, 10.100], loss: 0.002793, mae: 0.055114, mean_q: -0.071841
 64366/100000: episode: 1142, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 6.168, mean reward: 0.343 [0.268, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.566, 10.100], loss: 0.002763, mae: 0.052741, mean_q: -0.058851
 64401/100000: episode: 1143, duration: 0.167s, episode steps: 35, steps per second: 210, episode reward: 9.483, mean reward: 0.271 [0.198, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.035, 10.407], loss: 0.002780, mae: 0.054589, mean_q: -0.097730
 64413/100000: episode: 1144, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 4.056, mean reward: 0.338 [0.290, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.241, 10.100], loss: 0.002518, mae: 0.048651, mean_q: -0.131431
 64425/100000: episode: 1145, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 4.387, mean reward: 0.366 [0.283, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.452, 10.100], loss: 0.002826, mae: 0.053794, mean_q: -0.080220
 64464/100000: episode: 1146, duration: 0.197s, episode steps: 39, steps per second: 198, episode reward: 6.427, mean reward: 0.165 [0.010, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.772, 10.113], loss: 0.002755, mae: 0.054329, mean_q: -0.095269
 64535/100000: episode: 1147, duration: 0.379s, episode steps: 71, steps per second: 187, episode reward: 10.480, mean reward: 0.148 [0.010, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.733 [-0.270, 10.193], loss: 0.002776, mae: 0.054360, mean_q: -0.096644
 64574/100000: episode: 1148, duration: 0.217s, episode steps: 39, steps per second: 180, episode reward: 10.574, mean reward: 0.271 [0.104, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.358, 10.356], loss: 0.002838, mae: 0.054241, mean_q: -0.107487
 64586/100000: episode: 1149, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 3.831, mean reward: 0.319 [0.261, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.288, 10.100], loss: 0.002416, mae: 0.051007, mean_q: -0.129533
 64604/100000: episode: 1150, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 4.787, mean reward: 0.266 [0.206, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.803, 10.100], loss: 0.002751, mae: 0.052029, mean_q: -0.149421
 64648/100000: episode: 1151, duration: 0.218s, episode steps: 44, steps per second: 202, episode reward: 11.139, mean reward: 0.253 [0.124, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.319, 10.268], loss: 0.003106, mae: 0.056650, mean_q: -0.074663
 64660/100000: episode: 1152, duration: 0.058s, episode steps: 12, steps per second: 207, episode reward: 4.796, mean reward: 0.400 [0.347, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.182, 10.100], loss: 0.002441, mae: 0.052080, mean_q: -0.097968
 64704/100000: episode: 1153, duration: 0.221s, episode steps: 44, steps per second: 199, episode reward: 13.830, mean reward: 0.314 [0.112, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-1.588, 10.260], loss: 0.005094, mae: 0.065650, mean_q: -0.062019
 64722/100000: episode: 1154, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 5.846, mean reward: 0.325 [0.193, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.295, 10.100], loss: 0.004182, mae: 0.070809, mean_q: -0.085230
 64793/100000: episode: 1155, duration: 0.370s, episode steps: 71, steps per second: 192, episode reward: 11.762, mean reward: 0.166 [0.019, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.728 [-1.022, 10.100], loss: 0.003909, mae: 0.064293, mean_q: -0.080697
 64811/100000: episode: 1156, duration: 0.113s, episode steps: 18, steps per second: 159, episode reward: 5.549, mean reward: 0.308 [0.251, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.328, 10.100], loss: 0.002461, mae: 0.052164, mean_q: -0.010659
 64823/100000: episode: 1157, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 4.370, mean reward: 0.364 [0.249, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.268, 10.100], loss: 0.002969, mae: 0.055559, mean_q: -0.111237
 64858/100000: episode: 1158, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 9.391, mean reward: 0.268 [0.127, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.650, 10.346], loss: 0.002850, mae: 0.056540, mean_q: -0.030119
 64876/100000: episode: 1159, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 5.247, mean reward: 0.292 [0.164, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.261, 10.100], loss: 0.003472, mae: 0.063179, mean_q: 0.067848
 64947/100000: episode: 1160, duration: 0.354s, episode steps: 71, steps per second: 200, episode reward: 13.551, mean reward: 0.191 [0.033, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.721 [-0.189, 10.112], loss: 0.002647, mae: 0.052679, mean_q: -0.039951
 65009/100000: episode: 1161, duration: 0.309s, episode steps: 62, steps per second: 200, episode reward: 17.689, mean reward: 0.285 [0.116, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.806 [-0.349, 10.100], loss: 0.002735, mae: 0.054571, mean_q: 0.003755
 65047/100000: episode: 1162, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 7.668, mean reward: 0.202 [0.010, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-1.694, 10.107], loss: 0.003197, mae: 0.059298, mean_q: 0.016927
 65085/100000: episode: 1163, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 13.496, mean reward: 0.355 [0.219, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.642, 10.457], loss: 0.002830, mae: 0.053984, mean_q: -0.032841
 65147/100000: episode: 1164, duration: 0.316s, episode steps: 62, steps per second: 196, episode reward: 9.692, mean reward: 0.156 [0.025, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.801 [-0.243, 10.144], loss: 0.002913, mae: 0.055778, mean_q: -0.014096
 65165/100000: episode: 1165, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 7.360, mean reward: 0.409 [0.340, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.382, 10.100], loss: 0.003025, mae: 0.057425, mean_q: -0.001911
 65236/100000: episode: 1166, duration: 0.360s, episode steps: 71, steps per second: 197, episode reward: 17.419, mean reward: 0.245 [0.032, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.719 [-1.397, 10.100], loss: 0.002866, mae: 0.055552, mean_q: 0.043577
 65248/100000: episode: 1167, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 4.565, mean reward: 0.380 [0.324, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.399, 10.100], loss: 0.002522, mae: 0.052042, mean_q: 0.012619
 65292/100000: episode: 1168, duration: 0.221s, episode steps: 44, steps per second: 199, episode reward: 14.495, mean reward: 0.329 [0.156, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.551, 10.432], loss: 0.002858, mae: 0.054556, mean_q: -0.014684
 65322/100000: episode: 1169, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 9.514, mean reward: 0.317 [0.191, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.055, 10.340], loss: 0.002772, mae: 0.054642, mean_q: 0.042964
 65340/100000: episode: 1170, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 6.053, mean reward: 0.336 [0.238, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.153, 10.100], loss: 0.002981, mae: 0.057856, mean_q: 0.107125
 65384/100000: episode: 1171, duration: 0.215s, episode steps: 44, steps per second: 205, episode reward: 14.480, mean reward: 0.329 [0.210, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-1.214, 10.313], loss: 0.003033, mae: 0.056926, mean_q: 0.017501
 65428/100000: episode: 1172, duration: 0.237s, episode steps: 44, steps per second: 186, episode reward: 20.719, mean reward: 0.471 [0.329, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.610, 10.496], loss: 0.002595, mae: 0.053573, mean_q: 0.045173
 65440/100000: episode: 1173, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 4.233, mean reward: 0.353 [0.292, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.270, 10.100], loss: 0.002852, mae: 0.056084, mean_q: -0.000559
 65479/100000: episode: 1174, duration: 0.218s, episode steps: 39, steps per second: 179, episode reward: 13.911, mean reward: 0.357 [0.221, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.482, 10.463], loss: 0.002816, mae: 0.053580, mean_q: 0.045440
 65497/100000: episode: 1175, duration: 0.100s, episode steps: 18, steps per second: 179, episode reward: 7.241, mean reward: 0.402 [0.332, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.385, 10.100], loss: 0.003491, mae: 0.060187, mean_q: 0.056959
 65515/100000: episode: 1176, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 6.310, mean reward: 0.351 [0.268, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.915, 10.100], loss: 0.002875, mae: 0.057407, mean_q: 0.095495
 65586/100000: episode: 1177, duration: 0.367s, episode steps: 71, steps per second: 193, episode reward: 10.076, mean reward: 0.142 [0.008, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.735 [-0.517, 10.229], loss: 0.002773, mae: 0.054964, mean_q: 0.097175
 65648/100000: episode: 1178, duration: 0.305s, episode steps: 62, steps per second: 203, episode reward: 13.587, mean reward: 0.219 [0.035, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.821 [-0.638, 10.173], loss: 0.002813, mae: 0.055147, mean_q: 0.090158
 65660/100000: episode: 1179, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 4.791, mean reward: 0.399 [0.333, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.301, 10.100], loss: 0.002683, mae: 0.053442, mean_q: 0.170688
 65704/100000: episode: 1180, duration: 0.215s, episode steps: 44, steps per second: 204, episode reward: 10.223, mean reward: 0.232 [0.034, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.560, 10.167], loss: 0.003181, mae: 0.058183, mean_q: 0.097205
 65766/100000: episode: 1181, duration: 0.302s, episode steps: 62, steps per second: 205, episode reward: 10.943, mean reward: 0.176 [0.023, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.806 [-1.335, 10.100], loss: 0.002718, mae: 0.054144, mean_q: 0.104204
 65801/100000: episode: 1182, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 12.868, mean reward: 0.368 [0.179, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.508, 10.461], loss: 0.002638, mae: 0.052675, mean_q: 0.069572
 65845/100000: episode: 1183, duration: 0.230s, episode steps: 44, steps per second: 192, episode reward: 16.431, mean reward: 0.373 [0.211, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.621, 10.416], loss: 0.003428, mae: 0.060889, mean_q: 0.140537
 65883/100000: episode: 1184, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 8.739, mean reward: 0.230 [0.050, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.827, 10.212], loss: 0.003112, mae: 0.059731, mean_q: 0.133546
 65945/100000: episode: 1185, duration: 0.322s, episode steps: 62, steps per second: 192, episode reward: 17.576, mean reward: 0.283 [0.081, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.802 [-0.863, 10.100], loss: 0.002824, mae: 0.055832, mean_q: 0.109977
 65980/100000: episode: 1186, duration: 0.179s, episode steps: 35, steps per second: 195, episode reward: 5.651, mean reward: 0.161 [0.012, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.096, 10.132], loss: 0.002925, mae: 0.058662, mean_q: 0.150336
 66024/100000: episode: 1187, duration: 0.217s, episode steps: 44, steps per second: 203, episode reward: 13.344, mean reward: 0.303 [0.152, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.645, 10.329], loss: 0.002814, mae: 0.055416, mean_q: 0.156538
 66086/100000: episode: 1188, duration: 0.356s, episode steps: 62, steps per second: 174, episode reward: 17.143, mean reward: 0.276 [0.088, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.797 [-0.416, 10.100], loss: 0.002724, mae: 0.055057, mean_q: 0.163579
 66121/100000: episode: 1189, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 8.622, mean reward: 0.246 [0.062, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-2.130, 10.232], loss: 0.003282, mae: 0.061845, mean_q: 0.183302
 66165/100000: episode: 1190, duration: 0.231s, episode steps: 44, steps per second: 191, episode reward: 14.263, mean reward: 0.324 [0.028, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.230, 10.257], loss: 0.003188, mae: 0.060585, mean_q: 0.201820
 66195/100000: episode: 1191, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 7.094, mean reward: 0.236 [0.056, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.826, 10.223], loss: 0.003036, mae: 0.058315, mean_q: 0.160509
 66239/100000: episode: 1192, duration: 0.229s, episode steps: 44, steps per second: 192, episode reward: 16.844, mean reward: 0.383 [0.253, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.702, 10.413], loss: 0.003290, mae: 0.061918, mean_q: 0.192335
 66257/100000: episode: 1193, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 6.610, mean reward: 0.367 [0.287, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.147, 10.100], loss: 0.003248, mae: 0.061257, mean_q: 0.181009
 66319/100000: episode: 1194, duration: 0.316s, episode steps: 62, steps per second: 196, episode reward: 9.958, mean reward: 0.161 [0.021, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.813 [-1.225, 10.241], loss: 0.003005, mae: 0.058743, mean_q: 0.203829
 66331/100000: episode: 1195, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 4.812, mean reward: 0.401 [0.300, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.498, 10.100], loss: 0.002669, mae: 0.054977, mean_q: 0.227224
 66375/100000: episode: 1196, duration: 0.228s, episode steps: 44, steps per second: 193, episode reward: 14.511, mean reward: 0.330 [0.135, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.327, 10.343], loss: 0.002714, mae: 0.055145, mean_q: 0.184495
 66419/100000: episode: 1197, duration: 0.216s, episode steps: 44, steps per second: 204, episode reward: 11.424, mean reward: 0.260 [0.078, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.547, 10.229], loss: 0.002887, mae: 0.057612, mean_q: 0.240006
 66463/100000: episode: 1198, duration: 0.218s, episode steps: 44, steps per second: 202, episode reward: 15.348, mean reward: 0.349 [0.257, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.571, 10.455], loss: 0.002743, mae: 0.056296, mean_q: 0.220177
 66525/100000: episode: 1199, duration: 0.311s, episode steps: 62, steps per second: 199, episode reward: 15.293, mean reward: 0.247 [0.106, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.806 [-1.429, 10.100], loss: 0.002979, mae: 0.058340, mean_q: 0.239408
 66537/100000: episode: 1200, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 5.195, mean reward: 0.433 [0.373, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.399, 10.100], loss: 0.003125, mae: 0.060661, mean_q: 0.267917
 66581/100000: episode: 1201, duration: 0.221s, episode steps: 44, steps per second: 200, episode reward: 13.702, mean reward: 0.311 [0.221, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.564, 10.350], loss: 0.003123, mae: 0.061777, mean_q: 0.263703
[Info] 200-TH LEVEL FOUND: 0.8481519222259521, Considering 10/90 traces
 66619/100000: episode: 1202, duration: 4.038s, episode steps: 38, steps per second: 9, episode reward: 8.738, mean reward: 0.230 [0.109, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.803, 10.361], loss: 0.002840, mae: 0.056988, mean_q: 0.277674
 66651/100000: episode: 1203, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 15.368, mean reward: 0.480 [0.388, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.109, 10.575], loss: 0.002847, mae: 0.057916, mean_q: 0.246265
 66685/100000: episode: 1204, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 9.325, mean reward: 0.274 [0.075, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.574, 10.240], loss: 0.002956, mae: 0.056070, mean_q: 0.279707
 66719/100000: episode: 1205, duration: 0.162s, episode steps: 34, steps per second: 210, episode reward: 11.817, mean reward: 0.348 [0.250, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.235, 10.366], loss: 0.002987, mae: 0.057684, mean_q: 0.261530
 66753/100000: episode: 1206, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 15.159, mean reward: 0.446 [0.329, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.395, 10.581], loss: 0.002609, mae: 0.056053, mean_q: 0.301206
 66791/100000: episode: 1207, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 9.804, mean reward: 0.258 [0.017, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.271, 10.100], loss: 0.002888, mae: 0.058026, mean_q: 0.285783
 66800/100000: episode: 1208, duration: 0.052s, episode steps: 9, steps per second: 175, episode reward: 4.004, mean reward: 0.445 [0.363, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.587, 10.100], loss: 0.002959, mae: 0.059410, mean_q: 0.231930
 66834/100000: episode: 1209, duration: 0.161s, episode steps: 34, steps per second: 211, episode reward: 10.108, mean reward: 0.297 [0.018, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.410, 10.100], loss: 0.002915, mae: 0.058828, mean_q: 0.261111
 66847/100000: episode: 1210, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 4.683, mean reward: 0.360 [0.172, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.215, 10.100], loss: 0.003351, mae: 0.062884, mean_q: 0.339909
 66860/100000: episode: 1211, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 4.924, mean reward: 0.379 [0.263, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.690, 10.100], loss: 0.002771, mae: 0.056947, mean_q: 0.291638
 66895/100000: episode: 1212, duration: 0.184s, episode steps: 35, steps per second: 190, episode reward: 17.487, mean reward: 0.500 [0.387, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.405, 10.450], loss: 0.003093, mae: 0.061751, mean_q: 0.327101
 66926/100000: episode: 1213, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 12.815, mean reward: 0.413 [0.296, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.193, 10.438], loss: 0.002824, mae: 0.058014, mean_q: 0.333587
 66959/100000: episode: 1214, duration: 0.176s, episode steps: 33, steps per second: 188, episode reward: 8.142, mean reward: 0.247 [0.026, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.046, 10.100], loss: 0.002938, mae: 0.060052, mean_q: 0.342418
 66993/100000: episode: 1215, duration: 0.168s, episode steps: 34, steps per second: 202, episode reward: 12.337, mean reward: 0.363 [0.151, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-1.182, 10.273], loss: 0.002967, mae: 0.059478, mean_q: 0.347586
 67026/100000: episode: 1216, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 11.821, mean reward: 0.358 [0.191, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.035, 10.352], loss: 0.002883, mae: 0.059151, mean_q: 0.380000
 67061/100000: episode: 1217, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 16.298, mean reward: 0.466 [0.275, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.240, 10.477], loss: 0.002821, mae: 0.057241, mean_q: 0.337119
 67095/100000: episode: 1218, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 14.132, mean reward: 0.416 [0.331, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.980, 10.419], loss: 0.003066, mae: 0.060923, mean_q: 0.345514
 67128/100000: episode: 1219, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 11.743, mean reward: 0.356 [0.088, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.472, 10.310], loss: 0.002864, mae: 0.058962, mean_q: 0.373302
 67161/100000: episode: 1220, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 14.505, mean reward: 0.440 [0.339, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.384, 10.434], loss: 0.003178, mae: 0.060917, mean_q: 0.350364
 67192/100000: episode: 1221, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 10.675, mean reward: 0.344 [0.207, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.517, 10.322], loss: 0.002646, mae: 0.056087, mean_q: 0.367776
 67205/100000: episode: 1222, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 5.873, mean reward: 0.452 [0.328, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.365, 10.100], loss: 0.002573, mae: 0.054231, mean_q: 0.391133
 67238/100000: episode: 1223, duration: 0.174s, episode steps: 33, steps per second: 189, episode reward: 11.514, mean reward: 0.349 [0.212, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.529, 10.399], loss: 0.002807, mae: 0.058145, mean_q: 0.393513
 67247/100000: episode: 1224, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 3.947, mean reward: 0.439 [0.386, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.438, 10.100], loss: 0.003043, mae: 0.059639, mean_q: 0.444937
 67285/100000: episode: 1225, duration: 0.207s, episode steps: 38, steps per second: 183, episode reward: 16.084, mean reward: 0.423 [0.278, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.127, 10.508], loss: 0.002837, mae: 0.056788, mean_q: 0.404415
 67316/100000: episode: 1226, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 9.241, mean reward: 0.298 [0.178, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.497, 10.202], loss: 0.003591, mae: 0.066120, mean_q: 0.434142
 67348/100000: episode: 1227, duration: 0.161s, episode steps: 32, steps per second: 198, episode reward: 12.456, mean reward: 0.389 [0.294, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.240, 10.400], loss: 0.003168, mae: 0.061422, mean_q: 0.422606
 67381/100000: episode: 1228, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 14.211, mean reward: 0.431 [0.298, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.366, 10.349], loss: 0.002917, mae: 0.059334, mean_q: 0.424279
 67412/100000: episode: 1229, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 10.652, mean reward: 0.344 [0.154, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.270, 10.354], loss: 0.002929, mae: 0.058512, mean_q: 0.434080
 67445/100000: episode: 1230, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 13.031, mean reward: 0.395 [0.250, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.729, 10.504], loss: 0.003005, mae: 0.060380, mean_q: 0.426147
 67458/100000: episode: 1231, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 6.043, mean reward: 0.465 [0.362, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.659, 10.100], loss: 0.002559, mae: 0.055257, mean_q: 0.451857
 67491/100000: episode: 1232, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 12.739, mean reward: 0.386 [0.215, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.035, 10.368], loss: 0.002752, mae: 0.057223, mean_q: 0.463974
 67504/100000: episode: 1233, duration: 0.062s, episode steps: 13, steps per second: 209, episode reward: 5.577, mean reward: 0.429 [0.380, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.507, 10.100], loss: 0.003492, mae: 0.062035, mean_q: 0.503858
 67535/100000: episode: 1234, duration: 0.165s, episode steps: 31, steps per second: 187, episode reward: 12.140, mean reward: 0.392 [0.314, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.481, 10.546], loss: 0.003025, mae: 0.060757, mean_q: 0.471415
 67567/100000: episode: 1235, duration: 0.156s, episode steps: 32, steps per second: 205, episode reward: 9.438, mean reward: 0.295 [0.145, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.740, 10.310], loss: 0.002828, mae: 0.058113, mean_q: 0.456098
 67576/100000: episode: 1236, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 3.980, mean reward: 0.442 [0.362, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.434, 10.100], loss: 0.002755, mae: 0.056134, mean_q: 0.462993
 67585/100000: episode: 1237, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 3.697, mean reward: 0.411 [0.328, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.383, 10.100], loss: 0.002319, mae: 0.051249, mean_q: 0.416835
 67598/100000: episode: 1238, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 5.732, mean reward: 0.441 [0.315, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.379, 10.100], loss: 0.002471, mae: 0.055568, mean_q: 0.476032
 67630/100000: episode: 1239, duration: 0.157s, episode steps: 32, steps per second: 203, episode reward: 14.815, mean reward: 0.463 [0.393, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.457, 10.616], loss: 0.002960, mae: 0.060243, mean_q: 0.480548
 67639/100000: episode: 1240, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 4.228, mean reward: 0.470 [0.417, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.504, 10.100], loss: 0.003313, mae: 0.062314, mean_q: 0.497903
 67677/100000: episode: 1241, duration: 0.193s, episode steps: 38, steps per second: 197, episode reward: 13.294, mean reward: 0.350 [0.256, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.503, 10.385], loss: 0.002873, mae: 0.059078, mean_q: 0.486143
 67686/100000: episode: 1242, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 3.698, mean reward: 0.411 [0.370, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.787, 10.100], loss: 0.003294, mae: 0.062690, mean_q: 0.553510
 67719/100000: episode: 1243, duration: 0.163s, episode steps: 33, steps per second: 203, episode reward: 10.884, mean reward: 0.330 [0.196, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.759, 10.318], loss: 0.002867, mae: 0.058097, mean_q: 0.519601
 67752/100000: episode: 1244, duration: 0.196s, episode steps: 33, steps per second: 168, episode reward: 11.632, mean reward: 0.352 [0.137, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.133, 10.318], loss: 0.003084, mae: 0.061193, mean_q: 0.520041
 67785/100000: episode: 1245, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 10.600, mean reward: 0.321 [0.078, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.337, 10.199], loss: 0.002745, mae: 0.058763, mean_q: 0.533873
 67818/100000: episode: 1246, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 14.453, mean reward: 0.438 [0.343, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.622, 10.475], loss: 0.002918, mae: 0.059042, mean_q: 0.515548
 67849/100000: episode: 1247, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 12.599, mean reward: 0.406 [0.105, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.229, 10.364], loss: 0.002850, mae: 0.058839, mean_q: 0.532528
 67880/100000: episode: 1248, duration: 0.142s, episode steps: 31, steps per second: 218, episode reward: 9.140, mean reward: 0.295 [0.014, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-1.507, 10.140], loss: 0.002680, mae: 0.057299, mean_q: 0.520234
 67914/100000: episode: 1249, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 11.222, mean reward: 0.330 [0.177, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.585, 10.293], loss: 0.002767, mae: 0.057243, mean_q: 0.520657
 67945/100000: episode: 1250, duration: 0.173s, episode steps: 31, steps per second: 180, episode reward: 11.594, mean reward: 0.374 [0.224, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.035, 10.563], loss: 0.002911, mae: 0.060176, mean_q: 0.528706
 67983/100000: episode: 1251, duration: 0.185s, episode steps: 38, steps per second: 205, episode reward: 12.336, mean reward: 0.325 [0.157, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.766, 10.345], loss: 0.003082, mae: 0.062250, mean_q: 0.561212
 68018/100000: episode: 1252, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 14.206, mean reward: 0.406 [0.301, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-1.506, 10.458], loss: 0.002688, mae: 0.057792, mean_q: 0.559228
 68051/100000: episode: 1253, duration: 0.162s, episode steps: 33, steps per second: 204, episode reward: 15.020, mean reward: 0.455 [0.310, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.179, 10.601], loss: 0.002868, mae: 0.058798, mean_q: 0.570299
 68064/100000: episode: 1254, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 4.166, mean reward: 0.320 [0.185, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.303, 10.100], loss: 0.002948, mae: 0.059738, mean_q: 0.549497
 68073/100000: episode: 1255, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 3.698, mean reward: 0.411 [0.367, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.504, 10.100], loss: 0.003112, mae: 0.060391, mean_q: 0.562948
 68104/100000: episode: 1256, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 9.653, mean reward: 0.311 [0.187, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.373, 10.369], loss: 0.002699, mae: 0.057514, mean_q: 0.587807
 68137/100000: episode: 1257, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 9.914, mean reward: 0.300 [0.013, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.180, 10.182], loss: 0.002911, mae: 0.060244, mean_q: 0.582930
 68150/100000: episode: 1258, duration: 0.062s, episode steps: 13, steps per second: 209, episode reward: 5.356, mean reward: 0.412 [0.353, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.376, 10.100], loss: 0.002984, mae: 0.061136, mean_q: 0.591927
 68184/100000: episode: 1259, duration: 0.173s, episode steps: 34, steps per second: 197, episode reward: 9.269, mean reward: 0.273 [0.131, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.109, 10.360], loss: 0.002738, mae: 0.057299, mean_q: 0.588612
 68218/100000: episode: 1260, duration: 0.166s, episode steps: 34, steps per second: 204, episode reward: 11.348, mean reward: 0.334 [0.196, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.248, 10.306], loss: 0.002879, mae: 0.059922, mean_q: 0.587287
 68250/100000: episode: 1261, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 11.376, mean reward: 0.356 [0.240, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.576, 10.386], loss: 0.002943, mae: 0.060037, mean_q: 0.581364
 68284/100000: episode: 1262, duration: 0.180s, episode steps: 34, steps per second: 189, episode reward: 12.821, mean reward: 0.377 [0.208, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.574, 10.301], loss: 0.003095, mae: 0.061064, mean_q: 0.590774
 68322/100000: episode: 1263, duration: 0.195s, episode steps: 38, steps per second: 195, episode reward: 15.454, mean reward: 0.407 [0.287, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.276, 10.480], loss: 0.002716, mae: 0.057074, mean_q: 0.589575
 68335/100000: episode: 1264, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 5.518, mean reward: 0.424 [0.344, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.613, 10.100], loss: 0.002771, mae: 0.059118, mean_q: 0.590843
 68369/100000: episode: 1265, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 13.951, mean reward: 0.410 [0.254, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.513, 10.430], loss: 0.002725, mae: 0.057204, mean_q: 0.593787
 68407/100000: episode: 1266, duration: 0.178s, episode steps: 38, steps per second: 213, episode reward: 13.422, mean reward: 0.353 [0.123, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.781, 10.236], loss: 0.002714, mae: 0.058970, mean_q: 0.591692
 68442/100000: episode: 1267, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 12.576, mean reward: 0.359 [0.232, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.525, 10.365], loss: 0.003118, mae: 0.062091, mean_q: 0.582053
 68475/100000: episode: 1268, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 12.561, mean reward: 0.381 [0.199, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.346, 10.413], loss: 0.002721, mae: 0.058038, mean_q: 0.593192
 68488/100000: episode: 1269, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 5.424, mean reward: 0.417 [0.357, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.448, 10.100], loss: 0.002549, mae: 0.056763, mean_q: 0.584180
 68497/100000: episode: 1270, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 4.324, mean reward: 0.480 [0.405, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.612, 10.100], loss: 0.002763, mae: 0.057627, mean_q: 0.618811
 68510/100000: episode: 1271, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 5.737, mean reward: 0.441 [0.334, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.261, 10.100], loss: 0.002517, mae: 0.054933, mean_q: 0.589482
 68545/100000: episode: 1272, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 11.643, mean reward: 0.333 [0.251, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.195, 10.537], loss: 0.002982, mae: 0.059607, mean_q: 0.594815
 68578/100000: episode: 1273, duration: 0.155s, episode steps: 33, steps per second: 213, episode reward: 10.379, mean reward: 0.315 [0.094, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.343, 10.196], loss: 0.002767, mae: 0.056715, mean_q: 0.602722
 68616/100000: episode: 1274, duration: 0.189s, episode steps: 38, steps per second: 201, episode reward: 11.322, mean reward: 0.298 [0.119, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.657, 10.266], loss: 0.002843, mae: 0.059386, mean_q: 0.604342
 68650/100000: episode: 1275, duration: 0.161s, episode steps: 34, steps per second: 211, episode reward: 7.980, mean reward: 0.235 [0.082, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-1.100, 10.246], loss: 0.002952, mae: 0.061016, mean_q: 0.610444
 68681/100000: episode: 1276, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 11.195, mean reward: 0.361 [0.163, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.678, 10.316], loss: 0.002571, mae: 0.055805, mean_q: 0.599782
 68690/100000: episode: 1277, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 4.827, mean reward: 0.536 [0.418, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.423, 10.100], loss: 0.002531, mae: 0.055867, mean_q: 0.600906
 68722/100000: episode: 1278, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 12.941, mean reward: 0.404 [0.300, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.035, 10.681], loss: 0.003061, mae: 0.061455, mean_q: 0.611390
 68757/100000: episode: 1279, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 13.212, mean reward: 0.377 [0.258, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.854, 10.417], loss: 0.003288, mae: 0.064665, mean_q: 0.589904
 68790/100000: episode: 1280, duration: 0.176s, episode steps: 33, steps per second: 187, episode reward: 11.235, mean reward: 0.340 [0.212, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-1.249, 10.380], loss: 0.002786, mae: 0.059076, mean_q: 0.608366
 68823/100000: episode: 1281, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 13.846, mean reward: 0.420 [0.274, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.071, 10.411], loss: 0.002834, mae: 0.058357, mean_q: 0.605398
 68857/100000: episode: 1282, duration: 0.171s, episode steps: 34, steps per second: 199, episode reward: 14.075, mean reward: 0.414 [0.342, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.600, 10.497], loss: 0.002660, mae: 0.057322, mean_q: 0.615877
 68870/100000: episode: 1283, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 6.379, mean reward: 0.491 [0.332, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.534, 10.100], loss: 0.003067, mae: 0.060769, mean_q: 0.594424
 68902/100000: episode: 1284, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 14.610, mean reward: 0.457 [0.376, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.035, 10.546], loss: 0.002795, mae: 0.059595, mean_q: 0.617828
 68933/100000: episode: 1285, duration: 0.180s, episode steps: 31, steps per second: 172, episode reward: 10.579, mean reward: 0.341 [0.254, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.986, 10.424], loss: 0.002401, mae: 0.054195, mean_q: 0.623030
 68964/100000: episode: 1286, duration: 0.157s, episode steps: 31, steps per second: 197, episode reward: 11.967, mean reward: 0.386 [0.272, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.035, 10.501], loss: 0.002698, mae: 0.057149, mean_q: 0.627230
 68998/100000: episode: 1287, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 14.040, mean reward: 0.413 [0.299, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.816, 10.499], loss: 0.002886, mae: 0.059905, mean_q: 0.609882
 69030/100000: episode: 1288, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 9.293, mean reward: 0.290 [0.070, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.245, 10.202], loss: 0.002624, mae: 0.056691, mean_q: 0.619595
 69064/100000: episode: 1289, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 8.603, mean reward: 0.253 [0.037, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.035, 10.218], loss: 0.002905, mae: 0.059108, mean_q: 0.611363
 69077/100000: episode: 1290, duration: 0.064s, episode steps: 13, steps per second: 205, episode reward: 5.803, mean reward: 0.446 [0.329, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.715, 10.100], loss: 0.002941, mae: 0.057471, mean_q: 0.617721
 69090/100000: episode: 1291, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 5.473, mean reward: 0.421 [0.328, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.554, 10.100], loss: 0.003057, mae: 0.061149, mean_q: 0.623335
[Info] 300-TH LEVEL FOUND: 0.9821293354034424, Considering 10/90 traces
 69123/100000: episode: 1292, duration: 4.010s, episode steps: 33, steps per second: 8, episode reward: 7.863, mean reward: 0.238 [0.027, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.126, 10.100], loss: 0.003019, mae: 0.058150, mean_q: 0.630870
 69154/100000: episode: 1293, duration: 0.172s, episode steps: 31, steps per second: 181, episode reward: 15.334, mean reward: 0.495 [0.379, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.465, 10.545], loss: 0.002380, mae: 0.053942, mean_q: 0.624998
 69185/100000: episode: 1294, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 11.589, mean reward: 0.374 [0.246, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.035, 10.402], loss: 0.002676, mae: 0.057341, mean_q: 0.615182
 69218/100000: episode: 1295, duration: 0.188s, episode steps: 33, steps per second: 175, episode reward: 10.378, mean reward: 0.314 [0.172, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.035, 10.367], loss: 0.002597, mae: 0.056359, mean_q: 0.635139
 69243/100000: episode: 1296, duration: 0.127s, episode steps: 25, steps per second: 196, episode reward: 12.057, mean reward: 0.482 [0.375, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.678, 10.538], loss: 0.002880, mae: 0.059380, mean_q: 0.621532
 69276/100000: episode: 1297, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 10.778, mean reward: 0.327 [0.186, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.330, 10.336], loss: 0.002860, mae: 0.060238, mean_q: 0.615234
 69307/100000: episode: 1298, duration: 0.163s, episode steps: 31, steps per second: 191, episode reward: 16.405, mean reward: 0.529 [0.471, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.630, 10.544], loss: 0.002843, mae: 0.058307, mean_q: 0.612708
 69332/100000: episode: 1299, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 12.364, mean reward: 0.495 [0.404, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.715, 10.511], loss: 0.002534, mae: 0.055408, mean_q: 0.631139
 69362/100000: episode: 1300, duration: 0.146s, episode steps: 30, steps per second: 206, episode reward: 15.304, mean reward: 0.510 [0.324, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.035, 10.433], loss: 0.002702, mae: 0.058104, mean_q: 0.637157
 69393/100000: episode: 1301, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 11.073, mean reward: 0.357 [0.177, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.700, 10.274], loss: 0.002687, mae: 0.056756, mean_q: 0.635131
 69424/100000: episode: 1302, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 9.793, mean reward: 0.316 [0.186, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.238, 10.365], loss: 0.003103, mae: 0.061193, mean_q: 0.636477
 69435/100000: episode: 1303, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 5.950, mean reward: 0.541 [0.432, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-1.436, 10.530], loss: 0.003043, mae: 0.060594, mean_q: 0.679318
 69446/100000: episode: 1304, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 6.183, mean reward: 0.562 [0.519, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.585], loss: 0.002935, mae: 0.058450, mean_q: 0.655753
[Info] FALSIFICATION!
 69450/100000: episode: 1305, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 11.957, mean reward: 2.989 [0.639, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.015, 10.666], loss: 0.003023, mae: 0.055521, mean_q: 0.634855
 69550/100000: episode: 1306, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.369, mean reward: -0.174 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.563, 10.173], loss: 0.002921, mae: 0.059381, mean_q: 0.631688
 69650/100000: episode: 1307, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.719, mean reward: -0.187 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.637, 10.181], loss: 0.031299, mae: 0.086149, mean_q: 0.617685
 69750/100000: episode: 1308, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.692, mean reward: -0.197 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.087, 10.230], loss: 0.004505, mae: 0.063172, mean_q: 0.605700
 69850/100000: episode: 1309, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -18.642, mean reward: -0.186 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.430, 10.098], loss: 0.016178, mae: 0.064713, mean_q: 0.573367
 69950/100000: episode: 1310, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.924, mean reward: -0.179 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.850, 10.225], loss: 0.016781, mae: 0.070968, mean_q: 0.584453
 70050/100000: episode: 1311, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.555, mean reward: -0.166 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.044, 10.098], loss: 0.003333, mae: 0.060202, mean_q: 0.533342
 70150/100000: episode: 1312, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.623, mean reward: -0.176 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.482, 10.134], loss: 0.016360, mae: 0.068648, mean_q: 0.521268
 70250/100000: episode: 1313, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.233, mean reward: -0.192 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.067, 10.217], loss: 0.002855, mae: 0.057612, mean_q: 0.516479
 70350/100000: episode: 1314, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.751, mean reward: -0.188 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.268, 10.361], loss: 0.003329, mae: 0.058018, mean_q: 0.481722
 70450/100000: episode: 1315, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.865, mean reward: -0.159 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.230, 10.138], loss: 0.016585, mae: 0.069498, mean_q: 0.469236
 70550/100000: episode: 1316, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -18.089, mean reward: -0.181 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.238, 10.098], loss: 0.002879, mae: 0.056678, mean_q: 0.444957
 70650/100000: episode: 1317, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.253, mean reward: -0.173 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.538, 10.159], loss: 0.002589, mae: 0.054034, mean_q: 0.425762
 70750/100000: episode: 1318, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -13.704, mean reward: -0.137 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.335, 10.432], loss: 0.003322, mae: 0.056599, mean_q: 0.413034
 70850/100000: episode: 1319, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -15.058, mean reward: -0.151 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.096, 10.298], loss: 0.002795, mae: 0.056214, mean_q: 0.401043
 70950/100000: episode: 1320, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.606, mean reward: -0.186 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.432, 10.098], loss: 0.002638, mae: 0.054411, mean_q: 0.379113
 71050/100000: episode: 1321, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.981, mean reward: -0.160 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.436, 10.098], loss: 0.016257, mae: 0.067016, mean_q: 0.358836
 71150/100000: episode: 1322, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.942, mean reward: -0.169 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.973, 10.310], loss: 0.002708, mae: 0.055435, mean_q: 0.338491
 71250/100000: episode: 1323, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.530, mean reward: -0.165 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.347, 10.184], loss: 0.015538, mae: 0.061975, mean_q: 0.320910
 71350/100000: episode: 1324, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.083, mean reward: -0.161 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.006, 10.134], loss: 0.002510, mae: 0.052542, mean_q: 0.286846
 71450/100000: episode: 1325, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.652, mean reward: -0.167 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.776, 10.110], loss: 0.015800, mae: 0.062856, mean_q: 0.267960
 71550/100000: episode: 1326, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -15.910, mean reward: -0.159 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.758, 10.098], loss: 0.015795, mae: 0.064623, mean_q: 0.291743
 71650/100000: episode: 1327, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.621, mean reward: -0.166 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.458, 10.196], loss: 0.002537, mae: 0.052639, mean_q: 0.266406
 71750/100000: episode: 1328, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.774, mean reward: -0.168 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.479, 10.098], loss: 0.002622, mae: 0.053234, mean_q: 0.239012
 71850/100000: episode: 1329, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.647, mean reward: -0.166 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.462, 10.286], loss: 0.002385, mae: 0.051376, mean_q: 0.185398
 71950/100000: episode: 1330, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -19.231, mean reward: -0.192 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.193, 10.164], loss: 0.002525, mae: 0.052137, mean_q: 0.175087
 72050/100000: episode: 1331, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.955, mean reward: -0.180 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.859, 10.098], loss: 0.006274, mae: 0.068014, mean_q: 0.185839
 72150/100000: episode: 1332, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -20.379, mean reward: -0.204 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.492, 10.153], loss: 0.006674, mae: 0.066933, mean_q: 0.152932
 72250/100000: episode: 1333, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.527, mean reward: -0.165 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.652, 10.157], loss: 0.004964, mae: 0.060691, mean_q: 0.130025
 72350/100000: episode: 1334, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -20.005, mean reward: -0.200 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.814, 10.133], loss: 0.004762, mae: 0.061520, mean_q: 0.125596
 72450/100000: episode: 1335, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.602, mean reward: -0.186 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.862, 10.142], loss: 0.031257, mae: 0.082943, mean_q: 0.092831
 72550/100000: episode: 1336, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.953, mean reward: -0.170 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.314, 10.180], loss: 0.004874, mae: 0.062544, mean_q: 0.051409
 72650/100000: episode: 1337, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -15.341, mean reward: -0.153 [-1.000, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.424, 10.196], loss: 0.004474, mae: 0.060563, mean_q: 0.036002
 72750/100000: episode: 1338, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -11.458, mean reward: -0.115 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.700, 10.377], loss: 0.004047, mae: 0.060424, mean_q: 0.054961
 72850/100000: episode: 1339, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -15.412, mean reward: -0.154 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.487, 10.379], loss: 0.003056, mae: 0.055564, mean_q: -0.031343
 72950/100000: episode: 1340, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.656, mean reward: -0.157 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.604, 10.154], loss: 0.003433, mae: 0.056251, mean_q: -0.020194
 73050/100000: episode: 1341, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -18.251, mean reward: -0.183 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.263, 10.098], loss: 0.004217, mae: 0.061066, mean_q: -0.054936
 73150/100000: episode: 1342, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.765, mean reward: -0.158 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.430, 10.268], loss: 0.002335, mae: 0.048832, mean_q: -0.048821
 73250/100000: episode: 1343, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.540, mean reward: -0.175 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.053, 10.098], loss: 0.016116, mae: 0.062721, mean_q: -0.091858
 73350/100000: episode: 1344, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.361, mean reward: -0.174 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.362, 10.098], loss: 0.016102, mae: 0.063054, mean_q: -0.088444
 73450/100000: episode: 1345, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.565, mean reward: -0.176 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.789, 10.098], loss: 0.015452, mae: 0.057295, mean_q: -0.118659
 73550/100000: episode: 1346, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.335, mean reward: -0.193 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.915, 10.098], loss: 0.028652, mae: 0.066586, mean_q: -0.167364
 73650/100000: episode: 1347, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -15.506, mean reward: -0.155 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.330, 10.098], loss: 0.002611, mae: 0.051251, mean_q: -0.141309
 73750/100000: episode: 1348, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.038, mean reward: -0.180 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.945, 10.185], loss: 0.002364, mae: 0.048263, mean_q: -0.173918
 73850/100000: episode: 1349, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.328, mean reward: -0.163 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.953, 10.098], loss: 0.015381, mae: 0.057546, mean_q: -0.193904
 73950/100000: episode: 1350, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.300, mean reward: -0.153 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.709, 10.223], loss: 0.002369, mae: 0.048061, mean_q: -0.229078
 74050/100000: episode: 1351, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.467, mean reward: -0.175 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.710, 10.098], loss: 0.028987, mae: 0.063790, mean_q: -0.207187
 74150/100000: episode: 1352, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.052, mean reward: -0.171 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.936, 10.098], loss: 0.003212, mae: 0.055072, mean_q: -0.260102
 74250/100000: episode: 1353, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.751, mean reward: -0.188 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.743, 10.098], loss: 0.002375, mae: 0.047690, mean_q: -0.273578
 74350/100000: episode: 1354, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -19.530, mean reward: -0.195 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.669, 10.098], loss: 0.015370, mae: 0.055340, mean_q: -0.334011
 74450/100000: episode: 1355, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.184, mean reward: -0.182 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.173, 10.245], loss: 0.002858, mae: 0.053270, mean_q: -0.317488
 74550/100000: episode: 1356, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.411, mean reward: -0.194 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.813, 10.114], loss: 0.002431, mae: 0.048717, mean_q: -0.320470
 74650/100000: episode: 1357, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.809, mean reward: -0.178 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.683, 10.147], loss: 0.002578, mae: 0.049735, mean_q: -0.334703
 74750/100000: episode: 1358, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -15.704, mean reward: -0.157 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.523, 10.098], loss: 0.002293, mae: 0.045966, mean_q: -0.357490
 74850/100000: episode: 1359, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -14.310, mean reward: -0.143 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.806, 10.098], loss: 0.002430, mae: 0.048140, mean_q: -0.314923
 74950/100000: episode: 1360, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.500, mean reward: -0.195 [-1.000, 0.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.837, 10.265], loss: 0.002386, mae: 0.047272, mean_q: -0.314908
 75050/100000: episode: 1361, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -13.469, mean reward: -0.135 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.547, 10.098], loss: 0.002484, mae: 0.049447, mean_q: -0.292496
 75150/100000: episode: 1362, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.018, mean reward: -0.160 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.728, 10.098], loss: 0.002375, mae: 0.047446, mean_q: -0.321379
 75250/100000: episode: 1363, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.024, mean reward: -0.140 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.847, 10.343], loss: 0.002400, mae: 0.048068, mean_q: -0.346582
 75350/100000: episode: 1364, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -14.550, mean reward: -0.146 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.661, 10.098], loss: 0.002277, mae: 0.047435, mean_q: -0.277362
 75450/100000: episode: 1365, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.198, mean reward: -0.182 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.002, 10.098], loss: 0.002315, mae: 0.046848, mean_q: -0.317311
 75550/100000: episode: 1366, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -12.913, mean reward: -0.129 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.274, 10.098], loss: 0.002335, mae: 0.047927, mean_q: -0.310015
 75650/100000: episode: 1367, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.656, mean reward: -0.177 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.228, 10.098], loss: 0.002345, mae: 0.048252, mean_q: -0.327499
 75750/100000: episode: 1368, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.788, mean reward: -0.198 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.899, 10.098], loss: 0.002372, mae: 0.048077, mean_q: -0.312088
 75850/100000: episode: 1369, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -14.570, mean reward: -0.146 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.063, 10.098], loss: 0.002738, mae: 0.048528, mean_q: -0.310044
 75950/100000: episode: 1370, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.621, mean reward: -0.176 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.459, 10.098], loss: 0.006011, mae: 0.071221, mean_q: -0.307765
 76050/100000: episode: 1371, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -12.692, mean reward: -0.127 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.837, 10.544], loss: 0.005610, mae: 0.063817, mean_q: -0.307242
 76150/100000: episode: 1372, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -17.497, mean reward: -0.175 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.635, 10.179], loss: 0.002707, mae: 0.051270, mean_q: -0.341845
 76250/100000: episode: 1373, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.797, mean reward: -0.168 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.025, 10.436], loss: 0.002622, mae: 0.049824, mean_q: -0.320769
 76350/100000: episode: 1374, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.344, mean reward: -0.183 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.587, 10.098], loss: 0.002361, mae: 0.047080, mean_q: -0.308764
 76450/100000: episode: 1375, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -14.571, mean reward: -0.146 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.636, 10.098], loss: 0.002251, mae: 0.047033, mean_q: -0.321953
 76550/100000: episode: 1376, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.318, mean reward: -0.163 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.194, 10.356], loss: 0.002323, mae: 0.046515, mean_q: -0.336169
 76650/100000: episode: 1377, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -14.626, mean reward: -0.146 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.647, 10.286], loss: 0.002459, mae: 0.048970, mean_q: -0.308886
 76750/100000: episode: 1378, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.910, mean reward: -0.169 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.751, 10.207], loss: 0.002299, mae: 0.047092, mean_q: -0.339349
 76850/100000: episode: 1379, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -15.741, mean reward: -0.157 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.321, 10.273], loss: 0.002451, mae: 0.048504, mean_q: -0.312438
 76950/100000: episode: 1380, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.786, mean reward: -0.178 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.405, 10.098], loss: 0.002558, mae: 0.049629, mean_q: -0.302892
 77050/100000: episode: 1381, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -17.780, mean reward: -0.178 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.213, 10.098], loss: 0.002424, mae: 0.048985, mean_q: -0.277687
 77150/100000: episode: 1382, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.539, mean reward: -0.155 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.909, 10.098], loss: 0.002549, mae: 0.049861, mean_q: -0.290976
 77250/100000: episode: 1383, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -16.438, mean reward: -0.164 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.753, 10.098], loss: 0.002387, mae: 0.047525, mean_q: -0.286184
 77350/100000: episode: 1384, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -15.615, mean reward: -0.156 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.072, 10.098], loss: 0.002314, mae: 0.047216, mean_q: -0.312693
 77450/100000: episode: 1385, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.628, mean reward: -0.166 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.285, 10.098], loss: 0.002522, mae: 0.049387, mean_q: -0.308343
 77550/100000: episode: 1386, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -16.565, mean reward: -0.166 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.565, 10.150], loss: 0.002330, mae: 0.046825, mean_q: -0.309904
 77650/100000: episode: 1387, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.866, mean reward: -0.199 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.981, 10.123], loss: 0.002385, mae: 0.047760, mean_q: -0.330503
 77750/100000: episode: 1388, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.813, mean reward: -0.188 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.633, 10.098], loss: 0.002339, mae: 0.048048, mean_q: -0.295781
 77850/100000: episode: 1389, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.136, mean reward: -0.171 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.485, 10.099], loss: 0.002367, mae: 0.047942, mean_q: -0.312548
 77950/100000: episode: 1390, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -12.004, mean reward: -0.120 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.406, 10.263], loss: 0.002408, mae: 0.048578, mean_q: -0.276942
 78050/100000: episode: 1391, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -7.995, mean reward: -0.080 [-1.000, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.504, 10.576], loss: 0.002422, mae: 0.047922, mean_q: -0.311140
 78150/100000: episode: 1392, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.525, mean reward: -0.155 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.301, 10.129], loss: 0.002412, mae: 0.047805, mean_q: -0.341440
 78250/100000: episode: 1393, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -9.758, mean reward: -0.098 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.963, 10.441], loss: 0.002386, mae: 0.047725, mean_q: -0.313871
 78350/100000: episode: 1394, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.319, mean reward: -0.163 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.330, 10.281], loss: 0.002219, mae: 0.046807, mean_q: -0.320576
 78450/100000: episode: 1395, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -13.705, mean reward: -0.137 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.814, 10.314], loss: 0.002410, mae: 0.049859, mean_q: -0.273216
 78550/100000: episode: 1396, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -19.148, mean reward: -0.191 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.460, 10.310], loss: 0.002328, mae: 0.048760, mean_q: -0.311833
 78650/100000: episode: 1397, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -19.474, mean reward: -0.195 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.447, 10.192], loss: 0.003773, mae: 0.057878, mean_q: -0.326446
 78750/100000: episode: 1398, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -19.196, mean reward: -0.192 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.335, 10.098], loss: 0.002340, mae: 0.048049, mean_q: -0.305407
 78850/100000: episode: 1399, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.663, mean reward: -0.157 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.748, 10.098], loss: 0.002948, mae: 0.053171, mean_q: -0.315465
 78950/100000: episode: 1400, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.760, mean reward: -0.178 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.704, 10.240], loss: 0.002469, mae: 0.048827, mean_q: -0.312253
 79050/100000: episode: 1401, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.792, mean reward: -0.188 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.277, 10.223], loss: 0.002270, mae: 0.046914, mean_q: -0.290909
 79150/100000: episode: 1402, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.134, mean reward: -0.171 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.871, 10.098], loss: 0.002183, mae: 0.046791, mean_q: -0.284355
 79250/100000: episode: 1403, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -13.632, mean reward: -0.136 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.568, 10.480], loss: 0.002337, mae: 0.047569, mean_q: -0.310818
 79350/100000: episode: 1404, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.294, mean reward: -0.173 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.310, 10.208], loss: 0.002392, mae: 0.047648, mean_q: -0.341566
[Info] 100-TH LEVEL FOUND: 0.6345527768135071, Considering 10/90 traces
 79450/100000: episode: 1405, duration: 4.312s, episode steps: 100, steps per second: 23, episode reward: -13.736, mean reward: -0.137 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.166, 10.098], loss: 0.002396, mae: 0.047745, mean_q: -0.299800
 79458/100000: episode: 1406, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 3.011, mean reward: 0.376 [0.321, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.417], loss: 0.002481, mae: 0.047559, mean_q: -0.306772
 79520/100000: episode: 1407, duration: 0.313s, episode steps: 62, steps per second: 198, episode reward: 14.400, mean reward: 0.232 [0.044, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.800 [-0.086, 10.100], loss: 0.002308, mae: 0.046966, mean_q: -0.302983
 79534/100000: episode: 1408, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 4.582, mean reward: 0.327 [0.254, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.314, 10.100], loss: 0.002560, mae: 0.048800, mean_q: -0.290879
 79574/100000: episode: 1409, duration: 0.204s, episode steps: 40, steps per second: 196, episode reward: 9.378, mean reward: 0.234 [0.035, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.802, 10.100], loss: 0.002527, mae: 0.049480, mean_q: -0.329900
 79592/100000: episode: 1410, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 6.856, mean reward: 0.381 [0.321, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.560, 10.446], loss: 0.002269, mae: 0.046197, mean_q: -0.308698
 79606/100000: episode: 1411, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 5.053, mean reward: 0.361 [0.239, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.360, 10.100], loss: 0.002285, mae: 0.047150, mean_q: -0.342583
 79646/100000: episode: 1412, duration: 0.216s, episode steps: 40, steps per second: 185, episode reward: 9.440, mean reward: 0.236 [0.075, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-1.251, 10.260], loss: 0.002504, mae: 0.048972, mean_q: -0.289591
 79654/100000: episode: 1413, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 2.634, mean reward: 0.329 [0.261, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.380], loss: 0.002072, mae: 0.046079, mean_q: -0.276092
 79683/100000: episode: 1414, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 8.794, mean reward: 0.303 [0.192, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.035, 10.257], loss: 0.002543, mae: 0.049695, mean_q: -0.252489
 79729/100000: episode: 1415, duration: 0.246s, episode steps: 46, steps per second: 187, episode reward: 13.970, mean reward: 0.304 [0.159, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.220, 10.280], loss: 0.002396, mae: 0.048793, mean_q: -0.266603
 79791/100000: episode: 1416, duration: 0.304s, episode steps: 62, steps per second: 204, episode reward: 12.417, mean reward: 0.200 [0.007, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.814 [-0.228, 10.121], loss: 0.002340, mae: 0.047965, mean_q: -0.221652
 79812/100000: episode: 1417, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 8.777, mean reward: 0.418 [0.359, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.137, 10.496], loss: 0.002144, mae: 0.046838, mean_q: -0.272496
 79841/100000: episode: 1418, duration: 0.136s, episode steps: 29, steps per second: 213, episode reward: 7.084, mean reward: 0.244 [0.137, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.035, 10.338], loss: 0.002202, mae: 0.046435, mean_q: -0.263624
 79887/100000: episode: 1419, duration: 0.234s, episode steps: 46, steps per second: 196, episode reward: 12.630, mean reward: 0.275 [0.069, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.308, 10.227], loss: 0.002063, mae: 0.045879, mean_q: -0.219177
 79901/100000: episode: 1420, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 3.468, mean reward: 0.248 [0.112, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.129, 10.100], loss: 0.002267, mae: 0.046536, mean_q: -0.272091
 79910/100000: episode: 1421, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 3.044, mean reward: 0.338 [0.310, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.427], loss: 0.002419, mae: 0.049477, mean_q: -0.212207
 79924/100000: episode: 1422, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 4.108, mean reward: 0.293 [0.231, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.869, 10.100], loss: 0.002760, mae: 0.053350, mean_q: -0.119032
 79932/100000: episode: 1423, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 2.939, mean reward: 0.367 [0.310, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.289, 10.468], loss: 0.002223, mae: 0.047400, mean_q: -0.185766
 79978/100000: episode: 1424, duration: 0.223s, episode steps: 46, steps per second: 206, episode reward: 10.289, mean reward: 0.224 [0.033, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.489, 10.176], loss: 0.002435, mae: 0.050048, mean_q: -0.195365
 80024/100000: episode: 1425, duration: 0.236s, episode steps: 46, steps per second: 195, episode reward: 11.676, mean reward: 0.254 [0.057, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.405, 10.305], loss: 0.002306, mae: 0.048987, mean_q: -0.238905
 80038/100000: episode: 1426, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 3.974, mean reward: 0.284 [0.226, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.035, 10.100], loss: 0.002218, mae: 0.046580, mean_q: -0.265943
 80046/100000: episode: 1427, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 3.088, mean reward: 0.386 [0.305, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.379, 10.431], loss: 0.002119, mae: 0.046076, mean_q: -0.228017
 80055/100000: episode: 1428, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 2.762, mean reward: 0.307 [0.249, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.335], loss: 0.002401, mae: 0.049323, mean_q: -0.112783
 80063/100000: episode: 1429, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 2.848, mean reward: 0.356 [0.314, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.466], loss: 0.002004, mae: 0.045033, mean_q: -0.235957
 80109/100000: episode: 1430, duration: 0.235s, episode steps: 46, steps per second: 196, episode reward: 12.712, mean reward: 0.276 [0.034, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.221, 10.218], loss: 0.002743, mae: 0.053931, mean_q: -0.166094
 80138/100000: episode: 1431, duration: 0.144s, episode steps: 29, steps per second: 201, episode reward: 9.072, mean reward: 0.313 [0.172, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.980, 10.308], loss: 0.002837, mae: 0.055144, mean_q: -0.100131
 80146/100000: episode: 1432, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 3.175, mean reward: 0.397 [0.299, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.054, 10.487], loss: 0.002935, mae: 0.054511, mean_q: -0.093310
 80208/100000: episode: 1433, duration: 0.316s, episode steps: 62, steps per second: 196, episode reward: 19.305, mean reward: 0.311 [0.085, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.797 [-0.289, 10.100], loss: 0.002507, mae: 0.051435, mean_q: -0.157352
 80226/100000: episode: 1434, duration: 0.086s, episode steps: 18, steps per second: 210, episode reward: 6.158, mean reward: 0.342 [0.236, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.426], loss: 0.002519, mae: 0.049456, mean_q: -0.198422
 80244/100000: episode: 1435, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 6.691, mean reward: 0.372 [0.280, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-1.095, 10.434], loss: 0.002420, mae: 0.048828, mean_q: -0.157671
 80306/100000: episode: 1436, duration: 0.301s, episode steps: 62, steps per second: 206, episode reward: 14.202, mean reward: 0.229 [0.047, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.811 [-0.526, 10.240], loss: 0.002345, mae: 0.049318, mean_q: -0.189820
 80327/100000: episode: 1437, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 6.117, mean reward: 0.291 [0.201, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.325], loss: 0.002900, mae: 0.052173, mean_q: -0.185876
 80336/100000: episode: 1438, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 3.205, mean reward: 0.356 [0.302, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.276, 10.411], loss: 0.002613, mae: 0.052063, mean_q: -0.211980
 80354/100000: episode: 1439, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 8.022, mean reward: 0.446 [0.343, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.745, 10.538], loss: 0.002623, mae: 0.052764, mean_q: -0.170945
 80372/100000: episode: 1440, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 5.607, mean reward: 0.312 [0.194, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.036, 10.367], loss: 0.002070, mae: 0.045967, mean_q: -0.189581
 80401/100000: episode: 1441, duration: 0.152s, episode steps: 29, steps per second: 190, episode reward: 8.236, mean reward: 0.284 [0.130, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.630, 10.339], loss: 0.002438, mae: 0.049948, mean_q: -0.090974
 80409/100000: episode: 1442, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 2.525, mean reward: 0.316 [0.283, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.431], loss: 0.002983, mae: 0.055205, mean_q: -0.050075
 80430/100000: episode: 1443, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 6.949, mean reward: 0.331 [0.233, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.462], loss: 0.002270, mae: 0.048017, mean_q: -0.098311
 80459/100000: episode: 1444, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 10.371, mean reward: 0.358 [0.259, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.107, 10.344], loss: 0.002271, mae: 0.047474, mean_q: -0.145629
 80488/100000: episode: 1445, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 13.110, mean reward: 0.452 [0.389, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.841, 10.660], loss: 0.002530, mae: 0.051767, mean_q: -0.092888
 80550/100000: episode: 1446, duration: 0.298s, episode steps: 62, steps per second: 208, episode reward: 10.700, mean reward: 0.173 [0.056, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.801 [-0.226, 10.100], loss: 0.002303, mae: 0.048798, mean_q: -0.139401
 80579/100000: episode: 1447, duration: 0.159s, episode steps: 29, steps per second: 183, episode reward: 8.808, mean reward: 0.304 [0.180, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.599, 10.344], loss: 0.002348, mae: 0.048118, mean_q: -0.078505
 80593/100000: episode: 1448, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 4.135, mean reward: 0.295 [0.240, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.237, 10.100], loss: 0.002919, mae: 0.054763, mean_q: -0.086888
 80611/100000: episode: 1449, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 5.359, mean reward: 0.298 [0.236, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.434], loss: 0.002728, mae: 0.053893, mean_q: -0.070886
 80619/100000: episode: 1450, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 2.474, mean reward: 0.309 [0.266, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.419], loss: 0.003194, mae: 0.059904, mean_q: -0.109357
 80627/100000: episode: 1451, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 3.062, mean reward: 0.383 [0.298, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.401, 10.487], loss: 0.002445, mae: 0.051005, mean_q: -0.115959
 80641/100000: episode: 1452, duration: 0.068s, episode steps: 14, steps per second: 207, episode reward: 3.839, mean reward: 0.274 [0.180, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.115, 10.100], loss: 0.002020, mae: 0.046502, mean_q: -0.119171
 80670/100000: episode: 1453, duration: 0.144s, episode steps: 29, steps per second: 201, episode reward: 8.387, mean reward: 0.289 [0.153, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.307, 10.284], loss: 0.002191, mae: 0.048187, mean_q: -0.138606
 80732/100000: episode: 1454, duration: 0.328s, episode steps: 62, steps per second: 189, episode reward: 9.836, mean reward: 0.159 [0.031, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.810 [-0.307, 10.136], loss: 0.002281, mae: 0.048810, mean_q: -0.080780
 80740/100000: episode: 1455, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 2.720, mean reward: 0.340 [0.252, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.709, 10.407], loss: 0.002765, mae: 0.055230, mean_q: -0.093784
 80769/100000: episode: 1456, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 8.910, mean reward: 0.307 [0.127, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.683, 10.373], loss: 0.002977, mae: 0.057156, mean_q: 0.006291
 80815/100000: episode: 1457, duration: 0.227s, episode steps: 46, steps per second: 203, episode reward: 12.070, mean reward: 0.262 [0.015, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.624, 10.100], loss: 0.002424, mae: 0.049206, mean_q: -0.076424
 80861/100000: episode: 1458, duration: 0.252s, episode steps: 46, steps per second: 182, episode reward: 21.943, mean reward: 0.477 [0.309, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.588, 10.565], loss: 0.002350, mae: 0.050050, mean_q: -0.051345
 80901/100000: episode: 1459, duration: 0.224s, episode steps: 40, steps per second: 179, episode reward: 7.948, mean reward: 0.199 [0.069, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.420, 10.208], loss: 0.002599, mae: 0.052363, mean_q: -0.023648
 80909/100000: episode: 1460, duration: 0.040s, episode steps: 8, steps per second: 202, episode reward: 2.666, mean reward: 0.333 [0.275, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.345], loss: 0.002717, mae: 0.052462, mean_q: -0.098293
 80971/100000: episode: 1461, duration: 0.312s, episode steps: 62, steps per second: 199, episode reward: 14.895, mean reward: 0.240 [0.042, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.813 [-0.800, 10.190], loss: 0.002695, mae: 0.053280, mean_q: -0.043130
 81000/100000: episode: 1462, duration: 0.168s, episode steps: 29, steps per second: 173, episode reward: 9.457, mean reward: 0.326 [0.166, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.084, 10.360], loss: 0.002581, mae: 0.052063, mean_q: -0.044152
 81008/100000: episode: 1463, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 2.878, mean reward: 0.360 [0.282, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.442], loss: 0.002241, mae: 0.049896, mean_q: -0.088573
 81070/100000: episode: 1464, duration: 0.318s, episode steps: 62, steps per second: 195, episode reward: 10.539, mean reward: 0.170 [0.027, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.813 [-0.645, 10.100], loss: 0.002370, mae: 0.051394, mean_q: -0.065978
 81078/100000: episode: 1465, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 3.168, mean reward: 0.396 [0.346, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.525], loss: 0.002755, mae: 0.055213, mean_q: 0.046825
 81087/100000: episode: 1466, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 3.897, mean reward: 0.433 [0.328, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.512], loss: 0.002089, mae: 0.047821, mean_q: -0.102158
 81095/100000: episode: 1467, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 3.206, mean reward: 0.401 [0.340, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.813, 10.493], loss: 0.002647, mae: 0.052119, mean_q: -0.031300
 81103/100000: episode: 1468, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 2.756, mean reward: 0.344 [0.297, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.435], loss: 0.019040, mae: 0.077842, mean_q: -0.014290
 81124/100000: episode: 1469, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 7.300, mean reward: 0.348 [0.252, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.241, 10.476], loss: 0.009404, mae: 0.093338, mean_q: 0.003917
 81186/100000: episode: 1470, duration: 0.308s, episode steps: 62, steps per second: 202, episode reward: 16.537, mean reward: 0.267 [0.053, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.808 [-0.565, 10.100], loss: 0.004379, mae: 0.067394, mean_q: -0.013648
 81200/100000: episode: 1471, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 4.006, mean reward: 0.286 [0.139, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.181, 10.100], loss: 0.004447, mae: 0.065181, mean_q: 0.039453
 81229/100000: episode: 1472, duration: 0.140s, episode steps: 29, steps per second: 207, episode reward: 8.921, mean reward: 0.308 [0.133, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.921, 10.279], loss: 0.004571, mae: 0.061940, mean_q: -0.029165
 81250/100000: episode: 1473, duration: 0.116s, episode steps: 21, steps per second: 180, episode reward: 7.911, mean reward: 0.377 [0.298, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.195, 10.435], loss: 0.003744, mae: 0.060631, mean_q: 0.056554
 81268/100000: episode: 1474, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 6.245, mean reward: 0.347 [0.252, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.510, 10.460], loss: 0.002510, mae: 0.054337, mean_q: 0.019244
 81282/100000: episode: 1475, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 4.716, mean reward: 0.337 [0.281, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.147, 10.100], loss: 0.002535, mae: 0.053131, mean_q: 0.013123
 81290/100000: episode: 1476, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 2.613, mean reward: 0.327 [0.261, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.355], loss: 0.002133, mae: 0.049792, mean_q: 0.026563
 81330/100000: episode: 1477, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 14.558, mean reward: 0.364 [0.229, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.148, 10.429], loss: 0.002471, mae: 0.050628, mean_q: 0.019793
 81338/100000: episode: 1478, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 3.623, mean reward: 0.453 [0.387, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.681, 10.592], loss: 0.002379, mae: 0.050685, mean_q: 0.079159
 81346/100000: episode: 1479, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 2.826, mean reward: 0.353 [0.301, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.484], loss: 0.002856, mae: 0.053408, mean_q: 0.000043
 81354/100000: episode: 1480, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 2.856, mean reward: 0.357 [0.326, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.471], loss: 0.002213, mae: 0.048353, mean_q: -0.049133
 81362/100000: episode: 1481, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 3.000, mean reward: 0.375 [0.348, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.578, 10.417], loss: 0.002603, mae: 0.051519, mean_q: 0.030770
 81376/100000: episode: 1482, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 4.192, mean reward: 0.299 [0.200, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.588, 10.100], loss: 0.002721, mae: 0.054813, mean_q: 0.034255
 81384/100000: episode: 1483, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 2.360, mean reward: 0.295 [0.230, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.117, 10.376], loss: 0.002605, mae: 0.054252, mean_q: -0.026664
 81446/100000: episode: 1484, duration: 0.311s, episode steps: 62, steps per second: 199, episode reward: 13.213, mean reward: 0.213 [0.048, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.812 [-0.558, 10.241], loss: 0.002915, mae: 0.057032, mean_q: 0.070238
 81475/100000: episode: 1485, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 11.723, mean reward: 0.404 [0.328, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.467, 10.376], loss: 0.002315, mae: 0.049254, mean_q: 0.023073
 81515/100000: episode: 1486, duration: 0.195s, episode steps: 40, steps per second: 205, episode reward: 9.292, mean reward: 0.232 [0.048, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.885, 10.100], loss: 0.002583, mae: 0.053012, mean_q: 0.060645
 81536/100000: episode: 1487, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 8.554, mean reward: 0.407 [0.341, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.506], loss: 0.002484, mae: 0.053004, mean_q: 0.066293
 81544/100000: episode: 1488, duration: 0.050s, episode steps: 8, steps per second: 162, episode reward: 3.155, mean reward: 0.394 [0.332, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.520], loss: 0.003242, mae: 0.059212, mean_q: 0.018503
 81558/100000: episode: 1489, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 4.033, mean reward: 0.288 [0.159, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.060, 10.100], loss: 0.002501, mae: 0.052302, mean_q: 0.055927
 81576/100000: episode: 1490, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 4.828, mean reward: 0.268 [0.050, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.588, 10.216], loss: 0.002464, mae: 0.051615, mean_q: 0.102206
 81605/100000: episode: 1491, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 10.987, mean reward: 0.379 [0.281, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.504, 10.477], loss: 0.002674, mae: 0.053093, mean_q: 0.061821
 81634/100000: episode: 1492, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 10.042, mean reward: 0.346 [0.257, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.035, 10.442], loss: 0.002798, mae: 0.055698, mean_q: 0.073060
 81674/100000: episode: 1493, duration: 0.211s, episode steps: 40, steps per second: 190, episode reward: 17.216, mean reward: 0.430 [0.236, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-1.735, 10.332], loss: 0.002338, mae: 0.051465, mean_q: 0.096499
 81703/100000: episode: 1494, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 6.618, mean reward: 0.228 [0.061, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.559, 10.335], loss: 0.002672, mae: 0.053420, mean_q: 0.104405
[Info] 200-TH LEVEL FOUND: 0.8507251739501953, Considering 10/90 traces
 81721/100000: episode: 1495, duration: 3.922s, episode steps: 18, steps per second: 5, episode reward: 7.247, mean reward: 0.403 [0.222, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.616, 10.404], loss: 0.002350, mae: 0.048779, mean_q: 0.109816
 81733/100000: episode: 1496, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 5.015, mean reward: 0.418 [0.321, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.387], loss: 0.002088, mae: 0.046794, mean_q: 0.113210
 81758/100000: episode: 1497, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 8.609, mean reward: 0.344 [0.201, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.554, 10.538], loss: 0.002639, mae: 0.052646, mean_q: 0.090645
 81768/100000: episode: 1498, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 4.782, mean reward: 0.478 [0.440, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.112, 10.601], loss: 0.002551, mae: 0.054399, mean_q: 0.083054
 81804/100000: episode: 1499, duration: 0.184s, episode steps: 36, steps per second: 195, episode reward: 9.251, mean reward: 0.257 [0.023, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.728, 10.179], loss: 0.002721, mae: 0.053951, mean_q: 0.090145
 81815/100000: episode: 1500, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 5.365, mean reward: 0.488 [0.401, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-1.156, 10.578], loss: 0.002765, mae: 0.056810, mean_q: 0.165367
 81826/100000: episode: 1501, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 4.378, mean reward: 0.398 [0.203, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.335], loss: 0.002549, mae: 0.052248, mean_q: 0.148989
 81839/100000: episode: 1502, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 5.284, mean reward: 0.406 [0.336, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.568], loss: 0.002357, mae: 0.050421, mean_q: 0.111376
 81850/100000: episode: 1503, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 4.134, mean reward: 0.376 [0.338, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.435, 10.425], loss: 0.002269, mae: 0.048979, mean_q: 0.184971
 81861/100000: episode: 1504, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 5.030, mean reward: 0.457 [0.367, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.067, 10.541], loss: 0.003118, mae: 0.056780, mean_q: 0.088515
 81874/100000: episode: 1505, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 4.019, mean reward: 0.309 [0.161, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.321], loss: 0.002875, mae: 0.056177, mean_q: 0.222325
 81906/100000: episode: 1506, duration: 0.169s, episode steps: 32, steps per second: 190, episode reward: 10.858, mean reward: 0.339 [0.228, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.035, 10.456], loss: 0.002845, mae: 0.057000, mean_q: 0.170824
 81917/100000: episode: 1507, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 5.471, mean reward: 0.497 [0.418, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.504], loss: 0.002831, mae: 0.054592, mean_q: 0.122323
 81927/100000: episode: 1508, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 3.926, mean reward: 0.393 [0.266, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.424], loss: 0.002819, mae: 0.058317, mean_q: 0.271916
 81949/100000: episode: 1509, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 9.860, mean reward: 0.448 [0.356, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.498, 10.531], loss: 0.002425, mae: 0.052788, mean_q: 0.105841
 81981/100000: episode: 1510, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 10.786, mean reward: 0.337 [0.197, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.678, 10.317], loss: 0.002755, mae: 0.054446, mean_q: 0.152627
 81998/100000: episode: 1511, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 7.103, mean reward: 0.418 [0.350, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.557], loss: 0.002743, mae: 0.055145, mean_q: 0.144004
 82010/100000: episode: 1512, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 5.842, mean reward: 0.487 [0.426, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.619], loss: 0.002443, mae: 0.053471, mean_q: 0.174439
 82023/100000: episode: 1513, duration: 0.064s, episode steps: 13, steps per second: 205, episode reward: 6.080, mean reward: 0.468 [0.392, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.605], loss: 0.002432, mae: 0.052618, mean_q: 0.079201
 82055/100000: episode: 1514, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 14.738, mean reward: 0.461 [0.254, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.035, 10.474], loss: 0.002525, mae: 0.052710, mean_q: 0.132493
 82066/100000: episode: 1515, duration: 0.053s, episode steps: 11, steps per second: 208, episode reward: 4.945, mean reward: 0.450 [0.406, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.557, 10.558], loss: 0.002769, mae: 0.054614, mean_q: 0.171124
 82076/100000: episode: 1516, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 5.296, mean reward: 0.530 [0.474, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.131, 10.622], loss: 0.002328, mae: 0.051334, mean_q: 0.212881
 82087/100000: episode: 1517, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 4.613, mean reward: 0.419 [0.320, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-2.032, 10.554], loss: 0.002839, mae: 0.055557, mean_q: 0.186312
 82123/100000: episode: 1518, duration: 0.188s, episode steps: 36, steps per second: 191, episode reward: 9.839, mean reward: 0.273 [0.156, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.035, 10.280], loss: 0.002927, mae: 0.057385, mean_q: 0.198156
 82136/100000: episode: 1519, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 5.761, mean reward: 0.443 [0.382, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.139, 10.468], loss: 0.002634, mae: 0.054586, mean_q: 0.284795
 82147/100000: episode: 1520, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 4.582, mean reward: 0.417 [0.343, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.540], loss: 0.002658, mae: 0.051788, mean_q: 0.131788
 82158/100000: episode: 1521, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 4.393, mean reward: 0.399 [0.329, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-1.746, 10.393], loss: 0.002553, mae: 0.052654, mean_q: 0.147177
 82175/100000: episode: 1522, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 8.131, mean reward: 0.478 [0.380, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.077, 10.508], loss: 0.002915, mae: 0.057475, mean_q: 0.253594
 82185/100000: episode: 1523, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 4.425, mean reward: 0.442 [0.290, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.551], loss: 0.003142, mae: 0.060933, mean_q: 0.245066
 82210/100000: episode: 1524, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 9.577, mean reward: 0.383 [0.243, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.526, 10.365], loss: 0.002843, mae: 0.056390, mean_q: 0.179690
 82221/100000: episode: 1525, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 5.301, mean reward: 0.482 [0.429, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.212, 10.507], loss: 0.002520, mae: 0.051548, mean_q: 0.155438
 82233/100000: episode: 1526, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 4.727, mean reward: 0.394 [0.358, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.503], loss: 0.002757, mae: 0.054919, mean_q: 0.213288
 82258/100000: episode: 1527, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 8.702, mean reward: 0.348 [0.146, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.070, 10.277], loss: 0.002777, mae: 0.056569, mean_q: 0.219286
 82290/100000: episode: 1528, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 11.948, mean reward: 0.373 [0.205, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.035, 10.439], loss: 0.002704, mae: 0.054712, mean_q: 0.255578
 82326/100000: episode: 1529, duration: 0.175s, episode steps: 36, steps per second: 206, episode reward: 10.982, mean reward: 0.305 [0.203, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.489, 10.285], loss: 0.003049, mae: 0.057937, mean_q: 0.245260
 82362/100000: episode: 1530, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 13.898, mean reward: 0.386 [0.116, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.286, 10.258], loss: 0.002788, mae: 0.057476, mean_q: 0.227198
 82373/100000: episode: 1531, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 4.109, mean reward: 0.374 [0.291, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.457], loss: 0.002274, mae: 0.050956, mean_q: 0.184993
 82385/100000: episode: 1532, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 4.536, mean reward: 0.378 [0.312, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.414], loss: 0.002207, mae: 0.051181, mean_q: 0.257816
 82396/100000: episode: 1533, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 4.674, mean reward: 0.425 [0.366, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.778, 10.522], loss: 0.002949, mae: 0.059166, mean_q: 0.263883
 82408/100000: episode: 1534, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 5.291, mean reward: 0.441 [0.387, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.513], loss: 0.003020, mae: 0.056539, mean_q: 0.146337
 82440/100000: episode: 1535, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 12.475, mean reward: 0.390 [0.251, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.300, 10.516], loss: 0.002690, mae: 0.055176, mean_q: 0.290982
 82452/100000: episode: 1536, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 5.193, mean reward: 0.433 [0.346, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.438], loss: 0.002346, mae: 0.051782, mean_q: 0.165242
 82484/100000: episode: 1537, duration: 0.157s, episode steps: 32, steps per second: 204, episode reward: 12.163, mean reward: 0.380 [0.190, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.386, 10.284], loss: 0.003059, mae: 0.059220, mean_q: 0.248726
 82497/100000: episode: 1538, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 5.779, mean reward: 0.445 [0.399, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.901, 10.573], loss: 0.003438, mae: 0.063702, mean_q: 0.245427
 82522/100000: episode: 1539, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 8.623, mean reward: 0.345 [0.242, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.419, 10.507], loss: 0.002681, mae: 0.054571, mean_q: 0.287513
 82532/100000: episode: 1540, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 4.045, mean reward: 0.404 [0.340, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.534, 10.418], loss: 0.002485, mae: 0.054744, mean_q: 0.361787
 82542/100000: episode: 1541, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 4.354, mean reward: 0.435 [0.317, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.527], loss: 0.003080, mae: 0.058027, mean_q: 0.369840
 82564/100000: episode: 1542, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 7.856, mean reward: 0.357 [0.291, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.394], loss: 0.002924, mae: 0.058074, mean_q: 0.289359
 82575/100000: episode: 1543, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 4.540, mean reward: 0.413 [0.353, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.247, 10.495], loss: 0.002516, mae: 0.053647, mean_q: 0.281777
 82607/100000: episode: 1544, duration: 0.167s, episode steps: 32, steps per second: 191, episode reward: 8.626, mean reward: 0.270 [0.041, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.406, 10.313], loss: 0.002591, mae: 0.053569, mean_q: 0.279332
 82618/100000: episode: 1545, duration: 0.054s, episode steps: 11, steps per second: 204, episode reward: 5.433, mean reward: 0.494 [0.467, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-1.340, 10.587], loss: 0.002626, mae: 0.054108, mean_q: 0.298567
 82635/100000: episode: 1546, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 8.022, mean reward: 0.472 [0.405, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.597], loss: 0.003018, mae: 0.059678, mean_q: 0.323637
 82671/100000: episode: 1547, duration: 0.170s, episode steps: 36, steps per second: 212, episode reward: 14.488, mean reward: 0.402 [0.246, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.586, 10.413], loss: 0.002807, mae: 0.055844, mean_q: 0.330628
 82693/100000: episode: 1548, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 8.666, mean reward: 0.394 [0.339, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.682, 10.527], loss: 0.002603, mae: 0.055157, mean_q: 0.328840
 82710/100000: episode: 1549, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 7.403, mean reward: 0.435 [0.375, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.527], loss: 0.002582, mae: 0.053617, mean_q: 0.302484
 82721/100000: episode: 1550, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 4.731, mean reward: 0.430 [0.370, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.541], loss: 0.002201, mae: 0.049583, mean_q: 0.251001
 82731/100000: episode: 1551, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 3.920, mean reward: 0.392 [0.292, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.431, 10.389], loss: 0.002464, mae: 0.054197, mean_q: 0.328303
 82763/100000: episode: 1552, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 11.121, mean reward: 0.348 [0.180, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.737, 10.352], loss: 0.002292, mae: 0.050903, mean_q: 0.306315
 82785/100000: episode: 1553, duration: 0.133s, episode steps: 22, steps per second: 166, episode reward: 8.616, mean reward: 0.392 [0.332, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.435], loss: 0.002539, mae: 0.053299, mean_q: 0.314852
 82796/100000: episode: 1554, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 4.538, mean reward: 0.413 [0.354, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.477], loss: 0.002560, mae: 0.053383, mean_q: 0.337170
 82807/100000: episode: 1555, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 4.549, mean reward: 0.414 [0.356, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.458], loss: 0.002298, mae: 0.051499, mean_q: 0.386613
 82829/100000: episode: 1556, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 10.989, mean reward: 0.500 [0.402, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.670], loss: 0.002617, mae: 0.055264, mean_q: 0.374672
 82846/100000: episode: 1557, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 7.846, mean reward: 0.462 [0.381, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.573], loss: 0.002728, mae: 0.056109, mean_q: 0.350694
 82871/100000: episode: 1558, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 9.646, mean reward: 0.386 [0.283, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.092, 10.574], loss: 0.002612, mae: 0.054423, mean_q: 0.331113
 82893/100000: episode: 1559, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 5.844, mean reward: 0.266 [0.046, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.923, 10.177], loss: 0.002554, mae: 0.053226, mean_q: 0.339223
 82925/100000: episode: 1560, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 10.507, mean reward: 0.328 [0.163, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.491, 10.300], loss: 0.002890, mae: 0.058000, mean_q: 0.382800
 82937/100000: episode: 1561, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 6.344, mean reward: 0.529 [0.448, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.685], loss: 0.002282, mae: 0.051140, mean_q: 0.328395
 82959/100000: episode: 1562, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 8.252, mean reward: 0.375 [0.204, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.924, 10.367], loss: 0.003118, mae: 0.060816, mean_q: 0.377814
 82970/100000: episode: 1563, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 4.578, mean reward: 0.416 [0.327, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.470], loss: 0.002896, mae: 0.056607, mean_q: 0.371599
 82995/100000: episode: 1564, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 10.650, mean reward: 0.426 [0.378, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.493], loss: 0.002662, mae: 0.055231, mean_q: 0.373687
 83027/100000: episode: 1565, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 12.367, mean reward: 0.386 [0.219, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.388, 10.316], loss: 0.002815, mae: 0.058713, mean_q: 0.410596
 83037/100000: episode: 1566, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 4.428, mean reward: 0.443 [0.386, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.709, 10.434], loss: 0.002773, mae: 0.057591, mean_q: 0.364519
 83050/100000: episode: 1567, duration: 0.062s, episode steps: 13, steps per second: 211, episode reward: 5.838, mean reward: 0.449 [0.337, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.813, 10.494], loss: 0.002640, mae: 0.054133, mean_q: 0.318976
 83072/100000: episode: 1568, duration: 0.111s, episode steps: 22, steps per second: 199, episode reward: 8.530, mean reward: 0.388 [0.163, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.355], loss: 0.003033, mae: 0.057414, mean_q: 0.424050
 83083/100000: episode: 1569, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 5.228, mean reward: 0.475 [0.305, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.374], loss: 0.002586, mae: 0.054697, mean_q: 0.411259
 83105/100000: episode: 1570, duration: 0.103s, episode steps: 22, steps per second: 213, episode reward: 7.141, mean reward: 0.325 [0.148, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.587, 10.317], loss: 0.003293, mae: 0.061760, mean_q: 0.409268
 83137/100000: episode: 1571, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 10.786, mean reward: 0.337 [0.170, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.035, 10.355], loss: 0.002579, mae: 0.054921, mean_q: 0.399023
 83148/100000: episode: 1572, duration: 0.078s, episode steps: 11, steps per second: 142, episode reward: 5.331, mean reward: 0.485 [0.447, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.438, 10.606], loss: 0.002204, mae: 0.050567, mean_q: 0.385784
 83184/100000: episode: 1573, duration: 0.196s, episode steps: 36, steps per second: 183, episode reward: 13.506, mean reward: 0.375 [0.171, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.162, 10.304], loss: 0.002766, mae: 0.056984, mean_q: 0.406554
 83196/100000: episode: 1574, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 6.128, mean reward: 0.511 [0.374, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.949, 10.469], loss: 0.002600, mae: 0.054520, mean_q: 0.388293
 83228/100000: episode: 1575, duration: 0.166s, episode steps: 32, steps per second: 193, episode reward: 15.552, mean reward: 0.486 [0.372, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.420, 10.500], loss: 0.002589, mae: 0.055082, mean_q: 0.417865
 83264/100000: episode: 1576, duration: 0.189s, episode steps: 36, steps per second: 191, episode reward: 13.390, mean reward: 0.372 [0.201, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.035, 10.325], loss: 0.002660, mae: 0.056866, mean_q: 0.449964
 83275/100000: episode: 1577, duration: 0.062s, episode steps: 11, steps per second: 179, episode reward: 4.260, mean reward: 0.387 [0.274, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.421], loss: 0.002754, mae: 0.054089, mean_q: 0.433317
 83292/100000: episode: 1578, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 7.117, mean reward: 0.419 [0.356, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.258, 10.523], loss: 0.002636, mae: 0.056513, mean_q: 0.462734
 83305/100000: episode: 1579, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 6.244, mean reward: 0.480 [0.419, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.363, 10.604], loss: 0.002411, mae: 0.054106, mean_q: 0.477573
 83318/100000: episode: 1580, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 5.781, mean reward: 0.445 [0.369, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.056, 10.593], loss: 0.002555, mae: 0.053947, mean_q: 0.460690
 83350/100000: episode: 1581, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 15.372, mean reward: 0.480 [0.385, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.913, 10.581], loss: 0.002656, mae: 0.057138, mean_q: 0.500990
 83386/100000: episode: 1582, duration: 0.189s, episode steps: 36, steps per second: 191, episode reward: 10.989, mean reward: 0.305 [0.156, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.035, 10.346], loss: 0.002622, mae: 0.054900, mean_q: 0.457377
 83397/100000: episode: 1583, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 4.246, mean reward: 0.386 [0.319, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.456], loss: 0.002450, mae: 0.053735, mean_q: 0.435597
 83414/100000: episode: 1584, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 8.273, mean reward: 0.487 [0.413, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.516], loss: 0.002850, mae: 0.056728, mean_q: 0.466653
[Info] 300-TH LEVEL FOUND: 1.008066177368164, Considering 10/90 traces
 83427/100000: episode: 1585, duration: 3.867s, episode steps: 13, steps per second: 3, episode reward: 6.410, mean reward: 0.493 [0.435, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.107, 10.547], loss: 0.002524, mae: 0.054595, mean_q: 0.470942
 83437/100000: episode: 1586, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 5.838, mean reward: 0.584 [0.503, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.103, 10.706], loss: 0.002433, mae: 0.055251, mean_q: 0.503251
 83457/100000: episode: 1587, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 9.989, mean reward: 0.499 [0.382, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.477], loss: 0.002548, mae: 0.054014, mean_q: 0.439989
 83465/100000: episode: 1588, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 3.832, mean reward: 0.479 [0.423, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.832, 10.550], loss: 0.002689, mae: 0.055392, mean_q: 0.438880
 83473/100000: episode: 1589, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 4.346, mean reward: 0.543 [0.517, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.609], loss: 0.002810, mae: 0.058286, mean_q: 0.538912
 83493/100000: episode: 1590, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 10.698, mean reward: 0.535 [0.292, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.264, 10.472], loss: 0.002738, mae: 0.058404, mean_q: 0.531016
 83501/100000: episode: 1591, duration: 0.040s, episode steps: 8, steps per second: 201, episode reward: 3.720, mean reward: 0.465 [0.365, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.496], loss: 0.002990, mae: 0.059634, mean_q: 0.578192
 83509/100000: episode: 1592, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 4.379, mean reward: 0.547 [0.480, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.669], loss: 0.002724, mae: 0.053708, mean_q: 0.465889
 83517/100000: episode: 1593, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 4.105, mean reward: 0.513 [0.439, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.562], loss: 0.002183, mae: 0.050634, mean_q: 0.427186
 83537/100000: episode: 1594, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 8.782, mean reward: 0.439 [0.295, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.326, 10.395], loss: 0.002528, mae: 0.052990, mean_q: 0.509100
 83557/100000: episode: 1595, duration: 0.097s, episode steps: 20, steps per second: 206, episode reward: 9.975, mean reward: 0.499 [0.373, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.769, 10.345], loss: 0.002516, mae: 0.055557, mean_q: 0.523112
 83565/100000: episode: 1596, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 3.789, mean reward: 0.474 [0.418, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.545], loss: 0.002557, mae: 0.054449, mean_q: 0.608882
 83573/100000: episode: 1597, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 3.995, mean reward: 0.499 [0.375, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.640], loss: 0.002901, mae: 0.060120, mean_q: 0.487911
 83581/100000: episode: 1598, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 3.711, mean reward: 0.464 [0.335, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.463], loss: 0.002782, mae: 0.057719, mean_q: 0.528870
 83601/100000: episode: 1599, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 9.090, mean reward: 0.455 [0.412, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.499, 10.561], loss: 0.002817, mae: 0.058272, mean_q: 0.560560
 83615/100000: episode: 1600, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 6.510, mean reward: 0.465 [0.364, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-1.062, 10.386], loss: 0.002958, mae: 0.059141, mean_q: 0.501175
 83623/100000: episode: 1601, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 4.365, mean reward: 0.546 [0.476, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.619, 10.702], loss: 0.002805, mae: 0.058504, mean_q: 0.557241
 83631/100000: episode: 1602, duration: 0.059s, episode steps: 8, steps per second: 136, episode reward: 3.642, mean reward: 0.455 [0.324, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.459, 10.534], loss: 0.002999, mae: 0.059414, mean_q: 0.497401
 83639/100000: episode: 1603, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 4.685, mean reward: 0.586 [0.553, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.638], loss: 0.002697, mae: 0.055623, mean_q: 0.489919
 83647/100000: episode: 1604, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 3.600, mean reward: 0.450 [0.403, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.277, 10.508], loss: 0.002755, mae: 0.056901, mean_q: 0.528983
 83661/100000: episode: 1605, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 7.026, mean reward: 0.502 [0.429, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.212, 10.505], loss: 0.002797, mae: 0.057979, mean_q: 0.519003
 83669/100000: episode: 1606, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 4.369, mean reward: 0.546 [0.509, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.619], loss: 0.002986, mae: 0.056416, mean_q: 0.580873
 83679/100000: episode: 1607, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 4.888, mean reward: 0.489 [0.412, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.533], loss: 0.002621, mae: 0.054855, mean_q: 0.568854
 83699/100000: episode: 1608, duration: 0.113s, episode steps: 20, steps per second: 178, episode reward: 10.477, mean reward: 0.524 [0.418, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.455, 10.515], loss: 0.002525, mae: 0.054236, mean_q: 0.568703
 83707/100000: episode: 1609, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 3.418, mean reward: 0.427 [0.349, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.555], loss: 0.002197, mae: 0.050670, mean_q: 0.616374
 83721/100000: episode: 1610, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 7.485, mean reward: 0.535 [0.452, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.644], loss: 0.003002, mae: 0.058166, mean_q: 0.528278
 83727/100000: episode: 1611, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 3.054, mean reward: 0.509 [0.408, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.655], loss: 0.002531, mae: 0.056501, mean_q: 0.577251
 83735/100000: episode: 1612, duration: 0.056s, episode steps: 8, steps per second: 144, episode reward: 3.827, mean reward: 0.478 [0.403, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.620], loss: 0.003479, mae: 0.063106, mean_q: 0.503565
 83745/100000: episode: 1613, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 4.497, mean reward: 0.450 [0.341, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.523], loss: 0.003220, mae: 0.060942, mean_q: 0.595998
 83759/100000: episode: 1614, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 7.636, mean reward: 0.545 [0.456, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.647], loss: 0.002958, mae: 0.059184, mean_q: 0.559292
 83779/100000: episode: 1615, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 9.934, mean reward: 0.497 [0.421, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.328, 10.472], loss: 0.002492, mae: 0.054931, mean_q: 0.545985
 83787/100000: episode: 1616, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 4.397, mean reward: 0.550 [0.509, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.525, 10.606], loss: 0.002993, mae: 0.058570, mean_q: 0.549564
 83807/100000: episode: 1617, duration: 0.094s, episode steps: 20, steps per second: 212, episode reward: 9.707, mean reward: 0.485 [0.396, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.871, 10.488], loss: 0.002403, mae: 0.053714, mean_q: 0.606043
 83813/100000: episode: 1618, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 3.100, mean reward: 0.517 [0.491, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.623], loss: 0.002385, mae: 0.053333, mean_q: 0.544572
 83823/100000: episode: 1619, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 4.692, mean reward: 0.469 [0.382, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.576], loss: 0.002373, mae: 0.050697, mean_q: 0.531779
 83831/100000: episode: 1620, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 4.554, mean reward: 0.569 [0.507, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.625], loss: 0.002964, mae: 0.058418, mean_q: 0.587825
 83837/100000: episode: 1621, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 3.125, mean reward: 0.521 [0.477, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.325, 10.534], loss: 0.003085, mae: 0.060842, mean_q: 0.596739
 83857/100000: episode: 1622, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 8.315, mean reward: 0.416 [0.301, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.103, 10.502], loss: 0.002809, mae: 0.058354, mean_q: 0.572843
 83865/100000: episode: 1623, duration: 0.054s, episode steps: 8, steps per second: 147, episode reward: 3.992, mean reward: 0.499 [0.456, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.605], loss: 0.002816, mae: 0.059313, mean_q: 0.596658
 83879/100000: episode: 1624, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 6.116, mean reward: 0.437 [0.385, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.484], loss: 0.002452, mae: 0.054192, mean_q: 0.607754
 83887/100000: episode: 1625, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 3.906, mean reward: 0.488 [0.402, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.287, 10.570], loss: 0.002483, mae: 0.055303, mean_q: 0.615983
 83907/100000: episode: 1626, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 11.215, mean reward: 0.561 [0.452, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.613], loss: 0.002433, mae: 0.054498, mean_q: 0.622540
 83927/100000: episode: 1627, duration: 0.129s, episode steps: 20, steps per second: 155, episode reward: 10.297, mean reward: 0.515 [0.419, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.148, 10.515], loss: 0.002711, mae: 0.056795, mean_q: 0.614888
 83935/100000: episode: 1628, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 4.201, mean reward: 0.525 [0.450, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.695], loss: 0.003278, mae: 0.061965, mean_q: 0.635794
 83943/100000: episode: 1629, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 3.857, mean reward: 0.482 [0.396, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.325, 10.507], loss: 0.002763, mae: 0.058608, mean_q: 0.617461
 83951/100000: episode: 1630, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 4.444, mean reward: 0.555 [0.461, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.628], loss: 0.002307, mae: 0.052662, mean_q: 0.617446
 83959/100000: episode: 1631, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 3.826, mean reward: 0.478 [0.420, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.581], loss: 0.002509, mae: 0.054650, mean_q: 0.596641
 83967/100000: episode: 1632, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 3.970, mean reward: 0.496 [0.421, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.531], loss: 0.002617, mae: 0.057025, mean_q: 0.647102
 83981/100000: episode: 1633, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 6.022, mean reward: 0.430 [0.288, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.461], loss: 0.002787, mae: 0.057966, mean_q: 0.651960
 83989/100000: episode: 1634, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 2.632, mean reward: 0.329 [0.219, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.425], loss: 0.002269, mae: 0.051879, mean_q: 0.627613
 83997/100000: episode: 1635, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 4.226, mean reward: 0.528 [0.494, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.595], loss: 0.002306, mae: 0.052459, mean_q: 0.632252
 84005/100000: episode: 1636, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 4.360, mean reward: 0.545 [0.486, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.629], loss: 0.002682, mae: 0.054648, mean_q: 0.628072
 84013/100000: episode: 1637, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 3.252, mean reward: 0.406 [0.345, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.451, 10.516], loss: 0.002294, mae: 0.052604, mean_q: 0.632905
 84027/100000: episode: 1638, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 6.923, mean reward: 0.495 [0.346, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.481], loss: 0.002654, mae: 0.056085, mean_q: 0.641880
 84035/100000: episode: 1639, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 3.764, mean reward: 0.471 [0.433, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.536], loss: 0.002517, mae: 0.053467, mean_q: 0.604032
 84043/100000: episode: 1640, duration: 0.056s, episode steps: 8, steps per second: 143, episode reward: 4.319, mean reward: 0.540 [0.413, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.740], loss: 0.003040, mae: 0.061941, mean_q: 0.669455
[Info] FALSIFICATION!
 84062/100000: episode: 1641, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 19.898, mean reward: 1.047 [0.495, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.928, 10.554], loss: 0.002610, mae: 0.056864, mean_q: 0.656933
 84162/100000: episode: 1642, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.267, mean reward: -0.153 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.336, 10.098], loss: 0.031603, mae: 0.083470, mean_q: 0.650809
 84262/100000: episode: 1643, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.020, mean reward: -0.180 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.044, 10.232], loss: 0.031889, mae: 0.093392, mean_q: 0.633436
 84362/100000: episode: 1644, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.598, mean reward: -0.146 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.485, 10.291], loss: 0.016776, mae: 0.070058, mean_q: 0.615652
 84462/100000: episode: 1645, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -12.646, mean reward: -0.126 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.923, 10.367], loss: 0.003131, mae: 0.059415, mean_q: 0.603093
 84562/100000: episode: 1646, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -11.889, mean reward: -0.119 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.175, 10.318], loss: 0.002758, mae: 0.057559, mean_q: 0.585974
 84662/100000: episode: 1647, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.650, mean reward: -0.167 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.330, 10.218], loss: 0.017510, mae: 0.075290, mean_q: 0.591148
 84762/100000: episode: 1648, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.745, mean reward: -0.157 [-1.000, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.328, 10.127], loss: 0.003069, mae: 0.058785, mean_q: 0.550086
 84862/100000: episode: 1649, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.704, mean reward: -0.187 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.782, 10.256], loss: 0.004572, mae: 0.059850, mean_q: 0.525644
 84962/100000: episode: 1650, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.192, mean reward: -0.172 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.454, 10.121], loss: 0.003630, mae: 0.059850, mean_q: 0.524042
 85062/100000: episode: 1651, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.413, mean reward: -0.174 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.449, 10.098], loss: 0.002984, mae: 0.056901, mean_q: 0.481683
 85162/100000: episode: 1652, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -20.455, mean reward: -0.205 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.580, 10.098], loss: 0.016179, mae: 0.063862, mean_q: 0.473302
 85262/100000: episode: 1653, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.824, mean reward: -0.168 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.285, 10.216], loss: 0.029326, mae: 0.078056, mean_q: 0.466296
 85362/100000: episode: 1654, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.167, mean reward: -0.192 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.214, 10.124], loss: 0.016162, mae: 0.065784, mean_q: 0.446949
 85462/100000: episode: 1655, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.880, mean reward: -0.189 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.335, 10.098], loss: 0.016420, mae: 0.067467, mean_q: 0.433784
 85562/100000: episode: 1656, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.469, mean reward: -0.175 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.061, 10.098], loss: 0.016379, mae: 0.066606, mean_q: 0.396067
 85662/100000: episode: 1657, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.450, mean reward: -0.184 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.804, 10.202], loss: 0.002575, mae: 0.054554, mean_q: 0.396475
 85762/100000: episode: 1658, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -19.182, mean reward: -0.192 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.774, 10.296], loss: 0.030262, mae: 0.079669, mean_q: 0.380683
 85862/100000: episode: 1659, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -19.681, mean reward: -0.197 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.732, 10.098], loss: 0.016215, mae: 0.064047, mean_q: 0.362473
 85962/100000: episode: 1660, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.574, mean reward: -0.166 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.098, 10.098], loss: 0.002715, mae: 0.055102, mean_q: 0.328690
 86062/100000: episode: 1661, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -16.630, mean reward: -0.166 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.455, 10.294], loss: 0.029501, mae: 0.072851, mean_q: 0.311251
 86162/100000: episode: 1662, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -13.735, mean reward: -0.137 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.846, 10.187], loss: 0.002624, mae: 0.053315, mean_q: 0.325981
 86262/100000: episode: 1663, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.844, mean reward: -0.188 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.723, 10.098], loss: 0.002473, mae: 0.051941, mean_q: 0.292733
 86362/100000: episode: 1664, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -11.151, mean reward: -0.112 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.628, 10.098], loss: 0.002266, mae: 0.049542, mean_q: 0.246090
 86462/100000: episode: 1665, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.314, mean reward: -0.163 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.742, 10.130], loss: 0.028160, mae: 0.065289, mean_q: 0.217127
 86562/100000: episode: 1666, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.034, mean reward: -0.190 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.536, 10.319], loss: 0.015813, mae: 0.060523, mean_q: 0.220105
 86662/100000: episode: 1667, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.930, mean reward: -0.189 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.526, 10.098], loss: 0.002472, mae: 0.051350, mean_q: 0.215545
 86762/100000: episode: 1668, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -14.150, mean reward: -0.141 [-1.000, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-2.188, 10.098], loss: 0.015394, mae: 0.056177, mean_q: 0.168088
 86862/100000: episode: 1669, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -17.606, mean reward: -0.176 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.615, 10.214], loss: 0.029835, mae: 0.076785, mean_q: 0.167526
 86962/100000: episode: 1670, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.209, mean reward: -0.172 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.962, 10.098], loss: 0.004605, mae: 0.063746, mean_q: 0.150912
 87062/100000: episode: 1671, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -15.187, mean reward: -0.152 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.417, 10.098], loss: 0.015788, mae: 0.061143, mean_q: 0.118888
 87162/100000: episode: 1672, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.397, mean reward: -0.154 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.822, 10.262], loss: 0.003255, mae: 0.055993, mean_q: 0.076386
 87262/100000: episode: 1673, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -15.479, mean reward: -0.155 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.702, 10.098], loss: 0.002303, mae: 0.048566, mean_q: 0.067969
 87362/100000: episode: 1674, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.811, mean reward: -0.188 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.285, 10.154], loss: 0.002286, mae: 0.048863, mean_q: 0.027428
 87462/100000: episode: 1675, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.020, mean reward: -0.190 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.311, 10.158], loss: 0.016075, mae: 0.059288, mean_q: 0.018868
 87562/100000: episode: 1676, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -20.110, mean reward: -0.201 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.650, 10.098], loss: 0.002609, mae: 0.050774, mean_q: 0.017660
 87662/100000: episode: 1677, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.585, mean reward: -0.176 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.589, 10.098], loss: 0.015865, mae: 0.058867, mean_q: -0.018911
 87762/100000: episode: 1678, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.646, mean reward: -0.166 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.586, 10.100], loss: 0.016085, mae: 0.060402, mean_q: -0.003923
 87862/100000: episode: 1679, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.331, mean reward: -0.193 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.113, 10.098], loss: 0.055497, mae: 0.084388, mean_q: -0.041798
 87962/100000: episode: 1680, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -15.373, mean reward: -0.154 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.406, 10.098], loss: 0.016337, mae: 0.061899, mean_q: -0.022760
 88062/100000: episode: 1681, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -17.080, mean reward: -0.171 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.783, 10.155], loss: 0.015922, mae: 0.060536, mean_q: -0.071864
 88162/100000: episode: 1682, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -17.133, mean reward: -0.171 [-1.000, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.492, 10.098], loss: 0.002373, mae: 0.048414, mean_q: -0.108133
 88262/100000: episode: 1683, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.806, mean reward: -0.188 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.617, 10.253], loss: 0.016159, mae: 0.059880, mean_q: -0.130692
 88362/100000: episode: 1684, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.554, mean reward: -0.176 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.956, 10.106], loss: 0.002365, mae: 0.048821, mean_q: -0.189021
 88462/100000: episode: 1685, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -14.313, mean reward: -0.143 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.670, 10.273], loss: 0.002560, mae: 0.049228, mean_q: -0.146435
 88562/100000: episode: 1686, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.911, mean reward: -0.159 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.828, 10.098], loss: 0.016041, mae: 0.059251, mean_q: -0.213844
 88662/100000: episode: 1687, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.714, mean reward: -0.167 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.392, 10.098], loss: 0.002564, mae: 0.049611, mean_q: -0.211181
 88762/100000: episode: 1688, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.134, mean reward: -0.181 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.406, 10.098], loss: 0.015384, mae: 0.055652, mean_q: -0.271278
 88862/100000: episode: 1689, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.311, mean reward: -0.193 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.894, 10.210], loss: 0.002242, mae: 0.046257, mean_q: -0.290322
 88962/100000: episode: 1690, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -15.355, mean reward: -0.154 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.568, 10.305], loss: 0.002309, mae: 0.047693, mean_q: -0.318894
 89062/100000: episode: 1691, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -18.127, mean reward: -0.181 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.982, 10.098], loss: 0.002346, mae: 0.046742, mean_q: -0.339129
 89162/100000: episode: 1692, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.581, mean reward: -0.186 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.789, 10.270], loss: 0.002356, mae: 0.047037, mean_q: -0.336580
 89262/100000: episode: 1693, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.112, mean reward: -0.161 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.341, 10.098], loss: 0.002460, mae: 0.047915, mean_q: -0.334009
 89362/100000: episode: 1694, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.326, mean reward: -0.173 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.015, 10.112], loss: 0.002356, mae: 0.047846, mean_q: -0.275334
 89462/100000: episode: 1695, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -14.186, mean reward: -0.142 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.926, 10.098], loss: 0.002554, mae: 0.048757, mean_q: -0.316962
 89562/100000: episode: 1696, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.176, mean reward: -0.162 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.538, 10.098], loss: 0.002273, mae: 0.047287, mean_q: -0.308639
 89662/100000: episode: 1697, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -11.346, mean reward: -0.113 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.362, 10.180], loss: 0.002349, mae: 0.047218, mean_q: -0.334733
 89762/100000: episode: 1698, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.606, mean reward: -0.186 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.985, 10.098], loss: 0.002463, mae: 0.047970, mean_q: -0.315973
 89862/100000: episode: 1699, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -15.998, mean reward: -0.160 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.468, 10.267], loss: 0.002319, mae: 0.046788, mean_q: -0.339605
 89962/100000: episode: 1700, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.878, mean reward: -0.179 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.553, 10.211], loss: 0.002376, mae: 0.047354, mean_q: -0.303815
 90062/100000: episode: 1701, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.330, mean reward: -0.173 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.350, 10.098], loss: 0.002437, mae: 0.048585, mean_q: -0.303547
 90162/100000: episode: 1702, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.772, mean reward: -0.188 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.520, 10.098], loss: 0.004263, mae: 0.060442, mean_q: -0.296205
 90262/100000: episode: 1703, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -11.967, mean reward: -0.120 [-1.000, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.596, 10.450], loss: 0.003429, mae: 0.056734, mean_q: -0.328293
 90362/100000: episode: 1704, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.124, mean reward: -0.181 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.340, 10.235], loss: 0.003273, mae: 0.056291, mean_q: -0.344074
 90462/100000: episode: 1705, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.890, mean reward: -0.179 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.495, 10.098], loss: 0.002847, mae: 0.053508, mean_q: -0.309518
 90562/100000: episode: 1706, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.981, mean reward: -0.180 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.594, 10.185], loss: 0.002995, mae: 0.055903, mean_q: -0.334725
 90662/100000: episode: 1707, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -19.515, mean reward: -0.195 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.454, 10.215], loss: 0.002742, mae: 0.053037, mean_q: -0.306220
 90762/100000: episode: 1708, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -20.400, mean reward: -0.204 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.506, 10.255], loss: 0.002601, mae: 0.050851, mean_q: -0.321476
 90862/100000: episode: 1709, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -11.375, mean reward: -0.114 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.474, 10.098], loss: 0.002550, mae: 0.050406, mean_q: -0.332707
 90962/100000: episode: 1710, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -20.428, mean reward: -0.204 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.218, 10.098], loss: 0.002535, mae: 0.051567, mean_q: -0.319978
 91062/100000: episode: 1711, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.581, mean reward: -0.156 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.954, 10.258], loss: 0.003260, mae: 0.055945, mean_q: -0.357136
 91162/100000: episode: 1712, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: -13.873, mean reward: -0.139 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.862, 10.098], loss: 0.002538, mae: 0.049708, mean_q: -0.337361
 91262/100000: episode: 1713, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.997, mean reward: -0.160 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.639, 10.098], loss: 0.002231, mae: 0.047121, mean_q: -0.338987
 91362/100000: episode: 1714, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -12.740, mean reward: -0.127 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.456, 10.250], loss: 0.002420, mae: 0.049282, mean_q: -0.320090
 91462/100000: episode: 1715, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.556, mean reward: -0.176 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.735, 10.236], loss: 0.002471, mae: 0.050392, mean_q: -0.319000
 91562/100000: episode: 1716, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -13.611, mean reward: -0.136 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.281, 10.098], loss: 0.002484, mae: 0.049518, mean_q: -0.314132
 91662/100000: episode: 1717, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -17.445, mean reward: -0.174 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.395, 10.098], loss: 0.002505, mae: 0.050825, mean_q: -0.320202
 91762/100000: episode: 1718, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -20.375, mean reward: -0.204 [-1.000, 0.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.764, 10.290], loss: 0.002549, mae: 0.050802, mean_q: -0.289031
 91862/100000: episode: 1719, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.298, mean reward: -0.163 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.420, 10.098], loss: 0.002519, mae: 0.050115, mean_q: -0.295047
 91962/100000: episode: 1720, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -20.121, mean reward: -0.201 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.336, 10.217], loss: 0.002550, mae: 0.050984, mean_q: -0.317293
 92062/100000: episode: 1721, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -12.733, mean reward: -0.127 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.150, 10.098], loss: 0.002646, mae: 0.052080, mean_q: -0.282756
 92162/100000: episode: 1722, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.549, mean reward: -0.175 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.636, 10.098], loss: 0.002593, mae: 0.051846, mean_q: -0.313150
 92262/100000: episode: 1723, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.681, mean reward: -0.177 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.599, 10.098], loss: 0.002479, mae: 0.051029, mean_q: -0.298206
 92362/100000: episode: 1724, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -16.589, mean reward: -0.166 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.941, 10.098], loss: 0.002558, mae: 0.050924, mean_q: -0.326255
 92462/100000: episode: 1725, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.945, mean reward: -0.169 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.239, 10.146], loss: 0.002483, mae: 0.049351, mean_q: -0.328966
 92562/100000: episode: 1726, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.005, mean reward: -0.170 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.803, 10.098], loss: 0.002412, mae: 0.050051, mean_q: -0.314175
 92662/100000: episode: 1727, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -16.421, mean reward: -0.164 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.509, 10.330], loss: 0.002490, mae: 0.049263, mean_q: -0.328860
 92762/100000: episode: 1728, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.543, mean reward: -0.175 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.942, 10.264], loss: 0.002408, mae: 0.048834, mean_q: -0.341984
 92862/100000: episode: 1729, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -14.842, mean reward: -0.148 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.556, 10.098], loss: 0.003116, mae: 0.056939, mean_q: -0.298979
 92962/100000: episode: 1730, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.178, mean reward: -0.182 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.864, 10.098], loss: 0.003098, mae: 0.056240, mean_q: -0.288417
 93062/100000: episode: 1731, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -15.171, mean reward: -0.152 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.263, 10.234], loss: 0.003125, mae: 0.055972, mean_q: -0.302951
 93162/100000: episode: 1732, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.286, mean reward: -0.163 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.888, 10.271], loss: 0.002464, mae: 0.050070, mean_q: -0.319477
 93262/100000: episode: 1733, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.937, mean reward: -0.169 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.530, 10.098], loss: 0.002563, mae: 0.050296, mean_q: -0.313693
 93362/100000: episode: 1734, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -20.496, mean reward: -0.205 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.546, 10.196], loss: 0.002369, mae: 0.048176, mean_q: -0.340427
 93462/100000: episode: 1735, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.244, mean reward: -0.182 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.345, 10.098], loss: 0.002591, mae: 0.051213, mean_q: -0.300703
 93562/100000: episode: 1736, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.181, mean reward: -0.202 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.733, 10.163], loss: 0.002482, mae: 0.049478, mean_q: -0.325856
 93662/100000: episode: 1737, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -17.251, mean reward: -0.173 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.804, 10.210], loss: 0.002627, mae: 0.051674, mean_q: -0.327096
 93762/100000: episode: 1738, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -13.441, mean reward: -0.134 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.055, 10.098], loss: 0.002746, mae: 0.055182, mean_q: -0.316266
 93862/100000: episode: 1739, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.780, mean reward: -0.178 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.839, 10.098], loss: 0.002520, mae: 0.050921, mean_q: -0.296798
 93962/100000: episode: 1740, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -12.431, mean reward: -0.124 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.695, 10.304], loss: 0.002647, mae: 0.052687, mean_q: -0.315361
[Info] 100-TH LEVEL FOUND: 0.6499338150024414, Considering 10/90 traces
 94062/100000: episode: 1741, duration: 4.252s, episode steps: 100, steps per second: 24, episode reward: -16.966, mean reward: -0.170 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.967, 10.243], loss: 0.002608, mae: 0.051950, mean_q: -0.315448
 94097/100000: episode: 1742, duration: 0.170s, episode steps: 35, steps per second: 206, episode reward: 7.716, mean reward: 0.220 [0.073, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.283, 10.206], loss: 0.002681, mae: 0.051671, mean_q: -0.303112
 94152/100000: episode: 1743, duration: 0.275s, episode steps: 55, steps per second: 200, episode reward: 19.349, mean reward: 0.352 [0.046, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.849 [-0.393, 10.230], loss: 0.002330, mae: 0.048062, mean_q: -0.331816
 94159/100000: episode: 1744, duration: 0.035s, episode steps: 7, steps per second: 197, episode reward: 2.143, mean reward: 0.306 [0.267, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.541, 10.100], loss: 0.002398, mae: 0.049768, mean_q: -0.251051
 94164/100000: episode: 1745, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 2.101, mean reward: 0.420 [0.361, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.332, 10.100], loss: 0.002877, mae: 0.055933, mean_q: -0.180534
 94170/100000: episode: 1746, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 2.201, mean reward: 0.367 [0.354, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.353, 10.100], loss: 0.002484, mae: 0.050584, mean_q: -0.321530
 94177/100000: episode: 1747, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 2.441, mean reward: 0.349 [0.276, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.451, 10.100], loss: 0.002810, mae: 0.055211, mean_q: -0.367997
 94183/100000: episode: 1748, duration: 0.031s, episode steps: 6, steps per second: 192, episode reward: 1.944, mean reward: 0.324 [0.265, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.403, 10.100], loss: 0.002317, mae: 0.050072, mean_q: -0.305446
 94238/100000: episode: 1749, duration: 0.291s, episode steps: 55, steps per second: 189, episode reward: 13.001, mean reward: 0.236 [0.116, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.417, 10.392], loss: 0.002756, mae: 0.051721, mean_q: -0.291243
 94246/100000: episode: 1750, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 2.790, mean reward: 0.349 [0.311, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.286, 10.100], loss: 0.002925, mae: 0.055110, mean_q: -0.277709
 94301/100000: episode: 1751, duration: 0.302s, episode steps: 55, steps per second: 182, episode reward: 13.180, mean reward: 0.240 [0.092, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-1.063, 10.214], loss: 0.002684, mae: 0.052660, mean_q: -0.265848
 94356/100000: episode: 1752, duration: 0.284s, episode steps: 55, steps per second: 194, episode reward: 22.449, mean reward: 0.408 [0.252, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.386, 10.553], loss: 0.002718, mae: 0.052720, mean_q: -0.242684
 94364/100000: episode: 1753, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 2.396, mean reward: 0.299 [0.213, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.697, 10.100], loss: 0.002294, mae: 0.051134, mean_q: -0.166902
 94371/100000: episode: 1754, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 2.644, mean reward: 0.378 [0.318, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.365, 10.100], loss: 0.002425, mae: 0.052744, mean_q: -0.291820
 94376/100000: episode: 1755, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 1.557, mean reward: 0.311 [0.282, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.261, 10.100], loss: 0.002277, mae: 0.049012, mean_q: -0.284334
 94393/100000: episode: 1756, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 5.226, mean reward: 0.307 [0.126, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.619, 10.100], loss: 0.002667, mae: 0.053007, mean_q: -0.236185
 94399/100000: episode: 1757, duration: 0.034s, episode steps: 6, steps per second: 176, episode reward: 2.324, mean reward: 0.387 [0.348, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.348, 10.100], loss: 0.002647, mae: 0.052892, mean_q: -0.252933
 94416/100000: episode: 1758, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 5.896, mean reward: 0.347 [0.169, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.877, 10.100], loss: 0.002353, mae: 0.050029, mean_q: -0.202983
 94471/100000: episode: 1759, duration: 0.280s, episode steps: 55, steps per second: 197, episode reward: 21.588, mean reward: 0.393 [0.181, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.845 [-1.029, 10.382], loss: 0.002600, mae: 0.051356, mean_q: -0.252773
 94488/100000: episode: 1760, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 5.174, mean reward: 0.304 [0.217, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.711, 10.100], loss: 0.002995, mae: 0.053137, mean_q: -0.240525
 94493/100000: episode: 1761, duration: 0.026s, episode steps: 5, steps per second: 189, episode reward: 1.610, mean reward: 0.322 [0.277, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.287, 10.100], loss: 0.002502, mae: 0.050288, mean_q: -0.221051
 94510/100000: episode: 1762, duration: 0.080s, episode steps: 17, steps per second: 211, episode reward: 6.283, mean reward: 0.370 [0.294, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.220, 10.100], loss: 0.002618, mae: 0.051611, mean_q: -0.208963
[Info] FALSIFICATION!
 94511/100000: episode: 1763, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-0.025, 8.163], loss: 0.001405, mae: 0.040138, mean_q: -0.317578
 94611/100000: episode: 1764, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.448, mean reward: -0.164 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.312, 10.098], loss: 0.002858, mae: 0.054218, mean_q: -0.257911
 94711/100000: episode: 1765, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.997, mean reward: -0.190 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.005, 10.098], loss: 0.016979, mae: 0.062064, mean_q: -0.248470
 94811/100000: episode: 1766, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -19.311, mean reward: -0.193 [-1.000, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.988, 10.162], loss: 0.002923, mae: 0.056485, mean_q: -0.201597
 94911/100000: episode: 1767, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -14.880, mean reward: -0.149 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.474, 10.098], loss: 0.002727, mae: 0.053143, mean_q: -0.237661
 95011/100000: episode: 1768, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.616, mean reward: -0.166 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.335, 10.098], loss: 0.002946, mae: 0.055161, mean_q: -0.229375
 95111/100000: episode: 1769, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -12.042, mean reward: -0.120 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.512, 10.098], loss: 0.016484, mae: 0.060595, mean_q: -0.204573
 95211/100000: episode: 1770, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.955, mean reward: -0.170 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.558, 10.273], loss: 0.002807, mae: 0.055003, mean_q: -0.216372
 95311/100000: episode: 1771, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -15.899, mean reward: -0.159 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.280, 10.125], loss: 0.002595, mae: 0.051614, mean_q: -0.233715
 95411/100000: episode: 1772, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -14.191, mean reward: -0.142 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.782, 10.098], loss: 0.002728, mae: 0.053444, mean_q: -0.259350
 95511/100000: episode: 1773, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.046, mean reward: -0.140 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.793, 10.098], loss: 0.002679, mae: 0.052033, mean_q: -0.237253
 95611/100000: episode: 1774, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -19.357, mean reward: -0.194 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.477, 10.098], loss: 0.002865, mae: 0.054798, mean_q: -0.210781
 95711/100000: episode: 1775, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -18.292, mean reward: -0.183 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.866, 10.124], loss: 0.002454, mae: 0.050892, mean_q: -0.227662
 95811/100000: episode: 1776, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -20.148, mean reward: -0.201 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.836, 10.098], loss: 0.016929, mae: 0.064338, mean_q: -0.208030
 95911/100000: episode: 1777, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.207, mean reward: -0.192 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.318, 10.098], loss: 0.002486, mae: 0.050576, mean_q: -0.231581
 96011/100000: episode: 1778, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.267, mean reward: -0.133 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.878, 10.264], loss: 0.002687, mae: 0.051961, mean_q: -0.240779
 96111/100000: episode: 1779, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.443, mean reward: -0.194 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.090, 10.099], loss: 0.002761, mae: 0.052968, mean_q: -0.225012
 96211/100000: episode: 1780, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -19.786, mean reward: -0.198 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.173, 10.098], loss: 0.005593, mae: 0.069433, mean_q: -0.188502
 96311/100000: episode: 1781, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -14.138, mean reward: -0.141 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.073, 10.263], loss: 0.002775, mae: 0.053201, mean_q: -0.220825
 96411/100000: episode: 1782, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.320, mean reward: -0.153 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.880, 10.372], loss: 0.015942, mae: 0.056339, mean_q: -0.230010
 96511/100000: episode: 1783, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.327, mean reward: -0.143 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.873, 10.098], loss: 0.002595, mae: 0.051962, mean_q: -0.216105
 96611/100000: episode: 1784, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.988, mean reward: -0.190 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.602, 10.098], loss: 0.015744, mae: 0.055335, mean_q: -0.201297
 96711/100000: episode: 1785, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -6.191, mean reward: -0.062 [-1.000, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.007, 10.244], loss: 0.003483, mae: 0.059540, mean_q: -0.236530
 96811/100000: episode: 1786, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.481, mean reward: -0.185 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.791, 10.098], loss: 0.015953, mae: 0.057494, mean_q: -0.200576
 96911/100000: episode: 1787, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.842, mean reward: -0.188 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.934, 10.098], loss: 0.002974, mae: 0.055290, mean_q: -0.191649
 97011/100000: episode: 1788, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -15.235, mean reward: -0.152 [-1.000, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.700, 10.098], loss: 0.016170, mae: 0.058279, mean_q: -0.220559
 97111/100000: episode: 1789, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.390, mean reward: -0.194 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.909, 10.102], loss: 0.017042, mae: 0.067390, mean_q: -0.233763
 97211/100000: episode: 1790, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -19.293, mean reward: -0.193 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.544, 10.165], loss: 0.028899, mae: 0.061153, mean_q: -0.228730
 97311/100000: episode: 1791, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.258, mean reward: -0.193 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.667, 10.098], loss: 0.003053, mae: 0.055583, mean_q: -0.225627
 97411/100000: episode: 1792, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -19.224, mean reward: -0.192 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.673, 10.098], loss: 0.002650, mae: 0.050569, mean_q: -0.230617
 97511/100000: episode: 1793, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -7.829, mean reward: -0.078 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.992, 10.098], loss: 0.030195, mae: 0.068750, mean_q: -0.204425
 97611/100000: episode: 1794, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.627, mean reward: -0.186 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.743, 10.158], loss: 0.002677, mae: 0.051150, mean_q: -0.210784
 97711/100000: episode: 1795, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.505, mean reward: -0.175 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.341, 10.098], loss: 0.015781, mae: 0.057413, mean_q: -0.204553
 97811/100000: episode: 1796, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -19.149, mean reward: -0.191 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.312, 10.166], loss: 0.002726, mae: 0.052886, mean_q: -0.251366
 97911/100000: episode: 1797, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -15.097, mean reward: -0.151 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.438, 10.098], loss: 0.002591, mae: 0.050457, mean_q: -0.229032
 98011/100000: episode: 1798, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.984, mean reward: -0.180 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.457, 10.122], loss: 0.015360, mae: 0.051842, mean_q: -0.240760
 98111/100000: episode: 1799, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -11.213, mean reward: -0.112 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.956, 10.267], loss: 0.003156, mae: 0.055874, mean_q: -0.239840
 98211/100000: episode: 1800, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -15.225, mean reward: -0.152 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.938, 10.098], loss: 0.002425, mae: 0.048892, mean_q: -0.259214
 98311/100000: episode: 1801, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -14.516, mean reward: -0.145 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.974, 10.138], loss: 0.028928, mae: 0.060702, mean_q: -0.225095
 98411/100000: episode: 1802, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -12.921, mean reward: -0.129 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.495, 10.187], loss: 0.016043, mae: 0.060820, mean_q: -0.182100
 98511/100000: episode: 1803, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -14.135, mean reward: -0.141 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.183, 10.098], loss: 0.002532, mae: 0.049661, mean_q: -0.245395
 98611/100000: episode: 1804, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -15.100, mean reward: -0.151 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.533, 10.373], loss: 0.002743, mae: 0.052458, mean_q: -0.208785
 98711/100000: episode: 1805, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -17.911, mean reward: -0.179 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.921, 10.286], loss: 0.002542, mae: 0.049827, mean_q: -0.226664
 98811/100000: episode: 1806, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -18.484, mean reward: -0.185 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.645, 10.098], loss: 0.002606, mae: 0.050467, mean_q: -0.263001
 98911/100000: episode: 1807, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.342, mean reward: -0.163 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.068, 10.269], loss: 0.002586, mae: 0.050178, mean_q: -0.241505
 99011/100000: episode: 1808, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -18.911, mean reward: -0.189 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.477, 10.098], loss: 0.015988, mae: 0.057726, mean_q: -0.231313
 99111/100000: episode: 1809, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -18.261, mean reward: -0.183 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.179, 10.221], loss: 0.015559, mae: 0.056304, mean_q: -0.203516
 99211/100000: episode: 1810, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.964, mean reward: -0.180 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.441, 10.098], loss: 0.002521, mae: 0.049991, mean_q: -0.249944
 99311/100000: episode: 1811, duration: 0.464s, episode steps: 100, steps per second: 215, episode reward: -16.154, mean reward: -0.162 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.793, 10.111], loss: 0.002422, mae: 0.048401, mean_q: -0.260609
 99411/100000: episode: 1812, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -16.708, mean reward: -0.167 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.426, 10.098], loss: 0.002622, mae: 0.050372, mean_q: -0.270178
 99511/100000: episode: 1813, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -18.915, mean reward: -0.189 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.630, 10.098], loss: 0.002302, mae: 0.047331, mean_q: -0.315353
 99611/100000: episode: 1814, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.293, mean reward: -0.183 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.739, 10.126], loss: 0.002299, mae: 0.047360, mean_q: -0.307540
 99711/100000: episode: 1815, duration: 0.463s, episode steps: 100, steps per second: 216, episode reward: -15.553, mean reward: -0.156 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.934, 10.178], loss: 0.002316, mae: 0.047398, mean_q: -0.313784
 99811/100000: episode: 1816, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -19.765, mean reward: -0.198 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.859, 10.098], loss: 0.002565, mae: 0.051451, mean_q: -0.303478
 99911/100000: episode: 1817, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -12.569, mean reward: -0.126 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.088, 10.302], loss: 0.002742, mae: 0.050125, mean_q: -0.280838
done, took 574.081 seconds
[Info] End Importance Splitting. Falsification occurred 6 times.
