Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.181s, episode steps: 100, steps per second: 551, episode reward: -20.880, mean reward: -0.209 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.249, 10.118], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.070s, episode steps: 100, steps per second: 1426, episode reward: -14.334, mean reward: -0.143 [-1.000, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.289, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.069s, episode steps: 100, steps per second: 1454, episode reward: -17.903, mean reward: -0.179 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.288, 10.229], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.060s, episode steps: 100, steps per second: 1669, episode reward: -12.877, mean reward: -0.129 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.659, 10.188], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.060s, episode steps: 100, steps per second: 1660, episode reward: -18.447, mean reward: -0.184 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.953, 10.291], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.183s, episode steps: 100, steps per second: 85, episode reward: -18.819, mean reward: -0.188 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.251, 10.198], loss: 0.055403, mae: 0.230517, mean_q: -0.138921
   700/100000: episode: 7, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.466, mean reward: -0.165 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.559, 10.319], loss: 0.018601, mae: 0.133083, mean_q: -0.266738
   800/100000: episode: 8, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.678, mean reward: -0.197 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.362, 10.098], loss: 0.013863, mae: 0.113326, mean_q: -0.300159
   900/100000: episode: 9, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -15.559, mean reward: -0.156 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.273, 10.098], loss: 0.011598, mae: 0.100453, mean_q: -0.328110
  1000/100000: episode: 10, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -11.262, mean reward: -0.113 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.695, 10.348], loss: 0.010909, mae: 0.096586, mean_q: -0.345946
  1100/100000: episode: 11, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.529, mean reward: -0.185 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.545, 10.098], loss: 0.010147, mae: 0.094439, mean_q: -0.342351
  1200/100000: episode: 12, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.933, mean reward: -0.189 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.818, 10.261], loss: 0.009469, mae: 0.089042, mean_q: -0.324411
  1300/100000: episode: 13, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.132, mean reward: -0.181 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.292, 10.098], loss: 0.009984, mae: 0.094413, mean_q: -0.292788
  1400/100000: episode: 14, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.050, mean reward: -0.170 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.807, 10.098], loss: 0.008497, mae: 0.086850, mean_q: -0.351150
  1500/100000: episode: 15, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.431, mean reward: -0.174 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.951, 10.206], loss: 0.008413, mae: 0.086531, mean_q: -0.334583
  1600/100000: episode: 16, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.626, mean reward: -0.156 [-1.000, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.801, 10.211], loss: 0.007883, mae: 0.084310, mean_q: -0.345591
  1700/100000: episode: 17, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -18.265, mean reward: -0.183 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.243, 10.098], loss: 0.007773, mae: 0.082983, mean_q: -0.311327
  1800/100000: episode: 18, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.513, mean reward: -0.185 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.531, 10.098], loss: 0.007422, mae: 0.078138, mean_q: -0.359547
  1900/100000: episode: 19, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.386, mean reward: -0.164 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.606, 10.098], loss: 0.006791, mae: 0.077676, mean_q: -0.310098
  2000/100000: episode: 20, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.274, mean reward: -0.183 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.359, 10.241], loss: 0.006692, mae: 0.079062, mean_q: -0.322034
  2100/100000: episode: 21, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.525, mean reward: -0.175 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.672, 10.171], loss: 0.007047, mae: 0.077481, mean_q: -0.352234
  2200/100000: episode: 22, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -20.198, mean reward: -0.202 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.306, 10.146], loss: 0.008143, mae: 0.084921, mean_q: -0.326425
  2300/100000: episode: 23, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -20.056, mean reward: -0.201 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.192, 10.098], loss: 0.007103, mae: 0.079485, mean_q: -0.354084
  2400/100000: episode: 24, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.096, mean reward: -0.151 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.474, 10.371], loss: 0.006619, mae: 0.079279, mean_q: -0.309879
  2500/100000: episode: 25, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.181, mean reward: -0.182 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.564, 10.164], loss: 0.007490, mae: 0.080591, mean_q: -0.351306
  2600/100000: episode: 26, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.894, mean reward: -0.199 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.915, 10.098], loss: 0.005576, mae: 0.073279, mean_q: -0.343992
  2700/100000: episode: 27, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -20.285, mean reward: -0.203 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.543, 10.201], loss: 0.007099, mae: 0.077647, mean_q: -0.348541
  2800/100000: episode: 28, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -13.666, mean reward: -0.137 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.031, 10.240], loss: 0.007638, mae: 0.082117, mean_q: -0.356153
  2900/100000: episode: 29, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.506, mean reward: -0.195 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.184, 10.120], loss: 0.005973, mae: 0.073475, mean_q: -0.319382
  3000/100000: episode: 30, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.495, mean reward: -0.165 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.886, 10.098], loss: 0.005666, mae: 0.071710, mean_q: -0.342461
  3100/100000: episode: 31, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.938, mean reward: -0.179 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.614, 10.098], loss: 0.005776, mae: 0.071705, mean_q: -0.382294
  3200/100000: episode: 32, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -15.653, mean reward: -0.157 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.672, 10.098], loss: 0.005700, mae: 0.071066, mean_q: -0.329241
  3300/100000: episode: 33, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -13.927, mean reward: -0.139 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.013, 10.098], loss: 0.005271, mae: 0.069604, mean_q: -0.280817
  3400/100000: episode: 34, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.066, mean reward: -0.181 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.063, 10.245], loss: 0.005670, mae: 0.070811, mean_q: -0.317563
  3500/100000: episode: 35, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.184, mean reward: -0.172 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.033, 10.224], loss: 0.006894, mae: 0.079877, mean_q: -0.343040
  3600/100000: episode: 36, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -19.772, mean reward: -0.198 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.071, 10.098], loss: 0.006104, mae: 0.076987, mean_q: -0.342508
  3700/100000: episode: 37, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.056, mean reward: -0.181 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.483, 10.438], loss: 0.007049, mae: 0.080867, mean_q: -0.326803
  3800/100000: episode: 38, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.846, mean reward: -0.178 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.187, 10.098], loss: 0.005888, mae: 0.074171, mean_q: -0.347054
  3900/100000: episode: 39, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.211, mean reward: -0.182 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.353, 10.098], loss: 0.004989, mae: 0.068187, mean_q: -0.371293
  4000/100000: episode: 40, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -20.309, mean reward: -0.203 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.729, 10.179], loss: 0.005454, mae: 0.071713, mean_q: -0.291058
  4100/100000: episode: 41, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.397, mean reward: -0.164 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.398, 10.120], loss: 0.004822, mae: 0.067444, mean_q: -0.330616
  4200/100000: episode: 42, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -12.672, mean reward: -0.127 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.461, 10.098], loss: 0.004708, mae: 0.067477, mean_q: -0.340038
  4300/100000: episode: 43, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.412, mean reward: -0.174 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.011, 10.098], loss: 0.005300, mae: 0.069780, mean_q: -0.359577
  4400/100000: episode: 44, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -14.656, mean reward: -0.147 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.005, 10.363], loss: 0.004568, mae: 0.065830, mean_q: -0.356293
  4500/100000: episode: 45, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -14.026, mean reward: -0.140 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.831, 10.414], loss: 0.004840, mae: 0.068599, mean_q: -0.316651
  4600/100000: episode: 46, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.030, mean reward: -0.160 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.665, 10.304], loss: 0.004901, mae: 0.068872, mean_q: -0.323830
  4700/100000: episode: 47, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.006, mean reward: -0.130 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.420, 10.098], loss: 0.004730, mae: 0.067527, mean_q: -0.314872
  4800/100000: episode: 48, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.233, mean reward: -0.182 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.289, 10.098], loss: 0.004924, mae: 0.068256, mean_q: -0.330554
  4900/100000: episode: 49, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.875, mean reward: -0.159 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.901, 10.098], loss: 0.004340, mae: 0.066742, mean_q: -0.319773
  5000/100000: episode: 50, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -19.449, mean reward: -0.194 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.604, 10.294], loss: 0.005859, mae: 0.076095, mean_q: -0.307088
  5100/100000: episode: 51, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -14.578, mean reward: -0.146 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.599, 10.222], loss: 0.004009, mae: 0.063708, mean_q: -0.323006
  5200/100000: episode: 52, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.923, mean reward: -0.189 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.611, 10.148], loss: 0.004388, mae: 0.066560, mean_q: -0.315895
  5300/100000: episode: 53, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.303, mean reward: -0.163 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.196, 10.098], loss: 0.004682, mae: 0.067531, mean_q: -0.304222
  5400/100000: episode: 54, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.833, mean reward: -0.188 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.433, 10.098], loss: 0.004625, mae: 0.066254, mean_q: -0.314877
  5500/100000: episode: 55, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -7.679, mean reward: -0.077 [-1.000, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.171, 10.098], loss: 0.004930, mae: 0.067877, mean_q: -0.326001
  5600/100000: episode: 56, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.817, mean reward: -0.188 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.071, 10.098], loss: 0.004706, mae: 0.066632, mean_q: -0.337357
  5700/100000: episode: 57, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.859, mean reward: -0.179 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.398, 10.423], loss: 0.004126, mae: 0.063597, mean_q: -0.328238
  5800/100000: episode: 58, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.337, mean reward: -0.193 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.547, 10.168], loss: 0.004826, mae: 0.067885, mean_q: -0.316522
  5900/100000: episode: 59, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -19.820, mean reward: -0.198 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.771, 10.185], loss: 0.004715, mae: 0.067790, mean_q: -0.341855
  6000/100000: episode: 60, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -14.373, mean reward: -0.144 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.528, 10.317], loss: 0.004881, mae: 0.069616, mean_q: -0.321597
  6100/100000: episode: 61, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -15.360, mean reward: -0.154 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.718, 10.387], loss: 0.006127, mae: 0.075916, mean_q: -0.317946
  6200/100000: episode: 62, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.670, mean reward: -0.167 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.507, 10.098], loss: 0.005462, mae: 0.073052, mean_q: -0.334862
  6300/100000: episode: 63, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.974, mean reward: -0.190 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.894, 10.098], loss: 0.004198, mae: 0.063143, mean_q: -0.335210
  6400/100000: episode: 64, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -15.051, mean reward: -0.151 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.954, 10.098], loss: 0.004880, mae: 0.068112, mean_q: -0.316749
  6500/100000: episode: 65, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -13.776, mean reward: -0.138 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.909, 10.098], loss: 0.005056, mae: 0.068595, mean_q: -0.344335
  6600/100000: episode: 66, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.494, mean reward: -0.185 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.732, 10.098], loss: 0.004226, mae: 0.065393, mean_q: -0.288018
  6700/100000: episode: 67, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.638, mean reward: -0.186 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.825, 10.171], loss: 0.004238, mae: 0.065388, mean_q: -0.298132
  6800/100000: episode: 68, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.968, mean reward: -0.180 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.656, 10.297], loss: 0.006368, mae: 0.075453, mean_q: -0.360771
  6900/100000: episode: 69, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.419, mean reward: -0.174 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.021, 10.098], loss: 0.004166, mae: 0.066100, mean_q: -0.309000
  7000/100000: episode: 70, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.726, mean reward: -0.187 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.410, 10.163], loss: 0.003918, mae: 0.062817, mean_q: -0.317889
  7100/100000: episode: 71, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.084, mean reward: -0.171 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.207, 10.320], loss: 0.005241, mae: 0.070173, mean_q: -0.330155
  7200/100000: episode: 72, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -19.290, mean reward: -0.193 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.926, 10.198], loss: 0.003669, mae: 0.062275, mean_q: -0.301595
  7300/100000: episode: 73, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.891, mean reward: -0.179 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.925, 10.168], loss: 0.003991, mae: 0.063441, mean_q: -0.345557
  7400/100000: episode: 74, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.220, mean reward: -0.192 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.490, 10.102], loss: 0.003813, mae: 0.062094, mean_q: -0.320941
  7500/100000: episode: 75, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -13.680, mean reward: -0.137 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.390, 10.098], loss: 0.003886, mae: 0.063511, mean_q: -0.329061
  7600/100000: episode: 76, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.890, mean reward: -0.169 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.393, 10.264], loss: 0.004421, mae: 0.067716, mean_q: -0.347017
  7700/100000: episode: 77, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.738, mean reward: -0.167 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.539, 10.234], loss: 0.004471, mae: 0.065439, mean_q: -0.348238
  7800/100000: episode: 78, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -20.303, mean reward: -0.203 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.544, 10.212], loss: 0.004414, mae: 0.066100, mean_q: -0.320500
  7900/100000: episode: 79, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.289, mean reward: -0.153 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.492, 10.098], loss: 0.004823, mae: 0.068347, mean_q: -0.310783
  8000/100000: episode: 80, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.419, mean reward: -0.174 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.330, 10.156], loss: 0.004220, mae: 0.063616, mean_q: -0.331109
  8100/100000: episode: 81, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.338, mean reward: -0.183 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.256, 10.098], loss: 0.003924, mae: 0.062907, mean_q: -0.307558
  8200/100000: episode: 82, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -7.959, mean reward: -0.080 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.073, 10.098], loss: 0.004235, mae: 0.063003, mean_q: -0.313112
  8300/100000: episode: 83, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.913, mean reward: -0.199 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.582, 10.098], loss: 0.003962, mae: 0.063542, mean_q: -0.309842
  8400/100000: episode: 84, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.771, mean reward: -0.178 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.885, 10.295], loss: 0.004189, mae: 0.063543, mean_q: -0.292736
  8500/100000: episode: 85, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.315, mean reward: -0.173 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.072, 10.098], loss: 0.004461, mae: 0.064763, mean_q: -0.299378
  8600/100000: episode: 86, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.656, mean reward: -0.167 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.030, 10.260], loss: 0.004700, mae: 0.067481, mean_q: -0.335444
  8700/100000: episode: 87, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.856, mean reward: -0.179 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.542, 10.202], loss: 0.004567, mae: 0.067279, mean_q: -0.328867
  8800/100000: episode: 88, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.368, mean reward: -0.184 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.651, 10.098], loss: 0.004997, mae: 0.070312, mean_q: -0.325172
  8900/100000: episode: 89, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -20.579, mean reward: -0.206 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.538, 10.348], loss: 0.004268, mae: 0.064489, mean_q: -0.326391
  9000/100000: episode: 90, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.693, mean reward: -0.197 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.768, 10.169], loss: 0.004065, mae: 0.065209, mean_q: -0.329750
  9100/100000: episode: 91, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.567, mean reward: -0.186 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.252, 10.098], loss: 0.004085, mae: 0.065586, mean_q: -0.323645
  9200/100000: episode: 92, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.334, mean reward: -0.153 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.572, 10.098], loss: 0.004085, mae: 0.065338, mean_q: -0.297054
  9300/100000: episode: 93, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -14.091, mean reward: -0.141 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.813, 10.098], loss: 0.004516, mae: 0.068628, mean_q: -0.319169
  9400/100000: episode: 94, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.575, mean reward: -0.156 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.112, 10.316], loss: 0.004064, mae: 0.067081, mean_q: -0.303371
  9500/100000: episode: 95, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -19.551, mean reward: -0.196 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.099, 10.098], loss: 0.003624, mae: 0.061827, mean_q: -0.323914
  9600/100000: episode: 96, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.228, mean reward: -0.162 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.416, 10.347], loss: 0.003955, mae: 0.065418, mean_q: -0.332853
  9700/100000: episode: 97, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.807, mean reward: -0.188 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.086, 10.098], loss: 0.004023, mae: 0.064561, mean_q: -0.289287
  9800/100000: episode: 98, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.203, mean reward: -0.152 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.437, 10.243], loss: 0.003818, mae: 0.063412, mean_q: -0.322119
  9900/100000: episode: 99, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -19.716, mean reward: -0.197 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.781, 10.111], loss: 0.004534, mae: 0.066113, mean_q: -0.344416
 10000/100000: episode: 100, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -13.727, mean reward: -0.137 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.061, 10.098], loss: 0.003268, mae: 0.058119, mean_q: -0.340186
 10100/100000: episode: 101, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -10.278, mean reward: -0.103 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.435, 10.191], loss: 0.003477, mae: 0.060228, mean_q: -0.339282
 10200/100000: episode: 102, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.781, mean reward: -0.178 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.902, 10.241], loss: 0.003469, mae: 0.058940, mean_q: -0.345811
 10300/100000: episode: 103, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -18.111, mean reward: -0.181 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.169, 10.098], loss: 0.003170, mae: 0.057654, mean_q: -0.345474
 10400/100000: episode: 104, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -13.339, mean reward: -0.133 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.663, 10.174], loss: 0.003612, mae: 0.060863, mean_q: -0.324326
 10500/100000: episode: 105, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.263, mean reward: -0.173 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.141, 10.244], loss: 0.003429, mae: 0.060261, mean_q: -0.345667
 10600/100000: episode: 106, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -20.355, mean reward: -0.204 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.964, 10.098], loss: 0.003728, mae: 0.062807, mean_q: -0.317853
 10700/100000: episode: 107, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.514, mean reward: -0.175 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.197, 10.352], loss: 0.004180, mae: 0.064166, mean_q: -0.315074
 10800/100000: episode: 108, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.849, mean reward: -0.138 [-1.000, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.019, 10.098], loss: 0.003660, mae: 0.063979, mean_q: -0.324272
 10900/100000: episode: 109, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.576, mean reward: -0.176 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.754, 10.098], loss: 0.003677, mae: 0.063186, mean_q: -0.311883
 11000/100000: episode: 110, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.453, mean reward: -0.175 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.575, 10.098], loss: 0.004232, mae: 0.065732, mean_q: -0.305054
 11100/100000: episode: 111, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -20.093, mean reward: -0.201 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.240, 10.204], loss: 0.003225, mae: 0.058844, mean_q: -0.293556
 11200/100000: episode: 112, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.020, mean reward: -0.170 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.376, 10.166], loss: 0.003332, mae: 0.058861, mean_q: -0.330257
 11300/100000: episode: 113, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.450, mean reward: -0.174 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.903, 10.098], loss: 0.003967, mae: 0.066165, mean_q: -0.309557
 11400/100000: episode: 114, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.628, mean reward: -0.176 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.866, 10.098], loss: 0.003592, mae: 0.061828, mean_q: -0.327794
 11500/100000: episode: 115, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.229, mean reward: -0.182 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.517, 10.195], loss: 0.003832, mae: 0.064081, mean_q: -0.288411
 11600/100000: episode: 116, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.280, mean reward: -0.153 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.171, 10.098], loss: 0.004068, mae: 0.064050, mean_q: -0.343400
 11700/100000: episode: 117, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -13.204, mean reward: -0.132 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.814, 10.098], loss: 0.003475, mae: 0.059794, mean_q: -0.348020
 11800/100000: episode: 118, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -20.017, mean reward: -0.200 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.784, 10.174], loss: 0.003751, mae: 0.062609, mean_q: -0.334484
 11900/100000: episode: 119, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.217, mean reward: -0.152 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.847, 10.238], loss: 0.003834, mae: 0.063171, mean_q: -0.297882
 12000/100000: episode: 120, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -11.787, mean reward: -0.118 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.019, 10.267], loss: 0.003803, mae: 0.064294, mean_q: -0.322026
 12100/100000: episode: 121, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -14.054, mean reward: -0.141 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.042, 10.098], loss: 0.003873, mae: 0.065534, mean_q: -0.318717
 12200/100000: episode: 122, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.535, mean reward: -0.165 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.002, 10.214], loss: 0.003649, mae: 0.062853, mean_q: -0.336547
 12300/100000: episode: 123, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.500, mean reward: -0.145 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.766, 10.312], loss: 0.003236, mae: 0.059470, mean_q: -0.337309
 12400/100000: episode: 124, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.208, mean reward: -0.172 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.619, 10.098], loss: 0.003335, mae: 0.060760, mean_q: -0.331970
 12500/100000: episode: 125, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.531, mean reward: -0.175 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.613, 10.098], loss: 0.003778, mae: 0.063632, mean_q: -0.312837
 12600/100000: episode: 126, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.307, mean reward: -0.183 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.237, 10.131], loss: 0.003480, mae: 0.061258, mean_q: -0.318925
 12700/100000: episode: 127, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.048, mean reward: -0.170 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.482, 10.098], loss: 0.003762, mae: 0.064406, mean_q: -0.316003
 12800/100000: episode: 128, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -20.055, mean reward: -0.201 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.444, 10.140], loss: 0.003351, mae: 0.060465, mean_q: -0.302480
 12900/100000: episode: 129, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.304, mean reward: -0.183 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.399, 10.217], loss: 0.004039, mae: 0.066682, mean_q: -0.316252
 13000/100000: episode: 130, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -20.457, mean reward: -0.205 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.136, 10.098], loss: 0.003322, mae: 0.058840, mean_q: -0.314735
 13100/100000: episode: 131, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.306, mean reward: -0.173 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.317, 10.098], loss: 0.003857, mae: 0.064776, mean_q: -0.334840
 13200/100000: episode: 132, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -16.012, mean reward: -0.160 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.835, 10.172], loss: 0.003547, mae: 0.061667, mean_q: -0.309152
 13300/100000: episode: 133, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -13.186, mean reward: -0.132 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.660, 10.308], loss: 0.003254, mae: 0.059292, mean_q: -0.296304
 13400/100000: episode: 134, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.524, mean reward: -0.155 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.804, 10.183], loss: 0.003202, mae: 0.059068, mean_q: -0.334936
 13500/100000: episode: 135, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.890, mean reward: -0.169 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.813, 10.098], loss: 0.004062, mae: 0.066154, mean_q: -0.354127
 13600/100000: episode: 136, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.166, mean reward: -0.162 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.230, 10.361], loss: 0.003024, mae: 0.058787, mean_q: -0.302664
 13700/100000: episode: 137, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.364, mean reward: -0.164 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.823, 10.183], loss: 0.003399, mae: 0.059847, mean_q: -0.317302
 13800/100000: episode: 138, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.206, mean reward: -0.162 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.700, 10.220], loss: 0.003980, mae: 0.065128, mean_q: -0.312299
 13900/100000: episode: 139, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.782, mean reward: -0.178 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.494, 10.196], loss: 0.003102, mae: 0.057730, mean_q: -0.309362
 14000/100000: episode: 140, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.813, mean reward: -0.158 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.622, 10.182], loss: 0.002954, mae: 0.057018, mean_q: -0.316380
 14100/100000: episode: 141, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.744, mean reward: -0.187 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.509, 10.098], loss: 0.003277, mae: 0.059845, mean_q: -0.347246
 14200/100000: episode: 142, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.964, mean reward: -0.190 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.256, 10.133], loss: 0.003220, mae: 0.058511, mean_q: -0.293890
 14300/100000: episode: 143, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -20.057, mean reward: -0.201 [-1.000, 0.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.694, 10.178], loss: 0.003151, mae: 0.057789, mean_q: -0.306739
 14400/100000: episode: 144, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -14.627, mean reward: -0.146 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.798, 10.098], loss: 0.002969, mae: 0.056065, mean_q: -0.304633
 14500/100000: episode: 145, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.652, mean reward: -0.127 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.490, 10.156], loss: 0.003193, mae: 0.058003, mean_q: -0.329755
 14600/100000: episode: 146, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.046, mean reward: -0.180 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.900, 10.195], loss: 0.003081, mae: 0.058478, mean_q: -0.322418
 14700/100000: episode: 147, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.480, mean reward: -0.175 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.936, 10.233], loss: 0.003567, mae: 0.062237, mean_q: -0.327450
 14800/100000: episode: 148, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -19.352, mean reward: -0.194 [-1.000, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.752, 10.098], loss: 0.003689, mae: 0.064605, mean_q: -0.338883
 14900/100000: episode: 149, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -14.260, mean reward: -0.143 [-1.000, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.538, 10.285], loss: 0.003376, mae: 0.060547, mean_q: -0.355818
 15000/100000: episode: 150, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.534, mean reward: -0.175 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.352, 10.144], loss: 0.003340, mae: 0.060831, mean_q: -0.309229
 15100/100000: episode: 151, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -9.345, mean reward: -0.093 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.243, 10.098], loss: 0.003078, mae: 0.057343, mean_q: -0.342780
 15200/100000: episode: 152, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.994, mean reward: -0.190 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.231, 10.098], loss: 0.003061, mae: 0.057403, mean_q: -0.327025
 15300/100000: episode: 153, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.662, mean reward: -0.167 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.437, 10.331], loss: 0.003129, mae: 0.058857, mean_q: -0.318919
 15400/100000: episode: 154, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -20.936, mean reward: -0.209 [-1.000, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.620, 10.133], loss: 0.005430, mae: 0.072651, mean_q: -0.294233
 15500/100000: episode: 155, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.453, mean reward: -0.195 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.466, 10.141], loss: 0.004131, mae: 0.066001, mean_q: -0.313499
 15600/100000: episode: 156, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.313, mean reward: -0.143 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.408, 10.151], loss: 0.003198, mae: 0.058338, mean_q: -0.326295
 15700/100000: episode: 157, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.761, mean reward: -0.198 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.846, 10.110], loss: 0.004269, mae: 0.066883, mean_q: -0.339648
 15800/100000: episode: 158, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.109, mean reward: -0.191 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.987, 10.098], loss: 0.003166, mae: 0.057338, mean_q: -0.381549
 15900/100000: episode: 159, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -20.261, mean reward: -0.203 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.562, 10.098], loss: 0.003294, mae: 0.059593, mean_q: -0.319869
 16000/100000: episode: 160, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.549, mean reward: -0.165 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.136, 10.098], loss: 0.003653, mae: 0.061293, mean_q: -0.320376
 16100/100000: episode: 161, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.758, mean reward: -0.158 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.664, 10.098], loss: 0.003638, mae: 0.063440, mean_q: -0.329181
 16200/100000: episode: 162, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.440, mean reward: -0.164 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.634, 10.098], loss: 0.002924, mae: 0.057060, mean_q: -0.345570
 16300/100000: episode: 163, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -14.333, mean reward: -0.143 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.221, 10.098], loss: 0.003244, mae: 0.059796, mean_q: -0.323199
 16400/100000: episode: 164, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.649, mean reward: -0.196 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.328, 10.175], loss: 0.003028, mae: 0.057047, mean_q: -0.323787
 16500/100000: episode: 165, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.013, mean reward: -0.190 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.371, 10.098], loss: 0.003588, mae: 0.061492, mean_q: -0.299290
 16600/100000: episode: 166, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.366, mean reward: -0.174 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.711, 10.098], loss: 0.003357, mae: 0.059853, mean_q: -0.323123
 16700/100000: episode: 167, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.190, mean reward: -0.172 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.202, 10.098], loss: 0.003262, mae: 0.059035, mean_q: -0.322800
 16800/100000: episode: 168, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -13.830, mean reward: -0.138 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.453, 10.280], loss: 0.003070, mae: 0.056707, mean_q: -0.329316
 16900/100000: episode: 169, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.702, mean reward: -0.167 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.924, 10.098], loss: 0.004412, mae: 0.062226, mean_q: -0.317129
 17000/100000: episode: 170, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -14.418, mean reward: -0.144 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.937, 10.098], loss: 0.003437, mae: 0.059828, mean_q: -0.301998
 17100/100000: episode: 171, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -17.136, mean reward: -0.171 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.579, 10.162], loss: 0.003247, mae: 0.059136, mean_q: -0.337994
 17200/100000: episode: 172, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.598, mean reward: -0.176 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.513, 10.404], loss: 0.002901, mae: 0.055282, mean_q: -0.323630
 17300/100000: episode: 173, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.169, mean reward: -0.172 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.646, 10.098], loss: 0.003053, mae: 0.057676, mean_q: -0.316234
 17400/100000: episode: 174, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -14.211, mean reward: -0.142 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.263, 10.214], loss: 0.003314, mae: 0.058638, mean_q: -0.347955
 17500/100000: episode: 175, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.807, mean reward: -0.168 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.351, 10.098], loss: 0.002944, mae: 0.055679, mean_q: -0.331856
 17600/100000: episode: 176, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -17.908, mean reward: -0.179 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.909, 10.098], loss: 0.003193, mae: 0.057330, mean_q: -0.354311
 17700/100000: episode: 177, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -19.899, mean reward: -0.199 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.569, 10.101], loss: 0.003584, mae: 0.061359, mean_q: -0.326403
 17800/100000: episode: 178, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.811, mean reward: -0.178 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.734, 10.316], loss: 0.003204, mae: 0.058155, mean_q: -0.349369
 17900/100000: episode: 179, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.708, mean reward: -0.197 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.389, 10.368], loss: 0.003400, mae: 0.060605, mean_q: -0.294513
 18000/100000: episode: 180, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.112, mean reward: -0.181 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.610, 10.098], loss: 0.003119, mae: 0.057811, mean_q: -0.343550
 18100/100000: episode: 181, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.394, mean reward: -0.144 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.977, 10.098], loss: 0.003019, mae: 0.056300, mean_q: -0.339296
 18200/100000: episode: 182, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.908, mean reward: -0.189 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.684, 10.098], loss: 0.003038, mae: 0.056111, mean_q: -0.302107
 18300/100000: episode: 183, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.212, mean reward: -0.172 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.838, 10.141], loss: 0.002847, mae: 0.055181, mean_q: -0.342844
 18400/100000: episode: 184, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -15.433, mean reward: -0.154 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.678, 10.098], loss: 0.002891, mae: 0.055620, mean_q: -0.308676
 18500/100000: episode: 185, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -12.250, mean reward: -0.123 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.874, 10.098], loss: 0.002842, mae: 0.054657, mean_q: -0.316721
 18600/100000: episode: 186, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.606, mean reward: -0.146 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.390, 10.310], loss: 0.002857, mae: 0.054683, mean_q: -0.297487
 18700/100000: episode: 187, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.636, mean reward: -0.176 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.615, 10.098], loss: 0.003207, mae: 0.058360, mean_q: -0.298928
 18800/100000: episode: 188, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.041, mean reward: -0.190 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.849, 10.104], loss: 0.003262, mae: 0.059624, mean_q: -0.334652
 18900/100000: episode: 189, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.293, mean reward: -0.163 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.500, 10.244], loss: 0.003131, mae: 0.058869, mean_q: -0.321774
 19000/100000: episode: 190, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.603, mean reward: -0.166 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.255, 10.098], loss: 0.003118, mae: 0.058000, mean_q: -0.307535
 19100/100000: episode: 191, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.603, mean reward: -0.166 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.736, 10.461], loss: 0.002949, mae: 0.055594, mean_q: -0.348081
 19200/100000: episode: 192, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.997, mean reward: -0.160 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.978, 10.144], loss: 0.002907, mae: 0.055141, mean_q: -0.323954
 19300/100000: episode: 193, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.258, mean reward: -0.173 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.833, 10.106], loss: 0.003662, mae: 0.061743, mean_q: -0.302334
 19400/100000: episode: 194, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -19.288, mean reward: -0.193 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.333, 10.186], loss: 0.004007, mae: 0.064128, mean_q: -0.317754
 19500/100000: episode: 195, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -5.315, mean reward: -0.053 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.346, 10.098], loss: 0.002841, mae: 0.054331, mean_q: -0.373581
 19600/100000: episode: 196, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.422, mean reward: -0.154 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.666, 10.188], loss: 0.002983, mae: 0.057012, mean_q: -0.345772
 19700/100000: episode: 197, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.081, mean reward: -0.171 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.576, 10.098], loss: 0.003271, mae: 0.059359, mean_q: -0.331352
 19800/100000: episode: 198, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.231, mean reward: -0.152 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.050, 10.098], loss: 0.002795, mae: 0.053859, mean_q: -0.282851
 19900/100000: episode: 199, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.376, mean reward: -0.184 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.216, 10.183], loss: 0.002713, mae: 0.053730, mean_q: -0.319462
 20000/100000: episode: 200, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.982, mean reward: -0.180 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.291, 10.098], loss: 0.002899, mae: 0.055507, mean_q: -0.344315
 20100/100000: episode: 201, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -20.307, mean reward: -0.203 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.990, 10.203], loss: 0.002957, mae: 0.057126, mean_q: -0.303904
 20200/100000: episode: 202, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.820, mean reward: -0.188 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.773, 10.261], loss: 0.002938, mae: 0.057408, mean_q: -0.275619
 20300/100000: episode: 203, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.980, mean reward: -0.160 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.735, 10.193], loss: 0.002717, mae: 0.054246, mean_q: -0.325394
 20400/100000: episode: 204, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -19.706, mean reward: -0.197 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.578, 10.098], loss: 0.003134, mae: 0.058297, mean_q: -0.306316
 20500/100000: episode: 205, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.444, mean reward: -0.174 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.425, 10.153], loss: 0.002680, mae: 0.052625, mean_q: -0.325903
 20600/100000: episode: 206, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.854, mean reward: -0.189 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.236, 10.098], loss: 0.005647, mae: 0.076638, mean_q: -0.320206
 20700/100000: episode: 207, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.110, mean reward: -0.151 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.675, 10.268], loss: 0.003150, mae: 0.059023, mean_q: -0.309802
 20800/100000: episode: 208, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.982, mean reward: -0.190 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.857, 10.098], loss: 0.003384, mae: 0.061364, mean_q: -0.320291
 20900/100000: episode: 209, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.984, mean reward: -0.180 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.278, 10.100], loss: 0.002965, mae: 0.056934, mean_q: -0.285883
 21000/100000: episode: 210, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -10.898, mean reward: -0.109 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.110, 10.446], loss: 0.002865, mae: 0.056200, mean_q: -0.320240
 21100/100000: episode: 211, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -17.522, mean reward: -0.175 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.797, 10.098], loss: 0.002685, mae: 0.054882, mean_q: -0.248448
 21200/100000: episode: 212, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.604, mean reward: -0.156 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.738, 10.098], loss: 0.002867, mae: 0.055798, mean_q: -0.314830
 21300/100000: episode: 213, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.359, mean reward: -0.184 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.666, 10.197], loss: 0.002949, mae: 0.056985, mean_q: -0.314977
 21400/100000: episode: 214, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.136, mean reward: -0.181 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.258, 10.105], loss: 0.002813, mae: 0.055046, mean_q: -0.315532
 21500/100000: episode: 215, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.480, mean reward: -0.175 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.777, 10.378], loss: 0.002831, mae: 0.054543, mean_q: -0.317707
 21600/100000: episode: 216, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -21.659, mean reward: -0.217 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.651, 10.098], loss: 0.003063, mae: 0.058831, mean_q: -0.304145
 21700/100000: episode: 217, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.094, mean reward: -0.161 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.413, 10.098], loss: 0.003238, mae: 0.059538, mean_q: -0.354721
 21800/100000: episode: 218, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.616, mean reward: -0.186 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.672, 10.098], loss: 0.003070, mae: 0.058593, mean_q: -0.316119
 21900/100000: episode: 219, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -19.490, mean reward: -0.195 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.867, 10.190], loss: 0.002667, mae: 0.052838, mean_q: -0.329465
 22000/100000: episode: 220, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.827, mean reward: -0.168 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.506, 10.253], loss: 0.002837, mae: 0.054365, mean_q: -0.325945
 22100/100000: episode: 221, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.781, mean reward: -0.198 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.327, 10.103], loss: 0.002814, mae: 0.054006, mean_q: -0.346033
 22200/100000: episode: 222, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.738, mean reward: -0.187 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.854, 10.211], loss: 0.002866, mae: 0.056205, mean_q: -0.326520
 22300/100000: episode: 223, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.614, mean reward: -0.176 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.531, 10.098], loss: 0.002823, mae: 0.054869, mean_q: -0.324750
 22400/100000: episode: 224, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.073, mean reward: -0.151 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.726, 10.222], loss: 0.002685, mae: 0.052793, mean_q: -0.343886
 22500/100000: episode: 225, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -14.633, mean reward: -0.146 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.372, 10.098], loss: 0.002771, mae: 0.055277, mean_q: -0.322404
 22600/100000: episode: 226, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.401, mean reward: -0.174 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.126, 10.261], loss: 0.003853, mae: 0.060423, mean_q: -0.303343
 22700/100000: episode: 227, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.066, mean reward: -0.161 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.015, 10.098], loss: 0.005027, mae: 0.070871, mean_q: -0.346496
 22800/100000: episode: 228, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.001, mean reward: -0.190 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.128, 10.098], loss: 0.004858, mae: 0.071586, mean_q: -0.315130
 22900/100000: episode: 229, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.691, mean reward: -0.197 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.495, 10.188], loss: 0.003198, mae: 0.060216, mean_q: -0.288627
 23000/100000: episode: 230, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -20.195, mean reward: -0.202 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.177, 10.098], loss: 0.002834, mae: 0.054275, mean_q: -0.309953
 23100/100000: episode: 231, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -12.496, mean reward: -0.125 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-1.363, 10.098], loss: 0.002833, mae: 0.055194, mean_q: -0.341014
 23200/100000: episode: 232, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.511, mean reward: -0.155 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.370, 10.315], loss: 0.002610, mae: 0.051775, mean_q: -0.352450
 23300/100000: episode: 233, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -12.900, mean reward: -0.129 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.784, 10.592], loss: 0.002662, mae: 0.052837, mean_q: -0.341508
 23400/100000: episode: 234, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -13.591, mean reward: -0.136 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.347, 10.202], loss: 0.002586, mae: 0.051110, mean_q: -0.356505
 23500/100000: episode: 235, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.802, mean reward: -0.198 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.527, 10.160], loss: 0.002750, mae: 0.054177, mean_q: -0.329294
 23600/100000: episode: 236, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.889, mean reward: -0.159 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.988, 10.165], loss: 0.002723, mae: 0.052805, mean_q: -0.324635
 23700/100000: episode: 237, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -13.757, mean reward: -0.138 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.706, 10.098], loss: 0.002705, mae: 0.053106, mean_q: -0.335089
 23800/100000: episode: 238, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -21.110, mean reward: -0.211 [-1.000, 0.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.481, 10.098], loss: 0.002615, mae: 0.051672, mean_q: -0.299977
 23900/100000: episode: 239, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.818, mean reward: -0.178 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.824, 10.330], loss: 0.002955, mae: 0.056047, mean_q: -0.309601
 24000/100000: episode: 240, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -13.968, mean reward: -0.140 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.888, 10.098], loss: 0.002836, mae: 0.056065, mean_q: -0.334299
 24100/100000: episode: 241, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -13.703, mean reward: -0.137 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.418, 10.250], loss: 0.002945, mae: 0.056627, mean_q: -0.290106
 24200/100000: episode: 242, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.091, mean reward: -0.161 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.459, 10.300], loss: 0.003765, mae: 0.060433, mean_q: -0.317128
 24300/100000: episode: 243, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.294, mean reward: -0.153 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.084, 10.262], loss: 0.004801, mae: 0.072640, mean_q: -0.316682
 24400/100000: episode: 244, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.074, mean reward: -0.181 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.208, 10.098], loss: 0.002624, mae: 0.052721, mean_q: -0.332153
 24500/100000: episode: 245, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.466, mean reward: -0.185 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.817, 10.098], loss: 0.002784, mae: 0.054184, mean_q: -0.325317
 24600/100000: episode: 246, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -20.287, mean reward: -0.203 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.411, 10.098], loss: 0.002905, mae: 0.056504, mean_q: -0.297712
 24700/100000: episode: 247, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -12.443, mean reward: -0.124 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.108, 10.098], loss: 0.002664, mae: 0.052891, mean_q: -0.343311
 24800/100000: episode: 248, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -16.568, mean reward: -0.166 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.584, 10.353], loss: 0.002806, mae: 0.054587, mean_q: -0.308605
 24900/100000: episode: 249, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.086, mean reward: -0.171 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.677, 10.098], loss: 0.002904, mae: 0.056997, mean_q: -0.327178
 25000/100000: episode: 250, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -15.841, mean reward: -0.158 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.089, 10.300], loss: 0.003931, mae: 0.059893, mean_q: -0.326745
 25100/100000: episode: 251, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.989, mean reward: -0.170 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.602, 10.098], loss: 0.003037, mae: 0.059194, mean_q: -0.308472
 25200/100000: episode: 252, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -19.216, mean reward: -0.192 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.396, 10.098], loss: 0.002848, mae: 0.055002, mean_q: -0.317351
 25300/100000: episode: 253, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.073, mean reward: -0.181 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.666, 10.098], loss: 0.003047, mae: 0.057308, mean_q: -0.311965
 25400/100000: episode: 254, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -20.296, mean reward: -0.203 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.084, 10.153], loss: 0.002929, mae: 0.056504, mean_q: -0.320083
 25500/100000: episode: 255, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.563, mean reward: -0.156 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.913, 10.226], loss: 0.002726, mae: 0.053442, mean_q: -0.343095
 25600/100000: episode: 256, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.178, mean reward: -0.122 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.867, 10.352], loss: 0.002671, mae: 0.052298, mean_q: -0.324237
 25700/100000: episode: 257, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -11.728, mean reward: -0.117 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.778, 10.098], loss: 0.002738, mae: 0.053903, mean_q: -0.301818
 25800/100000: episode: 258, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.990, mean reward: -0.170 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.210, 10.473], loss: 0.003029, mae: 0.055507, mean_q: -0.332989
 25900/100000: episode: 259, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -16.763, mean reward: -0.168 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.714, 10.117], loss: 0.002939, mae: 0.056127, mean_q: -0.341729
 26000/100000: episode: 260, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -13.506, mean reward: -0.135 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.924, 10.098], loss: 0.002638, mae: 0.052293, mean_q: -0.352815
 26100/100000: episode: 261, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.033, mean reward: -0.170 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.000, 10.137], loss: 0.002569, mae: 0.051383, mean_q: -0.312346
 26200/100000: episode: 262, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -11.104, mean reward: -0.111 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.851, 10.248], loss: 0.002673, mae: 0.053432, mean_q: -0.287625
 26300/100000: episode: 263, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -20.101, mean reward: -0.201 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.866, 10.133], loss: 0.002875, mae: 0.054856, mean_q: -0.307612
 26400/100000: episode: 264, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.366, mean reward: -0.174 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.443, 10.109], loss: 0.002798, mae: 0.054965, mean_q: -0.317670
 26500/100000: episode: 265, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -19.867, mean reward: -0.199 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.260, 10.098], loss: 0.002507, mae: 0.050790, mean_q: -0.289188
 26600/100000: episode: 266, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.198, mean reward: -0.142 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.785, 10.249], loss: 0.002683, mae: 0.053443, mean_q: -0.321125
 26700/100000: episode: 267, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -20.242, mean reward: -0.202 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.817, 10.098], loss: 0.002617, mae: 0.053250, mean_q: -0.279716
 26800/100000: episode: 268, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -13.203, mean reward: -0.132 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.838, 10.465], loss: 0.002662, mae: 0.053078, mean_q: -0.309463
 26900/100000: episode: 269, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.625, mean reward: -0.186 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.620, 10.180], loss: 0.002481, mae: 0.051041, mean_q: -0.273075
 27000/100000: episode: 270, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.584, mean reward: -0.166 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.198, 10.098], loss: 0.002879, mae: 0.057640, mean_q: -0.324575
 27100/100000: episode: 271, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.085, mean reward: -0.171 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.558, 10.098], loss: 0.002473, mae: 0.051212, mean_q: -0.278571
 27200/100000: episode: 272, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.589, mean reward: -0.166 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.552, 10.258], loss: 0.002752, mae: 0.054604, mean_q: -0.304286
 27300/100000: episode: 273, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -15.963, mean reward: -0.160 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.652, 10.098], loss: 0.002423, mae: 0.050302, mean_q: -0.328683
 27400/100000: episode: 274, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.358, mean reward: -0.164 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.598, 10.098], loss: 0.002520, mae: 0.050534, mean_q: -0.315488
 27500/100000: episode: 275, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.627, mean reward: -0.166 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.685, 10.168], loss: 0.002503, mae: 0.051026, mean_q: -0.295563
 27600/100000: episode: 276, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -15.100, mean reward: -0.151 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.919, 10.178], loss: 0.002969, mae: 0.055821, mean_q: -0.327961
 27700/100000: episode: 277, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.446, mean reward: -0.144 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.256, 10.098], loss: 0.004471, mae: 0.065604, mean_q: -0.311134
 27800/100000: episode: 278, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -15.339, mean reward: -0.153 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.314, 10.098], loss: 0.002868, mae: 0.055403, mean_q: -0.305563
 27900/100000: episode: 279, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.227, mean reward: -0.142 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.994, 10.098], loss: 0.002543, mae: 0.051644, mean_q: -0.301730
 28000/100000: episode: 280, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.003, mean reward: -0.190 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.249, 10.137], loss: 0.002295, mae: 0.048631, mean_q: -0.310073
 28100/100000: episode: 281, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.978, mean reward: -0.170 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.809, 10.098], loss: 0.002437, mae: 0.050649, mean_q: -0.336398
 28200/100000: episode: 282, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -7.027, mean reward: -0.070 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.634, 10.391], loss: 0.002607, mae: 0.052268, mean_q: -0.306867
 28300/100000: episode: 283, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -7.223, mean reward: -0.072 [-1.000, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.109, 10.464], loss: 0.002481, mae: 0.050482, mean_q: -0.323600
 28400/100000: episode: 284, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -14.139, mean reward: -0.141 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.417, 10.098], loss: 0.002435, mae: 0.050227, mean_q: -0.336209
 28500/100000: episode: 285, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.139, mean reward: -0.191 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.335, 10.098], loss: 0.002587, mae: 0.051646, mean_q: -0.317777
 28600/100000: episode: 286, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.499, mean reward: -0.175 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.644, 10.394], loss: 0.002823, mae: 0.055678, mean_q: -0.276839
 28700/100000: episode: 287, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.821, mean reward: -0.178 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.617, 10.369], loss: 0.002695, mae: 0.054152, mean_q: -0.292056
 28800/100000: episode: 288, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -20.065, mean reward: -0.201 [-1.000, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.198, 10.309], loss: 0.002691, mae: 0.053935, mean_q: -0.265141
 28900/100000: episode: 289, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.329, mean reward: -0.163 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.112, 10.098], loss: 0.002663, mae: 0.053260, mean_q: -0.328143
 29000/100000: episode: 290, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -13.301, mean reward: -0.133 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.569, 10.098], loss: 0.003205, mae: 0.059149, mean_q: -0.282839
 29100/100000: episode: 291, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -12.896, mean reward: -0.129 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.004, 10.098], loss: 0.003470, mae: 0.061560, mean_q: -0.285304
 29200/100000: episode: 292, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.769, mean reward: -0.178 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.058, 10.285], loss: 0.002517, mae: 0.050873, mean_q: -0.300583
 29300/100000: episode: 293, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.738, mean reward: -0.177 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.792, 10.325], loss: 0.002507, mae: 0.050850, mean_q: -0.339539
 29400/100000: episode: 294, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -18.268, mean reward: -0.183 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.962, 10.101], loss: 0.003001, mae: 0.058193, mean_q: -0.295633
 29500/100000: episode: 295, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -11.308, mean reward: -0.113 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.426, 10.318], loss: 0.002450, mae: 0.051248, mean_q: -0.282900
 29600/100000: episode: 296, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -12.121, mean reward: -0.121 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.957, 10.366], loss: 0.002581, mae: 0.050911, mean_q: -0.324530
 29700/100000: episode: 297, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.949, mean reward: -0.189 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.156, 10.098], loss: 0.002507, mae: 0.050482, mean_q: -0.293007
 29800/100000: episode: 298, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.022, mean reward: -0.170 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.329, 10.098], loss: 0.002367, mae: 0.049350, mean_q: -0.296734
 29900/100000: episode: 299, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -14.318, mean reward: -0.143 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.797, 10.098], loss: 0.002423, mae: 0.048724, mean_q: -0.334375
 30000/100000: episode: 300, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.246, mean reward: -0.172 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.619, 10.136], loss: 0.002482, mae: 0.049885, mean_q: -0.330248
 30100/100000: episode: 301, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -20.008, mean reward: -0.200 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.880, 10.098], loss: 0.002691, mae: 0.053067, mean_q: -0.265450
 30200/100000: episode: 302, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.716, mean reward: -0.157 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.018, 10.257], loss: 0.002674, mae: 0.053069, mean_q: -0.283791
 30300/100000: episode: 303, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -15.517, mean reward: -0.155 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.811, 10.098], loss: 0.002471, mae: 0.050106, mean_q: -0.323262
 30400/100000: episode: 304, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.172, mean reward: -0.192 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.115, 10.098], loss: 0.002530, mae: 0.050283, mean_q: -0.307414
 30500/100000: episode: 305, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.007, mean reward: -0.160 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.873, 10.259], loss: 0.006525, mae: 0.071615, mean_q: -0.314219
 30600/100000: episode: 306, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.448, mean reward: -0.164 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.555, 10.098], loss: 0.003113, mae: 0.057491, mean_q: -0.306321
 30700/100000: episode: 307, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.732, mean reward: -0.197 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.101, 10.126], loss: 0.002362, mae: 0.048705, mean_q: -0.329738
 30800/100000: episode: 308, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.935, mean reward: -0.169 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.605, 10.098], loss: 0.002609, mae: 0.052108, mean_q: -0.279162
 30900/100000: episode: 309, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.954, mean reward: -0.160 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.843, 10.098], loss: 0.002679, mae: 0.052856, mean_q: -0.269344
 31000/100000: episode: 310, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.187, mean reward: -0.152 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.716, 10.098], loss: 0.002500, mae: 0.050217, mean_q: -0.304680
 31100/100000: episode: 311, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -16.675, mean reward: -0.167 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.244, 10.376], loss: 0.002793, mae: 0.053501, mean_q: -0.321096
 31200/100000: episode: 312, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.435, mean reward: -0.154 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.037, 10.098], loss: 0.002771, mae: 0.054649, mean_q: -0.275369
 31300/100000: episode: 313, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.914, mean reward: -0.189 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.769, 10.136], loss: 0.002507, mae: 0.050007, mean_q: -0.305876
 31400/100000: episode: 314, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.735, mean reward: -0.157 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.725, 10.276], loss: 0.002553, mae: 0.050848, mean_q: -0.304654
 31500/100000: episode: 315, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: -19.558, mean reward: -0.196 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.059, 10.168], loss: 0.002805, mae: 0.053183, mean_q: -0.293618
 31600/100000: episode: 316, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.918, mean reward: -0.159 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.885, 10.098], loss: 0.002697, mae: 0.053489, mean_q: -0.297838
 31700/100000: episode: 317, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.770, mean reward: -0.168 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.537, 10.161], loss: 0.004455, mae: 0.063275, mean_q: -0.304262
 31800/100000: episode: 318, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.563, mean reward: -0.186 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.481, 10.098], loss: 0.003022, mae: 0.056895, mean_q: -0.326124
 31900/100000: episode: 319, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.662, mean reward: -0.177 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.598, 10.222], loss: 0.002571, mae: 0.051756, mean_q: -0.287636
 32000/100000: episode: 320, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.364, mean reward: -0.184 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.396, 10.148], loss: 0.002821, mae: 0.053394, mean_q: -0.338663
 32100/100000: episode: 321, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -12.665, mean reward: -0.127 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.519, 10.136], loss: 0.002597, mae: 0.052122, mean_q: -0.322054
 32200/100000: episode: 322, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -20.482, mean reward: -0.205 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.496, 10.143], loss: 0.002631, mae: 0.052462, mean_q: -0.280106
 32300/100000: episode: 323, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.335, mean reward: -0.173 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.319, 10.245], loss: 0.002593, mae: 0.051043, mean_q: -0.346232
 32400/100000: episode: 324, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.873, mean reward: -0.179 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.209, 10.098], loss: 0.002499, mae: 0.050422, mean_q: -0.316323
 32500/100000: episode: 325, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -18.056, mean reward: -0.181 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.577, 10.234], loss: 0.002648, mae: 0.051732, mean_q: -0.282604
 32600/100000: episode: 326, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -12.370, mean reward: -0.124 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.478, 10.098], loss: 0.002696, mae: 0.051896, mean_q: -0.306447
 32700/100000: episode: 327, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.972, mean reward: -0.190 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.599, 10.098], loss: 0.002703, mae: 0.052729, mean_q: -0.278147
 32800/100000: episode: 328, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.399, mean reward: -0.154 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.467, 10.296], loss: 0.002523, mae: 0.051383, mean_q: -0.330158
 32900/100000: episode: 329, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -16.416, mean reward: -0.164 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.572, 10.115], loss: 0.002712, mae: 0.052429, mean_q: -0.297837
 33000/100000: episode: 330, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.128, mean reward: -0.161 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.897, 10.098], loss: 0.002467, mae: 0.050156, mean_q: -0.306003
 33100/100000: episode: 331, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -20.092, mean reward: -0.201 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.969, 10.147], loss: 0.002633, mae: 0.051459, mean_q: -0.318894
 33200/100000: episode: 332, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.403, mean reward: -0.154 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.800, 10.098], loss: 0.002687, mae: 0.052339, mean_q: -0.265796
 33300/100000: episode: 333, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.006, mean reward: -0.170 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.431, 10.113], loss: 0.002545, mae: 0.051439, mean_q: -0.305933
 33400/100000: episode: 334, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -14.965, mean reward: -0.150 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.773, 10.098], loss: 0.002484, mae: 0.049889, mean_q: -0.337125
 33500/100000: episode: 335, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.533, mean reward: -0.185 [-1.000, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.196, 10.283], loss: 0.002731, mae: 0.052522, mean_q: -0.314729
 33600/100000: episode: 336, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.769, mean reward: -0.178 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.539, 10.098], loss: 0.002573, mae: 0.052021, mean_q: -0.296326
 33700/100000: episode: 337, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -19.616, mean reward: -0.196 [-1.000, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.729, 10.155], loss: 0.002563, mae: 0.051040, mean_q: -0.364536
 33800/100000: episode: 338, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.179, mean reward: -0.172 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.957, 10.098], loss: 0.002638, mae: 0.052270, mean_q: -0.299257
 33900/100000: episode: 339, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -12.104, mean reward: -0.121 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.629, 10.098], loss: 0.002444, mae: 0.049891, mean_q: -0.330530
 34000/100000: episode: 340, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -13.239, mean reward: -0.132 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.665, 10.136], loss: 0.002488, mae: 0.049709, mean_q: -0.323806
 34100/100000: episode: 341, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -10.200, mean reward: -0.102 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.526, 10.098], loss: 0.002610, mae: 0.050477, mean_q: -0.304260
 34200/100000: episode: 342, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -11.712, mean reward: -0.117 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.105, 10.098], loss: 0.002566, mae: 0.050884, mean_q: -0.331667
 34300/100000: episode: 343, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -9.477, mean reward: -0.095 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.789, 10.382], loss: 0.002512, mae: 0.050215, mean_q: -0.321855
 34400/100000: episode: 344, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.342, mean reward: -0.173 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.806, 10.156], loss: 0.002559, mae: 0.049842, mean_q: -0.343848
 34500/100000: episode: 345, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -20.084, mean reward: -0.201 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.623, 10.098], loss: 0.003697, mae: 0.058360, mean_q: -0.290419
 34600/100000: episode: 346, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.179, mean reward: -0.132 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.548, 10.303], loss: 0.004104, mae: 0.064634, mean_q: -0.312117
 34700/100000: episode: 347, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.331, mean reward: -0.163 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.695, 10.307], loss: 0.002968, mae: 0.054634, mean_q: -0.322994
 34800/100000: episode: 348, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.851, mean reward: -0.169 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.758, 10.170], loss: 0.002574, mae: 0.050318, mean_q: -0.314313
 34900/100000: episode: 349, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.615, mean reward: -0.156 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.865, 10.098], loss: 0.002545, mae: 0.049912, mean_q: -0.303750
 35000/100000: episode: 350, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.009, mean reward: -0.170 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.612, 10.098], loss: 0.002600, mae: 0.051065, mean_q: -0.304589
 35100/100000: episode: 351, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -16.780, mean reward: -0.168 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.427, 10.098], loss: 0.002586, mae: 0.050240, mean_q: -0.311788
 35200/100000: episode: 352, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.052, mean reward: -0.141 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.130, 10.400], loss: 0.002536, mae: 0.050441, mean_q: -0.294722
 35300/100000: episode: 353, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.251, mean reward: -0.183 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.327, 10.098], loss: 0.002638, mae: 0.052313, mean_q: -0.277170
 35400/100000: episode: 354, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.034, mean reward: -0.190 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.412, 10.143], loss: 0.002498, mae: 0.049854, mean_q: -0.325722
 35500/100000: episode: 355, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.662, mean reward: -0.207 [-1.000, 0.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.525, 10.098], loss: 0.002763, mae: 0.052337, mean_q: -0.277312
 35600/100000: episode: 356, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.212, mean reward: -0.182 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.808, 10.158], loss: 0.002507, mae: 0.049451, mean_q: -0.294009
 35700/100000: episode: 357, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -13.826, mean reward: -0.138 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.395, 10.324], loss: 0.002773, mae: 0.053434, mean_q: -0.314311
 35800/100000: episode: 358, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -19.510, mean reward: -0.195 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.639, 10.098], loss: 0.002476, mae: 0.049612, mean_q: -0.299716
 35900/100000: episode: 359, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.426, mean reward: -0.184 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.610, 10.136], loss: 0.002592, mae: 0.051016, mean_q: -0.341497
 36000/100000: episode: 360, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -16.097, mean reward: -0.161 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.838, 10.286], loss: 0.002613, mae: 0.050701, mean_q: -0.295885
 36100/100000: episode: 361, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.123, mean reward: -0.171 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.142, 10.254], loss: 0.002578, mae: 0.051059, mean_q: -0.307763
 36200/100000: episode: 362, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -14.821, mean reward: -0.148 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.786, 10.098], loss: 0.002572, mae: 0.050556, mean_q: -0.330907
 36300/100000: episode: 363, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.807, mean reward: -0.188 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.696, 10.098], loss: 0.002635, mae: 0.051092, mean_q: -0.334836
 36400/100000: episode: 364, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.883, mean reward: -0.169 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.915, 10.098], loss: 0.002657, mae: 0.051257, mean_q: -0.298616
 36500/100000: episode: 365, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.169, mean reward: -0.182 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.447, 10.098], loss: 0.002502, mae: 0.049339, mean_q: -0.329140
 36600/100000: episode: 366, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -13.077, mean reward: -0.131 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.551, 10.098], loss: 0.002618, mae: 0.050240, mean_q: -0.325914
 36700/100000: episode: 367, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.372, mean reward: -0.184 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.058, 10.098], loss: 0.002607, mae: 0.051057, mean_q: -0.313780
 36800/100000: episode: 368, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.420, mean reward: -0.154 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.942, 10.098], loss: 0.002703, mae: 0.052545, mean_q: -0.289062
 36900/100000: episode: 369, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -16.585, mean reward: -0.166 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.174, 10.134], loss: 0.002392, mae: 0.048871, mean_q: -0.326157
 37000/100000: episode: 370, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -11.808, mean reward: -0.118 [-1.000, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.017, 10.363], loss: 0.002488, mae: 0.050021, mean_q: -0.316395
 37100/100000: episode: 371, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -19.074, mean reward: -0.191 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.970, 10.098], loss: 0.002597, mae: 0.050295, mean_q: -0.325518
 37200/100000: episode: 372, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -14.923, mean reward: -0.149 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.581, 10.348], loss: 0.002414, mae: 0.048557, mean_q: -0.315006
 37300/100000: episode: 373, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.109, mean reward: -0.171 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.722, 10.280], loss: 0.002515, mae: 0.050426, mean_q: -0.291079
 37400/100000: episode: 374, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.485, mean reward: -0.185 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.400, 10.098], loss: 0.002616, mae: 0.051058, mean_q: -0.312943
 37500/100000: episode: 375, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.776, mean reward: -0.198 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.651, 10.103], loss: 0.002668, mae: 0.051698, mean_q: -0.289931
 37600/100000: episode: 376, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.191, mean reward: -0.162 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.270, 10.098], loss: 0.002480, mae: 0.049660, mean_q: -0.307368
 37700/100000: episode: 377, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.481, mean reward: -0.145 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.554, 10.098], loss: 0.002540, mae: 0.050722, mean_q: -0.298091
 37800/100000: episode: 378, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.651, mean reward: -0.167 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.660, 10.289], loss: 0.003284, mae: 0.057423, mean_q: -0.263088
 37900/100000: episode: 379, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.155, mean reward: -0.182 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.448, 10.180], loss: 0.002646, mae: 0.053787, mean_q: -0.315869
 38000/100000: episode: 380, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.531, mean reward: -0.195 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.634, 10.098], loss: 0.002319, mae: 0.048275, mean_q: -0.358956
 38100/100000: episode: 381, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -11.581, mean reward: -0.116 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.810, 10.466], loss: 0.002668, mae: 0.052427, mean_q: -0.271387
 38200/100000: episode: 382, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.947, mean reward: -0.189 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.989, 10.098], loss: 0.002451, mae: 0.049051, mean_q: -0.323361
 38300/100000: episode: 383, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -14.844, mean reward: -0.148 [-1.000, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.979, 10.411], loss: 0.002457, mae: 0.049430, mean_q: -0.310919
 38400/100000: episode: 384, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -13.834, mean reward: -0.138 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.782, 10.220], loss: 0.002455, mae: 0.048928, mean_q: -0.311887
 38500/100000: episode: 385, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.334, mean reward: -0.143 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.897, 10.191], loss: 0.002430, mae: 0.048899, mean_q: -0.314261
 38600/100000: episode: 386, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.870, mean reward: -0.169 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.816, 10.311], loss: 0.002462, mae: 0.049899, mean_q: -0.312755
 38700/100000: episode: 387, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.529, mean reward: -0.155 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.983, 10.437], loss: 0.002347, mae: 0.048545, mean_q: -0.360680
 38800/100000: episode: 388, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.413, mean reward: -0.164 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.477, 10.098], loss: 0.002665, mae: 0.053607, mean_q: -0.317801
 38900/100000: episode: 389, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.833, mean reward: -0.178 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.458, 10.116], loss: 0.002417, mae: 0.049343, mean_q: -0.318754
 39000/100000: episode: 390, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.350, mean reward: -0.183 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.770, 10.119], loss: 0.002637, mae: 0.051779, mean_q: -0.325511
 39100/100000: episode: 391, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.363, mean reward: -0.154 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.313, 10.330], loss: 0.002413, mae: 0.049618, mean_q: -0.297572
 39200/100000: episode: 392, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.599, mean reward: -0.166 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.128, 10.180], loss: 0.002566, mae: 0.050769, mean_q: -0.309415
 39300/100000: episode: 393, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.924, mean reward: -0.159 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.393, 10.399], loss: 0.002754, mae: 0.054081, mean_q: -0.286034
 39400/100000: episode: 394, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.743, mean reward: -0.177 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.923, 10.098], loss: 0.002426, mae: 0.049337, mean_q: -0.295658
 39500/100000: episode: 395, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.152, mean reward: -0.152 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.966, 10.314], loss: 0.002586, mae: 0.051770, mean_q: -0.280843
 39600/100000: episode: 396, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.615, mean reward: -0.156 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.737, 10.098], loss: 0.002678, mae: 0.052122, mean_q: -0.318389
 39700/100000: episode: 397, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.563, mean reward: -0.196 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.783, 10.098], loss: 0.003908, mae: 0.061283, mean_q: -0.310682
 39800/100000: episode: 398, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.086, mean reward: -0.161 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.703, 10.099], loss: 0.002816, mae: 0.054663, mean_q: -0.306059
 39900/100000: episode: 399, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -10.699, mean reward: -0.107 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.221, 10.338], loss: 0.002443, mae: 0.049367, mean_q: -0.329009
 40000/100000: episode: 400, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.923, mean reward: -0.199 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.039, 10.232], loss: 0.002570, mae: 0.050698, mean_q: -0.309675
 40100/100000: episode: 401, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.234, mean reward: -0.152 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.828, 10.177], loss: 0.002448, mae: 0.048416, mean_q: -0.345248
 40200/100000: episode: 402, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.419, mean reward: -0.184 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.965, 10.303], loss: 0.002445, mae: 0.049603, mean_q: -0.326282
 40300/100000: episode: 403, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.076, mean reward: -0.171 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.287, 10.098], loss: 0.002488, mae: 0.049517, mean_q: -0.286725
 40400/100000: episode: 404, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.835, mean reward: -0.158 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.445, 10.320], loss: 0.002596, mae: 0.049763, mean_q: -0.324492
 40500/100000: episode: 405, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.794, mean reward: -0.178 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.437, 10.254], loss: 0.002382, mae: 0.048883, mean_q: -0.334346
 40600/100000: episode: 406, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -20.698, mean reward: -0.207 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.394, 10.098], loss: 0.002360, mae: 0.048492, mean_q: -0.286377
 40700/100000: episode: 407, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.730, mean reward: -0.167 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.707, 10.357], loss: 0.002771, mae: 0.052937, mean_q: -0.267291
 40800/100000: episode: 408, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.843, mean reward: -0.178 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.074, 10.145], loss: 0.002576, mae: 0.050549, mean_q: -0.305026
 40900/100000: episode: 409, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.422, mean reward: -0.154 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.693, 10.336], loss: 0.002574, mae: 0.049914, mean_q: -0.337482
 41000/100000: episode: 410, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.817, mean reward: -0.158 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.606, 10.142], loss: 0.002464, mae: 0.049550, mean_q: -0.328474
 41100/100000: episode: 411, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.606, mean reward: -0.156 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.446, 10.135], loss: 0.002530, mae: 0.050154, mean_q: -0.278776
 41200/100000: episode: 412, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.414, mean reward: -0.174 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.908, 10.098], loss: 0.003468, mae: 0.055133, mean_q: -0.334145
 41300/100000: episode: 413, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -10.560, mean reward: -0.106 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.756, 10.248], loss: 0.006971, mae: 0.081005, mean_q: -0.342043
 41400/100000: episode: 414, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -21.369, mean reward: -0.214 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.513, 10.217], loss: 0.002530, mae: 0.051060, mean_q: -0.323040
 41500/100000: episode: 415, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.100, mean reward: -0.181 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.637, 10.203], loss: 0.002540, mae: 0.051087, mean_q: -0.337715
 41600/100000: episode: 416, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.538, mean reward: -0.175 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.737, 10.098], loss: 0.002558, mae: 0.051940, mean_q: -0.288738
 41700/100000: episode: 417, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.848, mean reward: -0.178 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.645, 10.120], loss: 0.002644, mae: 0.052919, mean_q: -0.301955
 41800/100000: episode: 418, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -15.527, mean reward: -0.155 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.365, 10.181], loss: 0.002625, mae: 0.050952, mean_q: -0.295635
 41900/100000: episode: 419, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.798, mean reward: -0.198 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.678, 10.190], loss: 0.002589, mae: 0.050958, mean_q: -0.309902
 42000/100000: episode: 420, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.007, mean reward: -0.180 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.229, 10.098], loss: 0.002633, mae: 0.050228, mean_q: -0.339277
 42100/100000: episode: 421, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -12.963, mean reward: -0.130 [-1.000, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.218, 10.221], loss: 0.002633, mae: 0.051165, mean_q: -0.300702
 42200/100000: episode: 422, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -11.310, mean reward: -0.113 [-1.000, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.561, 10.098], loss: 0.002487, mae: 0.049274, mean_q: -0.314621
 42300/100000: episode: 423, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.314, mean reward: -0.183 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.104, 10.098], loss: 0.002587, mae: 0.050846, mean_q: -0.298528
 42400/100000: episode: 424, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -15.612, mean reward: -0.156 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.033, 10.098], loss: 0.002522, mae: 0.050090, mean_q: -0.316843
 42500/100000: episode: 425, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -18.146, mean reward: -0.181 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.785, 10.098], loss: 0.002486, mae: 0.049151, mean_q: -0.329505
 42600/100000: episode: 426, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -13.662, mean reward: -0.137 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.647, 10.319], loss: 0.002805, mae: 0.052370, mean_q: -0.307924
 42700/100000: episode: 427, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.345, mean reward: -0.183 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.884, 10.098], loss: 0.002625, mae: 0.051286, mean_q: -0.277497
 42800/100000: episode: 428, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.182, mean reward: -0.192 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.865, 10.151], loss: 0.002685, mae: 0.052482, mean_q: -0.295899
 42900/100000: episode: 429, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -6.351, mean reward: -0.064 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.740, 10.398], loss: 0.002685, mae: 0.050519, mean_q: -0.325789
 43000/100000: episode: 430, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -11.555, mean reward: -0.116 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.964, 10.431], loss: 0.002521, mae: 0.049779, mean_q: -0.317823
 43100/100000: episode: 431, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.786, mean reward: -0.178 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.921, 10.098], loss: 0.002745, mae: 0.051949, mean_q: -0.297788
 43200/100000: episode: 432, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.941, mean reward: -0.189 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.388, 10.155], loss: 0.002545, mae: 0.049714, mean_q: -0.313403
 43300/100000: episode: 433, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -16.666, mean reward: -0.167 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.307, 10.098], loss: 0.002773, mae: 0.052610, mean_q: -0.272623
 43400/100000: episode: 434, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.418, mean reward: -0.184 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.084, 10.098], loss: 0.002778, mae: 0.054187, mean_q: -0.314269
 43500/100000: episode: 435, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.721, mean reward: -0.187 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.412, 10.269], loss: 0.002666, mae: 0.050699, mean_q: -0.331056
 43600/100000: episode: 436, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.673, mean reward: -0.177 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.607, 10.251], loss: 0.002895, mae: 0.053935, mean_q: -0.293365
 43700/100000: episode: 437, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.981, mean reward: -0.160 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.935, 10.098], loss: 0.002532, mae: 0.049623, mean_q: -0.305955
 43800/100000: episode: 438, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.078, mean reward: -0.181 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.166, 10.145], loss: 0.006023, mae: 0.067646, mean_q: -0.322631
 43900/100000: episode: 439, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.264, mean reward: -0.163 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.238, 10.098], loss: 0.003698, mae: 0.064106, mean_q: -0.306978
 44000/100000: episode: 440, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.326, mean reward: -0.153 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.043, 10.098], loss: 0.002621, mae: 0.050664, mean_q: -0.311217
 44100/100000: episode: 441, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -7.595, mean reward: -0.076 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.960, 10.478], loss: 0.002665, mae: 0.051452, mean_q: -0.314584
 44200/100000: episode: 442, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -8.424, mean reward: -0.084 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.615, 10.305], loss: 0.002557, mae: 0.050299, mean_q: -0.272707
 44300/100000: episode: 443, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.358, mean reward: -0.184 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.604, 10.180], loss: 0.002706, mae: 0.050797, mean_q: -0.316442
 44400/100000: episode: 444, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.155, mean reward: -0.172 [-1.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.706, 10.098], loss: 0.002583, mae: 0.049663, mean_q: -0.317214
 44500/100000: episode: 445, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.991, mean reward: -0.190 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.622, 10.144], loss: 0.002526, mae: 0.049690, mean_q: -0.321673
 44600/100000: episode: 446, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.887, mean reward: -0.149 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.521, 10.098], loss: 0.002715, mae: 0.051620, mean_q: -0.312244
 44700/100000: episode: 447, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.187, mean reward: -0.182 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.200, 10.201], loss: 0.002570, mae: 0.049232, mean_q: -0.326895
 44800/100000: episode: 448, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.208, mean reward: -0.162 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.689, 10.282], loss: 0.002510, mae: 0.049140, mean_q: -0.308899
 44900/100000: episode: 449, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.033, mean reward: -0.170 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.016, 10.127], loss: 0.002657, mae: 0.050432, mean_q: -0.313638
 45000/100000: episode: 450, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -20.282, mean reward: -0.203 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.334, 10.163], loss: 0.002628, mae: 0.050366, mean_q: -0.299375
 45100/100000: episode: 451, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.474, mean reward: -0.175 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.837, 10.304], loss: 0.002623, mae: 0.050546, mean_q: -0.305789
 45200/100000: episode: 452, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.863, mean reward: -0.189 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.101, 10.189], loss: 0.002605, mae: 0.049631, mean_q: -0.329394
 45300/100000: episode: 453, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -13.248, mean reward: -0.132 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.815, 10.098], loss: 0.002556, mae: 0.049193, mean_q: -0.324721
 45400/100000: episode: 454, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.276, mean reward: -0.183 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.054, 10.231], loss: 0.002564, mae: 0.049049, mean_q: -0.304075
 45500/100000: episode: 455, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -14.853, mean reward: -0.149 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.701, 10.279], loss: 0.002821, mae: 0.053447, mean_q: -0.303980
 45600/100000: episode: 456, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -15.691, mean reward: -0.157 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.305, 10.345], loss: 0.002938, mae: 0.054238, mean_q: -0.298212
 45700/100000: episode: 457, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.917, mean reward: -0.189 [-1.000, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.036, 10.273], loss: 0.002584, mae: 0.050652, mean_q: -0.272926
 45800/100000: episode: 458, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.701, mean reward: -0.157 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.033, 10.098], loss: 0.002592, mae: 0.049626, mean_q: -0.317296
 45900/100000: episode: 459, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.836, mean reward: -0.198 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.994, 10.118], loss: 0.002662, mae: 0.051096, mean_q: -0.290273
 46000/100000: episode: 460, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -11.975, mean reward: -0.120 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.100, 10.175], loss: 0.002519, mae: 0.048779, mean_q: -0.334758
 46100/100000: episode: 461, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.352, mean reward: -0.154 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.859, 10.181], loss: 0.002529, mae: 0.049992, mean_q: -0.319667
 46200/100000: episode: 462, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.106, mean reward: -0.171 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.930, 10.098], loss: 0.002436, mae: 0.049677, mean_q: -0.294994
 46300/100000: episode: 463, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -10.370, mean reward: -0.104 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.911, 10.098], loss: 0.002513, mae: 0.047814, mean_q: -0.322270
 46400/100000: episode: 464, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.673, mean reward: -0.177 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.641, 10.301], loss: 0.002712, mae: 0.051067, mean_q: -0.312178
 46500/100000: episode: 465, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.548, mean reward: -0.175 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.347, 10.248], loss: 0.002641, mae: 0.050427, mean_q: -0.333203
 46600/100000: episode: 466, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.401, mean reward: -0.164 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.968, 10.378], loss: 0.002971, mae: 0.053970, mean_q: -0.306718
 46700/100000: episode: 467, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.998, mean reward: -0.180 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.082, 10.098], loss: 0.002477, mae: 0.049868, mean_q: -0.297352
 46800/100000: episode: 468, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -9.218, mean reward: -0.092 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.631, 10.098], loss: 0.002456, mae: 0.048837, mean_q: -0.297487
 46900/100000: episode: 469, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.189, mean reward: -0.182 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.026, 10.169], loss: 0.002616, mae: 0.050561, mean_q: -0.275623
 47000/100000: episode: 470, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -20.162, mean reward: -0.202 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.615, 10.172], loss: 0.002605, mae: 0.049968, mean_q: -0.309427
 47100/100000: episode: 471, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -13.135, mean reward: -0.131 [-1.000, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.721, 10.441], loss: 0.002585, mae: 0.049959, mean_q: -0.324283
 47200/100000: episode: 472, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.304, mean reward: -0.173 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.629, 10.098], loss: 0.002599, mae: 0.050572, mean_q: -0.274981
 47300/100000: episode: 473, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.244, mean reward: -0.182 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.787, 10.098], loss: 0.002391, mae: 0.048676, mean_q: -0.301661
 47400/100000: episode: 474, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.501, mean reward: -0.185 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.570, 10.133], loss: 0.002487, mae: 0.050050, mean_q: -0.286968
 47500/100000: episode: 475, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.359, mean reward: -0.124 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.904, 10.416], loss: 0.004387, mae: 0.061805, mean_q: -0.306667
 47600/100000: episode: 476, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.261, mean reward: -0.183 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.077, 10.325], loss: 0.002658, mae: 0.051991, mean_q: -0.306660
 47700/100000: episode: 477, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -17.220, mean reward: -0.172 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.981, 10.122], loss: 0.002591, mae: 0.049901, mean_q: -0.262171
 47800/100000: episode: 478, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.370, mean reward: -0.184 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.848, 10.131], loss: 0.002511, mae: 0.049118, mean_q: -0.288865
 47900/100000: episode: 479, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.082, mean reward: -0.181 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.569, 10.157], loss: 0.002319, mae: 0.046621, mean_q: -0.342092
 48000/100000: episode: 480, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -20.442, mean reward: -0.204 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.719, 10.098], loss: 0.002324, mae: 0.047455, mean_q: -0.302177
 48100/100000: episode: 481, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.973, mean reward: -0.190 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.975, 10.237], loss: 0.002490, mae: 0.049114, mean_q: -0.297119
 48200/100000: episode: 482, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.903, mean reward: -0.169 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.883, 10.331], loss: 0.002730, mae: 0.051879, mean_q: -0.306134
 48300/100000: episode: 483, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.406, mean reward: -0.164 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.253, 10.313], loss: 0.002807, mae: 0.051660, mean_q: -0.305732
 48400/100000: episode: 484, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -19.925, mean reward: -0.199 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.177, 10.115], loss: 0.002643, mae: 0.051083, mean_q: -0.295771
 48500/100000: episode: 485, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.594, mean reward: -0.186 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.764, 10.098], loss: 0.002740, mae: 0.050839, mean_q: -0.322045
 48600/100000: episode: 486, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -15.411, mean reward: -0.154 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.768, 10.221], loss: 0.002504, mae: 0.049134, mean_q: -0.302350
 48700/100000: episode: 487, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.117, mean reward: -0.191 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.448, 10.228], loss: 0.002526, mae: 0.049935, mean_q: -0.309737
 48800/100000: episode: 488, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.704, mean reward: -0.157 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.394, 10.098], loss: 0.002543, mae: 0.050212, mean_q: -0.300600
 48900/100000: episode: 489, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -13.542, mean reward: -0.135 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.335, 10.289], loss: 0.003412, mae: 0.057353, mean_q: -0.319920
 49000/100000: episode: 490, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -13.567, mean reward: -0.136 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.338, 10.105], loss: 0.002566, mae: 0.050975, mean_q: -0.316991
 49100/100000: episode: 491, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.090, mean reward: -0.151 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.213, 10.231], loss: 0.002700, mae: 0.050844, mean_q: -0.309061
 49200/100000: episode: 492, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.975, mean reward: -0.180 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.984, 10.098], loss: 0.002454, mae: 0.048453, mean_q: -0.310638
 49300/100000: episode: 493, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.511, mean reward: -0.175 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.180, 10.098], loss: 0.002531, mae: 0.049309, mean_q: -0.320908
 49400/100000: episode: 494, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.675, mean reward: -0.167 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.945, 10.098], loss: 0.002415, mae: 0.048766, mean_q: -0.293367
 49500/100000: episode: 495, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -12.535, mean reward: -0.125 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.764, 10.229], loss: 0.002659, mae: 0.049778, mean_q: -0.359449
 49600/100000: episode: 496, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.811, mean reward: -0.178 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.071, 10.098], loss: 0.002394, mae: 0.047745, mean_q: -0.318527
 49700/100000: episode: 497, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.321, mean reward: -0.193 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.644, 10.316], loss: 0.002749, mae: 0.052642, mean_q: -0.337881
 49800/100000: episode: 498, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.045, mean reward: -0.150 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.700, 10.246], loss: 0.002789, mae: 0.051952, mean_q: -0.310133
 49900/100000: episode: 499, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.511, mean reward: -0.175 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.413, 10.098], loss: 0.002461, mae: 0.048892, mean_q: -0.353748
 50000/100000: episode: 500, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -12.156, mean reward: -0.122 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.565, 10.392], loss: 0.002635, mae: 0.050551, mean_q: -0.320842
 50100/100000: episode: 501, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -14.879, mean reward: -0.149 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.383, 10.300], loss: 0.002422, mae: 0.048478, mean_q: -0.327980
 50200/100000: episode: 502, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -14.308, mean reward: -0.143 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.603, 10.098], loss: 0.002421, mae: 0.047987, mean_q: -0.317530
 50300/100000: episode: 503, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.544, mean reward: -0.185 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.545, 10.267], loss: 0.005742, mae: 0.071971, mean_q: -0.324772
 50400/100000: episode: 504, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.232, mean reward: -0.162 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.927, 10.313], loss: 0.002387, mae: 0.048582, mean_q: -0.337302
 50500/100000: episode: 505, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -11.098, mean reward: -0.111 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.711, 10.098], loss: 0.002458, mae: 0.049136, mean_q: -0.268105
 50600/100000: episode: 506, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -20.097, mean reward: -0.201 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.186, 10.213], loss: 0.002364, mae: 0.047549, mean_q: -0.312055
 50700/100000: episode: 507, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -20.226, mean reward: -0.202 [-1.000, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.722, 10.169], loss: 0.002293, mae: 0.047238, mean_q: -0.332336
 50800/100000: episode: 508, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -15.727, mean reward: -0.157 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.695, 10.098], loss: 0.002533, mae: 0.049425, mean_q: -0.300835
 50900/100000: episode: 509, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.849, mean reward: -0.178 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.296, 10.098], loss: 0.002535, mae: 0.050956, mean_q: -0.287196
 51000/100000: episode: 510, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.917, mean reward: -0.189 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.306, 10.098], loss: 0.002371, mae: 0.048025, mean_q: -0.310960
 51100/100000: episode: 511, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.141, mean reward: -0.191 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.585, 10.299], loss: 0.002449, mae: 0.049155, mean_q: -0.273693
 51200/100000: episode: 512, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.683, mean reward: -0.177 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.294, 10.159], loss: 0.002325, mae: 0.047471, mean_q: -0.324780
 51300/100000: episode: 513, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -20.225, mean reward: -0.202 [-1.000, 0.249], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.205, 10.098], loss: 0.002478, mae: 0.048757, mean_q: -0.340966
 51400/100000: episode: 514, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -12.563, mean reward: -0.126 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.748, 10.243], loss: 0.002303, mae: 0.047330, mean_q: -0.324419
 51500/100000: episode: 515, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -18.901, mean reward: -0.189 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.437, 10.206], loss: 0.002349, mae: 0.047385, mean_q: -0.341134
 51600/100000: episode: 516, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.495, mean reward: -0.165 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.857, 10.098], loss: 0.002468, mae: 0.048835, mean_q: -0.325746
 51700/100000: episode: 517, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.805, mean reward: -0.178 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.900, 10.122], loss: 0.002447, mae: 0.048788, mean_q: -0.289796
 51800/100000: episode: 518, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.395, mean reward: -0.164 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.970, 10.320], loss: 0.002422, mae: 0.049793, mean_q: -0.307814
 51900/100000: episode: 519, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.084, mean reward: -0.171 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.480, 10.105], loss: 0.002413, mae: 0.049471, mean_q: -0.295616
 52000/100000: episode: 520, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -20.289, mean reward: -0.203 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.685, 10.098], loss: 0.002267, mae: 0.047433, mean_q: -0.296122
 52100/100000: episode: 521, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.349, mean reward: -0.163 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.557, 10.098], loss: 0.002168, mae: 0.045903, mean_q: -0.335660
 52200/100000: episode: 522, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.229, mean reward: -0.192 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.760, 10.113], loss: 0.002387, mae: 0.048889, mean_q: -0.294947
 52300/100000: episode: 523, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.280, mean reward: -0.183 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.911, 10.206], loss: 0.002291, mae: 0.047826, mean_q: -0.312892
 52400/100000: episode: 524, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -16.203, mean reward: -0.162 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.833, 10.098], loss: 0.002417, mae: 0.048930, mean_q: -0.329938
 52500/100000: episode: 525, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -9.836, mean reward: -0.098 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.576, 10.098], loss: 0.002239, mae: 0.047334, mean_q: -0.337896
 52600/100000: episode: 526, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.031, mean reward: -0.190 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.718, 10.115], loss: 0.002421, mae: 0.048343, mean_q: -0.331613
 52700/100000: episode: 527, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -10.132, mean reward: -0.101 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.976, 10.098], loss: 0.002267, mae: 0.047672, mean_q: -0.320635
 52800/100000: episode: 528, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.172, mean reward: -0.172 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.922, 10.126], loss: 0.002379, mae: 0.049168, mean_q: -0.308638
 52900/100000: episode: 529, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.051, mean reward: -0.181 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.498, 10.098], loss: 0.008068, mae: 0.080212, mean_q: -0.291352
 53000/100000: episode: 530, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -11.352, mean reward: -0.114 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.004, 10.098], loss: 0.007860, mae: 0.078307, mean_q: -0.311114
 53100/100000: episode: 531, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.906, mean reward: -0.189 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.875, 10.098], loss: 0.002556, mae: 0.050272, mean_q: -0.298695
 53200/100000: episode: 532, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -7.768, mean reward: -0.078 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.324, 10.400], loss: 0.002261, mae: 0.047113, mean_q: -0.335073
 53300/100000: episode: 533, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.803, mean reward: -0.148 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.764, 10.098], loss: 0.002506, mae: 0.049003, mean_q: -0.305469
 53400/100000: episode: 534, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.614, mean reward: -0.156 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.859, 10.098], loss: 0.002273, mae: 0.047111, mean_q: -0.319977
 53500/100000: episode: 535, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.683, mean reward: -0.167 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.106, 10.215], loss: 0.002335, mae: 0.047222, mean_q: -0.331895
 53600/100000: episode: 536, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -11.948, mean reward: -0.119 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.009, 10.098], loss: 0.002404, mae: 0.048833, mean_q: -0.292626
 53700/100000: episode: 537, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.129, mean reward: -0.171 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.782, 10.098], loss: 0.002399, mae: 0.048257, mean_q: -0.309097
 53800/100000: episode: 538, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.551, mean reward: -0.186 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.005, 10.195], loss: 0.002458, mae: 0.049405, mean_q: -0.287438
 53900/100000: episode: 539, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -15.830, mean reward: -0.158 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.489, 10.275], loss: 0.002847, mae: 0.052912, mean_q: -0.280127
 54000/100000: episode: 540, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.332, mean reward: -0.183 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.124, 10.134], loss: 0.002535, mae: 0.049195, mean_q: -0.296381
 54100/100000: episode: 541, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.782, mean reward: -0.188 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.222, 10.255], loss: 0.002512, mae: 0.049511, mean_q: -0.307942
 54200/100000: episode: 542, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -13.360, mean reward: -0.134 [-1.000, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.448, 10.265], loss: 0.002427, mae: 0.048082, mean_q: -0.323560
 54300/100000: episode: 543, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -20.443, mean reward: -0.204 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.572, 10.099], loss: 0.002415, mae: 0.047931, mean_q: -0.299358
 54400/100000: episode: 544, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -14.462, mean reward: -0.145 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.497, 10.098], loss: 0.002418, mae: 0.048198, mean_q: -0.325683
 54500/100000: episode: 545, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -12.935, mean reward: -0.129 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.895, 10.366], loss: 0.002506, mae: 0.050212, mean_q: -0.337733
 54600/100000: episode: 546, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.036, mean reward: -0.180 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.657, 10.098], loss: 0.002501, mae: 0.049066, mean_q: -0.288985
 54700/100000: episode: 547, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.590, mean reward: -0.156 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.729, 10.110], loss: 0.002439, mae: 0.048374, mean_q: -0.297288
 54800/100000: episode: 548, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.449, mean reward: -0.184 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.868, 10.200], loss: 0.002576, mae: 0.048643, mean_q: -0.353300
 54900/100000: episode: 549, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.996, mean reward: -0.180 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.591, 10.128], loss: 0.002420, mae: 0.048416, mean_q: -0.299844
 55000/100000: episode: 550, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.527, mean reward: -0.175 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.238, 10.205], loss: 0.002588, mae: 0.050527, mean_q: -0.318742
 55100/100000: episode: 551, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.443, mean reward: -0.174 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.718, 10.146], loss: 0.002514, mae: 0.048715, mean_q: -0.319549
 55200/100000: episode: 552, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.686, mean reward: -0.167 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.837, 10.345], loss: 0.002523, mae: 0.049170, mean_q: -0.292500
 55300/100000: episode: 553, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -15.999, mean reward: -0.160 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.207, 10.098], loss: 0.002602, mae: 0.049870, mean_q: -0.318562
 55400/100000: episode: 554, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.570, mean reward: -0.176 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.017, 10.098], loss: 0.002594, mae: 0.050972, mean_q: -0.300714
 55500/100000: episode: 555, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.301, mean reward: -0.173 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.670, 10.098], loss: 0.002815, mae: 0.052869, mean_q: -0.310885
 55600/100000: episode: 556, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -20.366, mean reward: -0.204 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.863, 10.197], loss: 0.002498, mae: 0.048782, mean_q: -0.315289
 55700/100000: episode: 557, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.466, mean reward: -0.165 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.481, 10.098], loss: 0.002601, mae: 0.049313, mean_q: -0.333102
 55800/100000: episode: 558, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.375, mean reward: -0.174 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.017, 10.098], loss: 0.002502, mae: 0.049457, mean_q: -0.293930
 55900/100000: episode: 559, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -19.857, mean reward: -0.199 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.259, 10.098], loss: 0.003605, mae: 0.059271, mean_q: -0.356449
 56000/100000: episode: 560, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -20.802, mean reward: -0.208 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.242, 10.177], loss: 0.002703, mae: 0.053397, mean_q: -0.312195
 56100/100000: episode: 561, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.154, mean reward: -0.172 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.027, 10.353], loss: 0.002419, mae: 0.048748, mean_q: -0.327613
 56200/100000: episode: 562, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.645, mean reward: -0.186 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.424, 10.098], loss: 0.002530, mae: 0.049604, mean_q: -0.309485
 56300/100000: episode: 563, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.495, mean reward: -0.165 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.145, 10.098], loss: 0.002589, mae: 0.049034, mean_q: -0.335988
 56400/100000: episode: 564, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -15.039, mean reward: -0.150 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.289, 10.330], loss: 0.002670, mae: 0.051218, mean_q: -0.293704
 56500/100000: episode: 565, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.222, mean reward: -0.172 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.229, 10.122], loss: 0.002641, mae: 0.050928, mean_q: -0.318535
 56600/100000: episode: 566, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -15.858, mean reward: -0.159 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.032, 10.197], loss: 0.002587, mae: 0.050577, mean_q: -0.295206
 56700/100000: episode: 567, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -13.191, mean reward: -0.132 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.926, 10.098], loss: 0.002704, mae: 0.051239, mean_q: -0.293175
 56800/100000: episode: 568, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.619, mean reward: -0.176 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.630, 10.181], loss: 0.002483, mae: 0.049533, mean_q: -0.318730
 56900/100000: episode: 569, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -16.445, mean reward: -0.164 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.988, 10.098], loss: 0.002477, mae: 0.049036, mean_q: -0.321925
 57000/100000: episode: 570, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.541, mean reward: -0.195 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.338, 10.098], loss: 0.002688, mae: 0.050625, mean_q: -0.287464
 57100/100000: episode: 571, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -17.511, mean reward: -0.175 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.294, 10.098], loss: 0.002783, mae: 0.051166, mean_q: -0.309606
 57200/100000: episode: 572, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.630, mean reward: -0.176 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.229, 10.098], loss: 0.002555, mae: 0.049054, mean_q: -0.330319
 57300/100000: episode: 573, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -8.510, mean reward: -0.085 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.294, 10.328], loss: 0.002536, mae: 0.049538, mean_q: -0.297991
 57400/100000: episode: 574, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.041, mean reward: -0.170 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.010, 10.202], loss: 0.002757, mae: 0.052312, mean_q: -0.295919
 57500/100000: episode: 575, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -10.749, mean reward: -0.107 [-1.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.035, 10.098], loss: 0.002554, mae: 0.049752, mean_q: -0.311252
 57600/100000: episode: 576, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.965, mean reward: -0.160 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.548, 10.226], loss: 0.002565, mae: 0.049125, mean_q: -0.310730
 57700/100000: episode: 577, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.986, mean reward: -0.160 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.418, 10.165], loss: 0.002587, mae: 0.050124, mean_q: -0.318552
 57800/100000: episode: 578, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -14.460, mean reward: -0.145 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.390, 10.098], loss: 0.008507, mae: 0.078619, mean_q: -0.306437
 57900/100000: episode: 579, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -20.153, mean reward: -0.202 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.867, 10.188], loss: 0.003268, mae: 0.056374, mean_q: -0.274928
 58000/100000: episode: 580, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.400, mean reward: -0.184 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.837, 10.098], loss: 0.002828, mae: 0.054734, mean_q: -0.260137
 58100/100000: episode: 581, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.375, mean reward: -0.184 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.393, 10.098], loss: 0.002610, mae: 0.051286, mean_q: -0.333368
 58200/100000: episode: 582, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.390, mean reward: -0.194 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.377, 10.098], loss: 0.002528, mae: 0.050546, mean_q: -0.295791
 58300/100000: episode: 583, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.088, mean reward: -0.151 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.363, 10.206], loss: 0.002643, mae: 0.051577, mean_q: -0.281258
 58400/100000: episode: 584, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.388, mean reward: -0.144 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.391, 10.509], loss: 0.002669, mae: 0.052044, mean_q: -0.337054
 58500/100000: episode: 585, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.981, mean reward: -0.180 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.041, 10.375], loss: 0.002633, mae: 0.050912, mean_q: -0.345375
 58600/100000: episode: 586, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.250, mean reward: -0.152 [-1.000, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.018, 10.259], loss: 0.002476, mae: 0.049553, mean_q: -0.318896
 58700/100000: episode: 587, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.452, mean reward: -0.165 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.914, 10.290], loss: 0.002620, mae: 0.051169, mean_q: -0.307347
 58800/100000: episode: 588, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.669, mean reward: -0.187 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.440, 10.098], loss: 0.002447, mae: 0.049288, mean_q: -0.314578
 58900/100000: episode: 589, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -17.107, mean reward: -0.171 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.727, 10.156], loss: 0.002328, mae: 0.047351, mean_q: -0.343011
 59000/100000: episode: 590, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -8.497, mean reward: -0.085 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.058, 10.098], loss: 0.003126, mae: 0.055128, mean_q: -0.297466
 59100/100000: episode: 591, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: -16.054, mean reward: -0.161 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.983, 10.098], loss: 0.002606, mae: 0.052411, mean_q: -0.309095
 59200/100000: episode: 592, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.887, mean reward: -0.179 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.775, 10.098], loss: 0.002592, mae: 0.050748, mean_q: -0.285223
 59300/100000: episode: 593, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -12.181, mean reward: -0.122 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.403, 10.098], loss: 0.002424, mae: 0.047950, mean_q: -0.312534
 59400/100000: episode: 594, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -12.735, mean reward: -0.127 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.740, 10.098], loss: 0.002231, mae: 0.046079, mean_q: -0.317140
 59500/100000: episode: 595, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -19.503, mean reward: -0.195 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.646, 10.098], loss: 0.002546, mae: 0.049188, mean_q: -0.289629
 59600/100000: episode: 596, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.013, mean reward: -0.180 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.263, 10.098], loss: 0.002203, mae: 0.045546, mean_q: -0.323537
 59700/100000: episode: 597, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.786, mean reward: -0.168 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.626, 10.098], loss: 0.002350, mae: 0.047683, mean_q: -0.310081
 59800/100000: episode: 598, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -13.308, mean reward: -0.133 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-1.174, 10.098], loss: 0.002394, mae: 0.048436, mean_q: -0.310123
 59900/100000: episode: 599, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.272, mean reward: -0.183 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.269, 10.098], loss: 0.002491, mae: 0.048886, mean_q: -0.295105
 60000/100000: episode: 600, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.966, mean reward: -0.190 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.893, 10.118], loss: 0.002403, mae: 0.048405, mean_q: -0.313892
 60100/100000: episode: 601, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.152, mean reward: -0.162 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.660, 10.212], loss: 0.002553, mae: 0.049783, mean_q: -0.327930
 60200/100000: episode: 602, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.605, mean reward: -0.186 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.284, 10.103], loss: 0.002402, mae: 0.048642, mean_q: -0.307449
 60300/100000: episode: 603, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.258, mean reward: -0.173 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.799, 10.289], loss: 0.002433, mae: 0.049001, mean_q: -0.309004
 60400/100000: episode: 604, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -13.683, mean reward: -0.137 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.566, 10.098], loss: 0.002315, mae: 0.047029, mean_q: -0.330123
 60500/100000: episode: 605, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.285, mean reward: -0.153 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.403, 10.167], loss: 0.002499, mae: 0.049617, mean_q: -0.324123
 60600/100000: episode: 606, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.793, mean reward: -0.148 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.003, 10.298], loss: 0.003245, mae: 0.055221, mean_q: -0.307354
 60700/100000: episode: 607, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.565, mean reward: -0.176 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.683, 10.296], loss: 0.003417, mae: 0.059175, mean_q: -0.320658
 60800/100000: episode: 608, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.598, mean reward: -0.186 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.112, 10.207], loss: 0.002581, mae: 0.050501, mean_q: -0.309605
 60900/100000: episode: 609, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.410, mean reward: -0.164 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.409, 10.180], loss: 0.002264, mae: 0.046921, mean_q: -0.330127
 61000/100000: episode: 610, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -20.191, mean reward: -0.202 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.116, 10.098], loss: 0.002499, mae: 0.049549, mean_q: -0.323595
 61100/100000: episode: 611, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.927, mean reward: -0.199 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.534, 10.148], loss: 0.002350, mae: 0.048067, mean_q: -0.333985
 61200/100000: episode: 612, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.974, mean reward: -0.150 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.400, 10.212], loss: 0.002401, mae: 0.048484, mean_q: -0.307138
 61300/100000: episode: 613, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.238, mean reward: -0.192 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.865, 10.098], loss: 0.002217, mae: 0.046211, mean_q: -0.334221
 61400/100000: episode: 614, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.848, mean reward: -0.178 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.856, 10.246], loss: 0.002248, mae: 0.046365, mean_q: -0.309691
 61500/100000: episode: 615, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.238, mean reward: -0.172 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.006, 10.132], loss: 0.002466, mae: 0.049329, mean_q: -0.286859
 61600/100000: episode: 616, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.003, mean reward: -0.190 [-1.000, 0.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.786, 10.168], loss: 0.002357, mae: 0.048183, mean_q: -0.281482
 61700/100000: episode: 617, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -9.452, mean reward: -0.095 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.568, 10.098], loss: 0.002458, mae: 0.049045, mean_q: -0.343543
 61800/100000: episode: 618, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.499, mean reward: -0.185 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.305, 10.154], loss: 0.002301, mae: 0.047200, mean_q: -0.322212
 61900/100000: episode: 619, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.485, mean reward: -0.175 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.630, 10.273], loss: 0.002267, mae: 0.046613, mean_q: -0.290047
 62000/100000: episode: 620, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.720, mean reward: -0.197 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.531, 10.098], loss: 0.002256, mae: 0.046252, mean_q: -0.328584
 62100/100000: episode: 621, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -15.935, mean reward: -0.159 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.678, 10.098], loss: 0.002304, mae: 0.047558, mean_q: -0.300994
 62200/100000: episode: 622, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.459, mean reward: -0.175 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.814, 10.236], loss: 0.002268, mae: 0.047030, mean_q: -0.308630
 62300/100000: episode: 623, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.270, mean reward: -0.163 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.052, 10.098], loss: 0.002450, mae: 0.048384, mean_q: -0.324047
 62400/100000: episode: 624, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -11.449, mean reward: -0.114 [-1.000, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.401, 10.341], loss: 0.002506, mae: 0.050315, mean_q: -0.320114
 62500/100000: episode: 625, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.133, mean reward: -0.171 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.649, 10.098], loss: 0.002664, mae: 0.051622, mean_q: -0.277154
 62600/100000: episode: 626, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.667, mean reward: -0.197 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.137, 10.098], loss: 0.002247, mae: 0.046350, mean_q: -0.327322
 62700/100000: episode: 627, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.508, mean reward: -0.165 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.702, 10.263], loss: 0.002452, mae: 0.049101, mean_q: -0.304300
 62800/100000: episode: 628, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.938, mean reward: -0.189 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.927, 10.138], loss: 0.002292, mae: 0.046514, mean_q: -0.314898
 62900/100000: episode: 629, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.571, mean reward: -0.156 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.661, 10.132], loss: 0.002190, mae: 0.045758, mean_q: -0.340176
 63000/100000: episode: 630, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.935, mean reward: -0.189 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.782, 10.131], loss: 0.002249, mae: 0.047461, mean_q: -0.288364
 63100/100000: episode: 631, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.528, mean reward: -0.165 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.556, 10.142], loss: 0.002442, mae: 0.048105, mean_q: -0.312575
 63200/100000: episode: 632, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.980, mean reward: -0.180 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.621, 10.219], loss: 0.002290, mae: 0.047493, mean_q: -0.289929
 63300/100000: episode: 633, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -15.156, mean reward: -0.152 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.996, 10.098], loss: 0.002559, mae: 0.049213, mean_q: -0.337768
 63400/100000: episode: 634, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.675, mean reward: -0.177 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.320, 10.098], loss: 0.002301, mae: 0.047343, mean_q: -0.330586
 63500/100000: episode: 635, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.875, mean reward: -0.189 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.398, 10.353], loss: 0.002458, mae: 0.048272, mean_q: -0.296970
 63600/100000: episode: 636, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.487, mean reward: -0.175 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.643, 10.098], loss: 0.002725, mae: 0.052468, mean_q: -0.301960
 63700/100000: episode: 637, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -20.217, mean reward: -0.202 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.461, 10.124], loss: 0.002136, mae: 0.045345, mean_q: -0.344069
 63800/100000: episode: 638, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -15.186, mean reward: -0.152 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.758, 10.098], loss: 0.002289, mae: 0.046636, mean_q: -0.332942
 63900/100000: episode: 639, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.793, mean reward: -0.168 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.620, 10.098], loss: 0.002288, mae: 0.046595, mean_q: -0.326400
 64000/100000: episode: 640, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.712, mean reward: -0.187 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.103, 10.111], loss: 0.002292, mae: 0.047492, mean_q: -0.327938
 64100/100000: episode: 641, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.533, mean reward: -0.145 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.107, 10.098], loss: 0.002334, mae: 0.048374, mean_q: -0.333453
 64200/100000: episode: 642, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.667, mean reward: -0.187 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.521, 10.098], loss: 0.002518, mae: 0.049100, mean_q: -0.318998
 64300/100000: episode: 643, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.626, mean reward: -0.176 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.232, 10.262], loss: 0.003900, mae: 0.055895, mean_q: -0.260667
 64400/100000: episode: 644, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.651, mean reward: -0.157 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.375, 10.283], loss: 0.004426, mae: 0.062944, mean_q: -0.342677
 64500/100000: episode: 645, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.909, mean reward: -0.199 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.207, 10.098], loss: 0.003054, mae: 0.057197, mean_q: -0.342600
 64600/100000: episode: 646, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.813, mean reward: -0.168 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.492, 10.333], loss: 0.002360, mae: 0.048467, mean_q: -0.353372
 64700/100000: episode: 647, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -20.266, mean reward: -0.203 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.837, 10.127], loss: 0.004072, mae: 0.053168, mean_q: -0.317232
 64800/100000: episode: 648, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.660, mean reward: -0.167 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.012, 10.272], loss: 0.005485, mae: 0.068572, mean_q: -0.316816
 64900/100000: episode: 649, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -14.721, mean reward: -0.147 [-1.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.107, 10.098], loss: 0.002468, mae: 0.049830, mean_q: -0.350299
 65000/100000: episode: 650, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.682, mean reward: -0.167 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.638, 10.122], loss: 0.002263, mae: 0.046065, mean_q: -0.386501
 65100/100000: episode: 651, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.054, mean reward: -0.161 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.530, 10.098], loss: 0.002340, mae: 0.047381, mean_q: -0.318409
 65200/100000: episode: 652, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -15.674, mean reward: -0.157 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.909, 10.098], loss: 0.002162, mae: 0.045129, mean_q: -0.307903
 65300/100000: episode: 653, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.530, mean reward: -0.165 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.543, 10.098], loss: 0.002266, mae: 0.045975, mean_q: -0.363239
 65400/100000: episode: 654, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.733, mean reward: -0.197 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.869, 10.171], loss: 0.002303, mae: 0.047279, mean_q: -0.308459
 65500/100000: episode: 655, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.852, mean reward: -0.179 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.937, 10.145], loss: 0.002281, mae: 0.046515, mean_q: -0.332775
 65600/100000: episode: 656, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.395, mean reward: -0.184 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.520, 10.222], loss: 0.002306, mae: 0.046484, mean_q: -0.336367
 65700/100000: episode: 657, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.966, mean reward: -0.180 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.600, 10.098], loss: 0.002292, mae: 0.046379, mean_q: -0.327513
 65800/100000: episode: 658, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.140, mean reward: -0.181 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.421, 10.181], loss: 0.002372, mae: 0.047583, mean_q: -0.315079
 65900/100000: episode: 659, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -10.531, mean reward: -0.105 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.366, 10.098], loss: 0.002304, mae: 0.047245, mean_q: -0.320547
 66000/100000: episode: 660, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.658, mean reward: -0.167 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.949, 10.177], loss: 0.002418, mae: 0.048383, mean_q: -0.284967
 66100/100000: episode: 661, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.698, mean reward: -0.187 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.290, 10.269], loss: 0.002196, mae: 0.045477, mean_q: -0.313762
 66200/100000: episode: 662, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.000, mean reward: -0.180 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.808, 10.194], loss: 0.002405, mae: 0.048274, mean_q: -0.304969
 66300/100000: episode: 663, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -11.433, mean reward: -0.114 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.741, 10.098], loss: 0.002297, mae: 0.047154, mean_q: -0.344614
 66400/100000: episode: 664, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.460, mean reward: -0.175 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.294, 10.098], loss: 0.002242, mae: 0.046472, mean_q: -0.298266
 66500/100000: episode: 665, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -11.608, mean reward: -0.116 [-1.000, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.042, 10.098], loss: 0.002440, mae: 0.048000, mean_q: -0.335095
 66600/100000: episode: 666, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -14.887, mean reward: -0.149 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.883, 10.184], loss: 0.002599, mae: 0.049765, mean_q: -0.294319
 66700/100000: episode: 667, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.341, mean reward: -0.183 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.694, 10.098], loss: 0.002460, mae: 0.048525, mean_q: -0.325264
 66800/100000: episode: 668, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.144, mean reward: -0.191 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.362, 10.098], loss: 0.002587, mae: 0.049228, mean_q: -0.335018
 66900/100000: episode: 669, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -13.920, mean reward: -0.139 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.147, 10.293], loss: 0.002812, mae: 0.051833, mean_q: -0.290698
 67000/100000: episode: 670, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -11.583, mean reward: -0.116 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.790, 10.314], loss: 0.002442, mae: 0.048392, mean_q: -0.346949
 67100/100000: episode: 671, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.416, mean reward: -0.154 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.693, 10.351], loss: 0.004952, mae: 0.066913, mean_q: -0.290212
 67200/100000: episode: 672, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.611, mean reward: -0.186 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.208, 10.158], loss: 0.002847, mae: 0.053562, mean_q: -0.318241
 67300/100000: episode: 673, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.372, mean reward: -0.194 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.757, 10.098], loss: 0.002356, mae: 0.046884, mean_q: -0.363706
 67400/100000: episode: 674, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.706, mean reward: -0.197 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.856, 10.098], loss: 0.002555, mae: 0.049270, mean_q: -0.299635
 67500/100000: episode: 675, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.862, mean reward: -0.179 [-1.000, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.760, 10.163], loss: 0.002398, mae: 0.048053, mean_q: -0.302199
 67600/100000: episode: 676, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.688, mean reward: -0.187 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.475, 10.246], loss: 0.002600, mae: 0.049373, mean_q: -0.307686
 67700/100000: episode: 677, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.521, mean reward: -0.185 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.093, 10.231], loss: 0.002693, mae: 0.049780, mean_q: -0.333749
 67800/100000: episode: 678, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.021, mean reward: -0.180 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.046, 10.150], loss: 0.002450, mae: 0.048578, mean_q: -0.322287
 67900/100000: episode: 679, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -12.715, mean reward: -0.127 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.459, 10.431], loss: 0.002544, mae: 0.049200, mean_q: -0.319241
 68000/100000: episode: 680, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.182, mean reward: -0.182 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.059, 10.098], loss: 0.002557, mae: 0.049774, mean_q: -0.317595
 68100/100000: episode: 681, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.043, mean reward: -0.170 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.808, 10.202], loss: 0.002714, mae: 0.051267, mean_q: -0.317753
 68200/100000: episode: 682, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.038, mean reward: -0.170 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.259, 10.159], loss: 0.002659, mae: 0.049122, mean_q: -0.339474
 68300/100000: episode: 683, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.828, mean reward: -0.158 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.917, 10.098], loss: 0.002635, mae: 0.050302, mean_q: -0.282475
 68400/100000: episode: 684, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -13.427, mean reward: -0.134 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.463, 10.330], loss: 0.002618, mae: 0.050141, mean_q: -0.331938
 68500/100000: episode: 685, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -15.887, mean reward: -0.159 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.976, 10.144], loss: 0.002495, mae: 0.047279, mean_q: -0.350064
 68600/100000: episode: 686, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -9.767, mean reward: -0.098 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.252, 10.509], loss: 0.002625, mae: 0.050487, mean_q: -0.302204
 68700/100000: episode: 687, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -19.566, mean reward: -0.196 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.196, 10.368], loss: 0.002714, mae: 0.050635, mean_q: -0.306227
 68800/100000: episode: 688, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.986, mean reward: -0.180 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.953, 10.098], loss: 0.002359, mae: 0.046867, mean_q: -0.331907
 68900/100000: episode: 689, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.102, mean reward: -0.171 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.528, 10.098], loss: 0.002732, mae: 0.051033, mean_q: -0.301412
 69000/100000: episode: 690, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.803, mean reward: -0.158 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.833, 10.520], loss: 0.003667, mae: 0.059624, mean_q: -0.350856
 69100/100000: episode: 691, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.256, mean reward: -0.193 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.900, 10.098], loss: 0.002627, mae: 0.051014, mean_q: -0.332855
 69200/100000: episode: 692, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.232, mean reward: -0.172 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.522, 10.301], loss: 0.002489, mae: 0.048425, mean_q: -0.317238
 69300/100000: episode: 693, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.437, mean reward: -0.184 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.830, 10.176], loss: 0.002846, mae: 0.052351, mean_q: -0.310330
 69400/100000: episode: 694, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.811, mean reward: -0.188 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.351, 10.175], loss: 0.002859, mae: 0.052784, mean_q: -0.322095
 69500/100000: episode: 695, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.357, mean reward: -0.164 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.988, 10.122], loss: 0.002700, mae: 0.049919, mean_q: -0.325030
 69600/100000: episode: 696, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -15.867, mean reward: -0.159 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.778, 10.300], loss: 0.002572, mae: 0.049893, mean_q: -0.293081
 69700/100000: episode: 697, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -12.892, mean reward: -0.129 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.865, 10.342], loss: 0.002822, mae: 0.052063, mean_q: -0.321049
 69800/100000: episode: 698, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.355, mean reward: -0.184 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.512, 10.118], loss: 0.002602, mae: 0.049577, mean_q: -0.305419
 69900/100000: episode: 699, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.535, mean reward: -0.175 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.543, 10.334], loss: 0.002536, mae: 0.049346, mean_q: -0.328416
 70000/100000: episode: 700, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.848, mean reward: -0.198 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.903, 10.107], loss: 0.002771, mae: 0.051368, mean_q: -0.327623
 70100/100000: episode: 701, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.149, mean reward: -0.151 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.395, 10.253], loss: 0.002712, mae: 0.051304, mean_q: -0.332260
 70200/100000: episode: 702, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.740, mean reward: -0.177 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.584, 10.177], loss: 0.002560, mae: 0.049443, mean_q: -0.302390
 70300/100000: episode: 703, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.581, mean reward: -0.186 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.718, 10.098], loss: 0.002717, mae: 0.051246, mean_q: -0.309074
 70400/100000: episode: 704, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.192, mean reward: -0.142 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.927, 10.319], loss: 0.002660, mae: 0.050554, mean_q: -0.314245
 70500/100000: episode: 705, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.806, mean reward: -0.148 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.953, 10.127], loss: 0.002694, mae: 0.051432, mean_q: -0.305086
 70600/100000: episode: 706, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.494, mean reward: -0.195 [-1.000, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.544, 10.098], loss: 0.002437, mae: 0.048481, mean_q: -0.323341
 70700/100000: episode: 707, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.713, mean reward: -0.177 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.654, 10.098], loss: 0.003555, mae: 0.057105, mean_q: -0.337260
 70800/100000: episode: 708, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.008, mean reward: -0.180 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.996, 10.147], loss: 0.002451, mae: 0.048032, mean_q: -0.329544
 70900/100000: episode: 709, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.006, mean reward: -0.150 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.425, 10.098], loss: 0.002756, mae: 0.050989, mean_q: -0.332875
 71000/100000: episode: 710, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.987, mean reward: -0.160 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.326, 10.098], loss: 0.002522, mae: 0.049034, mean_q: -0.329963
 71100/100000: episode: 711, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -12.857, mean reward: -0.129 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.257, 10.098], loss: 0.002774, mae: 0.051044, mean_q: -0.319176
 71200/100000: episode: 712, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -19.774, mean reward: -0.198 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.848, 10.098], loss: 0.002711, mae: 0.051279, mean_q: -0.298337
 71300/100000: episode: 713, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -12.460, mean reward: -0.125 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.465, 10.098], loss: 0.002707, mae: 0.050829, mean_q: -0.302106
 71400/100000: episode: 714, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.549, mean reward: -0.165 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.425, 10.324], loss: 0.005951, mae: 0.069597, mean_q: -0.313932
 71500/100000: episode: 715, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.983, mean reward: -0.160 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.767, 10.098], loss: 0.002920, mae: 0.055150, mean_q: -0.318606
 71600/100000: episode: 716, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.424, mean reward: -0.164 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.907, 10.381], loss: 0.002595, mae: 0.049988, mean_q: -0.286779
 71700/100000: episode: 717, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -11.362, mean reward: -0.114 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.570, 10.346], loss: 0.002372, mae: 0.046751, mean_q: -0.339047
 71800/100000: episode: 718, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -13.713, mean reward: -0.137 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.760, 10.098], loss: 0.002434, mae: 0.047457, mean_q: -0.337613
 71900/100000: episode: 719, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -13.113, mean reward: -0.131 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.599, 10.098], loss: 0.002426, mae: 0.048499, mean_q: -0.310947
 72000/100000: episode: 720, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -17.937, mean reward: -0.179 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.291, 10.098], loss: 0.002592, mae: 0.049436, mean_q: -0.311160
 72100/100000: episode: 721, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.861, mean reward: -0.169 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.920, 10.098], loss: 0.002508, mae: 0.049437, mean_q: -0.310180
 72200/100000: episode: 722, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.849, mean reward: -0.158 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.846, 10.157], loss: 0.002491, mae: 0.048558, mean_q: -0.327282
 72300/100000: episode: 723, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -15.678, mean reward: -0.157 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.078, 10.228], loss: 0.002374, mae: 0.048345, mean_q: -0.292908
 72400/100000: episode: 724, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.149, mean reward: -0.151 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.991, 10.098], loss: 0.002506, mae: 0.048843, mean_q: -0.320564
 72500/100000: episode: 725, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -14.056, mean reward: -0.141 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.548, 10.303], loss: 0.002767, mae: 0.052283, mean_q: -0.298367
 72600/100000: episode: 726, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.848, mean reward: -0.188 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.951, 10.254], loss: 0.002731, mae: 0.051778, mean_q: -0.311924
 72700/100000: episode: 727, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.766, mean reward: -0.178 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.245, 10.173], loss: 0.002474, mae: 0.048924, mean_q: -0.317726
 72800/100000: episode: 728, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -17.602, mean reward: -0.176 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.901, 10.098], loss: 0.002512, mae: 0.049301, mean_q: -0.302025
 72900/100000: episode: 729, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -16.575, mean reward: -0.166 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.806, 10.147], loss: 0.002555, mae: 0.048952, mean_q: -0.328139
 73000/100000: episode: 730, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.328, mean reward: -0.153 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.532, 10.451], loss: 0.002626, mae: 0.050024, mean_q: -0.293364
 73100/100000: episode: 731, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.596, mean reward: -0.156 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.213, 10.451], loss: 0.002437, mae: 0.048193, mean_q: -0.287982
 73200/100000: episode: 732, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.654, mean reward: -0.177 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.806, 10.186], loss: 0.002568, mae: 0.049029, mean_q: -0.310175
 73300/100000: episode: 733, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -3.948, mean reward: -0.039 [-1.000, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.414, 10.646], loss: 0.002459, mae: 0.048556, mean_q: -0.283785
 73400/100000: episode: 734, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.215, mean reward: -0.172 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.617, 10.098], loss: 0.002536, mae: 0.048218, mean_q: -0.316489
 73500/100000: episode: 735, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -13.044, mean reward: -0.130 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.026, 10.098], loss: 0.002606, mae: 0.049345, mean_q: -0.311429
 73600/100000: episode: 736, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.944, mean reward: -0.149 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.217, 10.098], loss: 0.002466, mae: 0.048005, mean_q: -0.299592
 73700/100000: episode: 737, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -17.114, mean reward: -0.171 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.886, 10.264], loss: 0.002464, mae: 0.049948, mean_q: -0.313427
 73800/100000: episode: 738, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.088, mean reward: -0.191 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.602, 10.098], loss: 0.002556, mae: 0.049457, mean_q: -0.304465
 73900/100000: episode: 739, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.285, mean reward: -0.163 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.437, 10.220], loss: 0.002588, mae: 0.049750, mean_q: -0.293045
 74000/100000: episode: 740, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.612, mean reward: -0.186 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.259, 10.098], loss: 0.002630, mae: 0.050696, mean_q: -0.291688
 74100/100000: episode: 741, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -13.786, mean reward: -0.138 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.205, 10.098], loss: 0.002568, mae: 0.050202, mean_q: -0.287575
 74200/100000: episode: 742, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.140, mean reward: -0.181 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.300, 10.335], loss: 0.002610, mae: 0.050147, mean_q: -0.299516
 74300/100000: episode: 743, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.315, mean reward: -0.193 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.840, 10.098], loss: 0.002570, mae: 0.049860, mean_q: -0.294579
 74400/100000: episode: 744, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.534, mean reward: -0.175 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.296, 10.098], loss: 0.002270, mae: 0.047126, mean_q: -0.323083
 74500/100000: episode: 745, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.465, mean reward: -0.155 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.342, 10.098], loss: 0.002739, mae: 0.051581, mean_q: -0.286648
 74600/100000: episode: 746, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.454, mean reward: -0.165 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.578, 10.373], loss: 0.002566, mae: 0.050229, mean_q: -0.290086
 74700/100000: episode: 747, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.754, mean reward: -0.168 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.844, 10.098], loss: 0.002783, mae: 0.050642, mean_q: -0.299073
 74800/100000: episode: 748, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.098, mean reward: -0.191 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.246, 10.098], loss: 0.002682, mae: 0.050462, mean_q: -0.307969
 74900/100000: episode: 749, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -15.766, mean reward: -0.158 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.864, 10.186], loss: 0.002506, mae: 0.049306, mean_q: -0.300096
 75000/100000: episode: 750, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.676, mean reward: -0.177 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.606, 10.098], loss: 0.002557, mae: 0.048729, mean_q: -0.330497
 75100/100000: episode: 751, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.240, mean reward: -0.192 [-1.000, 0.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.552, 10.144], loss: 0.002552, mae: 0.048451, mean_q: -0.327910
 75200/100000: episode: 752, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.311, mean reward: -0.183 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.117, 10.166], loss: 0.002671, mae: 0.050251, mean_q: -0.299242
 75300/100000: episode: 753, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.088, mean reward: -0.181 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.284, 10.235], loss: 0.002640, mae: 0.050538, mean_q: -0.301691
 75400/100000: episode: 754, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.049, mean reward: -0.190 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.318, 10.098], loss: 0.002578, mae: 0.050303, mean_q: -0.312571
 75500/100000: episode: 755, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.885, mean reward: -0.209 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.430, 10.098], loss: 0.005968, mae: 0.068063, mean_q: -0.288460
 75600/100000: episode: 756, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.013, mean reward: -0.150 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.508, 10.202], loss: 0.003651, mae: 0.061517, mean_q: -0.281735
 75700/100000: episode: 757, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.905, mean reward: -0.179 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.880, 10.098], loss: 0.002547, mae: 0.049247, mean_q: -0.318806
 75800/100000: episode: 758, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -22.081, mean reward: -0.221 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.562, 10.098], loss: 0.002360, mae: 0.047317, mean_q: -0.347681
 75900/100000: episode: 759, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.538, mean reward: -0.175 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.415, 10.349], loss: 0.002396, mae: 0.048543, mean_q: -0.335163
 76000/100000: episode: 760, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.470, mean reward: -0.175 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.062, 10.132], loss: 0.002828, mae: 0.051930, mean_q: -0.298422
 76100/100000: episode: 761, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.542, mean reward: -0.175 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.289, 10.331], loss: 0.002507, mae: 0.049208, mean_q: -0.310111
 76200/100000: episode: 762, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.002, mean reward: -0.180 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.637, 10.122], loss: 0.002694, mae: 0.050713, mean_q: -0.312170
 76300/100000: episode: 763, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.771, mean reward: -0.178 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.984, 10.098], loss: 0.002549, mae: 0.049704, mean_q: -0.307360
 76400/100000: episode: 764, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.056, mean reward: -0.141 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.034, 10.399], loss: 0.002690, mae: 0.050323, mean_q: -0.329298
 76500/100000: episode: 765, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.791, mean reward: -0.148 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.801, 10.098], loss: 0.002765, mae: 0.051261, mean_q: -0.313498
 76600/100000: episode: 766, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -20.296, mean reward: -0.203 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.413, 10.307], loss: 0.002541, mae: 0.049110, mean_q: -0.324982
 76700/100000: episode: 767, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -19.173, mean reward: -0.192 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.270, 10.098], loss: 0.002564, mae: 0.049537, mean_q: -0.281238
 76800/100000: episode: 768, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.124, mean reward: -0.171 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.384, 10.098], loss: 0.002538, mae: 0.049522, mean_q: -0.329582
 76900/100000: episode: 769, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -19.207, mean reward: -0.192 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.662, 10.194], loss: 0.002481, mae: 0.048728, mean_q: -0.324931
 77000/100000: episode: 770, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.515, mean reward: -0.195 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.290, 10.098], loss: 0.002546, mae: 0.049488, mean_q: -0.319014
 77100/100000: episode: 771, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.775, mean reward: -0.168 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.235, 10.098], loss: 0.002499, mae: 0.048591, mean_q: -0.337891
 77200/100000: episode: 772, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.872, mean reward: -0.179 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.087, 10.257], loss: 0.002631, mae: 0.050445, mean_q: -0.297552
 77300/100000: episode: 773, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.370, mean reward: -0.184 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.387, 10.098], loss: 0.002318, mae: 0.047101, mean_q: -0.315673
 77400/100000: episode: 774, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.395, mean reward: -0.154 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.475, 10.098], loss: 0.002557, mae: 0.048875, mean_q: -0.322501
 77500/100000: episode: 775, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.084, mean reward: -0.191 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.214, 10.098], loss: 0.002481, mae: 0.048616, mean_q: -0.325449
 77600/100000: episode: 776, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.902, mean reward: -0.159 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.209, 10.355], loss: 0.002497, mae: 0.048054, mean_q: -0.340437
 77700/100000: episode: 777, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.623, mean reward: -0.156 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.220, 10.306], loss: 0.002661, mae: 0.050305, mean_q: -0.301806
 77800/100000: episode: 778, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.623, mean reward: -0.176 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.263, 10.098], loss: 0.002457, mae: 0.049020, mean_q: -0.298717
 77900/100000: episode: 779, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.722, mean reward: -0.187 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.150, 10.098], loss: 0.002535, mae: 0.049871, mean_q: -0.318490
 78000/100000: episode: 780, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.467, mean reward: -0.175 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.646, 10.098], loss: 0.002650, mae: 0.050177, mean_q: -0.301888
 78100/100000: episode: 781, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.189, mean reward: -0.152 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.381, 10.315], loss: 0.002364, mae: 0.047374, mean_q: -0.325504
 78200/100000: episode: 782, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.109, mean reward: -0.171 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.523, 10.098], loss: 0.002629, mae: 0.049605, mean_q: -0.319962
 78300/100000: episode: 783, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.739, mean reward: -0.197 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.818, 10.098], loss: 0.002620, mae: 0.050065, mean_q: -0.317014
 78400/100000: episode: 784, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.855, mean reward: -0.149 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.257, 10.406], loss: 0.002750, mae: 0.051197, mean_q: -0.340556
 78500/100000: episode: 785, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.763, mean reward: -0.158 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.349, 10.098], loss: 0.002589, mae: 0.049712, mean_q: -0.318762
 78600/100000: episode: 786, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.204, mean reward: -0.172 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.925, 10.131], loss: 0.002437, mae: 0.048479, mean_q: -0.315111
 78700/100000: episode: 787, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.423, mean reward: -0.184 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.761, 10.163], loss: 0.002466, mae: 0.048360, mean_q: -0.338440
 78800/100000: episode: 788, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -14.515, mean reward: -0.145 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.777, 10.098], loss: 0.002519, mae: 0.049182, mean_q: -0.349244
 78900/100000: episode: 789, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -17.162, mean reward: -0.172 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.831, 10.170], loss: 0.002471, mae: 0.048642, mean_q: -0.340050
 79000/100000: episode: 790, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.627, mean reward: -0.196 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.080, 10.146], loss: 0.002829, mae: 0.055031, mean_q: -0.327235
 79100/100000: episode: 791, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.651, mean reward: -0.187 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.663, 10.308], loss: 0.007919, mae: 0.078001, mean_q: -0.351760
 79200/100000: episode: 792, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.335, mean reward: -0.183 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.559, 10.098], loss: 0.004595, mae: 0.062852, mean_q: -0.337598
 79300/100000: episode: 793, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -14.140, mean reward: -0.141 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.514, 10.401], loss: 0.002357, mae: 0.048956, mean_q: -0.318895
 79400/100000: episode: 794, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.433, mean reward: -0.174 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.805, 10.098], loss: 0.002320, mae: 0.047894, mean_q: -0.310908
 79500/100000: episode: 795, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.389, mean reward: -0.184 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.145, 10.098], loss: 0.002399, mae: 0.049294, mean_q: -0.293143
 79600/100000: episode: 796, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.601, mean reward: -0.186 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.816, 10.204], loss: 0.002315, mae: 0.046995, mean_q: -0.353150
 79700/100000: episode: 797, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.681, mean reward: -0.177 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.508, 10.098], loss: 0.002283, mae: 0.047575, mean_q: -0.314008
 79800/100000: episode: 798, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.782, mean reward: -0.178 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.951, 10.248], loss: 0.002238, mae: 0.046958, mean_q: -0.312944
 79900/100000: episode: 799, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -12.338, mean reward: -0.123 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.310, 10.368], loss: 0.002356, mae: 0.047860, mean_q: -0.347460
 80000/100000: episode: 800, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.543, mean reward: -0.175 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.119, 10.098], loss: 0.002353, mae: 0.047672, mean_q: -0.330392
 80100/100000: episode: 801, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -13.520, mean reward: -0.135 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.838, 10.129], loss: 0.002216, mae: 0.045696, mean_q: -0.334792
 80200/100000: episode: 802, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -15.117, mean reward: -0.151 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.657, 10.098], loss: 0.002378, mae: 0.048337, mean_q: -0.321508
 80300/100000: episode: 803, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.789, mean reward: -0.188 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.230, 10.171], loss: 0.002329, mae: 0.047466, mean_q: -0.283268
 80400/100000: episode: 804, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -14.649, mean reward: -0.146 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.514, 10.298], loss: 0.002227, mae: 0.046703, mean_q: -0.339296
 80500/100000: episode: 805, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.928, mean reward: -0.189 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.968, 10.098], loss: 0.002351, mae: 0.048254, mean_q: -0.319071
 80600/100000: episode: 806, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.012, mean reward: -0.170 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.591, 10.098], loss: 0.002307, mae: 0.046327, mean_q: -0.379388
 80700/100000: episode: 807, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -18.532, mean reward: -0.185 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.843, 10.249], loss: 0.002462, mae: 0.048983, mean_q: -0.322660
 80800/100000: episode: 808, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -16.911, mean reward: -0.169 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.746, 10.186], loss: 0.002323, mae: 0.047013, mean_q: -0.339792
 80900/100000: episode: 809, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -12.270, mean reward: -0.123 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.755, 10.262], loss: 0.002429, mae: 0.048944, mean_q: -0.303036
 81000/100000: episode: 810, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.111, mean reward: -0.181 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.785, 10.098], loss: 0.002310, mae: 0.047186, mean_q: -0.303691
 81100/100000: episode: 811, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -19.543, mean reward: -0.195 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.341, 10.148], loss: 0.002249, mae: 0.046070, mean_q: -0.333885
 81200/100000: episode: 812, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.572, mean reward: -0.166 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.485, 10.127], loss: 0.002264, mae: 0.047156, mean_q: -0.309423
 81300/100000: episode: 813, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -16.643, mean reward: -0.166 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.550, 10.152], loss: 0.002197, mae: 0.046080, mean_q: -0.343856
 81400/100000: episode: 814, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.454, mean reward: -0.175 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.740, 10.098], loss: 0.002281, mae: 0.047249, mean_q: -0.335589
 81500/100000: episode: 815, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.952, mean reward: -0.170 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.252, 10.134], loss: 0.002410, mae: 0.047881, mean_q: -0.337436
 81600/100000: episode: 816, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.112, mean reward: -0.181 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.063, 10.155], loss: 0.002280, mae: 0.046940, mean_q: -0.330409
 81700/100000: episode: 817, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.627, mean reward: -0.186 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.949, 10.098], loss: 0.002328, mae: 0.047240, mean_q: -0.358273
 81800/100000: episode: 818, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.613, mean reward: -0.166 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.939, 10.301], loss: 0.002640, mae: 0.052450, mean_q: -0.311571
 81900/100000: episode: 819, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.269, mean reward: -0.173 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.214, 10.098], loss: 0.002398, mae: 0.048837, mean_q: -0.352539
 82000/100000: episode: 820, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -15.470, mean reward: -0.155 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.777, 10.301], loss: 0.002380, mae: 0.047726, mean_q: -0.312801
 82100/100000: episode: 821, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.622, mean reward: -0.196 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.077, 10.170], loss: 0.002261, mae: 0.046098, mean_q: -0.338912
 82200/100000: episode: 822, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -17.018, mean reward: -0.170 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.479, 10.181], loss: 0.002462, mae: 0.048406, mean_q: -0.341209
 82300/100000: episode: 823, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.476, mean reward: -0.165 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.466, 10.098], loss: 0.002607, mae: 0.050354, mean_q: -0.319679
 82400/100000: episode: 824, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -12.816, mean reward: -0.128 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.363, 10.098], loss: 0.002479, mae: 0.048712, mean_q: -0.358745
 82500/100000: episode: 825, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.562, mean reward: -0.156 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.552, 10.098], loss: 0.002551, mae: 0.049775, mean_q: -0.331044
 82600/100000: episode: 826, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.171, mean reward: -0.192 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.469, 10.208], loss: 0.002343, mae: 0.047729, mean_q: -0.307325
 82700/100000: episode: 827, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.692, mean reward: -0.157 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.686, 10.098], loss: 0.002609, mae: 0.050022, mean_q: -0.303050
 82800/100000: episode: 828, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -17.828, mean reward: -0.178 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.790, 10.226], loss: 0.002469, mae: 0.048887, mean_q: -0.353976
 82900/100000: episode: 829, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -8.624, mean reward: -0.086 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.616, 10.098], loss: 0.002550, mae: 0.049851, mean_q: -0.310217
 83000/100000: episode: 830, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.819, mean reward: -0.198 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.559, 10.208], loss: 0.002755, mae: 0.051763, mean_q: -0.298748
 83100/100000: episode: 831, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.693, mean reward: -0.187 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.705, 10.171], loss: 0.002585, mae: 0.049603, mean_q: -0.336927
 83200/100000: episode: 832, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.479, mean reward: -0.125 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.093, 10.165], loss: 0.002536, mae: 0.049274, mean_q: -0.339623
 83300/100000: episode: 833, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.637, mean reward: -0.186 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.594, 10.098], loss: 0.005769, mae: 0.063789, mean_q: -0.295222
 83400/100000: episode: 834, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.509, mean reward: -0.165 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.992, 10.307], loss: 0.004836, mae: 0.065626, mean_q: -0.290952
 83500/100000: episode: 835, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.541, mean reward: -0.155 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.302, 10.238], loss: 0.002329, mae: 0.048663, mean_q: -0.321692
 83600/100000: episode: 836, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.393, mean reward: -0.144 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.100, 10.098], loss: 0.002458, mae: 0.050464, mean_q: -0.312407
 83700/100000: episode: 837, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.455, mean reward: -0.175 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.827, 10.098], loss: 0.002310, mae: 0.048087, mean_q: -0.316587
 83800/100000: episode: 838, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -11.381, mean reward: -0.114 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.516, 10.098], loss: 0.002380, mae: 0.048972, mean_q: -0.319892
 83900/100000: episode: 839, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.525, mean reward: -0.155 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.531, 10.098], loss: 0.002360, mae: 0.048152, mean_q: -0.299892
 84000/100000: episode: 840, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -15.411, mean reward: -0.154 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.302, 10.233], loss: 0.002522, mae: 0.050001, mean_q: -0.271786
 84100/100000: episode: 841, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -15.906, mean reward: -0.159 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.958, 10.098], loss: 0.002424, mae: 0.048604, mean_q: -0.342687
 84200/100000: episode: 842, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.727, mean reward: -0.157 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.723, 10.098], loss: 0.002608, mae: 0.050382, mean_q: -0.299472
 84300/100000: episode: 843, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.174, mean reward: -0.162 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.605, 10.334], loss: 0.002409, mae: 0.049362, mean_q: -0.286675
 84400/100000: episode: 844, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.488, mean reward: -0.145 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.713, 10.098], loss: 0.002253, mae: 0.046684, mean_q: -0.324680
 84500/100000: episode: 845, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.275, mean reward: -0.193 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.397, 10.222], loss: 0.002572, mae: 0.050491, mean_q: -0.295976
 84600/100000: episode: 846, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.456, mean reward: -0.195 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.279, 10.098], loss: 0.002449, mae: 0.049441, mean_q: -0.318504
 84700/100000: episode: 847, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.500, mean reward: -0.155 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.244, 10.098], loss: 0.002486, mae: 0.048507, mean_q: -0.314460
 84800/100000: episode: 848, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.136, mean reward: -0.161 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.667, 10.257], loss: 0.002421, mae: 0.048682, mean_q: -0.298498
 84900/100000: episode: 849, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -15.437, mean reward: -0.154 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.154, 10.332], loss: 0.002662, mae: 0.051655, mean_q: -0.284002
 85000/100000: episode: 850, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -12.492, mean reward: -0.125 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.478, 10.152], loss: 0.002475, mae: 0.049185, mean_q: -0.303754
 85100/100000: episode: 851, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -12.990, mean reward: -0.130 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.358, 10.302], loss: 0.002415, mae: 0.048728, mean_q: -0.301635
 85200/100000: episode: 852, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -15.359, mean reward: -0.154 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.784, 10.353], loss: 0.002665, mae: 0.052780, mean_q: -0.323607
 85300/100000: episode: 853, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.474, mean reward: -0.155 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.467, 10.098], loss: 0.002503, mae: 0.049500, mean_q: -0.299181
 85400/100000: episode: 854, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.292, mean reward: -0.173 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.311, 10.185], loss: 0.002357, mae: 0.047937, mean_q: -0.336512
 85500/100000: episode: 855, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.006, mean reward: -0.190 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.921, 10.137], loss: 0.002255, mae: 0.046582, mean_q: -0.321763
 85600/100000: episode: 856, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.139, mean reward: -0.151 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.843, 10.098], loss: 0.002228, mae: 0.046245, mean_q: -0.329406
 85700/100000: episode: 857, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.313, mean reward: -0.153 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.140, 10.398], loss: 0.002230, mae: 0.046321, mean_q: -0.294101
 85800/100000: episode: 858, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.853, mean reward: -0.189 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.680, 10.234], loss: 0.002482, mae: 0.049475, mean_q: -0.301704
 85900/100000: episode: 859, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -13.721, mean reward: -0.137 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.689, 10.098], loss: 0.002321, mae: 0.047744, mean_q: -0.281553
 86000/100000: episode: 860, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.694, mean reward: -0.167 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.442, 10.098], loss: 0.002554, mae: 0.049855, mean_q: -0.295656
 86100/100000: episode: 861, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.813, mean reward: -0.178 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.554, 10.098], loss: 0.002474, mae: 0.048109, mean_q: -0.327788
 86200/100000: episode: 862, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.914, mean reward: -0.159 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.993, 10.098], loss: 0.002501, mae: 0.048466, mean_q: -0.292182
 86300/100000: episode: 863, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.700, mean reward: -0.187 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.942, 10.216], loss: 0.002375, mae: 0.047632, mean_q: -0.336022
 86400/100000: episode: 864, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.098, mean reward: -0.181 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.718, 10.251], loss: 0.002433, mae: 0.049209, mean_q: -0.260919
 86500/100000: episode: 865, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.802, mean reward: -0.148 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.406, 10.098], loss: 0.002413, mae: 0.048443, mean_q: -0.315156
 86600/100000: episode: 866, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.905, mean reward: -0.159 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.361, 10.206], loss: 0.002382, mae: 0.048312, mean_q: -0.302955
 86700/100000: episode: 867, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: -11.721, mean reward: -0.117 [-1.000, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.602, 10.098], loss: 0.002522, mae: 0.049765, mean_q: -0.320098
 86800/100000: episode: 868, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.752, mean reward: -0.198 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.346, 10.168], loss: 0.002296, mae: 0.047216, mean_q: -0.306915
 86900/100000: episode: 869, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.812, mean reward: -0.188 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.763, 10.098], loss: 0.002338, mae: 0.047402, mean_q: -0.311202
 87000/100000: episode: 870, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -11.900, mean reward: -0.119 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.811, 10.098], loss: 0.002389, mae: 0.049360, mean_q: -0.284795
 87100/100000: episode: 871, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.952, mean reward: -0.150 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.807, 10.098], loss: 0.002365, mae: 0.048484, mean_q: -0.316189
 87200/100000: episode: 872, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.349, mean reward: -0.173 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.776, 10.267], loss: 0.006133, mae: 0.072293, mean_q: -0.261107
 87300/100000: episode: 873, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.365, mean reward: -0.154 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.778, 10.196], loss: 0.004396, mae: 0.062449, mean_q: -0.303826
 87400/100000: episode: 874, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -19.651, mean reward: -0.197 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.310, 10.149], loss: 0.002453, mae: 0.048676, mean_q: -0.312098
 87500/100000: episode: 875, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.393, mean reward: -0.164 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.889, 10.140], loss: 0.002501, mae: 0.047822, mean_q: -0.317737
 87600/100000: episode: 876, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -12.037, mean reward: -0.120 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.354, 10.358], loss: 0.002505, mae: 0.048310, mean_q: -0.304168
 87700/100000: episode: 877, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.819, mean reward: -0.188 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.466, 10.098], loss: 0.002448, mae: 0.047371, mean_q: -0.307116
 87800/100000: episode: 878, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.198, mean reward: -0.162 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.589, 10.098], loss: 0.002180, mae: 0.045795, mean_q: -0.283424
 87900/100000: episode: 879, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -12.635, mean reward: -0.126 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.881, 10.186], loss: 0.002490, mae: 0.047818, mean_q: -0.284818
 88000/100000: episode: 880, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.993, mean reward: -0.180 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.548, 10.240], loss: 0.002471, mae: 0.047658, mean_q: -0.277442
 88100/100000: episode: 881, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.035, mean reward: -0.170 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.255, 10.330], loss: 0.002571, mae: 0.048399, mean_q: -0.318209
 88200/100000: episode: 882, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.574, mean reward: -0.196 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.734, 10.297], loss: 0.002385, mae: 0.046930, mean_q: -0.298863
 88300/100000: episode: 883, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.720, mean reward: -0.167 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.115, 10.231], loss: 0.002393, mae: 0.046438, mean_q: -0.303588
 88400/100000: episode: 884, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.704, mean reward: -0.177 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.529, 10.233], loss: 0.002539, mae: 0.048306, mean_q: -0.335337
 88500/100000: episode: 885, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -13.281, mean reward: -0.133 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.489, 10.220], loss: 0.002422, mae: 0.047282, mean_q: -0.307821
 88600/100000: episode: 886, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.765, mean reward: -0.148 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.459, 10.098], loss: 0.002344, mae: 0.047150, mean_q: -0.289006
 88700/100000: episode: 887, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.138, mean reward: -0.191 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.177, 10.128], loss: 0.002220, mae: 0.044888, mean_q: -0.329405
 88800/100000: episode: 888, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.175, mean reward: -0.152 [-1.000, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.465, 10.098], loss: 0.002342, mae: 0.046595, mean_q: -0.301455
 88900/100000: episode: 889, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -20.428, mean reward: -0.204 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.771, 10.109], loss: 0.002473, mae: 0.048135, mean_q: -0.273533
 89000/100000: episode: 890, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.234, mean reward: -0.162 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.973, 10.098], loss: 0.002511, mae: 0.047834, mean_q: -0.313935
 89100/100000: episode: 891, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.884, mean reward: -0.179 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.462, 10.242], loss: 0.002348, mae: 0.045569, mean_q: -0.358387
 89200/100000: episode: 892, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.182, mean reward: -0.162 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.395, 10.165], loss: 0.002639, mae: 0.050622, mean_q: -0.325873
 89300/100000: episode: 893, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.828, mean reward: -0.188 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.017, 10.286], loss: 0.002361, mae: 0.046638, mean_q: -0.331229
 89400/100000: episode: 894, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.943, mean reward: -0.169 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.919, 10.098], loss: 0.002494, mae: 0.048877, mean_q: -0.309809
 89500/100000: episode: 895, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.792, mean reward: -0.158 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.248, 10.244], loss: 0.002508, mae: 0.048591, mean_q: -0.298936
 89600/100000: episode: 896, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.986, mean reward: -0.180 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.790, 10.102], loss: 0.002341, mae: 0.046278, mean_q: -0.291359
 89700/100000: episode: 897, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.102, mean reward: -0.171 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.570, 10.098], loss: 0.002483, mae: 0.047823, mean_q: -0.315618
 89800/100000: episode: 898, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -13.873, mean reward: -0.139 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.135, 10.098], loss: 0.002621, mae: 0.048330, mean_q: -0.309368
 89900/100000: episode: 899, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -5.703, mean reward: -0.057 [-1.000, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.175, 10.098], loss: 0.002494, mae: 0.047139, mean_q: -0.316397
 90000/100000: episode: 900, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -18.952, mean reward: -0.190 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.746, 10.236], loss: 0.002376, mae: 0.046969, mean_q: -0.290384
 90100/100000: episode: 901, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.469, mean reward: -0.165 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.142, 10.350], loss: 0.002469, mae: 0.047987, mean_q: -0.331577
 90200/100000: episode: 902, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.899, mean reward: -0.149 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.512, 10.098], loss: 0.002550, mae: 0.049548, mean_q: -0.291771
 90300/100000: episode: 903, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.825, mean reward: -0.178 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.880, 10.183], loss: 0.002419, mae: 0.046503, mean_q: -0.304304
 90400/100000: episode: 904, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -20.234, mean reward: -0.202 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.752, 10.098], loss: 0.002510, mae: 0.049020, mean_q: -0.318502
 90500/100000: episode: 905, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.110, mean reward: -0.181 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.897, 10.475], loss: 0.002727, mae: 0.049572, mean_q: -0.338685
 90600/100000: episode: 906, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.651, mean reward: -0.177 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.715, 10.098], loss: 0.002452, mae: 0.048090, mean_q: -0.331099
 90700/100000: episode: 907, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.438, mean reward: -0.194 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.135, 10.098], loss: 0.002813, mae: 0.052284, mean_q: -0.295385
 90800/100000: episode: 908, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.581, mean reward: -0.146 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.462, 10.098], loss: 0.002498, mae: 0.048212, mean_q: -0.303894
 90900/100000: episode: 909, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.278, mean reward: -0.173 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.698, 10.299], loss: 0.002745, mae: 0.049998, mean_q: -0.298333
 91000/100000: episode: 910, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -17.180, mean reward: -0.172 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.460, 10.098], loss: 0.002688, mae: 0.048868, mean_q: -0.321271
 91100/100000: episode: 911, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.831, mean reward: -0.178 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.384, 10.343], loss: 0.002534, mae: 0.049337, mean_q: -0.296058
 91200/100000: episode: 912, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -20.490, mean reward: -0.205 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.631, 10.178], loss: 0.002726, mae: 0.049999, mean_q: -0.306354
 91300/100000: episode: 913, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.425, mean reward: -0.184 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.619, 10.115], loss: 0.002609, mae: 0.049410, mean_q: -0.297099
 91400/100000: episode: 914, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.581, mean reward: -0.136 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.964, 10.098], loss: 0.002740, mae: 0.049877, mean_q: -0.327173
 91500/100000: episode: 915, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.937, mean reward: -0.159 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.078, 10.098], loss: 0.002739, mae: 0.050492, mean_q: -0.330697
 91600/100000: episode: 916, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -17.227, mean reward: -0.172 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.276, 10.098], loss: 0.002665, mae: 0.049982, mean_q: -0.307269
 91700/100000: episode: 917, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.912, mean reward: -0.179 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.241, 10.098], loss: 0.002712, mae: 0.050105, mean_q: -0.274948
 91800/100000: episode: 918, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.647, mean reward: -0.186 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.528, 10.098], loss: 0.002191, mae: 0.045676, mean_q: -0.322505
 91900/100000: episode: 919, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -18.907, mean reward: -0.189 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.668, 10.386], loss: 0.002742, mae: 0.052109, mean_q: -0.270076
 92000/100000: episode: 920, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.399, mean reward: -0.184 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.591, 10.215], loss: 0.005247, mae: 0.066295, mean_q: -0.310961
 92100/100000: episode: 921, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.794, mean reward: -0.148 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.542, 10.098], loss: 0.003553, mae: 0.057925, mean_q: -0.313176
 92200/100000: episode: 922, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.946, mean reward: -0.179 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.741, 10.272], loss: 0.002682, mae: 0.051416, mean_q: -0.318111
 92300/100000: episode: 923, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -17.635, mean reward: -0.176 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.972, 10.109], loss: 0.002543, mae: 0.048494, mean_q: -0.311151
 92400/100000: episode: 924, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.964, mean reward: -0.150 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.553, 10.098], loss: 0.002344, mae: 0.047623, mean_q: -0.316593
 92500/100000: episode: 925, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -20.188, mean reward: -0.202 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.779, 10.158], loss: 0.002506, mae: 0.048119, mean_q: -0.339261
 92600/100000: episode: 926, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.126, mean reward: -0.171 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.346, 10.452], loss: 0.002322, mae: 0.047012, mean_q: -0.332694
 92700/100000: episode: 927, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.606, mean reward: -0.186 [-1.000, 0.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.854, 10.241], loss: 0.002553, mae: 0.047757, mean_q: -0.344452
 92800/100000: episode: 928, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -13.774, mean reward: -0.138 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.319, 10.181], loss: 0.002303, mae: 0.045480, mean_q: -0.337045
 92900/100000: episode: 929, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.080, mean reward: -0.191 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.174, 10.101], loss: 0.002366, mae: 0.046997, mean_q: -0.327108
 93000/100000: episode: 930, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.061, mean reward: -0.181 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.471, 10.212], loss: 0.002635, mae: 0.048969, mean_q: -0.310520
 93100/100000: episode: 931, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -15.415, mean reward: -0.154 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.984, 10.098], loss: 0.002526, mae: 0.048617, mean_q: -0.333028
 93200/100000: episode: 932, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -6.533, mean reward: -0.065 [-1.000, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.954, 10.098], loss: 0.002396, mae: 0.047726, mean_q: -0.344002
 93300/100000: episode: 933, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -12.343, mean reward: -0.123 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.958, 10.098], loss: 0.002320, mae: 0.046689, mean_q: -0.321634
 93400/100000: episode: 934, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.486, mean reward: -0.155 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.702, 10.098], loss: 0.002559, mae: 0.049045, mean_q: -0.324429
 93500/100000: episode: 935, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.549, mean reward: -0.175 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.974, 10.167], loss: 0.002553, mae: 0.049083, mean_q: -0.333134
 93600/100000: episode: 936, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.850, mean reward: -0.188 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.897, 10.118], loss: 0.002527, mae: 0.047997, mean_q: -0.348985
 93700/100000: episode: 937, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -11.195, mean reward: -0.112 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.581, 10.393], loss: 0.002829, mae: 0.051778, mean_q: -0.293280
 93800/100000: episode: 938, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -19.361, mean reward: -0.194 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.244, 10.098], loss: 0.005540, mae: 0.066705, mean_q: -0.297341
 93900/100000: episode: 939, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.506, mean reward: -0.175 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.387, 10.098], loss: 0.002652, mae: 0.053473, mean_q: -0.317540
 94000/100000: episode: 940, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.592, mean reward: -0.146 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.029, 10.366], loss: 0.002484, mae: 0.049134, mean_q: -0.291198
 94100/100000: episode: 941, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.856, mean reward: -0.189 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.940, 10.098], loss: 0.002586, mae: 0.049284, mean_q: -0.304676
 94200/100000: episode: 942, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.500, mean reward: -0.195 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.841, 10.181], loss: 0.002516, mae: 0.048191, mean_q: -0.300735
 94300/100000: episode: 943, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -20.726, mean reward: -0.207 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.238, 10.098], loss: 0.002257, mae: 0.046544, mean_q: -0.327007
 94400/100000: episode: 944, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.532, mean reward: -0.165 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.583, 10.287], loss: 0.002335, mae: 0.046519, mean_q: -0.318770
 94500/100000: episode: 945, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.041, mean reward: -0.190 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.110, 10.239], loss: 0.002518, mae: 0.048026, mean_q: -0.316781
 94600/100000: episode: 946, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.931, mean reward: -0.199 [-1.000, 0.249], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.684, 10.196], loss: 0.002251, mae: 0.045394, mean_q: -0.360000
 94700/100000: episode: 947, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.727, mean reward: -0.197 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.200, 10.098], loss: 0.002400, mae: 0.047436, mean_q: -0.320130
 94800/100000: episode: 948, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.174, mean reward: -0.192 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.717, 10.098], loss: 0.002524, mae: 0.048530, mean_q: -0.328535
 94900/100000: episode: 949, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.392, mean reward: -0.154 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.839, 10.098], loss: 0.002209, mae: 0.045544, mean_q: -0.339477
 95000/100000: episode: 950, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -11.872, mean reward: -0.119 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.593, 10.098], loss: 0.002345, mae: 0.046208, mean_q: -0.344326
 95100/100000: episode: 951, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.990, mean reward: -0.180 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.767, 10.098], loss: 0.002450, mae: 0.047846, mean_q: -0.317318
 95200/100000: episode: 952, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.432, mean reward: -0.194 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.038, 10.098], loss: 0.002472, mae: 0.048272, mean_q: -0.318768
 95300/100000: episode: 953, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.340, mean reward: -0.183 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.899, 10.098], loss: 0.002301, mae: 0.046475, mean_q: -0.346188
 95400/100000: episode: 954, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.668, mean reward: -0.157 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.410, 10.118], loss: 0.002279, mae: 0.046480, mean_q: -0.337437
 95500/100000: episode: 955, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.905, mean reward: -0.159 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.788, 10.098], loss: 0.002504, mae: 0.048048, mean_q: -0.331965
 95600/100000: episode: 956, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -14.975, mean reward: -0.150 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.107, 10.098], loss: 0.002558, mae: 0.047996, mean_q: -0.362731
 95700/100000: episode: 957, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -19.003, mean reward: -0.190 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.982, 10.098], loss: 0.002432, mae: 0.047758, mean_q: -0.313122
 95800/100000: episode: 958, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.161, mean reward: -0.162 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.519, 10.362], loss: 0.002512, mae: 0.049231, mean_q: -0.320576
 95900/100000: episode: 959, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.911, mean reward: -0.179 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.856, 10.098], loss: 0.002492, mae: 0.047856, mean_q: -0.301712
 96000/100000: episode: 960, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -20.129, mean reward: -0.201 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.557, 10.139], loss: 0.002638, mae: 0.049533, mean_q: -0.306082
 96100/100000: episode: 961, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -15.819, mean reward: -0.158 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.519, 10.506], loss: 0.002401, mae: 0.047323, mean_q: -0.302543
 96200/100000: episode: 962, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.948, mean reward: -0.209 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.298, 10.127], loss: 0.002356, mae: 0.046090, mean_q: -0.353564
 96300/100000: episode: 963, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.187, mean reward: -0.172 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.368, 10.098], loss: 0.002512, mae: 0.048024, mean_q: -0.341411
 96400/100000: episode: 964, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -19.465, mean reward: -0.195 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.543, 10.135], loss: 0.002428, mae: 0.047904, mean_q: -0.314632
 96500/100000: episode: 965, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -19.705, mean reward: -0.197 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.767, 10.098], loss: 0.002472, mae: 0.049147, mean_q: -0.343251
 96600/100000: episode: 966, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -15.699, mean reward: -0.157 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.220, 10.150], loss: 0.002706, mae: 0.049925, mean_q: -0.340531
 96700/100000: episode: 967, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.504, mean reward: -0.165 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.639, 10.098], loss: 0.002469, mae: 0.048909, mean_q: -0.316697
 96800/100000: episode: 968, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -14.437, mean reward: -0.144 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.110, 10.122], loss: 0.002409, mae: 0.047396, mean_q: -0.334851
 96900/100000: episode: 969, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.025, mean reward: -0.180 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.279, 10.098], loss: 0.002724, mae: 0.050773, mean_q: -0.314601
 97000/100000: episode: 970, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -15.199, mean reward: -0.152 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.313, 10.098], loss: 0.002635, mae: 0.049706, mean_q: -0.321442
 97100/100000: episode: 971, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.150, mean reward: -0.181 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.364, 10.148], loss: 0.002412, mae: 0.047884, mean_q: -0.327262
 97200/100000: episode: 972, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.308, mean reward: -0.153 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.794, 10.098], loss: 0.002537, mae: 0.049579, mean_q: -0.316863
 97300/100000: episode: 973, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.690, mean reward: -0.197 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.603, 10.098], loss: 0.003088, mae: 0.055586, mean_q: -0.322718
 97400/100000: episode: 974, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -13.568, mean reward: -0.136 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.539, 10.098], loss: 0.003481, mae: 0.056660, mean_q: -0.300426
 97500/100000: episode: 975, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.299, mean reward: -0.173 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.776, 10.167], loss: 0.002533, mae: 0.050653, mean_q: -0.321801
 97600/100000: episode: 976, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.466, mean reward: -0.165 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.368, 10.178], loss: 0.002253, mae: 0.046655, mean_q: -0.294750
 97700/100000: episode: 977, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.835, mean reward: -0.178 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.609, 10.160], loss: 0.002347, mae: 0.045795, mean_q: -0.359333
 97800/100000: episode: 978, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -15.052, mean reward: -0.151 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.214, 10.098], loss: 0.002393, mae: 0.047500, mean_q: -0.292731
 97900/100000: episode: 979, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.281, mean reward: -0.173 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.750, 10.098], loss: 0.002489, mae: 0.048107, mean_q: -0.336040
 98000/100000: episode: 980, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.231, mean reward: -0.182 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.480, 10.228], loss: 0.002633, mae: 0.049330, mean_q: -0.311295
 98100/100000: episode: 981, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.372, mean reward: -0.164 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.469, 10.098], loss: 0.002399, mae: 0.047383, mean_q: -0.321853
 98200/100000: episode: 982, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.164, mean reward: -0.172 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.436, 10.286], loss: 0.002533, mae: 0.048237, mean_q: -0.323786
 98300/100000: episode: 983, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.517, mean reward: -0.185 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.633, 10.098], loss: 0.002601, mae: 0.048950, mean_q: -0.320298
 98400/100000: episode: 984, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.322, mean reward: -0.193 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.273, 10.149], loss: 0.002443, mae: 0.047440, mean_q: -0.341772
 98500/100000: episode: 985, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.813, mean reward: -0.168 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.403, 10.120], loss: 0.002520, mae: 0.048660, mean_q: -0.330020
 98600/100000: episode: 986, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.716, mean reward: -0.177 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.296, 10.294], loss: 0.002426, mae: 0.048434, mean_q: -0.322690
 98700/100000: episode: 987, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -10.734, mean reward: -0.107 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.599, 10.302], loss: 0.002382, mae: 0.047169, mean_q: -0.321357
 98800/100000: episode: 988, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.860, mean reward: -0.189 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.822, 10.098], loss: 0.002559, mae: 0.048207, mean_q: -0.327431
 98900/100000: episode: 989, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.606, mean reward: -0.146 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.603, 10.285], loss: 0.002508, mae: 0.048573, mean_q: -0.316065
 99000/100000: episode: 990, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.752, mean reward: -0.178 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.525, 10.118], loss: 0.002495, mae: 0.047852, mean_q: -0.331883
 99100/100000: episode: 991, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -10.155, mean reward: -0.102 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.497, 10.569], loss: 0.002484, mae: 0.048586, mean_q: -0.323463
 99200/100000: episode: 992, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -13.263, mean reward: -0.133 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.364, 10.098], loss: 0.002472, mae: 0.049018, mean_q: -0.296129
 99300/100000: episode: 993, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.350, mean reward: -0.174 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.629, 10.185], loss: 0.002356, mae: 0.047134, mean_q: -0.317872
 99400/100000: episode: 994, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.428, mean reward: -0.184 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.613, 10.098], loss: 0.002508, mae: 0.049212, mean_q: -0.327089
 99500/100000: episode: 995, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.558, mean reward: -0.176 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.569, 10.129], loss: 0.002774, mae: 0.051131, mean_q: -0.314611
 99600/100000: episode: 996, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.952, mean reward: -0.190 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.684, 10.098], loss: 0.002688, mae: 0.050499, mean_q: -0.306151
 99700/100000: episode: 997, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -12.445, mean reward: -0.124 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.000, 10.249], loss: 0.002525, mae: 0.049261, mean_q: -0.291364
 99800/100000: episode: 998, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -14.641, mean reward: -0.146 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.477, 10.098], loss: 0.002772, mae: 0.051102, mean_q: -0.319047
 99900/100000: episode: 999, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.467, mean reward: -0.185 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.610, 10.328], loss: 0.002481, mae: 0.047975, mean_q: -0.346370
 100000/100000: episode: 1000, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -9.652, mean reward: -0.097 [-1.000, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.675, 10.246], loss: 0.003162, mae: 0.057768, mean_q: -0.282339
done, took 535.653 seconds
[Info] End Uniform Random Simulation. Falsification occurred 0 times.
