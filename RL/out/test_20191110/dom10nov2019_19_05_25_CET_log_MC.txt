Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.167s, episode steps: 100, steps per second: 598, episode reward: -21.835, mean reward: -0.218 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.244, 10.128], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.062s, episode steps: 100, steps per second: 1611, episode reward: -20.318, mean reward: -0.203 [-1.000, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.732, 10.125], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.066s, episode steps: 100, steps per second: 1526, episode reward: -16.661, mean reward: -0.167 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.350, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.060s, episode steps: 100, steps per second: 1656, episode reward: -17.137, mean reward: -0.171 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.690, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.060s, episode steps: 100, steps per second: 1673, episode reward: -15.800, mean reward: -0.158 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.985, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.263s, episode steps: 100, steps per second: 79, episode reward: -15.674, mean reward: -0.157 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.544, 10.098], loss: 0.060360, mae: 0.239185, mean_q: 0.337525
   700/100000: episode: 7, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.929, mean reward: -0.169 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.217, 10.151], loss: 0.017637, mae: 0.128885, mean_q: 0.056414
   800/100000: episode: 8, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.970, mean reward: -0.150 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.811, 10.098], loss: 0.015599, mae: 0.117775, mean_q: -0.074843
   900/100000: episode: 9, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -17.269, mean reward: -0.173 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.529, 10.098], loss: 0.011213, mae: 0.099629, mean_q: -0.199970
  1000/100000: episode: 10, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.232, mean reward: -0.192 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.750, 10.159], loss: 0.010682, mae: 0.098376, mean_q: -0.245946
  1100/100000: episode: 11, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -11.524, mean reward: -0.115 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.694, 10.098], loss: 0.011412, mae: 0.099120, mean_q: -0.298242
  1200/100000: episode: 12, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.064, mean reward: -0.181 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.943, 10.144], loss: 0.010466, mae: 0.095744, mean_q: -0.261807
  1300/100000: episode: 13, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.401, mean reward: -0.184 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.627, 10.216], loss: 0.008712, mae: 0.086912, mean_q: -0.302624
  1400/100000: episode: 14, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.325, mean reward: -0.173 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.809, 10.142], loss: 0.008545, mae: 0.083356, mean_q: -0.323412
  1500/100000: episode: 15, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.296, mean reward: -0.183 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.059, 10.114], loss: 0.007958, mae: 0.081889, mean_q: -0.337348
  1600/100000: episode: 16, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -13.401, mean reward: -0.134 [-1.000, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.916, 10.226], loss: 0.008115, mae: 0.081894, mean_q: -0.340663
  1700/100000: episode: 17, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.334, mean reward: -0.183 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.657, 10.285], loss: 0.007868, mae: 0.080988, mean_q: -0.307295
  1800/100000: episode: 18, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.534, mean reward: -0.175 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.167, 10.098], loss: 0.007149, mae: 0.078176, mean_q: -0.343844
  1900/100000: episode: 19, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.259, mean reward: -0.193 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.499, 10.098], loss: 0.007629, mae: 0.079762, mean_q: -0.332842
  2000/100000: episode: 20, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.655, mean reward: -0.167 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.577, 10.329], loss: 0.006944, mae: 0.079044, mean_q: -0.330821
  2100/100000: episode: 21, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.799, mean reward: -0.158 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.179, 10.098], loss: 0.006922, mae: 0.076331, mean_q: -0.317941
  2200/100000: episode: 22, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.976, mean reward: -0.190 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.847, 10.109], loss: 0.007399, mae: 0.080865, mean_q: -0.313660
  2300/100000: episode: 23, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.324, mean reward: -0.163 [-1.000, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.870, 10.166], loss: 0.006637, mae: 0.077467, mean_q: -0.310239
  2400/100000: episode: 24, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.655, mean reward: -0.187 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.701, 10.107], loss: 0.006531, mae: 0.074513, mean_q: -0.335614
  2500/100000: episode: 25, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.677, mean reward: -0.197 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.635, 10.116], loss: 0.007033, mae: 0.076218, mean_q: -0.349821
  2600/100000: episode: 26, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.321, mean reward: -0.163 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.407, 10.210], loss: 0.007252, mae: 0.078496, mean_q: -0.351149
  2700/100000: episode: 27, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.181, mean reward: -0.182 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.716, 10.273], loss: 0.006514, mae: 0.075003, mean_q: -0.314900
  2800/100000: episode: 28, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.424, mean reward: -0.184 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.891, 10.274], loss: 0.005434, mae: 0.071113, mean_q: -0.319045
  2900/100000: episode: 29, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.661, mean reward: -0.147 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.440, 10.098], loss: 0.006988, mae: 0.078352, mean_q: -0.335181
  3000/100000: episode: 30, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.078, mean reward: -0.171 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.598, 10.098], loss: 0.005761, mae: 0.073998, mean_q: -0.324786
  3100/100000: episode: 31, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.933, mean reward: -0.169 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.648, 10.162], loss: 0.005617, mae: 0.071314, mean_q: -0.306663
  3200/100000: episode: 32, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.185, mean reward: -0.192 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.860, 10.098], loss: 0.005531, mae: 0.071707, mean_q: -0.363469
  3300/100000: episode: 33, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.015, mean reward: -0.180 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.064, 10.340], loss: 0.006077, mae: 0.072984, mean_q: -0.344393
  3400/100000: episode: 34, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.558, mean reward: -0.186 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.379, 10.157], loss: 0.006032, mae: 0.074223, mean_q: -0.296483
  3500/100000: episode: 35, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -11.265, mean reward: -0.113 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.754, 10.098], loss: 0.005178, mae: 0.068186, mean_q: -0.345155
  3600/100000: episode: 36, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.491, mean reward: -0.175 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.978, 10.098], loss: 0.006015, mae: 0.075509, mean_q: -0.352288
  3700/100000: episode: 37, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -13.506, mean reward: -0.135 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.278, 10.098], loss: 0.005733, mae: 0.072017, mean_q: -0.321371
  3800/100000: episode: 38, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -20.509, mean reward: -0.205 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.973, 10.098], loss: 0.005578, mae: 0.071339, mean_q: -0.344723
  3900/100000: episode: 39, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.627, mean reward: -0.186 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.225, 10.234], loss: 0.005104, mae: 0.068924, mean_q: -0.328072
  4000/100000: episode: 40, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.154, mean reward: -0.182 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.022, 10.342], loss: 0.004984, mae: 0.070153, mean_q: -0.305493
  4100/100000: episode: 41, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.387, mean reward: -0.184 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.699, 10.170], loss: 0.004499, mae: 0.066873, mean_q: -0.337380
  4200/100000: episode: 42, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -20.710, mean reward: -0.207 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.080, 10.098], loss: 0.004803, mae: 0.067313, mean_q: -0.308334
  4300/100000: episode: 43, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.149, mean reward: -0.171 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.900, 10.346], loss: 0.005866, mae: 0.073984, mean_q: -0.350989
  4400/100000: episode: 44, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.594, mean reward: -0.126 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.576, 10.354], loss: 0.005103, mae: 0.070342, mean_q: -0.336715
  4500/100000: episode: 45, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.455, mean reward: -0.185 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.456, 10.215], loss: 0.005664, mae: 0.073566, mean_q: -0.334968
  4600/100000: episode: 46, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.851, mean reward: -0.179 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.014, 10.114], loss: 0.004786, mae: 0.067717, mean_q: -0.348236
  4700/100000: episode: 47, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -14.577, mean reward: -0.146 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.160, 10.098], loss: 0.004566, mae: 0.067053, mean_q: -0.351736
  4800/100000: episode: 48, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -16.987, mean reward: -0.170 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.818, 10.098], loss: 0.004639, mae: 0.066553, mean_q: -0.343604
  4900/100000: episode: 49, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -19.954, mean reward: -0.200 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.414, 10.157], loss: 0.005239, mae: 0.071070, mean_q: -0.303598
  5000/100000: episode: 50, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -18.638, mean reward: -0.186 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.756, 10.306], loss: 0.005546, mae: 0.070576, mean_q: -0.327884
  5100/100000: episode: 51, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.303, mean reward: -0.153 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.816, 10.385], loss: 0.004880, mae: 0.068173, mean_q: -0.322554
  5200/100000: episode: 52, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.091, mean reward: -0.171 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.183, 10.098], loss: 0.005257, mae: 0.070153, mean_q: -0.301413
  5300/100000: episode: 53, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -15.718, mean reward: -0.157 [-1.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.686, 10.098], loss: 0.004941, mae: 0.069937, mean_q: -0.304382
  5400/100000: episode: 54, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -12.448, mean reward: -0.124 [-1.000, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.376, 10.498], loss: 0.004261, mae: 0.064444, mean_q: -0.333936
  5500/100000: episode: 55, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -7.975, mean reward: -0.080 [-1.000, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.814, 10.098], loss: 0.004436, mae: 0.066653, mean_q: -0.331271
  5600/100000: episode: 56, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.223, mean reward: -0.182 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.342, 10.098], loss: 0.005743, mae: 0.072821, mean_q: -0.348004
  5700/100000: episode: 57, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.216, mean reward: -0.172 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.034, 10.098], loss: 0.004345, mae: 0.065618, mean_q: -0.314281
  5800/100000: episode: 58, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.771, mean reward: -0.188 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.437, 10.267], loss: 0.005480, mae: 0.073015, mean_q: -0.298090
  5900/100000: episode: 59, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.251, mean reward: -0.163 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.199, 10.284], loss: 0.005616, mae: 0.069831, mean_q: -0.333355
  6000/100000: episode: 60, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -16.652, mean reward: -0.167 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.740, 10.098], loss: 0.005430, mae: 0.071849, mean_q: -0.308704
  6100/100000: episode: 61, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -18.231, mean reward: -0.182 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.133, 10.120], loss: 0.004319, mae: 0.064410, mean_q: -0.326126
  6200/100000: episode: 62, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -14.128, mean reward: -0.141 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.672, 10.098], loss: 0.004309, mae: 0.065474, mean_q: -0.313352
  6300/100000: episode: 63, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.698, mean reward: -0.187 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.341, 10.283], loss: 0.004456, mae: 0.066750, mean_q: -0.303515
  6400/100000: episode: 64, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -13.473, mean reward: -0.135 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.576, 10.098], loss: 0.005631, mae: 0.073455, mean_q: -0.337094
  6500/100000: episode: 65, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.864, mean reward: -0.169 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.261, 10.307], loss: 0.005413, mae: 0.073118, mean_q: -0.312740
  6600/100000: episode: 66, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.594, mean reward: -0.186 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.952, 10.134], loss: 0.004334, mae: 0.065836, mean_q: -0.303861
  6700/100000: episode: 67, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -16.748, mean reward: -0.167 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.730, 10.108], loss: 0.004619, mae: 0.066343, mean_q: -0.350631
  6800/100000: episode: 68, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -13.402, mean reward: -0.134 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.119, 10.098], loss: 0.004472, mae: 0.066555, mean_q: -0.338583
  6900/100000: episode: 69, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.072, mean reward: -0.191 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.946, 10.296], loss: 0.004265, mae: 0.064583, mean_q: -0.356677
  7000/100000: episode: 70, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -13.447, mean reward: -0.134 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.859, 10.104], loss: 0.004760, mae: 0.068931, mean_q: -0.317631
  7100/100000: episode: 71, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.913, mean reward: -0.179 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.288, 10.147], loss: 0.004727, mae: 0.066304, mean_q: -0.309981
  7200/100000: episode: 72, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.003, mean reward: -0.180 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.570, 10.312], loss: 0.004967, mae: 0.068440, mean_q: -0.308292
  7300/100000: episode: 73, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.006, mean reward: -0.180 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.081, 10.321], loss: 0.005461, mae: 0.071373, mean_q: -0.286482
  7400/100000: episode: 74, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.862, mean reward: -0.159 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.928, 10.119], loss: 0.004393, mae: 0.065752, mean_q: -0.329260
  7500/100000: episode: 75, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.961, mean reward: -0.180 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.889, 10.098], loss: 0.004866, mae: 0.069572, mean_q: -0.294529
  7600/100000: episode: 76, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.891, mean reward: -0.199 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.647, 10.185], loss: 0.004676, mae: 0.067425, mean_q: -0.351618
  7700/100000: episode: 77, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -18.533, mean reward: -0.185 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.733, 10.245], loss: 0.003880, mae: 0.063396, mean_q: -0.301830
  7800/100000: episode: 78, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.797, mean reward: -0.168 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.670, 10.328], loss: 0.004035, mae: 0.065585, mean_q: -0.315948
  7900/100000: episode: 79, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -17.924, mean reward: -0.179 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.502, 10.098], loss: 0.003984, mae: 0.064243, mean_q: -0.298393
  8000/100000: episode: 80, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.476, mean reward: -0.175 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.446, 10.098], loss: 0.003950, mae: 0.063724, mean_q: -0.326772
  8100/100000: episode: 81, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -20.889, mean reward: -0.209 [-1.000, 0.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.354, 10.098], loss: 0.004485, mae: 0.065366, mean_q: -0.311104
  8200/100000: episode: 82, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.509, mean reward: -0.195 [-1.000, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.779, 10.216], loss: 0.004257, mae: 0.067076, mean_q: -0.317072
  8300/100000: episode: 83, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.504, mean reward: -0.175 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.678, 10.157], loss: 0.004467, mae: 0.066964, mean_q: -0.328007
  8400/100000: episode: 84, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.225, mean reward: -0.152 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.002, 10.385], loss: 0.004436, mae: 0.066405, mean_q: -0.319651
  8500/100000: episode: 85, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -17.678, mean reward: -0.177 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.931, 10.098], loss: 0.004261, mae: 0.065107, mean_q: -0.347098
  8600/100000: episode: 86, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -16.614, mean reward: -0.166 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.931, 10.151], loss: 0.003766, mae: 0.061508, mean_q: -0.339331
  8700/100000: episode: 87, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.722, mean reward: -0.197 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.880, 10.098], loss: 0.003788, mae: 0.061883, mean_q: -0.332082
  8800/100000: episode: 88, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.181, mean reward: -0.192 [-1.000, 0.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.925, 10.098], loss: 0.004379, mae: 0.065185, mean_q: -0.305302
  8900/100000: episode: 89, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.908, mean reward: -0.169 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.940, 10.165], loss: 0.004844, mae: 0.067497, mean_q: -0.297859
  9000/100000: episode: 90, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.228, mean reward: -0.192 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.733, 10.131], loss: 0.003510, mae: 0.060566, mean_q: -0.332826
  9100/100000: episode: 91, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -11.778, mean reward: -0.118 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.272, 10.325], loss: 0.003990, mae: 0.064208, mean_q: -0.287601
  9200/100000: episode: 92, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.775, mean reward: -0.178 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.670, 10.098], loss: 0.004771, mae: 0.068301, mean_q: -0.317341
  9300/100000: episode: 93, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -14.070, mean reward: -0.141 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.688, 10.380], loss: 0.004871, mae: 0.067372, mean_q: -0.290265
  9400/100000: episode: 94, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.208, mean reward: -0.192 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.609, 10.098], loss: 0.004017, mae: 0.063542, mean_q: -0.315822
  9500/100000: episode: 95, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.601, mean reward: -0.196 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.717, 10.192], loss: 0.005133, mae: 0.068392, mean_q: -0.333364
  9600/100000: episode: 96, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -5.420, mean reward: -0.054 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.671, 10.501], loss: 0.004080, mae: 0.063949, mean_q: -0.319495
  9700/100000: episode: 97, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -11.586, mean reward: -0.116 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.558, 10.098], loss: 0.004390, mae: 0.064606, mean_q: -0.302309
  9800/100000: episode: 98, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -16.825, mean reward: -0.168 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.853, 10.098], loss: 0.004606, mae: 0.067236, mean_q: -0.370377
  9900/100000: episode: 99, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -12.296, mean reward: -0.123 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.061, 10.098], loss: 0.005194, mae: 0.069353, mean_q: -0.323639
 10000/100000: episode: 100, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -16.123, mean reward: -0.161 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.900, 10.098], loss: 0.004115, mae: 0.064507, mean_q: -0.318090
 10100/100000: episode: 101, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -15.519, mean reward: -0.155 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.059, 10.098], loss: 0.004392, mae: 0.065885, mean_q: -0.273278
 10200/100000: episode: 102, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.238, mean reward: -0.192 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.431, 10.098], loss: 0.003803, mae: 0.061859, mean_q: -0.339808
 10300/100000: episode: 103, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.682, mean reward: -0.187 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.654, 10.260], loss: 0.004157, mae: 0.063770, mean_q: -0.347581
 10400/100000: episode: 104, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.745, mean reward: -0.157 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.497, 10.307], loss: 0.003769, mae: 0.061652, mean_q: -0.341453
 10500/100000: episode: 105, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.313, mean reward: -0.183 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.880, 10.098], loss: 0.003988, mae: 0.061511, mean_q: -0.335425
 10600/100000: episode: 106, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.234, mean reward: -0.132 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.761, 10.278], loss: 0.004820, mae: 0.068619, mean_q: -0.299739
 10700/100000: episode: 107, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.487, mean reward: -0.195 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.836, 10.260], loss: 0.004454, mae: 0.064999, mean_q: -0.300977
 10800/100000: episode: 108, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.979, mean reward: -0.150 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.784, 10.098], loss: 0.003857, mae: 0.063811, mean_q: -0.335870
 10900/100000: episode: 109, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.681, mean reward: -0.197 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.019, 10.098], loss: 0.003156, mae: 0.056840, mean_q: -0.341226
 11000/100000: episode: 110, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -11.878, mean reward: -0.119 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.474, 10.228], loss: 0.003767, mae: 0.061096, mean_q: -0.338540
 11100/100000: episode: 111, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.975, mean reward: -0.160 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.490, 10.098], loss: 0.003959, mae: 0.062901, mean_q: -0.285656
 11200/100000: episode: 112, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.099, mean reward: -0.191 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.646, 10.098], loss: 0.006913, mae: 0.075594, mean_q: -0.314134
 11300/100000: episode: 113, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -12.763, mean reward: -0.128 [-1.000, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.235, 10.098], loss: 0.004502, mae: 0.066237, mean_q: -0.282377
 11400/100000: episode: 114, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.524, mean reward: -0.175 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.381, 10.296], loss: 0.005435, mae: 0.068393, mean_q: -0.299857
 11500/100000: episode: 115, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.240, mean reward: -0.142 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.077, 10.098], loss: 0.004081, mae: 0.062216, mean_q: -0.326581
 11600/100000: episode: 116, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.034, mean reward: -0.180 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.518, 10.212], loss: 0.004261, mae: 0.064242, mean_q: -0.277556
 11700/100000: episode: 117, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.192, mean reward: -0.182 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.522, 10.255], loss: 0.003618, mae: 0.060273, mean_q: -0.293165
 11800/100000: episode: 118, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.936, mean reward: -0.179 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.037, 10.160], loss: 0.003281, mae: 0.057233, mean_q: -0.281376
 11900/100000: episode: 119, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.644, mean reward: -0.186 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.524, 10.098], loss: 0.003454, mae: 0.059621, mean_q: -0.279571
 12000/100000: episode: 120, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -14.574, mean reward: -0.146 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.834, 10.148], loss: 0.004184, mae: 0.064089, mean_q: -0.303921
 12100/100000: episode: 121, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.858, mean reward: -0.179 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.729, 10.098], loss: 0.003585, mae: 0.059812, mean_q: -0.316245
 12200/100000: episode: 122, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.994, mean reward: -0.180 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.442, 10.098], loss: 0.003313, mae: 0.057000, mean_q: -0.328848
 12300/100000: episode: 123, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -11.986, mean reward: -0.120 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.118, 10.274], loss: 0.005712, mae: 0.069498, mean_q: -0.332030
 12400/100000: episode: 124, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.152, mean reward: -0.142 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.553, 10.296], loss: 0.004734, mae: 0.066865, mean_q: -0.298009
 12500/100000: episode: 125, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -14.829, mean reward: -0.148 [-1.000, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.304, 10.102], loss: 0.003661, mae: 0.061033, mean_q: -0.303993
 12600/100000: episode: 126, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.087, mean reward: -0.161 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.728, 10.341], loss: 0.003497, mae: 0.059911, mean_q: -0.355760
 12700/100000: episode: 127, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.068, mean reward: -0.171 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.308, 10.099], loss: 0.003543, mae: 0.060971, mean_q: -0.302506
 12800/100000: episode: 128, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.255, mean reward: -0.173 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.187, 10.285], loss: 0.003281, mae: 0.058317, mean_q: -0.329260
 12900/100000: episode: 129, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.980, mean reward: -0.160 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.541, 10.098], loss: 0.003978, mae: 0.061711, mean_q: -0.320386
 13000/100000: episode: 130, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.627, mean reward: -0.186 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.749, 10.098], loss: 0.003843, mae: 0.062026, mean_q: -0.306347
 13100/100000: episode: 131, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.997, mean reward: -0.190 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.033, 10.190], loss: 0.003576, mae: 0.059249, mean_q: -0.309126
 13200/100000: episode: 132, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.071, mean reward: -0.161 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.199, 10.098], loss: 0.006385, mae: 0.072347, mean_q: -0.321868
 13300/100000: episode: 133, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.953, mean reward: -0.180 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.648, 10.098], loss: 0.003168, mae: 0.057484, mean_q: -0.348446
 13400/100000: episode: 134, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -9.534, mean reward: -0.095 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.296, 10.098], loss: 0.004051, mae: 0.065973, mean_q: -0.314641
 13500/100000: episode: 135, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -17.839, mean reward: -0.178 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.996, 10.296], loss: 0.003637, mae: 0.061676, mean_q: -0.341499
 13600/100000: episode: 136, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -10.184, mean reward: -0.102 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.570, 10.098], loss: 0.004034, mae: 0.061979, mean_q: -0.292248
 13700/100000: episode: 137, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.537, mean reward: -0.155 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.398, 10.098], loss: 0.003474, mae: 0.059765, mean_q: -0.302150
 13800/100000: episode: 138, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -13.749, mean reward: -0.137 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.440, 10.098], loss: 0.004846, mae: 0.066534, mean_q: -0.301033
 13900/100000: episode: 139, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.386, mean reward: -0.144 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.370, 10.098], loss: 0.003967, mae: 0.063938, mean_q: -0.293436
 14000/100000: episode: 140, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -19.031, mean reward: -0.190 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.129, 10.098], loss: 0.003771, mae: 0.062342, mean_q: -0.310715
 14100/100000: episode: 141, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.525, mean reward: -0.185 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.517, 10.158], loss: 0.003608, mae: 0.061189, mean_q: -0.304842
 14200/100000: episode: 142, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.013, mean reward: -0.150 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.808, 10.098], loss: 0.003343, mae: 0.058412, mean_q: -0.301503
 14300/100000: episode: 143, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.939, mean reward: -0.159 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.419, 10.192], loss: 0.003412, mae: 0.059927, mean_q: -0.296861
 14400/100000: episode: 144, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -13.619, mean reward: -0.136 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.041, 10.392], loss: 0.003681, mae: 0.063905, mean_q: -0.295869
 14500/100000: episode: 145, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -12.569, mean reward: -0.126 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.473, 10.406], loss: 0.004274, mae: 0.064257, mean_q: -0.281319
 14600/100000: episode: 146, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -16.734, mean reward: -0.167 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.909, 10.098], loss: 0.004583, mae: 0.069241, mean_q: -0.280176
 14700/100000: episode: 147, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -13.193, mean reward: -0.132 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.544, 10.259], loss: 0.004020, mae: 0.064130, mean_q: -0.331344
 14800/100000: episode: 148, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.894, mean reward: -0.169 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.921, 10.098], loss: 0.003324, mae: 0.058012, mean_q: -0.323013
 14900/100000: episode: 149, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.876, mean reward: -0.189 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.781, 10.098], loss: 0.003485, mae: 0.058832, mean_q: -0.318710
 15000/100000: episode: 150, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -18.418, mean reward: -0.184 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.064, 10.098], loss: 0.006105, mae: 0.073186, mean_q: -0.306405
 15100/100000: episode: 151, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -20.757, mean reward: -0.208 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.800, 10.098], loss: 0.003944, mae: 0.064416, mean_q: -0.282257
 15200/100000: episode: 152, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.460, mean reward: -0.195 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.971, 10.143], loss: 0.003645, mae: 0.061623, mean_q: -0.305711
 15300/100000: episode: 153, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -19.223, mean reward: -0.192 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.140, 10.247], loss: 0.003802, mae: 0.061784, mean_q: -0.314114
 15400/100000: episode: 154, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.886, mean reward: -0.179 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.457, 10.101], loss: 0.003568, mae: 0.060315, mean_q: -0.334322
 15500/100000: episode: 155, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.637, mean reward: -0.186 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.012, 10.098], loss: 0.003795, mae: 0.063308, mean_q: -0.309951
 15600/100000: episode: 156, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.771, mean reward: -0.168 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.641, 10.098], loss: 0.003516, mae: 0.060041, mean_q: -0.323207
 15700/100000: episode: 157, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.337, mean reward: -0.183 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.100, 10.109], loss: 0.003180, mae: 0.057584, mean_q: -0.341549
 15800/100000: episode: 158, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -14.217, mean reward: -0.142 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.847, 10.140], loss: 0.003783, mae: 0.061488, mean_q: -0.317295
 15900/100000: episode: 159, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.526, mean reward: -0.185 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.558, 10.151], loss: 0.003903, mae: 0.062654, mean_q: -0.302398
 16000/100000: episode: 160, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.118, mean reward: -0.181 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.788, 10.098], loss: 0.003642, mae: 0.061898, mean_q: -0.333654
 16100/100000: episode: 161, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -20.093, mean reward: -0.201 [-1.000, 0.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.735, 10.231], loss: 0.003333, mae: 0.056972, mean_q: -0.319434
 16200/100000: episode: 162, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -12.911, mean reward: -0.129 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.232, 10.098], loss: 0.003843, mae: 0.062776, mean_q: -0.305392
 16300/100000: episode: 163, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.595, mean reward: -0.186 [-1.000, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.709, 10.187], loss: 0.003531, mae: 0.059957, mean_q: -0.288921
 16400/100000: episode: 164, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.024, mean reward: -0.170 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.524, 10.433], loss: 0.003411, mae: 0.060235, mean_q: -0.299002
 16500/100000: episode: 165, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.904, mean reward: -0.179 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.959, 10.129], loss: 0.003246, mae: 0.057855, mean_q: -0.311430
 16600/100000: episode: 166, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.206, mean reward: -0.182 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.890, 10.170], loss: 0.003657, mae: 0.061730, mean_q: -0.318975
 16700/100000: episode: 167, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -14.498, mean reward: -0.145 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.226, 10.370], loss: 0.003662, mae: 0.061878, mean_q: -0.328780
 16800/100000: episode: 168, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -13.303, mean reward: -0.133 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.732, 10.519], loss: 0.003391, mae: 0.059245, mean_q: -0.337339
 16900/100000: episode: 169, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.208, mean reward: -0.152 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.708, 10.134], loss: 0.004971, mae: 0.067080, mean_q: -0.329780
 17000/100000: episode: 170, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.838, mean reward: -0.178 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.246, 10.098], loss: 0.003101, mae: 0.057275, mean_q: -0.288787
 17100/100000: episode: 171, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.395, mean reward: -0.144 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.013, 10.237], loss: 0.003118, mae: 0.056827, mean_q: -0.302080
 17200/100000: episode: 172, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -12.425, mean reward: -0.124 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.400, 10.098], loss: 0.003555, mae: 0.061178, mean_q: -0.293736
 17300/100000: episode: 173, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.455, mean reward: -0.185 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.443, 10.299], loss: 0.003204, mae: 0.058427, mean_q: -0.322506
 17400/100000: episode: 174, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.220, mean reward: -0.182 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.315, 10.132], loss: 0.003138, mae: 0.056212, mean_q: -0.333806
 17500/100000: episode: 175, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.683, mean reward: -0.177 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.451, 10.098], loss: 0.003231, mae: 0.058208, mean_q: -0.302373
 17600/100000: episode: 176, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.550, mean reward: -0.185 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.601, 10.153], loss: 0.003690, mae: 0.061720, mean_q: -0.303781
 17700/100000: episode: 177, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -19.232, mean reward: -0.192 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.552, 10.206], loss: 0.004017, mae: 0.064261, mean_q: -0.291547
 17800/100000: episode: 178, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.123, mean reward: -0.171 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.748, 10.098], loss: 0.003788, mae: 0.060763, mean_q: -0.279933
 17900/100000: episode: 179, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.565, mean reward: -0.186 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.785, 10.156], loss: 0.003797, mae: 0.064414, mean_q: -0.327826
 18000/100000: episode: 180, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.249, mean reward: -0.172 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.391, 10.098], loss: 0.002988, mae: 0.055620, mean_q: -0.357432
 18100/100000: episode: 181, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.637, mean reward: -0.186 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.801, 10.098], loss: 0.003268, mae: 0.056509, mean_q: -0.342662
 18200/100000: episode: 182, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.561, mean reward: -0.196 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.676, 10.262], loss: 0.003086, mae: 0.056160, mean_q: -0.331410
 18300/100000: episode: 183, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.353, mean reward: -0.194 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.672, 10.328], loss: 0.003339, mae: 0.058327, mean_q: -0.313744
 18400/100000: episode: 184, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -19.889, mean reward: -0.199 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.239, 10.132], loss: 0.002975, mae: 0.055465, mean_q: -0.332236
 18500/100000: episode: 185, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -19.498, mean reward: -0.195 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.917, 10.170], loss: 0.003196, mae: 0.057586, mean_q: -0.346647
 18600/100000: episode: 186, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -12.560, mean reward: -0.126 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.387, 10.098], loss: 0.003071, mae: 0.057204, mean_q: -0.321286
 18700/100000: episode: 187, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.651, mean reward: -0.187 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.786, 10.247], loss: 0.003218, mae: 0.058778, mean_q: -0.330974
 18800/100000: episode: 188, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.217, mean reward: -0.162 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.770, 10.140], loss: 0.002972, mae: 0.054448, mean_q: -0.356803
 18900/100000: episode: 189, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.494, mean reward: -0.185 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.570, 10.277], loss: 0.003099, mae: 0.056692, mean_q: -0.338321
 19000/100000: episode: 190, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -19.044, mean reward: -0.190 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.941, 10.098], loss: 0.003068, mae: 0.055697, mean_q: -0.325995
 19100/100000: episode: 191, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.223, mean reward: -0.182 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.070, 10.098], loss: 0.004436, mae: 0.065302, mean_q: -0.306556
 19200/100000: episode: 192, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -7.571, mean reward: -0.076 [-1.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.691, 10.098], loss: 0.003791, mae: 0.061622, mean_q: -0.335111
 19300/100000: episode: 193, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -14.174, mean reward: -0.142 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.364, 10.255], loss: 0.003044, mae: 0.056012, mean_q: -0.303628
 19400/100000: episode: 194, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.924, mean reward: -0.159 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.296, 10.098], loss: 0.002945, mae: 0.055225, mean_q: -0.278165
 19500/100000: episode: 195, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.862, mean reward: -0.179 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.985, 10.238], loss: 0.003183, mae: 0.057317, mean_q: -0.377468
 19600/100000: episode: 196, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.722, mean reward: -0.187 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.855, 10.098], loss: 0.003077, mae: 0.056843, mean_q: -0.305487
 19700/100000: episode: 197, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.132, mean reward: -0.181 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.306, 10.220], loss: 0.003010, mae: 0.055418, mean_q: -0.363192
 19800/100000: episode: 198, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.132, mean reward: -0.161 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.434, 10.260], loss: 0.003168, mae: 0.057492, mean_q: -0.337776
 19900/100000: episode: 199, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.864, mean reward: -0.179 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.362, 10.130], loss: 0.003120, mae: 0.056447, mean_q: -0.334223
 20000/100000: episode: 200, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -13.088, mean reward: -0.131 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.974, 10.132], loss: 0.003317, mae: 0.056912, mean_q: -0.328245
 20100/100000: episode: 201, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.219, mean reward: -0.192 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.493, 10.104], loss: 0.003011, mae: 0.057193, mean_q: -0.310592
 20200/100000: episode: 202, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.144, mean reward: -0.191 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.200, 10.098], loss: 0.002779, mae: 0.053209, mean_q: -0.311105
 20300/100000: episode: 203, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.794, mean reward: -0.188 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.781, 10.098], loss: 0.003528, mae: 0.060185, mean_q: -0.330388
 20400/100000: episode: 204, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.287, mean reward: -0.183 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.257, 10.110], loss: 0.003014, mae: 0.057010, mean_q: -0.329932
 20500/100000: episode: 205, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.481, mean reward: -0.185 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.954, 10.249], loss: 0.003210, mae: 0.057428, mean_q: -0.327222
 20600/100000: episode: 206, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.710, mean reward: -0.177 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.862, 10.098], loss: 0.002572, mae: 0.051144, mean_q: -0.332297
 20700/100000: episode: 207, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.786, mean reward: -0.168 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.558, 10.098], loss: 0.003714, mae: 0.060413, mean_q: -0.331344
 20800/100000: episode: 208, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.368, mean reward: -0.154 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.561, 10.098], loss: 0.003044, mae: 0.056567, mean_q: -0.317936
 20900/100000: episode: 209, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.876, mean reward: -0.199 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.002, 10.098], loss: 0.002794, mae: 0.052623, mean_q: -0.336079
 21000/100000: episode: 210, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -21.669, mean reward: -0.217 [-1.000, 0.243], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.939, 10.098], loss: 0.002754, mae: 0.053537, mean_q: -0.329248
 21100/100000: episode: 211, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.346, mean reward: -0.173 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.171, 10.272], loss: 0.002806, mae: 0.052776, mean_q: -0.319939
 21200/100000: episode: 212, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.268, mean reward: -0.173 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.929, 10.098], loss: 0.003224, mae: 0.058398, mean_q: -0.349488
 21300/100000: episode: 213, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -20.009, mean reward: -0.200 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.408, 10.109], loss: 0.003022, mae: 0.055877, mean_q: -0.318990
 21400/100000: episode: 214, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.941, mean reward: -0.169 [-1.000, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.686, 10.098], loss: 0.003182, mae: 0.057304, mean_q: -0.310157
 21500/100000: episode: 215, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -13.196, mean reward: -0.132 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.561, 10.423], loss: 0.003763, mae: 0.062662, mean_q: -0.284562
 21600/100000: episode: 216, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.390, mean reward: -0.164 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.668, 10.358], loss: 0.003009, mae: 0.054986, mean_q: -0.370382
 21700/100000: episode: 217, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -20.927, mean reward: -0.209 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.102, 10.278], loss: 0.002847, mae: 0.054292, mean_q: -0.322928
 21800/100000: episode: 218, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.781, mean reward: -0.178 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.268, 10.102], loss: 0.003051, mae: 0.055299, mean_q: -0.315256
 21900/100000: episode: 219, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.217, mean reward: -0.152 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.607, 10.098], loss: 0.003166, mae: 0.058918, mean_q: -0.303070
 22000/100000: episode: 220, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.755, mean reward: -0.198 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.676, 10.110], loss: 0.003197, mae: 0.057381, mean_q: -0.307343
 22100/100000: episode: 221, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.708, mean reward: -0.167 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.445, 10.098], loss: 0.003314, mae: 0.057520, mean_q: -0.312022
 22200/100000: episode: 222, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -5.998, mean reward: -0.060 [-1.000, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.556, 10.098], loss: 0.003135, mae: 0.057017, mean_q: -0.304214
 22300/100000: episode: 223, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -20.725, mean reward: -0.207 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.283, 10.162], loss: 0.002792, mae: 0.054347, mean_q: -0.298814
 22400/100000: episode: 224, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -13.666, mean reward: -0.137 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.663, 10.356], loss: 0.002693, mae: 0.051892, mean_q: -0.332843
 22500/100000: episode: 225, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.496, mean reward: -0.155 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.864, 10.098], loss: 0.003770, mae: 0.057888, mean_q: -0.364443
 22600/100000: episode: 226, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.358, mean reward: -0.184 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.646, 10.248], loss: 0.003218, mae: 0.058777, mean_q: -0.322497
 22700/100000: episode: 227, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -16.134, mean reward: -0.161 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.110, 10.123], loss: 0.003239, mae: 0.056974, mean_q: -0.332771
 22800/100000: episode: 228, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.743, mean reward: -0.177 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.975, 10.281], loss: 0.002739, mae: 0.052316, mean_q: -0.337379
 22900/100000: episode: 229, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -13.376, mean reward: -0.134 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.362, 10.098], loss: 0.002976, mae: 0.055559, mean_q: -0.340497
 23000/100000: episode: 230, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.010, mean reward: -0.170 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.286, 10.384], loss: 0.002946, mae: 0.055254, mean_q: -0.333024
 23100/100000: episode: 231, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -20.325, mean reward: -0.203 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.609, 10.289], loss: 0.003338, mae: 0.059020, mean_q: -0.329216
 23200/100000: episode: 232, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -12.472, mean reward: -0.125 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.281, 10.483], loss: 0.002997, mae: 0.054340, mean_q: -0.314839
 23300/100000: episode: 233, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -12.908, mean reward: -0.129 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.744, 10.346], loss: 0.003250, mae: 0.056819, mean_q: -0.302442
 23400/100000: episode: 234, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -8.902, mean reward: -0.089 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.729, 10.098], loss: 0.005242, mae: 0.072180, mean_q: -0.319116
 23500/100000: episode: 235, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.323, mean reward: -0.163 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.908, 10.201], loss: 0.002817, mae: 0.053848, mean_q: -0.333079
 23600/100000: episode: 236, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.761, mean reward: -0.178 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.487, 10.125], loss: 0.003015, mae: 0.055370, mean_q: -0.291276
 23700/100000: episode: 237, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.208, mean reward: -0.142 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.642, 10.098], loss: 0.002930, mae: 0.054870, mean_q: -0.300726
 23800/100000: episode: 238, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -10.334, mean reward: -0.103 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.709, 10.098], loss: 0.003258, mae: 0.058528, mean_q: -0.317907
 23900/100000: episode: 239, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.968, mean reward: -0.180 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.318, 10.346], loss: 0.002983, mae: 0.055876, mean_q: -0.258304
 24000/100000: episode: 240, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -20.109, mean reward: -0.201 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.690, 10.155], loss: 0.002764, mae: 0.052859, mean_q: -0.328645
 24100/100000: episode: 241, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.476, mean reward: -0.175 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.324, 10.098], loss: 0.003168, mae: 0.057528, mean_q: -0.315626
 24200/100000: episode: 242, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.448, mean reward: -0.154 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.409, 10.339], loss: 0.002958, mae: 0.054319, mean_q: -0.284089
 24300/100000: episode: 243, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -17.020, mean reward: -0.170 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.488, 10.098], loss: 0.002930, mae: 0.055133, mean_q: -0.276405
 24400/100000: episode: 244, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.770, mean reward: -0.168 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.915, 10.120], loss: 0.003778, mae: 0.061352, mean_q: -0.303677
 24500/100000: episode: 245, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.508, mean reward: -0.195 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.941, 10.098], loss: 0.003160, mae: 0.056823, mean_q: -0.327446
 24600/100000: episode: 246, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -20.217, mean reward: -0.202 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.803, 10.308], loss: 0.002710, mae: 0.051970, mean_q: -0.309316
 24700/100000: episode: 247, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.481, mean reward: -0.185 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.617, 10.098], loss: 0.002576, mae: 0.051193, mean_q: -0.348475
 24800/100000: episode: 248, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.740, mean reward: -0.167 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.942, 10.151], loss: 0.003351, mae: 0.058363, mean_q: -0.321471
 24900/100000: episode: 249, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.619, mean reward: -0.176 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.051, 10.179], loss: 0.003009, mae: 0.056063, mean_q: -0.280839
 25000/100000: episode: 250, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.133, mean reward: -0.181 [-1.000, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.080, 10.127], loss: 0.003044, mae: 0.055431, mean_q: -0.328260
 25100/100000: episode: 251, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -13.988, mean reward: -0.140 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.206, 10.098], loss: 0.002825, mae: 0.054284, mean_q: -0.293893
 25200/100000: episode: 252, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.999, mean reward: -0.180 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.800, 10.236], loss: 0.003135, mae: 0.057889, mean_q: -0.313102
 25300/100000: episode: 253, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.380, mean reward: -0.144 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.277, 10.098], loss: 0.002672, mae: 0.052226, mean_q: -0.340229
 25400/100000: episode: 254, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.956, mean reward: -0.160 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.086, 10.301], loss: 0.002882, mae: 0.054035, mean_q: -0.327705
 25500/100000: episode: 255, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.929, mean reward: -0.159 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.444, 10.112], loss: 0.002770, mae: 0.052398, mean_q: -0.301421
 25600/100000: episode: 256, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.678, mean reward: -0.187 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.795, 10.162], loss: 0.002676, mae: 0.051447, mean_q: -0.342132
 25700/100000: episode: 257, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.655, mean reward: -0.187 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.191, 10.098], loss: 0.003563, mae: 0.060217, mean_q: -0.291185
 25800/100000: episode: 258, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.796, mean reward: -0.168 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.818, 10.098], loss: 0.002714, mae: 0.053254, mean_q: -0.286249
 25900/100000: episode: 259, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -20.359, mean reward: -0.204 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.720, 10.116], loss: 0.004417, mae: 0.061872, mean_q: -0.321265
 26000/100000: episode: 260, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -12.898, mean reward: -0.129 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.535, 10.098], loss: 0.002692, mae: 0.052770, mean_q: -0.314213
 26100/100000: episode: 261, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.541, mean reward: -0.165 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.014, 10.325], loss: 0.002839, mae: 0.053191, mean_q: -0.327905
 26200/100000: episode: 262, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.937, mean reward: -0.179 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.990, 10.225], loss: 0.002894, mae: 0.053933, mean_q: -0.296955
 26300/100000: episode: 263, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.772, mean reward: -0.158 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.879, 10.158], loss: 0.002896, mae: 0.054041, mean_q: -0.322393
 26400/100000: episode: 264, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.320, mean reward: -0.173 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.755, 10.098], loss: 0.003322, mae: 0.058719, mean_q: -0.297389
 26500/100000: episode: 265, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -13.939, mean reward: -0.139 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.165, 10.431], loss: 0.002544, mae: 0.050314, mean_q: -0.334238
 26600/100000: episode: 266, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.580, mean reward: -0.176 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.912, 10.098], loss: 0.002539, mae: 0.051017, mean_q: -0.315590
 26700/100000: episode: 267, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -12.415, mean reward: -0.124 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.885, 10.115], loss: 0.002823, mae: 0.053628, mean_q: -0.301999
 26800/100000: episode: 268, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.207, mean reward: -0.172 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.600, 10.098], loss: 0.002523, mae: 0.050052, mean_q: -0.324738
 26900/100000: episode: 269, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -14.990, mean reward: -0.150 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.584, 10.098], loss: 0.002759, mae: 0.054309, mean_q: -0.287726
 27000/100000: episode: 270, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.920, mean reward: -0.149 [-1.000, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.224, 10.439], loss: 0.002540, mae: 0.050932, mean_q: -0.291837
 27100/100000: episode: 271, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.625, mean reward: -0.176 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.516, 10.098], loss: 0.002776, mae: 0.052281, mean_q: -0.305647
 27200/100000: episode: 272, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.831, mean reward: -0.188 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.481, 10.098], loss: 0.002490, mae: 0.050194, mean_q: -0.312652
 27300/100000: episode: 273, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -15.335, mean reward: -0.153 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.688, 10.098], loss: 0.003508, mae: 0.056334, mean_q: -0.303278
 27400/100000: episode: 274, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -14.675, mean reward: -0.147 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.231, 10.223], loss: 0.003847, mae: 0.060353, mean_q: -0.313796
 27500/100000: episode: 275, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -15.722, mean reward: -0.157 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.745, 10.098], loss: 0.002842, mae: 0.053835, mean_q: -0.275050
 27600/100000: episode: 276, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -9.715, mean reward: -0.097 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.073, 10.098], loss: 0.002803, mae: 0.053622, mean_q: -0.301787
 27700/100000: episode: 277, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.961, mean reward: -0.170 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.882, 10.098], loss: 0.002774, mae: 0.052917, mean_q: -0.285481
 27800/100000: episode: 278, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.147, mean reward: -0.181 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.365, 10.126], loss: 0.005512, mae: 0.069341, mean_q: -0.293663
 27900/100000: episode: 279, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.278, mean reward: -0.143 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.253, 10.229], loss: 0.002847, mae: 0.054637, mean_q: -0.273673
 28000/100000: episode: 280, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -17.031, mean reward: -0.170 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.560, 10.215], loss: 0.002685, mae: 0.051661, mean_q: -0.287707
 28100/100000: episode: 281, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.921, mean reward: -0.189 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.727, 10.098], loss: 0.002608, mae: 0.051086, mean_q: -0.309282
 28200/100000: episode: 282, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -13.511, mean reward: -0.135 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.699, 10.098], loss: 0.003008, mae: 0.054916, mean_q: -0.303978
 28300/100000: episode: 283, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.809, mean reward: -0.168 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.875, 10.189], loss: 0.002887, mae: 0.056013, mean_q: -0.299925
 28400/100000: episode: 284, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.334, mean reward: -0.193 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.193, 10.098], loss: 0.002663, mae: 0.052604, mean_q: -0.285004
 28500/100000: episode: 285, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -14.747, mean reward: -0.147 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.594, 10.098], loss: 0.002535, mae: 0.050793, mean_q: -0.304469
 28600/100000: episode: 286, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.932, mean reward: -0.199 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.735, 10.098], loss: 0.002674, mae: 0.051879, mean_q: -0.335939
 28700/100000: episode: 287, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -13.926, mean reward: -0.139 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.159, 10.098], loss: 0.002640, mae: 0.051569, mean_q: -0.311528
 28800/100000: episode: 288, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -14.688, mean reward: -0.147 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.512, 10.098], loss: 0.002678, mae: 0.051924, mean_q: -0.273197
 28900/100000: episode: 289, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -13.994, mean reward: -0.140 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.546, 10.339], loss: 0.002750, mae: 0.054397, mean_q: -0.313274
 29000/100000: episode: 290, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.922, mean reward: -0.169 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.222, 10.098], loss: 0.003197, mae: 0.057424, mean_q: -0.257819
 29100/100000: episode: 291, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -20.033, mean reward: -0.200 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.574, 10.161], loss: 0.002496, mae: 0.051170, mean_q: -0.325134
 29200/100000: episode: 292, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -15.751, mean reward: -0.158 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.694, 10.220], loss: 0.002727, mae: 0.053056, mean_q: -0.303475
 29300/100000: episode: 293, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.580, mean reward: -0.146 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.917, 10.098], loss: 0.002488, mae: 0.050698, mean_q: -0.306207
 29400/100000: episode: 294, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.920, mean reward: -0.139 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.379, 10.098], loss: 0.002841, mae: 0.053599, mean_q: -0.342277
 29500/100000: episode: 295, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.233, mean reward: -0.182 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.169, 10.098], loss: 0.003269, mae: 0.058328, mean_q: -0.303166
 29600/100000: episode: 296, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.996, mean reward: -0.190 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.262, 10.176], loss: 0.002863, mae: 0.055335, mean_q: -0.307973
 29700/100000: episode: 297, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.730, mean reward: -0.197 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.338, 10.098], loss: 0.002539, mae: 0.050843, mean_q: -0.349809
 29800/100000: episode: 298, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -15.770, mean reward: -0.158 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.804, 10.254], loss: 0.002620, mae: 0.051456, mean_q: -0.336067
 29900/100000: episode: 299, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.801, mean reward: -0.168 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.427, 10.098], loss: 0.002830, mae: 0.052844, mean_q: -0.336134
 30000/100000: episode: 300, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.148, mean reward: -0.191 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.213, 10.164], loss: 0.002831, mae: 0.053595, mean_q: -0.309300
 30100/100000: episode: 301, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.945, mean reward: -0.189 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.343, 10.098], loss: 0.002695, mae: 0.052930, mean_q: -0.346491
 30200/100000: episode: 302, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.522, mean reward: -0.185 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.480, 10.348], loss: 0.002789, mae: 0.053755, mean_q: -0.288280
 30300/100000: episode: 303, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -20.657, mean reward: -0.207 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.748, 10.122], loss: 0.002600, mae: 0.051606, mean_q: -0.294751
 30400/100000: episode: 304, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -18.424, mean reward: -0.184 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.400, 10.098], loss: 0.002743, mae: 0.052837, mean_q: -0.337961
 30500/100000: episode: 305, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.741, mean reward: -0.137 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.745, 10.098], loss: 0.002535, mae: 0.050592, mean_q: -0.329727
 30600/100000: episode: 306, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.366, mean reward: -0.174 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.805, 10.098], loss: 0.002611, mae: 0.050506, mean_q: -0.311052
 30700/100000: episode: 307, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.172, mean reward: -0.182 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.995, 10.247], loss: 0.002742, mae: 0.052558, mean_q: -0.324571
 30800/100000: episode: 308, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.838, mean reward: -0.148 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.856, 10.109], loss: 0.002500, mae: 0.050703, mean_q: -0.323713
 30900/100000: episode: 309, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.234, mean reward: -0.172 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.851, 10.174], loss: 0.002599, mae: 0.051245, mean_q: -0.322012
 31000/100000: episode: 310, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.311, mean reward: -0.173 [-1.000, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.329, 10.098], loss: 0.002821, mae: 0.054145, mean_q: -0.297563
 31100/100000: episode: 311, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.514, mean reward: -0.185 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.779, 10.148], loss: 0.002868, mae: 0.054103, mean_q: -0.321026
 31200/100000: episode: 312, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.160, mean reward: -0.162 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.538, 10.098], loss: 0.002655, mae: 0.051514, mean_q: -0.318623
 31300/100000: episode: 313, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -17.583, mean reward: -0.176 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.001, 10.098], loss: 0.002643, mae: 0.052311, mean_q: -0.314257
 31400/100000: episode: 314, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.062, mean reward: -0.181 [-1.000, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.765, 10.272], loss: 0.002555, mae: 0.049822, mean_q: -0.301586
 31500/100000: episode: 315, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.831, mean reward: -0.178 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.788, 10.098], loss: 0.002958, mae: 0.054428, mean_q: -0.289553
 31600/100000: episode: 316, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.345, mean reward: -0.183 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.816, 10.256], loss: 0.002779, mae: 0.053305, mean_q: -0.289214
 31700/100000: episode: 317, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.302, mean reward: -0.173 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.343, 10.098], loss: 0.002678, mae: 0.050997, mean_q: -0.308918
 31800/100000: episode: 318, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.601, mean reward: -0.186 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.863, 10.289], loss: 0.002431, mae: 0.050202, mean_q: -0.313481
 31900/100000: episode: 319, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -13.813, mean reward: -0.138 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.649, 10.098], loss: 0.002873, mae: 0.054062, mean_q: -0.350266
 32000/100000: episode: 320, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.743, mean reward: -0.177 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.922, 10.098], loss: 0.002652, mae: 0.051964, mean_q: -0.307969
 32100/100000: episode: 321, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -20.526, mean reward: -0.205 [-1.000, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.269, 10.098], loss: 0.002921, mae: 0.055698, mean_q: -0.348627
 32200/100000: episode: 322, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.713, mean reward: -0.187 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.507, 10.098], loss: 0.002980, mae: 0.056888, mean_q: -0.324432
 32300/100000: episode: 323, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.498, mean reward: -0.165 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.435, 10.339], loss: 0.002615, mae: 0.051589, mean_q: -0.291414
 32400/100000: episode: 324, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.592, mean reward: -0.176 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.285, 10.341], loss: 0.002579, mae: 0.050644, mean_q: -0.357818
 32500/100000: episode: 325, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.336, mean reward: -0.173 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.741, 10.098], loss: 0.002624, mae: 0.052148, mean_q: -0.314866
 32600/100000: episode: 326, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.118, mean reward: -0.171 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.824, 10.098], loss: 0.002689, mae: 0.051940, mean_q: -0.327610
 32700/100000: episode: 327, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.962, mean reward: -0.170 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.404, 10.098], loss: 0.002600, mae: 0.049868, mean_q: -0.334662
 32800/100000: episode: 328, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.893, mean reward: -0.149 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.853, 10.284], loss: 0.002699, mae: 0.052207, mean_q: -0.309852
 32900/100000: episode: 329, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.412, mean reward: -0.164 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.770, 10.260], loss: 0.002725, mae: 0.052596, mean_q: -0.304528
 33000/100000: episode: 330, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.116, mean reward: -0.191 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.264, 10.220], loss: 0.002636, mae: 0.051843, mean_q: -0.299415
 33100/100000: episode: 331, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -20.222, mean reward: -0.202 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.019, 10.218], loss: 0.002500, mae: 0.049267, mean_q: -0.312146
 33200/100000: episode: 332, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.831, mean reward: -0.178 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.645, 10.118], loss: 0.002623, mae: 0.050361, mean_q: -0.365732
 33300/100000: episode: 333, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.391, mean reward: -0.154 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.780, 10.098], loss: 0.002577, mae: 0.051446, mean_q: -0.310744
 33400/100000: episode: 334, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.053, mean reward: -0.171 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.422, 10.266], loss: 0.002798, mae: 0.053759, mean_q: -0.322594
 33500/100000: episode: 335, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.915, mean reward: -0.169 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.543, 10.098], loss: 0.003908, mae: 0.059080, mean_q: -0.340212
 33600/100000: episode: 336, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -12.162, mean reward: -0.122 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.346, 10.098], loss: 0.004137, mae: 0.067589, mean_q: -0.282184
 33700/100000: episode: 337, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -20.687, mean reward: -0.207 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.625, 10.098], loss: 0.002597, mae: 0.051748, mean_q: -0.322278
 33800/100000: episode: 338, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -14.571, mean reward: -0.146 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.538, 10.209], loss: 0.002445, mae: 0.050097, mean_q: -0.315433
 33900/100000: episode: 339, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.060, mean reward: -0.171 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.173, 10.316], loss: 0.002758, mae: 0.052593, mean_q: -0.342012
 34000/100000: episode: 340, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.816, mean reward: -0.188 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.815, 10.098], loss: 0.002679, mae: 0.051604, mean_q: -0.330849
 34100/100000: episode: 341, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.501, mean reward: -0.145 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.131, 10.098], loss: 0.002744, mae: 0.052728, mean_q: -0.317780
 34200/100000: episode: 342, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.178, mean reward: -0.172 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.090, 10.098], loss: 0.002757, mae: 0.052240, mean_q: -0.301240
 34300/100000: episode: 343, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -19.448, mean reward: -0.194 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.237, 10.218], loss: 0.002424, mae: 0.049219, mean_q: -0.325941
 34400/100000: episode: 344, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.495, mean reward: -0.175 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.997, 10.099], loss: 0.002877, mae: 0.053954, mean_q: -0.315294
 34500/100000: episode: 345, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -21.111, mean reward: -0.211 [-1.000, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.664, 10.098], loss: 0.002509, mae: 0.050359, mean_q: -0.326507
 34600/100000: episode: 346, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.998, mean reward: -0.160 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.504, 10.170], loss: 0.002497, mae: 0.050516, mean_q: -0.333306
 34700/100000: episode: 347, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.268, mean reward: -0.153 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.222, 10.110], loss: 0.002753, mae: 0.052189, mean_q: -0.338966
 34800/100000: episode: 348, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.216, mean reward: -0.162 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.901, 10.098], loss: 0.002720, mae: 0.051800, mean_q: -0.309938
 34900/100000: episode: 349, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.556, mean reward: -0.166 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.136, 10.098], loss: 0.002606, mae: 0.050811, mean_q: -0.320288
 35000/100000: episode: 350, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -14.989, mean reward: -0.150 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.305, 10.238], loss: 0.002629, mae: 0.051591, mean_q: -0.332722
 35100/100000: episode: 351, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.466, mean reward: -0.145 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.539, 10.098], loss: 0.002926, mae: 0.054695, mean_q: -0.321308
 35200/100000: episode: 352, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.595, mean reward: -0.176 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.194, 10.098], loss: 0.002763, mae: 0.052937, mean_q: -0.304058
 35300/100000: episode: 353, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.224, mean reward: -0.182 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.684, 10.098], loss: 0.002568, mae: 0.051170, mean_q: -0.345161
 35400/100000: episode: 354, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.038, mean reward: -0.170 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.268, 10.098], loss: 0.002750, mae: 0.053148, mean_q: -0.365446
 35500/100000: episode: 355, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -20.070, mean reward: -0.201 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.685, 10.098], loss: 0.002551, mae: 0.050837, mean_q: -0.336060
 35600/100000: episode: 356, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.860, mean reward: -0.169 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.684, 10.171], loss: 0.002471, mae: 0.049449, mean_q: -0.331026
 35700/100000: episode: 357, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -13.973, mean reward: -0.140 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.569, 10.098], loss: 0.002612, mae: 0.050580, mean_q: -0.297300
 35800/100000: episode: 358, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.307, mean reward: -0.173 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.983, 10.098], loss: 0.002688, mae: 0.052167, mean_q: -0.311230
 35900/100000: episode: 359, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.683, mean reward: -0.157 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.255, 10.220], loss: 0.002435, mae: 0.048730, mean_q: -0.382046
 36000/100000: episode: 360, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -14.094, mean reward: -0.141 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.800, 10.376], loss: 0.002612, mae: 0.052091, mean_q: -0.327854
 36100/100000: episode: 361, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.212, mean reward: -0.172 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.678, 10.325], loss: 0.005319, mae: 0.072166, mean_q: -0.361092
 36200/100000: episode: 362, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -20.205, mean reward: -0.202 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.464, 10.154], loss: 0.003128, mae: 0.058603, mean_q: -0.315746
 36300/100000: episode: 363, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.779, mean reward: -0.148 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.045, 10.098], loss: 0.002567, mae: 0.051476, mean_q: -0.341439
 36400/100000: episode: 364, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.294, mean reward: -0.183 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.734, 10.098], loss: 0.002549, mae: 0.050204, mean_q: -0.318848
 36500/100000: episode: 365, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.371, mean reward: -0.184 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.482, 10.098], loss: 0.002566, mae: 0.050221, mean_q: -0.324308
 36600/100000: episode: 366, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -20.352, mean reward: -0.204 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.844, 10.136], loss: 0.002674, mae: 0.051995, mean_q: -0.282316
 36700/100000: episode: 367, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.659, mean reward: -0.197 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.496, 10.098], loss: 0.002768, mae: 0.052940, mean_q: -0.334431
 36800/100000: episode: 368, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.542, mean reward: -0.155 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.522, 10.260], loss: 0.002531, mae: 0.050213, mean_q: -0.329761
 36900/100000: episode: 369, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -14.227, mean reward: -0.142 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.161, 10.098], loss: 0.002553, mae: 0.050073, mean_q: -0.309540
 37000/100000: episode: 370, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -21.147, mean reward: -0.211 [-1.000, 0.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.231, 10.098], loss: 0.002535, mae: 0.050602, mean_q: -0.297323
 37100/100000: episode: 371, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -12.457, mean reward: -0.125 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.167, 10.441], loss: 0.002483, mae: 0.050027, mean_q: -0.347939
 37200/100000: episode: 372, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.562, mean reward: -0.176 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.849, 10.098], loss: 0.002477, mae: 0.050289, mean_q: -0.324532
 37300/100000: episode: 373, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.428, mean reward: -0.184 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.359, 10.098], loss: 0.002419, mae: 0.048797, mean_q: -0.328872
 37400/100000: episode: 374, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.839, mean reward: -0.148 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.403, 10.240], loss: 0.002454, mae: 0.049066, mean_q: -0.316671
 37500/100000: episode: 375, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.537, mean reward: -0.185 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.714, 10.108], loss: 0.002547, mae: 0.050036, mean_q: -0.343876
 37600/100000: episode: 376, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -11.282, mean reward: -0.113 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.987, 10.098], loss: 0.002583, mae: 0.051669, mean_q: -0.317772
 37700/100000: episode: 377, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.421, mean reward: -0.184 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.625, 10.098], loss: 0.002467, mae: 0.050732, mean_q: -0.315790
 37800/100000: episode: 378, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.773, mean reward: -0.178 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.104, 10.098], loss: 0.002605, mae: 0.051234, mean_q: -0.307414
 37900/100000: episode: 379, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.975, mean reward: -0.180 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.494, 10.400], loss: 0.002634, mae: 0.053117, mean_q: -0.316676
 38000/100000: episode: 380, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -18.025, mean reward: -0.180 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.124, 10.098], loss: 0.002474, mae: 0.050128, mean_q: -0.323049
 38100/100000: episode: 381, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.584, mean reward: -0.166 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.091, 10.098], loss: 0.004302, mae: 0.062421, mean_q: -0.302328
 38200/100000: episode: 382, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.734, mean reward: -0.177 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.872, 10.098], loss: 0.002732, mae: 0.053392, mean_q: -0.324200
 38300/100000: episode: 383, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -13.614, mean reward: -0.136 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.059, 10.098], loss: 0.002547, mae: 0.049569, mean_q: -0.341874
 38400/100000: episode: 384, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.061, mean reward: -0.161 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.142, 10.187], loss: 0.002446, mae: 0.049776, mean_q: -0.287347
 38500/100000: episode: 385, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -13.600, mean reward: -0.136 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.249, 10.118], loss: 0.002543, mae: 0.050317, mean_q: -0.313178
 38600/100000: episode: 386, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -14.589, mean reward: -0.146 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.627, 10.214], loss: 0.002309, mae: 0.047791, mean_q: -0.346378
 38700/100000: episode: 387, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.461, mean reward: -0.155 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.453, 10.098], loss: 0.002454, mae: 0.052503, mean_q: -0.329610
 38800/100000: episode: 388, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -14.687, mean reward: -0.147 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.369, 10.358], loss: 0.002459, mae: 0.049523, mean_q: -0.318287
 38900/100000: episode: 389, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.701, mean reward: -0.177 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.756, 10.098], loss: 0.002455, mae: 0.049240, mean_q: -0.307635
 39000/100000: episode: 390, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -9.791, mean reward: -0.098 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.523, 10.098], loss: 0.002541, mae: 0.050584, mean_q: -0.322120
 39100/100000: episode: 391, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.111, mean reward: -0.171 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.599, 10.098], loss: 0.002380, mae: 0.048487, mean_q: -0.336471
 39200/100000: episode: 392, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.704, mean reward: -0.167 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.740, 10.260], loss: 0.002409, mae: 0.049282, mean_q: -0.311140
 39300/100000: episode: 393, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.961, mean reward: -0.180 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.851, 10.098], loss: 0.002557, mae: 0.049981, mean_q: -0.308602
 39400/100000: episode: 394, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.881, mean reward: -0.179 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.732, 10.098], loss: 0.002462, mae: 0.049831, mean_q: -0.295484
 39500/100000: episode: 395, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -13.972, mean reward: -0.140 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.419, 10.216], loss: 0.002522, mae: 0.049701, mean_q: -0.327307
 39600/100000: episode: 396, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.397, mean reward: -0.154 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.630, 10.438], loss: 0.002422, mae: 0.049237, mean_q: -0.315581
 39700/100000: episode: 397, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -17.464, mean reward: -0.175 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.748, 10.098], loss: 0.002422, mae: 0.049278, mean_q: -0.281904
 39800/100000: episode: 398, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -9.263, mean reward: -0.093 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.392, 10.263], loss: 0.004674, mae: 0.064099, mean_q: -0.320311
 39900/100000: episode: 399, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.794, mean reward: -0.198 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.973, 10.213], loss: 0.004871, mae: 0.068555, mean_q: -0.299218
 40000/100000: episode: 400, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.718, mean reward: -0.197 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.341, 10.124], loss: 0.002632, mae: 0.053364, mean_q: -0.325257
 40100/100000: episode: 401, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.532, mean reward: -0.175 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.256, 10.291], loss: 0.003155, mae: 0.056738, mean_q: -0.314821
 40200/100000: episode: 402, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.591, mean reward: -0.186 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.833, 10.189], loss: 0.002614, mae: 0.052062, mean_q: -0.329891
 40300/100000: episode: 403, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -13.864, mean reward: -0.139 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.530, 10.098], loss: 0.002598, mae: 0.051195, mean_q: -0.267803
 40400/100000: episode: 404, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.453, mean reward: -0.155 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.289, 10.245], loss: 0.002417, mae: 0.048984, mean_q: -0.348183
 40500/100000: episode: 405, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.619, mean reward: -0.186 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.423, 10.115], loss: 0.002628, mae: 0.051633, mean_q: -0.282511
 40600/100000: episode: 406, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -13.045, mean reward: -0.130 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.543, 10.266], loss: 0.002573, mae: 0.050616, mean_q: -0.312123
 40700/100000: episode: 407, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.167, mean reward: -0.162 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.305, 10.352], loss: 0.003001, mae: 0.057126, mean_q: -0.326483
 40800/100000: episode: 408, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.958, mean reward: -0.160 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.968, 10.098], loss: 0.002644, mae: 0.051522, mean_q: -0.301427
 40900/100000: episode: 409, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.923, mean reward: -0.179 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.632, 10.168], loss: 0.002453, mae: 0.049202, mean_q: -0.296765
 41000/100000: episode: 410, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.953, mean reward: -0.150 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.664, 10.098], loss: 0.002532, mae: 0.050266, mean_q: -0.322719
 41100/100000: episode: 411, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -20.345, mean reward: -0.203 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.059, 10.114], loss: 0.002571, mae: 0.050647, mean_q: -0.306133
 41200/100000: episode: 412, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.994, mean reward: -0.180 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.807, 10.478], loss: 0.002610, mae: 0.051521, mean_q: -0.313494
 41300/100000: episode: 413, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.594, mean reward: -0.146 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.819, 10.098], loss: 0.002487, mae: 0.049253, mean_q: -0.338313
 41400/100000: episode: 414, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.071, mean reward: -0.161 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.389, 10.201], loss: 0.002460, mae: 0.049263, mean_q: -0.310916
 41500/100000: episode: 415, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.942, mean reward: -0.159 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.962, 10.098], loss: 0.002384, mae: 0.048214, mean_q: -0.324611
 41600/100000: episode: 416, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.104, mean reward: -0.161 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.502, 10.098], loss: 0.002760, mae: 0.053055, mean_q: -0.286591
 41700/100000: episode: 417, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.980, mean reward: -0.180 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.743, 10.098], loss: 0.002612, mae: 0.051512, mean_q: -0.299675
 41800/100000: episode: 418, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.037, mean reward: -0.140 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.517, 10.349], loss: 0.002551, mae: 0.051128, mean_q: -0.313138
 41900/100000: episode: 419, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.868, mean reward: -0.149 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.570, 10.247], loss: 0.002488, mae: 0.049912, mean_q: -0.285558
 42000/100000: episode: 420, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.835, mean reward: -0.188 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.938, 10.268], loss: 0.003039, mae: 0.053632, mean_q: -0.289918
 42100/100000: episode: 421, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.459, mean reward: -0.185 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.532, 10.179], loss: 0.002642, mae: 0.052170, mean_q: -0.256755
 42200/100000: episode: 422, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.589, mean reward: -0.186 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.378, 10.098], loss: 0.002441, mae: 0.049047, mean_q: -0.299523
 42300/100000: episode: 423, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -12.928, mean reward: -0.129 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.576, 10.098], loss: 0.002526, mae: 0.049569, mean_q: -0.316746
 42400/100000: episode: 424, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.802, mean reward: -0.198 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.988, 10.098], loss: 0.002538, mae: 0.050123, mean_q: -0.303868
 42500/100000: episode: 425, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -12.070, mean reward: -0.121 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.198, 10.098], loss: 0.002433, mae: 0.048380, mean_q: -0.310735
 42600/100000: episode: 426, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.845, mean reward: -0.188 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.583, 10.156], loss: 0.002779, mae: 0.054320, mean_q: -0.318640
 42700/100000: episode: 427, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.732, mean reward: -0.187 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.471, 10.124], loss: 0.002517, mae: 0.051603, mean_q: -0.292302
 42800/100000: episode: 428, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -18.097, mean reward: -0.181 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.114, 10.098], loss: 0.002462, mae: 0.049934, mean_q: -0.310455
 42900/100000: episode: 429, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.225, mean reward: -0.182 [-1.000, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.047, 10.098], loss: 0.002488, mae: 0.049927, mean_q: -0.319646
 43000/100000: episode: 430, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.280, mean reward: -0.173 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.893, 10.098], loss: 0.002729, mae: 0.052069, mean_q: -0.312743
 43100/100000: episode: 431, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.294, mean reward: -0.143 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.852, 10.098], loss: 0.002472, mae: 0.050208, mean_q: -0.280174
 43200/100000: episode: 432, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.194, mean reward: -0.162 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.274, 10.261], loss: 0.002431, mae: 0.049773, mean_q: -0.299047
 43300/100000: episode: 433, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -13.433, mean reward: -0.134 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.827, 10.098], loss: 0.002280, mae: 0.046786, mean_q: -0.321070
 43400/100000: episode: 434, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -9.374, mean reward: -0.094 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.503, 10.098], loss: 0.002479, mae: 0.049445, mean_q: -0.339194
 43500/100000: episode: 435, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.394, mean reward: -0.174 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.335, 10.194], loss: 0.002453, mae: 0.049073, mean_q: -0.307970
 43600/100000: episode: 436, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.425, mean reward: -0.164 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.888, 10.098], loss: 0.002534, mae: 0.049801, mean_q: -0.315643
 43700/100000: episode: 437, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.221, mean reward: -0.192 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.060, 10.240], loss: 0.002527, mae: 0.049579, mean_q: -0.324372
 43800/100000: episode: 438, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -12.970, mean reward: -0.130 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.147, 10.098], loss: 0.006199, mae: 0.073452, mean_q: -0.305811
 43900/100000: episode: 439, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.355, mean reward: -0.194 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.790, 10.105], loss: 0.002497, mae: 0.050827, mean_q: -0.301803
 44000/100000: episode: 440, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -18.928, mean reward: -0.189 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.688, 10.176], loss: 0.002440, mae: 0.049748, mean_q: -0.295237
 44100/100000: episode: 441, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.573, mean reward: -0.176 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.989, 10.098], loss: 0.002452, mae: 0.049854, mean_q: -0.321428
 44200/100000: episode: 442, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -19.365, mean reward: -0.194 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.567, 10.106], loss: 0.002480, mae: 0.049853, mean_q: -0.327879
 44300/100000: episode: 443, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.222, mean reward: -0.182 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.606, 10.356], loss: 0.002335, mae: 0.049055, mean_q: -0.277306
 44400/100000: episode: 444, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -19.562, mean reward: -0.196 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.326, 10.098], loss: 0.002446, mae: 0.049640, mean_q: -0.317343
 44500/100000: episode: 445, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.982, mean reward: -0.180 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.633, 10.098], loss: 0.002444, mae: 0.049050, mean_q: -0.331174
 44600/100000: episode: 446, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.052, mean reward: -0.181 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.948, 10.098], loss: 0.002626, mae: 0.050790, mean_q: -0.291853
 44700/100000: episode: 447, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -9.962, mean reward: -0.100 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.793, 10.098], loss: 0.002265, mae: 0.047263, mean_q: -0.318344
 44800/100000: episode: 448, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.810, mean reward: -0.168 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.667, 10.334], loss: 0.002306, mae: 0.048177, mean_q: -0.321183
 44900/100000: episode: 449, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.608, mean reward: -0.186 [-1.000, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.536, 10.114], loss: 0.002350, mae: 0.048105, mean_q: -0.346081
 45000/100000: episode: 450, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.973, mean reward: -0.180 [-1.000, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.757, 10.098], loss: 0.002242, mae: 0.047984, mean_q: -0.314805
 45100/100000: episode: 451, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.807, mean reward: -0.158 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.020, 10.226], loss: 0.002176, mae: 0.046032, mean_q: -0.322135
 45200/100000: episode: 452, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.932, mean reward: -0.189 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.827, 10.098], loss: 0.002235, mae: 0.046817, mean_q: -0.289670
 45300/100000: episode: 453, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.727, mean reward: -0.177 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.753, 10.345], loss: 0.002620, mae: 0.050564, mean_q: -0.307173
 45400/100000: episode: 454, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.598, mean reward: -0.166 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.191, 10.151], loss: 0.002414, mae: 0.049350, mean_q: -0.306784
 45500/100000: episode: 455, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.507, mean reward: -0.175 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.532, 10.358], loss: 0.002146, mae: 0.047586, mean_q: -0.332232
 45600/100000: episode: 456, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.724, mean reward: -0.157 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.051, 10.098], loss: 0.002248, mae: 0.048154, mean_q: -0.353877
 45700/100000: episode: 457, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.795, mean reward: -0.188 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.869, 10.187], loss: 0.002179, mae: 0.046178, mean_q: -0.328763
 45800/100000: episode: 458, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -9.751, mean reward: -0.098 [-1.000, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.526, 10.098], loss: 0.002764, mae: 0.052941, mean_q: -0.294332
 45900/100000: episode: 459, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -16.867, mean reward: -0.169 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.480, 10.285], loss: 0.002307, mae: 0.049195, mean_q: -0.317331
 46000/100000: episode: 460, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -9.959, mean reward: -0.100 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.525, 10.098], loss: 0.002174, mae: 0.046815, mean_q: -0.347581
 46100/100000: episode: 461, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -14.482, mean reward: -0.145 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.252, 10.098], loss: 0.002138, mae: 0.046071, mean_q: -0.326682
 46200/100000: episode: 462, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.345, mean reward: -0.163 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.874, 10.199], loss: 0.002314, mae: 0.047496, mean_q: -0.339878
 46300/100000: episode: 463, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.017, mean reward: -0.190 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.220, 10.108], loss: 0.002383, mae: 0.047944, mean_q: -0.333934
 46400/100000: episode: 464, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.067, mean reward: -0.171 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.619, 10.127], loss: 0.002430, mae: 0.049747, mean_q: -0.294196
 46500/100000: episode: 465, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.007, mean reward: -0.180 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.257, 10.187], loss: 0.004343, mae: 0.064593, mean_q: -0.299713
 46600/100000: episode: 466, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.811, mean reward: -0.188 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.469, 10.113], loss: 0.002432, mae: 0.050161, mean_q: -0.318103
 46700/100000: episode: 467, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.298, mean reward: -0.173 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.720, 10.098], loss: 0.002304, mae: 0.048463, mean_q: -0.332722
 46800/100000: episode: 468, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.391, mean reward: -0.164 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.539, 10.417], loss: 0.002491, mae: 0.049939, mean_q: -0.328680
 46900/100000: episode: 469, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.138, mean reward: -0.181 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.909, 10.226], loss: 0.002310, mae: 0.048027, mean_q: -0.306592
 47000/100000: episode: 470, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.188, mean reward: -0.182 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.040, 10.098], loss: 0.002455, mae: 0.050017, mean_q: -0.331152
 47100/100000: episode: 471, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -20.044, mean reward: -0.200 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.972, 10.098], loss: 0.002281, mae: 0.047516, mean_q: -0.274301
 47200/100000: episode: 472, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.907, mean reward: -0.179 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.544, 10.098], loss: 0.002329, mae: 0.047742, mean_q: -0.318800
 47300/100000: episode: 473, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -20.719, mean reward: -0.207 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.249, 10.137], loss: 0.002290, mae: 0.047099, mean_q: -0.314719
 47400/100000: episode: 474, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -14.992, mean reward: -0.150 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.530, 10.098], loss: 0.002358, mae: 0.049641, mean_q: -0.317565
 47500/100000: episode: 475, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.154, mean reward: -0.182 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.908, 10.229], loss: 0.002597, mae: 0.051584, mean_q: -0.288815
 47600/100000: episode: 476, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.128, mean reward: -0.171 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.548, 10.177], loss: 0.002402, mae: 0.050180, mean_q: -0.304125
 47700/100000: episode: 477, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.035, mean reward: -0.160 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.820, 10.369], loss: 0.002166, mae: 0.046421, mean_q: -0.330146
 47800/100000: episode: 478, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.543, mean reward: -0.165 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.222, 10.098], loss: 0.002274, mae: 0.047108, mean_q: -0.290027
 47900/100000: episode: 479, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.786, mean reward: -0.168 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.057, 10.186], loss: 0.002173, mae: 0.046221, mean_q: -0.322013
 48000/100000: episode: 480, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.372, mean reward: -0.164 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.191, 10.098], loss: 0.002408, mae: 0.049389, mean_q: -0.296834
 48100/100000: episode: 481, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.716, mean reward: -0.187 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.526, 10.098], loss: 0.002363, mae: 0.047576, mean_q: -0.328392
 48200/100000: episode: 482, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.820, mean reward: -0.178 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.155, 10.106], loss: 0.002302, mae: 0.047875, mean_q: -0.351943
 48300/100000: episode: 483, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -11.284, mean reward: -0.113 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.680, 10.279], loss: 0.002602, mae: 0.051091, mean_q: -0.331517
 48400/100000: episode: 484, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.150, mean reward: -0.152 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.292, 10.270], loss: 0.002271, mae: 0.047468, mean_q: -0.335488
 48500/100000: episode: 485, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.871, mean reward: -0.169 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.958, 10.225], loss: 0.002370, mae: 0.048755, mean_q: -0.303582
 48600/100000: episode: 486, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.145, mean reward: -0.151 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.576, 10.098], loss: 0.002362, mae: 0.048431, mean_q: -0.324415
 48700/100000: episode: 487, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.873, mean reward: -0.189 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.256, 10.115], loss: 0.002474, mae: 0.048663, mean_q: -0.321435
 48800/100000: episode: 488, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.852, mean reward: -0.189 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.370, 10.192], loss: 0.004262, mae: 0.065983, mean_q: -0.313508
 48900/100000: episode: 489, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.131, mean reward: -0.171 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.805, 10.242], loss: 0.002342, mae: 0.048675, mean_q: -0.330789
 49000/100000: episode: 490, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.746, mean reward: -0.187 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.771, 10.098], loss: 0.002348, mae: 0.047810, mean_q: -0.347580
 49100/100000: episode: 491, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -14.002, mean reward: -0.140 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.637, 10.424], loss: 0.002387, mae: 0.048309, mean_q: -0.330770
 49200/100000: episode: 492, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -11.502, mean reward: -0.115 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.665, 10.098], loss: 0.002387, mae: 0.048184, mean_q: -0.317174
 49300/100000: episode: 493, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.622, mean reward: -0.166 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.028, 10.129], loss: 0.002400, mae: 0.048134, mean_q: -0.333462
 49400/100000: episode: 494, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -14.164, mean reward: -0.142 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.065, 10.289], loss: 0.002482, mae: 0.049610, mean_q: -0.269199
 49500/100000: episode: 495, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -5.958, mean reward: -0.060 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.430, 10.434], loss: 0.003071, mae: 0.058173, mean_q: -0.301843
 49600/100000: episode: 496, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.931, mean reward: -0.199 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.102, 10.178], loss: 0.002573, mae: 0.051272, mean_q: -0.306473
 49700/100000: episode: 497, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.186, mean reward: -0.182 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.560, 10.098], loss: 0.002634, mae: 0.053272, mean_q: -0.295398
 49800/100000: episode: 498, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.805, mean reward: -0.188 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.406, 10.145], loss: 0.002641, mae: 0.051273, mean_q: -0.322461
 49900/100000: episode: 499, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.423, mean reward: -0.134 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.892, 10.277], loss: 0.002439, mae: 0.049710, mean_q: -0.287234
 50000/100000: episode: 500, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -12.971, mean reward: -0.130 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.823, 10.256], loss: 0.002752, mae: 0.051726, mean_q: -0.306708
 50100/100000: episode: 501, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.153, mean reward: -0.162 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.686, 10.098], loss: 0.002748, mae: 0.051014, mean_q: -0.322565
 50200/100000: episode: 502, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.446, mean reward: -0.144 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.465, 10.098], loss: 0.002595, mae: 0.050107, mean_q: -0.307066
 50300/100000: episode: 503, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -19.941, mean reward: -0.199 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.901, 10.103], loss: 0.002458, mae: 0.048496, mean_q: -0.288813
 50400/100000: episode: 504, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.201, mean reward: -0.152 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.318, 10.216], loss: 0.002259, mae: 0.046505, mean_q: -0.339363
 50500/100000: episode: 505, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.205, mean reward: -0.162 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.823, 10.098], loss: 0.002552, mae: 0.050770, mean_q: -0.293275
 50600/100000: episode: 506, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -20.575, mean reward: -0.206 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.726, 10.101], loss: 0.002628, mae: 0.051030, mean_q: -0.302871
 50700/100000: episode: 507, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.562, mean reward: -0.166 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.658, 10.250], loss: 0.002548, mae: 0.049603, mean_q: -0.340907
 50800/100000: episode: 508, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.214, mean reward: -0.162 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.452, 10.098], loss: 0.003457, mae: 0.059569, mean_q: -0.328420
 50900/100000: episode: 509, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -9.561, mean reward: -0.096 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.842, 10.098], loss: 0.002632, mae: 0.051873, mean_q: -0.312398
 51000/100000: episode: 510, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.447, mean reward: -0.144 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.064, 10.098], loss: 0.002512, mae: 0.049928, mean_q: -0.313577
 51100/100000: episode: 511, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.022, mean reward: -0.170 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.704, 10.247], loss: 0.002292, mae: 0.047830, mean_q: -0.321508
 51200/100000: episode: 512, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.367, mean reward: -0.154 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.490, 10.238], loss: 0.002533, mae: 0.050917, mean_q: -0.290639
 51300/100000: episode: 513, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.366, mean reward: -0.184 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.033, 10.125], loss: 0.002453, mae: 0.049156, mean_q: -0.314565
 51400/100000: episode: 514, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.862, mean reward: -0.189 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.078, 10.098], loss: 0.002419, mae: 0.048326, mean_q: -0.311425
 51500/100000: episode: 515, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.708, mean reward: -0.187 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.879, 10.167], loss: 0.002363, mae: 0.048043, mean_q: -0.310653
 51600/100000: episode: 516, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.765, mean reward: -0.188 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.496, 10.098], loss: 0.002568, mae: 0.050316, mean_q: -0.307763
 51700/100000: episode: 517, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.542, mean reward: -0.185 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.213, 10.278], loss: 0.002785, mae: 0.053530, mean_q: -0.311180
 51800/100000: episode: 518, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.135, mean reward: -0.191 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.570, 10.098], loss: 0.002450, mae: 0.048495, mean_q: -0.345542
 51900/100000: episode: 519, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.958, mean reward: -0.130 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.950, 10.534], loss: 0.002819, mae: 0.053436, mean_q: -0.292872
 52000/100000: episode: 520, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.348, mean reward: -0.183 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.479, 10.100], loss: 0.003328, mae: 0.054810, mean_q: -0.308802
 52100/100000: episode: 521, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.037, mean reward: -0.160 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.363, 10.098], loss: 0.004471, mae: 0.067826, mean_q: -0.327149
 52200/100000: episode: 522, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.698, mean reward: -0.177 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.428, 10.257], loss: 0.002521, mae: 0.050934, mean_q: -0.312236
 52300/100000: episode: 523, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.801, mean reward: -0.178 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.603, 10.098], loss: 0.002631, mae: 0.051975, mean_q: -0.281008
 52400/100000: episode: 524, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.969, mean reward: -0.180 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.535, 10.098], loss: 0.002584, mae: 0.050417, mean_q: -0.322018
 52500/100000: episode: 525, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.623, mean reward: -0.146 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.700, 10.098], loss: 0.002540, mae: 0.049666, mean_q: -0.289970
 52600/100000: episode: 526, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.954, mean reward: -0.180 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.007, 10.190], loss: 0.002614, mae: 0.050479, mean_q: -0.304574
 52700/100000: episode: 527, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.691, mean reward: -0.197 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.532, 10.098], loss: 0.002684, mae: 0.051042, mean_q: -0.337317
 52800/100000: episode: 528, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.499, mean reward: -0.185 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.725, 10.098], loss: 0.002455, mae: 0.048989, mean_q: -0.322534
 52900/100000: episode: 529, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.505, mean reward: -0.145 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.657, 10.098], loss: 0.002512, mae: 0.049289, mean_q: -0.284239
 53000/100000: episode: 530, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.247, mean reward: -0.172 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.116, 10.098], loss: 0.002650, mae: 0.050230, mean_q: -0.348082
 53100/100000: episode: 531, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.318, mean reward: -0.183 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.604, 10.098], loss: 0.002786, mae: 0.052311, mean_q: -0.307758
 53200/100000: episode: 532, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.227, mean reward: -0.162 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.871, 10.350], loss: 0.002475, mae: 0.048316, mean_q: -0.320654
 53300/100000: episode: 533, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -13.779, mean reward: -0.138 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.607, 10.098], loss: 0.002669, mae: 0.052021, mean_q: -0.351506
 53400/100000: episode: 534, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.069, mean reward: -0.171 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.028, 10.288], loss: 0.002383, mae: 0.047878, mean_q: -0.314645
 53500/100000: episode: 535, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.754, mean reward: -0.168 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.393, 10.098], loss: 0.002635, mae: 0.049970, mean_q: -0.319389
 53600/100000: episode: 536, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.672, mean reward: -0.167 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.188, 10.258], loss: 0.002706, mae: 0.051488, mean_q: -0.321863
 53700/100000: episode: 537, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -17.041, mean reward: -0.170 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.253, 10.288], loss: 0.002534, mae: 0.049786, mean_q: -0.298157
 53800/100000: episode: 538, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.925, mean reward: -0.179 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.685, 10.098], loss: 0.002622, mae: 0.050741, mean_q: -0.303614
 53900/100000: episode: 539, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.290, mean reward: -0.153 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.084, 10.098], loss: 0.002895, mae: 0.054646, mean_q: -0.301915
 54000/100000: episode: 540, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.832, mean reward: -0.168 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.834, 10.145], loss: 0.002939, mae: 0.053695, mean_q: -0.315415
 54100/100000: episode: 541, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.691, mean reward: -0.167 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.678, 10.098], loss: 0.002520, mae: 0.049514, mean_q: -0.302264
 54200/100000: episode: 542, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -13.873, mean reward: -0.139 [-1.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.336, 10.311], loss: 0.002570, mae: 0.049697, mean_q: -0.306794
 54300/100000: episode: 543, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -9.986, mean reward: -0.100 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.956, 10.429], loss: 0.002589, mae: 0.049047, mean_q: -0.326451
 54400/100000: episode: 544, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.981, mean reward: -0.140 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.969, 10.298], loss: 0.002492, mae: 0.048963, mean_q: -0.321124
 54500/100000: episode: 545, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.024, mean reward: -0.160 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.292, 10.134], loss: 0.002656, mae: 0.050120, mean_q: -0.303617
 54600/100000: episode: 546, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.778, mean reward: -0.178 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.258, 10.098], loss: 0.002571, mae: 0.049664, mean_q: -0.313109
 54700/100000: episode: 547, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -18.100, mean reward: -0.181 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.720, 10.098], loss: 0.002451, mae: 0.048988, mean_q: -0.293146
 54800/100000: episode: 548, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.574, mean reward: -0.146 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.335, 10.099], loss: 0.002556, mae: 0.049403, mean_q: -0.292110
 54900/100000: episode: 549, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -19.245, mean reward: -0.192 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.255, 10.098], loss: 0.004500, mae: 0.063629, mean_q: -0.293926
 55000/100000: episode: 550, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -19.428, mean reward: -0.194 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.456, 10.217], loss: 0.002508, mae: 0.049557, mean_q: -0.309465
 55100/100000: episode: 551, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -11.740, mean reward: -0.117 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.714, 10.098], loss: 0.002544, mae: 0.050123, mean_q: -0.315947
 55200/100000: episode: 552, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.887, mean reward: -0.139 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.075, 10.169], loss: 0.002617, mae: 0.051657, mean_q: -0.293808
 55300/100000: episode: 553, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -18.470, mean reward: -0.185 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.297, 10.098], loss: 0.002410, mae: 0.048431, mean_q: -0.335867
 55400/100000: episode: 554, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -15.233, mean reward: -0.152 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.443, 10.098], loss: 0.002517, mae: 0.049487, mean_q: -0.317888
 55500/100000: episode: 555, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -20.935, mean reward: -0.209 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.255, 10.159], loss: 0.002280, mae: 0.046215, mean_q: -0.321563
 55600/100000: episode: 556, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.027, mean reward: -0.180 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.785, 10.098], loss: 0.002442, mae: 0.048189, mean_q: -0.315841
 55700/100000: episode: 557, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.365, mean reward: -0.184 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.576, 10.098], loss: 0.002419, mae: 0.047614, mean_q: -0.322974
 55800/100000: episode: 558, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.437, mean reward: -0.184 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.842, 10.098], loss: 0.002413, mae: 0.048666, mean_q: -0.332736
 55900/100000: episode: 559, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.921, mean reward: -0.179 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.002, 10.234], loss: 0.002429, mae: 0.047443, mean_q: -0.324127
 56000/100000: episode: 560, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.518, mean reward: -0.175 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.343, 10.136], loss: 0.002438, mae: 0.047620, mean_q: -0.288888
 56100/100000: episode: 561, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.236, mean reward: -0.162 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.293, 10.098], loss: 0.002360, mae: 0.047811, mean_q: -0.298755
 56200/100000: episode: 562, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.209, mean reward: -0.182 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.153, 10.244], loss: 0.002417, mae: 0.048064, mean_q: -0.314523
 56300/100000: episode: 563, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.800, mean reward: -0.168 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.675, 10.098], loss: 0.002329, mae: 0.047355, mean_q: -0.285478
 56400/100000: episode: 564, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.421, mean reward: -0.164 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.112, 10.242], loss: 0.002632, mae: 0.050333, mean_q: -0.304845
 56500/100000: episode: 565, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.519, mean reward: -0.175 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.327, 10.098], loss: 0.002511, mae: 0.048725, mean_q: -0.324626
 56600/100000: episode: 566, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -15.888, mean reward: -0.159 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.465, 10.098], loss: 0.002506, mae: 0.049318, mean_q: -0.365226
 56700/100000: episode: 567, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -13.697, mean reward: -0.137 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.698, 10.467], loss: 0.004315, mae: 0.063683, mean_q: -0.318642
 56800/100000: episode: 568, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.596, mean reward: -0.166 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.841, 10.189], loss: 0.002623, mae: 0.053044, mean_q: -0.284364
 56900/100000: episode: 569, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.266, mean reward: -0.173 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.312, 10.098], loss: 0.002394, mae: 0.048030, mean_q: -0.336571
 57000/100000: episode: 570, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.529, mean reward: -0.155 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.699, 10.098], loss: 0.002329, mae: 0.046847, mean_q: -0.286845
 57100/100000: episode: 571, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.218, mean reward: -0.182 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.198, 10.298], loss: 0.002534, mae: 0.048898, mean_q: -0.279047
 57200/100000: episode: 572, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -14.710, mean reward: -0.147 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.817, 10.098], loss: 0.002624, mae: 0.049768, mean_q: -0.298237
 57300/100000: episode: 573, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.500, mean reward: -0.165 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.394, 10.311], loss: 0.002445, mae: 0.047861, mean_q: -0.316192
 57400/100000: episode: 574, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.095, mean reward: -0.161 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.742, 10.144], loss: 0.002935, mae: 0.056093, mean_q: -0.321723
 57500/100000: episode: 575, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.604, mean reward: -0.196 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.355, 10.188], loss: 0.002612, mae: 0.051656, mean_q: -0.338810
 57600/100000: episode: 576, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.955, mean reward: -0.180 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.851, 10.374], loss: 0.002520, mae: 0.049413, mean_q: -0.305755
 57700/100000: episode: 577, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -13.460, mean reward: -0.135 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.623, 10.098], loss: 0.002191, mae: 0.045982, mean_q: -0.312024
 57800/100000: episode: 578, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -17.745, mean reward: -0.177 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.579, 10.283], loss: 0.002590, mae: 0.049121, mean_q: -0.300415
 57900/100000: episode: 579, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.899, mean reward: -0.189 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.752, 10.162], loss: 0.002373, mae: 0.048687, mean_q: -0.308375
 58000/100000: episode: 580, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.599, mean reward: -0.196 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.866, 10.180], loss: 0.002429, mae: 0.048107, mean_q: -0.325537
 58100/100000: episode: 581, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.950, mean reward: -0.179 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.526, 10.307], loss: 0.002539, mae: 0.048965, mean_q: -0.339751
 58200/100000: episode: 582, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.032, mean reward: -0.180 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.557, 10.230], loss: 0.002649, mae: 0.050724, mean_q: -0.300483
 58300/100000: episode: 583, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.446, mean reward: -0.184 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.774, 10.098], loss: 0.002549, mae: 0.050174, mean_q: -0.307285
 58400/100000: episode: 584, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.393, mean reward: -0.184 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.780, 10.098], loss: 0.002518, mae: 0.049156, mean_q: -0.308363
 58500/100000: episode: 585, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.593, mean reward: -0.146 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.861, 10.098], loss: 0.002414, mae: 0.048756, mean_q: -0.300480
 58600/100000: episode: 586, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -19.224, mean reward: -0.192 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.848, 10.180], loss: 0.002429, mae: 0.048032, mean_q: -0.342341
 58700/100000: episode: 587, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.025, mean reward: -0.180 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.517, 10.098], loss: 0.002534, mae: 0.049388, mean_q: -0.304997
 58800/100000: episode: 588, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.435, mean reward: -0.164 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.754, 10.111], loss: 0.002577, mae: 0.049868, mean_q: -0.305300
 58900/100000: episode: 589, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.228, mean reward: -0.162 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.004, 10.098], loss: 0.002558, mae: 0.049506, mean_q: -0.347788
 59000/100000: episode: 590, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.385, mean reward: -0.194 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.795, 10.133], loss: 0.002486, mae: 0.048825, mean_q: -0.316718
 59100/100000: episode: 591, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -12.853, mean reward: -0.129 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.658, 10.354], loss: 0.002673, mae: 0.050496, mean_q: -0.303365
 59200/100000: episode: 592, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.000, mean reward: -0.180 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.794, 10.098], loss: 0.002619, mae: 0.050747, mean_q: -0.290877
 59300/100000: episode: 593, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.978, mean reward: -0.140 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.744, 10.098], loss: 0.002330, mae: 0.048167, mean_q: -0.316922
 59400/100000: episode: 594, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.893, mean reward: -0.149 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.087, 10.123], loss: 0.003001, mae: 0.054408, mean_q: -0.326936
 59500/100000: episode: 595, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.832, mean reward: -0.178 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.280, 10.137], loss: 0.002442, mae: 0.049437, mean_q: -0.325654
 59600/100000: episode: 596, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.600, mean reward: -0.186 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.658, 10.098], loss: 0.002604, mae: 0.048698, mean_q: -0.352108
 59700/100000: episode: 597, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.625, mean reward: -0.186 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.358, 10.098], loss: 0.002556, mae: 0.049636, mean_q: -0.334377
 59800/100000: episode: 598, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.485, mean reward: -0.155 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.347, 10.163], loss: 0.002356, mae: 0.047270, mean_q: -0.310075
 59900/100000: episode: 599, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -14.904, mean reward: -0.149 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.134, 10.396], loss: 0.002533, mae: 0.049338, mean_q: -0.325257
 60000/100000: episode: 600, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.394, mean reward: -0.184 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.903, 10.355], loss: 0.002686, mae: 0.052050, mean_q: -0.306416
 60100/100000: episode: 601, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.440, mean reward: -0.184 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.126, 10.253], loss: 0.002698, mae: 0.051353, mean_q: -0.308099
 60200/100000: episode: 602, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -16.003, mean reward: -0.160 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.333, 10.407], loss: 0.002534, mae: 0.049544, mean_q: -0.322598
 60300/100000: episode: 603, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.983, mean reward: -0.190 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.955, 10.152], loss: 0.002524, mae: 0.049790, mean_q: -0.335561
 60400/100000: episode: 604, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -17.849, mean reward: -0.178 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.772, 10.198], loss: 0.002556, mae: 0.049437, mean_q: -0.336288
 60500/100000: episode: 605, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -10.116, mean reward: -0.101 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.594, 10.357], loss: 0.002597, mae: 0.050043, mean_q: -0.336137
 60600/100000: episode: 606, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.280, mean reward: -0.153 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.080, 10.393], loss: 0.002809, mae: 0.052639, mean_q: -0.336768
 60700/100000: episode: 607, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.785, mean reward: -0.178 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.831, 10.098], loss: 0.003847, mae: 0.061346, mean_q: -0.283207
 60800/100000: episode: 608, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.631, mean reward: -0.196 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.573, 10.107], loss: 0.002816, mae: 0.054247, mean_q: -0.295621
 60900/100000: episode: 609, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.654, mean reward: -0.177 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.682, 10.129], loss: 0.002796, mae: 0.051774, mean_q: -0.305008
 61000/100000: episode: 610, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.897, mean reward: -0.199 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.691, 10.233], loss: 0.002583, mae: 0.050333, mean_q: -0.334550
 61100/100000: episode: 611, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.782, mean reward: -0.188 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.877, 10.098], loss: 0.002420, mae: 0.048283, mean_q: -0.305260
 61200/100000: episode: 612, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.276, mean reward: -0.153 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.940, 10.098], loss: 0.002671, mae: 0.050676, mean_q: -0.351413
 61300/100000: episode: 613, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.148, mean reward: -0.181 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.595, 10.098], loss: 0.002715, mae: 0.051961, mean_q: -0.325253
 61400/100000: episode: 614, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.868, mean reward: -0.159 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.427, 10.098], loss: 0.002670, mae: 0.052030, mean_q: -0.300739
 61500/100000: episode: 615, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.306, mean reward: -0.143 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.105, 10.098], loss: 0.002563, mae: 0.050177, mean_q: -0.311705
 61600/100000: episode: 616, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.864, mean reward: -0.169 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.317, 10.098], loss: 0.002817, mae: 0.052440, mean_q: -0.291924
 61700/100000: episode: 617, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -14.012, mean reward: -0.140 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.012, 10.098], loss: 0.002745, mae: 0.051454, mean_q: -0.315810
 61800/100000: episode: 618, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.589, mean reward: -0.186 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.834, 10.175], loss: 0.002614, mae: 0.050430, mean_q: -0.332438
 61900/100000: episode: 619, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.899, mean reward: -0.189 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.682, 10.098], loss: 0.002663, mae: 0.050533, mean_q: -0.322719
 62000/100000: episode: 620, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.479, mean reward: -0.195 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.443, 10.237], loss: 0.002622, mae: 0.049719, mean_q: -0.307431
 62100/100000: episode: 621, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -10.491, mean reward: -0.105 [-1.000, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.999, 10.098], loss: 0.002816, mae: 0.053313, mean_q: -0.311019
 62200/100000: episode: 622, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.902, mean reward: -0.169 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.419, 10.399], loss: 0.002784, mae: 0.052609, mean_q: -0.314876
 62300/100000: episode: 623, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -18.289, mean reward: -0.183 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.375, 10.098], loss: 0.002427, mae: 0.049461, mean_q: -0.329988
 62400/100000: episode: 624, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -17.854, mean reward: -0.179 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.431, 10.098], loss: 0.002561, mae: 0.049666, mean_q: -0.336534
 62500/100000: episode: 625, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.997, mean reward: -0.190 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.320, 10.098], loss: 0.002520, mae: 0.049620, mean_q: -0.316575
 62600/100000: episode: 626, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -13.005, mean reward: -0.130 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.596, 10.331], loss: 0.002468, mae: 0.048930, mean_q: -0.321517
 62700/100000: episode: 627, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.795, mean reward: -0.158 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.280, 10.098], loss: 0.002554, mae: 0.049509, mean_q: -0.335688
 62800/100000: episode: 628, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.782, mean reward: -0.158 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.439, 10.419], loss: 0.002763, mae: 0.051879, mean_q: -0.296882
 62900/100000: episode: 629, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -16.764, mean reward: -0.168 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.726, 10.339], loss: 0.002872, mae: 0.053623, mean_q: -0.332795
 63000/100000: episode: 630, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.342, mean reward: -0.163 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.815, 10.098], loss: 0.004913, mae: 0.072310, mean_q: -0.318155
 63100/100000: episode: 631, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -21.622, mean reward: -0.216 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.889, 10.098], loss: 0.002654, mae: 0.051774, mean_q: -0.298370
 63200/100000: episode: 632, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.440, mean reward: -0.174 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.984, 10.150], loss: 0.002487, mae: 0.049760, mean_q: -0.322974
 63300/100000: episode: 633, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -19.245, mean reward: -0.192 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.506, 10.153], loss: 0.002683, mae: 0.051951, mean_q: -0.266628
 63400/100000: episode: 634, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.542, mean reward: -0.195 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.289, 10.098], loss: 0.002439, mae: 0.049000, mean_q: -0.324105
 63500/100000: episode: 635, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -13.251, mean reward: -0.133 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.676, 10.196], loss: 0.002518, mae: 0.048912, mean_q: -0.331336
 63600/100000: episode: 636, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.898, mean reward: -0.149 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.520, 10.248], loss: 0.002681, mae: 0.052252, mean_q: -0.319599
 63700/100000: episode: 637, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.089, mean reward: -0.161 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.174, 10.098], loss: 0.002785, mae: 0.051571, mean_q: -0.336513
 63800/100000: episode: 638, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.753, mean reward: -0.188 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.875, 10.191], loss: 0.002614, mae: 0.050765, mean_q: -0.269027
 63900/100000: episode: 639, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.105, mean reward: -0.181 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.669, 10.304], loss: 0.002520, mae: 0.049172, mean_q: -0.311918
 64000/100000: episode: 640, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -11.948, mean reward: -0.119 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.015, 10.098], loss: 0.002618, mae: 0.051015, mean_q: -0.302804
 64100/100000: episode: 641, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.457, mean reward: -0.185 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.977, 10.098], loss: 0.002856, mae: 0.053101, mean_q: -0.342547
 64200/100000: episode: 642, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.025, mean reward: -0.150 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.646, 10.098], loss: 0.002654, mae: 0.051185, mean_q: -0.304623
 64300/100000: episode: 643, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -15.102, mean reward: -0.151 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.238, 10.390], loss: 0.002525, mae: 0.049419, mean_q: -0.346275
 64400/100000: episode: 644, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.812, mean reward: -0.178 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.427, 10.098], loss: 0.002253, mae: 0.045734, mean_q: -0.350296
 64500/100000: episode: 645, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.101, mean reward: -0.171 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.581, 10.098], loss: 0.002374, mae: 0.048473, mean_q: -0.340472
 64600/100000: episode: 646, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.817, mean reward: -0.198 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.333, 10.098], loss: 0.002660, mae: 0.051010, mean_q: -0.307952
 64700/100000: episode: 647, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.332, mean reward: -0.143 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.135, 10.098], loss: 0.004271, mae: 0.061600, mean_q: -0.320531
 64800/100000: episode: 648, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.291, mean reward: -0.173 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.609, 10.297], loss: 0.003003, mae: 0.055464, mean_q: -0.320393
 64900/100000: episode: 649, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -19.507, mean reward: -0.195 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.678, 10.192], loss: 0.002752, mae: 0.054393, mean_q: -0.311433
 65000/100000: episode: 650, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -13.413, mean reward: -0.134 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.367, 10.130], loss: 0.002440, mae: 0.050439, mean_q: -0.352196
 65100/100000: episode: 651, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.612, mean reward: -0.156 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.980, 10.202], loss: 0.002661, mae: 0.053529, mean_q: -0.322667
 65200/100000: episode: 652, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -19.436, mean reward: -0.194 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.654, 10.098], loss: 0.002378, mae: 0.048754, mean_q: -0.310934
 65300/100000: episode: 653, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.675, mean reward: -0.187 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.776, 10.312], loss: 0.002676, mae: 0.050196, mean_q: -0.340386
 65400/100000: episode: 654, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -13.430, mean reward: -0.134 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.010, 10.268], loss: 0.002670, mae: 0.052793, mean_q: -0.281577
 65500/100000: episode: 655, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.404, mean reward: -0.144 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.428, 10.098], loss: 0.002598, mae: 0.051645, mean_q: -0.316647
 65600/100000: episode: 656, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.875, mean reward: -0.169 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.952, 10.098], loss: 0.002424, mae: 0.048642, mean_q: -0.305684
 65700/100000: episode: 657, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.809, mean reward: -0.198 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.464, 10.098], loss: 0.002586, mae: 0.050758, mean_q: -0.320233
 65800/100000: episode: 658, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.090, mean reward: -0.171 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.900, 10.098], loss: 0.002479, mae: 0.049209, mean_q: -0.331479
 65900/100000: episode: 659, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.485, mean reward: -0.175 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.265, 10.098], loss: 0.002514, mae: 0.049251, mean_q: -0.293721
 66000/100000: episode: 660, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.453, mean reward: -0.155 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.277, 10.098], loss: 0.002464, mae: 0.048557, mean_q: -0.328313
 66100/100000: episode: 661, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -17.975, mean reward: -0.180 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.237, 10.336], loss: 0.002442, mae: 0.049323, mean_q: -0.334330
 66200/100000: episode: 662, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -17.051, mean reward: -0.171 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.205, 10.098], loss: 0.002505, mae: 0.049057, mean_q: -0.324085
 66300/100000: episode: 663, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.004, mean reward: -0.160 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.969, 10.098], loss: 0.002323, mae: 0.047701, mean_q: -0.298268
 66400/100000: episode: 664, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.219, mean reward: -0.172 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.738, 10.158], loss: 0.002270, mae: 0.047440, mean_q: -0.329340
 66500/100000: episode: 665, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.801, mean reward: -0.178 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.672, 10.098], loss: 0.002447, mae: 0.048716, mean_q: -0.339389
 66600/100000: episode: 666, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.361, mean reward: -0.164 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.163, 10.098], loss: 0.002445, mae: 0.048542, mean_q: -0.316490
 66700/100000: episode: 667, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.938, mean reward: -0.179 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.694, 10.098], loss: 0.002305, mae: 0.047841, mean_q: -0.287765
 66800/100000: episode: 668, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.818, mean reward: -0.158 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.461, 10.098], loss: 0.002735, mae: 0.052943, mean_q: -0.325433
 66900/100000: episode: 669, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.123, mean reward: -0.171 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.859, 10.098], loss: 0.002312, mae: 0.047491, mean_q: -0.341194
 67000/100000: episode: 670, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -8.266, mean reward: -0.083 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.100, 10.344], loss: 0.002344, mae: 0.048287, mean_q: -0.341476
 67100/100000: episode: 671, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -20.349, mean reward: -0.203 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.693, 10.202], loss: 0.002450, mae: 0.049569, mean_q: -0.296583
 67200/100000: episode: 672, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -14.381, mean reward: -0.144 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.238, 10.098], loss: 0.002443, mae: 0.049381, mean_q: -0.306866
 67300/100000: episode: 673, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.489, mean reward: -0.155 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.909, 10.098], loss: 0.003140, mae: 0.056027, mean_q: -0.313460
 67400/100000: episode: 674, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.253, mean reward: -0.173 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.071, 10.264], loss: 0.002454, mae: 0.049193, mean_q: -0.312991
 67500/100000: episode: 675, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.714, mean reward: -0.197 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.316, 10.098], loss: 0.002633, mae: 0.051162, mean_q: -0.355256
 67600/100000: episode: 676, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.602, mean reward: -0.176 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.093, 10.131], loss: 0.002531, mae: 0.049647, mean_q: -0.318313
 67700/100000: episode: 677, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -13.898, mean reward: -0.139 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.635, 10.098], loss: 0.002462, mae: 0.049468, mean_q: -0.326812
 67800/100000: episode: 678, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.326, mean reward: -0.173 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.464, 10.249], loss: 0.002579, mae: 0.050499, mean_q: -0.296075
 67900/100000: episode: 679, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.631, mean reward: -0.196 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.222, 10.098], loss: 0.002439, mae: 0.048550, mean_q: -0.334411
 68000/100000: episode: 680, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.617, mean reward: -0.186 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.393, 10.399], loss: 0.002661, mae: 0.052734, mean_q: -0.301476
 68100/100000: episode: 681, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.726, mean reward: -0.187 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.499, 10.098], loss: 0.002604, mae: 0.050102, mean_q: -0.328383
 68200/100000: episode: 682, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.778, mean reward: -0.188 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.283, 10.123], loss: 0.002386, mae: 0.048479, mean_q: -0.333461
 68300/100000: episode: 683, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.494, mean reward: -0.165 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.414, 10.098], loss: 0.007462, mae: 0.073978, mean_q: -0.309688
 68400/100000: episode: 684, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.540, mean reward: -0.175 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.555, 10.098], loss: 0.002516, mae: 0.051233, mean_q: -0.319243
 68500/100000: episode: 685, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.702, mean reward: -0.187 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.212, 10.217], loss: 0.002425, mae: 0.049626, mean_q: -0.294162
 68600/100000: episode: 686, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -20.302, mean reward: -0.203 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.727, 10.098], loss: 0.002567, mae: 0.050414, mean_q: -0.315198
 68700/100000: episode: 687, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.387, mean reward: -0.184 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.443, 10.101], loss: 0.002440, mae: 0.049431, mean_q: -0.329383
 68800/100000: episode: 688, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.258, mean reward: -0.193 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.467, 10.098], loss: 0.002466, mae: 0.049953, mean_q: -0.280047
 68900/100000: episode: 689, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.037, mean reward: -0.190 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.289, 10.156], loss: 0.002266, mae: 0.047537, mean_q: -0.343118
 69000/100000: episode: 690, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.868, mean reward: -0.169 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.023, 10.301], loss: 0.002539, mae: 0.049714, mean_q: -0.329741
 69100/100000: episode: 691, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.646, mean reward: -0.176 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.023, 10.098], loss: 0.002487, mae: 0.049568, mean_q: -0.293049
 69200/100000: episode: 692, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.033, mean reward: -0.150 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.860, 10.098], loss: 0.002169, mae: 0.046178, mean_q: -0.347676
 69300/100000: episode: 693, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -13.289, mean reward: -0.133 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.611, 10.098], loss: 0.002381, mae: 0.047338, mean_q: -0.358858
 69400/100000: episode: 694, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -12.301, mean reward: -0.123 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.748, 10.153], loss: 0.002413, mae: 0.049080, mean_q: -0.274950
 69500/100000: episode: 695, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -20.814, mean reward: -0.208 [-1.000, 0.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.512, 10.153], loss: 0.002396, mae: 0.048584, mean_q: -0.304815
 69600/100000: episode: 696, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.697, mean reward: -0.157 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.261, 10.377], loss: 0.002550, mae: 0.050722, mean_q: -0.318147
 69700/100000: episode: 697, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.827, mean reward: -0.188 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.698, 10.161], loss: 0.002834, mae: 0.054956, mean_q: -0.293470
 69800/100000: episode: 698, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.055, mean reward: -0.161 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.549, 10.098], loss: 0.002459, mae: 0.049207, mean_q: -0.315026
 69900/100000: episode: 699, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -17.373, mean reward: -0.174 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.217, 10.098], loss: 0.002471, mae: 0.048633, mean_q: -0.322905
 70000/100000: episode: 700, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.966, mean reward: -0.190 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.296, 10.114], loss: 0.002363, mae: 0.047888, mean_q: -0.336680
 70100/100000: episode: 701, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -14.266, mean reward: -0.143 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.869, 10.344], loss: 0.002402, mae: 0.047534, mean_q: -0.355900
 70200/100000: episode: 702, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.366, mean reward: -0.144 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.623, 10.098], loss: 0.002353, mae: 0.047411, mean_q: -0.348346
 70300/100000: episode: 703, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -15.637, mean reward: -0.156 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.422, 10.098], loss: 0.002413, mae: 0.048587, mean_q: -0.313003
 70400/100000: episode: 704, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.893, mean reward: -0.179 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.718, 10.098], loss: 0.004806, mae: 0.062036, mean_q: -0.328420
 70500/100000: episode: 705, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.596, mean reward: -0.166 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.255, 10.143], loss: 0.003136, mae: 0.057231, mean_q: -0.338006
 70600/100000: episode: 706, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.997, mean reward: -0.190 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.831, 10.135], loss: 0.002665, mae: 0.051843, mean_q: -0.324323
 70700/100000: episode: 707, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -16.983, mean reward: -0.170 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.793, 10.244], loss: 0.002400, mae: 0.047665, mean_q: -0.343702
 70800/100000: episode: 708, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.565, mean reward: -0.186 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.023, 10.254], loss: 0.002526, mae: 0.049941, mean_q: -0.312348
 70900/100000: episode: 709, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.351, mean reward: -0.164 [-1.000, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.146, 10.098], loss: 0.002394, mae: 0.047917, mean_q: -0.338272
 71000/100000: episode: 710, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.807, mean reward: -0.188 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.897, 10.098], loss: 0.002657, mae: 0.050942, mean_q: -0.297776
 71100/100000: episode: 711, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -20.367, mean reward: -0.204 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.381, 10.098], loss: 0.002321, mae: 0.047292, mean_q: -0.330592
 71200/100000: episode: 712, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.452, mean reward: -0.185 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.566, 10.164], loss: 0.002340, mae: 0.047257, mean_q: -0.316430
 71300/100000: episode: 713, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.460, mean reward: -0.175 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.995, 10.231], loss: 0.002338, mae: 0.047314, mean_q: -0.317651
 71400/100000: episode: 714, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.715, mean reward: -0.167 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.781, 10.400], loss: 0.002374, mae: 0.048095, mean_q: -0.324984
 71500/100000: episode: 715, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.363, mean reward: -0.164 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.288, 10.098], loss: 0.002700, mae: 0.050928, mean_q: -0.281622
 71600/100000: episode: 716, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.007, mean reward: -0.190 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.757, 10.124], loss: 0.002399, mae: 0.048646, mean_q: -0.339996
 71700/100000: episode: 717, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.869, mean reward: -0.169 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.964, 10.119], loss: 0.002738, mae: 0.050671, mean_q: -0.329963
 71800/100000: episode: 718, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.845, mean reward: -0.158 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.369, 10.098], loss: 0.002671, mae: 0.050521, mean_q: -0.347114
 71900/100000: episode: 719, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -13.936, mean reward: -0.139 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.526, 10.325], loss: 0.002568, mae: 0.050077, mean_q: -0.333199
 72000/100000: episode: 720, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -17.678, mean reward: -0.177 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.411, 10.164], loss: 0.003840, mae: 0.061260, mean_q: -0.338690
 72100/100000: episode: 721, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.469, mean reward: -0.175 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.887, 10.098], loss: 0.002494, mae: 0.051309, mean_q: -0.326359
 72200/100000: episode: 722, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -19.981, mean reward: -0.200 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.416, 10.098], loss: 0.002575, mae: 0.049410, mean_q: -0.329949
 72300/100000: episode: 723, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -20.993, mean reward: -0.210 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.431, 10.202], loss: 0.002673, mae: 0.051369, mean_q: -0.311599
 72400/100000: episode: 724, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.540, mean reward: -0.195 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.977, 10.098], loss: 0.002578, mae: 0.049650, mean_q: -0.331774
 72500/100000: episode: 725, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -13.718, mean reward: -0.137 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.736, 10.197], loss: 0.002866, mae: 0.051939, mean_q: -0.319451
 72600/100000: episode: 726, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.852, mean reward: -0.179 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.453, 10.277], loss: 0.002575, mae: 0.051440, mean_q: -0.332217
 72700/100000: episode: 727, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -13.266, mean reward: -0.133 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.436, 10.172], loss: 0.002463, mae: 0.048232, mean_q: -0.321190
 72800/100000: episode: 728, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.672, mean reward: -0.167 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.022, 10.246], loss: 0.002436, mae: 0.047832, mean_q: -0.337359
 72900/100000: episode: 729, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.291, mean reward: -0.183 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.425, 10.098], loss: 0.002536, mae: 0.048800, mean_q: -0.348232
 73000/100000: episode: 730, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.500, mean reward: -0.165 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.987, 10.098], loss: 0.002483, mae: 0.049045, mean_q: -0.354368
 73100/100000: episode: 731, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.104, mean reward: -0.161 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.848, 10.098], loss: 0.002329, mae: 0.047367, mean_q: -0.330889
 73200/100000: episode: 732, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.164, mean reward: -0.172 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.062, 10.098], loss: 0.002700, mae: 0.052293, mean_q: -0.302769
 73300/100000: episode: 733, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.817, mean reward: -0.178 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.353, 10.098], loss: 0.002617, mae: 0.049925, mean_q: -0.312919
 73400/100000: episode: 734, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.930, mean reward: -0.179 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.675, 10.098], loss: 0.002588, mae: 0.049511, mean_q: -0.312146
 73500/100000: episode: 735, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -17.376, mean reward: -0.174 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.582, 10.242], loss: 0.002445, mae: 0.048719, mean_q: -0.343572
 73600/100000: episode: 736, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -17.563, mean reward: -0.176 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.176, 10.098], loss: 0.002429, mae: 0.049485, mean_q: -0.309462
 73700/100000: episode: 737, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.918, mean reward: -0.189 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.789, 10.103], loss: 0.002432, mae: 0.048554, mean_q: -0.326464
 73800/100000: episode: 738, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.383, mean reward: -0.144 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.426, 10.098], loss: 0.002721, mae: 0.052883, mean_q: -0.301538
 73900/100000: episode: 739, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -10.943, mean reward: -0.109 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.974, 10.098], loss: 0.002940, mae: 0.054618, mean_q: -0.312308
 74000/100000: episode: 740, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.907, mean reward: -0.169 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.368, 10.242], loss: 0.002549, mae: 0.049928, mean_q: -0.305473
 74100/100000: episode: 741, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.573, mean reward: -0.196 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.961, 10.112], loss: 0.002499, mae: 0.048841, mean_q: -0.320907
 74200/100000: episode: 742, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.918, mean reward: -0.169 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.764, 10.307], loss: 0.004063, mae: 0.064982, mean_q: -0.341688
 74300/100000: episode: 743, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -9.713, mean reward: -0.097 [-1.000, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.685, 10.460], loss: 0.002467, mae: 0.048213, mean_q: -0.300018
 74400/100000: episode: 744, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -20.887, mean reward: -0.209 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.805, 10.176], loss: 0.002493, mae: 0.048471, mean_q: -0.330625
 74500/100000: episode: 745, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.924, mean reward: -0.179 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.687, 10.186], loss: 0.002536, mae: 0.049170, mean_q: -0.302059
 74600/100000: episode: 746, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.805, mean reward: -0.158 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.387, 10.213], loss: 0.002382, mae: 0.046833, mean_q: -0.317200
 74700/100000: episode: 747, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -16.657, mean reward: -0.167 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.685, 10.199], loss: 0.002602, mae: 0.049180, mean_q: -0.326856
 74800/100000: episode: 748, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.716, mean reward: -0.147 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.940, 10.333], loss: 0.002697, mae: 0.050423, mean_q: -0.306497
 74900/100000: episode: 749, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -14.504, mean reward: -0.145 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.643, 10.212], loss: 0.002631, mae: 0.050500, mean_q: -0.306783
 75000/100000: episode: 750, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -17.562, mean reward: -0.176 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.954, 10.098], loss: 0.002670, mae: 0.049330, mean_q: -0.326014
 75100/100000: episode: 751, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.285, mean reward: -0.163 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.560, 10.098], loss: 0.002563, mae: 0.048847, mean_q: -0.350474
 75200/100000: episode: 752, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.578, mean reward: -0.146 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.873, 10.421], loss: 0.002746, mae: 0.050859, mean_q: -0.321343
 75300/100000: episode: 753, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.208, mean reward: -0.182 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.780, 10.178], loss: 0.002579, mae: 0.048942, mean_q: -0.355153
 75400/100000: episode: 754, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -12.719, mean reward: -0.127 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.953, 10.144], loss: 0.002801, mae: 0.051508, mean_q: -0.334133
 75500/100000: episode: 755, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -14.572, mean reward: -0.146 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.865, 10.315], loss: 0.002682, mae: 0.050673, mean_q: -0.321978
 75600/100000: episode: 756, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.807, mean reward: -0.188 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.831, 10.098], loss: 0.002648, mae: 0.051021, mean_q: -0.296098
 75700/100000: episode: 757, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.390, mean reward: -0.144 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.059, 10.098], loss: 0.002504, mae: 0.049155, mean_q: -0.320270
 75800/100000: episode: 758, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.025, mean reward: -0.170 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.716, 10.315], loss: 0.002521, mae: 0.049057, mean_q: -0.345196
 75900/100000: episode: 759, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.839, mean reward: -0.158 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.732, 10.166], loss: 0.002558, mae: 0.049678, mean_q: -0.300712
 76000/100000: episode: 760, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.405, mean reward: -0.174 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.670, 10.098], loss: 0.002583, mae: 0.049763, mean_q: -0.317591
 76100/100000: episode: 761, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -10.778, mean reward: -0.108 [-1.000, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.849, 10.098], loss: 0.002608, mae: 0.050515, mean_q: -0.294963
 76200/100000: episode: 762, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.478, mean reward: -0.195 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.974, 10.145], loss: 0.002739, mae: 0.051078, mean_q: -0.306966
 76300/100000: episode: 763, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.495, mean reward: -0.175 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.595, 10.116], loss: 0.002577, mae: 0.050483, mean_q: -0.289649
 76400/100000: episode: 764, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.090, mean reward: -0.191 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.786, 10.304], loss: 0.002586, mae: 0.050932, mean_q: -0.288979
 76500/100000: episode: 765, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.132, mean reward: -0.151 [-1.000, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.767, 10.098], loss: 0.002533, mae: 0.049403, mean_q: -0.322209
 76600/100000: episode: 766, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.670, mean reward: -0.177 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.900, 10.098], loss: 0.002737, mae: 0.051515, mean_q: -0.309707
 76700/100000: episode: 767, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.896, mean reward: -0.179 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.124, 10.250], loss: 0.002659, mae: 0.051637, mean_q: -0.334832
 76800/100000: episode: 768, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.654, mean reward: -0.167 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.393, 10.098], loss: 0.003285, mae: 0.057672, mean_q: -0.316364
 76900/100000: episode: 769, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -16.601, mean reward: -0.166 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.879, 10.140], loss: 0.002551, mae: 0.050659, mean_q: -0.327840
 77000/100000: episode: 770, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.231, mean reward: -0.152 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.973, 10.098], loss: 0.002523, mae: 0.050622, mean_q: -0.300347
 77100/100000: episode: 771, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -7.658, mean reward: -0.077 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.021, 10.098], loss: 0.002432, mae: 0.047961, mean_q: -0.318737
 77200/100000: episode: 772, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -14.438, mean reward: -0.144 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.584, 10.098], loss: 0.002683, mae: 0.050178, mean_q: -0.328903
 77300/100000: episode: 773, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -17.339, mean reward: -0.173 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.059, 10.098], loss: 0.002665, mae: 0.049995, mean_q: -0.292600
 77400/100000: episode: 774, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -9.364, mean reward: -0.094 [-1.000, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.386, 10.098], loss: 0.002524, mae: 0.048996, mean_q: -0.302283
 77500/100000: episode: 775, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -15.477, mean reward: -0.155 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.547, 10.098], loss: 0.002435, mae: 0.047950, mean_q: -0.345879
 77600/100000: episode: 776, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -14.347, mean reward: -0.143 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.922, 10.098], loss: 0.002840, mae: 0.051707, mean_q: -0.303102
 77700/100000: episode: 777, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.414, mean reward: -0.184 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.210, 10.098], loss: 0.002866, mae: 0.051711, mean_q: -0.337562
 77800/100000: episode: 778, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -16.734, mean reward: -0.167 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.410, 10.328], loss: 0.002887, mae: 0.053403, mean_q: -0.322404
 77900/100000: episode: 779, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.174, mean reward: -0.192 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.576, 10.098], loss: 0.002572, mae: 0.049940, mean_q: -0.286089
 78000/100000: episode: 780, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.128, mean reward: -0.191 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.429, 10.098], loss: 0.002909, mae: 0.051814, mean_q: -0.313365
 78100/100000: episode: 781, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -18.583, mean reward: -0.186 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.741, 10.224], loss: 0.002724, mae: 0.051101, mean_q: -0.326072
 78200/100000: episode: 782, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -14.874, mean reward: -0.149 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.418, 10.098], loss: 0.002713, mae: 0.051348, mean_q: -0.293632
 78300/100000: episode: 783, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -16.672, mean reward: -0.167 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.661, 10.304], loss: 0.002678, mae: 0.050052, mean_q: -0.316944
 78400/100000: episode: 784, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.360, mean reward: -0.184 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.688, 10.098], loss: 0.003100, mae: 0.054318, mean_q: -0.301110
 78500/100000: episode: 785, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -13.655, mean reward: -0.137 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.209, 10.098], loss: 0.002710, mae: 0.052005, mean_q: -0.285070
 78600/100000: episode: 786, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -2.705, mean reward: -0.027 [-1.000, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.604, 10.413], loss: 0.002581, mae: 0.049221, mean_q: -0.326879
 78700/100000: episode: 787, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.829, mean reward: -0.168 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.708, 10.202], loss: 0.002981, mae: 0.053368, mean_q: -0.293676
 78800/100000: episode: 788, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -10.600, mean reward: -0.106 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.451, 10.472], loss: 0.002870, mae: 0.054898, mean_q: -0.257009
 78900/100000: episode: 789, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.544, mean reward: -0.175 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.775, 10.098], loss: 0.002661, mae: 0.050640, mean_q: -0.328664
 79000/100000: episode: 790, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.338, mean reward: -0.173 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.514, 10.098], loss: 0.002744, mae: 0.051984, mean_q: -0.302589
 79100/100000: episode: 791, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.835, mean reward: -0.188 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.346, 10.164], loss: 0.002654, mae: 0.051274, mean_q: -0.298403
 79200/100000: episode: 792, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.152, mean reward: -0.162 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.955, 10.098], loss: 0.003503, mae: 0.057118, mean_q: -0.286853
 79300/100000: episode: 793, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.280, mean reward: -0.193 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.468, 10.246], loss: 0.003582, mae: 0.059256, mean_q: -0.324011
 79400/100000: episode: 794, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -13.760, mean reward: -0.138 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.236, 10.098], loss: 0.002708, mae: 0.050745, mean_q: -0.311621
 79500/100000: episode: 795, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.317, mean reward: -0.143 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.236, 10.321], loss: 0.002706, mae: 0.050432, mean_q: -0.307131
 79600/100000: episode: 796, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -19.922, mean reward: -0.199 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.205, 10.098], loss: 0.002816, mae: 0.052069, mean_q: -0.330336
 79700/100000: episode: 797, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -11.951, mean reward: -0.120 [-1.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.314, 10.098], loss: 0.002901, mae: 0.052924, mean_q: -0.289371
 79800/100000: episode: 798, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.316, mean reward: -0.143 [-1.000, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.814, 10.098], loss: 0.003345, mae: 0.059084, mean_q: -0.323022
 79900/100000: episode: 799, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -20.267, mean reward: -0.203 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.208, 10.098], loss: 0.002889, mae: 0.053245, mean_q: -0.285528
 80000/100000: episode: 800, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.377, mean reward: -0.184 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.052, 10.166], loss: 0.002960, mae: 0.053308, mean_q: -0.315739
 80100/100000: episode: 801, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.348, mean reward: -0.183 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.798, 10.277], loss: 0.003033, mae: 0.053584, mean_q: -0.316446
 80200/100000: episode: 802, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.547, mean reward: -0.175 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.388, 10.165], loss: 0.002739, mae: 0.051020, mean_q: -0.310062
 80300/100000: episode: 803, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -17.882, mean reward: -0.179 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.714, 10.163], loss: 0.002876, mae: 0.052209, mean_q: -0.276657
 80400/100000: episode: 804, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -19.193, mean reward: -0.192 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.976, 10.098], loss: 0.002615, mae: 0.050218, mean_q: -0.284862
 80500/100000: episode: 805, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.725, mean reward: -0.177 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.074, 10.098], loss: 0.002816, mae: 0.051583, mean_q: -0.310659
 80600/100000: episode: 806, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.169, mean reward: -0.182 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.539, 10.169], loss: 0.003036, mae: 0.055594, mean_q: -0.303132
 80700/100000: episode: 807, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -14.809, mean reward: -0.148 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.505, 10.098], loss: 0.002796, mae: 0.050178, mean_q: -0.270406
 80800/100000: episode: 808, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.222, mean reward: -0.182 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.603, 10.125], loss: 0.003048, mae: 0.054682, mean_q: -0.290459
 80900/100000: episode: 809, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.612, mean reward: -0.166 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.943, 10.380], loss: 0.002697, mae: 0.052028, mean_q: -0.304278
 81000/100000: episode: 810, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.025, mean reward: -0.170 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.663, 10.211], loss: 0.003123, mae: 0.054049, mean_q: -0.283533
 81100/100000: episode: 811, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -10.664, mean reward: -0.107 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.838, 10.379], loss: 0.003007, mae: 0.053009, mean_q: -0.294827
 81200/100000: episode: 812, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.100, mean reward: -0.191 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.192, 10.139], loss: 0.002800, mae: 0.050966, mean_q: -0.339674
 81300/100000: episode: 813, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.428, mean reward: -0.164 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.419, 10.259], loss: 0.003222, mae: 0.054679, mean_q: -0.334995
 81400/100000: episode: 814, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.151, mean reward: -0.152 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.402, 10.243], loss: 0.004214, mae: 0.065219, mean_q: -0.311730
 81500/100000: episode: 815, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.851, mean reward: -0.159 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.529, 10.098], loss: 0.002799, mae: 0.051226, mean_q: -0.314639
 81600/100000: episode: 816, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.567, mean reward: -0.196 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.935, 10.098], loss: 0.002813, mae: 0.051328, mean_q: -0.317244
 81700/100000: episode: 817, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -15.155, mean reward: -0.152 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.338, 10.162], loss: 0.002717, mae: 0.049557, mean_q: -0.324463
 81800/100000: episode: 818, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -18.380, mean reward: -0.184 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.957, 10.242], loss: 0.002820, mae: 0.051145, mean_q: -0.302934
 81900/100000: episode: 819, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -11.583, mean reward: -0.116 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.640, 10.098], loss: 0.002658, mae: 0.050788, mean_q: -0.297558
 82000/100000: episode: 820, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -19.583, mean reward: -0.196 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.409, 10.098], loss: 0.002723, mae: 0.049949, mean_q: -0.306347
 82100/100000: episode: 821, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.509, mean reward: -0.185 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.451, 10.159], loss: 0.002806, mae: 0.051270, mean_q: -0.297550
 82200/100000: episode: 822, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.529, mean reward: -0.185 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.919, 10.098], loss: 0.002762, mae: 0.050878, mean_q: -0.301590
 82300/100000: episode: 823, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.178, mean reward: -0.182 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.258, 10.137], loss: 0.002847, mae: 0.051483, mean_q: -0.316226
 82400/100000: episode: 824, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -18.512, mean reward: -0.185 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.949, 10.202], loss: 0.002841, mae: 0.051775, mean_q: -0.315055
 82500/100000: episode: 825, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -10.859, mean reward: -0.109 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.607, 10.319], loss: 0.002633, mae: 0.049396, mean_q: -0.352401
 82600/100000: episode: 826, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.541, mean reward: -0.165 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.280, 10.349], loss: 0.002890, mae: 0.052819, mean_q: -0.321728
 82700/100000: episode: 827, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.778, mean reward: -0.188 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.226, 10.098], loss: 0.002881, mae: 0.052153, mean_q: -0.349443
 82800/100000: episode: 828, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.631, mean reward: -0.176 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.311, 10.098], loss: 0.002849, mae: 0.054238, mean_q: -0.320664
 82900/100000: episode: 829, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.093, mean reward: -0.171 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.838, 10.098], loss: 0.002698, mae: 0.051524, mean_q: -0.347237
 83000/100000: episode: 830, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.684, mean reward: -0.187 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.023, 10.305], loss: 0.002733, mae: 0.050907, mean_q: -0.284755
 83100/100000: episode: 831, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -15.321, mean reward: -0.153 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.646, 10.141], loss: 0.002377, mae: 0.047317, mean_q: -0.329701
 83200/100000: episode: 832, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.042, mean reward: -0.170 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.475, 10.098], loss: 0.002785, mae: 0.050252, mean_q: -0.350013
 83300/100000: episode: 833, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.315, mean reward: -0.183 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.581, 10.098], loss: 0.002876, mae: 0.052331, mean_q: -0.286832
 83400/100000: episode: 834, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.059, mean reward: -0.131 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.805, 10.098], loss: 0.002915, mae: 0.051424, mean_q: -0.338628
 83500/100000: episode: 835, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.328, mean reward: -0.163 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.028, 10.098], loss: 0.002740, mae: 0.050003, mean_q: -0.314858
 83600/100000: episode: 836, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.448, mean reward: -0.184 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.626, 10.098], loss: 0.002654, mae: 0.050536, mean_q: -0.287599
 83700/100000: episode: 837, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.625, mean reward: -0.176 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.736, 10.266], loss: 0.002897, mae: 0.051932, mean_q: -0.296972
 83800/100000: episode: 838, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.328, mean reward: -0.183 [-1.000, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.888, 10.236], loss: 0.002640, mae: 0.049866, mean_q: -0.317966
 83900/100000: episode: 839, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -13.706, mean reward: -0.137 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.574, 10.512], loss: 0.002728, mae: 0.050219, mean_q: -0.345432
 84000/100000: episode: 840, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -20.016, mean reward: -0.200 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.354, 10.098], loss: 0.002723, mae: 0.050843, mean_q: -0.317803
 84100/100000: episode: 841, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -16.920, mean reward: -0.169 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.826, 10.098], loss: 0.002779, mae: 0.051954, mean_q: -0.339090
 84200/100000: episode: 842, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.720, mean reward: -0.187 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.287, 10.098], loss: 0.002781, mae: 0.050822, mean_q: -0.313014
 84300/100000: episode: 843, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.699, mean reward: -0.167 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.796, 10.119], loss: 0.002965, mae: 0.052817, mean_q: -0.348580
 84400/100000: episode: 844, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.070, mean reward: -0.181 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.478, 10.098], loss: 0.002687, mae: 0.050249, mean_q: -0.301408
 84500/100000: episode: 845, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.387, mean reward: -0.144 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.993, 10.098], loss: 0.002840, mae: 0.052194, mean_q: -0.303676
 84600/100000: episode: 846, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.786, mean reward: -0.178 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.606, 10.114], loss: 0.002660, mae: 0.050113, mean_q: -0.328550
 84700/100000: episode: 847, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.726, mean reward: -0.187 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.730, 10.098], loss: 0.002755, mae: 0.051349, mean_q: -0.324770
 84800/100000: episode: 848, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.517, mean reward: -0.185 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.290, 10.249], loss: 0.002723, mae: 0.050400, mean_q: -0.330827
 84900/100000: episode: 849, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -19.121, mean reward: -0.191 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.665, 10.165], loss: 0.002534, mae: 0.049147, mean_q: -0.308693
 85000/100000: episode: 850, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.153, mean reward: -0.152 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.182, 10.275], loss: 0.002600, mae: 0.049487, mean_q: -0.338122
 85100/100000: episode: 851, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -18.662, mean reward: -0.187 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.016, 10.125], loss: 0.002837, mae: 0.052274, mean_q: -0.297388
 85200/100000: episode: 852, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -17.365, mean reward: -0.174 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.629, 10.218], loss: 0.002631, mae: 0.050858, mean_q: -0.316698
 85300/100000: episode: 853, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -10.465, mean reward: -0.105 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.788, 10.098], loss: 0.006961, mae: 0.076853, mean_q: -0.307073
 85400/100000: episode: 854, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.559, mean reward: -0.176 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.102, 10.125], loss: 0.002735, mae: 0.051814, mean_q: -0.359456
 85500/100000: episode: 855, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -15.886, mean reward: -0.159 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.536, 10.219], loss: 0.002666, mae: 0.051018, mean_q: -0.319213
 85600/100000: episode: 856, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -14.450, mean reward: -0.145 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.229, 10.222], loss: 0.002490, mae: 0.048478, mean_q: -0.311225
 85700/100000: episode: 857, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.146, mean reward: -0.151 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.772, 10.098], loss: 0.002587, mae: 0.049873, mean_q: -0.306451
 85800/100000: episode: 858, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.385, mean reward: -0.194 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.881, 10.098], loss: 0.002493, mae: 0.048747, mean_q: -0.295105
 85900/100000: episode: 859, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -14.451, mean reward: -0.145 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.383, 10.339], loss: 0.002625, mae: 0.049914, mean_q: -0.322377
 86000/100000: episode: 860, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -13.339, mean reward: -0.133 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.823, 10.345], loss: 0.002716, mae: 0.051141, mean_q: -0.326119
 86100/100000: episode: 861, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -19.007, mean reward: -0.190 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.483, 10.098], loss: 0.002692, mae: 0.049989, mean_q: -0.320409
 86200/100000: episode: 862, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -15.413, mean reward: -0.154 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.356, 10.123], loss: 0.002363, mae: 0.046499, mean_q: -0.360651
 86300/100000: episode: 863, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: -17.417, mean reward: -0.174 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.775, 10.098], loss: 0.002800, mae: 0.050861, mean_q: -0.339824
 86400/100000: episode: 864, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -19.566, mean reward: -0.196 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.039, 10.098], loss: 0.002735, mae: 0.050774, mean_q: -0.302763
 86500/100000: episode: 865, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -20.316, mean reward: -0.203 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.426, 10.187], loss: 0.002431, mae: 0.048419, mean_q: -0.330993
 86600/100000: episode: 866, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -17.970, mean reward: -0.180 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.551, 10.304], loss: 0.002775, mae: 0.050791, mean_q: -0.298901
 86700/100000: episode: 867, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -17.771, mean reward: -0.178 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.744, 10.141], loss: 0.002434, mae: 0.048350, mean_q: -0.309914
 86800/100000: episode: 868, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.204, mean reward: -0.152 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.570, 10.098], loss: 0.002557, mae: 0.049801, mean_q: -0.311471
 86900/100000: episode: 869, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.086, mean reward: -0.131 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.384, 10.353], loss: 0.002445, mae: 0.049444, mean_q: -0.342621
 87000/100000: episode: 870, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -17.546, mean reward: -0.175 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.831, 10.127], loss: 0.002503, mae: 0.048480, mean_q: -0.312727
 87100/100000: episode: 871, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -13.530, mean reward: -0.135 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.255, 10.327], loss: 0.002515, mae: 0.049668, mean_q: -0.324582
 87200/100000: episode: 872, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -11.931, mean reward: -0.119 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.730, 10.367], loss: 0.002650, mae: 0.050602, mean_q: -0.321020
 87300/100000: episode: 873, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -16.189, mean reward: -0.162 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.647, 10.232], loss: 0.002578, mae: 0.049677, mean_q: -0.334782
 87400/100000: episode: 874, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -18.053, mean reward: -0.181 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.840, 10.098], loss: 0.002621, mae: 0.049111, mean_q: -0.343158
 87500/100000: episode: 875, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -10.791, mean reward: -0.108 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.584, 10.437], loss: 0.002777, mae: 0.052194, mean_q: -0.312080
 87600/100000: episode: 876, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.458, mean reward: -0.185 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.088, 10.109], loss: 0.002774, mae: 0.052943, mean_q: -0.295119
 87700/100000: episode: 877, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -17.323, mean reward: -0.173 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.476, 10.100], loss: 0.007708, mae: 0.076098, mean_q: -0.312637
 87800/100000: episode: 878, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -19.349, mean reward: -0.193 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.244, 10.098], loss: 0.002794, mae: 0.053665, mean_q: -0.327795
 87900/100000: episode: 879, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.801, mean reward: -0.168 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.076, 10.098], loss: 0.002488, mae: 0.047637, mean_q: -0.384716
 88000/100000: episode: 880, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.216, mean reward: -0.172 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.487, 10.098], loss: 0.002638, mae: 0.050460, mean_q: -0.315331
 88100/100000: episode: 881, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -13.986, mean reward: -0.140 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.914, 10.101], loss: 0.002757, mae: 0.051902, mean_q: -0.317023
 88200/100000: episode: 882, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.495, mean reward: -0.185 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.223, 10.190], loss: 0.002634, mae: 0.050546, mean_q: -0.294772
 88300/100000: episode: 883, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.227, mean reward: -0.162 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.163, 10.098], loss: 0.002593, mae: 0.049578, mean_q: -0.308214
 88400/100000: episode: 884, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.154, mean reward: -0.162 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.808, 10.236], loss: 0.002639, mae: 0.050087, mean_q: -0.319654
 88500/100000: episode: 885, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.945, mean reward: -0.149 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.235, 10.098], loss: 0.002478, mae: 0.048588, mean_q: -0.302551
 88600/100000: episode: 886, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -19.482, mean reward: -0.195 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.346, 10.204], loss: 0.002688, mae: 0.050231, mean_q: -0.304171
 88700/100000: episode: 887, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -20.329, mean reward: -0.203 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.162, 10.098], loss: 0.002541, mae: 0.048912, mean_q: -0.352518
 88800/100000: episode: 888, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.296, mean reward: -0.173 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.012, 10.263], loss: 0.002505, mae: 0.048759, mean_q: -0.322475
 88900/100000: episode: 889, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.144, mean reward: -0.161 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.765, 10.311], loss: 0.002510, mae: 0.048902, mean_q: -0.292207
 89000/100000: episode: 890, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.728, mean reward: -0.177 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.578, 10.098], loss: 0.002775, mae: 0.050154, mean_q: -0.334586
 89100/100000: episode: 891, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.786, mean reward: -0.178 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.167, 10.173], loss: 0.002544, mae: 0.048325, mean_q: -0.351400
 89200/100000: episode: 892, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.839, mean reward: -0.178 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.394, 10.098], loss: 0.002889, mae: 0.051474, mean_q: -0.317136
 89300/100000: episode: 893, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.215, mean reward: -0.152 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.220, 10.098], loss: 0.002678, mae: 0.050575, mean_q: -0.302101
 89400/100000: episode: 894, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.933, mean reward: -0.179 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.680, 10.106], loss: 0.002544, mae: 0.048455, mean_q: -0.314848
 89500/100000: episode: 895, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.015, mean reward: -0.160 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.631, 10.098], loss: 0.002652, mae: 0.049818, mean_q: -0.342269
 89600/100000: episode: 896, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -15.739, mean reward: -0.157 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.432, 10.164], loss: 0.002707, mae: 0.051495, mean_q: -0.296475
 89700/100000: episode: 897, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.288, mean reward: -0.183 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.515, 10.110], loss: 0.002531, mae: 0.050239, mean_q: -0.289724
 89800/100000: episode: 898, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.779, mean reward: -0.158 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.792, 10.098], loss: 0.002435, mae: 0.047688, mean_q: -0.277191
 89900/100000: episode: 899, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.749, mean reward: -0.197 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.306, 10.169], loss: 0.002642, mae: 0.050839, mean_q: -0.309507
 90000/100000: episode: 900, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -15.790, mean reward: -0.158 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.859, 10.249], loss: 0.002912, mae: 0.053187, mean_q: -0.326615
 90100/100000: episode: 901, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -13.111, mean reward: -0.131 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.986, 10.288], loss: 0.002609, mae: 0.049343, mean_q: -0.305335
 90200/100000: episode: 902, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.402, mean reward: -0.154 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.797, 10.098], loss: 0.002577, mae: 0.049738, mean_q: -0.318313
 90300/100000: episode: 903, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.021, mean reward: -0.180 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.944, 10.098], loss: 0.002515, mae: 0.049621, mean_q: -0.317329
 90400/100000: episode: 904, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -10.971, mean reward: -0.110 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.915, 10.098], loss: 0.002518, mae: 0.048690, mean_q: -0.292921
 90500/100000: episode: 905, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.664, mean reward: -0.167 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.523, 10.098], loss: 0.002601, mae: 0.048799, mean_q: -0.312151
 90600/100000: episode: 906, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.945, mean reward: -0.169 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.293, 10.123], loss: 0.002853, mae: 0.052542, mean_q: -0.330893
 90700/100000: episode: 907, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.690, mean reward: -0.187 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.273, 10.098], loss: 0.005886, mae: 0.071908, mean_q: -0.307550
 90800/100000: episode: 908, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -15.274, mean reward: -0.153 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.789, 10.098], loss: 0.002675, mae: 0.051078, mean_q: -0.317744
 90900/100000: episode: 909, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.171, mean reward: -0.172 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.172, 10.098], loss: 0.002639, mae: 0.049822, mean_q: -0.289038
 91000/100000: episode: 910, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.855, mean reward: -0.179 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.885, 10.190], loss: 0.002573, mae: 0.048277, mean_q: -0.325119
 91100/100000: episode: 911, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -18.049, mean reward: -0.180 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.429, 10.243], loss: 0.002417, mae: 0.047927, mean_q: -0.297056
 91200/100000: episode: 912, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -13.890, mean reward: -0.139 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.641, 10.098], loss: 0.002479, mae: 0.048276, mean_q: -0.318789
 91300/100000: episode: 913, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -17.115, mean reward: -0.171 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.671, 10.116], loss: 0.002516, mae: 0.048944, mean_q: -0.306042
 91400/100000: episode: 914, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.019, mean reward: -0.180 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.257, 10.389], loss: 0.002553, mae: 0.049516, mean_q: -0.312747
 91500/100000: episode: 915, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.480, mean reward: -0.195 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.799, 10.098], loss: 0.002428, mae: 0.047556, mean_q: -0.314871
 91600/100000: episode: 916, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -5.492, mean reward: -0.055 [-1.000, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.452, 10.098], loss: 0.002467, mae: 0.046901, mean_q: -0.328665
 91700/100000: episode: 917, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -8.626, mean reward: -0.086 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.146, 10.395], loss: 0.002623, mae: 0.049596, mean_q: -0.302743
 91800/100000: episode: 918, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.207, mean reward: -0.172 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.897, 10.098], loss: 0.002610, mae: 0.048850, mean_q: -0.319145
 91900/100000: episode: 919, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.414, mean reward: -0.174 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.486, 10.098], loss: 0.002537, mae: 0.047631, mean_q: -0.328445
 92000/100000: episode: 920, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -20.062, mean reward: -0.201 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.432, 10.106], loss: 0.002558, mae: 0.048265, mean_q: -0.312914
 92100/100000: episode: 921, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -14.839, mean reward: -0.148 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.693, 10.098], loss: 0.002732, mae: 0.050116, mean_q: -0.313643
 92200/100000: episode: 922, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.332, mean reward: -0.163 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.461, 10.158], loss: 0.002438, mae: 0.047547, mean_q: -0.339683
 92300/100000: episode: 923, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -18.615, mean reward: -0.186 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.681, 10.098], loss: 0.002510, mae: 0.048568, mean_q: -0.305563
 92400/100000: episode: 924, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.542, mean reward: -0.185 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.161, 10.098], loss: 0.002664, mae: 0.050212, mean_q: -0.308206
 92500/100000: episode: 925, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.945, mean reward: -0.169 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.590, 10.098], loss: 0.002686, mae: 0.050612, mean_q: -0.317872
 92600/100000: episode: 926, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.508, mean reward: -0.175 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.908, 10.455], loss: 0.002369, mae: 0.047489, mean_q: -0.333168
 92700/100000: episode: 927, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.824, mean reward: -0.158 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.657, 10.316], loss: 0.002421, mae: 0.046563, mean_q: -0.324773
 92800/100000: episode: 928, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.156, mean reward: -0.192 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.199, 10.213], loss: 0.002849, mae: 0.052282, mean_q: -0.318083
 92900/100000: episode: 929, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -11.328, mean reward: -0.113 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-1.251, 10.098], loss: 0.002762, mae: 0.050828, mean_q: -0.332180
 93000/100000: episode: 930, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -16.025, mean reward: -0.160 [-1.000, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.735, 10.460], loss: 0.002530, mae: 0.049868, mean_q: -0.298749
 93100/100000: episode: 931, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -16.887, mean reward: -0.169 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.935, 10.278], loss: 0.002589, mae: 0.049384, mean_q: -0.334865
 93200/100000: episode: 932, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -12.491, mean reward: -0.125 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.394, 10.098], loss: 0.002700, mae: 0.050410, mean_q: -0.286401
 93300/100000: episode: 933, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.567, mean reward: -0.166 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.957, 10.406], loss: 0.002768, mae: 0.050564, mean_q: -0.313885
 93400/100000: episode: 934, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.806, mean reward: -0.188 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.645, 10.300], loss: 0.002494, mae: 0.048963, mean_q: -0.314764
 93500/100000: episode: 935, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.456, mean reward: -0.185 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.223, 10.261], loss: 0.002630, mae: 0.050654, mean_q: -0.295013
 93600/100000: episode: 936, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -15.911, mean reward: -0.159 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.974, 10.221], loss: 0.002735, mae: 0.050508, mean_q: -0.287540
 93700/100000: episode: 937, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -16.444, mean reward: -0.164 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.473, 10.098], loss: 0.002738, mae: 0.051116, mean_q: -0.266982
 93800/100000: episode: 938, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -15.545, mean reward: -0.155 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.596, 10.211], loss: 0.002496, mae: 0.048656, mean_q: -0.312189
 93900/100000: episode: 939, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.076, mean reward: -0.191 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.020, 10.098], loss: 0.002593, mae: 0.048872, mean_q: -0.309071
 94000/100000: episode: 940, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.766, mean reward: -0.168 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.361, 10.263], loss: 0.002716, mae: 0.052454, mean_q: -0.303914
 94100/100000: episode: 941, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.990, mean reward: -0.170 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.780, 10.098], loss: 0.002870, mae: 0.051992, mean_q: -0.306605
 94200/100000: episode: 942, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -9.463, mean reward: -0.095 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.572, 10.501], loss: 0.006598, mae: 0.074916, mean_q: -0.324725
 94300/100000: episode: 943, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.031, mean reward: -0.180 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.564, 10.098], loss: 0.003042, mae: 0.055281, mean_q: -0.317368
 94400/100000: episode: 944, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.740, mean reward: -0.177 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.223, 10.117], loss: 0.002856, mae: 0.051635, mean_q: -0.321249
 94500/100000: episode: 945, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.154, mean reward: -0.152 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.729, 10.230], loss: 0.002701, mae: 0.050399, mean_q: -0.304221
 94600/100000: episode: 946, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.643, mean reward: -0.186 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.714, 10.444], loss: 0.002601, mae: 0.049265, mean_q: -0.315729
 94700/100000: episode: 947, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.914, mean reward: -0.159 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.495, 10.098], loss: 0.002664, mae: 0.050029, mean_q: -0.278944
 94800/100000: episode: 948, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.675, mean reward: -0.177 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.804, 10.098], loss: 0.002774, mae: 0.051330, mean_q: -0.271750
 94900/100000: episode: 949, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.379, mean reward: -0.164 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.492, 10.098], loss: 0.002772, mae: 0.049488, mean_q: -0.336236
 95000/100000: episode: 950, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.114, mean reward: -0.181 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.036, 10.098], loss: 0.002554, mae: 0.049116, mean_q: -0.315356
 95100/100000: episode: 951, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -12.868, mean reward: -0.129 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.294, 10.098], loss: 0.002757, mae: 0.050878, mean_q: -0.292321
 95200/100000: episode: 952, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -14.305, mean reward: -0.143 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.569, 10.271], loss: 0.002505, mae: 0.048688, mean_q: -0.299479
 95300/100000: episode: 953, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.233, mean reward: -0.162 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.693, 10.260], loss: 0.002667, mae: 0.049799, mean_q: -0.312997
 95400/100000: episode: 954, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.894, mean reward: -0.189 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.815, 10.230], loss: 0.002603, mae: 0.048334, mean_q: -0.346498
 95500/100000: episode: 955, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -16.111, mean reward: -0.161 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.642, 10.098], loss: 0.002476, mae: 0.048444, mean_q: -0.285769
 95600/100000: episode: 956, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -19.400, mean reward: -0.194 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.355, 10.099], loss: 0.002678, mae: 0.050108, mean_q: -0.319851
 95700/100000: episode: 957, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.713, mean reward: -0.177 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.463, 10.098], loss: 0.002792, mae: 0.052153, mean_q: -0.299158
 95800/100000: episode: 958, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -13.598, mean reward: -0.136 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.578, 10.286], loss: 0.002698, mae: 0.050366, mean_q: -0.301951
 95900/100000: episode: 959, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.954, mean reward: -0.170 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.814, 10.252], loss: 0.002600, mae: 0.049672, mean_q: -0.295424
 96000/100000: episode: 960, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.971, mean reward: -0.190 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.246, 10.114], loss: 0.002635, mae: 0.049561, mean_q: -0.332267
 96100/100000: episode: 961, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -15.175, mean reward: -0.152 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.495, 10.098], loss: 0.002697, mae: 0.050357, mean_q: -0.301243
 96200/100000: episode: 962, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.448, mean reward: -0.164 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.173, 10.103], loss: 0.002856, mae: 0.051945, mean_q: -0.317046
 96300/100000: episode: 963, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -14.989, mean reward: -0.150 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.994, 10.289], loss: 0.002927, mae: 0.052856, mean_q: -0.302976
 96400/100000: episode: 964, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -16.283, mean reward: -0.163 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.850, 10.098], loss: 0.002655, mae: 0.050163, mean_q: -0.331938
 96500/100000: episode: 965, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -16.392, mean reward: -0.164 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.544, 10.206], loss: 0.002823, mae: 0.052691, mean_q: -0.299499
 96600/100000: episode: 966, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -17.212, mean reward: -0.172 [-1.000, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.291, 10.188], loss: 0.002575, mae: 0.050158, mean_q: -0.306881
 96700/100000: episode: 967, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.653, mean reward: -0.187 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.361, 10.098], loss: 0.002656, mae: 0.049814, mean_q: -0.327646
 96800/100000: episode: 968, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -20.734, mean reward: -0.207 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.542, 10.098], loss: 0.002699, mae: 0.050333, mean_q: -0.321228
 96900/100000: episode: 969, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -17.109, mean reward: -0.171 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.364, 10.098], loss: 0.002532, mae: 0.048215, mean_q: -0.336566
 97000/100000: episode: 970, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.639, mean reward: -0.176 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.668, 10.098], loss: 0.002828, mae: 0.051375, mean_q: -0.312451
 97100/100000: episode: 971, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -15.155, mean reward: -0.152 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.713, 10.098], loss: 0.002600, mae: 0.049582, mean_q: -0.310595
 97200/100000: episode: 972, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -18.358, mean reward: -0.184 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.283, 10.442], loss: 0.002521, mae: 0.049144, mean_q: -0.320944
 97300/100000: episode: 973, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.286, mean reward: -0.173 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.678, 10.098], loss: 0.002675, mae: 0.050503, mean_q: -0.302476
 97400/100000: episode: 974, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -13.333, mean reward: -0.133 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.944, 10.098], loss: 0.002711, mae: 0.050618, mean_q: -0.328648
 97500/100000: episode: 975, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -15.951, mean reward: -0.160 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.149, 10.197], loss: 0.002722, mae: 0.050321, mean_q: -0.295876
 97600/100000: episode: 976, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -11.970, mean reward: -0.120 [-1.000, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.538, 10.497], loss: 0.002832, mae: 0.052060, mean_q: -0.298549
 97700/100000: episode: 977, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -18.518, mean reward: -0.185 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.525, 10.138], loss: 0.002549, mae: 0.050777, mean_q: -0.287629
 97800/100000: episode: 978, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.101, mean reward: -0.151 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.994, 10.300], loss: 0.002609, mae: 0.050918, mean_q: -0.309457
 97900/100000: episode: 979, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -15.119, mean reward: -0.151 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.549, 10.098], loss: 0.003117, mae: 0.054800, mean_q: -0.323651
 98000/100000: episode: 980, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -16.892, mean reward: -0.169 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.178, 10.118], loss: 0.002863, mae: 0.055631, mean_q: -0.313666
 98100/100000: episode: 981, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -19.295, mean reward: -0.193 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.452, 10.098], loss: 0.002929, mae: 0.054213, mean_q: -0.300083
 98200/100000: episode: 982, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.011, mean reward: -0.180 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.051, 10.158], loss: 0.002752, mae: 0.051388, mean_q: -0.318913
 98300/100000: episode: 983, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.457, mean reward: -0.195 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.997, 10.244], loss: 0.002656, mae: 0.049826, mean_q: -0.329856
 98400/100000: episode: 984, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -12.870, mean reward: -0.129 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.365, 10.098], loss: 0.002556, mae: 0.050030, mean_q: -0.299902
 98500/100000: episode: 985, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -19.517, mean reward: -0.195 [-1.000, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.683, 10.098], loss: 0.002697, mae: 0.050397, mean_q: -0.354222
 98600/100000: episode: 986, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.033, mean reward: -0.180 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.240, 10.098], loss: 0.002648, mae: 0.051607, mean_q: -0.335316
 98700/100000: episode: 987, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.684, mean reward: -0.127 [-1.000, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.032, 10.146], loss: 0.003716, mae: 0.058989, mean_q: -0.299256
 98800/100000: episode: 988, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.782, mean reward: -0.178 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.223, 10.115], loss: 0.003161, mae: 0.057723, mean_q: -0.293465
 98900/100000: episode: 989, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -20.477, mean reward: -0.205 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.798, 10.128], loss: 0.002797, mae: 0.051842, mean_q: -0.302525
 99000/100000: episode: 990, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.053, mean reward: -0.181 [-1.000, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.340, 10.243], loss: 0.002750, mae: 0.051026, mean_q: -0.339127
 99100/100000: episode: 991, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -19.065, mean reward: -0.191 [-1.000, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.293, 10.098], loss: 0.002717, mae: 0.051143, mean_q: -0.278217
 99200/100000: episode: 992, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -19.011, mean reward: -0.190 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.439, 10.127], loss: 0.002710, mae: 0.050863, mean_q: -0.305361
 99300/100000: episode: 993, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -20.983, mean reward: -0.210 [-1.000, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.604, 10.098], loss: 0.002829, mae: 0.051685, mean_q: -0.282595
 99400/100000: episode: 994, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -15.318, mean reward: -0.153 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.054, 10.098], loss: 0.002777, mae: 0.050785, mean_q: -0.314980
 99500/100000: episode: 995, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.823, mean reward: -0.188 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.513, 10.098], loss: 0.002794, mae: 0.051813, mean_q: -0.286977
 99600/100000: episode: 996, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.394, mean reward: -0.164 [-1.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.184, 10.098], loss: 0.002731, mae: 0.049954, mean_q: -0.342418
 99700/100000: episode: 997, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -16.446, mean reward: -0.164 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.867, 10.224], loss: 0.002697, mae: 0.049953, mean_q: -0.336240
 99800/100000: episode: 998, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.402, mean reward: -0.154 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.783, 10.232], loss: 0.002937, mae: 0.051559, mean_q: -0.326941
 99900/100000: episode: 999, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.395, mean reward: -0.184 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.208, 10.123], loss: 0.002641, mae: 0.049585, mean_q: -0.333473
 100000/100000: episode: 1000, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.139, mean reward: -0.151 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.431, 10.098], loss: 0.002878, mae: 0.052707, mean_q: -0.301716
done, took 524.924 seconds
[Info] End Uniform Random Simulation. Falsification occurred 0 times.
