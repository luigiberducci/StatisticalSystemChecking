Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.173s, episode steps: 100, steps per second: 577, episode reward: -18.117, mean reward: -0.181 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.507, 10.154], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.064s, episode steps: 100, steps per second: 1566, episode reward: -13.982, mean reward: -0.140 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.939, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.063s, episode steps: 100, steps per second: 1598, episode reward: -16.549, mean reward: -0.165 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.406, 10.289], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.063s, episode steps: 100, steps per second: 1578, episode reward: -18.804, mean reward: -0.188 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.257, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.062s, episode steps: 100, steps per second: 1602, episode reward: -17.493, mean reward: -0.175 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.466, 10.295], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.188s, episode steps: 100, steps per second: 84, episode reward: -18.132, mean reward: -0.181 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.901, 10.152], loss: 0.076717, mae: 0.276327, mean_q: 0.200044
   700/100000: episode: 7, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.240, mean reward: -0.172 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.914, 10.098], loss: 0.020317, mae: 0.141089, mean_q: -0.069192
   800/100000: episode: 8, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -14.540, mean reward: -0.145 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.590, 10.166], loss: 0.012651, mae: 0.107638, mean_q: -0.166781
   900/100000: episode: 9, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.868, mean reward: -0.169 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.103, 10.177], loss: 0.009405, mae: 0.093963, mean_q: -0.216367
  1000/100000: episode: 10, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.278, mean reward: -0.173 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.203, 10.112], loss: 0.009116, mae: 0.091355, mean_q: -0.312191
  1100/100000: episode: 11, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -12.837, mean reward: -0.128 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.083, 10.278], loss: 0.008543, mae: 0.087195, mean_q: -0.275539
  1200/100000: episode: 12, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.365, mean reward: -0.174 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.212, 10.098], loss: 0.008100, mae: 0.085224, mean_q: -0.287481
  1300/100000: episode: 13, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.759, mean reward: -0.178 [-1.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.424, 10.147], loss: 0.008185, mae: 0.084223, mean_q: -0.286915
  1400/100000: episode: 14, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.412, mean reward: -0.154 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.155, 10.346], loss: 0.006609, mae: 0.078198, mean_q: -0.329123
  1500/100000: episode: 15, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.884, mean reward: -0.159 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.748, 10.119], loss: 0.008377, mae: 0.087878, mean_q: -0.337608
  1600/100000: episode: 16, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.737, mean reward: -0.197 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.121, 10.244], loss: 0.007602, mae: 0.084590, mean_q: -0.354568
  1700/100000: episode: 17, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -10.407, mean reward: -0.104 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.678, 10.098], loss: 0.006480, mae: 0.078082, mean_q: -0.317145
  1800/100000: episode: 18, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.917, mean reward: -0.169 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.046, 10.257], loss: 0.006721, mae: 0.081812, mean_q: -0.287956
  1900/100000: episode: 19, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -18.073, mean reward: -0.181 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.488, 10.098], loss: 0.007377, mae: 0.083783, mean_q: -0.316467
  2000/100000: episode: 20, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.418, mean reward: -0.174 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.751, 10.135], loss: 0.007333, mae: 0.082954, mean_q: -0.307202
  2100/100000: episode: 21, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.187, mean reward: -0.172 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.607, 10.098], loss: 0.006984, mae: 0.078775, mean_q: -0.307824
  2200/100000: episode: 22, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.197, mean reward: -0.182 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.524, 10.098], loss: 0.006040, mae: 0.074370, mean_q: -0.337285
  2300/100000: episode: 23, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -14.664, mean reward: -0.147 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.208, 10.098], loss: 0.007427, mae: 0.082122, mean_q: -0.295434
  2400/100000: episode: 24, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.169, mean reward: -0.162 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.825, 10.098], loss: 0.005870, mae: 0.076615, mean_q: -0.307345
  2500/100000: episode: 25, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.669, mean reward: -0.157 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.400, 10.098], loss: 0.005160, mae: 0.070206, mean_q: -0.317057
  2600/100000: episode: 26, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -11.901, mean reward: -0.119 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.655, 10.098], loss: 0.006300, mae: 0.077726, mean_q: -0.323106
  2700/100000: episode: 27, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.865, mean reward: -0.169 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.139, 10.098], loss: 0.005857, mae: 0.074551, mean_q: -0.329439
  2800/100000: episode: 28, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.669, mean reward: -0.187 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.579, 10.173], loss: 0.005414, mae: 0.073713, mean_q: -0.319538
  2900/100000: episode: 29, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.785, mean reward: -0.188 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.983, 10.112], loss: 0.005589, mae: 0.073263, mean_q: -0.300511
  3000/100000: episode: 30, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -9.238, mean reward: -0.092 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.618, 10.538], loss: 0.005586, mae: 0.074664, mean_q: -0.317781
  3100/100000: episode: 31, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -15.446, mean reward: -0.154 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.864, 10.098], loss: 0.005975, mae: 0.075122, mean_q: -0.333694
  3200/100000: episode: 32, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -17.111, mean reward: -0.171 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.237, 10.098], loss: 0.005588, mae: 0.073763, mean_q: -0.327961
  3300/100000: episode: 33, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.825, mean reward: -0.178 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.525, 10.098], loss: 0.005874, mae: 0.074800, mean_q: -0.310005
  3400/100000: episode: 34, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.860, mean reward: -0.169 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.430, 10.250], loss: 0.005674, mae: 0.072807, mean_q: -0.310144
  3500/100000: episode: 35, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -15.974, mean reward: -0.160 [-1.000, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.524, 10.098], loss: 0.005154, mae: 0.068874, mean_q: -0.334458
  3600/100000: episode: 36, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.737, mean reward: -0.147 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.636, 10.106], loss: 0.005780, mae: 0.073331, mean_q: -0.329223
  3700/100000: episode: 37, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.006, mean reward: -0.160 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.195, 10.098], loss: 0.004981, mae: 0.069183, mean_q: -0.280413
  3800/100000: episode: 38, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.795, mean reward: -0.178 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.778, 10.098], loss: 0.004669, mae: 0.067205, mean_q: -0.332380
  3900/100000: episode: 39, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -8.118, mean reward: -0.081 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.300, 10.349], loss: 0.004675, mae: 0.068004, mean_q: -0.290223
  4000/100000: episode: 40, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.542, mean reward: -0.145 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.632, 10.098], loss: 0.004952, mae: 0.071031, mean_q: -0.306846
  4100/100000: episode: 41, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.996, mean reward: -0.160 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.973, 10.284], loss: 0.004372, mae: 0.066451, mean_q: -0.311962
  4200/100000: episode: 42, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -15.931, mean reward: -0.159 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.506, 10.098], loss: 0.005193, mae: 0.071829, mean_q: -0.297877
  4300/100000: episode: 43, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -19.400, mean reward: -0.194 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.956, 10.098], loss: 0.004691, mae: 0.069629, mean_q: -0.301398
  4400/100000: episode: 44, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.048, mean reward: -0.180 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.648, 10.168], loss: 0.004900, mae: 0.070372, mean_q: -0.319875
  4500/100000: episode: 45, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.443, mean reward: -0.184 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.667, 10.141], loss: 0.004501, mae: 0.068756, mean_q: -0.330994
  4600/100000: episode: 46, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -15.285, mean reward: -0.153 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.713, 10.098], loss: 0.005162, mae: 0.072253, mean_q: -0.287102
  4700/100000: episode: 47, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.750, mean reward: -0.188 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.047, 10.098], loss: 0.004384, mae: 0.066318, mean_q: -0.293922
  4800/100000: episode: 48, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.096, mean reward: -0.191 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.578, 10.172], loss: 0.005221, mae: 0.071998, mean_q: -0.343793
  4900/100000: episode: 49, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.599, mean reward: -0.176 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.561, 10.098], loss: 0.004175, mae: 0.065201, mean_q: -0.316740
  5000/100000: episode: 50, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: -16.964, mean reward: -0.170 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.512, 10.421], loss: 0.004664, mae: 0.069702, mean_q: -0.311756
  5100/100000: episode: 51, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.970, mean reward: -0.160 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.818, 10.098], loss: 0.004547, mae: 0.068423, mean_q: -0.280991
  5200/100000: episode: 52, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.587, mean reward: -0.206 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.544, 10.098], loss: 0.004272, mae: 0.066844, mean_q: -0.303002
  5300/100000: episode: 53, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.242, mean reward: -0.182 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.938, 10.107], loss: 0.005653, mae: 0.075347, mean_q: -0.297083
  5400/100000: episode: 54, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.567, mean reward: -0.156 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.689, 10.134], loss: 0.004340, mae: 0.067468, mean_q: -0.308081
  5500/100000: episode: 55, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: -16.872, mean reward: -0.169 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.012, 10.098], loss: 0.004976, mae: 0.070393, mean_q: -0.313404
  5600/100000: episode: 56, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -14.029, mean reward: -0.140 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.089, 10.098], loss: 0.005019, mae: 0.074283, mean_q: -0.291379
  5700/100000: episode: 57, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.923, mean reward: -0.179 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.425, 10.098], loss: 0.005014, mae: 0.072232, mean_q: -0.307745
  5800/100000: episode: 58, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.006, mean reward: -0.170 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.740, 10.140], loss: 0.004295, mae: 0.067152, mean_q: -0.329203
  5900/100000: episode: 59, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -20.048, mean reward: -0.200 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.854, 10.098], loss: 0.006071, mae: 0.077442, mean_q: -0.333158
  6000/100000: episode: 60, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.494, mean reward: -0.175 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.642, 10.348], loss: 0.003811, mae: 0.062365, mean_q: -0.334256
  6100/100000: episode: 61, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.367, mean reward: -0.154 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.047, 10.111], loss: 0.004859, mae: 0.068919, mean_q: -0.308410
  6200/100000: episode: 62, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -12.121, mean reward: -0.121 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.038, 10.098], loss: 0.004236, mae: 0.066068, mean_q: -0.324336
  6300/100000: episode: 63, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -13.956, mean reward: -0.140 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-2.514, 10.098], loss: 0.004194, mae: 0.065546, mean_q: -0.307586
  6400/100000: episode: 64, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.440, mean reward: -0.184 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.662, 10.199], loss: 0.004661, mae: 0.065897, mean_q: -0.281118
  6500/100000: episode: 65, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.506, mean reward: -0.185 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.852, 10.311], loss: 0.004278, mae: 0.066901, mean_q: -0.294042
  6600/100000: episode: 66, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.987, mean reward: -0.150 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.490, 10.098], loss: 0.004079, mae: 0.065744, mean_q: -0.284604
  6700/100000: episode: 67, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -12.772, mean reward: -0.128 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.398, 10.098], loss: 0.004447, mae: 0.067464, mean_q: -0.322135
  6800/100000: episode: 68, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -11.148, mean reward: -0.111 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.640, 10.365], loss: 0.006143, mae: 0.077518, mean_q: -0.291837
  6900/100000: episode: 69, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.196, mean reward: -0.152 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.542, 10.309], loss: 0.004579, mae: 0.068157, mean_q: -0.289137
  7000/100000: episode: 70, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -14.752, mean reward: -0.148 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.827, 10.098], loss: 0.003901, mae: 0.063840, mean_q: -0.310104
  7100/100000: episode: 71, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -14.939, mean reward: -0.149 [-1.000, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.660, 10.224], loss: 0.003912, mae: 0.065455, mean_q: -0.313801
  7200/100000: episode: 72, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.892, mean reward: -0.179 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.302, 10.195], loss: 0.003953, mae: 0.064946, mean_q: -0.283679
  7300/100000: episode: 73, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -20.193, mean reward: -0.202 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.671, 10.204], loss: 0.003773, mae: 0.063441, mean_q: -0.286986
  7400/100000: episode: 74, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.915, mean reward: -0.189 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.769, 10.098], loss: 0.003698, mae: 0.062335, mean_q: -0.308607
  7500/100000: episode: 75, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -20.459, mean reward: -0.205 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.113, 10.173], loss: 0.004626, mae: 0.070464, mean_q: -0.290175
  7600/100000: episode: 76, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.726, mean reward: -0.167 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.800, 10.398], loss: 0.003916, mae: 0.064999, mean_q: -0.302917
  7700/100000: episode: 77, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.221, mean reward: -0.182 [-1.000, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.559, 10.161], loss: 0.003771, mae: 0.063563, mean_q: -0.316843
  7800/100000: episode: 78, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.952, mean reward: -0.180 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.022, 10.221], loss: 0.003622, mae: 0.061674, mean_q: -0.312291
  7900/100000: episode: 79, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.745, mean reward: -0.187 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.481, 10.098], loss: 0.003800, mae: 0.063378, mean_q: -0.326921
  8000/100000: episode: 80, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.609, mean reward: -0.176 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.613, 10.134], loss: 0.003640, mae: 0.062937, mean_q: -0.307771
  8100/100000: episode: 81, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -9.056, mean reward: -0.091 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.749, 10.098], loss: 0.003499, mae: 0.060024, mean_q: -0.336011
  8200/100000: episode: 82, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -20.000, mean reward: -0.200 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.214, 10.231], loss: 0.003950, mae: 0.064847, mean_q: -0.318941
  8300/100000: episode: 83, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -19.677, mean reward: -0.197 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.891, 10.098], loss: 0.003641, mae: 0.062549, mean_q: -0.300426
  8400/100000: episode: 84, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.160, mean reward: -0.182 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.848, 10.242], loss: 0.003735, mae: 0.062402, mean_q: -0.327247
  8500/100000: episode: 85, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -17.215, mean reward: -0.172 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.197, 10.098], loss: 0.005683, mae: 0.074221, mean_q: -0.328220
  8600/100000: episode: 86, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -19.066, mean reward: -0.191 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.307, 10.184], loss: 0.006845, mae: 0.080483, mean_q: -0.305516
  8700/100000: episode: 87, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.353, mean reward: -0.184 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.838, 10.098], loss: 0.003710, mae: 0.062929, mean_q: -0.327466
  8800/100000: episode: 88, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -19.247, mean reward: -0.192 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.583, 10.282], loss: 0.003692, mae: 0.063071, mean_q: -0.297426
  8900/100000: episode: 89, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.028, mean reward: -0.190 [-1.000, 0.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.802, 10.098], loss: 0.003565, mae: 0.060676, mean_q: -0.317346
  9000/100000: episode: 90, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.090, mean reward: -0.181 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.154, 10.098], loss: 0.003607, mae: 0.062340, mean_q: -0.314915
  9100/100000: episode: 91, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.860, mean reward: -0.179 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.490, 10.131], loss: 0.003147, mae: 0.057817, mean_q: -0.330972
  9200/100000: episode: 92, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -20.996, mean reward: -0.210 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.128, 10.098], loss: 0.003284, mae: 0.059440, mean_q: -0.302541
  9300/100000: episode: 93, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.168, mean reward: -0.172 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.534, 10.383], loss: 0.004537, mae: 0.069288, mean_q: -0.312135
  9400/100000: episode: 94, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.254, mean reward: -0.193 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.617, 10.098], loss: 0.003705, mae: 0.061402, mean_q: -0.323140
  9500/100000: episode: 95, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.936, mean reward: -0.169 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.586, 10.098], loss: 0.003340, mae: 0.059029, mean_q: -0.299607
  9600/100000: episode: 96, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -14.040, mean reward: -0.140 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.769, 10.098], loss: 0.003696, mae: 0.062425, mean_q: -0.307565
  9700/100000: episode: 97, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.583, mean reward: -0.176 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.154, 10.098], loss: 0.005430, mae: 0.068437, mean_q: -0.318208
  9800/100000: episode: 98, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.811, mean reward: -0.188 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.707, 10.216], loss: 0.003526, mae: 0.061386, mean_q: -0.359566
  9900/100000: episode: 99, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -9.845, mean reward: -0.098 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.802, 10.098], loss: 0.004796, mae: 0.071235, mean_q: -0.324135
 10000/100000: episode: 100, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.650, mean reward: -0.146 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.757, 10.098], loss: 0.003897, mae: 0.064375, mean_q: -0.325264
 10100/100000: episode: 101, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.156, mean reward: -0.182 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.488, 10.098], loss: 0.003908, mae: 0.064789, mean_q: -0.317422
 10200/100000: episode: 102, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.173, mean reward: -0.192 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.063, 10.238], loss: 0.003476, mae: 0.060758, mean_q: -0.308543
 10300/100000: episode: 103, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.335, mean reward: -0.153 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.780, 10.098], loss: 0.004280, mae: 0.065439, mean_q: -0.308640
 10400/100000: episode: 104, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.317, mean reward: -0.173 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.613, 10.254], loss: 0.002910, mae: 0.055209, mean_q: -0.315211
 10500/100000: episode: 105, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.661, mean reward: -0.197 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.197, 10.186], loss: 0.003171, mae: 0.057189, mean_q: -0.327827
 10600/100000: episode: 106, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.853, mean reward: -0.199 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.759, 10.150], loss: 0.004169, mae: 0.065386, mean_q: -0.308465
 10700/100000: episode: 107, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -20.923, mean reward: -0.209 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.943, 10.112], loss: 0.003147, mae: 0.057810, mean_q: -0.299256
 10800/100000: episode: 108, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -15.433, mean reward: -0.154 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.047, 10.392], loss: 0.003321, mae: 0.059483, mean_q: -0.332405
 10900/100000: episode: 109, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.884, mean reward: -0.169 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.949, 10.213], loss: 0.003998, mae: 0.063666, mean_q: -0.336301
 11000/100000: episode: 110, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.078, mean reward: -0.171 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.972, 10.165], loss: 0.003402, mae: 0.058250, mean_q: -0.346684
 11100/100000: episode: 111, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -20.171, mean reward: -0.202 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.546, 10.108], loss: 0.003336, mae: 0.059933, mean_q: -0.289943
 11200/100000: episode: 112, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -11.041, mean reward: -0.110 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.252, 10.283], loss: 0.003436, mae: 0.059938, mean_q: -0.304920
 11300/100000: episode: 113, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -16.416, mean reward: -0.164 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.624, 10.098], loss: 0.003787, mae: 0.061158, mean_q: -0.326188
 11400/100000: episode: 114, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.975, mean reward: -0.170 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.457, 10.098], loss: 0.003043, mae: 0.056386, mean_q: -0.294727
 11500/100000: episode: 115, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.111, mean reward: -0.171 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.746, 10.098], loss: 0.003099, mae: 0.056089, mean_q: -0.347736
 11600/100000: episode: 116, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -14.603, mean reward: -0.146 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.207, 10.166], loss: 0.003162, mae: 0.057279, mean_q: -0.360491
 11700/100000: episode: 117, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -13.267, mean reward: -0.133 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.727, 10.300], loss: 0.003206, mae: 0.058042, mean_q: -0.320330
 11800/100000: episode: 118, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -15.298, mean reward: -0.153 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.062, 10.357], loss: 0.003552, mae: 0.060971, mean_q: -0.322256
 11900/100000: episode: 119, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.771, mean reward: -0.168 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.215, 10.243], loss: 0.003053, mae: 0.056687, mean_q: -0.319454
 12000/100000: episode: 120, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -13.722, mean reward: -0.137 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.354, 10.356], loss: 0.003104, mae: 0.057921, mean_q: -0.332347
 12100/100000: episode: 121, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.331, mean reward: -0.173 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.640, 10.162], loss: 0.002913, mae: 0.055449, mean_q: -0.323568
 12200/100000: episode: 122, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.242, mean reward: -0.192 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.384, 10.098], loss: 0.002931, mae: 0.055554, mean_q: -0.307691
 12300/100000: episode: 123, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.699, mean reward: -0.177 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.510, 10.098], loss: 0.002864, mae: 0.054231, mean_q: -0.318114
 12400/100000: episode: 124, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -15.696, mean reward: -0.157 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.305, 10.098], loss: 0.003077, mae: 0.057683, mean_q: -0.318379
 12500/100000: episode: 125, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.677, mean reward: -0.187 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.816, 10.098], loss: 0.003095, mae: 0.056311, mean_q: -0.341224
 12600/100000: episode: 126, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -17.339, mean reward: -0.173 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.775, 10.098], loss: 0.003145, mae: 0.058491, mean_q: -0.326102
 12700/100000: episode: 127, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -17.804, mean reward: -0.178 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.281, 10.125], loss: 0.002812, mae: 0.054417, mean_q: -0.310038
 12800/100000: episode: 128, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.454, mean reward: -0.185 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.376, 10.098], loss: 0.004109, mae: 0.059924, mean_q: -0.308907
 12900/100000: episode: 129, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.894, mean reward: -0.169 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.992, 10.098], loss: 0.004367, mae: 0.064796, mean_q: -0.317851
 13000/100000: episode: 130, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.897, mean reward: -0.189 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.868, 10.126], loss: 0.003083, mae: 0.057401, mean_q: -0.352766
 13100/100000: episode: 131, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -10.917, mean reward: -0.109 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.464, 10.484], loss: 0.003161, mae: 0.054617, mean_q: -0.333234
 13200/100000: episode: 132, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -18.012, mean reward: -0.180 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.958, 10.098], loss: 0.004421, mae: 0.065006, mean_q: -0.364887
 13300/100000: episode: 133, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.907, mean reward: -0.159 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.055, 10.098], loss: 0.002773, mae: 0.053826, mean_q: -0.345938
 13400/100000: episode: 134, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.410, mean reward: -0.184 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.297, 10.121], loss: 0.002934, mae: 0.055773, mean_q: -0.308503
 13500/100000: episode: 135, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.612, mean reward: -0.126 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-0.385, 10.098], loss: 0.002662, mae: 0.052481, mean_q: -0.332140
 13600/100000: episode: 136, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.732, mean reward: -0.167 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.410, 10.098], loss: 0.003266, mae: 0.058456, mean_q: -0.338922
 13700/100000: episode: 137, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.649, mean reward: -0.166 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.044, 10.200], loss: 0.003192, mae: 0.058226, mean_q: -0.309600
 13800/100000: episode: 138, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -12.828, mean reward: -0.128 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.452, 10.098], loss: 0.002936, mae: 0.054647, mean_q: -0.305643
 13900/100000: episode: 139, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.793, mean reward: -0.198 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.927, 10.098], loss: 0.002882, mae: 0.054449, mean_q: -0.326595
 14000/100000: episode: 140, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -14.011, mean reward: -0.140 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.543, 10.098], loss: 0.002859, mae: 0.055707, mean_q: -0.307439
 14100/100000: episode: 141, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.176, mean reward: -0.182 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.634, 10.256], loss: 0.003098, mae: 0.057528, mean_q: -0.303221
 14200/100000: episode: 142, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.541, mean reward: -0.165 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.396, 10.098], loss: 0.003396, mae: 0.060034, mean_q: -0.301396
 14300/100000: episode: 143, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.654, mean reward: -0.177 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.969, 10.424], loss: 0.002948, mae: 0.055779, mean_q: -0.298883
 14400/100000: episode: 144, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -20.059, mean reward: -0.201 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.724, 10.129], loss: 0.002936, mae: 0.054836, mean_q: -0.324131
 14500/100000: episode: 145, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.373, mean reward: -0.154 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.256, 10.098], loss: 0.003014, mae: 0.056627, mean_q: -0.315122
 14600/100000: episode: 146, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -14.142, mean reward: -0.141 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.150, 10.098], loss: 0.003567, mae: 0.060685, mean_q: -0.335254
 14700/100000: episode: 147, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -14.015, mean reward: -0.140 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.599, 10.098], loss: 0.004639, mae: 0.063361, mean_q: -0.300331
 14800/100000: episode: 148, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.732, mean reward: -0.177 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.565, 10.340], loss: 0.006417, mae: 0.071513, mean_q: -0.310703
 14900/100000: episode: 149, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.822, mean reward: -0.148 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.759, 10.222], loss: 0.003277, mae: 0.058274, mean_q: -0.295439
[Info] 100-TH LEVEL FOUND: 0.5693159103393555, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.342s, episode steps: 100, steps per second: 23, episode reward: -16.901, mean reward: -0.169 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.678, 10.353], loss: 0.003053, mae: 0.056481, mean_q: -0.312476
 15016/100000: episode: 151, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 3.085, mean reward: 0.193 [0.135, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.264], loss: 0.002537, mae: 0.050673, mean_q: -0.380639
 15023/100000: episode: 152, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 2.714, mean reward: 0.388 [0.351, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.512], loss: 0.002989, mae: 0.054734, mean_q: -0.323579
 15039/100000: episode: 153, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 4.427, mean reward: 0.277 [0.172, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-1.586, 10.372], loss: 0.003170, mae: 0.058143, mean_q: -0.293244
 15055/100000: episode: 154, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 3.049, mean reward: 0.191 [0.102, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.338, 10.211], loss: 0.003056, mae: 0.056394, mean_q: -0.255831
 15062/100000: episode: 155, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 2.657, mean reward: 0.380 [0.342, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.448], loss: 0.003281, mae: 0.059561, mean_q: -0.274379
 15087/100000: episode: 156, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 7.617, mean reward: 0.305 [0.175, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.080, 10.481], loss: 0.002768, mae: 0.054251, mean_q: -0.346848
 15131/100000: episode: 157, duration: 0.244s, episode steps: 44, steps per second: 180, episode reward: 10.422, mean reward: 0.237 [0.093, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.710, 10.227], loss: 0.002914, mae: 0.055207, mean_q: -0.238684
 15175/100000: episode: 158, duration: 0.227s, episode steps: 44, steps per second: 194, episode reward: 10.291, mean reward: 0.234 [0.062, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.209, 10.226], loss: 0.002809, mae: 0.053941, mean_q: -0.265316
 15191/100000: episode: 159, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 4.063, mean reward: 0.254 [0.153, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.560, 10.410], loss: 0.003192, mae: 0.057410, mean_q: -0.282610
 15198/100000: episode: 160, duration: 0.046s, episode steps: 7, steps per second: 154, episode reward: 2.347, mean reward: 0.335 [0.293, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.352], loss: 0.002802, mae: 0.054400, mean_q: -0.296916
 15212/100000: episode: 161, duration: 0.067s, episode steps: 14, steps per second: 208, episode reward: 3.897, mean reward: 0.278 [0.204, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.296], loss: 0.002731, mae: 0.053453, mean_q: -0.297679
 15233/100000: episode: 162, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 4.543, mean reward: 0.216 [0.128, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.533, 10.337], loss: 0.002872, mae: 0.054976, mean_q: -0.280005
 15249/100000: episode: 163, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 4.650, mean reward: 0.291 [0.175, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.406], loss: 0.003103, mae: 0.057256, mean_q: -0.276852
 15293/100000: episode: 164, duration: 0.223s, episode steps: 44, steps per second: 197, episode reward: 17.671, mean reward: 0.402 [0.219, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.219, 10.592], loss: 0.003008, mae: 0.055723, mean_q: -0.266621
 15333/100000: episode: 165, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 15.962, mean reward: 0.399 [0.228, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.906, 10.388], loss: 0.003716, mae: 0.061541, mean_q: -0.299292
 15373/100000: episode: 166, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 15.145, mean reward: 0.379 [0.276, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.097, 10.414], loss: 0.002892, mae: 0.055390, mean_q: -0.262797
 15384/100000: episode: 167, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 3.201, mean reward: 0.291 [0.225, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.595, 10.357], loss: 0.003941, mae: 0.061802, mean_q: -0.252701
 15422/100000: episode: 168, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 7.684, mean reward: 0.202 [0.104, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.553, 10.100], loss: 0.003572, mae: 0.062149, mean_q: -0.240751
 15466/100000: episode: 169, duration: 0.233s, episode steps: 44, steps per second: 189, episode reward: 10.957, mean reward: 0.249 [0.035, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.641, 10.162], loss: 0.003151, mae: 0.057669, mean_q: -0.247673
 15491/100000: episode: 170, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 8.275, mean reward: 0.331 [0.216, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.111, 10.409], loss: 0.002987, mae: 0.055326, mean_q: -0.233266
 15529/100000: episode: 171, duration: 0.205s, episode steps: 38, steps per second: 186, episode reward: 8.971, mean reward: 0.236 [0.058, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.715, 10.100], loss: 0.003073, mae: 0.055324, mean_q: -0.222547
 15554/100000: episode: 172, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 7.833, mean reward: 0.313 [0.166, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.472, 10.304], loss: 0.003022, mae: 0.056278, mean_q: -0.239622
 15598/100000: episode: 173, duration: 0.220s, episode steps: 44, steps per second: 200, episode reward: 14.115, mean reward: 0.321 [0.172, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.322, 10.471], loss: 0.003314, mae: 0.058832, mean_q: -0.203645
 15619/100000: episode: 174, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 3.935, mean reward: 0.187 [0.076, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.599, 10.251], loss: 0.003155, mae: 0.056737, mean_q: -0.224561
 15657/100000: episode: 175, duration: 0.195s, episode steps: 38, steps per second: 195, episode reward: 8.691, mean reward: 0.229 [0.106, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.951, 10.353], loss: 0.003360, mae: 0.059733, mean_q: -0.172037
 15678/100000: episode: 176, duration: 0.120s, episode steps: 21, steps per second: 174, episode reward: 4.126, mean reward: 0.196 [0.098, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.262], loss: 0.003550, mae: 0.060816, mean_q: -0.153306
 15718/100000: episode: 177, duration: 0.223s, episode steps: 40, steps per second: 179, episode reward: 11.954, mean reward: 0.299 [0.196, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.193, 10.326], loss: 0.003614, mae: 0.060205, mean_q: -0.197008
 15762/100000: episode: 178, duration: 0.223s, episode steps: 44, steps per second: 197, episode reward: 9.825, mean reward: 0.223 [0.072, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.871, 10.368], loss: 0.003405, mae: 0.059075, mean_q: -0.205185
 15802/100000: episode: 179, duration: 0.224s, episode steps: 40, steps per second: 178, episode reward: 11.072, mean reward: 0.277 [0.091, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-1.017, 10.392], loss: 0.003187, mae: 0.057325, mean_q: -0.177489
 15818/100000: episode: 180, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 4.576, mean reward: 0.286 [0.196, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.815, 10.392], loss: 0.003503, mae: 0.061626, mean_q: -0.096017
 15825/100000: episode: 181, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 2.544, mean reward: 0.363 [0.323, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.450], loss: 0.003258, mae: 0.056539, mean_q: -0.206026
 15850/100000: episode: 182, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 6.538, mean reward: 0.262 [0.135, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.508, 10.327], loss: 0.003281, mae: 0.059721, mean_q: -0.119699
 15871/100000: episode: 183, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 4.537, mean reward: 0.216 [0.126, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.439, 10.293], loss: 0.003079, mae: 0.056527, mean_q: -0.182655
 15915/100000: episode: 184, duration: 0.228s, episode steps: 44, steps per second: 193, episode reward: 11.361, mean reward: 0.258 [0.078, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.697, 10.183], loss: 0.003020, mae: 0.055964, mean_q: -0.165813
 15936/100000: episode: 185, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 3.657, mean reward: 0.174 [0.074, 0.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.324], loss: 0.003871, mae: 0.065671, mean_q: -0.153952
 15961/100000: episode: 186, duration: 0.135s, episode steps: 25, steps per second: 186, episode reward: 8.019, mean reward: 0.321 [0.217, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.429], loss: 0.003341, mae: 0.062291, mean_q: -0.130204
 16001/100000: episode: 187, duration: 0.217s, episode steps: 40, steps per second: 184, episode reward: 10.509, mean reward: 0.263 [0.052, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.769, 10.219], loss: 0.003506, mae: 0.061948, mean_q: -0.105211
 16022/100000: episode: 188, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 3.032, mean reward: 0.144 [0.062, 0.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.130, 10.212], loss: 0.003036, mae: 0.056727, mean_q: -0.126081
 16066/100000: episode: 189, duration: 0.234s, episode steps: 44, steps per second: 188, episode reward: 8.241, mean reward: 0.187 [0.048, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.162, 10.141], loss: 0.003469, mae: 0.062122, mean_q: -0.065746
 16077/100000: episode: 190, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 3.481, mean reward: 0.316 [0.266, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.432], loss: 0.003705, mae: 0.061806, mean_q: -0.136902
 16088/100000: episode: 191, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 3.426, mean reward: 0.311 [0.212, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.361], loss: 0.003182, mae: 0.060839, mean_q: -0.147691
 16099/100000: episode: 192, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 3.723, mean reward: 0.338 [0.240, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-1.814, 10.415], loss: 0.004349, mae: 0.063805, mean_q: -0.089135
 16137/100000: episode: 193, duration: 0.196s, episode steps: 38, steps per second: 194, episode reward: 10.559, mean reward: 0.278 [0.155, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-1.227, 10.312], loss: 0.005678, mae: 0.071775, mean_q: -0.167545
 16181/100000: episode: 194, duration: 0.242s, episode steps: 44, steps per second: 182, episode reward: 8.658, mean reward: 0.197 [0.034, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.196, 10.179], loss: 0.004916, mae: 0.067006, mean_q: -0.152502
 16219/100000: episode: 195, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 8.116, mean reward: 0.214 [0.057, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.192, 10.145], loss: 0.004474, mae: 0.068151, mean_q: -0.128178
 16259/100000: episode: 196, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 10.283, mean reward: 0.257 [0.150, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.088, 10.307], loss: 0.004256, mae: 0.068305, mean_q: -0.084873
 16275/100000: episode: 197, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 5.215, mean reward: 0.326 [0.203, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.488], loss: 0.003790, mae: 0.063734, mean_q: -0.104357
 16300/100000: episode: 198, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 5.706, mean reward: 0.228 [0.076, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.614, 10.110], loss: 0.003233, mae: 0.060943, mean_q: -0.130842
 16344/100000: episode: 199, duration: 0.216s, episode steps: 44, steps per second: 203, episode reward: 9.962, mean reward: 0.226 [0.034, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.322, 10.100], loss: 0.003196, mae: 0.059179, mean_q: -0.115571
 16355/100000: episode: 200, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 3.363, mean reward: 0.306 [0.264, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.738, 10.385], loss: 0.002730, mae: 0.057257, mean_q: 0.027427
 16371/100000: episode: 201, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 5.303, mean reward: 0.331 [0.191, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.467], loss: 0.002898, mae: 0.057397, mean_q: -0.110192
 16385/100000: episode: 202, duration: 0.078s, episode steps: 14, steps per second: 178, episode reward: 5.495, mean reward: 0.392 [0.342, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.435, 10.444], loss: 0.003110, mae: 0.059057, mean_q: -0.068180
 16392/100000: episode: 203, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 2.229, mean reward: 0.318 [0.239, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.441], loss: 0.003686, mae: 0.063882, mean_q: -0.164108
 16413/100000: episode: 204, duration: 0.111s, episode steps: 21, steps per second: 190, episode reward: 2.914, mean reward: 0.139 [0.028, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.061, 10.167], loss: 0.002921, mae: 0.056937, mean_q: -0.055236
 16457/100000: episode: 205, duration: 0.224s, episode steps: 44, steps per second: 196, episode reward: 9.776, mean reward: 0.222 [0.044, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-1.086, 10.164], loss: 0.003106, mae: 0.058438, mean_q: -0.084958
 16468/100000: episode: 206, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 3.286, mean reward: 0.299 [0.172, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.481, 10.374], loss: 0.002691, mae: 0.054942, mean_q: -0.113227
 16489/100000: episode: 207, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 3.687, mean reward: 0.176 [0.083, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.286], loss: 0.003234, mae: 0.058822, mean_q: -0.082563
 16514/100000: episode: 208, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 6.134, mean reward: 0.245 [0.104, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.271], loss: 0.003687, mae: 0.062079, mean_q: -0.135645
 16530/100000: episode: 209, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 4.226, mean reward: 0.264 [0.074, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.073, 10.277], loss: 0.003357, mae: 0.062024, mean_q: 0.004445
 16568/100000: episode: 210, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 10.556, mean reward: 0.278 [0.144, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.466, 10.281], loss: 0.003497, mae: 0.061479, mean_q: -0.042831
 16593/100000: episode: 211, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 9.018, mean reward: 0.361 [0.249, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.474, 10.560], loss: 0.003333, mae: 0.060657, mean_q: -0.046761
 16637/100000: episode: 212, duration: 0.245s, episode steps: 44, steps per second: 180, episode reward: 10.423, mean reward: 0.237 [0.020, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.161, 10.100], loss: 0.003205, mae: 0.060340, mean_q: -0.004611
 16658/100000: episode: 213, duration: 0.105s, episode steps: 21, steps per second: 199, episode reward: 4.648, mean reward: 0.221 [0.093, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.312, 10.315], loss: 0.003396, mae: 0.062041, mean_q: -0.021764
 16702/100000: episode: 214, duration: 0.245s, episode steps: 44, steps per second: 179, episode reward: 10.461, mean reward: 0.238 [0.026, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.947, 10.134], loss: 0.003055, mae: 0.058763, mean_q: 0.001119
 16716/100000: episode: 215, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 3.013, mean reward: 0.215 [0.155, 0.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.391, 10.296], loss: 0.003395, mae: 0.060654, mean_q: 0.046037
 16760/100000: episode: 216, duration: 0.225s, episode steps: 44, steps per second: 195, episode reward: 7.550, mean reward: 0.172 [0.030, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.255, 10.100], loss: 0.003226, mae: 0.059786, mean_q: -0.024548
 16804/100000: episode: 217, duration: 0.213s, episode steps: 44, steps per second: 206, episode reward: 8.922, mean reward: 0.203 [0.023, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-1.125, 10.167], loss: 0.003815, mae: 0.065610, mean_q: 0.008022
 16815/100000: episode: 218, duration: 0.070s, episode steps: 11, steps per second: 156, episode reward: 2.812, mean reward: 0.256 [0.152, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.302], loss: 0.003347, mae: 0.062979, mean_q: 0.013024
 16831/100000: episode: 219, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 3.150, mean reward: 0.197 [0.089, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.272, 10.305], loss: 0.002961, mae: 0.057088, mean_q: -0.061527
 16869/100000: episode: 220, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 11.792, mean reward: 0.310 [0.076, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.035, 10.260], loss: 0.003113, mae: 0.059362, mean_q: 0.005956
 16876/100000: episode: 221, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 2.533, mean reward: 0.362 [0.340, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.436], loss: 0.003778, mae: 0.064449, mean_q: 0.007109
 16883/100000: episode: 222, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 2.251, mean reward: 0.322 [0.306, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.414], loss: 0.004274, mae: 0.069302, mean_q: 0.006048
 16927/100000: episode: 223, duration: 0.228s, episode steps: 44, steps per second: 193, episode reward: 13.315, mean reward: 0.303 [0.177, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.653, 10.318], loss: 0.003213, mae: 0.060479, mean_q: -0.011930
 16948/100000: episode: 224, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 3.011, mean reward: 0.143 [0.078, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.320, 10.277], loss: 0.003560, mae: 0.061819, mean_q: -0.018212
 16969/100000: episode: 225, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 4.434, mean reward: 0.211 [0.120, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.915, 10.287], loss: 0.003818, mae: 0.066093, mean_q: 0.046075
 17013/100000: episode: 226, duration: 0.251s, episode steps: 44, steps per second: 176, episode reward: 11.905, mean reward: 0.271 [0.064, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.198, 10.100], loss: 0.003360, mae: 0.062539, mean_q: 0.009262
 17053/100000: episode: 227, duration: 0.238s, episode steps: 40, steps per second: 168, episode reward: 10.292, mean reward: 0.257 [0.157, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.286, 10.284], loss: 0.003391, mae: 0.061103, mean_q: 0.039192
 17060/100000: episode: 228, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 2.372, mean reward: 0.339 [0.261, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.170, 10.445], loss: 0.004581, mae: 0.069173, mean_q: 0.085757
 17074/100000: episode: 229, duration: 0.092s, episode steps: 14, steps per second: 153, episode reward: 4.566, mean reward: 0.326 [0.175, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-1.094, 10.273], loss: 0.005235, mae: 0.075895, mean_q: 0.085292
 17085/100000: episode: 230, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 2.681, mean reward: 0.244 [0.203, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.331], loss: 0.004012, mae: 0.066695, mean_q: 0.001198
 17099/100000: episode: 231, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 4.662, mean reward: 0.333 [0.218, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.405], loss: 0.004298, mae: 0.068655, mean_q: 0.018584
 17124/100000: episode: 232, duration: 0.142s, episode steps: 25, steps per second: 175, episode reward: 6.143, mean reward: 0.246 [0.083, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.061, 10.559], loss: 0.003851, mae: 0.065331, mean_q: 0.040853
 17164/100000: episode: 233, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 11.627, mean reward: 0.291 [0.131, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.542, 10.321], loss: 0.003276, mae: 0.060822, mean_q: 0.033382
 17208/100000: episode: 234, duration: 0.229s, episode steps: 44, steps per second: 192, episode reward: 15.239, mean reward: 0.346 [0.155, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.197, 10.305], loss: 0.003514, mae: 0.062868, mean_q: 0.064023
 17222/100000: episode: 235, duration: 0.067s, episode steps: 14, steps per second: 209, episode reward: 4.283, mean reward: 0.306 [0.242, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.445], loss: 0.003933, mae: 0.067124, mean_q: 0.025489
 17266/100000: episode: 236, duration: 0.230s, episode steps: 44, steps per second: 191, episode reward: 9.908, mean reward: 0.225 [0.029, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.741, 10.100], loss: 0.004266, mae: 0.068753, mean_q: 0.040901
 17304/100000: episode: 237, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 16.408, mean reward: 0.432 [0.233, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.134, 10.503], loss: 0.003988, mae: 0.067943, mean_q: 0.077548
 17320/100000: episode: 238, duration: 0.098s, episode steps: 16, steps per second: 164, episode reward: 3.288, mean reward: 0.205 [0.145, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.399, 10.326], loss: 0.003002, mae: 0.059560, mean_q: 0.111436
 17331/100000: episode: 239, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 2.590, mean reward: 0.235 [0.126, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.228], loss: 0.004176, mae: 0.071521, mean_q: 0.131254
[Info] 200-TH LEVEL FOUND: 0.7375316023826599, Considering 10/90 traces
 17338/100000: episode: 240, duration: 3.886s, episode steps: 7, steps per second: 2, episode reward: 2.330, mean reward: 0.333 [0.285, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.393], loss: 0.003798, mae: 0.066735, mean_q: 0.147623
 17371/100000: episode: 241, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 8.650, mean reward: 0.262 [0.083, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.730, 10.400], loss: 0.003391, mae: 0.061637, mean_q: 0.096395
[Info] FALSIFICATION!
 17396/100000: episode: 242, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 22.945, mean reward: 0.918 [0.349, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-1.358, 10.532], loss: 0.003227, mae: 0.061660, mean_q: 0.123441
 17496/100000: episode: 243, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.215, mean reward: -0.162 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.392, 10.321], loss: 0.018973, mae: 0.080604, mean_q: 0.112252
 17596/100000: episode: 244, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.069, mean reward: -0.171 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.508, 10.109], loss: 0.005139, mae: 0.072753, mean_q: 0.092971
 17696/100000: episode: 245, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.749, mean reward: -0.187 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.280, 10.142], loss: 0.003779, mae: 0.064447, mean_q: 0.116218
 17796/100000: episode: 246, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.851, mean reward: -0.159 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.750, 10.128], loss: 0.004498, mae: 0.067232, mean_q: 0.075328
 17896/100000: episode: 247, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.907, mean reward: -0.179 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.394, 10.317], loss: 0.003571, mae: 0.064321, mean_q: 0.092084
 17996/100000: episode: 248, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.566, mean reward: -0.176 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.996, 10.098], loss: 0.018822, mae: 0.081772, mean_q: 0.087810
 18096/100000: episode: 249, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.479, mean reward: -0.145 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.952, 10.201], loss: 0.005311, mae: 0.070840, mean_q: 0.073262
 18196/100000: episode: 250, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.051, mean reward: -0.171 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.730, 10.098], loss: 0.047476, mae: 0.106426, mean_q: 0.100247
 18296/100000: episode: 251, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.823, mean reward: -0.168 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.236, 10.226], loss: 0.003452, mae: 0.063007, mean_q: 0.084193
 18396/100000: episode: 252, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.257, mean reward: -0.163 [-1.000, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.423, 10.180], loss: 0.017502, mae: 0.073344, mean_q: 0.075133
 18496/100000: episode: 253, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.768, mean reward: -0.188 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.059, 10.190], loss: 0.017760, mae: 0.075557, mean_q: 0.128950
 18596/100000: episode: 254, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -18.289, mean reward: -0.183 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.176, 10.356], loss: 0.003661, mae: 0.064696, mean_q: 0.083879
 18696/100000: episode: 255, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.647, mean reward: -0.156 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.942, 10.353], loss: 0.016872, mae: 0.069226, mean_q: 0.084328
 18796/100000: episode: 256, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.942, mean reward: -0.189 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.534, 10.198], loss: 0.003685, mae: 0.064168, mean_q: 0.110592
 18896/100000: episode: 257, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -12.273, mean reward: -0.123 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.205, 10.098], loss: 0.017703, mae: 0.076057, mean_q: 0.082925
 18996/100000: episode: 258, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.814, mean reward: -0.178 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.825, 10.215], loss: 0.003462, mae: 0.063511, mean_q: 0.096817
 19096/100000: episode: 259, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.146, mean reward: -0.191 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.547, 10.158], loss: 0.017576, mae: 0.075634, mean_q: 0.103088
 19196/100000: episode: 260, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.813, mean reward: -0.158 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.546, 10.098], loss: 0.003465, mae: 0.063362, mean_q: 0.116139
 19296/100000: episode: 261, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.717, mean reward: -0.187 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.540, 10.098], loss: 0.003571, mae: 0.063974, mean_q: 0.121359
 19396/100000: episode: 262, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.206, mean reward: -0.172 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.583, 10.098], loss: 0.017152, mae: 0.072106, mean_q: 0.087177
 19496/100000: episode: 263, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -18.128, mean reward: -0.181 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.470, 10.098], loss: 0.017945, mae: 0.079304, mean_q: 0.068840
 19596/100000: episode: 264, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.150, mean reward: -0.171 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.909, 10.230], loss: 0.017167, mae: 0.071968, mean_q: 0.099501
 19696/100000: episode: 265, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -13.844, mean reward: -0.138 [-1.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.134, 10.098], loss: 0.031574, mae: 0.086556, mean_q: 0.095595
 19796/100000: episode: 266, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.721, mean reward: -0.197 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.017, 10.098], loss: 0.017322, mae: 0.074878, mean_q: 0.112198
 19896/100000: episode: 267, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.785, mean reward: -0.188 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.338, 10.098], loss: 0.003626, mae: 0.064719, mean_q: 0.099515
 19996/100000: episode: 268, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.916, mean reward: -0.179 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.454, 10.098], loss: 0.017968, mae: 0.079662, mean_q: 0.073461
 20096/100000: episode: 269, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.262, mean reward: -0.163 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.716, 10.098], loss: 0.016863, mae: 0.070790, mean_q: 0.053807
 20196/100000: episode: 270, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -19.569, mean reward: -0.196 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.607, 10.098], loss: 0.032185, mae: 0.090794, mean_q: 0.078132
 20296/100000: episode: 271, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.617, mean reward: -0.166 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.911, 10.206], loss: 0.003284, mae: 0.061808, mean_q: 0.012465
 20396/100000: episode: 272, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -15.807, mean reward: -0.158 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.914, 10.098], loss: 0.004877, mae: 0.066296, mean_q: 0.000972
 20496/100000: episode: 273, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -19.138, mean reward: -0.191 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.210, 10.211], loss: 0.003885, mae: 0.064810, mean_q: 0.026694
 20596/100000: episode: 274, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.787, mean reward: -0.168 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.238, 10.098], loss: 0.003997, mae: 0.064725, mean_q: -0.052495
 20696/100000: episode: 275, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -12.110, mean reward: -0.121 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.537, 10.397], loss: 0.016594, mae: 0.067256, mean_q: -0.044969
 20796/100000: episode: 276, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.750, mean reward: -0.177 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.351, 10.163], loss: 0.033339, mae: 0.088436, mean_q: -0.051919
 20896/100000: episode: 277, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -14.345, mean reward: -0.143 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.543, 10.098], loss: 0.004422, mae: 0.066117, mean_q: -0.091681
 20996/100000: episode: 278, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.497, mean reward: -0.195 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.603, 10.142], loss: 0.016957, mae: 0.071264, mean_q: -0.089513
 21096/100000: episode: 279, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.378, mean reward: -0.184 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.564, 10.150], loss: 0.003095, mae: 0.057684, mean_q: -0.093454
 21196/100000: episode: 280, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.414, mean reward: -0.144 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.758, 10.098], loss: 0.016710, mae: 0.069825, mean_q: -0.124059
 21296/100000: episode: 281, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.287, mean reward: -0.183 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.637, 10.098], loss: 0.016312, mae: 0.062941, mean_q: -0.130929
 21396/100000: episode: 282, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.728, mean reward: -0.147 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.600, 10.249], loss: 0.003586, mae: 0.061415, mean_q: -0.182931
 21496/100000: episode: 283, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -20.904, mean reward: -0.209 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.662, 10.176], loss: 0.016389, mae: 0.065283, mean_q: -0.212541
 21596/100000: episode: 284, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -14.590, mean reward: -0.146 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.653, 10.098], loss: 0.017287, mae: 0.072342, mean_q: -0.148168
 21696/100000: episode: 285, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -15.110, mean reward: -0.151 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.938, 10.098], loss: 0.016820, mae: 0.068934, mean_q: -0.197977
 21796/100000: episode: 286, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -18.609, mean reward: -0.186 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.931, 10.214], loss: 0.003045, mae: 0.057324, mean_q: -0.201549
 21896/100000: episode: 287, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.717, mean reward: -0.157 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.394, 10.098], loss: 0.016585, mae: 0.065567, mean_q: -0.230403
 21996/100000: episode: 288, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.954, mean reward: -0.180 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.918, 10.144], loss: 0.003117, mae: 0.056883, mean_q: -0.261912
 22096/100000: episode: 289, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -12.388, mean reward: -0.124 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.812, 10.128], loss: 0.002813, mae: 0.054043, mean_q: -0.279954
 22196/100000: episode: 290, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.263, mean reward: -0.153 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.912, 10.169], loss: 0.042283, mae: 0.075820, mean_q: -0.283263
 22296/100000: episode: 291, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.620, mean reward: -0.166 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.273, 10.108], loss: 0.003207, mae: 0.056851, mean_q: -0.297570
 22396/100000: episode: 292, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.658, mean reward: -0.187 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.424, 10.340], loss: 0.002966, mae: 0.055497, mean_q: -0.326477
 22496/100000: episode: 293, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -19.764, mean reward: -0.198 [-1.000, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.857, 10.201], loss: 0.003825, mae: 0.058972, mean_q: -0.316390
 22596/100000: episode: 294, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.459, mean reward: -0.165 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.360, 10.233], loss: 0.003967, mae: 0.061025, mean_q: -0.297950
 22696/100000: episode: 295, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.661, mean reward: -0.187 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.427, 10.098], loss: 0.005042, mae: 0.065196, mean_q: -0.330001
 22796/100000: episode: 296, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.718, mean reward: -0.187 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.435, 10.098], loss: 0.002830, mae: 0.055075, mean_q: -0.288408
 22896/100000: episode: 297, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.431, mean reward: -0.194 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.493, 10.318], loss: 0.003067, mae: 0.057246, mean_q: -0.309455
 22996/100000: episode: 298, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.325, mean reward: -0.183 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.600, 10.098], loss: 0.002991, mae: 0.056762, mean_q: -0.321962
 23096/100000: episode: 299, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.491, mean reward: -0.155 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.513, 10.116], loss: 0.002799, mae: 0.053202, mean_q: -0.340781
 23196/100000: episode: 300, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -12.250, mean reward: -0.123 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.995, 10.098], loss: 0.003047, mae: 0.056076, mean_q: -0.286103
 23296/100000: episode: 301, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.465, mean reward: -0.195 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.026, 10.098], loss: 0.003125, mae: 0.058225, mean_q: -0.346648
 23396/100000: episode: 302, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -9.768, mean reward: -0.098 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.960, 10.418], loss: 0.002895, mae: 0.055152, mean_q: -0.316285
 23496/100000: episode: 303, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -20.084, mean reward: -0.201 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.919, 10.098], loss: 0.002846, mae: 0.054263, mean_q: -0.306719
 23596/100000: episode: 304, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -14.813, mean reward: -0.148 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.963, 10.169], loss: 0.002857, mae: 0.054948, mean_q: -0.284418
 23696/100000: episode: 305, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -13.861, mean reward: -0.139 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.594, 10.098], loss: 0.002792, mae: 0.053750, mean_q: -0.321501
 23796/100000: episode: 306, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -19.112, mean reward: -0.191 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.057, 10.178], loss: 0.002657, mae: 0.052410, mean_q: -0.317722
 23896/100000: episode: 307, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -17.474, mean reward: -0.175 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.931, 10.252], loss: 0.002814, mae: 0.054430, mean_q: -0.309963
 23996/100000: episode: 308, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.132, mean reward: -0.171 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.947, 10.098], loss: 0.003049, mae: 0.056776, mean_q: -0.329668
 24096/100000: episode: 309, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.703, mean reward: -0.147 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.233, 10.098], loss: 0.002800, mae: 0.053641, mean_q: -0.342406
 24196/100000: episode: 310, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.103, mean reward: -0.171 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.713, 10.281], loss: 0.002790, mae: 0.053610, mean_q: -0.318067
 24296/100000: episode: 311, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.032, mean reward: -0.160 [-1.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.245, 10.098], loss: 0.002955, mae: 0.054571, mean_q: -0.333692
 24396/100000: episode: 312, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -12.627, mean reward: -0.126 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.118, 10.098], loss: 0.002871, mae: 0.053338, mean_q: -0.349762
 24496/100000: episode: 313, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -16.626, mean reward: -0.166 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.690, 10.098], loss: 0.002840, mae: 0.055268, mean_q: -0.306470
 24596/100000: episode: 314, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -20.383, mean reward: -0.204 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.223, 10.098], loss: 0.003117, mae: 0.056324, mean_q: -0.305532
 24696/100000: episode: 315, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -19.658, mean reward: -0.197 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.279, 10.098], loss: 0.005432, mae: 0.066773, mean_q: -0.313878
 24796/100000: episode: 316, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.952, mean reward: -0.180 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.625, 10.140], loss: 0.003004, mae: 0.056096, mean_q: -0.316530
 24896/100000: episode: 317, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.309, mean reward: -0.183 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.507, 10.098], loss: 0.003011, mae: 0.055047, mean_q: -0.277797
 24996/100000: episode: 318, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.141, mean reward: -0.161 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.617, 10.098], loss: 0.002848, mae: 0.054102, mean_q: -0.335649
 25096/100000: episode: 319, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.433, mean reward: -0.154 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.824, 10.098], loss: 0.003026, mae: 0.055869, mean_q: -0.320153
 25196/100000: episode: 320, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.537, mean reward: -0.165 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.497, 10.341], loss: 0.002863, mae: 0.053612, mean_q: -0.307946
 25296/100000: episode: 321, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.757, mean reward: -0.188 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.695, 10.190], loss: 0.003083, mae: 0.056550, mean_q: -0.294943
 25396/100000: episode: 322, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.693, mean reward: -0.197 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.500, 10.098], loss: 0.004076, mae: 0.061671, mean_q: -0.321887
 25496/100000: episode: 323, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.459, mean reward: -0.185 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.317, 10.172], loss: 0.002993, mae: 0.056642, mean_q: -0.318429
 25596/100000: episode: 324, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.126, mean reward: -0.191 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.664, 10.152], loss: 0.002944, mae: 0.054152, mean_q: -0.324305
 25696/100000: episode: 325, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.892, mean reward: -0.179 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.010, 10.357], loss: 0.003485, mae: 0.059151, mean_q: -0.353204
 25796/100000: episode: 326, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.550, mean reward: -0.196 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.633, 10.107], loss: 0.003681, mae: 0.061860, mean_q: -0.330756
 25896/100000: episode: 327, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.488, mean reward: -0.155 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.526, 10.289], loss: 0.006955, mae: 0.074183, mean_q: -0.370672
 25996/100000: episode: 328, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -19.484, mean reward: -0.195 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.675, 10.098], loss: 0.004643, mae: 0.063514, mean_q: -0.333872
 26096/100000: episode: 329, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -12.329, mean reward: -0.123 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.926, 10.098], loss: 0.002980, mae: 0.054579, mean_q: -0.327671
 26196/100000: episode: 330, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.476, mean reward: -0.175 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.212, 10.098], loss: 0.002910, mae: 0.054136, mean_q: -0.314124
 26296/100000: episode: 331, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -10.010, mean reward: -0.100 [-1.000, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.375, 10.098], loss: 0.002871, mae: 0.053136, mean_q: -0.334918
 26396/100000: episode: 332, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.445, mean reward: -0.174 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.860, 10.098], loss: 0.003151, mae: 0.055926, mean_q: -0.302168
 26496/100000: episode: 333, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.055, mean reward: -0.151 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.206, 10.245], loss: 0.003093, mae: 0.055848, mean_q: -0.298275
 26596/100000: episode: 334, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.517, mean reward: -0.195 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.567, 10.098], loss: 0.003251, mae: 0.057203, mean_q: -0.323273
 26696/100000: episode: 335, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.332, mean reward: -0.183 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.675, 10.098], loss: 0.002953, mae: 0.054578, mean_q: -0.326977
 26796/100000: episode: 336, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -13.463, mean reward: -0.135 [-1.000, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.260, 10.360], loss: 0.003008, mae: 0.054214, mean_q: -0.327869
 26896/100000: episode: 337, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -14.148, mean reward: -0.141 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.450, 10.255], loss: 0.002982, mae: 0.055005, mean_q: -0.336213
 26996/100000: episode: 338, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -12.212, mean reward: -0.122 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.693, 10.098], loss: 0.003075, mae: 0.055671, mean_q: -0.304194
 27096/100000: episode: 339, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -8.159, mean reward: -0.082 [-1.000, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.627, 10.637], loss: 0.002883, mae: 0.054214, mean_q: -0.325189
 27196/100000: episode: 340, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.368, mean reward: -0.174 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.624, 10.299], loss: 0.003214, mae: 0.056184, mean_q: -0.317760
 27296/100000: episode: 341, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -13.496, mean reward: -0.135 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.582, 10.421], loss: 0.003101, mae: 0.056177, mean_q: -0.287394
[Info] 100-TH LEVEL FOUND: 0.5551487803459167, Considering 10/90 traces
 27396/100000: episode: 342, duration: 4.320s, episode steps: 100, steps per second: 23, episode reward: -17.391, mean reward: -0.174 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.607, 10.138], loss: 0.003019, mae: 0.055256, mean_q: -0.299935
 27446/100000: episode: 343, duration: 0.246s, episode steps: 50, steps per second: 203, episode reward: 14.057, mean reward: 0.281 [0.029, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.654, 10.100], loss: 0.003906, mae: 0.061857, mean_q: -0.304747
 27466/100000: episode: 344, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 5.067, mean reward: 0.253 [0.132, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.218, 10.283], loss: 0.023449, mae: 0.098845, mean_q: -0.320361
[Info] FALSIFICATION!
 27479/100000: episode: 345, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 16.928, mean reward: 1.302 [0.506, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.673, 9.864], loss: 0.012662, mae: 0.115051, mean_q: -0.350264
 27579/100000: episode: 346, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.399, mean reward: -0.154 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.196, 10.205], loss: 0.004788, mae: 0.070284, mean_q: -0.294122
 27679/100000: episode: 347, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.576, mean reward: -0.186 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.919, 10.125], loss: 0.019332, mae: 0.072620, mean_q: -0.285726
 27779/100000: episode: 348, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.170, mean reward: -0.192 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.241, 10.126], loss: 0.003245, mae: 0.056932, mean_q: -0.324027
 27879/100000: episode: 349, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.947, mean reward: -0.189 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.645, 10.105], loss: 0.003239, mae: 0.056341, mean_q: -0.321125
 27979/100000: episode: 350, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -10.239, mean reward: -0.102 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.694, 10.098], loss: 0.017833, mae: 0.065479, mean_q: -0.290921
 28079/100000: episode: 351, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.511, mean reward: -0.155 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.858, 10.440], loss: 0.003185, mae: 0.057068, mean_q: -0.271638
 28179/100000: episode: 352, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -17.470, mean reward: -0.175 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.557, 10.098], loss: 0.003233, mae: 0.057414, mean_q: -0.250117
 28279/100000: episode: 353, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.140, mean reward: -0.151 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.406, 10.111], loss: 0.003214, mae: 0.056264, mean_q: -0.281003
 28379/100000: episode: 354, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.445, mean reward: -0.174 [-1.000, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.717, 10.128], loss: 0.018511, mae: 0.069160, mean_q: -0.287089
 28479/100000: episode: 355, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.937, mean reward: -0.169 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.374, 10.098], loss: 0.003001, mae: 0.054305, mean_q: -0.296671
 28579/100000: episode: 356, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.661, mean reward: -0.197 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.342, 10.317], loss: 0.018089, mae: 0.065916, mean_q: -0.281640
 28679/100000: episode: 357, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.232, mean reward: -0.172 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.140, 10.098], loss: 0.003305, mae: 0.058404, mean_q: -0.269006
 28779/100000: episode: 358, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -15.038, mean reward: -0.150 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.867, 10.098], loss: 0.002911, mae: 0.054162, mean_q: -0.300597
 28879/100000: episode: 359, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.175, mean reward: -0.182 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.652, 10.098], loss: 0.003083, mae: 0.055565, mean_q: -0.288904
 28979/100000: episode: 360, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.585, mean reward: -0.176 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.965, 10.382], loss: 0.003097, mae: 0.055632, mean_q: -0.273995
 29079/100000: episode: 361, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -20.207, mean reward: -0.202 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.118, 10.098], loss: 0.019055, mae: 0.073185, mean_q: -0.302364
 29179/100000: episode: 362, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -16.930, mean reward: -0.169 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.199, 10.098], loss: 0.003030, mae: 0.055342, mean_q: -0.283865
 29279/100000: episode: 363, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.768, mean reward: -0.158 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.126, 10.177], loss: 0.018193, mae: 0.066657, mean_q: -0.310955
 29379/100000: episode: 364, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.138, mean reward: -0.171 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.989, 10.147], loss: 0.017232, mae: 0.060900, mean_q: -0.298179
 29479/100000: episode: 365, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -21.722, mean reward: -0.217 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.743, 10.111], loss: 0.002901, mae: 0.053669, mean_q: -0.311861
 29579/100000: episode: 366, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.112, mean reward: -0.171 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.822, 10.098], loss: 0.002955, mae: 0.053489, mean_q: -0.301703
 29679/100000: episode: 367, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -17.696, mean reward: -0.177 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.639, 10.113], loss: 0.017212, mae: 0.061577, mean_q: -0.304371
 29779/100000: episode: 368, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -14.637, mean reward: -0.146 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.715, 10.274], loss: 0.017592, mae: 0.066204, mean_q: -0.305129
 29879/100000: episode: 369, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -16.531, mean reward: -0.165 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.724, 10.098], loss: 0.032181, mae: 0.078380, mean_q: -0.271619
 29979/100000: episode: 370, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.350, mean reward: -0.173 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.542, 10.323], loss: 0.019120, mae: 0.072855, mean_q: -0.276504
 30079/100000: episode: 371, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.601, mean reward: -0.186 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.091, 10.156], loss: 0.016703, mae: 0.060217, mean_q: -0.324038
 30179/100000: episode: 372, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.554, mean reward: -0.166 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.686, 10.098], loss: 0.002811, mae: 0.052317, mean_q: -0.295348
 30279/100000: episode: 373, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.186, mean reward: -0.172 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.431, 10.238], loss: 0.002852, mae: 0.052137, mean_q: -0.328683
 30379/100000: episode: 374, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -14.659, mean reward: -0.147 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.492, 10.449], loss: 0.016771, mae: 0.059438, mean_q: -0.309416
 30479/100000: episode: 375, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.281, mean reward: -0.183 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.350, 10.098], loss: 0.016279, mae: 0.055883, mean_q: -0.332320
 30579/100000: episode: 376, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -11.005, mean reward: -0.110 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.673, 10.098], loss: 0.017180, mae: 0.065365, mean_q: -0.287193
 30679/100000: episode: 377, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -12.186, mean reward: -0.122 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.091, 10.098], loss: 0.002859, mae: 0.051679, mean_q: -0.292477
 30779/100000: episode: 378, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.360, mean reward: -0.184 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.677, 10.203], loss: 0.016950, mae: 0.065640, mean_q: -0.256722
 30879/100000: episode: 379, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.440, mean reward: -0.174 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.823, 10.187], loss: 0.002859, mae: 0.052754, mean_q: -0.271733
 30979/100000: episode: 380, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.070, mean reward: -0.181 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.873, 10.098], loss: 0.029275, mae: 0.063409, mean_q: -0.267954
 31079/100000: episode: 381, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.942, mean reward: -0.199 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.635, 10.163], loss: 0.003834, mae: 0.059083, mean_q: -0.298142
 31179/100000: episode: 382, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -17.870, mean reward: -0.179 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.352, 10.164], loss: 0.016817, mae: 0.063679, mean_q: -0.258963
 31279/100000: episode: 383, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -19.018, mean reward: -0.190 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.826, 10.098], loss: 0.002993, mae: 0.053364, mean_q: -0.264973
 31379/100000: episode: 384, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.863, mean reward: -0.159 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.608, 10.249], loss: 0.016555, mae: 0.061656, mean_q: -0.345871
 31479/100000: episode: 385, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.254, mean reward: -0.173 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.380, 10.098], loss: 0.002930, mae: 0.052949, mean_q: -0.296618
 31579/100000: episode: 386, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.878, mean reward: -0.179 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.606, 10.197], loss: 0.016772, mae: 0.064319, mean_q: -0.275890
 31679/100000: episode: 387, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.554, mean reward: -0.176 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.520, 10.098], loss: 0.002770, mae: 0.051905, mean_q: -0.306152
 31779/100000: episode: 388, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -13.339, mean reward: -0.133 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.920, 10.098], loss: 0.002837, mae: 0.053082, mean_q: -0.296559
 31879/100000: episode: 389, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.339, mean reward: -0.163 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.310, 10.098], loss: 0.004007, mae: 0.057501, mean_q: -0.303239
 31979/100000: episode: 390, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -20.563, mean reward: -0.206 [-1.000, 0.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.651, 10.098], loss: 0.002800, mae: 0.053304, mean_q: -0.318460
 32079/100000: episode: 391, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.288, mean reward: -0.173 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.075, 10.106], loss: 0.017046, mae: 0.064403, mean_q: -0.299351
 32179/100000: episode: 392, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.675, mean reward: -0.177 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.432, 10.098], loss: 0.029100, mae: 0.069995, mean_q: -0.268465
 32279/100000: episode: 393, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.433, mean reward: -0.174 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.784, 10.098], loss: 0.002881, mae: 0.053619, mean_q: -0.287929
 32379/100000: episode: 394, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.093, mean reward: -0.181 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.606, 10.328], loss: 0.002718, mae: 0.052227, mean_q: -0.290527
 32479/100000: episode: 395, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.339, mean reward: -0.193 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.702, 10.167], loss: 0.003959, mae: 0.060071, mean_q: -0.332053
 32579/100000: episode: 396, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.673, mean reward: -0.177 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.428, 10.098], loss: 0.005348, mae: 0.064701, mean_q: -0.301424
 32679/100000: episode: 397, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -14.170, mean reward: -0.142 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.163, 10.389], loss: 0.004521, mae: 0.060492, mean_q: -0.315012
 32779/100000: episode: 398, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -16.214, mean reward: -0.162 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.580, 10.098], loss: 0.003137, mae: 0.055313, mean_q: -0.310003
 32879/100000: episode: 399, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.298, mean reward: -0.183 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.547, 10.098], loss: 0.002850, mae: 0.053385, mean_q: -0.344794
 32979/100000: episode: 400, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.774, mean reward: -0.188 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.896, 10.098], loss: 0.002505, mae: 0.049885, mean_q: -0.292866
 33079/100000: episode: 401, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.457, mean reward: -0.185 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.324, 10.098], loss: 0.002669, mae: 0.050904, mean_q: -0.332215
 33179/100000: episode: 402, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.440, mean reward: -0.194 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.720, 10.179], loss: 0.002485, mae: 0.048797, mean_q: -0.326860
 33279/100000: episode: 403, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.957, mean reward: -0.180 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.305, 10.145], loss: 0.002642, mae: 0.051552, mean_q: -0.282075
 33379/100000: episode: 404, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -19.056, mean reward: -0.191 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.887, 10.098], loss: 0.002704, mae: 0.051677, mean_q: -0.300668
 33479/100000: episode: 405, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -20.349, mean reward: -0.203 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.613, 10.112], loss: 0.002611, mae: 0.050701, mean_q: -0.305015
 33579/100000: episode: 406, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.362, mean reward: -0.184 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.501, 10.111], loss: 0.002635, mae: 0.050677, mean_q: -0.354032
 33679/100000: episode: 407, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -9.694, mean reward: -0.097 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.611, 10.264], loss: 0.002796, mae: 0.052545, mean_q: -0.309754
 33779/100000: episode: 408, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.957, mean reward: -0.170 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.546, 10.098], loss: 0.002660, mae: 0.050453, mean_q: -0.334109
 33879/100000: episode: 409, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.266, mean reward: -0.173 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.891, 10.098], loss: 0.002498, mae: 0.049889, mean_q: -0.311739
 33979/100000: episode: 410, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.988, mean reward: -0.140 [-1.000, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.891, 10.366], loss: 0.002738, mae: 0.052474, mean_q: -0.315106
 34079/100000: episode: 411, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -14.780, mean reward: -0.148 [-1.000, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.973, 10.098], loss: 0.002720, mae: 0.051319, mean_q: -0.330505
 34179/100000: episode: 412, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.221, mean reward: -0.152 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.775, 10.139], loss: 0.002730, mae: 0.051921, mean_q: -0.341864
 34279/100000: episode: 413, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -15.194, mean reward: -0.152 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.512, 10.310], loss: 0.002659, mae: 0.050897, mean_q: -0.345154
 34379/100000: episode: 414, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.616, mean reward: -0.186 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.262, 10.151], loss: 0.002724, mae: 0.051982, mean_q: -0.295121
 34479/100000: episode: 415, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.607, mean reward: -0.176 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.362, 10.098], loss: 0.003607, mae: 0.056586, mean_q: -0.327994
 34579/100000: episode: 416, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -14.367, mean reward: -0.144 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.213, 10.098], loss: 0.002885, mae: 0.055261, mean_q: -0.323373
 34679/100000: episode: 417, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.705, mean reward: -0.167 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.366, 10.100], loss: 0.002692, mae: 0.051373, mean_q: -0.328675
 34779/100000: episode: 418, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.801, mean reward: -0.178 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.993, 10.122], loss: 0.002560, mae: 0.049829, mean_q: -0.350424
 34879/100000: episode: 419, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -11.711, mean reward: -0.117 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.403, 10.162], loss: 0.002691, mae: 0.051278, mean_q: -0.315480
 34979/100000: episode: 420, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -15.840, mean reward: -0.158 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.836, 10.379], loss: 0.002735, mae: 0.051406, mean_q: -0.313512
 35079/100000: episode: 421, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -11.173, mean reward: -0.112 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.108, 10.245], loss: 0.002870, mae: 0.052881, mean_q: -0.301934
 35179/100000: episode: 422, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -20.072, mean reward: -0.201 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.002, 10.159], loss: 0.002752, mae: 0.051079, mean_q: -0.317908
 35279/100000: episode: 423, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -11.209, mean reward: -0.112 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.495, 10.288], loss: 0.002497, mae: 0.049575, mean_q: -0.314942
 35379/100000: episode: 424, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.922, mean reward: -0.189 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.547, 10.296], loss: 0.003551, mae: 0.056508, mean_q: -0.289201
 35479/100000: episode: 425, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.071, mean reward: -0.161 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.217, 10.169], loss: 0.003368, mae: 0.054962, mean_q: -0.347075
 35579/100000: episode: 426, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -11.630, mean reward: -0.116 [-1.000, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.909, 10.215], loss: 0.002719, mae: 0.052156, mean_q: -0.343690
 35679/100000: episode: 427, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -21.753, mean reward: -0.218 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.940, 10.188], loss: 0.003006, mae: 0.055815, mean_q: -0.315393
 35779/100000: episode: 428, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: -14.507, mean reward: -0.145 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.831, 10.098], loss: 0.002733, mae: 0.051681, mean_q: -0.302177
 35879/100000: episode: 429, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -12.118, mean reward: -0.121 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.636, 10.098], loss: 0.002813, mae: 0.052406, mean_q: -0.305086
 35979/100000: episode: 430, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.531, mean reward: -0.175 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.834, 10.151], loss: 0.002684, mae: 0.050988, mean_q: -0.327108
 36079/100000: episode: 431, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.701, mean reward: -0.157 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.672, 10.098], loss: 0.002758, mae: 0.052074, mean_q: -0.325417
 36179/100000: episode: 432, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.850, mean reward: -0.178 [-1.000, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.286, 10.098], loss: 0.003761, mae: 0.059192, mean_q: -0.313254
 36279/100000: episode: 433, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -19.556, mean reward: -0.196 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.534, 10.115], loss: 0.003011, mae: 0.054309, mean_q: -0.294186
 36379/100000: episode: 434, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -14.244, mean reward: -0.142 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.512, 10.275], loss: 0.002624, mae: 0.050930, mean_q: -0.301134
 36479/100000: episode: 435, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.952, mean reward: -0.180 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.397, 10.098], loss: 0.003006, mae: 0.053376, mean_q: -0.341262
 36579/100000: episode: 436, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.027, mean reward: -0.150 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.089, 10.272], loss: 0.003034, mae: 0.054784, mean_q: -0.286985
 36679/100000: episode: 437, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -12.860, mean reward: -0.129 [-1.000, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.061, 10.150], loss: 0.002887, mae: 0.052811, mean_q: -0.317713
 36779/100000: episode: 438, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.205, mean reward: -0.182 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.009, 10.344], loss: 0.002697, mae: 0.051154, mean_q: -0.291867
 36879/100000: episode: 439, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.980, mean reward: -0.170 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.980, 10.352], loss: 0.004260, mae: 0.061664, mean_q: -0.285362
 36979/100000: episode: 440, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.409, mean reward: -0.184 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.750, 10.235], loss: 0.003017, mae: 0.055151, mean_q: -0.298584
 37079/100000: episode: 441, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.143, mean reward: -0.171 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.970, 10.346], loss: 0.002875, mae: 0.052721, mean_q: -0.264352
 37179/100000: episode: 442, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.027, mean reward: -0.160 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.541, 10.098], loss: 0.002749, mae: 0.051014, mean_q: -0.311800
 37279/100000: episode: 443, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -18.270, mean reward: -0.183 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.381, 10.098], loss: 0.002766, mae: 0.050941, mean_q: -0.319039
 37379/100000: episode: 444, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.135, mean reward: -0.171 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.195, 10.147], loss: 0.005470, mae: 0.062694, mean_q: -0.316020
[Info] 100-TH LEVEL FOUND: 0.63826584815979, Considering 10/90 traces
 37479/100000: episode: 445, duration: 4.359s, episode steps: 100, steps per second: 23, episode reward: -19.170, mean reward: -0.192 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.090, 10.283], loss: 0.003790, mae: 0.058555, mean_q: -0.353516
 37521/100000: episode: 446, duration: 0.214s, episode steps: 42, steps per second: 196, episode reward: 8.678, mean reward: 0.207 [0.054, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.465, 10.100], loss: 0.002463, mae: 0.049211, mean_q: -0.310336
 37563/100000: episode: 447, duration: 0.222s, episode steps: 42, steps per second: 189, episode reward: 8.059, mean reward: 0.192 [0.016, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.811, 10.165], loss: 0.003052, mae: 0.054494, mean_q: -0.305602
 37599/100000: episode: 448, duration: 0.204s, episode steps: 36, steps per second: 177, episode reward: 8.367, mean reward: 0.232 [0.078, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.990, 10.222], loss: 0.002804, mae: 0.052358, mean_q: -0.252867
 37659/100000: episode: 449, duration: 0.303s, episode steps: 60, steps per second: 198, episode reward: 16.911, mean reward: 0.282 [0.172, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.819 [-0.443, 10.384], loss: 0.002846, mae: 0.053277, mean_q: -0.286234
 37701/100000: episode: 450, duration: 0.224s, episode steps: 42, steps per second: 187, episode reward: 11.200, mean reward: 0.267 [0.104, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.612, 10.581], loss: 0.002906, mae: 0.054425, mean_q: -0.243907
 37757/100000: episode: 451, duration: 0.285s, episode steps: 56, steps per second: 197, episode reward: 21.215, mean reward: 0.379 [0.186, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.845 [-1.624, 10.336], loss: 0.004867, mae: 0.063163, mean_q: -0.266434
 37777/100000: episode: 452, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 5.304, mean reward: 0.265 [0.190, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.526, 10.441], loss: 0.002954, mae: 0.054757, mean_q: -0.238914
 37813/100000: episode: 453, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 9.436, mean reward: 0.262 [0.019, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-1.682, 10.120], loss: 0.002865, mae: 0.054294, mean_q: -0.249266
 37845/100000: episode: 454, duration: 0.163s, episode steps: 32, steps per second: 197, episode reward: 7.760, mean reward: 0.243 [0.139, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.137, 10.300], loss: 0.003016, mae: 0.054698, mean_q: -0.239448
 37881/100000: episode: 455, duration: 0.205s, episode steps: 36, steps per second: 176, episode reward: 6.339, mean reward: 0.176 [0.071, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.663, 10.100], loss: 0.003046, mae: 0.052941, mean_q: -0.305242
 37927/100000: episode: 456, duration: 0.239s, episode steps: 46, steps per second: 192, episode reward: 12.086, mean reward: 0.263 [0.141, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-1.420, 10.329], loss: 0.003098, mae: 0.055201, mean_q: -0.260708
 37983/100000: episode: 457, duration: 0.296s, episode steps: 56, steps per second: 189, episode reward: 13.009, mean reward: 0.232 [0.053, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-0.382, 10.240], loss: 0.002992, mae: 0.054734, mean_q: -0.221153
 38017/100000: episode: 458, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 12.026, mean reward: 0.354 [0.180, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.623, 10.554], loss: 0.002773, mae: 0.054022, mean_q: -0.222925
 38063/100000: episode: 459, duration: 0.235s, episode steps: 46, steps per second: 196, episode reward: 12.921, mean reward: 0.281 [0.122, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.300, 10.187], loss: 0.002880, mae: 0.053158, mean_q: -0.184211
 38097/100000: episode: 460, duration: 0.183s, episode steps: 34, steps per second: 185, episode reward: 7.448, mean reward: 0.219 [0.047, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.035, 10.179], loss: 0.003118, mae: 0.055941, mean_q: -0.148279
 38157/100000: episode: 461, duration: 0.331s, episode steps: 60, steps per second: 181, episode reward: 9.908, mean reward: 0.165 [0.026, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.818 [-0.998, 10.100], loss: 0.002892, mae: 0.053153, mean_q: -0.222693
 38217/100000: episode: 462, duration: 0.327s, episode steps: 60, steps per second: 183, episode reward: 14.832, mean reward: 0.247 [0.042, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.822 [-0.499, 10.202], loss: 0.002981, mae: 0.054251, mean_q: -0.201185
 38253/100000: episode: 463, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 7.383, mean reward: 0.205 [0.033, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.263, 10.145], loss: 0.003179, mae: 0.057998, mean_q: -0.197394
 38299/100000: episode: 464, duration: 0.235s, episode steps: 46, steps per second: 196, episode reward: 11.881, mean reward: 0.258 [0.128, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.679, 10.443], loss: 0.002903, mae: 0.053845, mean_q: -0.191778
 38345/100000: episode: 465, duration: 0.238s, episode steps: 46, steps per second: 193, episode reward: 10.697, mean reward: 0.233 [0.024, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.932, 10.237], loss: 0.002979, mae: 0.054794, mean_q: -0.148355
 38401/100000: episode: 466, duration: 0.272s, episode steps: 56, steps per second: 206, episode reward: 7.828, mean reward: 0.140 [0.010, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.836 [-1.221, 10.100], loss: 0.003336, mae: 0.058309, mean_q: -0.156115
 38421/100000: episode: 467, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 6.603, mean reward: 0.330 [0.182, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-1.360, 10.349], loss: 0.003148, mae: 0.057150, mean_q: -0.121190
 38457/100000: episode: 468, duration: 0.186s, episode steps: 36, steps per second: 193, episode reward: 6.995, mean reward: 0.194 [0.041, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.173, 10.238], loss: 0.002971, mae: 0.053641, mean_q: -0.163850
 38489/100000: episode: 469, duration: 0.166s, episode steps: 32, steps per second: 193, episode reward: 6.461, mean reward: 0.202 [0.074, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.097, 10.221], loss: 0.003376, mae: 0.058947, mean_q: -0.139988
 38535/100000: episode: 470, duration: 0.244s, episode steps: 46, steps per second: 188, episode reward: 9.312, mean reward: 0.202 [0.023, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.439, 10.210], loss: 0.002984, mae: 0.055615, mean_q: -0.143844
 38555/100000: episode: 471, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 3.773, mean reward: 0.189 [0.084, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.326], loss: 0.003075, mae: 0.056987, mean_q: -0.067545
 38600/100000: episode: 472, duration: 0.222s, episode steps: 45, steps per second: 203, episode reward: 13.439, mean reward: 0.299 [0.139, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.178, 10.433], loss: 0.003331, mae: 0.056908, mean_q: -0.152321
 38632/100000: episode: 473, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 10.899, mean reward: 0.341 [0.164, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.268, 10.564], loss: 0.003343, mae: 0.059839, mean_q: -0.092481
 38674/100000: episode: 474, duration: 0.216s, episode steps: 42, steps per second: 194, episode reward: 9.541, mean reward: 0.227 [0.054, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.805, 10.284], loss: 0.003149, mae: 0.056722, mean_q: -0.116830
 38706/100000: episode: 475, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 9.308, mean reward: 0.291 [0.175, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.497, 10.279], loss: 0.003762, mae: 0.062795, mean_q: -0.110691
 38729/100000: episode: 476, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 4.808, mean reward: 0.209 [0.134, 0.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.362, 10.325], loss: 0.003458, mae: 0.060181, mean_q: -0.071180
 38752/100000: episode: 477, duration: 0.112s, episode steps: 23, steps per second: 206, episode reward: 6.790, mean reward: 0.295 [0.196, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.230, 10.488], loss: 0.004909, mae: 0.068517, mean_q: -0.134059
 38775/100000: episode: 478, duration: 0.130s, episode steps: 23, steps per second: 176, episode reward: 4.116, mean reward: 0.179 [0.064, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-1.041, 10.130], loss: 0.004109, mae: 0.065836, mean_q: -0.123062
 38820/100000: episode: 479, duration: 0.255s, episode steps: 45, steps per second: 176, episode reward: 10.518, mean reward: 0.234 [0.063, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.880, 10.100], loss: 0.003084, mae: 0.057496, mean_q: -0.107737
 38854/100000: episode: 480, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 4.800, mean reward: 0.141 [0.015, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.766, 10.103], loss: 0.003365, mae: 0.059148, mean_q: -0.100978
 38910/100000: episode: 481, duration: 0.288s, episode steps: 56, steps per second: 195, episode reward: 12.912, mean reward: 0.231 [0.099, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-0.763, 10.353], loss: 0.002864, mae: 0.054648, mean_q: -0.086000
 38955/100000: episode: 482, duration: 0.244s, episode steps: 45, steps per second: 184, episode reward: 14.154, mean reward: 0.315 [0.117, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.378, 10.307], loss: 0.003106, mae: 0.057766, mean_q: -0.040560
 39000/100000: episode: 483, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 11.392, mean reward: 0.253 [0.116, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.649, 10.208], loss: 0.003169, mae: 0.059208, mean_q: -0.057180
 39020/100000: episode: 484, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 4.590, mean reward: 0.230 [0.095, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.404, 10.318], loss: 0.002866, mae: 0.055125, mean_q: -0.011253
 39043/100000: episode: 485, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 3.434, mean reward: 0.149 [0.023, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.074, 10.152], loss: 0.002950, mae: 0.056874, mean_q: -0.015588
 39066/100000: episode: 486, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 5.256, mean reward: 0.229 [0.022, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.214, 10.210], loss: 0.003070, mae: 0.058785, mean_q: -0.004356
 39098/100000: episode: 487, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 9.230, mean reward: 0.288 [0.198, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-1.759, 10.393], loss: 0.003761, mae: 0.064486, mean_q: -0.003123
 39144/100000: episode: 488, duration: 0.228s, episode steps: 46, steps per second: 202, episode reward: 12.079, mean reward: 0.263 [0.138, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.740, 10.246], loss: 0.008317, mae: 0.075974, mean_q: -0.032517
 39204/100000: episode: 489, duration: 0.319s, episode steps: 60, steps per second: 188, episode reward: 16.603, mean reward: 0.277 [0.130, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.812 [-0.361, 10.188], loss: 0.006691, mae: 0.076672, mean_q: -0.024747
 39224/100000: episode: 490, duration: 0.117s, episode steps: 20, steps per second: 172, episode reward: 6.471, mean reward: 0.324 [0.189, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.874, 10.385], loss: 0.003212, mae: 0.058865, mean_q: -0.114781
 39247/100000: episode: 491, duration: 0.140s, episode steps: 23, steps per second: 164, episode reward: 4.946, mean reward: 0.215 [0.111, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.146], loss: 0.003122, mae: 0.057970, mean_q: 0.010392
 39283/100000: episode: 492, duration: 0.195s, episode steps: 36, steps per second: 185, episode reward: 7.255, mean reward: 0.202 [0.009, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.734, 10.109], loss: 0.003315, mae: 0.060837, mean_q: -0.014862
 39343/100000: episode: 493, duration: 0.323s, episode steps: 60, steps per second: 186, episode reward: 14.262, mean reward: 0.238 [0.006, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.809 [-0.676, 10.130], loss: 0.003061, mae: 0.057358, mean_q: -0.006887
 39363/100000: episode: 494, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 6.755, mean reward: 0.338 [0.218, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.783, 10.403], loss: 0.003092, mae: 0.057566, mean_q: -0.008326
 39419/100000: episode: 495, duration: 0.281s, episode steps: 56, steps per second: 199, episode reward: 17.537, mean reward: 0.313 [0.137, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-0.746, 10.405], loss: 0.002929, mae: 0.057709, mean_q: 0.009830
 39451/100000: episode: 496, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 5.080, mean reward: 0.159 [0.021, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-1.155, 10.100], loss: 0.003100, mae: 0.058796, mean_q: -0.024421
 39496/100000: episode: 497, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 10.178, mean reward: 0.226 [0.024, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-1.203, 10.316], loss: 0.003000, mae: 0.058027, mean_q: 0.017761
 39556/100000: episode: 498, duration: 0.300s, episode steps: 60, steps per second: 200, episode reward: 10.095, mean reward: 0.168 [0.013, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.811 [-0.545, 10.190], loss: 0.003408, mae: 0.061231, mean_q: 0.037154
 39576/100000: episode: 499, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 4.923, mean reward: 0.246 [0.081, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.550, 10.259], loss: 0.003716, mae: 0.063439, mean_q: 0.041398
 39636/100000: episode: 500, duration: 0.308s, episode steps: 60, steps per second: 195, episode reward: 12.375, mean reward: 0.206 [0.027, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.818 [-0.661, 10.188], loss: 0.003094, mae: 0.058533, mean_q: 0.018392
 39696/100000: episode: 501, duration: 0.324s, episode steps: 60, steps per second: 185, episode reward: 20.493, mean reward: 0.342 [0.218, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.818 [-0.534, 10.411], loss: 0.002995, mae: 0.057560, mean_q: 0.019581
 39719/100000: episode: 502, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 5.124, mean reward: 0.223 [0.020, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.420, 10.142], loss: 0.003345, mae: 0.060171, mean_q: -0.003719
 39761/100000: episode: 503, duration: 0.230s, episode steps: 42, steps per second: 183, episode reward: 8.002, mean reward: 0.191 [0.026, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.615, 10.103], loss: 0.002990, mae: 0.058125, mean_q: 0.070612
 39803/100000: episode: 504, duration: 0.224s, episode steps: 42, steps per second: 188, episode reward: 12.328, mean reward: 0.294 [0.157, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.315, 10.418], loss: 0.002961, mae: 0.057217, mean_q: 0.027918
 39826/100000: episode: 505, duration: 0.121s, episode steps: 23, steps per second: 191, episode reward: 7.981, mean reward: 0.347 [0.241, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.042, 10.423], loss: 0.003120, mae: 0.059823, mean_q: 0.122578
 39858/100000: episode: 506, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 7.136, mean reward: 0.223 [0.102, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.240, 10.273], loss: 0.003037, mae: 0.059174, mean_q: 0.075727
 39903/100000: episode: 507, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 10.180, mean reward: 0.226 [0.104, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.167, 10.282], loss: 0.002955, mae: 0.057029, mean_q: 0.089704
 39959/100000: episode: 508, duration: 0.316s, episode steps: 56, steps per second: 177, episode reward: 16.139, mean reward: 0.288 [0.162, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.379, 10.405], loss: 0.003446, mae: 0.061396, mean_q: 0.086443
 40004/100000: episode: 509, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 13.871, mean reward: 0.308 [0.064, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.541, 10.227], loss: 0.003113, mae: 0.059227, mean_q: 0.111199
 40024/100000: episode: 510, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 4.991, mean reward: 0.250 [0.149, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.362], loss: 0.003621, mae: 0.063451, mean_q: 0.135556
 40084/100000: episode: 511, duration: 0.298s, episode steps: 60, steps per second: 201, episode reward: 14.683, mean reward: 0.245 [0.026, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.806 [-0.377, 10.196], loss: 0.003225, mae: 0.059177, mean_q: 0.075557
 40120/100000: episode: 512, duration: 0.173s, episode steps: 36, steps per second: 208, episode reward: 12.509, mean reward: 0.347 [0.235, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-1.338, 10.463], loss: 0.003008, mae: 0.057958, mean_q: 0.091756
 40152/100000: episode: 513, duration: 0.160s, episode steps: 32, steps per second: 199, episode reward: 6.431, mean reward: 0.201 [0.105, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.035, 10.170], loss: 0.003167, mae: 0.058941, mean_q: 0.092761
 40172/100000: episode: 514, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 5.509, mean reward: 0.275 [0.189, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.372], loss: 0.003231, mae: 0.059302, mean_q: 0.099119
 40204/100000: episode: 515, duration: 0.179s, episode steps: 32, steps per second: 179, episode reward: 10.088, mean reward: 0.315 [0.130, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.546, 10.408], loss: 0.003543, mae: 0.063085, mean_q: 0.129997
 40246/100000: episode: 516, duration: 0.215s, episode steps: 42, steps per second: 195, episode reward: 11.965, mean reward: 0.285 [0.143, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.917, 10.317], loss: 0.003416, mae: 0.061451, mean_q: 0.121061
 40278/100000: episode: 517, duration: 0.155s, episode steps: 32, steps per second: 207, episode reward: 6.318, mean reward: 0.197 [0.067, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.372, 10.209], loss: 0.003097, mae: 0.059977, mean_q: 0.122567
 40338/100000: episode: 518, duration: 0.308s, episode steps: 60, steps per second: 195, episode reward: 9.681, mean reward: 0.161 [0.035, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.810 [-1.340, 10.100], loss: 0.003009, mae: 0.058638, mean_q: 0.160844
 40398/100000: episode: 519, duration: 0.326s, episode steps: 60, steps per second: 184, episode reward: 18.042, mean reward: 0.301 [0.084, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.810 [-0.626, 10.405], loss: 0.003059, mae: 0.057946, mean_q: 0.143264
 40421/100000: episode: 520, duration: 0.127s, episode steps: 23, steps per second: 182, episode reward: 8.421, mean reward: 0.366 [0.184, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.460], loss: 0.003074, mae: 0.058450, mean_q: 0.199765
 40477/100000: episode: 521, duration: 0.295s, episode steps: 56, steps per second: 190, episode reward: 16.547, mean reward: 0.295 [0.102, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-0.248, 10.373], loss: 0.003072, mae: 0.059142, mean_q: 0.140361
 40500/100000: episode: 522, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 4.579, mean reward: 0.199 [0.014, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.043, 10.148], loss: 0.003283, mae: 0.059369, mean_q: 0.107815
 40523/100000: episode: 523, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 6.710, mean reward: 0.292 [0.187, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.185, 10.392], loss: 0.003274, mae: 0.059953, mean_q: 0.170088
 40543/100000: episode: 524, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 5.937, mean reward: 0.297 [0.182, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.058, 10.388], loss: 0.003246, mae: 0.059585, mean_q: 0.139312
 40566/100000: episode: 525, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 3.935, mean reward: 0.171 [0.018, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.505, 10.100], loss: 0.002998, mae: 0.060188, mean_q: 0.232142
 40600/100000: episode: 526, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 12.606, mean reward: 0.371 [0.224, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.437, 10.381], loss: 0.003210, mae: 0.060008, mean_q: 0.220786
 40636/100000: episode: 527, duration: 0.199s, episode steps: 36, steps per second: 181, episode reward: 9.317, mean reward: 0.259 [0.119, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.035, 10.429], loss: 0.002793, mae: 0.056747, mean_q: 0.200341
 40696/100000: episode: 528, duration: 0.323s, episode steps: 60, steps per second: 186, episode reward: 13.477, mean reward: 0.225 [0.046, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.810 [-1.255, 10.100], loss: 0.003592, mae: 0.065005, mean_q: 0.207235
 40752/100000: episode: 529, duration: 0.312s, episode steps: 56, steps per second: 180, episode reward: 13.625, mean reward: 0.243 [0.128, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-0.346, 10.364], loss: 0.003074, mae: 0.059404, mean_q: 0.208484
 40808/100000: episode: 530, duration: 0.289s, episode steps: 56, steps per second: 193, episode reward: 14.662, mean reward: 0.262 [0.156, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-1.668, 10.245], loss: 0.003249, mae: 0.060502, mean_q: 0.229797
 40850/100000: episode: 531, duration: 0.224s, episode steps: 42, steps per second: 188, episode reward: 8.541, mean reward: 0.203 [0.013, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.855, 10.138], loss: 0.003141, mae: 0.059949, mean_q: 0.253501
 40884/100000: episode: 532, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 12.376, mean reward: 0.364 [0.231, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.148, 10.438], loss: 0.003532, mae: 0.063925, mean_q: 0.267287
 40918/100000: episode: 533, duration: 0.173s, episode steps: 34, steps per second: 197, episode reward: 9.322, mean reward: 0.274 [0.136, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.477, 10.322], loss: 0.003363, mae: 0.062447, mean_q: 0.212177
 40950/100000: episode: 534, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 10.113, mean reward: 0.316 [0.226, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.709, 10.460], loss: 0.003323, mae: 0.062133, mean_q: 0.262866
[Info] 200-TH LEVEL FOUND: 0.7378243207931519, Considering 10/90 traces
 40995/100000: episode: 535, duration: 4.098s, episode steps: 45, steps per second: 11, episode reward: 15.386, mean reward: 0.342 [0.207, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.186, 10.418], loss: 0.003525, mae: 0.063863, mean_q: 0.272087
 41047/100000: episode: 536, duration: 0.292s, episode steps: 52, steps per second: 178, episode reward: 13.870, mean reward: 0.267 [0.073, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.838, 10.201], loss: 0.003170, mae: 0.060401, mean_q: 0.261184
 41053/100000: episode: 537, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 2.965, mean reward: 0.494 [0.453, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.619], loss: 0.003982, mae: 0.067826, mean_q: 0.308624
 41071/100000: episode: 538, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 6.645, mean reward: 0.369 [0.274, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-1.353, 10.473], loss: 0.004518, mae: 0.072686, mean_q: 0.284187
 41118/100000: episode: 539, duration: 0.275s, episode steps: 47, steps per second: 171, episode reward: 9.017, mean reward: 0.192 [0.076, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.279, 10.100], loss: 0.003753, mae: 0.067079, mean_q: 0.294841
 41124/100000: episode: 540, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 2.624, mean reward: 0.437 [0.421, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.495], loss: 0.002706, mae: 0.056769, mean_q: 0.262805
 41162/100000: episode: 541, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 14.970, mean reward: 0.394 [0.251, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.082, 10.535], loss: 0.003373, mae: 0.063114, mean_q: 0.288388
 41168/100000: episode: 542, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 3.119, mean reward: 0.520 [0.468, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.572], loss: 0.002917, mae: 0.061114, mean_q: 0.346236
 41195/100000: episode: 543, duration: 0.149s, episode steps: 27, steps per second: 182, episode reward: 11.304, mean reward: 0.419 [0.351, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.845, 10.520], loss: 0.003766, mae: 0.067331, mean_q: 0.299060
 41216/100000: episode: 544, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 7.335, mean reward: 0.349 [0.243, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.389], loss: 0.003047, mae: 0.059685, mean_q: 0.284159
 41263/100000: episode: 545, duration: 0.253s, episode steps: 47, steps per second: 186, episode reward: 12.930, mean reward: 0.275 [0.009, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.314, 10.145], loss: 0.003436, mae: 0.063541, mean_q: 0.309267
 41301/100000: episode: 546, duration: 0.215s, episode steps: 38, steps per second: 177, episode reward: 10.456, mean reward: 0.275 [0.195, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.658, 10.372], loss: 0.003450, mae: 0.063956, mean_q: 0.347810
 41328/100000: episode: 547, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 9.528, mean reward: 0.353 [0.222, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.853, 10.227], loss: 0.003175, mae: 0.060893, mean_q: 0.346095
 41334/100000: episode: 548, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 2.810, mean reward: 0.468 [0.420, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.268, 10.615], loss: 0.003151, mae: 0.057963, mean_q: 0.285815
 41381/100000: episode: 549, duration: 0.241s, episode steps: 47, steps per second: 195, episode reward: 17.991, mean reward: 0.383 [0.102, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.621, 10.234], loss: 0.003186, mae: 0.061269, mean_q: 0.319057
 41433/100000: episode: 550, duration: 0.275s, episode steps: 52, steps per second: 189, episode reward: 19.412, mean reward: 0.373 [0.278, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.401, 10.475], loss: 0.003316, mae: 0.062358, mean_q: 0.340503
 41460/100000: episode: 551, duration: 0.133s, episode steps: 27, steps per second: 202, episode reward: 13.157, mean reward: 0.487 [0.287, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.111, 10.684], loss: 0.003486, mae: 0.064234, mean_q: 0.350371
 41487/100000: episode: 552, duration: 0.131s, episode steps: 27, steps per second: 206, episode reward: 10.572, mean reward: 0.392 [0.233, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.103, 10.313], loss: 0.003341, mae: 0.063090, mean_q: 0.362471
 41507/100000: episode: 553, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 6.098, mean reward: 0.305 [0.129, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.109, 10.268], loss: 0.003281, mae: 0.061790, mean_q: 0.323317
 41525/100000: episode: 554, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 7.982, mean reward: 0.443 [0.359, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.478], loss: 0.003489, mae: 0.064804, mean_q: 0.377981
 41572/100000: episode: 555, duration: 0.279s, episode steps: 47, steps per second: 168, episode reward: 19.923, mean reward: 0.424 [0.336, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-1.205, 10.628], loss: 0.003215, mae: 0.061355, mean_q: 0.363827
 41592/100000: episode: 556, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 6.225, mean reward: 0.311 [0.136, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.578, 10.267], loss: 0.003662, mae: 0.066600, mean_q: 0.381653
 41642/100000: episode: 557, duration: 0.267s, episode steps: 50, steps per second: 187, episode reward: 7.646, mean reward: 0.153 [0.002, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.372, 10.100], loss: 0.003517, mae: 0.064253, mean_q: 0.368861
 41692/100000: episode: 558, duration: 0.261s, episode steps: 50, steps per second: 192, episode reward: 14.751, mean reward: 0.295 [0.140, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.355, 10.273], loss: 0.003491, mae: 0.065063, mean_q: 0.406445
 41719/100000: episode: 559, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 10.218, mean reward: 0.378 [0.309, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.524, 10.316], loss: 0.003185, mae: 0.061289, mean_q: 0.431266
 41746/100000: episode: 560, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 10.241, mean reward: 0.379 [0.216, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.249, 10.348], loss: 0.003246, mae: 0.061410, mean_q: 0.402950
 41767/100000: episode: 561, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 8.574, mean reward: 0.408 [0.257, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.492], loss: 0.003264, mae: 0.061538, mean_q: 0.443503
 41817/100000: episode: 562, duration: 0.255s, episode steps: 50, steps per second: 196, episode reward: 11.324, mean reward: 0.226 [0.072, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.991, 10.204], loss: 0.003325, mae: 0.063252, mean_q: 0.419560
 41837/100000: episode: 563, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 7.175, mean reward: 0.359 [0.230, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.041, 10.446], loss: 0.003909, mae: 0.068440, mean_q: 0.444691
 41864/100000: episode: 564, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 11.047, mean reward: 0.409 [0.300, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.235, 10.505], loss: 0.003571, mae: 0.064784, mean_q: 0.440450
 41882/100000: episode: 565, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 8.644, mean reward: 0.480 [0.403, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.211, 10.512], loss: 0.003456, mae: 0.064110, mean_q: 0.421779
 41888/100000: episode: 566, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 2.797, mean reward: 0.466 [0.388, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.622, 10.537], loss: 0.004151, mae: 0.070242, mean_q: 0.393557
 41909/100000: episode: 567, duration: 0.109s, episode steps: 21, steps per second: 192, episode reward: 7.205, mean reward: 0.343 [0.249, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.429], loss: 0.003998, mae: 0.071442, mean_q: 0.468886
 41915/100000: episode: 568, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 2.756, mean reward: 0.459 [0.423, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.390, 10.614], loss: 0.003153, mae: 0.062013, mean_q: 0.418275
 41942/100000: episode: 569, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 9.267, mean reward: 0.343 [0.116, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.091, 10.274], loss: 0.003556, mae: 0.065722, mean_q: 0.457361
 41992/100000: episode: 570, duration: 0.282s, episode steps: 50, steps per second: 178, episode reward: 10.234, mean reward: 0.205 [0.020, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.343, 10.100], loss: 0.004982, mae: 0.068845, mean_q: 0.441168
 42010/100000: episode: 571, duration: 0.103s, episode steps: 18, steps per second: 176, episode reward: 6.422, mean reward: 0.357 [0.243, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.410], loss: 0.004589, mae: 0.071140, mean_q: 0.490539
 42048/100000: episode: 572, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 8.607, mean reward: 0.227 [0.070, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.540, 10.100], loss: 0.003637, mae: 0.065793, mean_q: 0.463351
 42054/100000: episode: 573, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 2.448, mean reward: 0.408 [0.364, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.531], loss: 0.003175, mae: 0.060718, mean_q: 0.451163
 42104/100000: episode: 574, duration: 0.267s, episode steps: 50, steps per second: 188, episode reward: 19.243, mean reward: 0.385 [0.220, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.370, 10.411], loss: 0.004459, mae: 0.068807, mean_q: 0.486038
 42142/100000: episode: 575, duration: 0.188s, episode steps: 38, steps per second: 202, episode reward: 16.039, mean reward: 0.422 [0.262, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.462, 10.448], loss: 0.003426, mae: 0.064494, mean_q: 0.465839
 42162/100000: episode: 576, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 8.271, mean reward: 0.414 [0.337, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.437], loss: 0.003367, mae: 0.064378, mean_q: 0.495987
 42180/100000: episode: 577, duration: 0.086s, episode steps: 18, steps per second: 208, episode reward: 7.368, mean reward: 0.409 [0.343, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.442], loss: 0.003452, mae: 0.064437, mean_q: 0.505020
 42201/100000: episode: 578, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 8.664, mean reward: 0.413 [0.347, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.358, 10.547], loss: 0.003048, mae: 0.061525, mean_q: 0.499624
 42253/100000: episode: 579, duration: 0.291s, episode steps: 52, steps per second: 179, episode reward: 14.865, mean reward: 0.286 [0.107, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.639, 10.354], loss: 0.003433, mae: 0.064420, mean_q: 0.511043
 42300/100000: episode: 580, duration: 0.261s, episode steps: 47, steps per second: 180, episode reward: 11.535, mean reward: 0.245 [0.118, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.643, 10.289], loss: 0.003621, mae: 0.066882, mean_q: 0.523825
 42327/100000: episode: 581, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 7.818, mean reward: 0.290 [0.166, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.308, 10.304], loss: 0.003413, mae: 0.065014, mean_q: 0.532665
 42379/100000: episode: 582, duration: 0.262s, episode steps: 52, steps per second: 198, episode reward: 15.586, mean reward: 0.300 [0.132, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-1.310, 10.435], loss: 0.003634, mae: 0.066174, mean_q: 0.534610
 42431/100000: episode: 583, duration: 0.289s, episode steps: 52, steps per second: 180, episode reward: 18.878, mean reward: 0.363 [0.206, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.365, 10.487], loss: 0.003193, mae: 0.062862, mean_q: 0.533040
 42469/100000: episode: 584, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 12.794, mean reward: 0.337 [0.147, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.447, 10.315], loss: 0.003362, mae: 0.063893, mean_q: 0.529706
 42487/100000: episode: 585, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 6.916, mean reward: 0.384 [0.251, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-1.011, 10.381], loss: 0.003265, mae: 0.063768, mean_q: 0.529513
 42505/100000: episode: 586, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 4.758, mean reward: 0.264 [0.104, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.257], loss: 0.003814, mae: 0.067451, mean_q: 0.530976
 42511/100000: episode: 587, duration: 0.042s, episode steps: 6, steps per second: 143, episode reward: 2.776, mean reward: 0.463 [0.428, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.226, 10.540], loss: 0.003879, mae: 0.068236, mean_q: 0.541028
 42529/100000: episode: 588, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 7.147, mean reward: 0.397 [0.338, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.039, 10.495], loss: 0.003292, mae: 0.063705, mean_q: 0.543642
 42556/100000: episode: 589, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 8.538, mean reward: 0.316 [0.198, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.368], loss: 0.003826, mae: 0.067593, mean_q: 0.540315
 42594/100000: episode: 590, duration: 0.188s, episode steps: 38, steps per second: 202, episode reward: 12.503, mean reward: 0.329 [0.166, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.522, 10.295], loss: 0.003423, mae: 0.064472, mean_q: 0.528411
 42614/100000: episode: 591, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 6.779, mean reward: 0.339 [0.085, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.263, 10.229], loss: 0.003669, mae: 0.067607, mean_q: 0.543748
 42632/100000: episode: 592, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 7.289, mean reward: 0.405 [0.297, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.428, 10.395], loss: 0.003557, mae: 0.065881, mean_q: 0.534612
 42670/100000: episode: 593, duration: 0.196s, episode steps: 38, steps per second: 194, episode reward: 13.814, mean reward: 0.364 [0.278, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.101, 10.537], loss: 0.003447, mae: 0.063728, mean_q: 0.524038
 42717/100000: episode: 594, duration: 0.270s, episode steps: 47, steps per second: 174, episode reward: 10.459, mean reward: 0.223 [0.021, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.913, 10.189], loss: 0.003757, mae: 0.068218, mean_q: 0.543824
 42755/100000: episode: 595, duration: 0.205s, episode steps: 38, steps per second: 185, episode reward: 12.572, mean reward: 0.331 [0.050, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.880, 10.212], loss: 0.003217, mae: 0.063514, mean_q: 0.533397
 42773/100000: episode: 596, duration: 0.086s, episode steps: 18, steps per second: 210, episode reward: 7.134, mean reward: 0.396 [0.281, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.036, 10.490], loss: 0.003429, mae: 0.064922, mean_q: 0.549076
 42825/100000: episode: 597, duration: 0.272s, episode steps: 52, steps per second: 191, episode reward: 16.413, mean reward: 0.316 [0.102, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.536, 10.271], loss: 0.003365, mae: 0.063198, mean_q: 0.542082
 42846/100000: episode: 598, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 8.433, mean reward: 0.402 [0.224, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.141, 10.504], loss: 0.003166, mae: 0.064282, mean_q: 0.541121
 42884/100000: episode: 599, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 12.646, mean reward: 0.333 [0.072, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.328, 10.174], loss: 0.003890, mae: 0.068511, mean_q: 0.550039
 42902/100000: episode: 600, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 6.783, mean reward: 0.377 [0.303, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.648, 10.454], loss: 0.003501, mae: 0.065979, mean_q: 0.541815
 42929/100000: episode: 601, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 10.330, mean reward: 0.383 [0.296, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.531, 10.508], loss: 0.003689, mae: 0.067670, mean_q: 0.548306
 42979/100000: episode: 602, duration: 0.262s, episode steps: 50, steps per second: 191, episode reward: 19.440, mean reward: 0.389 [0.162, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.498, 10.346], loss: 0.003624, mae: 0.065647, mean_q: 0.545697
 43017/100000: episode: 603, duration: 0.206s, episode steps: 38, steps per second: 184, episode reward: 13.656, mean reward: 0.359 [0.258, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.102, 10.521], loss: 0.003374, mae: 0.064462, mean_q: 0.564816
 43067/100000: episode: 604, duration: 0.254s, episode steps: 50, steps per second: 197, episode reward: 12.561, mean reward: 0.251 [0.113, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.355, 10.323], loss: 0.003210, mae: 0.062190, mean_q: 0.547580
 43073/100000: episode: 605, duration: 0.038s, episode steps: 6, steps per second: 160, episode reward: 2.343, mean reward: 0.391 [0.341, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.461], loss: 0.003696, mae: 0.066319, mean_q: 0.562970
 43100/100000: episode: 606, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 9.058, mean reward: 0.335 [0.216, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.326, 10.474], loss: 0.003725, mae: 0.068777, mean_q: 0.543896
 43152/100000: episode: 607, duration: 0.287s, episode steps: 52, steps per second: 181, episode reward: 19.272, mean reward: 0.371 [0.226, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.356, 10.421], loss: 0.003597, mae: 0.066165, mean_q: 0.562422
 43158/100000: episode: 608, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 2.632, mean reward: 0.439 [0.420, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.771, 10.490], loss: 0.003067, mae: 0.062719, mean_q: 0.548334
 43196/100000: episode: 609, duration: 0.196s, episode steps: 38, steps per second: 193, episode reward: 12.812, mean reward: 0.337 [0.180, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.099, 10.572], loss: 0.003585, mae: 0.065401, mean_q: 0.559453
 43216/100000: episode: 610, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 9.424, mean reward: 0.471 [0.378, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.491, 10.432], loss: 0.003153, mae: 0.061749, mean_q: 0.563893
 43243/100000: episode: 611, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 10.106, mean reward: 0.374 [0.217, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.511, 10.292], loss: 0.003488, mae: 0.065653, mean_q: 0.566520
 43293/100000: episode: 612, duration: 0.283s, episode steps: 50, steps per second: 176, episode reward: 14.916, mean reward: 0.298 [0.143, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-1.522, 10.304], loss: 0.003338, mae: 0.063437, mean_q: 0.563353
 43311/100000: episode: 613, duration: 0.117s, episode steps: 18, steps per second: 154, episode reward: 5.951, mean reward: 0.331 [0.144, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.420, 10.195], loss: 0.003554, mae: 0.063955, mean_q: 0.569102
 43363/100000: episode: 614, duration: 0.272s, episode steps: 52, steps per second: 191, episode reward: 13.504, mean reward: 0.260 [0.022, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-1.225, 10.327], loss: 0.003579, mae: 0.066396, mean_q: 0.557132
 43401/100000: episode: 615, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 14.494, mean reward: 0.381 [0.157, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.524, 10.252], loss: 0.003204, mae: 0.062876, mean_q: 0.571618
 43419/100000: episode: 616, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 5.441, mean reward: 0.302 [0.170, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.796, 10.262], loss: 0.003607, mae: 0.066312, mean_q: 0.558018
 43439/100000: episode: 617, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 9.492, mean reward: 0.475 [0.383, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-1.304, 10.519], loss: 0.003091, mae: 0.061432, mean_q: 0.569683
 43466/100000: episode: 618, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 10.626, mean reward: 0.394 [0.278, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.764, 10.418], loss: 0.003313, mae: 0.062687, mean_q: 0.567343
 43472/100000: episode: 619, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 3.172, mean reward: 0.529 [0.474, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.581], loss: 0.003411, mae: 0.064810, mean_q: 0.579910
 43510/100000: episode: 620, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 11.139, mean reward: 0.293 [0.184, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.084, 10.468], loss: 0.003328, mae: 0.063383, mean_q: 0.570881
 43530/100000: episode: 621, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 7.377, mean reward: 0.369 [0.264, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.366], loss: 0.003517, mae: 0.065749, mean_q: 0.565235
 43551/100000: episode: 622, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 6.280, mean reward: 0.299 [0.184, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.320], loss: 0.003284, mae: 0.062539, mean_q: 0.570752
 43578/100000: episode: 623, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 8.986, mean reward: 0.333 [0.204, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.741, 10.418], loss: 0.003487, mae: 0.063852, mean_q: 0.569975
 43596/100000: episode: 624, duration: 0.109s, episode steps: 18, steps per second: 166, episode reward: 6.954, mean reward: 0.386 [0.333, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.472, 10.458], loss: 0.003332, mae: 0.060865, mean_q: 0.566871
[Info] 300-TH LEVEL FOUND: 0.8701528310775757, Considering 10/90 traces
 43614/100000: episode: 625, duration: 3.946s, episode steps: 18, steps per second: 5, episode reward: 7.398, mean reward: 0.411 [0.348, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.257, 10.478], loss: 0.003521, mae: 0.061916, mean_q: 0.578739
 43648/100000: episode: 626, duration: 0.180s, episode steps: 34, steps per second: 188, episode reward: 13.458, mean reward: 0.396 [0.207, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.260, 10.361], loss: 0.003291, mae: 0.063048, mean_q: 0.574732
 43674/100000: episode: 627, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 11.246, mean reward: 0.433 [0.254, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.918, 10.350], loss: 0.002937, mae: 0.060229, mean_q: 0.573508
 43706/100000: episode: 628, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 11.617, mean reward: 0.363 [0.053, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.214, 10.354], loss: 0.003635, mae: 0.066724, mean_q: 0.576941
 43740/100000: episode: 629, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 13.740, mean reward: 0.404 [0.329, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.325, 10.424], loss: 0.003388, mae: 0.064342, mean_q: 0.580126
 43763/100000: episode: 630, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 9.031, mean reward: 0.393 [0.270, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.323, 10.457], loss: 0.003519, mae: 0.064743, mean_q: 0.593016
 43784/100000: episode: 631, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 8.992, mean reward: 0.428 [0.359, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.526], loss: 0.003402, mae: 0.064866, mean_q: 0.585481
 43810/100000: episode: 632, duration: 0.123s, episode steps: 26, steps per second: 211, episode reward: 13.185, mean reward: 0.507 [0.447, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.633], loss: 0.003451, mae: 0.062553, mean_q: 0.587569
 43827/100000: episode: 633, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 7.707, mean reward: 0.453 [0.311, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.591, 10.382], loss: 0.003239, mae: 0.062781, mean_q: 0.593621
 43853/100000: episode: 634, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 13.816, mean reward: 0.531 [0.402, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.532], loss: 0.003757, mae: 0.067338, mean_q: 0.605193
 43865/100000: episode: 635, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 5.827, mean reward: 0.486 [0.398, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.540], loss: 0.003850, mae: 0.067725, mean_q: 0.594756
 43891/100000: episode: 636, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 11.948, mean reward: 0.460 [0.334, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.503], loss: 0.003592, mae: 0.065116, mean_q: 0.593064
 43912/100000: episode: 637, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 7.829, mean reward: 0.373 [0.250, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.409], loss: 0.003307, mae: 0.062172, mean_q: 0.588606
 43944/100000: episode: 638, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 9.651, mean reward: 0.302 [0.021, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.308, 10.129], loss: 0.003391, mae: 0.065418, mean_q: 0.605281
 43965/100000: episode: 639, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 7.865, mean reward: 0.375 [0.280, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.254, 10.432], loss: 0.002965, mae: 0.059743, mean_q: 0.604753
 43976/100000: episode: 640, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 4.637, mean reward: 0.422 [0.398, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.218, 10.542], loss: 0.003523, mae: 0.064351, mean_q: 0.592812
 43999/100000: episode: 641, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 8.450, mean reward: 0.367 [0.197, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.330], loss: 0.003295, mae: 0.062572, mean_q: 0.601518
 44010/100000: episode: 642, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 5.086, mean reward: 0.462 [0.368, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.843, 10.497], loss: 0.003413, mae: 0.062794, mean_q: 0.593261
 44031/100000: episode: 643, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 10.667, mean reward: 0.508 [0.407, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.376, 10.585], loss: 0.003389, mae: 0.064064, mean_q: 0.600476
 44042/100000: episode: 644, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 4.289, mean reward: 0.390 [0.305, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.483], loss: 0.003848, mae: 0.068267, mean_q: 0.595246
 44068/100000: episode: 645, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 12.219, mean reward: 0.470 [0.366, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.317, 10.634], loss: 0.002914, mae: 0.059675, mean_q: 0.609340
 44089/100000: episode: 646, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 9.978, mean reward: 0.475 [0.417, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.563, 10.562], loss: 0.003257, mae: 0.061678, mean_q: 0.619985
 44123/100000: episode: 647, duration: 0.172s, episode steps: 34, steps per second: 198, episode reward: 12.700, mean reward: 0.374 [0.256, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.535, 10.395], loss: 0.003238, mae: 0.063335, mean_q: 0.606416
 44155/100000: episode: 648, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 14.802, mean reward: 0.463 [0.326, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.552], loss: 0.003278, mae: 0.062714, mean_q: 0.612764
 44178/100000: episode: 649, duration: 0.141s, episode steps: 23, steps per second: 163, episode reward: 8.826, mean reward: 0.384 [0.261, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.223, 10.400], loss: 0.003744, mae: 0.067770, mean_q: 0.618751
 44201/100000: episode: 650, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 10.664, mean reward: 0.464 [0.404, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.608], loss: 0.003642, mae: 0.064556, mean_q: 0.613748
 44224/100000: episode: 651, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 8.183, mean reward: 0.356 [0.236, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.238, 10.407], loss: 0.003260, mae: 0.062851, mean_q: 0.624739
 44235/100000: episode: 652, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 3.510, mean reward: 0.319 [0.212, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.476], loss: 0.003633, mae: 0.067902, mean_q: 0.612995
 44258/100000: episode: 653, duration: 0.127s, episode steps: 23, steps per second: 182, episode reward: 11.140, mean reward: 0.484 [0.302, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.446], loss: 0.003543, mae: 0.065013, mean_q: 0.614274
 44284/100000: episode: 654, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 12.017, mean reward: 0.462 [0.315, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.213, 10.366], loss: 0.003736, mae: 0.068257, mean_q: 0.621914
 44296/100000: episode: 655, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 6.612, mean reward: 0.551 [0.505, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.564], loss: 0.003143, mae: 0.060666, mean_q: 0.627749
 44317/100000: episode: 656, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 9.443, mean reward: 0.450 [0.340, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.497, 10.441], loss: 0.003557, mae: 0.067696, mean_q: 0.607879
 44349/100000: episode: 657, duration: 0.190s, episode steps: 32, steps per second: 168, episode reward: 12.204, mean reward: 0.381 [0.195, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.517, 10.308], loss: 0.003445, mae: 0.063080, mean_q: 0.627856
 44375/100000: episode: 658, duration: 0.156s, episode steps: 26, steps per second: 166, episode reward: 13.743, mean reward: 0.529 [0.414, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.931, 10.438], loss: 0.003307, mae: 0.063341, mean_q: 0.636581
 44407/100000: episode: 659, duration: 0.190s, episode steps: 32, steps per second: 169, episode reward: 10.774, mean reward: 0.337 [0.165, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.768, 10.320], loss: 0.002953, mae: 0.059864, mean_q: 0.632761
 44430/100000: episode: 660, duration: 0.137s, episode steps: 23, steps per second: 167, episode reward: 9.008, mean reward: 0.392 [0.247, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.369], loss: 0.003159, mae: 0.059611, mean_q: 0.648853
 44453/100000: episode: 661, duration: 0.134s, episode steps: 23, steps per second: 172, episode reward: 9.365, mean reward: 0.407 [0.171, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.421, 10.377], loss: 0.003296, mae: 0.063856, mean_q: 0.635028
 44476/100000: episode: 662, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 11.287, mean reward: 0.491 [0.353, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.888, 10.347], loss: 0.003184, mae: 0.062461, mean_q: 0.642337
 44510/100000: episode: 663, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 10.894, mean reward: 0.320 [0.063, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.752, 10.209], loss: 0.003487, mae: 0.064499, mean_q: 0.636786
 44542/100000: episode: 664, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 9.085, mean reward: 0.284 [0.129, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.035, 10.249], loss: 0.003374, mae: 0.062851, mean_q: 0.646852
 44568/100000: episode: 665, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 11.811, mean reward: 0.454 [0.352, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.474], loss: 0.003242, mae: 0.061695, mean_q: 0.648461
[Info] FALSIFICATION!
 44583/100000: episode: 666, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 17.277, mean reward: 1.152 [0.413, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.018, 10.749], loss: 0.003299, mae: 0.063561, mean_q: 0.653887
 44683/100000: episode: 667, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.668, mean reward: -0.177 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.439, 10.179], loss: 0.003269, mae: 0.062976, mean_q: 0.627487
 44783/100000: episode: 668, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.639, mean reward: -0.196 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.548, 10.344], loss: 0.004029, mae: 0.066251, mean_q: 0.599865
 44883/100000: episode: 669, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.163, mean reward: -0.192 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.416, 10.098], loss: 0.016888, mae: 0.067413, mean_q: 0.598971
 44983/100000: episode: 670, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.646, mean reward: -0.166 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.447, 10.098], loss: 0.017860, mae: 0.077519, mean_q: 0.574091
 45083/100000: episode: 671, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -11.771, mean reward: -0.118 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.535, 10.137], loss: 0.003694, mae: 0.064457, mean_q: 0.552520
 45183/100000: episode: 672, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.954, mean reward: -0.180 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.859, 10.198], loss: 0.030929, mae: 0.082893, mean_q: 0.558896
 45283/100000: episode: 673, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -15.461, mean reward: -0.155 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.706, 10.171], loss: 0.003112, mae: 0.059156, mean_q: 0.519012
 45383/100000: episode: 674, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -12.009, mean reward: -0.120 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.914, 10.098], loss: 0.044386, mae: 0.084420, mean_q: 0.524727
 45483/100000: episode: 675, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.811, mean reward: -0.168 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.792, 10.098], loss: 0.030696, mae: 0.079750, mean_q: 0.507328
 45583/100000: episode: 676, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.271, mean reward: -0.193 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.779, 10.098], loss: 0.003952, mae: 0.062509, mean_q: 0.484089
 45683/100000: episode: 677, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -12.825, mean reward: -0.128 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.109, 10.176], loss: 0.016968, mae: 0.070427, mean_q: 0.487681
 45783/100000: episode: 678, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.198, mean reward: -0.172 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.819, 10.148], loss: 0.043599, mae: 0.086337, mean_q: 0.466040
 45883/100000: episode: 679, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.758, mean reward: -0.178 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.707, 10.098], loss: 0.003301, mae: 0.061016, mean_q: 0.447156
 45983/100000: episode: 680, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.438, mean reward: -0.174 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.788, 10.098], loss: 0.016438, mae: 0.067240, mean_q: 0.412147
 46083/100000: episode: 681, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.284, mean reward: -0.183 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.689, 10.114], loss: 0.016395, mae: 0.067831, mean_q: 0.376731
 46183/100000: episode: 682, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -13.462, mean reward: -0.135 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.538, 10.487], loss: 0.002937, mae: 0.057851, mean_q: 0.357357
 46283/100000: episode: 683, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.916, mean reward: -0.179 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.294, 10.100], loss: 0.029341, mae: 0.070512, mean_q: 0.349758
 46383/100000: episode: 684, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -19.995, mean reward: -0.200 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.453, 10.098], loss: 0.016376, mae: 0.065830, mean_q: 0.300115
 46483/100000: episode: 685, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.947, mean reward: -0.179 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.837, 10.120], loss: 0.003031, mae: 0.059139, mean_q: 0.345215
 46583/100000: episode: 686, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.629, mean reward: -0.166 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.475, 10.159], loss: 0.029275, mae: 0.073201, mean_q: 0.267954
 46683/100000: episode: 687, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.538, mean reward: -0.165 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.163, 10.098], loss: 0.003145, mae: 0.058808, mean_q: 0.253594
 46783/100000: episode: 688, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.265, mean reward: -0.163 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.556, 10.200], loss: 0.002660, mae: 0.053498, mean_q: 0.219479
 46883/100000: episode: 689, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -16.968, mean reward: -0.170 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.606, 10.098], loss: 0.029267, mae: 0.069821, mean_q: 0.234835
 46983/100000: episode: 690, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.774, mean reward: -0.178 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.704, 10.177], loss: 0.017691, mae: 0.077203, mean_q: 0.221572
 47083/100000: episode: 691, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.229, mean reward: -0.182 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.140, 10.281], loss: 0.003167, mae: 0.058995, mean_q: 0.182797
 47183/100000: episode: 692, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.574, mean reward: -0.176 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.946, 10.098], loss: 0.002870, mae: 0.055605, mean_q: 0.180747
 47283/100000: episode: 693, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.272, mean reward: -0.163 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.320, 10.098], loss: 0.002952, mae: 0.057055, mean_q: 0.149016
 47383/100000: episode: 694, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -12.513, mean reward: -0.125 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.965, 10.098], loss: 0.015999, mae: 0.061741, mean_q: 0.140568
 47483/100000: episode: 695, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.722, mean reward: -0.177 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.403, 10.098], loss: 0.016925, mae: 0.068960, mean_q: 0.116108
 47583/100000: episode: 696, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.012, mean reward: -0.180 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.591, 10.196], loss: 0.002779, mae: 0.054741, mean_q: 0.108174
 47683/100000: episode: 697, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.721, mean reward: -0.197 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.804, 10.098], loss: 0.016936, mae: 0.067780, mean_q: 0.086094
 47783/100000: episode: 698, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -20.367, mean reward: -0.204 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.232, 10.117], loss: 0.016430, mae: 0.064291, mean_q: 0.076313
 47883/100000: episode: 699, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.575, mean reward: -0.166 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.401, 10.169], loss: 0.002839, mae: 0.054568, mean_q: 0.032744
 47983/100000: episode: 700, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.688, mean reward: -0.147 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.147, 10.098], loss: 0.017289, mae: 0.068982, mean_q: 0.009861
 48083/100000: episode: 701, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.612, mean reward: -0.176 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.809, 10.245], loss: 0.006717, mae: 0.070207, mean_q: 0.009810
 48183/100000: episode: 702, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -15.357, mean reward: -0.154 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.178, 10.098], loss: 0.004132, mae: 0.065556, mean_q: -0.035457
 48283/100000: episode: 703, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.607, mean reward: -0.156 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.878, 10.246], loss: 0.016360, mae: 0.063664, mean_q: -0.036829
 48383/100000: episode: 704, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -12.930, mean reward: -0.129 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.795, 10.098], loss: 0.029796, mae: 0.072796, mean_q: -0.045584
 48483/100000: episode: 705, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.449, mean reward: -0.174 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.597, 10.200], loss: 0.003024, mae: 0.055296, mean_q: -0.086195
 48583/100000: episode: 706, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.873, mean reward: -0.199 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.301, 10.098], loss: 0.017001, mae: 0.065831, mean_q: -0.089932
 48683/100000: episode: 707, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.213, mean reward: -0.172 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.966, 10.568], loss: 0.015953, mae: 0.059913, mean_q: -0.121370
 48783/100000: episode: 708, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.074, mean reward: -0.161 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.645, 10.181], loss: 0.002793, mae: 0.052839, mean_q: -0.156459
 48883/100000: episode: 709, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -17.076, mean reward: -0.171 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.730, 10.098], loss: 0.016492, mae: 0.062953, mean_q: -0.149573
 48983/100000: episode: 710, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -16.454, mean reward: -0.165 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.371, 10.139], loss: 0.043079, mae: 0.081197, mean_q: -0.200874
 49083/100000: episode: 711, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.038, mean reward: -0.170 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.409, 10.098], loss: 0.016193, mae: 0.062950, mean_q: -0.215354
 49183/100000: episode: 712, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.853, mean reward: -0.179 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.703, 10.157], loss: 0.016321, mae: 0.064192, mean_q: -0.238390
 49283/100000: episode: 713, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.591, mean reward: -0.196 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.428, 10.104], loss: 0.003455, mae: 0.058626, mean_q: -0.274776
 49383/100000: episode: 714, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -17.973, mean reward: -0.180 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.919, 10.111], loss: 0.002803, mae: 0.053291, mean_q: -0.275548
 49483/100000: episode: 715, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.544, mean reward: -0.165 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.964, 10.156], loss: 0.002805, mae: 0.052078, mean_q: -0.310568
 49583/100000: episode: 716, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.968, mean reward: -0.170 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.237, 10.311], loss: 0.016128, mae: 0.059264, mean_q: -0.307469
 49683/100000: episode: 717, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -20.442, mean reward: -0.204 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.467, 10.156], loss: 0.002714, mae: 0.051925, mean_q: -0.333768
 49783/100000: episode: 718, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -11.132, mean reward: -0.111 [-1.000, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.191, 10.098], loss: 0.002652, mae: 0.052426, mean_q: -0.333248
 49883/100000: episode: 719, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.913, mean reward: -0.169 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.665, 10.187], loss: 0.002730, mae: 0.051666, mean_q: -0.312962
 49983/100000: episode: 720, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.601, mean reward: -0.136 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.927, 10.098], loss: 0.002658, mae: 0.051138, mean_q: -0.316806
 50083/100000: episode: 721, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.704, mean reward: -0.187 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.493, 10.254], loss: 0.002898, mae: 0.055173, mean_q: -0.317649
 50183/100000: episode: 722, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -13.675, mean reward: -0.137 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.729, 10.150], loss: 0.002700, mae: 0.052627, mean_q: -0.330619
 50283/100000: episode: 723, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.794, mean reward: -0.138 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.434, 10.193], loss: 0.002732, mae: 0.051087, mean_q: -0.322909
 50383/100000: episode: 724, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.633, mean reward: -0.196 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.685, 10.098], loss: 0.002685, mae: 0.051072, mean_q: -0.310126
 50483/100000: episode: 725, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.984, mean reward: -0.160 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.359, 10.098], loss: 0.002850, mae: 0.052003, mean_q: -0.313209
 50583/100000: episode: 726, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -11.517, mean reward: -0.115 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.677, 10.192], loss: 0.002798, mae: 0.052432, mean_q: -0.339786
 50683/100000: episode: 727, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -15.098, mean reward: -0.151 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.154, 10.476], loss: 0.004099, mae: 0.060911, mean_q: -0.315161
 50783/100000: episode: 728, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.822, mean reward: -0.168 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.008, 10.377], loss: 0.004639, mae: 0.062590, mean_q: -0.303250
 50883/100000: episode: 729, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.605, mean reward: -0.166 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.454, 10.098], loss: 0.002644, mae: 0.050803, mean_q: -0.315210
 50983/100000: episode: 730, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.710, mean reward: -0.177 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.741, 10.099], loss: 0.002682, mae: 0.049915, mean_q: -0.315786
 51083/100000: episode: 731, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.715, mean reward: -0.157 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.262, 10.168], loss: 0.002861, mae: 0.052370, mean_q: -0.311596
 51183/100000: episode: 732, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.882, mean reward: -0.179 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.736, 10.098], loss: 0.002742, mae: 0.051217, mean_q: -0.308093
 51283/100000: episode: 733, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.000, mean reward: -0.170 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.185, 10.185], loss: 0.002849, mae: 0.051577, mean_q: -0.309050
 51383/100000: episode: 734, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -15.236, mean reward: -0.152 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.600, 10.372], loss: 0.002497, mae: 0.049472, mean_q: -0.327025
 51483/100000: episode: 735, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.399, mean reward: -0.164 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.632, 10.136], loss: 0.002535, mae: 0.049640, mean_q: -0.300579
 51583/100000: episode: 736, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.864, mean reward: -0.189 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.027, 10.098], loss: 0.002448, mae: 0.048351, mean_q: -0.346056
 51683/100000: episode: 737, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -20.156, mean reward: -0.202 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.872, 10.116], loss: 0.002633, mae: 0.050102, mean_q: -0.300710
 51783/100000: episode: 738, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.454, mean reward: -0.175 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.630, 10.111], loss: 0.002600, mae: 0.050531, mean_q: -0.270968
 51883/100000: episode: 739, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -16.737, mean reward: -0.167 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.883, 10.124], loss: 0.002580, mae: 0.051151, mean_q: -0.323471
 51983/100000: episode: 740, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -12.717, mean reward: -0.127 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.499, 10.328], loss: 0.006196, mae: 0.071790, mean_q: -0.360606
 52083/100000: episode: 741, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -13.759, mean reward: -0.138 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.885, 10.098], loss: 0.002943, mae: 0.054888, mean_q: -0.303334
 52183/100000: episode: 742, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.568, mean reward: -0.176 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.050, 10.341], loss: 0.002639, mae: 0.051146, mean_q: -0.300739
 52283/100000: episode: 743, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.075, mean reward: -0.191 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.384, 10.135], loss: 0.002535, mae: 0.050178, mean_q: -0.288746
 52383/100000: episode: 744, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.114, mean reward: -0.191 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.567, 10.103], loss: 0.002499, mae: 0.050073, mean_q: -0.305739
 52483/100000: episode: 745, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -10.319, mean reward: -0.103 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.493, 10.516], loss: 0.002495, mae: 0.049672, mean_q: -0.332373
 52583/100000: episode: 746, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.449, mean reward: -0.194 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.835, 10.252], loss: 0.002390, mae: 0.048940, mean_q: -0.277806
 52683/100000: episode: 747, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.046, mean reward: -0.160 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.668, 10.098], loss: 0.002515, mae: 0.049525, mean_q: -0.304919
 52783/100000: episode: 748, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.632, mean reward: -0.206 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.117, 10.127], loss: 0.002463, mae: 0.049421, mean_q: -0.311709
 52883/100000: episode: 749, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.408, mean reward: -0.174 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.506, 10.291], loss: 0.002461, mae: 0.049042, mean_q: -0.345542
 52983/100000: episode: 750, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.557, mean reward: -0.176 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.759, 10.098], loss: 0.002543, mae: 0.049506, mean_q: -0.324189
 53083/100000: episode: 751, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.152, mean reward: -0.152 [-1.000, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.908, 10.098], loss: 0.002518, mae: 0.049047, mean_q: -0.329054
 53183/100000: episode: 752, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -3.206, mean reward: -0.032 [-1.000, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.554, 10.098], loss: 0.002514, mae: 0.049196, mean_q: -0.295424
 53283/100000: episode: 753, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.482, mean reward: -0.175 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.722, 10.122], loss: 0.002507, mae: 0.050203, mean_q: -0.299870
 53383/100000: episode: 754, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.547, mean reward: -0.175 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.404, 10.122], loss: 0.002466, mae: 0.048750, mean_q: -0.306592
 53483/100000: episode: 755, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -17.748, mean reward: -0.177 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.537, 10.265], loss: 0.002318, mae: 0.046953, mean_q: -0.328221
 53583/100000: episode: 756, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.117, mean reward: -0.181 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.052, 10.098], loss: 0.002585, mae: 0.049890, mean_q: -0.281010
 53683/100000: episode: 757, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -21.370, mean reward: -0.214 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.323, 10.098], loss: 0.002576, mae: 0.049670, mean_q: -0.302701
 53783/100000: episode: 758, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.759, mean reward: -0.188 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.971, 10.104], loss: 0.002572, mae: 0.050053, mean_q: -0.337184
 53883/100000: episode: 759, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.399, mean reward: -0.174 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.572, 10.098], loss: 0.002841, mae: 0.053594, mean_q: -0.335381
 53983/100000: episode: 760, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.408, mean reward: -0.154 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.889, 10.256], loss: 0.002630, mae: 0.049546, mean_q: -0.354189
 54083/100000: episode: 761, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -16.649, mean reward: -0.166 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.511, 10.381], loss: 0.002511, mae: 0.048766, mean_q: -0.295822
 54183/100000: episode: 762, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.722, mean reward: -0.187 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.576, 10.098], loss: 0.002462, mae: 0.047617, mean_q: -0.304823
 54283/100000: episode: 763, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.343, mean reward: -0.163 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.575, 10.098], loss: 0.002476, mae: 0.048437, mean_q: -0.353298
 54383/100000: episode: 764, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.250, mean reward: -0.163 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.330, 10.229], loss: 0.002435, mae: 0.048363, mean_q: -0.269880
 54483/100000: episode: 765, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -20.058, mean reward: -0.201 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.000, 10.098], loss: 0.002218, mae: 0.045891, mean_q: -0.321530
[Info] 100-TH LEVEL FOUND: 0.6263673901557922, Considering 10/90 traces
 54583/100000: episode: 766, duration: 4.506s, episode steps: 100, steps per second: 22, episode reward: -18.080, mean reward: -0.181 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.769, 10.155], loss: 0.002426, mae: 0.047172, mean_q: -0.340326
 54587/100000: episode: 767, duration: 0.027s, episode steps: 4, steps per second: 149, episode reward: 1.421, mean reward: 0.355 [0.349, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.312, 10.100], loss: 0.002387, mae: 0.051377, mean_q: -0.324924
 54598/100000: episode: 768, duration: 0.054s, episode steps: 11, steps per second: 206, episode reward: 4.202, mean reward: 0.382 [0.329, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.525], loss: 0.002493, mae: 0.048349, mean_q: -0.426403
 54629/100000: episode: 769, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 7.305, mean reward: 0.236 [0.095, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.229, 10.100], loss: 0.002727, mae: 0.050774, mean_q: -0.319578
 54699/100000: episode: 770, duration: 0.349s, episode steps: 70, steps per second: 201, episode reward: 12.957, mean reward: 0.185 [0.037, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.734 [-0.373, 10.103], loss: 0.002490, mae: 0.050052, mean_q: -0.300456
 54703/100000: episode: 771, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 1.266, mean reward: 0.317 [0.291, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.185, 10.409], loss: 0.002340, mae: 0.049897, mean_q: -0.355526
 54707/100000: episode: 772, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 1.675, mean reward: 0.419 [0.367, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.465, 10.100], loss: 0.003020, mae: 0.057634, mean_q: -0.144055
 54728/100000: episode: 773, duration: 0.105s, episode steps: 21, steps per second: 199, episode reward: 5.192, mean reward: 0.247 [0.159, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.270, 10.100], loss: 0.002450, mae: 0.051244, mean_q: -0.271333
 54751/100000: episode: 774, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 5.017, mean reward: 0.218 [0.146, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.104, 10.314], loss: 0.002054, mae: 0.045736, mean_q: -0.276847
 54755/100000: episode: 775, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 1.548, mean reward: 0.387 [0.321, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.328, 10.100], loss: 0.001745, mae: 0.042213, mean_q: -0.314119
 54766/100000: episode: 776, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 3.584, mean reward: 0.326 [0.225, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.500], loss: 0.002171, mae: 0.045872, mean_q: -0.296282
 54789/100000: episode: 777, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 5.960, mean reward: 0.259 [0.147, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.401], loss: 0.002861, mae: 0.052219, mean_q: -0.290963
 54810/100000: episode: 778, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 5.943, mean reward: 0.283 [0.147, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.346, 10.428], loss: 0.002984, mae: 0.052643, mean_q: -0.270898
 54814/100000: episode: 779, duration: 0.031s, episode steps: 4, steps per second: 128, episode reward: 1.605, mean reward: 0.401 [0.363, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.291, 10.100], loss: 0.002392, mae: 0.046804, mean_q: -0.315387
 54845/100000: episode: 780, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 8.766, mean reward: 0.283 [0.137, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.209, 10.100], loss: 0.002238, mae: 0.047005, mean_q: -0.289150
 54856/100000: episode: 781, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 4.679, mean reward: 0.425 [0.301, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.595], loss: 0.002380, mae: 0.047713, mean_q: -0.222631
 54867/100000: episode: 782, duration: 0.075s, episode steps: 11, steps per second: 146, episode reward: 3.551, mean reward: 0.323 [0.225, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.228, 10.457], loss: 0.002139, mae: 0.043037, mean_q: -0.300585
 54898/100000: episode: 783, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 11.467, mean reward: 0.370 [0.296, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.273, 10.100], loss: 0.002425, mae: 0.047916, mean_q: -0.252845
 54934/100000: episode: 784, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 12.186, mean reward: 0.338 [0.217, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.869, 10.100], loss: 0.002825, mae: 0.052858, mean_q: -0.207350
 54955/100000: episode: 785, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 7.061, mean reward: 0.336 [0.193, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.184, 10.278], loss: 0.002834, mae: 0.051179, mean_q: -0.270253
 54991/100000: episode: 786, duration: 0.199s, episode steps: 36, steps per second: 181, episode reward: 8.555, mean reward: 0.238 [0.030, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.967, 10.127], loss: 0.002768, mae: 0.051165, mean_q: -0.281187
 55012/100000: episode: 787, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 5.846, mean reward: 0.278 [0.146, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.351], loss: 0.003247, mae: 0.055520, mean_q: -0.233655
 55082/100000: episode: 788, duration: 0.367s, episode steps: 70, steps per second: 190, episode reward: 12.815, mean reward: 0.183 [0.018, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.727 [-0.739, 10.100], loss: 0.002698, mae: 0.052769, mean_q: -0.195117
 55086/100000: episode: 789, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 1.477, mean reward: 0.369 [0.320, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.361, 10.100], loss: 0.003814, mae: 0.059671, mean_q: -0.116631
 55090/100000: episode: 790, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 1.324, mean reward: 0.331 [0.300, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.218, 10.458], loss: 0.002336, mae: 0.047364, mean_q: -0.191160
 55101/100000: episode: 791, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 3.094, mean reward: 0.281 [0.208, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.442], loss: 0.002384, mae: 0.049786, mean_q: -0.232852
 55122/100000: episode: 792, duration: 0.109s, episode steps: 21, steps per second: 192, episode reward: 6.567, mean reward: 0.313 [0.231, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.407, 10.100], loss: 0.002272, mae: 0.047865, mean_q: -0.247833
 55133/100000: episode: 793, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 2.961, mean reward: 0.269 [0.232, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.318], loss: 0.002632, mae: 0.050580, mean_q: -0.220377
 55169/100000: episode: 794, duration: 0.170s, episode steps: 36, steps per second: 212, episode reward: 11.101, mean reward: 0.308 [0.194, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.948, 10.100], loss: 0.002568, mae: 0.049026, mean_q: -0.295116
 55180/100000: episode: 795, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 4.252, mean reward: 0.387 [0.282, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.463], loss: 0.002918, mae: 0.052765, mean_q: -0.143193
 55201/100000: episode: 796, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 8.082, mean reward: 0.385 [0.243, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.083, 10.590], loss: 0.002798, mae: 0.051949, mean_q: -0.243437
 55232/100000: episode: 797, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 8.337, mean reward: 0.269 [0.158, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.428, 10.100], loss: 0.002893, mae: 0.054432, mean_q: -0.190589
 55298/100000: episode: 798, duration: 0.316s, episode steps: 66, steps per second: 209, episode reward: 11.590, mean reward: 0.176 [0.015, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.765 [-0.365, 10.100], loss: 0.002890, mae: 0.053500, mean_q: -0.207468
 55334/100000: episode: 799, duration: 0.185s, episode steps: 36, steps per second: 195, episode reward: 7.628, mean reward: 0.212 [0.032, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.220, 10.162], loss: 0.002718, mae: 0.052016, mean_q: -0.150632
 55370/100000: episode: 800, duration: 0.201s, episode steps: 36, steps per second: 179, episode reward: 12.143, mean reward: 0.337 [0.151, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.680, 10.100], loss: 0.002544, mae: 0.049857, mean_q: -0.295974
 55436/100000: episode: 801, duration: 0.358s, episode steps: 66, steps per second: 184, episode reward: 11.252, mean reward: 0.170 [0.015, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.765 [-0.714, 10.206], loss: 0.002669, mae: 0.051047, mean_q: -0.209045
 55440/100000: episode: 802, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 1.427, mean reward: 0.357 [0.344, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.426], loss: 0.003089, mae: 0.059185, mean_q: -0.217303
 55510/100000: episode: 803, duration: 0.383s, episode steps: 70, steps per second: 183, episode reward: 9.924, mean reward: 0.142 [0.013, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.730 [-1.029, 10.100], loss: 0.005303, mae: 0.065923, mean_q: -0.162801
 55533/100000: episode: 804, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 5.236, mean reward: 0.228 [0.110, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.278, 10.226], loss: 0.008764, mae: 0.074209, mean_q: -0.138064
 55569/100000: episode: 805, duration: 0.178s, episode steps: 36, steps per second: 203, episode reward: 6.358, mean reward: 0.177 [0.011, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.030, 10.236], loss: 0.003839, mae: 0.058651, mean_q: -0.186423
 55573/100000: episode: 806, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 1.646, mean reward: 0.412 [0.353, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.035, 10.543], loss: 0.004573, mae: 0.062335, mean_q: -0.060810
 55596/100000: episode: 807, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 6.732, mean reward: 0.293 [0.193, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.324], loss: 0.003186, mae: 0.058190, mean_q: -0.060975
 55607/100000: episode: 808, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 3.014, mean reward: 0.274 [0.189, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.155, 10.344], loss: 0.002597, mae: 0.051282, mean_q: -0.172260
 55611/100000: episode: 809, duration: 0.029s, episode steps: 4, steps per second: 136, episode reward: 1.481, mean reward: 0.370 [0.317, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.498], loss: 0.002500, mae: 0.050971, mean_q: -0.122863
 55634/100000: episode: 810, duration: 0.108s, episode steps: 23, steps per second: 213, episode reward: 6.896, mean reward: 0.300 [0.200, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.588, 10.352], loss: 0.002992, mae: 0.054633, mean_q: -0.077993
 55655/100000: episode: 811, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 7.354, mean reward: 0.350 [0.265, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.379, 10.461], loss: 0.002541, mae: 0.049057, mean_q: -0.137982
 55659/100000: episode: 812, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 1.413, mean reward: 0.353 [0.318, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.242, 10.100], loss: 0.002983, mae: 0.052911, mean_q: -0.266742
 55725/100000: episode: 813, duration: 0.363s, episode steps: 66, steps per second: 182, episode reward: 16.228, mean reward: 0.246 [0.100, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.762 [-1.442, 10.249], loss: 0.002619, mae: 0.051700, mean_q: -0.140482
 55756/100000: episode: 814, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 7.649, mean reward: 0.247 [0.133, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.196, 10.100], loss: 0.002727, mae: 0.052526, mean_q: -0.137740
 55792/100000: episode: 815, duration: 0.190s, episode steps: 36, steps per second: 190, episode reward: 11.232, mean reward: 0.312 [0.103, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.226, 10.100], loss: 0.002354, mae: 0.049017, mean_q: -0.105621
 55796/100000: episode: 816, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 1.478, mean reward: 0.369 [0.313, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.222, 10.100], loss: 0.003853, mae: 0.064263, mean_q: -0.041439
 55819/100000: episode: 817, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 6.411, mean reward: 0.279 [0.131, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.579, 10.240], loss: 0.002649, mae: 0.050310, mean_q: -0.145724
 55889/100000: episode: 818, duration: 0.378s, episode steps: 70, steps per second: 185, episode reward: 10.499, mean reward: 0.150 [0.034, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.737 [-0.250, 10.139], loss: 0.002657, mae: 0.050556, mean_q: -0.129804
 55893/100000: episode: 819, duration: 0.030s, episode steps: 4, steps per second: 131, episode reward: 1.436, mean reward: 0.359 [0.322, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.267, 10.100], loss: 0.002875, mae: 0.054761, mean_q: -0.187054
 55916/100000: episode: 820, duration: 0.131s, episode steps: 23, steps per second: 175, episode reward: 5.780, mean reward: 0.251 [0.188, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.234, 10.378], loss: 0.002645, mae: 0.051958, mean_q: -0.153383
 55920/100000: episode: 821, duration: 0.028s, episode steps: 4, steps per second: 142, episode reward: 1.239, mean reward: 0.310 [0.240, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.377, 10.100], loss: 0.003195, mae: 0.059529, mean_q: -0.140496
 55924/100000: episode: 822, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 1.411, mean reward: 0.353 [0.308, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.356, 10.100], loss: 0.002809, mae: 0.053845, mean_q: -0.057137
 55928/100000: episode: 823, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 1.286, mean reward: 0.322 [0.311, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.390], loss: 0.001812, mae: 0.043872, mean_q: -0.060408
 55932/100000: episode: 824, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 1.599, mean reward: 0.400 [0.365, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.530], loss: 0.002169, mae: 0.045641, mean_q: -0.135242
 55998/100000: episode: 825, duration: 0.364s, episode steps: 66, steps per second: 181, episode reward: 9.930, mean reward: 0.150 [0.017, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.778 [-1.698, 10.194], loss: 0.002869, mae: 0.054304, mean_q: -0.067244
 56034/100000: episode: 826, duration: 0.186s, episode steps: 36, steps per second: 194, episode reward: 7.744, mean reward: 0.215 [0.012, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.242, 10.100], loss: 0.002653, mae: 0.053081, mean_q: -0.077458
 56038/100000: episode: 827, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 1.352, mean reward: 0.338 [0.279, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.368], loss: 0.003137, mae: 0.051671, mean_q: -0.117738
 56061/100000: episode: 828, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 7.082, mean reward: 0.308 [0.224, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.349], loss: 0.002461, mae: 0.050795, mean_q: -0.042531
 56127/100000: episode: 829, duration: 0.339s, episode steps: 66, steps per second: 195, episode reward: 15.025, mean reward: 0.228 [0.053, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.761 [-0.540, 10.197], loss: 0.002887, mae: 0.053594, mean_q: -0.100761
 56131/100000: episode: 830, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 1.239, mean reward: 0.310 [0.256, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.241, 10.100], loss: 0.002803, mae: 0.053537, mean_q: 0.030097
 56201/100000: episode: 831, duration: 0.384s, episode steps: 70, steps per second: 182, episode reward: 21.054, mean reward: 0.301 [0.148, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.718 [-0.703, 10.265], loss: 0.002711, mae: 0.052763, mean_q: -0.065934
 56267/100000: episode: 832, duration: 0.361s, episode steps: 66, steps per second: 183, episode reward: 15.455, mean reward: 0.234 [0.032, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.766 [-1.381, 10.161], loss: 0.003013, mae: 0.054857, mean_q: -0.039963
 56337/100000: episode: 833, duration: 0.363s, episode steps: 70, steps per second: 193, episode reward: 20.075, mean reward: 0.287 [0.119, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.723 [-0.591, 10.310], loss: 0.002720, mae: 0.053487, mean_q: -0.010817
 56348/100000: episode: 834, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 2.779, mean reward: 0.253 [0.167, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.524, 10.323], loss: 0.003128, mae: 0.055789, mean_q: -0.106311
 56371/100000: episode: 835, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 6.046, mean reward: 0.263 [0.203, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.488, 10.341], loss: 0.002369, mae: 0.050142, mean_q: -0.037680
 56382/100000: episode: 836, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 3.192, mean reward: 0.290 [0.207, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.312], loss: 0.003304, mae: 0.060022, mean_q: -0.028734
 56386/100000: episode: 837, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 1.465, mean reward: 0.366 [0.339, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.479], loss: 0.001873, mae: 0.044709, mean_q: 0.011059
 56397/100000: episode: 838, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 3.349, mean reward: 0.304 [0.221, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.370], loss: 0.002673, mae: 0.054666, mean_q: -0.002367
 56401/100000: episode: 839, duration: 0.025s, episode steps: 4, steps per second: 157, episode reward: 1.374, mean reward: 0.343 [0.330, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.206, 10.100], loss: 0.002831, mae: 0.053546, mean_q: 0.094615
 56432/100000: episode: 840, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 10.604, mean reward: 0.342 [0.217, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.690, 10.100], loss: 0.002848, mae: 0.054149, mean_q: -0.021093
 56453/100000: episode: 841, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 7.397, mean reward: 0.352 [0.230, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.507], loss: 0.003125, mae: 0.056601, mean_q: -0.036796
 56457/100000: episode: 842, duration: 0.035s, episode steps: 4, steps per second: 115, episode reward: 1.512, mean reward: 0.378 [0.354, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.290, 10.100], loss: 0.003305, mae: 0.057546, mean_q: 0.027712
 56478/100000: episode: 843, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 6.899, mean reward: 0.329 [0.160, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.223, 10.347], loss: 0.002877, mae: 0.054124, mean_q: 0.030210
 56509/100000: episode: 844, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 10.614, mean reward: 0.342 [0.258, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.500, 10.100], loss: 0.003410, mae: 0.060521, mean_q: -0.000017
 56513/100000: episode: 845, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 1.593, mean reward: 0.398 [0.361, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.035, 10.512], loss: 0.002818, mae: 0.055793, mean_q: 0.066919
 56536/100000: episode: 846, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 8.875, mean reward: 0.386 [0.274, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.672, 10.518], loss: 0.002750, mae: 0.054546, mean_q: 0.052210
 56557/100000: episode: 847, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 7.684, mean reward: 0.366 [0.239, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.418, 10.603], loss: 0.002901, mae: 0.053339, mean_q: -0.012013
 56627/100000: episode: 848, duration: 0.403s, episode steps: 70, steps per second: 174, episode reward: 20.975, mean reward: 0.300 [0.170, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.728 [-1.741, 10.426], loss: 0.002813, mae: 0.054848, mean_q: 0.012823
 56638/100000: episode: 849, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 4.154, mean reward: 0.378 [0.310, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.551], loss: 0.002510, mae: 0.051801, mean_q: -0.003037
 56659/100000: episode: 850, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 7.327, mean reward: 0.349 [0.232, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.826, 10.100], loss: 0.002589, mae: 0.052223, mean_q: 0.064153
 56670/100000: episode: 851, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 2.575, mean reward: 0.234 [0.173, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.344], loss: 0.003039, mae: 0.056198, mean_q: 0.009590
 56691/100000: episode: 852, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 6.168, mean reward: 0.294 [0.162, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.761, 10.339], loss: 0.002866, mae: 0.054778, mean_q: 0.023065
 56695/100000: episode: 853, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 1.461, mean reward: 0.365 [0.304, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.459], loss: 0.002782, mae: 0.056904, mean_q: 0.113138
 56726/100000: episode: 854, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 9.361, mean reward: 0.302 [0.151, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.219, 10.100], loss: 0.002819, mae: 0.054606, mean_q: 0.010653
 56730/100000: episode: 855, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 1.318, mean reward: 0.330 [0.313, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.407], loss: 0.002651, mae: 0.051654, mean_q: -0.018015
[Info] 200-TH LEVEL FOUND: 0.7720127105712891, Considering 10/90 traces
 56766/100000: episode: 856, duration: 4.074s, episode steps: 36, steps per second: 9, episode reward: 12.885, mean reward: 0.358 [0.184, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.395, 10.100], loss: 0.002695, mae: 0.053939, mean_q: 0.055166
 56793/100000: episode: 857, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 8.929, mean reward: 0.331 [0.120, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.059, 10.100], loss: 0.002779, mae: 0.055146, mean_q: 0.060960
 56824/100000: episode: 858, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 11.323, mean reward: 0.365 [0.258, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.505, 10.100], loss: 0.002922, mae: 0.055094, mean_q: 0.075613
 56840/100000: episode: 859, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 8.183, mean reward: 0.511 [0.423, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.353, 10.100], loss: 0.003015, mae: 0.057183, mean_q: 0.048710
 56866/100000: episode: 860, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 7.373, mean reward: 0.284 [0.096, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.435, 10.100], loss: 0.002973, mae: 0.056166, mean_q: 0.044874
 56879/100000: episode: 861, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 4.911, mean reward: 0.378 [0.323, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.260, 10.100], loss: 0.002881, mae: 0.056093, mean_q: 0.099492
 56906/100000: episode: 862, duration: 0.126s, episode steps: 27, steps per second: 214, episode reward: 6.763, mean reward: 0.250 [0.031, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.293, 10.100], loss: 0.003114, mae: 0.058954, mean_q: 0.111481
 56922/100000: episode: 863, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 5.023, mean reward: 0.314 [0.276, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.527, 10.100], loss: 0.002623, mae: 0.052041, mean_q: 0.052630
 56934/100000: episode: 864, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 4.590, mean reward: 0.382 [0.326, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.468, 10.100], loss: 0.002999, mae: 0.057144, mean_q: 0.084370
 56965/100000: episode: 865, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 11.483, mean reward: 0.370 [0.244, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.519, 10.100], loss: 0.002973, mae: 0.057290, mean_q: 0.084813
 56995/100000: episode: 866, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 6.939, mean reward: 0.231 [0.049, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.555, 10.100], loss: 0.003068, mae: 0.056956, mean_q: 0.055362
 57008/100000: episode: 867, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 5.109, mean reward: 0.393 [0.365, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.385, 10.100], loss: 0.003264, mae: 0.064200, mean_q: 0.080256
 57035/100000: episode: 868, duration: 0.140s, episode steps: 27, steps per second: 192, episode reward: 9.749, mean reward: 0.361 [0.183, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.630, 10.100], loss: 0.003054, mae: 0.059239, mean_q: 0.034424
 57051/100000: episode: 869, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 6.807, mean reward: 0.425 [0.339, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.326, 10.100], loss: 0.002606, mae: 0.056957, mean_q: 0.107166
 57078/100000: episode: 870, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 8.474, mean reward: 0.314 [0.213, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.262, 10.100], loss: 0.002778, mae: 0.055959, mean_q: 0.100168
 57094/100000: episode: 871, duration: 0.110s, episode steps: 16, steps per second: 146, episode reward: 6.002, mean reward: 0.375 [0.302, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.309, 10.100], loss: 0.002733, mae: 0.055034, mean_q: 0.120203
 57121/100000: episode: 872, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 7.476, mean reward: 0.277 [0.125, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.398, 10.100], loss: 0.002705, mae: 0.053027, mean_q: 0.064138
 57133/100000: episode: 873, duration: 0.083s, episode steps: 12, steps per second: 145, episode reward: 4.840, mean reward: 0.403 [0.282, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-1.042, 10.100], loss: 0.002861, mae: 0.057801, mean_q: 0.159807
 57149/100000: episode: 874, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 6.809, mean reward: 0.426 [0.327, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.439, 10.100], loss: 0.003099, mae: 0.056528, mean_q: 0.098173
 57176/100000: episode: 875, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 7.685, mean reward: 0.285 [0.177, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.199, 10.100], loss: 0.003191, mae: 0.058612, mean_q: 0.162161
 57189/100000: episode: 876, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 5.247, mean reward: 0.404 [0.357, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.387, 10.100], loss: 0.002945, mae: 0.058264, mean_q: 0.149583
 57202/100000: episode: 877, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 4.714, mean reward: 0.363 [0.219, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.367, 10.100], loss: 0.003108, mae: 0.055734, mean_q: 0.151281
 57232/100000: episode: 878, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 10.854, mean reward: 0.362 [0.252, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.919, 10.100], loss: 0.002943, mae: 0.057448, mean_q: 0.185894
 57258/100000: episode: 879, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 7.258, mean reward: 0.279 [0.127, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.424, 10.100], loss: 0.002882, mae: 0.054715, mean_q: 0.148106
 57289/100000: episode: 880, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 6.696, mean reward: 0.216 [0.059, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.470, 10.118], loss: 0.002762, mae: 0.054184, mean_q: 0.135358
 57305/100000: episode: 881, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 7.183, mean reward: 0.449 [0.379, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.486, 10.100], loss: 0.002619, mae: 0.053083, mean_q: 0.111921
 57318/100000: episode: 882, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 4.883, mean reward: 0.376 [0.315, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.346, 10.100], loss: 0.003015, mae: 0.058337, mean_q: 0.155210
 57334/100000: episode: 883, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 6.921, mean reward: 0.433 [0.367, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.822, 10.100], loss: 0.002882, mae: 0.055909, mean_q: 0.166344
 57365/100000: episode: 884, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 7.867, mean reward: 0.254 [0.058, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.035, 10.100], loss: 0.003211, mae: 0.060379, mean_q: 0.193280
 57378/100000: episode: 885, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 4.866, mean reward: 0.374 [0.270, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.296, 10.100], loss: 0.003246, mae: 0.059720, mean_q: 0.238344
 57409/100000: episode: 886, duration: 0.160s, episode steps: 31, steps per second: 193, episode reward: 7.096, mean reward: 0.229 [0.054, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-1.015, 10.100], loss: 0.002743, mae: 0.056594, mean_q: 0.172998
 57440/100000: episode: 887, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 7.489, mean reward: 0.242 [0.128, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.547, 10.100], loss: 0.003461, mae: 0.061666, mean_q: 0.193197
 57471/100000: episode: 888, duration: 0.172s, episode steps: 31, steps per second: 181, episode reward: 8.395, mean reward: 0.271 [0.199, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.154, 10.100], loss: 0.003237, mae: 0.059757, mean_q: 0.213513
 57487/100000: episode: 889, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 6.717, mean reward: 0.420 [0.271, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.402, 10.100], loss: 0.003356, mae: 0.061299, mean_q: 0.205961
 57500/100000: episode: 890, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 6.470, mean reward: 0.498 [0.355, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.428, 10.100], loss: 0.002508, mae: 0.052008, mean_q: 0.212909
 57526/100000: episode: 891, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 9.897, mean reward: 0.381 [0.298, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.674, 10.100], loss: 0.002838, mae: 0.056724, mean_q: 0.176176
 57557/100000: episode: 892, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 16.038, mean reward: 0.517 [0.402, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.585, 10.100], loss: 0.002437, mae: 0.052418, mean_q: 0.210765
 57583/100000: episode: 893, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 11.022, mean reward: 0.424 [0.351, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.246, 10.100], loss: 0.003122, mae: 0.059290, mean_q: 0.254351
 57599/100000: episode: 894, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 6.987, mean reward: 0.437 [0.329, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.340, 10.100], loss: 0.003051, mae: 0.058044, mean_q: 0.175110
 57630/100000: episode: 895, duration: 0.181s, episode steps: 31, steps per second: 172, episode reward: 9.758, mean reward: 0.315 [0.199, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.287, 10.100], loss: 0.003052, mae: 0.058071, mean_q: 0.231974
 57646/100000: episode: 896, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 6.074, mean reward: 0.380 [0.248, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.287, 10.100], loss: 0.003021, mae: 0.058586, mean_q: 0.270419
 57659/100000: episode: 897, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 4.523, mean reward: 0.348 [0.281, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.400, 10.100], loss: 0.003429, mae: 0.063038, mean_q: 0.272896
 57671/100000: episode: 898, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 4.827, mean reward: 0.402 [0.302, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.368, 10.100], loss: 0.003176, mae: 0.058765, mean_q: 0.236363
 57683/100000: episode: 899, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 4.287, mean reward: 0.357 [0.291, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.315, 10.100], loss: 0.002804, mae: 0.056317, mean_q: 0.292882
 57695/100000: episode: 900, duration: 0.089s, episode steps: 12, steps per second: 135, episode reward: 4.432, mean reward: 0.369 [0.270, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.394, 10.100], loss: 0.003261, mae: 0.062539, mean_q: 0.327235
 57708/100000: episode: 901, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 4.308, mean reward: 0.331 [0.261, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.403, 10.100], loss: 0.003180, mae: 0.059419, mean_q: 0.279317
 57739/100000: episode: 902, duration: 0.169s, episode steps: 31, steps per second: 184, episode reward: 10.520, mean reward: 0.339 [0.162, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.002, 10.100], loss: 0.003326, mae: 0.061432, mean_q: 0.233168
 57770/100000: episode: 903, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 9.320, mean reward: 0.301 [0.189, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.196, 10.100], loss: 0.002814, mae: 0.057054, mean_q: 0.247860
 57783/100000: episode: 904, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 5.476, mean reward: 0.421 [0.274, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.733, 10.100], loss: 0.002626, mae: 0.053721, mean_q: 0.259604
 57809/100000: episode: 905, duration: 0.156s, episode steps: 26, steps per second: 167, episode reward: 8.299, mean reward: 0.319 [0.202, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.387, 10.100], loss: 0.002651, mae: 0.055410, mean_q: 0.243905
 57835/100000: episode: 906, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 11.084, mean reward: 0.426 [0.333, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-1.335, 10.100], loss: 0.003416, mae: 0.061394, mean_q: 0.291745
 57851/100000: episode: 907, duration: 0.099s, episode steps: 16, steps per second: 162, episode reward: 5.775, mean reward: 0.361 [0.271, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.453, 10.100], loss: 0.003288, mae: 0.061148, mean_q: 0.275058
 57864/100000: episode: 908, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 4.254, mean reward: 0.327 [0.262, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.962, 10.100], loss: 0.003574, mae: 0.062420, mean_q: 0.244884
 57895/100000: episode: 909, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 6.835, mean reward: 0.220 [0.071, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.123, 10.100], loss: 0.003077, mae: 0.058786, mean_q: 0.268640
 57925/100000: episode: 910, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 7.008, mean reward: 0.234 [0.065, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-1.332, 10.207], loss: 0.003228, mae: 0.060779, mean_q: 0.272428
 57952/100000: episode: 911, duration: 0.137s, episode steps: 27, steps per second: 198, episode reward: 9.723, mean reward: 0.360 [0.243, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.419, 10.100], loss: 0.003073, mae: 0.058973, mean_q: 0.296428
 57968/100000: episode: 912, duration: 0.083s, episode steps: 16, steps per second: 194, episode reward: 5.681, mean reward: 0.355 [0.215, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.083, 10.100], loss: 0.002924, mae: 0.058062, mean_q: 0.306288
 57998/100000: episode: 913, duration: 0.146s, episode steps: 30, steps per second: 205, episode reward: 9.049, mean reward: 0.302 [0.170, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.930, 10.100], loss: 0.003027, mae: 0.057820, mean_q: 0.289337
 58014/100000: episode: 914, duration: 0.100s, episode steps: 16, steps per second: 159, episode reward: 5.688, mean reward: 0.355 [0.249, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.532, 10.100], loss: 0.003003, mae: 0.059294, mean_q: 0.254254
 58027/100000: episode: 915, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 4.962, mean reward: 0.382 [0.305, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.399, 10.100], loss: 0.003154, mae: 0.063067, mean_q: 0.331268
 58057/100000: episode: 916, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 12.777, mean reward: 0.426 [0.303, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.534, 10.100], loss: 0.003043, mae: 0.059115, mean_q: 0.294637
 58088/100000: episode: 917, duration: 0.185s, episode steps: 31, steps per second: 167, episode reward: 7.238, mean reward: 0.233 [0.056, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-1.004, 10.100], loss: 0.003240, mae: 0.060377, mean_q: 0.300921
 58101/100000: episode: 918, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 5.963, mean reward: 0.459 [0.364, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.458, 10.100], loss: 0.002985, mae: 0.058412, mean_q: 0.296448
 58113/100000: episode: 919, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 3.910, mean reward: 0.326 [0.282, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.386, 10.100], loss: 0.002758, mae: 0.056139, mean_q: 0.311139
 58143/100000: episode: 920, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 8.508, mean reward: 0.284 [0.143, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.842, 10.100], loss: 0.002717, mae: 0.056381, mean_q: 0.306455
 58173/100000: episode: 921, duration: 0.149s, episode steps: 30, steps per second: 201, episode reward: 7.816, mean reward: 0.261 [0.066, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.056, 10.100], loss: 0.002886, mae: 0.056886, mean_q: 0.353335
 58204/100000: episode: 922, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 5.781, mean reward: 0.186 [0.098, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.773, 10.100], loss: 0.003171, mae: 0.061187, mean_q: 0.357032
 58217/100000: episode: 923, duration: 0.086s, episode steps: 13, steps per second: 152, episode reward: 5.075, mean reward: 0.390 [0.307, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.356, 10.100], loss: 0.003188, mae: 0.060185, mean_q: 0.390930
 58230/100000: episode: 924, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 3.483, mean reward: 0.268 [0.150, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.294, 10.100], loss: 0.002942, mae: 0.057815, mean_q: 0.326382
 58261/100000: episode: 925, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 9.286, mean reward: 0.300 [0.112, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.589, 10.100], loss: 0.003648, mae: 0.064851, mean_q: 0.312801
 58277/100000: episode: 926, duration: 0.080s, episode steps: 16, steps per second: 201, episode reward: 7.589, mean reward: 0.474 [0.406, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.428, 10.100], loss: 0.002728, mae: 0.056990, mean_q: 0.328538
 58308/100000: episode: 927, duration: 0.156s, episode steps: 31, steps per second: 198, episode reward: 9.425, mean reward: 0.304 [0.227, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.270, 10.100], loss: 0.002834, mae: 0.056549, mean_q: 0.330154
 58339/100000: episode: 928, duration: 0.163s, episode steps: 31, steps per second: 191, episode reward: 7.888, mean reward: 0.254 [0.102, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.846, 10.100], loss: 0.002938, mae: 0.058176, mean_q: 0.357028
 58352/100000: episode: 929, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 4.769, mean reward: 0.367 [0.280, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-1.139, 10.100], loss: 0.003024, mae: 0.058862, mean_q: 0.390978
 58382/100000: episode: 930, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 7.776, mean reward: 0.259 [0.098, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.592, 10.100], loss: 0.003050, mae: 0.059725, mean_q: 0.363146
 58412/100000: episode: 931, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 9.531, mean reward: 0.318 [0.188, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.354, 10.100], loss: 0.003547, mae: 0.064441, mean_q: 0.369437
 58428/100000: episode: 932, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 7.010, mean reward: 0.438 [0.332, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.983, 10.100], loss: 0.005467, mae: 0.071640, mean_q: 0.366547
 58440/100000: episode: 933, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 3.832, mean reward: 0.319 [0.218, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.391, 10.100], loss: 0.003530, mae: 0.065164, mean_q: 0.413228
 58453/100000: episode: 934, duration: 0.069s, episode steps: 13, steps per second: 190, episode reward: 4.845, mean reward: 0.373 [0.318, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.188, 10.100], loss: 0.003203, mae: 0.064606, mean_q: 0.406342
 58469/100000: episode: 935, duration: 0.083s, episode steps: 16, steps per second: 194, episode reward: 7.528, mean reward: 0.471 [0.381, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.843, 10.100], loss: 0.003035, mae: 0.061586, mean_q: 0.378864
 58496/100000: episode: 936, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 11.709, mean reward: 0.434 [0.282, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.517, 10.100], loss: 0.002773, mae: 0.057889, mean_q: 0.409018
 58512/100000: episode: 937, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 5.718, mean reward: 0.357 [0.263, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.531, 10.100], loss: 0.002995, mae: 0.061090, mean_q: 0.394497
 58539/100000: episode: 938, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 10.278, mean reward: 0.381 [0.275, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.928, 10.100], loss: 0.002795, mae: 0.057305, mean_q: 0.439483
 58552/100000: episode: 939, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 6.238, mean reward: 0.480 [0.401, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.341, 10.100], loss: 0.003100, mae: 0.060429, mean_q: 0.412991
 58565/100000: episode: 940, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 6.968, mean reward: 0.536 [0.366, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.425, 10.100], loss: 0.002490, mae: 0.054976, mean_q: 0.425595
 58591/100000: episode: 941, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 11.203, mean reward: 0.431 [0.335, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.704, 10.100], loss: 0.003149, mae: 0.059782, mean_q: 0.435225
 58622/100000: episode: 942, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 10.116, mean reward: 0.326 [0.089, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.269, 10.100], loss: 0.002832, mae: 0.057530, mean_q: 0.406701
 58649/100000: episode: 943, duration: 0.130s, episode steps: 27, steps per second: 208, episode reward: 9.805, mean reward: 0.363 [0.098, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.465, 10.100], loss: 0.003257, mae: 0.061122, mean_q: 0.438192
 58679/100000: episode: 944, duration: 0.154s, episode steps: 30, steps per second: 194, episode reward: 7.616, mean reward: 0.254 [0.010, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.413, 10.122], loss: 0.004521, mae: 0.063949, mean_q: 0.434548
 58695/100000: episode: 945, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 4.013, mean reward: 0.251 [0.126, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.327, 10.100], loss: 0.003445, mae: 0.065041, mean_q: 0.390280
[Info] 300-TH LEVEL FOUND: 0.8662660717964172, Considering 10/90 traces
 58707/100000: episode: 946, duration: 3.937s, episode steps: 12, steps per second: 3, episode reward: 3.731, mean reward: 0.311 [0.194, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.360, 10.100], loss: 0.003260, mae: 0.063359, mean_q: 0.453287
 58722/100000: episode: 947, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 6.624, mean reward: 0.442 [0.340, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.330, 10.100], loss: 0.002715, mae: 0.058249, mean_q: 0.485970
 58737/100000: episode: 948, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 7.140, mean reward: 0.476 [0.388, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.384, 10.100], loss: 0.003192, mae: 0.061894, mean_q: 0.435503
 58746/100000: episode: 949, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 3.771, mean reward: 0.419 [0.383, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.478, 10.100], loss: 0.003019, mae: 0.061820, mean_q: 0.423687
 58755/100000: episode: 950, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 4.114, mean reward: 0.457 [0.342, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.248, 10.100], loss: 0.002885, mae: 0.059929, mean_q: 0.471824
 58765/100000: episode: 951, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 3.685, mean reward: 0.368 [0.301, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.372, 10.100], loss: 0.003117, mae: 0.060692, mean_q: 0.435649
 58792/100000: episode: 952, duration: 0.153s, episode steps: 27, steps per second: 177, episode reward: 9.024, mean reward: 0.334 [0.212, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.110, 10.100], loss: 0.002995, mae: 0.059499, mean_q: 0.465363
 58819/100000: episode: 953, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 9.934, mean reward: 0.368 [0.221, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.358, 10.100], loss: 0.002812, mae: 0.057191, mean_q: 0.465116
 58846/100000: episode: 954, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 12.411, mean reward: 0.460 [0.323, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.327, 10.100], loss: 0.003054, mae: 0.060704, mean_q: 0.497838
 58855/100000: episode: 955, duration: 0.054s, episode steps: 9, steps per second: 165, episode reward: 3.965, mean reward: 0.441 [0.313, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.356, 10.100], loss: 0.002345, mae: 0.052479, mean_q: 0.463188
 58864/100000: episode: 956, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 3.734, mean reward: 0.415 [0.354, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.294, 10.100], loss: 0.003359, mae: 0.061648, mean_q: 0.511700
 58874/100000: episode: 957, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 4.764, mean reward: 0.476 [0.417, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.319, 10.100], loss: 0.003848, mae: 0.071082, mean_q: 0.452562
 58883/100000: episode: 958, duration: 0.062s, episode steps: 9, steps per second: 146, episode reward: 4.307, mean reward: 0.479 [0.427, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.492, 10.100], loss: 0.005170, mae: 0.063562, mean_q: 0.444947
 58898/100000: episode: 959, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 7.440, mean reward: 0.496 [0.447, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.418, 10.100], loss: 0.003425, mae: 0.062813, mean_q: 0.452504
 58907/100000: episode: 960, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 3.962, mean reward: 0.440 [0.392, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.432, 10.100], loss: 0.003375, mae: 0.062122, mean_q: 0.474304
 58922/100000: episode: 961, duration: 0.072s, episode steps: 15, steps per second: 207, episode reward: 6.989, mean reward: 0.466 [0.389, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.788, 10.100], loss: 0.003039, mae: 0.061747, mean_q: 0.479321
 58931/100000: episode: 962, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 4.932, mean reward: 0.548 [0.454, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.430, 10.100], loss: 0.002954, mae: 0.060122, mean_q: 0.488062
 58946/100000: episode: 963, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 6.225, mean reward: 0.415 [0.360, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.289, 10.100], loss: 0.002940, mae: 0.058871, mean_q: 0.509004
 58973/100000: episode: 964, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 11.569, mean reward: 0.428 [0.339, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.372, 10.100], loss: 0.002915, mae: 0.057887, mean_q: 0.490971
 58982/100000: episode: 965, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 4.885, mean reward: 0.543 [0.456, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.464, 10.100], loss: 0.003126, mae: 0.062166, mean_q: 0.479570
 58992/100000: episode: 966, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 4.144, mean reward: 0.414 [0.347, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.272, 10.100], loss: 0.003115, mae: 0.062403, mean_q: 0.532401
 59001/100000: episode: 967, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 4.044, mean reward: 0.449 [0.404, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.450, 10.100], loss: 0.003271, mae: 0.061709, mean_q: 0.534713
 59016/100000: episode: 968, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 6.512, mean reward: 0.434 [0.399, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.622, 10.100], loss: 0.002859, mae: 0.059221, mean_q: 0.497279
 59025/100000: episode: 969, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 4.321, mean reward: 0.480 [0.421, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.477, 10.100], loss: 0.002813, mae: 0.057408, mean_q: 0.550364
 59036/100000: episode: 970, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 5.359, mean reward: 0.487 [0.435, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.305, 10.100], loss: 0.003253, mae: 0.061325, mean_q: 0.515233
 59045/100000: episode: 971, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 4.351, mean reward: 0.483 [0.435, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.395, 10.100], loss: 0.002935, mae: 0.058217, mean_q: 0.504895
 59054/100000: episode: 972, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 4.089, mean reward: 0.454 [0.407, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.395, 10.100], loss: 0.002711, mae: 0.055444, mean_q: 0.563439
 59069/100000: episode: 973, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 5.866, mean reward: 0.391 [0.246, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.283, 10.100], loss: 0.003121, mae: 0.061373, mean_q: 0.516276
 59079/100000: episode: 974, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 4.937, mean reward: 0.494 [0.437, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.512, 10.100], loss: 0.004156, mae: 0.069914, mean_q: 0.504083
 59094/100000: episode: 975, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 6.909, mean reward: 0.461 [0.406, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.476, 10.100], loss: 0.003619, mae: 0.065786, mean_q: 0.554805
 59121/100000: episode: 976, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 11.086, mean reward: 0.411 [0.265, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.277, 10.100], loss: 0.002908, mae: 0.059524, mean_q: 0.533231
 59132/100000: episode: 977, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 5.248, mean reward: 0.477 [0.436, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.546, 10.100], loss: 0.003648, mae: 0.066842, mean_q: 0.543833
 59143/100000: episode: 978, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 5.569, mean reward: 0.506 [0.473, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.362, 10.100], loss: 0.002938, mae: 0.060339, mean_q: 0.553403
 59158/100000: episode: 979, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 5.125, mean reward: 0.342 [0.267, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.262, 10.100], loss: 0.002836, mae: 0.058214, mean_q: 0.582426
 59173/100000: episode: 980, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 5.814, mean reward: 0.388 [0.353, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.401, 10.100], loss: 0.002885, mae: 0.058842, mean_q: 0.582192
 59182/100000: episode: 981, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 4.298, mean reward: 0.478 [0.424, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.444, 10.100], loss: 0.002922, mae: 0.058685, mean_q: 0.569549
 59191/100000: episode: 982, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 3.434, mean reward: 0.382 [0.297, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.297, 10.100], loss: 0.002826, mae: 0.059944, mean_q: 0.573416
 59206/100000: episode: 983, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 7.530, mean reward: 0.502 [0.428, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.525, 10.100], loss: 0.003230, mae: 0.063291, mean_q: 0.569022
 59221/100000: episode: 984, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 6.607, mean reward: 0.440 [0.368, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.354, 10.100], loss: 0.003487, mae: 0.064498, mean_q: 0.574289
 59230/100000: episode: 985, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 3.685, mean reward: 0.409 [0.325, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.320, 10.100], loss: 0.003088, mae: 0.061209, mean_q: 0.572410
 59239/100000: episode: 986, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 4.352, mean reward: 0.484 [0.403, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.512, 10.100], loss: 0.002886, mae: 0.058231, mean_q: 0.563762
 59248/100000: episode: 987, duration: 0.054s, episode steps: 9, steps per second: 165, episode reward: 4.131, mean reward: 0.459 [0.426, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.459, 10.100], loss: 0.002817, mae: 0.058988, mean_q: 0.583472
 59257/100000: episode: 988, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 4.391, mean reward: 0.488 [0.440, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.479, 10.100], loss: 0.003051, mae: 0.062993, mean_q: 0.562319
 59267/100000: episode: 989, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 4.389, mean reward: 0.439 [0.393, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.466, 10.100], loss: 0.003924, mae: 0.068581, mean_q: 0.563008
 59282/100000: episode: 990, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 6.951, mean reward: 0.463 [0.328, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.404, 10.100], loss: 0.003326, mae: 0.064060, mean_q: 0.581721
 59297/100000: episode: 991, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 6.008, mean reward: 0.401 [0.361, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.302, 10.100], loss: 0.002664, mae: 0.056959, mean_q: 0.604643
 59306/100000: episode: 992, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 4.254, mean reward: 0.473 [0.316, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.274, 10.100], loss: 0.003470, mae: 0.065611, mean_q: 0.565601
 59315/100000: episode: 993, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 5.099, mean reward: 0.567 [0.461, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.644, 10.100], loss: 0.002739, mae: 0.056326, mean_q: 0.615172
 59324/100000: episode: 994, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 4.039, mean reward: 0.449 [0.382, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.505, 10.100], loss: 0.003613, mae: 0.067792, mean_q: 0.597033
 59339/100000: episode: 995, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 5.983, mean reward: 0.399 [0.242, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.207, 10.100], loss: 0.003583, mae: 0.066225, mean_q: 0.600354
 59354/100000: episode: 996, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 6.850, mean reward: 0.457 [0.398, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.407, 10.100], loss: 0.003445, mae: 0.063800, mean_q: 0.609851
[Info] FALSIFICATION!
 59355/100000: episode: 997, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.134, 9.911], loss: 0.002652, mae: 0.060407, mean_q: 0.569111
 59455/100000: episode: 998, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -12.392, mean reward: -0.124 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.398, 10.098], loss: 0.017166, mae: 0.075062, mean_q: 0.582979
 59555/100000: episode: 999, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.868, mean reward: -0.189 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.136, 10.379], loss: 0.029616, mae: 0.074017, mean_q: 0.587402
 59655/100000: episode: 1000, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -11.861, mean reward: -0.119 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.095, 10.125], loss: 0.003940, mae: 0.065603, mean_q: 0.575197
 59755/100000: episode: 1001, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -19.552, mean reward: -0.196 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.597, 10.283], loss: 0.029346, mae: 0.074842, mean_q: 0.539420
 59855/100000: episode: 1002, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.740, mean reward: -0.177 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.414, 10.141], loss: 0.016073, mae: 0.066294, mean_q: 0.518861
 59955/100000: episode: 1003, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -21.315, mean reward: -0.213 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.079, 10.205], loss: 0.031589, mae: 0.078901, mean_q: 0.505822
 60055/100000: episode: 1004, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -13.614, mean reward: -0.136 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.854, 10.101], loss: 0.004010, mae: 0.063603, mean_q: 0.493882
 60155/100000: episode: 1005, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.844, mean reward: -0.168 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.850, 10.191], loss: 0.017239, mae: 0.071860, mean_q: 0.473752
 60255/100000: episode: 1006, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.778, mean reward: -0.168 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.819, 10.098], loss: 0.003401, mae: 0.062452, mean_q: 0.461645
 60355/100000: episode: 1007, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.516, mean reward: -0.185 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.733, 10.098], loss: 0.015841, mae: 0.062393, mean_q: 0.423384
 60455/100000: episode: 1008, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.650, mean reward: -0.166 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.753, 10.098], loss: 0.004104, mae: 0.068139, mean_q: 0.435216
 60555/100000: episode: 1009, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.110, mean reward: -0.181 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.904, 10.152], loss: 0.016180, mae: 0.065604, mean_q: 0.408943
 60655/100000: episode: 1010, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.499, mean reward: -0.175 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.905, 10.098], loss: 0.015845, mae: 0.064204, mean_q: 0.394809
 60755/100000: episode: 1011, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -10.553, mean reward: -0.106 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.802, 10.144], loss: 0.002902, mae: 0.058152, mean_q: 0.354518
 60855/100000: episode: 1012, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.829, mean reward: -0.158 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.136, 10.098], loss: 0.015746, mae: 0.061399, mean_q: 0.345964
 60955/100000: episode: 1013, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.778, mean reward: -0.188 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.977, 10.098], loss: 0.003559, mae: 0.064152, mean_q: 0.356599
 61055/100000: episode: 1014, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.323, mean reward: -0.173 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.512, 10.198], loss: 0.003209, mae: 0.060582, mean_q: 0.348048
 61155/100000: episode: 1015, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.903, mean reward: -0.189 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.619, 10.114], loss: 0.029077, mae: 0.069583, mean_q: 0.321216
 61255/100000: episode: 1016, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.478, mean reward: -0.195 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.465, 10.203], loss: 0.002876, mae: 0.057077, mean_q: 0.282352
 61355/100000: episode: 1017, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.943, mean reward: -0.159 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.027, 10.239], loss: 0.002872, mae: 0.057248, mean_q: 0.276024
 61455/100000: episode: 1018, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.286, mean reward: -0.183 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.406, 10.280], loss: 0.002588, mae: 0.054035, mean_q: 0.258581
 61555/100000: episode: 1019, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.727, mean reward: -0.177 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.296, 10.098], loss: 0.002716, mae: 0.055523, mean_q: 0.241115
 61655/100000: episode: 1020, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -13.692, mean reward: -0.137 [-1.000, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.385, 10.123], loss: 0.002795, mae: 0.056304, mean_q: 0.214530
 61755/100000: episode: 1021, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.193, mean reward: -0.182 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.457, 10.105], loss: 0.002748, mae: 0.054569, mean_q: 0.190039
 61855/100000: episode: 1022, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -14.334, mean reward: -0.143 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.779, 10.098], loss: 0.016286, mae: 0.065180, mean_q: 0.208916
 61955/100000: episode: 1023, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.776, mean reward: -0.198 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.951, 10.163], loss: 0.002774, mae: 0.055746, mean_q: 0.140625
 62055/100000: episode: 1024, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -13.910, mean reward: -0.139 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.293, 10.098], loss: 0.015961, mae: 0.061460, mean_q: 0.105135
 62155/100000: episode: 1025, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -18.608, mean reward: -0.186 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.756, 10.098], loss: 0.016799, mae: 0.065900, mean_q: 0.134659
 62255/100000: episode: 1026, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.516, mean reward: -0.185 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.379, 10.277], loss: 0.003707, mae: 0.063386, mean_q: 0.078006
 62355/100000: episode: 1027, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.595, mean reward: -0.186 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.117, 10.098], loss: 0.016094, mae: 0.064573, mean_q: 0.061539
 62455/100000: episode: 1028, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -16.785, mean reward: -0.168 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.127, 10.186], loss: 0.002822, mae: 0.055445, mean_q: 0.084101
 62555/100000: episode: 1029, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -19.599, mean reward: -0.196 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.129, 10.098], loss: 0.002729, mae: 0.053842, mean_q: 0.034726
 62655/100000: episode: 1030, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -11.725, mean reward: -0.117 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.421, 10.144], loss: 0.002850, mae: 0.055257, mean_q: 0.046481
 62755/100000: episode: 1031, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -19.101, mean reward: -0.191 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.360, 10.205], loss: 0.002892, mae: 0.055138, mean_q: 0.018125
 62855/100000: episode: 1032, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.641, mean reward: -0.196 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.970, 10.106], loss: 0.015673, mae: 0.059375, mean_q: -0.010953
 62955/100000: episode: 1033, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.812, mean reward: -0.168 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.111, 10.098], loss: 0.002818, mae: 0.054457, mean_q: -0.040201
 63055/100000: episode: 1034, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.714, mean reward: -0.167 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.733, 10.237], loss: 0.016284, mae: 0.062302, mean_q: -0.052755
 63155/100000: episode: 1035, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.734, mean reward: -0.167 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.588, 10.249], loss: 0.002842, mae: 0.054052, mean_q: -0.074583
 63255/100000: episode: 1036, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -13.206, mean reward: -0.132 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.675, 10.244], loss: 0.002837, mae: 0.053761, mean_q: -0.088661
 63355/100000: episode: 1037, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.337, mean reward: -0.173 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.102, 10.216], loss: 0.015817, mae: 0.058698, mean_q: -0.110924
 63455/100000: episode: 1038, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -11.721, mean reward: -0.117 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.823, 10.130], loss: 0.002940, mae: 0.053844, mean_q: -0.155198
 63555/100000: episode: 1039, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -14.536, mean reward: -0.145 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.090, 10.098], loss: 0.002561, mae: 0.050171, mean_q: -0.177029
 63655/100000: episode: 1040, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.114, mean reward: -0.191 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.756, 10.137], loss: 0.029865, mae: 0.068951, mean_q: -0.163499
 63755/100000: episode: 1041, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.932, mean reward: -0.179 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.284, 10.173], loss: 0.002574, mae: 0.050959, mean_q: -0.211293
 63855/100000: episode: 1042, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.372, mean reward: -0.154 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.367, 10.098], loss: 0.015591, mae: 0.057064, mean_q: -0.191716
 63955/100000: episode: 1043, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.275, mean reward: -0.173 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.432, 10.098], loss: 0.002742, mae: 0.051466, mean_q: -0.268276
 64055/100000: episode: 1044, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.326, mean reward: -0.183 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.265, 10.098], loss: 0.016034, mae: 0.058458, mean_q: -0.252289
 64155/100000: episode: 1045, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.278, mean reward: -0.183 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.515, 10.098], loss: 0.002460, mae: 0.048935, mean_q: -0.267594
 64255/100000: episode: 1046, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -10.199, mean reward: -0.102 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.749, 10.098], loss: 0.015320, mae: 0.053737, mean_q: -0.297824
 64355/100000: episode: 1047, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -19.243, mean reward: -0.192 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.678, 10.110], loss: 0.002485, mae: 0.049607, mean_q: -0.311199
 64455/100000: episode: 1048, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.625, mean reward: -0.186 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.989, 10.098], loss: 0.002344, mae: 0.048249, mean_q: -0.322053
 64555/100000: episode: 1049, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.638, mean reward: -0.176 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.869, 10.098], loss: 0.003508, mae: 0.056453, mean_q: -0.318751
 64655/100000: episode: 1050, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.463, mean reward: -0.165 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.952, 10.184], loss: 0.002537, mae: 0.049663, mean_q: -0.314464
 64755/100000: episode: 1051, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.018, mean reward: -0.150 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.422, 10.320], loss: 0.003216, mae: 0.054819, mean_q: -0.362069
 64855/100000: episode: 1052, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.489, mean reward: -0.175 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.906, 10.098], loss: 0.002767, mae: 0.052030, mean_q: -0.292492
 64955/100000: episode: 1053, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.837, mean reward: -0.188 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.598, 10.098], loss: 0.002625, mae: 0.050654, mean_q: -0.311075
 65055/100000: episode: 1054, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -9.352, mean reward: -0.094 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.649, 10.098], loss: 0.002711, mae: 0.052756, mean_q: -0.301846
 65155/100000: episode: 1055, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.356, mean reward: -0.164 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.727, 10.121], loss: 0.002790, mae: 0.051999, mean_q: -0.340423
 65255/100000: episode: 1056, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.493, mean reward: -0.185 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.709, 10.163], loss: 0.002563, mae: 0.049815, mean_q: -0.308777
 65355/100000: episode: 1057, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.764, mean reward: -0.178 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.815, 10.098], loss: 0.002498, mae: 0.049151, mean_q: -0.312223
 65455/100000: episode: 1058, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.267, mean reward: -0.183 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.640, 10.113], loss: 0.002446, mae: 0.048563, mean_q: -0.317102
 65555/100000: episode: 1059, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -11.383, mean reward: -0.114 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.368, 10.380], loss: 0.002465, mae: 0.048520, mean_q: -0.304578
 65655/100000: episode: 1060, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.052, mean reward: -0.171 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.841, 10.195], loss: 0.002364, mae: 0.047526, mean_q: -0.347444
 65755/100000: episode: 1061, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.522, mean reward: -0.195 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.662, 10.183], loss: 0.002215, mae: 0.046062, mean_q: -0.325910
 65855/100000: episode: 1062, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.846, mean reward: -0.198 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.958, 10.098], loss: 0.002434, mae: 0.048826, mean_q: -0.327182
 65955/100000: episode: 1063, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.692, mean reward: -0.167 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.977, 10.098], loss: 0.002645, mae: 0.051481, mean_q: -0.313469
 66055/100000: episode: 1064, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.055, mean reward: -0.191 [-1.000, 0.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.402, 10.098], loss: 0.002406, mae: 0.048410, mean_q: -0.325612
 66155/100000: episode: 1065, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.152, mean reward: -0.182 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.786, 10.098], loss: 0.002502, mae: 0.050266, mean_q: -0.324076
 66255/100000: episode: 1066, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -13.154, mean reward: -0.132 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.957, 10.306], loss: 0.002372, mae: 0.048127, mean_q: -0.301819
 66355/100000: episode: 1067, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -14.722, mean reward: -0.147 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.864, 10.098], loss: 0.002319, mae: 0.047161, mean_q: -0.319749
 66455/100000: episode: 1068, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.585, mean reward: -0.156 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.097, 10.309], loss: 0.002395, mae: 0.048445, mean_q: -0.320680
 66555/100000: episode: 1069, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.134, mean reward: -0.161 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.776, 10.155], loss: 0.002462, mae: 0.048619, mean_q: -0.288979
 66655/100000: episode: 1070, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.311, mean reward: -0.163 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.063, 10.293], loss: 0.002698, mae: 0.050322, mean_q: -0.290326
 66755/100000: episode: 1071, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.089, mean reward: -0.181 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.642, 10.265], loss: 0.004488, mae: 0.062580, mean_q: -0.288921
 66855/100000: episode: 1072, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -16.076, mean reward: -0.161 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.089, 10.098], loss: 0.002517, mae: 0.050713, mean_q: -0.323285
 66955/100000: episode: 1073, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.385, mean reward: -0.164 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.570, 10.148], loss: 0.002399, mae: 0.049442, mean_q: -0.285781
 67055/100000: episode: 1074, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.164, mean reward: -0.152 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.534, 10.098], loss: 0.002297, mae: 0.047423, mean_q: -0.313209
 67155/100000: episode: 1075, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.284, mean reward: -0.153 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.364, 10.243], loss: 0.002360, mae: 0.049058, mean_q: -0.286592
 67255/100000: episode: 1076, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -20.215, mean reward: -0.202 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.926, 10.162], loss: 0.002283, mae: 0.047251, mean_q: -0.316723
 67355/100000: episode: 1077, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.382, mean reward: -0.154 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.024, 10.274], loss: 0.002154, mae: 0.045538, mean_q: -0.326112
 67455/100000: episode: 1078, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.613, mean reward: -0.186 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.336, 10.098], loss: 0.002285, mae: 0.047255, mean_q: -0.323791
 67555/100000: episode: 1079, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.210, mean reward: -0.162 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.126, 10.098], loss: 0.002492, mae: 0.049760, mean_q: -0.291605
 67655/100000: episode: 1080, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -14.231, mean reward: -0.142 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.525, 10.098], loss: 0.002564, mae: 0.050406, mean_q: -0.304470
 67755/100000: episode: 1081, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -14.193, mean reward: -0.142 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.873, 10.098], loss: 0.002410, mae: 0.050555, mean_q: -0.290343
 67855/100000: episode: 1082, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.220, mean reward: -0.182 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.503, 10.286], loss: 0.002298, mae: 0.047546, mean_q: -0.312823
 67955/100000: episode: 1083, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.484, mean reward: -0.185 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.662, 10.111], loss: 0.002365, mae: 0.048633, mean_q: -0.279229
 68055/100000: episode: 1084, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.748, mean reward: -0.157 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.503, 10.098], loss: 0.002317, mae: 0.047807, mean_q: -0.316487
 68155/100000: episode: 1085, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -13.357, mean reward: -0.134 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.280, 10.098], loss: 0.002181, mae: 0.046303, mean_q: -0.326581
 68255/100000: episode: 1086, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -20.045, mean reward: -0.200 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.366, 10.098], loss: 0.002392, mae: 0.050737, mean_q: -0.282297
 68355/100000: episode: 1087, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.003, mean reward: -0.160 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.845, 10.098], loss: 0.002450, mae: 0.048517, mean_q: -0.329452
 68455/100000: episode: 1088, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -15.225, mean reward: -0.152 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.284, 10.098], loss: 0.002888, mae: 0.054896, mean_q: -0.324503
 68555/100000: episode: 1089, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -9.566, mean reward: -0.096 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.262, 10.098], loss: 0.002467, mae: 0.050482, mean_q: -0.312363
 68655/100000: episode: 1090, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.323, mean reward: -0.163 [-1.000, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.416, 10.098], loss: 0.002298, mae: 0.047651, mean_q: -0.297015
 68755/100000: episode: 1091, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.656, mean reward: -0.177 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.883, 10.186], loss: 0.002295, mae: 0.047174, mean_q: -0.311641
 68855/100000: episode: 1092, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -14.197, mean reward: -0.142 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.376, 10.335], loss: 0.002423, mae: 0.049314, mean_q: -0.318182
 68955/100000: episode: 1093, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.614, mean reward: -0.166 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.815, 10.301], loss: 0.002474, mae: 0.049800, mean_q: -0.285974
 69055/100000: episode: 1094, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.908, mean reward: -0.159 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.466, 10.098], loss: 0.002235, mae: 0.046436, mean_q: -0.297520
 69155/100000: episode: 1095, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.597, mean reward: -0.156 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.254, 10.203], loss: 0.002311, mae: 0.048196, mean_q: -0.314935
 69255/100000: episode: 1096, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.167, mean reward: -0.152 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.409, 10.333], loss: 0.002472, mae: 0.049363, mean_q: -0.312045
[Info] 100-TH LEVEL FOUND: 0.6052295565605164, Considering 10/90 traces
 69355/100000: episode: 1097, duration: 4.352s, episode steps: 100, steps per second: 23, episode reward: -15.910, mean reward: -0.159 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.944, 10.098], loss: 0.002315, mae: 0.047748, mean_q: -0.322721
 69369/100000: episode: 1098, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 4.423, mean reward: 0.316 [0.253, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.234, 10.100], loss: 0.002326, mae: 0.046734, mean_q: -0.375664
 69387/100000: episode: 1099, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 5.563, mean reward: 0.309 [0.179, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.153, 10.100], loss: 0.002466, mae: 0.049890, mean_q: -0.299937
 69433/100000: episode: 1100, duration: 0.244s, episode steps: 46, steps per second: 189, episode reward: 12.791, mean reward: 0.278 [0.115, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.109, 10.100], loss: 0.002327, mae: 0.047770, mean_q: -0.345372
 69447/100000: episode: 1101, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 4.070, mean reward: 0.291 [0.213, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.191, 10.100], loss: 0.001893, mae: 0.042325, mean_q: -0.309461
 69486/100000: episode: 1102, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 10.887, mean reward: 0.279 [0.016, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.819, 10.100], loss: 0.002587, mae: 0.051149, mean_q: -0.277490
 69500/100000: episode: 1103, duration: 0.087s, episode steps: 14, steps per second: 160, episode reward: 4.457, mean reward: 0.318 [0.220, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.273, 10.100], loss: 0.002364, mae: 0.047633, mean_q: -0.294750
 69532/100000: episode: 1104, duration: 0.171s, episode steps: 32, steps per second: 188, episode reward: 12.058, mean reward: 0.377 [0.275, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.497, 10.100], loss: 0.002481, mae: 0.050033, mean_q: -0.214951
[Info] FALSIFICATION!
 69545/100000: episode: 1105, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 15.516, mean reward: 1.194 [0.402, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.699 [-0.073, 7.867], loss: 0.002595, mae: 0.049307, mean_q: -0.285314
 69645/100000: episode: 1106, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.867, mean reward: -0.179 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.806, 10.098], loss: 0.016487, mae: 0.058824, mean_q: -0.261135
 69745/100000: episode: 1107, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -13.271, mean reward: -0.133 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.086, 10.210], loss: 0.018675, mae: 0.071570, mean_q: -0.276940
 69845/100000: episode: 1108, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.559, mean reward: -0.156 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.373, 10.330], loss: 0.005308, mae: 0.066030, mean_q: -0.224073
 69945/100000: episode: 1109, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.527, mean reward: -0.185 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.007, 10.189], loss: 0.002390, mae: 0.049936, mean_q: -0.267139
 70045/100000: episode: 1110, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.397, mean reward: -0.174 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.973, 10.241], loss: 0.002304, mae: 0.047141, mean_q: -0.287276
 70145/100000: episode: 1111, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -13.641, mean reward: -0.136 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.728, 10.098], loss: 0.016453, mae: 0.058883, mean_q: -0.290875
 70245/100000: episode: 1112, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.515, mean reward: -0.165 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.678, 10.426], loss: 0.016198, mae: 0.058195, mean_q: -0.278315
 70345/100000: episode: 1113, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -16.821, mean reward: -0.168 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.220, 10.168], loss: 0.029252, mae: 0.063330, mean_q: -0.239987
 70445/100000: episode: 1114, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.750, mean reward: -0.167 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.698, 10.199], loss: 0.029613, mae: 0.065783, mean_q: -0.248760
 70545/100000: episode: 1115, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -10.724, mean reward: -0.107 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.541, 10.362], loss: 0.029600, mae: 0.066942, mean_q: -0.294919
 70645/100000: episode: 1116, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -19.073, mean reward: -0.191 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.775, 10.170], loss: 0.029536, mae: 0.063261, mean_q: -0.248405
 70745/100000: episode: 1117, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.580, mean reward: -0.146 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.149, 10.323], loss: 0.041704, mae: 0.067898, mean_q: -0.261993
 70845/100000: episode: 1118, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -13.997, mean reward: -0.140 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.726, 10.316], loss: 0.002299, mae: 0.047798, mean_q: -0.282556
 70945/100000: episode: 1119, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -10.715, mean reward: -0.107 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.121, 10.098], loss: 0.028674, mae: 0.060391, mean_q: -0.242137
 71045/100000: episode: 1120, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -14.872, mean reward: -0.149 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.874, 10.098], loss: 0.002339, mae: 0.048680, mean_q: -0.218705
 71145/100000: episode: 1121, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.768, mean reward: -0.198 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.725, 10.098], loss: 0.015368, mae: 0.051771, mean_q: -0.287920
 71245/100000: episode: 1122, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -16.919, mean reward: -0.169 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.439, 10.278], loss: 0.002589, mae: 0.051086, mean_q: -0.245729
 71345/100000: episode: 1123, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.278, mean reward: -0.183 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.443, 10.098], loss: 0.015560, mae: 0.055187, mean_q: -0.252285
 71445/100000: episode: 1124, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -14.653, mean reward: -0.147 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.039, 10.098], loss: 0.015557, mae: 0.054880, mean_q: -0.247131
 71545/100000: episode: 1125, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -17.399, mean reward: -0.174 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.015, 10.293], loss: 0.002327, mae: 0.049011, mean_q: -0.262645
 71645/100000: episode: 1126, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.487, mean reward: -0.205 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.507, 10.222], loss: 0.002280, mae: 0.048314, mean_q: -0.231756
 71745/100000: episode: 1127, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -16.266, mean reward: -0.163 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.822, 10.098], loss: 0.002221, mae: 0.046832, mean_q: -0.279859
 71845/100000: episode: 1128, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.300, mean reward: -0.143 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.924, 10.098], loss: 0.002317, mae: 0.047856, mean_q: -0.272537
 71945/100000: episode: 1129, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.201, mean reward: -0.182 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.103, 10.098], loss: 0.015178, mae: 0.051748, mean_q: -0.237135
 72045/100000: episode: 1130, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -15.494, mean reward: -0.155 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.610, 10.222], loss: 0.002587, mae: 0.050466, mean_q: -0.285929
 72145/100000: episode: 1131, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -13.382, mean reward: -0.134 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.578, 10.098], loss: 0.015596, mae: 0.054575, mean_q: -0.279897
 72245/100000: episode: 1132, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.314, mean reward: -0.163 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.154, 10.098], loss: 0.002441, mae: 0.050914, mean_q: -0.250118
 72345/100000: episode: 1133, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -14.736, mean reward: -0.147 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.217, 10.098], loss: 0.002101, mae: 0.045977, mean_q: -0.250407
 72445/100000: episode: 1134, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.824, mean reward: -0.158 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.256, 10.189], loss: 0.002244, mae: 0.046953, mean_q: -0.283360
 72545/100000: episode: 1135, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.947, mean reward: -0.189 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.263, 10.098], loss: 0.016363, mae: 0.060403, mean_q: -0.270340
 72645/100000: episode: 1136, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.942, mean reward: -0.169 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.787, 10.228], loss: 0.016413, mae: 0.061095, mean_q: -0.280758
 72745/100000: episode: 1137, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.140, mean reward: -0.161 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.164, 10.098], loss: 0.002269, mae: 0.047788, mean_q: -0.252913
 72845/100000: episode: 1138, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.715, mean reward: -0.197 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.744, 10.098], loss: 0.015824, mae: 0.055948, mean_q: -0.257029
 72945/100000: episode: 1139, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -18.255, mean reward: -0.183 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.683, 10.150], loss: 0.029012, mae: 0.064844, mean_q: -0.276206
 73045/100000: episode: 1140, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.326, mean reward: -0.193 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.645, 10.139], loss: 0.015502, mae: 0.057198, mean_q: -0.291085
 73145/100000: episode: 1141, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.040, mean reward: -0.190 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.223, 10.244], loss: 0.002409, mae: 0.048507, mean_q: -0.262173
 73245/100000: episode: 1142, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -13.131, mean reward: -0.131 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.780, 10.098], loss: 0.015941, mae: 0.058490, mean_q: -0.279696
 73345/100000: episode: 1143, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -13.766, mean reward: -0.138 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.809, 10.098], loss: 0.027642, mae: 0.059384, mean_q: -0.232386
 73445/100000: episode: 1144, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -7.375, mean reward: -0.074 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.836, 10.098], loss: 0.002426, mae: 0.048335, mean_q: -0.255676
 73545/100000: episode: 1145, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -17.121, mean reward: -0.171 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.028, 10.143], loss: 0.028779, mae: 0.061295, mean_q: -0.275641
 73645/100000: episode: 1146, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -13.427, mean reward: -0.134 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.936, 10.434], loss: 0.003142, mae: 0.053516, mean_q: -0.254128
 73745/100000: episode: 1147, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -19.670, mean reward: -0.197 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.774, 10.098], loss: 0.002326, mae: 0.047808, mean_q: -0.276834
 73845/100000: episode: 1148, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.947, mean reward: -0.179 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.869, 10.098], loss: 0.015532, mae: 0.055152, mean_q: -0.275397
 73945/100000: episode: 1149, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.816, mean reward: -0.178 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.379, 10.239], loss: 0.015679, mae: 0.055447, mean_q: -0.287278
 74045/100000: episode: 1150, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.356, mean reward: -0.184 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.501, 10.178], loss: 0.002338, mae: 0.047406, mean_q: -0.281812
 74145/100000: episode: 1151, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.632, mean reward: -0.156 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.849, 10.300], loss: 0.002324, mae: 0.047420, mean_q: -0.283202
 74245/100000: episode: 1152, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.700, mean reward: -0.187 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.665, 10.136], loss: 0.028956, mae: 0.063431, mean_q: -0.229772
 74345/100000: episode: 1153, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.656, mean reward: -0.157 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.641, 10.139], loss: 0.056331, mae: 0.088588, mean_q: -0.242017
 74445/100000: episode: 1154, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.919, mean reward: -0.159 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.968, 10.175], loss: 0.002511, mae: 0.050530, mean_q: -0.318769
 74545/100000: episode: 1155, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.204, mean reward: -0.162 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.357, 10.098], loss: 0.002308, mae: 0.047192, mean_q: -0.283938
 74645/100000: episode: 1156, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.500, mean reward: -0.185 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.649, 10.160], loss: 0.002371, mae: 0.047688, mean_q: -0.293350
 74745/100000: episode: 1157, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.320, mean reward: -0.183 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.536, 10.177], loss: 0.002416, mae: 0.049179, mean_q: -0.282145
 74845/100000: episode: 1158, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -12.788, mean reward: -0.128 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.016, 10.428], loss: 0.002325, mae: 0.046937, mean_q: -0.328056
 74945/100000: episode: 1159, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.909, mean reward: -0.169 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.791, 10.327], loss: 0.002092, mae: 0.044365, mean_q: -0.315975
 75045/100000: episode: 1160, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.714, mean reward: -0.187 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.685, 10.098], loss: 0.002330, mae: 0.046806, mean_q: -0.319079
 75145/100000: episode: 1161, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -9.717, mean reward: -0.097 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.986, 10.371], loss: 0.002419, mae: 0.047821, mean_q: -0.331470
 75245/100000: episode: 1162, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.358, mean reward: -0.184 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.924, 10.212], loss: 0.002316, mae: 0.047604, mean_q: -0.299121
 75345/100000: episode: 1163, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -12.338, mean reward: -0.123 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.103, 10.420], loss: 0.002318, mae: 0.047487, mean_q: -0.299858
 75445/100000: episode: 1164, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -13.537, mean reward: -0.135 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.031, 10.187], loss: 0.002365, mae: 0.048006, mean_q: -0.299842
 75545/100000: episode: 1165, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.882, mean reward: -0.159 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.908, 10.364], loss: 0.002383, mae: 0.048878, mean_q: -0.285354
 75645/100000: episode: 1166, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.339, mean reward: -0.193 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.191, 10.098], loss: 0.002342, mae: 0.047591, mean_q: -0.308734
 75745/100000: episode: 1167, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.962, mean reward: -0.170 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.608, 10.120], loss: 0.002454, mae: 0.048893, mean_q: -0.297101
 75845/100000: episode: 1168, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.546, mean reward: -0.195 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.677, 10.168], loss: 0.002362, mae: 0.047281, mean_q: -0.339050
 75945/100000: episode: 1169, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.318, mean reward: -0.193 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.274, 10.098], loss: 0.002352, mae: 0.048495, mean_q: -0.292552
 76045/100000: episode: 1170, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -20.831, mean reward: -0.208 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.960, 10.120], loss: 0.002518, mae: 0.049781, mean_q: -0.318959
 76145/100000: episode: 1171, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -13.295, mean reward: -0.133 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.988, 10.098], loss: 0.002198, mae: 0.047503, mean_q: -0.334282
 76245/100000: episode: 1172, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.862, mean reward: -0.179 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.116, 10.120], loss: 0.003678, mae: 0.057796, mean_q: -0.307362
 76345/100000: episode: 1173, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.449, mean reward: -0.154 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.039, 10.098], loss: 0.005714, mae: 0.063182, mean_q: -0.332721
 76445/100000: episode: 1174, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -16.766, mean reward: -0.168 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.528, 10.140], loss: 0.002567, mae: 0.050471, mean_q: -0.291991
 76545/100000: episode: 1175, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.830, mean reward: -0.198 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.081, 10.201], loss: 0.002367, mae: 0.047330, mean_q: -0.325925
 76645/100000: episode: 1176, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -11.892, mean reward: -0.119 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.909, 10.373], loss: 0.002458, mae: 0.048940, mean_q: -0.300017
 76745/100000: episode: 1177, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.324, mean reward: -0.173 [-1.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.883, 10.283], loss: 0.002579, mae: 0.050343, mean_q: -0.341616
 76845/100000: episode: 1178, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.169, mean reward: -0.192 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.155, 10.206], loss: 0.002500, mae: 0.049277, mean_q: -0.306186
 76945/100000: episode: 1179, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.239, mean reward: -0.172 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.448, 10.098], loss: 0.002473, mae: 0.048215, mean_q: -0.315511
 77045/100000: episode: 1180, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.105, mean reward: -0.181 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.521, 10.098], loss: 0.002434, mae: 0.048541, mean_q: -0.329478
 77145/100000: episode: 1181, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.473, mean reward: -0.175 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.622, 10.098], loss: 0.002408, mae: 0.046962, mean_q: -0.328179
 77245/100000: episode: 1182, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.251, mean reward: -0.183 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.660, 10.178], loss: 0.002418, mae: 0.048517, mean_q: -0.325049
 77345/100000: episode: 1183, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -14.141, mean reward: -0.141 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.904, 10.247], loss: 0.002545, mae: 0.050238, mean_q: -0.283596
 77445/100000: episode: 1184, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.137, mean reward: -0.161 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.242, 10.098], loss: 0.002369, mae: 0.047927, mean_q: -0.292186
 77545/100000: episode: 1185, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.633, mean reward: -0.196 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.283, 10.098], loss: 0.002245, mae: 0.046815, mean_q: -0.288540
 77645/100000: episode: 1186, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.745, mean reward: -0.177 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.508, 10.338], loss: 0.002270, mae: 0.047028, mean_q: -0.322597
 77745/100000: episode: 1187, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -17.755, mean reward: -0.178 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.988, 10.104], loss: 0.002354, mae: 0.047692, mean_q: -0.299356
 77845/100000: episode: 1188, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.221, mean reward: -0.182 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.508, 10.098], loss: 0.002256, mae: 0.046641, mean_q: -0.318364
 77945/100000: episode: 1189, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.355, mean reward: -0.164 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.897, 10.098], loss: 0.002190, mae: 0.045731, mean_q: -0.337219
 78045/100000: episode: 1190, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.651, mean reward: -0.197 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.641, 10.226], loss: 0.002234, mae: 0.045989, mean_q: -0.329516
 78145/100000: episode: 1191, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.785, mean reward: -0.178 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.390, 10.111], loss: 0.002420, mae: 0.048155, mean_q: -0.307027
 78245/100000: episode: 1192, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.026, mean reward: -0.160 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.204, 10.369], loss: 0.002320, mae: 0.046705, mean_q: -0.321276
 78345/100000: episode: 1193, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.349, mean reward: -0.183 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.947, 10.098], loss: 0.002408, mae: 0.048486, mean_q: -0.288072
 78445/100000: episode: 1194, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.944, mean reward: -0.199 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.093, 10.098], loss: 0.002299, mae: 0.047145, mean_q: -0.307436
 78545/100000: episode: 1195, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.174, mean reward: -0.182 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.332, 10.219], loss: 0.002337, mae: 0.047435, mean_q: -0.313666
 78645/100000: episode: 1196, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -20.351, mean reward: -0.204 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.465, 10.167], loss: 0.002302, mae: 0.046487, mean_q: -0.354703
 78745/100000: episode: 1197, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -19.829, mean reward: -0.198 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.167, 10.098], loss: 0.002360, mae: 0.047784, mean_q: -0.342066
 78845/100000: episode: 1198, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.346, mean reward: -0.183 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.653, 10.225], loss: 0.002390, mae: 0.047073, mean_q: -0.336351
 78945/100000: episode: 1199, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.654, mean reward: -0.187 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.838, 10.158], loss: 0.002619, mae: 0.050765, mean_q: -0.296095
 79045/100000: episode: 1200, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.985, mean reward: -0.160 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.315, 10.208], loss: 0.002311, mae: 0.048000, mean_q: -0.300407
 79145/100000: episode: 1201, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -20.275, mean reward: -0.203 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.558, 10.098], loss: 0.002367, mae: 0.048084, mean_q: -0.322941
 79245/100000: episode: 1202, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.288, mean reward: -0.173 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.380, 10.098], loss: 0.002377, mae: 0.047591, mean_q: -0.340225
 79345/100000: episode: 1203, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.361, mean reward: -0.174 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.311, 10.098], loss: 0.002420, mae: 0.050329, mean_q: -0.313241
 79445/100000: episode: 1204, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -14.957, mean reward: -0.150 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.306, 10.298], loss: 0.002369, mae: 0.047767, mean_q: -0.349315
[Info] 100-TH LEVEL FOUND: 0.6803434491157532, Considering 10/90 traces
 79545/100000: episode: 1205, duration: 4.393s, episode steps: 100, steps per second: 23, episode reward: -18.117, mean reward: -0.181 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.290, 10.163], loss: 0.002228, mae: 0.046524, mean_q: -0.340066
 79566/100000: episode: 1206, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 7.211, mean reward: 0.343 [0.131, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.240, 10.250], loss: 0.002433, mae: 0.049534, mean_q: -0.331245
 79594/100000: episode: 1207, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 11.037, mean reward: 0.394 [0.179, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.285], loss: 0.002446, mae: 0.049228, mean_q: -0.294671
 79615/100000: episode: 1208, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 6.413, mean reward: 0.305 [0.110, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.270, 10.344], loss: 0.004949, mae: 0.060542, mean_q: -0.265053
 79633/100000: episode: 1209, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 7.363, mean reward: 0.409 [0.354, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.963, 10.520], loss: 0.004236, mae: 0.061629, mean_q: -0.306826
 79649/100000: episode: 1210, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 6.397, mean reward: 0.400 [0.341, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.227, 10.100], loss: 0.005468, mae: 0.060512, mean_q: -0.321960
 79667/100000: episode: 1211, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 6.047, mean reward: 0.336 [0.284, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.508], loss: 0.004434, mae: 0.064311, mean_q: -0.276226
 79702/100000: episode: 1212, duration: 0.184s, episode steps: 35, steps per second: 190, episode reward: 10.927, mean reward: 0.312 [0.178, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.178, 10.100], loss: 0.005057, mae: 0.065646, mean_q: -0.236957
 79718/100000: episode: 1213, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 6.050, mean reward: 0.378 [0.330, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.237, 10.100], loss: 0.002880, mae: 0.057002, mean_q: -0.349657
 79753/100000: episode: 1214, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 6.949, mean reward: 0.199 [0.085, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.570, 10.100], loss: 0.002269, mae: 0.049173, mean_q: -0.244423
 79774/100000: episode: 1215, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 5.725, mean reward: 0.273 [0.059, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.036, 10.272], loss: 0.002289, mae: 0.047492, mean_q: -0.306917
 79816/100000: episode: 1216, duration: 0.203s, episode steps: 42, steps per second: 206, episode reward: 14.984, mean reward: 0.357 [0.089, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.777, 10.100], loss: 0.002428, mae: 0.048462, mean_q: -0.262789
 79844/100000: episode: 1217, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 6.785, mean reward: 0.242 [0.121, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.821, 10.416], loss: 0.002197, mae: 0.045499, mean_q: -0.287739
 79877/100000: episode: 1218, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 11.355, mean reward: 0.344 [0.096, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.035, 10.225], loss: 0.002255, mae: 0.047745, mean_q: -0.290704
 79919/100000: episode: 1219, duration: 0.216s, episode steps: 42, steps per second: 194, episode reward: 11.430, mean reward: 0.272 [0.084, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.092, 10.100], loss: 0.002309, mae: 0.047990, mean_q: -0.268955
 79935/100000: episode: 1220, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 5.533, mean reward: 0.346 [0.292, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.242, 10.100], loss: 0.002678, mae: 0.052438, mean_q: -0.273397
 79963/100000: episode: 1221, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 11.591, mean reward: 0.414 [0.339, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-1.348, 10.552], loss: 0.002367, mae: 0.049457, mean_q: -0.237147
 80005/100000: episode: 1222, duration: 0.216s, episode steps: 42, steps per second: 194, episode reward: 14.122, mean reward: 0.336 [0.229, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-1.623, 10.100], loss: 0.002679, mae: 0.051789, mean_q: -0.162999
 80026/100000: episode: 1223, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 6.802, mean reward: 0.324 [0.148, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.192, 10.262], loss: 0.002290, mae: 0.047519, mean_q: -0.227416
 80059/100000: episode: 1224, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 15.273, mean reward: 0.463 [0.348, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.117, 10.458], loss: 0.002553, mae: 0.051148, mean_q: -0.150832
 80094/100000: episode: 1225, duration: 0.177s, episode steps: 35, steps per second: 198, episode reward: 9.590, mean reward: 0.274 [0.022, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.594, 10.100], loss: 0.002367, mae: 0.049413, mean_q: -0.237690
 80112/100000: episode: 1226, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 4.933, mean reward: 0.274 [0.191, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.319], loss: 0.002405, mae: 0.049308, mean_q: -0.218557
 80147/100000: episode: 1227, duration: 0.184s, episode steps: 35, steps per second: 190, episode reward: 11.021, mean reward: 0.315 [0.100, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.422, 10.100], loss: 0.002804, mae: 0.054454, mean_q: -0.186141
 80189/100000: episode: 1228, duration: 0.222s, episode steps: 42, steps per second: 189, episode reward: 10.712, mean reward: 0.255 [0.044, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.467, 10.114], loss: 0.002911, mae: 0.055876, mean_q: -0.199068
 80231/100000: episode: 1229, duration: 0.227s, episode steps: 42, steps per second: 185, episode reward: 10.100, mean reward: 0.240 [0.008, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.678, 10.393], loss: 0.002828, mae: 0.054483, mean_q: -0.198878
 80252/100000: episode: 1230, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 6.655, mean reward: 0.317 [0.238, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.348], loss: 0.002476, mae: 0.050854, mean_q: -0.210995
 80273/100000: episode: 1231, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 5.783, mean reward: 0.275 [0.189, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.400], loss: 0.002639, mae: 0.051485, mean_q: -0.152749
 80295/100000: episode: 1232, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 6.271, mean reward: 0.285 [0.212, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.454, 10.354], loss: 0.002217, mae: 0.045678, mean_q: -0.268790
 80328/100000: episode: 1233, duration: 0.184s, episode steps: 33, steps per second: 180, episode reward: 12.510, mean reward: 0.379 [0.082, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.035, 10.278], loss: 0.002456, mae: 0.049436, mean_q: -0.197404
 80370/100000: episode: 1234, duration: 0.241s, episode steps: 42, steps per second: 174, episode reward: 12.704, mean reward: 0.302 [0.134, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.991, 10.100], loss: 0.002496, mae: 0.049102, mean_q: -0.183921
 80412/100000: episode: 1235, duration: 0.222s, episode steps: 42, steps per second: 189, episode reward: 11.131, mean reward: 0.265 [0.121, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-1.133, 10.100], loss: 0.002283, mae: 0.048864, mean_q: -0.137120
 80454/100000: episode: 1236, duration: 0.209s, episode steps: 42, steps per second: 201, episode reward: 14.178, mean reward: 0.338 [0.208, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.392, 10.100], loss: 0.002791, mae: 0.054372, mean_q: -0.120656
 80496/100000: episode: 1237, duration: 0.231s, episode steps: 42, steps per second: 182, episode reward: 9.065, mean reward: 0.216 [0.053, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.219, 10.100], loss: 0.002453, mae: 0.050145, mean_q: -0.192020
 80538/100000: episode: 1238, duration: 0.226s, episode steps: 42, steps per second: 185, episode reward: 9.162, mean reward: 0.218 [0.010, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.523, 10.246], loss: 0.002406, mae: 0.048263, mean_q: -0.167833
 80559/100000: episode: 1239, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 6.111, mean reward: 0.291 [0.181, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-1.222, 10.320], loss: 0.002681, mae: 0.052742, mean_q: -0.161431
 80581/100000: episode: 1240, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 6.504, mean reward: 0.296 [0.099, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.472], loss: 0.002882, mae: 0.054741, mean_q: -0.171321
 80599/100000: episode: 1241, duration: 0.106s, episode steps: 18, steps per second: 169, episode reward: 5.270, mean reward: 0.293 [0.175, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.558, 10.240], loss: 0.002503, mae: 0.051011, mean_q: -0.172676
 80621/100000: episode: 1242, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 6.284, mean reward: 0.286 [0.159, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.534, 10.283], loss: 0.002188, mae: 0.045942, mean_q: -0.151368
 80663/100000: episode: 1243, duration: 0.207s, episode steps: 42, steps per second: 203, episode reward: 9.547, mean reward: 0.227 [0.028, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.457, 10.100], loss: 0.002683, mae: 0.053921, mean_q: -0.076547
 80679/100000: episode: 1244, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 6.475, mean reward: 0.405 [0.319, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.280, 10.100], loss: 0.002113, mae: 0.049633, mean_q: -0.138466
 80695/100000: episode: 1245, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 6.898, mean reward: 0.431 [0.308, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.367, 10.100], loss: 0.002406, mae: 0.050554, mean_q: -0.112823
 80711/100000: episode: 1246, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 6.058, mean reward: 0.379 [0.300, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.379, 10.100], loss: 0.002412, mae: 0.050220, mean_q: -0.087385
 80729/100000: episode: 1247, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 5.528, mean reward: 0.307 [0.218, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-1.292, 10.331], loss: 0.002324, mae: 0.049990, mean_q: -0.053587
 80771/100000: episode: 1248, duration: 0.227s, episode steps: 42, steps per second: 185, episode reward: 11.708, mean reward: 0.279 [0.117, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.500, 10.100], loss: 0.002811, mae: 0.054779, mean_q: -0.075520
 80789/100000: episode: 1249, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 6.464, mean reward: 0.359 [0.256, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-1.289, 10.408], loss: 0.002704, mae: 0.053058, mean_q: -0.147681
 80831/100000: episode: 1250, duration: 0.217s, episode steps: 42, steps per second: 193, episode reward: 11.661, mean reward: 0.278 [0.041, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.887, 10.100], loss: 0.002580, mae: 0.052197, mean_q: -0.096998
 80852/100000: episode: 1251, duration: 0.135s, episode steps: 21, steps per second: 156, episode reward: 6.960, mean reward: 0.331 [0.218, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.478], loss: 0.003223, mae: 0.060246, mean_q: -0.012125
 80874/100000: episode: 1252, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 5.770, mean reward: 0.262 [0.070, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.402, 10.187], loss: 0.002620, mae: 0.052312, mean_q: -0.015838
 80902/100000: episode: 1253, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 7.374, mean reward: 0.263 [0.129, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.918, 10.260], loss: 0.002668, mae: 0.053388, mean_q: -0.072742
 80944/100000: episode: 1254, duration: 0.244s, episode steps: 42, steps per second: 172, episode reward: 13.634, mean reward: 0.325 [0.216, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.302, 10.100], loss: 0.002581, mae: 0.052832, mean_q: -0.075896
 80965/100000: episode: 1255, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 5.877, mean reward: 0.280 [0.053, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.200], loss: 0.002799, mae: 0.054849, mean_q: -0.031842
 80983/100000: episode: 1256, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 5.572, mean reward: 0.310 [0.248, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.193, 10.386], loss: 0.002244, mae: 0.048840, mean_q: -0.086778
 81018/100000: episode: 1257, duration: 0.177s, episode steps: 35, steps per second: 197, episode reward: 7.714, mean reward: 0.220 [0.013, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.156, 10.100], loss: 0.002435, mae: 0.050299, mean_q: -0.059199
 81051/100000: episode: 1258, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 11.096, mean reward: 0.336 [0.084, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.807, 10.312], loss: 0.002648, mae: 0.053725, mean_q: -0.072973
 81079/100000: episode: 1259, duration: 0.138s, episode steps: 28, steps per second: 203, episode reward: 11.366, mean reward: 0.406 [0.333, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.035, 10.637], loss: 0.002495, mae: 0.052188, mean_q: -0.048901
 81097/100000: episode: 1260, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 6.010, mean reward: 0.334 [0.069, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.349, 10.325], loss: 0.003021, mae: 0.058359, mean_q: -0.096021
 81115/100000: episode: 1261, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 6.851, mean reward: 0.381 [0.298, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.526], loss: 0.002624, mae: 0.053600, mean_q: -0.006132
 81150/100000: episode: 1262, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 12.120, mean reward: 0.346 [0.165, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.485, 10.100], loss: 0.002519, mae: 0.051670, mean_q: -0.025374
 81171/100000: episode: 1263, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 9.177, mean reward: 0.437 [0.318, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.484, 10.546], loss: 0.002763, mae: 0.054433, mean_q: 0.007182
 81192/100000: episode: 1264, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 6.071, mean reward: 0.289 [0.179, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.080, 10.165], loss: 0.002191, mae: 0.048105, mean_q: -0.040416
 81227/100000: episode: 1265, duration: 0.194s, episode steps: 35, steps per second: 180, episode reward: 9.341, mean reward: 0.267 [0.043, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.547, 10.100], loss: 0.002972, mae: 0.055741, mean_q: -0.014193
 81260/100000: episode: 1266, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 11.004, mean reward: 0.333 [0.231, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.231, 10.484], loss: 0.003024, mae: 0.057343, mean_q: 0.051038
 81302/100000: episode: 1267, duration: 0.232s, episode steps: 42, steps per second: 181, episode reward: 8.999, mean reward: 0.214 [0.019, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.241, 10.100], loss: 0.003022, mae: 0.057276, mean_q: 0.014959
 81323/100000: episode: 1268, duration: 0.110s, episode steps: 21, steps per second: 192, episode reward: 7.824, mean reward: 0.373 [0.207, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.093, 10.305], loss: 0.003154, mae: 0.055652, mean_q: -0.053070
 81358/100000: episode: 1269, duration: 0.208s, episode steps: 35, steps per second: 169, episode reward: 12.879, mean reward: 0.368 [0.212, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.507, 10.100], loss: 0.007438, mae: 0.071917, mean_q: 0.016428
 81374/100000: episode: 1270, duration: 0.087s, episode steps: 16, steps per second: 185, episode reward: 6.197, mean reward: 0.387 [0.304, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.509, 10.100], loss: 0.006735, mae: 0.071651, mean_q: -0.019679
 81395/100000: episode: 1271, duration: 0.110s, episode steps: 21, steps per second: 192, episode reward: 7.133, mean reward: 0.340 [0.246, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.690, 10.363], loss: 0.003539, mae: 0.064188, mean_q: 0.029703
 81416/100000: episode: 1272, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 5.417, mean reward: 0.258 [0.122, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.236], loss: 0.002746, mae: 0.055513, mean_q: 0.037174
 81449/100000: episode: 1273, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 14.322, mean reward: 0.434 [0.357, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.139, 10.417], loss: 0.002808, mae: 0.057008, mean_q: 0.031732
 81467/100000: episode: 1274, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 5.572, mean reward: 0.310 [0.096, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-1.531, 10.256], loss: 0.002857, mae: 0.056499, mean_q: 0.081485
 81509/100000: episode: 1275, duration: 0.220s, episode steps: 42, steps per second: 191, episode reward: 11.701, mean reward: 0.279 [0.060, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.249, 10.230], loss: 0.002605, mae: 0.053878, mean_q: 0.041669
 81530/100000: episode: 1276, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 5.983, mean reward: 0.285 [0.179, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.040, 10.305], loss: 0.003006, mae: 0.056634, mean_q: 0.063823
 81551/100000: episode: 1277, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 6.229, mean reward: 0.297 [0.231, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.359], loss: 0.002563, mae: 0.053844, mean_q: 0.075993
 81567/100000: episode: 1278, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 6.813, mean reward: 0.426 [0.376, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.680, 10.100], loss: 0.002687, mae: 0.054905, mean_q: 0.050252
 81609/100000: episode: 1279, duration: 0.222s, episode steps: 42, steps per second: 189, episode reward: 9.613, mean reward: 0.229 [0.032, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-1.975, 10.100], loss: 0.002697, mae: 0.054119, mean_q: 0.042616
 81651/100000: episode: 1280, duration: 0.212s, episode steps: 42, steps per second: 198, episode reward: 14.486, mean reward: 0.345 [0.255, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.403, 10.100], loss: 0.002565, mae: 0.053460, mean_q: 0.078152
 81667/100000: episode: 1281, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 6.805, mean reward: 0.425 [0.307, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.453, 10.100], loss: 0.002870, mae: 0.057084, mean_q: 0.127853
 81700/100000: episode: 1282, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 9.531, mean reward: 0.289 [0.173, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.487, 10.372], loss: 0.002566, mae: 0.053764, mean_q: 0.083535
 81735/100000: episode: 1283, duration: 0.189s, episode steps: 35, steps per second: 186, episode reward: 6.126, mean reward: 0.175 [0.022, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.746, 10.136], loss: 0.002642, mae: 0.054467, mean_q: 0.123297
 81770/100000: episode: 1284, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 11.040, mean reward: 0.315 [0.218, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.464, 10.100], loss: 0.002901, mae: 0.056023, mean_q: 0.078682
 81788/100000: episode: 1285, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 5.888, mean reward: 0.327 [0.236, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.335, 10.438], loss: 0.002616, mae: 0.052773, mean_q: 0.101985
 81830/100000: episode: 1286, duration: 0.215s, episode steps: 42, steps per second: 196, episode reward: 10.730, mean reward: 0.255 [0.135, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-2.251, 10.100], loss: 0.002844, mae: 0.055982, mean_q: 0.090271
 81865/100000: episode: 1287, duration: 0.174s, episode steps: 35, steps per second: 201, episode reward: 13.306, mean reward: 0.380 [0.273, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-1.134, 10.100], loss: 0.002872, mae: 0.055977, mean_q: 0.113063
 81881/100000: episode: 1288, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 5.617, mean reward: 0.351 [0.220, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.449, 10.100], loss: 0.002842, mae: 0.055969, mean_q: 0.128592
 81902/100000: episode: 1289, duration: 0.106s, episode steps: 21, steps per second: 197, episode reward: 8.050, mean reward: 0.383 [0.268, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.490], loss: 0.002763, mae: 0.054156, mean_q: 0.090439
 81923/100000: episode: 1290, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 6.571, mean reward: 0.313 [0.174, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.318], loss: 0.002686, mae: 0.054608, mean_q: 0.084636
 81939/100000: episode: 1291, duration: 0.080s, episode steps: 16, steps per second: 201, episode reward: 5.028, mean reward: 0.314 [0.245, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.171, 10.100], loss: 0.002619, mae: 0.054197, mean_q: 0.138592
 81960/100000: episode: 1292, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 7.033, mean reward: 0.335 [0.231, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.333], loss: 0.002692, mae: 0.053221, mean_q: 0.121219
 81978/100000: episode: 1293, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 5.052, mean reward: 0.281 [0.151, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.891, 10.308], loss: 0.002905, mae: 0.055321, mean_q: 0.090881
 81996/100000: episode: 1294, duration: 0.109s, episode steps: 18, steps per second: 166, episode reward: 5.396, mean reward: 0.300 [0.183, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.271], loss: 0.002847, mae: 0.057272, mean_q: 0.149030
[Info] 200-TH LEVEL FOUND: 0.8867976069450378, Considering 10/90 traces
 82012/100000: episode: 1295, duration: 3.946s, episode steps: 16, steps per second: 4, episode reward: 7.018, mean reward: 0.439 [0.364, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.439, 10.100], loss: 0.002765, mae: 0.055886, mean_q: 0.170352
 82036/100000: episode: 1296, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 9.801, mean reward: 0.408 [0.330, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.046, 10.382], loss: 0.002597, mae: 0.052005, mean_q: 0.105847
 82061/100000: episode: 1297, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 8.299, mean reward: 0.332 [0.147, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.254], loss: 0.002802, mae: 0.055934, mean_q: 0.131863
 82086/100000: episode: 1298, duration: 0.129s, episode steps: 25, steps per second: 193, episode reward: 11.712, mean reward: 0.468 [0.323, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.244, 10.491], loss: 0.002605, mae: 0.056456, mean_q: 0.157703
 82117/100000: episode: 1299, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 13.949, mean reward: 0.450 [0.263, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.035, 10.410], loss: 0.002379, mae: 0.050538, mean_q: 0.144176
 82147/100000: episode: 1300, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 10.221, mean reward: 0.341 [0.241, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.219, 10.370], loss: 0.002672, mae: 0.053985, mean_q: 0.222602
 82172/100000: episode: 1301, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 7.884, mean reward: 0.315 [0.202, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.052, 10.314], loss: 0.002623, mae: 0.054782, mean_q: 0.217318
 82190/100000: episode: 1302, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 7.384, mean reward: 0.410 [0.373, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.068, 10.472], loss: 0.003034, mae: 0.057683, mean_q: 0.184510
 82219/100000: episode: 1303, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 8.842, mean reward: 0.305 [0.106, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.848, 10.271], loss: 0.002915, mae: 0.057195, mean_q: 0.177831
 82250/100000: episode: 1304, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 15.327, mean reward: 0.494 [0.375, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.243, 10.582], loss: 0.002634, mae: 0.055770, mean_q: 0.228028
 82281/100000: episode: 1305, duration: 0.157s, episode steps: 31, steps per second: 197, episode reward: 14.856, mean reward: 0.479 [0.356, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.035, 10.466], loss: 0.002646, mae: 0.054365, mean_q: 0.182532
 82303/100000: episode: 1306, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 7.750, mean reward: 0.352 [0.167, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-1.061, 10.367], loss: 0.002859, mae: 0.056279, mean_q: 0.183956
 82327/100000: episode: 1307, duration: 0.146s, episode steps: 24, steps per second: 165, episode reward: 7.917, mean reward: 0.330 [0.213, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.392], loss: 0.002907, mae: 0.058286, mean_q: 0.234299
 82357/100000: episode: 1308, duration: 0.175s, episode steps: 30, steps per second: 172, episode reward: 10.404, mean reward: 0.347 [0.212, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.908, 10.366], loss: 0.002599, mae: 0.053534, mean_q: 0.212456
 82371/100000: episode: 1309, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 6.172, mean reward: 0.441 [0.412, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.394, 10.514], loss: 0.002742, mae: 0.056587, mean_q: 0.220596
 82400/100000: episode: 1310, duration: 0.171s, episode steps: 29, steps per second: 169, episode reward: 12.943, mean reward: 0.446 [0.310, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.224, 10.100], loss: 0.002728, mae: 0.054176, mean_q: 0.216031
 82425/100000: episode: 1311, duration: 0.150s, episode steps: 25, steps per second: 167, episode reward: 11.331, mean reward: 0.453 [0.375, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.045, 10.647], loss: 0.002726, mae: 0.056607, mean_q: 0.235738
 82449/100000: episode: 1312, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 9.176, mean reward: 0.382 [0.279, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.242, 10.444], loss: 0.003027, mae: 0.059626, mean_q: 0.229160
 82480/100000: episode: 1313, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 11.407, mean reward: 0.368 [0.182, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.307, 10.355], loss: 0.002492, mae: 0.053181, mean_q: 0.200848
 82509/100000: episode: 1314, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 8.467, mean reward: 0.292 [0.024, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.447, 10.100], loss: 0.002727, mae: 0.055744, mean_q: 0.228900
 82535/100000: episode: 1315, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 11.989, mean reward: 0.461 [0.279, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.214, 10.100], loss: 0.002824, mae: 0.056530, mean_q: 0.252305
 82559/100000: episode: 1316, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 7.780, mean reward: 0.324 [0.187, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.757, 10.284], loss: 0.002557, mae: 0.053302, mean_q: 0.223038
 82581/100000: episode: 1317, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 10.419, mean reward: 0.474 [0.397, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.142, 10.519], loss: 0.002751, mae: 0.055539, mean_q: 0.278826
 82603/100000: episode: 1318, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 10.015, mean reward: 0.455 [0.387, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.576, 10.495], loss: 0.003109, mae: 0.059997, mean_q: 0.261154
 82628/100000: episode: 1319, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 11.074, mean reward: 0.443 [0.342, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.108, 10.430], loss: 0.003057, mae: 0.059627, mean_q: 0.361352
 82650/100000: episode: 1320, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 11.455, mean reward: 0.521 [0.456, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.132, 10.658], loss: 0.002950, mae: 0.057541, mean_q: 0.272780
 82680/100000: episode: 1321, duration: 0.160s, episode steps: 30, steps per second: 188, episode reward: 10.108, mean reward: 0.337 [0.111, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.092, 10.227], loss: 0.002947, mae: 0.058475, mean_q: 0.289167
[Info] FALSIFICATION!
 82687/100000: episode: 1322, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 13.088, mean reward: 1.870 [0.451, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.707, 9.872], loss: 0.002850, mae: 0.056339, mean_q: 0.274328
 82787/100000: episode: 1323, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -12.609, mean reward: -0.126 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.364, 10.457], loss: 0.002726, mae: 0.055516, mean_q: 0.267608
 82887/100000: episode: 1324, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.785, mean reward: -0.168 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.461, 10.146], loss: 0.002721, mae: 0.055916, mean_q: 0.293435
 82987/100000: episode: 1325, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.865, mean reward: -0.199 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.081, 10.098], loss: 0.016894, mae: 0.067419, mean_q: 0.274656
 83087/100000: episode: 1326, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.794, mean reward: -0.158 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.552, 10.266], loss: 0.002624, mae: 0.054870, mean_q: 0.273481
 83187/100000: episode: 1327, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -15.669, mean reward: -0.157 [-1.000, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.628, 10.250], loss: 0.016817, mae: 0.067726, mean_q: 0.319705
 83287/100000: episode: 1328, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.564, mean reward: -0.166 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.648, 10.348], loss: 0.002698, mae: 0.056059, mean_q: 0.304707
 83387/100000: episode: 1329, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.674, mean reward: -0.157 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.091, 10.304], loss: 0.002637, mae: 0.055352, mean_q: 0.269568
 83487/100000: episode: 1330, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -18.235, mean reward: -0.182 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.417, 10.269], loss: 0.029907, mae: 0.070085, mean_q: 0.287365
 83587/100000: episode: 1331, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.169, mean reward: -0.192 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.445, 10.098], loss: 0.003180, mae: 0.061188, mean_q: 0.300893
 83687/100000: episode: 1332, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.275, mean reward: -0.153 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.023, 10.189], loss: 0.016539, mae: 0.065721, mean_q: 0.295790
 83787/100000: episode: 1333, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -15.833, mean reward: -0.158 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.560, 10.171], loss: 0.002734, mae: 0.056317, mean_q: 0.263551
 83887/100000: episode: 1334, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.455, mean reward: -0.185 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.095, 10.098], loss: 0.016679, mae: 0.066609, mean_q: 0.306429
 83987/100000: episode: 1335, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.748, mean reward: -0.147 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.837, 10.098], loss: 0.044371, mae: 0.083353, mean_q: 0.317790
 84087/100000: episode: 1336, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.496, mean reward: -0.185 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.283, 10.098], loss: 0.016467, mae: 0.065196, mean_q: 0.294598
 84187/100000: episode: 1337, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.278, mean reward: -0.193 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.718, 10.098], loss: 0.015920, mae: 0.059178, mean_q: 0.306620
 84287/100000: episode: 1338, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -14.839, mean reward: -0.148 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.923, 10.281], loss: 0.003300, mae: 0.060996, mean_q: 0.297528
 84387/100000: episode: 1339, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.528, mean reward: -0.165 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.656, 10.098], loss: 0.002868, mae: 0.056863, mean_q: 0.298138
 84487/100000: episode: 1340, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -15.436, mean reward: -0.154 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.053, 10.245], loss: 0.002531, mae: 0.053804, mean_q: 0.282783
 84587/100000: episode: 1341, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -13.962, mean reward: -0.140 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.255, 10.098], loss: 0.002672, mae: 0.054687, mean_q: 0.264305
 84687/100000: episode: 1342, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.032, mean reward: -0.190 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.615, 10.174], loss: 0.002534, mae: 0.052832, mean_q: 0.219897
 84787/100000: episode: 1343, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.638, mean reward: -0.186 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.573, 10.152], loss: 0.030731, mae: 0.074520, mean_q: 0.240300
 84887/100000: episode: 1344, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.766, mean reward: -0.178 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.432, 10.178], loss: 0.002761, mae: 0.056060, mean_q: 0.234158
 84987/100000: episode: 1345, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -12.215, mean reward: -0.122 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.832, 10.353], loss: 0.030019, mae: 0.071449, mean_q: 0.211709
 85087/100000: episode: 1346, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.818, mean reward: -0.158 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.776, 10.098], loss: 0.003798, mae: 0.061151, mean_q: 0.156942
 85187/100000: episode: 1347, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -20.341, mean reward: -0.203 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.367, 10.098], loss: 0.002496, mae: 0.052930, mean_q: 0.138060
 85287/100000: episode: 1348, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -20.220, mean reward: -0.202 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.069, 10.134], loss: 0.029363, mae: 0.067105, mean_q: 0.141771
 85387/100000: episode: 1349, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.430, mean reward: -0.174 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.692, 10.098], loss: 0.016697, mae: 0.064690, mean_q: 0.111103
 85487/100000: episode: 1350, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.791, mean reward: -0.168 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.588, 10.098], loss: 0.003596, mae: 0.060697, mean_q: 0.096989
 85587/100000: episode: 1351, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -11.090, mean reward: -0.111 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.941, 10.444], loss: 0.029980, mae: 0.069949, mean_q: 0.088644
 85687/100000: episode: 1352, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.667, mean reward: -0.167 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.076, 10.098], loss: 0.016252, mae: 0.060624, mean_q: 0.088774
 85787/100000: episode: 1353, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -12.813, mean reward: -0.128 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.933, 10.098], loss: 0.002571, mae: 0.051906, mean_q: 0.030025
 85887/100000: episode: 1354, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.478, mean reward: -0.185 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.854, 10.232], loss: 0.002434, mae: 0.050286, mean_q: 0.011399
 85987/100000: episode: 1355, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.843, mean reward: -0.178 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.904, 10.256], loss: 0.002845, mae: 0.054860, mean_q: -0.001186
 86087/100000: episode: 1356, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -11.326, mean reward: -0.113 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.861, 10.307], loss: 0.002711, mae: 0.053051, mean_q: 0.029939
 86187/100000: episode: 1357, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.616, mean reward: -0.176 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.835, 10.235], loss: 0.002495, mae: 0.051428, mean_q: -0.000852
 86287/100000: episode: 1358, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -17.677, mean reward: -0.177 [-1.000, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.436, 10.098], loss: 0.002321, mae: 0.049585, mean_q: -0.031128
 86387/100000: episode: 1359, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.912, mean reward: -0.189 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.827, 10.098], loss: 0.002346, mae: 0.050067, mean_q: -0.061008
 86487/100000: episode: 1360, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -15.346, mean reward: -0.153 [-1.000, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.377, 10.646], loss: 0.002336, mae: 0.048430, mean_q: -0.108030
 86587/100000: episode: 1361, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.698, mean reward: -0.197 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.558, 10.123], loss: 0.016224, mae: 0.059213, mean_q: -0.107569
 86687/100000: episode: 1362, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.871, mean reward: -0.179 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.734, 10.098], loss: 0.002380, mae: 0.049148, mean_q: -0.123343
 86787/100000: episode: 1363, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.244, mean reward: -0.152 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.769, 10.098], loss: 0.002194, mae: 0.046720, mean_q: -0.194133
 86887/100000: episode: 1364, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.333, mean reward: -0.163 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.087, 10.188], loss: 0.030812, mae: 0.071821, mean_q: -0.130995
 86987/100000: episode: 1365, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -15.123, mean reward: -0.151 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.890, 10.451], loss: 0.016524, mae: 0.061216, mean_q: -0.144109
 87087/100000: episode: 1366, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -20.143, mean reward: -0.201 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.245, 10.098], loss: 0.016232, mae: 0.059869, mean_q: -0.160459
 87187/100000: episode: 1367, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.530, mean reward: -0.185 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.560, 10.098], loss: 0.002504, mae: 0.049991, mean_q: -0.208928
 87287/100000: episode: 1368, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.942, mean reward: -0.189 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.117, 10.110], loss: 0.002347, mae: 0.047148, mean_q: -0.266735
 87387/100000: episode: 1369, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.365, mean reward: -0.184 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.281, 10.157], loss: 0.002284, mae: 0.047044, mean_q: -0.301507
 87487/100000: episode: 1370, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -16.069, mean reward: -0.161 [-1.000, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.573, 10.098], loss: 0.004043, mae: 0.056324, mean_q: -0.300329
 87587/100000: episode: 1371, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.386, mean reward: -0.174 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.167, 10.279], loss: 0.003565, mae: 0.058721, mean_q: -0.282032
 87687/100000: episode: 1372, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -16.337, mean reward: -0.163 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.086, 10.098], loss: 0.016200, mae: 0.056338, mean_q: -0.318139
 87787/100000: episode: 1373, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -14.407, mean reward: -0.144 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.412, 10.255], loss: 0.002321, mae: 0.046922, mean_q: -0.325491
 87887/100000: episode: 1374, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -18.447, mean reward: -0.184 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.763, 10.098], loss: 0.002291, mae: 0.046430, mean_q: -0.331832
 87987/100000: episode: 1375, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -14.749, mean reward: -0.147 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.038, 10.188], loss: 0.002245, mae: 0.045373, mean_q: -0.322707
 88087/100000: episode: 1376, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.881, mean reward: -0.169 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.981, 10.098], loss: 0.002271, mae: 0.045402, mean_q: -0.335603
 88187/100000: episode: 1377, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.155, mean reward: -0.192 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.575, 10.098], loss: 0.002170, mae: 0.044579, mean_q: -0.341099
 88287/100000: episode: 1378, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -12.819, mean reward: -0.128 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.250, 10.098], loss: 0.002453, mae: 0.046679, mean_q: -0.334664
 88387/100000: episode: 1379, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.978, mean reward: -0.170 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.554, 10.098], loss: 0.002423, mae: 0.047787, mean_q: -0.323587
 88487/100000: episode: 1380, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.626, mean reward: -0.186 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.582, 10.214], loss: 0.002128, mae: 0.044825, mean_q: -0.313809
 88587/100000: episode: 1381, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.357, mean reward: -0.194 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.274, 10.098], loss: 0.002159, mae: 0.045057, mean_q: -0.342002
 88687/100000: episode: 1382, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.333, mean reward: -0.183 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.261, 10.098], loss: 0.002324, mae: 0.047025, mean_q: -0.273716
 88787/100000: episode: 1383, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.229, mean reward: -0.182 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.568, 10.124], loss: 0.002307, mae: 0.046761, mean_q: -0.307726
 88887/100000: episode: 1384, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.596, mean reward: -0.186 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.424, 10.098], loss: 0.002160, mae: 0.045631, mean_q: -0.338723
 88987/100000: episode: 1385, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.314, mean reward: -0.183 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.374, 10.098], loss: 0.002233, mae: 0.046164, mean_q: -0.339843
 89087/100000: episode: 1386, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -14.580, mean reward: -0.146 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.702, 10.098], loss: 0.002167, mae: 0.045193, mean_q: -0.298968
 89187/100000: episode: 1387, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.928, mean reward: -0.179 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.657, 10.098], loss: 0.001995, mae: 0.043923, mean_q: -0.354912
 89287/100000: episode: 1388, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -20.789, mean reward: -0.208 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.027, 10.149], loss: 0.002243, mae: 0.046125, mean_q: -0.329838
 89387/100000: episode: 1389, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.289, mean reward: -0.143 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.288, 10.098], loss: 0.002321, mae: 0.046952, mean_q: -0.320001
 89487/100000: episode: 1390, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.938, mean reward: -0.189 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.828, 10.098], loss: 0.002271, mae: 0.046338, mean_q: -0.329718
 89587/100000: episode: 1391, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.668, mean reward: -0.177 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.208, 10.098], loss: 0.002178, mae: 0.046229, mean_q: -0.274785
 89687/100000: episode: 1392, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -20.646, mean reward: -0.206 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.757, 10.197], loss: 0.002248, mae: 0.045603, mean_q: -0.350969
 89787/100000: episode: 1393, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.488, mean reward: -0.175 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.621, 10.347], loss: 0.002256, mae: 0.046278, mean_q: -0.333965
 89887/100000: episode: 1394, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.060, mean reward: -0.191 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.316, 10.098], loss: 0.002415, mae: 0.049841, mean_q: -0.325254
 89987/100000: episode: 1395, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.995, mean reward: -0.190 [-1.000, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.825, 10.206], loss: 0.002135, mae: 0.045426, mean_q: -0.337449
 90087/100000: episode: 1396, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -14.912, mean reward: -0.149 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.114, 10.201], loss: 0.002300, mae: 0.046357, mean_q: -0.335541
 90187/100000: episode: 1397, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.398, mean reward: -0.164 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.884, 10.098], loss: 0.002267, mae: 0.046425, mean_q: -0.309132
 90287/100000: episode: 1398, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -18.690, mean reward: -0.187 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.695, 10.168], loss: 0.002505, mae: 0.048371, mean_q: -0.325969
 90387/100000: episode: 1399, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.499, mean reward: -0.185 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.937, 10.128], loss: 0.002281, mae: 0.047383, mean_q: -0.345180
 90487/100000: episode: 1400, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.197, mean reward: -0.172 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.278, 10.251], loss: 0.002295, mae: 0.046408, mean_q: -0.332741
 90587/100000: episode: 1401, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.538, mean reward: -0.165 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.951, 10.262], loss: 0.002219, mae: 0.045948, mean_q: -0.324316
 90687/100000: episode: 1402, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.156, mean reward: -0.182 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.487, 10.117], loss: 0.002326, mae: 0.047160, mean_q: -0.318302
 90787/100000: episode: 1403, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.216, mean reward: -0.172 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.500, 10.232], loss: 0.002328, mae: 0.047424, mean_q: -0.297997
 90887/100000: episode: 1404, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.747, mean reward: -0.187 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.811, 10.157], loss: 0.002296, mae: 0.045992, mean_q: -0.351696
 90987/100000: episode: 1405, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -15.332, mean reward: -0.153 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.855, 10.098], loss: 0.002423, mae: 0.046408, mean_q: -0.328840
 91087/100000: episode: 1406, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.344, mean reward: -0.183 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.547, 10.208], loss: 0.002379, mae: 0.047092, mean_q: -0.347647
 91187/100000: episode: 1407, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -15.028, mean reward: -0.150 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.419, 10.098], loss: 0.002438, mae: 0.048734, mean_q: -0.324129
 91287/100000: episode: 1408, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.841, mean reward: -0.178 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.744, 10.156], loss: 0.002501, mae: 0.050774, mean_q: -0.330429
 91387/100000: episode: 1409, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -20.248, mean reward: -0.202 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.451, 10.287], loss: 0.002383, mae: 0.047721, mean_q: -0.327780
 91487/100000: episode: 1410, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.924, mean reward: -0.159 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.257, 10.254], loss: 0.002305, mae: 0.047211, mean_q: -0.342029
 91587/100000: episode: 1411, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.304, mean reward: -0.193 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.453, 10.098], loss: 0.002476, mae: 0.048611, mean_q: -0.325904
 91687/100000: episode: 1412, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.866, mean reward: -0.189 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.571, 10.101], loss: 0.002366, mae: 0.047458, mean_q: -0.332056
 91787/100000: episode: 1413, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -14.331, mean reward: -0.143 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.660, 10.098], loss: 0.002352, mae: 0.047390, mean_q: -0.317694
 91887/100000: episode: 1414, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -11.629, mean reward: -0.116 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.164, 10.098], loss: 0.002436, mae: 0.048615, mean_q: -0.272286
 91987/100000: episode: 1415, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -19.240, mean reward: -0.192 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.900, 10.288], loss: 0.002343, mae: 0.047206, mean_q: -0.317343
 92087/100000: episode: 1416, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.427, mean reward: -0.164 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.816, 10.098], loss: 0.002273, mae: 0.046875, mean_q: -0.339327
 92187/100000: episode: 1417, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -14.019, mean reward: -0.140 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.386, 10.179], loss: 0.002197, mae: 0.045065, mean_q: -0.356266
 92287/100000: episode: 1418, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -12.520, mean reward: -0.125 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.980, 10.389], loss: 0.002458, mae: 0.048978, mean_q: -0.310554
 92387/100000: episode: 1419, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -13.838, mean reward: -0.138 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.609, 10.350], loss: 0.002405, mae: 0.047226, mean_q: -0.321379
 92487/100000: episode: 1420, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.426, mean reward: -0.164 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.737, 10.098], loss: 0.002340, mae: 0.047217, mean_q: -0.312408
 92587/100000: episode: 1421, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.403, mean reward: -0.174 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.447, 10.203], loss: 0.002313, mae: 0.048081, mean_q: -0.305467
[Info] 100-TH LEVEL FOUND: 0.5741925835609436, Considering 10/90 traces
 92687/100000: episode: 1422, duration: 4.381s, episode steps: 100, steps per second: 23, episode reward: -16.797, mean reward: -0.168 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.924, 10.310], loss: 0.002295, mae: 0.047701, mean_q: -0.310010
 92707/100000: episode: 1423, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 6.556, mean reward: 0.328 [0.253, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-1.283, 10.100], loss: 0.002594, mae: 0.050786, mean_q: -0.242260
 92737/100000: episode: 1424, duration: 0.145s, episode steps: 30, steps per second: 206, episode reward: 5.014, mean reward: 0.167 [0.022, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.840, 10.100], loss: 0.002425, mae: 0.047769, mean_q: -0.335755
 92792/100000: episode: 1425, duration: 0.317s, episode steps: 55, steps per second: 174, episode reward: 21.978, mean reward: 0.400 [0.287, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.853 [-0.849, 10.100], loss: 0.002512, mae: 0.051365, mean_q: -0.273241
 92847/100000: episode: 1426, duration: 0.280s, episode steps: 55, steps per second: 196, episode reward: 17.183, mean reward: 0.312 [0.175, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-0.296, 10.100], loss: 0.002350, mae: 0.047389, mean_q: -0.288277
 92895/100000: episode: 1427, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 11.930, mean reward: 0.249 [0.073, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.281, 10.301], loss: 0.002578, mae: 0.051183, mean_q: -0.255617
 92915/100000: episode: 1428, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 5.577, mean reward: 0.279 [0.211, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.350, 10.100], loss: 0.002423, mae: 0.049769, mean_q: -0.297759
 92963/100000: episode: 1429, duration: 0.255s, episode steps: 48, steps per second: 188, episode reward: 12.979, mean reward: 0.270 [0.056, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.458, 10.172], loss: 0.002329, mae: 0.048189, mean_q: -0.263342
 93018/100000: episode: 1430, duration: 0.297s, episode steps: 55, steps per second: 185, episode reward: 19.641, mean reward: 0.357 [0.123, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.853 [-1.808, 10.100], loss: 0.002618, mae: 0.050099, mean_q: -0.252037
 93062/100000: episode: 1431, duration: 0.237s, episode steps: 44, steps per second: 185, episode reward: 10.831, mean reward: 0.246 [0.109, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.794, 10.326], loss: 0.002216, mae: 0.047235, mean_q: -0.237642
 93086/100000: episode: 1432, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 7.688, mean reward: 0.320 [0.178, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.244, 10.343], loss: 0.002387, mae: 0.049272, mean_q: -0.191365
 93113/100000: episode: 1433, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 10.680, mean reward: 0.396 [0.293, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.483, 10.494], loss: 0.002435, mae: 0.048337, mean_q: -0.230505
 93161/100000: episode: 1434, duration: 0.247s, episode steps: 48, steps per second: 194, episode reward: 11.101, mean reward: 0.231 [0.114, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.332, 10.202], loss: 0.002588, mae: 0.051738, mean_q: -0.217136
 93185/100000: episode: 1435, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 9.059, mean reward: 0.377 [0.301, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.243, 10.444], loss: 0.002170, mae: 0.046794, mean_q: -0.229533
 93205/100000: episode: 1436, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 7.551, mean reward: 0.378 [0.211, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.441, 10.100], loss: 0.002492, mae: 0.048589, mean_q: -0.223228
 93235/100000: episode: 1437, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 9.502, mean reward: 0.317 [0.123, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.035, 10.252], loss: 0.002604, mae: 0.051350, mean_q: -0.170238
 93247/100000: episode: 1438, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 3.361, mean reward: 0.280 [0.213, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.057, 10.341], loss: 0.002517, mae: 0.053318, mean_q: -0.212610
 93267/100000: episode: 1439, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 6.015, mean reward: 0.301 [0.128, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.137, 10.100], loss: 0.002610, mae: 0.052183, mean_q: -0.232631
 93311/100000: episode: 1440, duration: 0.235s, episode steps: 44, steps per second: 187, episode reward: 9.809, mean reward: 0.223 [0.029, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.146, 10.136], loss: 0.002402, mae: 0.050043, mean_q: -0.203953
 93355/100000: episode: 1441, duration: 0.220s, episode steps: 44, steps per second: 200, episode reward: 6.923, mean reward: 0.157 [0.046, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.703, 10.201], loss: 0.002659, mae: 0.050274, mean_q: -0.205643
 93376/100000: episode: 1442, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 4.917, mean reward: 0.234 [0.154, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.136, 10.100], loss: 0.002267, mae: 0.047205, mean_q: -0.256957
 93388/100000: episode: 1443, duration: 0.068s, episode steps: 12, steps per second: 175, episode reward: 4.130, mean reward: 0.344 [0.247, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.164, 10.414], loss: 0.002456, mae: 0.050388, mean_q: -0.289553
 93400/100000: episode: 1444, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 3.696, mean reward: 0.308 [0.252, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.417], loss: 0.002135, mae: 0.046618, mean_q: -0.199676
 93455/100000: episode: 1445, duration: 0.287s, episode steps: 55, steps per second: 192, episode reward: 16.735, mean reward: 0.304 [0.114, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.707, 10.100], loss: 0.002533, mae: 0.051583, mean_q: -0.168334
 93480/100000: episode: 1446, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 7.408, mean reward: 0.296 [0.215, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.145, 10.100], loss: 0.002446, mae: 0.048242, mean_q: -0.167830
 93505/100000: episode: 1447, duration: 0.142s, episode steps: 25, steps per second: 177, episode reward: 7.284, mean reward: 0.291 [0.157, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.573, 10.100], loss: 0.002426, mae: 0.048431, mean_q: -0.147699
 93532/100000: episode: 1448, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 6.968, mean reward: 0.258 [0.172, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.047, 10.305], loss: 0.002903, mae: 0.056598, mean_q: -0.134770
 93552/100000: episode: 1449, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 7.257, mean reward: 0.363 [0.257, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.567, 10.100], loss: 0.002610, mae: 0.052873, mean_q: -0.135004
 93573/100000: episode: 1450, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 5.489, mean reward: 0.261 [0.155, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.859, 10.100], loss: 0.002350, mae: 0.050043, mean_q: -0.130226
 93617/100000: episode: 1451, duration: 0.241s, episode steps: 44, steps per second: 182, episode reward: 12.431, mean reward: 0.283 [0.093, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-1.018, 10.395], loss: 0.002415, mae: 0.049734, mean_q: -0.167132
 93661/100000: episode: 1452, duration: 0.239s, episode steps: 44, steps per second: 184, episode reward: 9.854, mean reward: 0.224 [0.058, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.402, 10.238], loss: 0.002211, mae: 0.047506, mean_q: -0.150583
 93688/100000: episode: 1453, duration: 0.164s, episode steps: 27, steps per second: 165, episode reward: 7.896, mean reward: 0.292 [0.076, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.389, 10.194], loss: 0.002301, mae: 0.048224, mean_q: -0.216027
 93700/100000: episode: 1454, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 5.264, mean reward: 0.439 [0.344, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.491], loss: 0.002840, mae: 0.055590, mean_q: -0.098031
 93712/100000: episode: 1455, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 3.718, mean reward: 0.310 [0.245, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.452], loss: 0.002593, mae: 0.052039, mean_q: -0.108063
 93756/100000: episode: 1456, duration: 0.239s, episode steps: 44, steps per second: 184, episode reward: 12.371, mean reward: 0.281 [0.107, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-1.367, 10.392], loss: 0.002662, mae: 0.051839, mean_q: -0.132608
 93781/100000: episode: 1457, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 4.690, mean reward: 0.188 [0.028, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.138, 10.100], loss: 0.002485, mae: 0.050561, mean_q: -0.154434
 93811/100000: episode: 1458, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 9.840, mean reward: 0.328 [0.190, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.083, 10.319], loss: 0.002679, mae: 0.052163, mean_q: -0.146346
 93838/100000: episode: 1459, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 6.543, mean reward: 0.242 [0.077, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.053, 10.227], loss: 0.002806, mae: 0.053326, mean_q: -0.112079
 93893/100000: episode: 1460, duration: 0.272s, episode steps: 55, steps per second: 202, episode reward: 13.477, mean reward: 0.245 [0.084, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-1.143, 10.100], loss: 0.002390, mae: 0.049793, mean_q: -0.081847
 93905/100000: episode: 1461, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 4.137, mean reward: 0.345 [0.239, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.833, 10.454], loss: 0.002751, mae: 0.055316, mean_q: -0.102210
 93925/100000: episode: 1462, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 7.092, mean reward: 0.355 [0.244, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.313, 10.100], loss: 0.002443, mae: 0.051140, mean_q: -0.127223
 93937/100000: episode: 1463, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 4.569, mean reward: 0.381 [0.318, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.516], loss: 0.002930, mae: 0.054132, mean_q: -0.055829
 93949/100000: episode: 1464, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 3.905, mean reward: 0.325 [0.208, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.281], loss: 0.002258, mae: 0.049245, mean_q: -0.124422
 93969/100000: episode: 1465, duration: 0.111s, episode steps: 20, steps per second: 179, episode reward: 7.860, mean reward: 0.393 [0.300, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.285, 10.100], loss: 0.002574, mae: 0.052346, mean_q: -0.038150
 93981/100000: episode: 1466, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 3.938, mean reward: 0.328 [0.243, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.392], loss: 0.002952, mae: 0.054188, mean_q: -0.091425
 94029/100000: episode: 1467, duration: 0.239s, episode steps: 48, steps per second: 201, episode reward: 15.105, mean reward: 0.315 [0.176, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.306, 10.473], loss: 0.002661, mae: 0.054098, mean_q: -0.071450
 94073/100000: episode: 1468, duration: 0.241s, episode steps: 44, steps per second: 182, episode reward: 8.578, mean reward: 0.195 [0.062, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-1.028, 10.310], loss: 0.002574, mae: 0.053460, mean_q: -0.058607
 94098/100000: episode: 1469, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 9.291, mean reward: 0.372 [0.183, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.349, 10.100], loss: 0.002598, mae: 0.055471, mean_q: -0.077072
 94110/100000: episode: 1470, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 4.560, mean reward: 0.380 [0.291, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.433, 10.347], loss: 0.002283, mae: 0.051980, mean_q: 0.023096
 94122/100000: episode: 1471, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 4.630, mean reward: 0.386 [0.291, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.462], loss: 0.002590, mae: 0.051854, mean_q: -0.015819
 94166/100000: episode: 1472, duration: 0.257s, episode steps: 44, steps per second: 171, episode reward: 12.784, mean reward: 0.291 [0.107, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.143, 10.321], loss: 0.002698, mae: 0.053211, mean_q: -0.082707
 94178/100000: episode: 1473, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 4.165, mean reward: 0.347 [0.303, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.602, 10.470], loss: 0.003103, mae: 0.058222, mean_q: -0.076475
 94190/100000: episode: 1474, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 4.128, mean reward: 0.344 [0.240, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.554, 10.410], loss: 0.002432, mae: 0.050291, mean_q: -0.017561
 94217/100000: episode: 1475, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 11.633, mean reward: 0.431 [0.210, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.566, 10.464], loss: 0.002796, mae: 0.056988, mean_q: -0.005448
 94242/100000: episode: 1476, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 6.275, mean reward: 0.251 [0.128, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.424, 10.100], loss: 0.002473, mae: 0.052532, mean_q: -0.072162
 94262/100000: episode: 1477, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 6.006, mean reward: 0.300 [0.217, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.354, 10.100], loss: 0.008363, mae: 0.077403, mean_q: -0.055495
 94310/100000: episode: 1478, duration: 0.257s, episode steps: 48, steps per second: 187, episode reward: 15.261, mean reward: 0.318 [0.157, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.283, 10.353], loss: 0.004215, mae: 0.067347, mean_q: -0.023820
 94358/100000: episode: 1479, duration: 0.241s, episode steps: 48, steps per second: 199, episode reward: 14.455, mean reward: 0.301 [0.189, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.286, 10.385], loss: 0.002775, mae: 0.055837, mean_q: -0.012245
 94383/100000: episode: 1480, duration: 0.135s, episode steps: 25, steps per second: 186, episode reward: 6.467, mean reward: 0.259 [0.099, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.413, 10.100], loss: 0.002678, mae: 0.055730, mean_q: -0.024042
 94431/100000: episode: 1481, duration: 0.252s, episode steps: 48, steps per second: 190, episode reward: 10.331, mean reward: 0.215 [0.066, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-1.257, 10.100], loss: 0.002213, mae: 0.048309, mean_q: -0.032073
 94461/100000: episode: 1482, duration: 0.140s, episode steps: 30, steps per second: 215, episode reward: 8.066, mean reward: 0.269 [0.129, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.067, 10.318], loss: 0.002514, mae: 0.052206, mean_q: 0.048493
 94481/100000: episode: 1483, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 6.696, mean reward: 0.335 [0.222, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.227, 10.100], loss: 0.002587, mae: 0.054085, mean_q: 0.055572
 94506/100000: episode: 1484, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 5.479, mean reward: 0.219 [0.094, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.309, 10.100], loss: 0.002558, mae: 0.052397, mean_q: 0.025857
 94561/100000: episode: 1485, duration: 0.297s, episode steps: 55, steps per second: 185, episode reward: 19.542, mean reward: 0.355 [0.238, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.846 [-1.210, 10.100], loss: 0.002688, mae: 0.053744, mean_q: 0.005336
 94582/100000: episode: 1486, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 9.472, mean reward: 0.451 [0.270, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.158, 10.100], loss: 0.002500, mae: 0.050268, mean_q: -0.074329
 94602/100000: episode: 1487, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 5.925, mean reward: 0.296 [0.142, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.254, 10.100], loss: 0.002380, mae: 0.050984, mean_q: 0.012213
 94622/100000: episode: 1488, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 7.060, mean reward: 0.353 [0.289, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.274, 10.100], loss: 0.002431, mae: 0.052071, mean_q: 0.038394
 94677/100000: episode: 1489, duration: 0.282s, episode steps: 55, steps per second: 195, episode reward: 18.462, mean reward: 0.336 [0.225, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.567, 10.100], loss: 0.002920, mae: 0.056761, mean_q: 0.034436
 94707/100000: episode: 1490, duration: 0.151s, episode steps: 30, steps per second: 199, episode reward: 8.083, mean reward: 0.269 [0.170, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.375, 10.387], loss: 0.002718, mae: 0.053774, mean_q: 0.057336
 94751/100000: episode: 1491, duration: 0.239s, episode steps: 44, steps per second: 184, episode reward: 11.510, mean reward: 0.262 [0.135, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.833, 10.297], loss: 0.002683, mae: 0.055044, mean_q: 0.085610
 94763/100000: episode: 1492, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 3.459, mean reward: 0.288 [0.222, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.548, 10.363], loss: 0.002174, mae: 0.049493, mean_q: 0.079734
 94790/100000: episode: 1493, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 8.356, mean reward: 0.309 [0.049, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.172, 10.207], loss: 0.002735, mae: 0.055126, mean_q: 0.057832
 94802/100000: episode: 1494, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 3.766, mean reward: 0.314 [0.232, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.458], loss: 0.002710, mae: 0.053425, mean_q: 0.029097
 94829/100000: episode: 1495, duration: 0.142s, episode steps: 27, steps per second: 191, episode reward: 11.120, mean reward: 0.412 [0.332, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.494], loss: 0.003128, mae: 0.059120, mean_q: 0.121089
 94849/100000: episode: 1496, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 6.699, mean reward: 0.335 [0.240, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.115, 10.100], loss: 0.002632, mae: 0.054240, mean_q: 0.060015
 94869/100000: episode: 1497, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 6.586, mean reward: 0.329 [0.261, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.412, 10.100], loss: 0.002402, mae: 0.051129, mean_q: 0.082040
 94889/100000: episode: 1498, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 6.860, mean reward: 0.343 [0.237, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.425, 10.100], loss: 0.002532, mae: 0.053281, mean_q: 0.137102
 94944/100000: episode: 1499, duration: 0.300s, episode steps: 55, steps per second: 183, episode reward: 19.316, mean reward: 0.351 [0.219, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-0.250, 10.100], loss: 0.002488, mae: 0.052924, mean_q: 0.094786
 94968/100000: episode: 1500, duration: 0.130s, episode steps: 24, steps per second: 184, episode reward: 8.206, mean reward: 0.342 [0.237, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.949, 10.408], loss: 0.002696, mae: 0.056196, mean_q: 0.064423
 95016/100000: episode: 1501, duration: 0.250s, episode steps: 48, steps per second: 192, episode reward: 19.903, mean reward: 0.415 [0.318, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.539, 10.415], loss: 0.002453, mae: 0.052888, mean_q: 0.104252
 95046/100000: episode: 1502, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 7.816, mean reward: 0.261 [0.082, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-1.339, 10.292], loss: 0.002574, mae: 0.054089, mean_q: 0.114952
 95070/100000: episode: 1503, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 7.984, mean reward: 0.333 [0.261, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.350], loss: 0.002526, mae: 0.053972, mean_q: 0.177271
 95125/100000: episode: 1504, duration: 0.288s, episode steps: 55, steps per second: 191, episode reward: 18.721, mean reward: 0.340 [0.064, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-0.863, 10.172], loss: 0.002379, mae: 0.052255, mean_q: 0.124310
 95155/100000: episode: 1505, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 7.948, mean reward: 0.265 [0.129, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.388, 10.311], loss: 0.002396, mae: 0.051882, mean_q: 0.128814
 95167/100000: episode: 1506, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 4.048, mean reward: 0.337 [0.252, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.417], loss: 0.002565, mae: 0.052813, mean_q: 0.086468
 95191/100000: episode: 1507, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 7.592, mean reward: 0.316 [0.221, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.330], loss: 0.002426, mae: 0.052008, mean_q: 0.142716
 95216/100000: episode: 1508, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 7.864, mean reward: 0.315 [0.203, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.323, 10.100], loss: 0.002613, mae: 0.055781, mean_q: 0.184921
 95264/100000: episode: 1509, duration: 0.242s, episode steps: 48, steps per second: 198, episode reward: 12.964, mean reward: 0.270 [0.175, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.323, 10.351], loss: 0.002612, mae: 0.054547, mean_q: 0.132094
 95312/100000: episode: 1510, duration: 0.261s, episode steps: 48, steps per second: 184, episode reward: 14.897, mean reward: 0.310 [0.082, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.293, 10.119], loss: 0.002656, mae: 0.055514, mean_q: 0.208679
 95324/100000: episode: 1511, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 3.639, mean reward: 0.303 [0.258, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.439, 10.363], loss: 0.002234, mae: 0.051511, mean_q: 0.167734
[Info] 200-TH LEVEL FOUND: 0.8621746897697449, Considering 10/90 traces
 95348/100000: episode: 1512, duration: 3.960s, episode steps: 24, steps per second: 6, episode reward: 7.655, mean reward: 0.319 [0.205, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.309, 10.356], loss: 0.002648, mae: 0.054795, mean_q: 0.180394
 95365/100000: episode: 1513, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 4.606, mean reward: 0.271 [0.105, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.441], loss: 0.002218, mae: 0.049661, mean_q: 0.151452
 95411/100000: episode: 1514, duration: 0.249s, episode steps: 46, steps per second: 185, episode reward: 16.066, mean reward: 0.349 [0.209, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.516, 10.100], loss: 0.002407, mae: 0.051763, mean_q: 0.191541
 95418/100000: episode: 1515, duration: 0.036s, episode steps: 7, steps per second: 197, episode reward: 3.576, mean reward: 0.511 [0.466, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-1.177, 10.100], loss: 0.002272, mae: 0.049266, mean_q: 0.079134
 95425/100000: episode: 1516, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 2.999, mean reward: 0.428 [0.359, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.385, 10.100], loss: 0.002998, mae: 0.058808, mean_q: 0.164154
 95470/100000: episode: 1517, duration: 0.265s, episode steps: 45, steps per second: 170, episode reward: 13.945, mean reward: 0.310 [0.101, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.353, 10.100], loss: 0.002809, mae: 0.057767, mean_q: 0.205954
 95515/100000: episode: 1518, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 8.688, mean reward: 0.193 [0.031, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-1.056, 10.100], loss: 0.002759, mae: 0.056645, mean_q: 0.190501
 95563/100000: episode: 1519, duration: 0.250s, episode steps: 48, steps per second: 192, episode reward: 18.381, mean reward: 0.383 [0.263, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.517, 10.100], loss: 0.002551, mae: 0.055347, mean_q: 0.232847
 95612/100000: episode: 1520, duration: 0.248s, episode steps: 49, steps per second: 198, episode reward: 13.455, mean reward: 0.275 [0.019, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.357, 10.116], loss: 0.002753, mae: 0.056948, mean_q: 0.206639
 95630/100000: episode: 1521, duration: 0.114s, episode steps: 18, steps per second: 158, episode reward: 7.327, mean reward: 0.407 [0.357, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.505, 10.554], loss: 0.003005, mae: 0.058510, mean_q: 0.202517
 95637/100000: episode: 1522, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 2.967, mean reward: 0.424 [0.402, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.487, 10.100], loss: 0.002335, mae: 0.052799, mean_q: 0.234525
 95685/100000: episode: 1523, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 15.210, mean reward: 0.317 [0.080, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.872, 10.100], loss: 0.002535, mae: 0.054130, mean_q: 0.238175
 95730/100000: episode: 1524, duration: 0.226s, episode steps: 45, steps per second: 199, episode reward: 8.932, mean reward: 0.198 [0.032, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.437, 10.377], loss: 0.002613, mae: 0.054940, mean_q: 0.265678
 95775/100000: episode: 1525, duration: 0.253s, episode steps: 45, steps per second: 178, episode reward: 8.982, mean reward: 0.200 [0.021, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.109, 10.177], loss: 0.002564, mae: 0.054151, mean_q: 0.246116
 95820/100000: episode: 1526, duration: 0.248s, episode steps: 45, steps per second: 182, episode reward: 11.795, mean reward: 0.262 [0.075, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.903, 10.100], loss: 0.002757, mae: 0.057149, mean_q: 0.264243
 95865/100000: episode: 1527, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: 10.178, mean reward: 0.226 [0.050, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.389, 10.100], loss: 0.002661, mae: 0.056659, mean_q: 0.259089
 95913/100000: episode: 1528, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 18.976, mean reward: 0.395 [0.231, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.238, 10.100], loss: 0.002548, mae: 0.054740, mean_q: 0.282036
 95959/100000: episode: 1529, duration: 0.244s, episode steps: 46, steps per second: 188, episode reward: 10.121, mean reward: 0.220 [0.029, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.586, 10.100], loss: 0.002613, mae: 0.055121, mean_q: 0.295130
 95977/100000: episode: 1530, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 6.450, mean reward: 0.358 [0.265, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.374, 10.428], loss: 0.002443, mae: 0.052382, mean_q: 0.287815
 95988/100000: episode: 1531, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 4.498, mean reward: 0.409 [0.357, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.353, 10.100], loss: 0.003098, mae: 0.062435, mean_q: 0.335534
 96033/100000: episode: 1532, duration: 0.240s, episode steps: 45, steps per second: 187, episode reward: 14.875, mean reward: 0.331 [0.202, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.632, 10.100], loss: 0.002730, mae: 0.056505, mean_q: 0.287321
 96081/100000: episode: 1533, duration: 0.263s, episode steps: 48, steps per second: 183, episode reward: 23.971, mean reward: 0.499 [0.383, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.245, 10.100], loss: 0.002673, mae: 0.056245, mean_q: 0.310937
 96092/100000: episode: 1534, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 4.800, mean reward: 0.436 [0.332, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.417, 10.100], loss: 0.002698, mae: 0.055277, mean_q: 0.328473
 96109/100000: episode: 1535, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 7.075, mean reward: 0.416 [0.372, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.453], loss: 0.002522, mae: 0.054868, mean_q: 0.303672
 96126/100000: episode: 1536, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 8.126, mean reward: 0.478 [0.417, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.607], loss: 0.002708, mae: 0.057874, mean_q: 0.322285
 96171/100000: episode: 1537, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 17.187, mean reward: 0.382 [0.232, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.430, 10.100], loss: 0.002615, mae: 0.055693, mean_q: 0.339084
 96189/100000: episode: 1538, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 8.488, mean reward: 0.472 [0.397, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-1.441, 10.546], loss: 0.002679, mae: 0.055855, mean_q: 0.304788
 96238/100000: episode: 1539, duration: 0.271s, episode steps: 49, steps per second: 181, episode reward: 17.807, mean reward: 0.363 [0.091, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.440, 10.100], loss: 0.002711, mae: 0.056709, mean_q: 0.335614
 96283/100000: episode: 1540, duration: 0.239s, episode steps: 45, steps per second: 188, episode reward: 9.330, mean reward: 0.207 [0.008, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-1.008, 10.100], loss: 0.002407, mae: 0.052656, mean_q: 0.356109
 96328/100000: episode: 1541, duration: 0.223s, episode steps: 45, steps per second: 202, episode reward: 14.175, mean reward: 0.315 [0.026, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.035, 10.150], loss: 0.002638, mae: 0.056094, mean_q: 0.370342
 96374/100000: episode: 1542, duration: 0.219s, episode steps: 46, steps per second: 210, episode reward: 13.080, mean reward: 0.284 [0.087, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.807, 10.100], loss: 0.002935, mae: 0.059918, mean_q: 0.412688
 96422/100000: episode: 1543, duration: 0.254s, episode steps: 48, steps per second: 189, episode reward: 13.157, mean reward: 0.274 [0.026, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.367, 10.141], loss: 0.002582, mae: 0.056226, mean_q: 0.367697
 96467/100000: episode: 1544, duration: 0.242s, episode steps: 45, steps per second: 186, episode reward: 10.373, mean reward: 0.231 [0.051, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.284, 10.329], loss: 0.002697, mae: 0.057345, mean_q: 0.401118
 96484/100000: episode: 1545, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 6.814, mean reward: 0.401 [0.162, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.401, 10.370], loss: 0.002440, mae: 0.053696, mean_q: 0.411412
 96531/100000: episode: 1546, duration: 0.235s, episode steps: 47, steps per second: 200, episode reward: 14.682, mean reward: 0.312 [0.042, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.663, 10.121], loss: 0.002738, mae: 0.057168, mean_q: 0.404675
 96579/100000: episode: 1547, duration: 0.258s, episode steps: 48, steps per second: 186, episode reward: 23.062, mean reward: 0.480 [0.340, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-1.017, 10.100], loss: 0.002470, mae: 0.054028, mean_q: 0.377042
 96625/100000: episode: 1548, duration: 0.247s, episode steps: 46, steps per second: 186, episode reward: 13.478, mean reward: 0.293 [0.125, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.658, 10.100], loss: 0.002450, mae: 0.054421, mean_q: 0.405081
 96672/100000: episode: 1549, duration: 0.263s, episode steps: 47, steps per second: 179, episode reward: 11.363, mean reward: 0.242 [0.013, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-1.525, 10.129], loss: 0.002635, mae: 0.056200, mean_q: 0.426029
 96717/100000: episode: 1550, duration: 0.254s, episode steps: 45, steps per second: 177, episode reward: 8.584, mean reward: 0.191 [0.030, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.334, 10.171], loss: 0.002531, mae: 0.054520, mean_q: 0.436930
 96724/100000: episode: 1551, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 2.924, mean reward: 0.418 [0.336, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.406, 10.100], loss: 0.002540, mae: 0.055715, mean_q: 0.459061
 96773/100000: episode: 1552, duration: 0.244s, episode steps: 49, steps per second: 201, episode reward: 17.799, mean reward: 0.363 [0.283, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.829, 10.100], loss: 0.002699, mae: 0.057481, mean_q: 0.457432
 96818/100000: episode: 1553, duration: 0.221s, episode steps: 45, steps per second: 204, episode reward: 11.622, mean reward: 0.258 [0.074, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.035, 10.100], loss: 0.002585, mae: 0.056437, mean_q: 0.455329
 96864/100000: episode: 1554, duration: 0.241s, episode steps: 46, steps per second: 191, episode reward: 10.054, mean reward: 0.219 [0.014, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-1.050, 10.142], loss: 0.006769, mae: 0.069018, mean_q: 0.435209
 96881/100000: episode: 1555, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 8.142, mean reward: 0.479 [0.374, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.349, 10.471], loss: 0.003714, mae: 0.065405, mean_q: 0.467571
 96927/100000: episode: 1556, duration: 0.243s, episode steps: 46, steps per second: 189, episode reward: 11.959, mean reward: 0.260 [0.034, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.433, 10.118], loss: 0.004134, mae: 0.061793, mean_q: 0.472072
 96975/100000: episode: 1557, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 18.459, mean reward: 0.385 [0.248, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.418, 10.100], loss: 0.002986, mae: 0.059754, mean_q: 0.474885
 96993/100000: episode: 1558, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 6.925, mean reward: 0.385 [0.324, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.224, 10.424], loss: 0.004058, mae: 0.060356, mean_q: 0.525489
 97038/100000: episode: 1559, duration: 0.226s, episode steps: 45, steps per second: 199, episode reward: 15.295, mean reward: 0.340 [0.036, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-1.398, 10.141], loss: 0.002897, mae: 0.059093, mean_q: 0.503796
 97086/100000: episode: 1560, duration: 0.263s, episode steps: 48, steps per second: 183, episode reward: 13.667, mean reward: 0.285 [0.012, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.444, 10.113], loss: 0.002868, mae: 0.058887, mean_q: 0.514648
 97103/100000: episode: 1561, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 5.002, mean reward: 0.294 [0.196, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.309], loss: 0.002767, mae: 0.057843, mean_q: 0.523599
 97149/100000: episode: 1562, duration: 0.244s, episode steps: 46, steps per second: 188, episode reward: 13.137, mean reward: 0.286 [0.141, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.820, 10.100], loss: 0.003513, mae: 0.060438, mean_q: 0.504168
 97198/100000: episode: 1563, duration: 0.264s, episode steps: 49, steps per second: 185, episode reward: 14.498, mean reward: 0.296 [0.136, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-1.059, 10.100], loss: 0.002957, mae: 0.059609, mean_q: 0.544714
 97209/100000: episode: 1564, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 5.129, mean reward: 0.466 [0.417, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.356, 10.100], loss: 0.002624, mae: 0.055029, mean_q: 0.522266
 97256/100000: episode: 1565, duration: 0.236s, episode steps: 47, steps per second: 199, episode reward: 11.968, mean reward: 0.255 [0.038, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.178, 10.100], loss: 0.002322, mae: 0.053153, mean_q: 0.543408
 97303/100000: episode: 1566, duration: 0.245s, episode steps: 47, steps per second: 192, episode reward: 14.633, mean reward: 0.311 [0.109, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.737, 10.100], loss: 0.002840, mae: 0.058914, mean_q: 0.544428
 97350/100000: episode: 1567, duration: 0.247s, episode steps: 47, steps per second: 190, episode reward: 12.764, mean reward: 0.272 [0.044, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.443, 10.100], loss: 0.003035, mae: 0.059505, mean_q: 0.563957
 97395/100000: episode: 1568, duration: 0.268s, episode steps: 45, steps per second: 168, episode reward: 9.667, mean reward: 0.215 [0.047, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.448, 10.100], loss: 0.002642, mae: 0.057371, mean_q: 0.562142
 97440/100000: episode: 1569, duration: 0.237s, episode steps: 45, steps per second: 190, episode reward: 10.260, mean reward: 0.228 [0.050, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.463, 10.169], loss: 0.002691, mae: 0.057813, mean_q: 0.563955
 97485/100000: episode: 1570, duration: 0.239s, episode steps: 45, steps per second: 188, episode reward: 12.499, mean reward: 0.278 [0.115, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.591, 10.354], loss: 0.002764, mae: 0.059221, mean_q: 0.587965
 97492/100000: episode: 1571, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 2.773, mean reward: 0.396 [0.360, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.336, 10.100], loss: 0.003096, mae: 0.059885, mean_q: 0.602111
 97538/100000: episode: 1572, duration: 0.251s, episode steps: 46, steps per second: 183, episode reward: 13.235, mean reward: 0.288 [0.034, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.642, 10.100], loss: 0.002682, mae: 0.058038, mean_q: 0.577236
 97583/100000: episode: 1573, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 15.692, mean reward: 0.349 [0.236, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.323, 10.100], loss: 0.002951, mae: 0.060269, mean_q: 0.592449
 97600/100000: episode: 1574, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 7.736, mean reward: 0.455 [0.344, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.374, 10.496], loss: 0.002555, mae: 0.056614, mean_q: 0.590283
 97607/100000: episode: 1575, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 3.684, mean reward: 0.526 [0.432, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.607, 10.100], loss: 0.002713, mae: 0.058051, mean_q: 0.605574
 97614/100000: episode: 1576, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 3.167, mean reward: 0.452 [0.404, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.669, 10.100], loss: 0.002646, mae: 0.057824, mean_q: 0.568723
 97632/100000: episode: 1577, duration: 0.090s, episode steps: 18, steps per second: 201, episode reward: 7.939, mean reward: 0.441 [0.377, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.539], loss: 0.002559, mae: 0.054899, mean_q: 0.578592
 97677/100000: episode: 1578, duration: 0.254s, episode steps: 45, steps per second: 177, episode reward: 12.699, mean reward: 0.282 [0.063, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.159, 10.163], loss: 0.002282, mae: 0.053390, mean_q: 0.578206
 97724/100000: episode: 1579, duration: 0.248s, episode steps: 47, steps per second: 190, episode reward: 16.826, mean reward: 0.358 [0.136, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-1.132, 10.100], loss: 0.002633, mae: 0.056431, mean_q: 0.580194
 97769/100000: episode: 1580, duration: 0.254s, episode steps: 45, steps per second: 177, episode reward: 18.015, mean reward: 0.400 [0.282, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.425, 10.100], loss: 0.002760, mae: 0.058634, mean_q: 0.587312
 97780/100000: episode: 1581, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 4.877, mean reward: 0.443 [0.391, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.506, 10.100], loss: 0.002875, mae: 0.059239, mean_q: 0.585803
 97787/100000: episode: 1582, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 2.642, mean reward: 0.377 [0.346, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.399, 10.100], loss: 0.002900, mae: 0.060753, mean_q: 0.606262
 97798/100000: episode: 1583, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 5.458, mean reward: 0.496 [0.388, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.489, 10.100], loss: 0.002435, mae: 0.054151, mean_q: 0.577519
 97809/100000: episode: 1584, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 4.828, mean reward: 0.439 [0.395, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.269, 10.100], loss: 0.002536, mae: 0.056879, mean_q: 0.579512
 97820/100000: episode: 1585, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 4.868, mean reward: 0.443 [0.393, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.738, 10.100], loss: 0.002364, mae: 0.054305, mean_q: 0.585765
 97838/100000: episode: 1586, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 4.714, mean reward: 0.262 [0.182, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.317], loss: 0.002676, mae: 0.057698, mean_q: 0.587712
 97849/100000: episode: 1587, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 4.422, mean reward: 0.402 [0.355, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.416, 10.100], loss: 0.003083, mae: 0.063644, mean_q: 0.579865
 97866/100000: episode: 1588, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 6.400, mean reward: 0.376 [0.262, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.760, 10.366], loss: 0.002964, mae: 0.061334, mean_q: 0.588593
 97914/100000: episode: 1589, duration: 0.249s, episode steps: 48, steps per second: 193, episode reward: 17.033, mean reward: 0.355 [0.143, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.566, 10.100], loss: 0.002570, mae: 0.057123, mean_q: 0.590628
 97959/100000: episode: 1590, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 17.157, mean reward: 0.381 [0.242, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.725, 10.100], loss: 0.002585, mae: 0.056917, mean_q: 0.604009
 98008/100000: episode: 1591, duration: 0.245s, episode steps: 49, steps per second: 200, episode reward: 11.862, mean reward: 0.242 [0.030, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.385, 10.233], loss: 0.002761, mae: 0.058932, mean_q: 0.595563
 98053/100000: episode: 1592, duration: 0.220s, episode steps: 45, steps per second: 205, episode reward: 11.647, mean reward: 0.259 [0.069, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.972, 10.171], loss: 0.002546, mae: 0.056185, mean_q: 0.592286
 98070/100000: episode: 1593, duration: 0.080s, episode steps: 17, steps per second: 211, episode reward: 6.551, mean reward: 0.385 [0.296, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.190, 10.494], loss: 0.002449, mae: 0.054719, mean_q: 0.597259
 98116/100000: episode: 1594, duration: 0.235s, episode steps: 46, steps per second: 195, episode reward: 9.408, mean reward: 0.205 [0.015, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.984, 10.100], loss: 0.002639, mae: 0.057127, mean_q: 0.591188
 98123/100000: episode: 1595, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 3.381, mean reward: 0.483 [0.446, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.359, 10.100], loss: 0.002877, mae: 0.059623, mean_q: 0.590759
 98141/100000: episode: 1596, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 8.245, mean reward: 0.458 [0.367, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.185, 10.535], loss: 0.002413, mae: 0.055313, mean_q: 0.588747
 98186/100000: episode: 1597, duration: 0.230s, episode steps: 45, steps per second: 196, episode reward: 9.601, mean reward: 0.213 [0.028, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.711, 10.194], loss: 0.002573, mae: 0.056501, mean_q: 0.595333
 98235/100000: episode: 1598, duration: 0.262s, episode steps: 49, steps per second: 187, episode reward: 17.989, mean reward: 0.367 [0.151, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.552, 10.227], loss: 0.002540, mae: 0.056906, mean_q: 0.590931
 98282/100000: episode: 1599, duration: 0.242s, episode steps: 47, steps per second: 194, episode reward: 12.535, mean reward: 0.267 [0.114, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-1.060, 10.100], loss: 0.002811, mae: 0.059111, mean_q: 0.589432
 98289/100000: episode: 1600, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 3.104, mean reward: 0.443 [0.405, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.362, 10.100], loss: 0.002230, mae: 0.052882, mean_q: 0.603507
 98335/100000: episode: 1601, duration: 0.247s, episode steps: 46, steps per second: 187, episode reward: 11.852, mean reward: 0.258 [0.015, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.975, 10.229], loss: 0.002669, mae: 0.057800, mean_q: 0.592109
[Info] 300-TH LEVEL FOUND: 1.004174828529358, Considering 10/90 traces
 98380/100000: episode: 1602, duration: 4.119s, episode steps: 45, steps per second: 11, episode reward: 10.463, mean reward: 0.233 [0.044, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.714, 10.351], loss: 0.002703, mae: 0.057604, mean_q: 0.593610
 98417/100000: episode: 1603, duration: 0.193s, episode steps: 37, steps per second: 191, episode reward: 16.684, mean reward: 0.451 [0.252, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.380, 10.100], loss: 0.002478, mae: 0.055270, mean_q: 0.602007
 98422/100000: episode: 1604, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 2.279, mean reward: 0.456 [0.425, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.207, 10.100], loss: 0.002301, mae: 0.054081, mean_q: 0.579815
 98428/100000: episode: 1605, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 2.524, mean reward: 0.421 [0.299, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.386, 10.100], loss: 0.002712, mae: 0.060235, mean_q: 0.587221
 98434/100000: episode: 1606, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 2.476, mean reward: 0.413 [0.375, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.305, 10.100], loss: 0.002697, mae: 0.059052, mean_q: 0.602612
 98476/100000: episode: 1607, duration: 0.207s, episode steps: 42, steps per second: 203, episode reward: 18.218, mean reward: 0.434 [0.332, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.659, 10.100], loss: 0.002601, mae: 0.056007, mean_q: 0.602184
 98518/100000: episode: 1608, duration: 0.216s, episode steps: 42, steps per second: 195, episode reward: 15.878, mean reward: 0.378 [0.171, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.306, 10.100], loss: 0.002611, mae: 0.056107, mean_q: 0.604875
[Info] FALSIFICATION!
 98530/100000: episode: 1609, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 16.567, mean reward: 1.381 [0.510, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.691 [-0.014, 8.010], loss: 0.002837, mae: 0.060230, mean_q: 0.600434
 98630/100000: episode: 1610, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -20.131, mean reward: -0.201 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.913, 10.132], loss: 0.002627, mae: 0.057474, mean_q: 0.591975
 98730/100000: episode: 1611, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -15.091, mean reward: -0.151 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.873, 10.098], loss: 0.027321, mae: 0.072789, mean_q: 0.575894
 98830/100000: episode: 1612, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -14.779, mean reward: -0.148 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.324, 10.365], loss: 0.026733, mae: 0.078775, mean_q: 0.571084
 98930/100000: episode: 1613, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -20.283, mean reward: -0.203 [-1.000, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.572, 10.129], loss: 0.004552, mae: 0.063532, mean_q: 0.528641
 99030/100000: episode: 1614, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.078, mean reward: -0.161 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.484, 10.391], loss: 0.003487, mae: 0.059788, mean_q: 0.514405
 99130/100000: episode: 1615, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.473, mean reward: -0.195 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.167, 10.295], loss: 0.014668, mae: 0.064082, mean_q: 0.504786
 99230/100000: episode: 1616, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -13.419, mean reward: -0.134 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.225, 10.098], loss: 0.014553, mae: 0.064827, mean_q: 0.484798
 99330/100000: episode: 1617, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.339, mean reward: -0.183 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.770, 10.098], loss: 0.002670, mae: 0.057130, mean_q: 0.461027
 99430/100000: episode: 1618, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -16.435, mean reward: -0.164 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.888, 10.229], loss: 0.002424, mae: 0.054148, mean_q: 0.429977
 99530/100000: episode: 1619, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -10.719, mean reward: -0.107 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.577, 10.098], loss: 0.002490, mae: 0.053957, mean_q: 0.426192
 99630/100000: episode: 1620, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.019, mean reward: -0.180 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.359, 10.420], loss: 0.002435, mae: 0.053996, mean_q: 0.411026
 99730/100000: episode: 1621, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -12.152, mean reward: -0.122 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.515, 10.098], loss: 0.026464, mae: 0.072329, mean_q: 0.395594
 99830/100000: episode: 1622, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -14.227, mean reward: -0.142 [-1.000, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.530, 10.098], loss: 0.025750, mae: 0.068627, mean_q: 0.382062
 99930/100000: episode: 1623, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -12.133, mean reward: -0.121 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.147, 10.098], loss: 0.013909, mae: 0.060935, mean_q: 0.366789
done, took 585.249 seconds
[Info] End Importance Splitting. Falsification occurred 7 times.
