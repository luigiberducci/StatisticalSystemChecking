Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.202s, episode steps: 100, steps per second: 494, episode reward: -16.009, mean reward: -0.160 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.076, 10.181], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.094s, episode steps: 100, steps per second: 1060, episode reward: -16.315, mean reward: -0.163 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.053, 10.391], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.113s, episode steps: 100, steps per second: 889, episode reward: -16.923, mean reward: -0.169 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.481, 10.278], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.100s, episode steps: 100, steps per second: 997, episode reward: -18.130, mean reward: -0.181 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.774, 10.250], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.083s, episode steps: 100, steps per second: 1211, episode reward: -8.471, mean reward: -0.085 [-1.000, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.925, 10.484], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.253s, episode steps: 100, steps per second: 80, episode reward: -16.685, mean reward: -0.167 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.901, 10.098], loss: 0.047341, mae: 0.212313, mean_q: -0.138889
   700/100000: episode: 7, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.083, mean reward: -0.171 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.846, 10.402], loss: 0.019205, mae: 0.135597, mean_q: -0.218419
   800/100000: episode: 8, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -16.564, mean reward: -0.166 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.585, 10.244], loss: 0.014261, mae: 0.120488, mean_q: -0.264697
   900/100000: episode: 9, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -12.947, mean reward: -0.129 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.081, 10.098], loss: 0.012737, mae: 0.110010, mean_q: -0.282392
  1000/100000: episode: 10, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -18.795, mean reward: -0.188 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.994, 10.210], loss: 0.012364, mae: 0.105370, mean_q: -0.309162
  1100/100000: episode: 11, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -18.580, mean reward: -0.186 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.166, 10.098], loss: 0.011048, mae: 0.099205, mean_q: -0.320427
  1200/100000: episode: 12, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -13.545, mean reward: -0.135 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.389, 10.248], loss: 0.010143, mae: 0.093871, mean_q: -0.307812
  1300/100000: episode: 13, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -10.821, mean reward: -0.108 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.480, 10.098], loss: 0.010486, mae: 0.094855, mean_q: -0.306410
  1400/100000: episode: 14, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -19.553, mean reward: -0.196 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.903, 10.252], loss: 0.009948, mae: 0.094242, mean_q: -0.294851
  1500/100000: episode: 15, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -13.122, mean reward: -0.131 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.065, 10.098], loss: 0.010277, mae: 0.095232, mean_q: -0.327561
  1600/100000: episode: 16, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -15.092, mean reward: -0.151 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.617, 10.165], loss: 0.009313, mae: 0.091543, mean_q: -0.286591
  1700/100000: episode: 17, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -16.877, mean reward: -0.169 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.980, 10.098], loss: 0.008517, mae: 0.087247, mean_q: -0.316930
  1800/100000: episode: 18, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -20.387, mean reward: -0.204 [-1.000, 0.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.315, 10.101], loss: 0.007772, mae: 0.082073, mean_q: -0.286255
  1900/100000: episode: 19, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -19.920, mean reward: -0.199 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.859, 10.186], loss: 0.007656, mae: 0.082354, mean_q: -0.294844
  2000/100000: episode: 20, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.382, mean reward: -0.184 [-1.000, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.659, 10.098], loss: 0.007691, mae: 0.084026, mean_q: -0.301575
  2100/100000: episode: 21, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.320, mean reward: -0.183 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.986, 10.098], loss: 0.006638, mae: 0.077960, mean_q: -0.300692
  2200/100000: episode: 22, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -13.678, mean reward: -0.137 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.442, 10.209], loss: 0.005870, mae: 0.074832, mean_q: -0.312402
  2300/100000: episode: 23, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -17.567, mean reward: -0.176 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.511, 10.186], loss: 0.006565, mae: 0.078146, mean_q: -0.317755
  2400/100000: episode: 24, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: -19.321, mean reward: -0.193 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.459, 10.268], loss: 0.005972, mae: 0.074314, mean_q: -0.305958
  2500/100000: episode: 25, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: -17.312, mean reward: -0.173 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.925, 10.098], loss: 0.006537, mae: 0.079222, mean_q: -0.357417
  2600/100000: episode: 26, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: -15.809, mean reward: -0.158 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.369, 10.418], loss: 0.006198, mae: 0.077339, mean_q: -0.324221
  2700/100000: episode: 27, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: -17.287, mean reward: -0.173 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.175, 10.269], loss: 0.006061, mae: 0.075960, mean_q: -0.307266
  2800/100000: episode: 28, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: -18.390, mean reward: -0.184 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.224, 10.124], loss: 0.006256, mae: 0.075732, mean_q: -0.314331
  2900/100000: episode: 29, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: -14.656, mean reward: -0.147 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.666, 10.098], loss: 0.006156, mae: 0.074690, mean_q: -0.324160
  3000/100000: episode: 30, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -15.229, mean reward: -0.152 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.727, 10.098], loss: 0.005445, mae: 0.071813, mean_q: -0.325562
  3100/100000: episode: 31, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -16.938, mean reward: -0.169 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.780, 10.098], loss: 0.005327, mae: 0.072362, mean_q: -0.315313
  3200/100000: episode: 32, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -17.419, mean reward: -0.174 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.451, 10.222], loss: 0.005448, mae: 0.070356, mean_q: -0.311006
  3300/100000: episode: 33, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -20.385, mean reward: -0.204 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.896, 10.098], loss: 0.006260, mae: 0.073944, mean_q: -0.307917
  3400/100000: episode: 34, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -19.808, mean reward: -0.198 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.702, 10.098], loss: 0.005462, mae: 0.070447, mean_q: -0.356839
  3500/100000: episode: 35, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: -17.166, mean reward: -0.172 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.687, 10.098], loss: 0.006236, mae: 0.078074, mean_q: -0.327548
  3600/100000: episode: 36, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -17.122, mean reward: -0.171 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.855, 10.098], loss: 0.004440, mae: 0.066581, mean_q: -0.331185
  3700/100000: episode: 37, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -17.358, mean reward: -0.174 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.160, 10.098], loss: 0.004583, mae: 0.067504, mean_q: -0.331760
  3800/100000: episode: 38, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -14.820, mean reward: -0.148 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.736, 10.098], loss: 0.004819, mae: 0.068493, mean_q: -0.298598
  3900/100000: episode: 39, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -18.872, mean reward: -0.189 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.704, 10.246], loss: 0.004997, mae: 0.069619, mean_q: -0.329033
  4000/100000: episode: 40, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -12.414, mean reward: -0.124 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.870, 10.098], loss: 0.005675, mae: 0.074559, mean_q: -0.328201
  4100/100000: episode: 41, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -13.451, mean reward: -0.135 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.977, 10.098], loss: 0.006009, mae: 0.075195, mean_q: -0.329344
  4200/100000: episode: 42, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: -18.071, mean reward: -0.181 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.000, 10.098], loss: 0.006900, mae: 0.077227, mean_q: -0.310383
  4300/100000: episode: 43, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: -17.639, mean reward: -0.176 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.735, 10.195], loss: 0.004803, mae: 0.068631, mean_q: -0.317149
  4400/100000: episode: 44, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -16.778, mean reward: -0.168 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.561, 10.098], loss: 0.005170, mae: 0.069175, mean_q: -0.317980
  4500/100000: episode: 45, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -16.311, mean reward: -0.163 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.752, 10.098], loss: 0.005913, mae: 0.074317, mean_q: -0.339016
  4600/100000: episode: 46, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: -12.354, mean reward: -0.124 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.800, 10.390], loss: 0.005094, mae: 0.071871, mean_q: -0.306948
  4700/100000: episode: 47, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -14.436, mean reward: -0.144 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.088, 10.098], loss: 0.005468, mae: 0.072304, mean_q: -0.323438
  4800/100000: episode: 48, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: -16.409, mean reward: -0.164 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.403, 10.276], loss: 0.005034, mae: 0.069498, mean_q: -0.299672
  4900/100000: episode: 49, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: -16.322, mean reward: -0.163 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.377, 10.098], loss: 0.004823, mae: 0.067936, mean_q: -0.326212
  5000/100000: episode: 50, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -13.883, mean reward: -0.139 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.572, 10.098], loss: 0.004976, mae: 0.068898, mean_q: -0.332927
  5100/100000: episode: 51, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -15.884, mean reward: -0.159 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.643, 10.200], loss: 0.004887, mae: 0.068873, mean_q: -0.309420
  5200/100000: episode: 52, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -18.391, mean reward: -0.184 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.637, 10.098], loss: 0.004958, mae: 0.068958, mean_q: -0.291957
  5300/100000: episode: 53, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -10.421, mean reward: -0.104 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.729, 10.098], loss: 0.005058, mae: 0.069678, mean_q: -0.301757
  5400/100000: episode: 54, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: -18.735, mean reward: -0.187 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.671, 10.123], loss: 0.004327, mae: 0.068241, mean_q: -0.275455
  5500/100000: episode: 55, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -18.747, mean reward: -0.187 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.088, 10.105], loss: 0.004572, mae: 0.066100, mean_q: -0.314288
  5600/100000: episode: 56, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: -16.025, mean reward: -0.160 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.846, 10.165], loss: 0.005137, mae: 0.071029, mean_q: -0.292625
  5700/100000: episode: 57, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: -16.045, mean reward: -0.160 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.583, 10.234], loss: 0.004864, mae: 0.067764, mean_q: -0.305715
  5800/100000: episode: 58, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.637, mean reward: -0.156 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.543, 10.319], loss: 0.004947, mae: 0.071844, mean_q: -0.313036
  5900/100000: episode: 59, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: -15.915, mean reward: -0.159 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.600, 10.118], loss: 0.004713, mae: 0.069539, mean_q: -0.305491
  6000/100000: episode: 60, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -14.750, mean reward: -0.148 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.096, 10.098], loss: 0.004272, mae: 0.065243, mean_q: -0.316257
  6100/100000: episode: 61, duration: 0.608s, episode steps: 100, steps per second: 165, episode reward: -13.911, mean reward: -0.139 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.473, 10.529], loss: 0.004457, mae: 0.068607, mean_q: -0.303259
  6200/100000: episode: 62, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: -9.261, mean reward: -0.093 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.833, 10.098], loss: 0.004762, mae: 0.068650, mean_q: -0.321920
  6300/100000: episode: 63, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: -18.348, mean reward: -0.183 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.093, 10.098], loss: 0.004041, mae: 0.064368, mean_q: -0.324548
  6400/100000: episode: 64, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -13.577, mean reward: -0.136 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.054, 10.098], loss: 0.004843, mae: 0.070694, mean_q: -0.301964
  6500/100000: episode: 65, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: -18.540, mean reward: -0.185 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.743, 10.165], loss: 0.004517, mae: 0.067834, mean_q: -0.359217
  6600/100000: episode: 66, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.095, mean reward: -0.161 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.954, 10.221], loss: 0.004580, mae: 0.068558, mean_q: -0.309889
  6700/100000: episode: 67, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -15.565, mean reward: -0.156 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.520, 10.098], loss: 0.004400, mae: 0.066422, mean_q: -0.333991
  6800/100000: episode: 68, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -15.460, mean reward: -0.155 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.403, 10.147], loss: 0.004502, mae: 0.068653, mean_q: -0.309019
  6900/100000: episode: 69, duration: 0.660s, episode steps: 100, steps per second: 152, episode reward: -18.784, mean reward: -0.188 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.390, 10.098], loss: 0.004676, mae: 0.066449, mean_q: -0.327876
  7000/100000: episode: 70, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -16.396, mean reward: -0.164 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.714, 10.098], loss: 0.004753, mae: 0.070619, mean_q: -0.296729
  7100/100000: episode: 71, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.836, mean reward: -0.148 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.836, 10.098], loss: 0.004520, mae: 0.069276, mean_q: -0.303249
  7200/100000: episode: 72, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: -10.459, mean reward: -0.105 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.393, 10.385], loss: 0.003948, mae: 0.065615, mean_q: -0.292459
  7300/100000: episode: 73, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -15.598, mean reward: -0.156 [-1.000, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.923, 10.337], loss: 0.003911, mae: 0.063022, mean_q: -0.328910
  7400/100000: episode: 74, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: -17.678, mean reward: -0.177 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.346, 10.098], loss: 0.005104, mae: 0.070957, mean_q: -0.320160
  7500/100000: episode: 75, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.366, mean reward: -0.184 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.531, 10.098], loss: 0.004674, mae: 0.069438, mean_q: -0.307285
  7600/100000: episode: 76, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -13.434, mean reward: -0.134 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.980, 10.248], loss: 0.003878, mae: 0.063796, mean_q: -0.287647
  7700/100000: episode: 77, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -13.673, mean reward: -0.137 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.041, 10.098], loss: 0.004636, mae: 0.068048, mean_q: -0.307275
  7800/100000: episode: 78, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: -19.429, mean reward: -0.194 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.134, 10.098], loss: 0.004002, mae: 0.065684, mean_q: -0.293914
  7900/100000: episode: 79, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: -16.201, mean reward: -0.162 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.895, 10.254], loss: 0.004239, mae: 0.067569, mean_q: -0.340889
  8000/100000: episode: 80, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: -18.427, mean reward: -0.184 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.268, 10.160], loss: 0.004842, mae: 0.071209, mean_q: -0.292149
  8100/100000: episode: 81, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: -18.934, mean reward: -0.189 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.741, 10.098], loss: 0.005126, mae: 0.072830, mean_q: -0.310328
  8200/100000: episode: 82, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: -17.875, mean reward: -0.179 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.819, 10.098], loss: 0.004513, mae: 0.068285, mean_q: -0.316140
  8300/100000: episode: 83, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -10.436, mean reward: -0.104 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.296, 10.102], loss: 0.004353, mae: 0.069535, mean_q: -0.319229
  8400/100000: episode: 84, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: -13.363, mean reward: -0.134 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.013, 10.098], loss: 0.005052, mae: 0.071493, mean_q: -0.299856
  8500/100000: episode: 85, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: -16.366, mean reward: -0.164 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.561, 10.326], loss: 0.006062, mae: 0.078423, mean_q: -0.294905
  8600/100000: episode: 86, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.220, mean reward: -0.192 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.900, 10.202], loss: 0.004372, mae: 0.068140, mean_q: -0.286032
  8700/100000: episode: 87, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -19.064, mean reward: -0.191 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.782, 10.369], loss: 0.004993, mae: 0.070614, mean_q: -0.263959
  8800/100000: episode: 88, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -18.117, mean reward: -0.181 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.088, 10.106], loss: 0.004830, mae: 0.070404, mean_q: -0.322880
  8900/100000: episode: 89, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -18.233, mean reward: -0.182 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.756, 10.173], loss: 0.005394, mae: 0.070418, mean_q: -0.306940
  9000/100000: episode: 90, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: -17.129, mean reward: -0.171 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.030, 10.098], loss: 0.005008, mae: 0.070220, mean_q: -0.290304
  9100/100000: episode: 91, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: -16.572, mean reward: -0.166 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.001, 10.357], loss: 0.004390, mae: 0.067054, mean_q: -0.301446
  9200/100000: episode: 92, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: -18.145, mean reward: -0.181 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.228, 10.168], loss: 0.003749, mae: 0.063027, mean_q: -0.337063
  9300/100000: episode: 93, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -12.465, mean reward: -0.125 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.879, 10.380], loss: 0.003751, mae: 0.063901, mean_q: -0.297613
  9400/100000: episode: 94, duration: 0.647s, episode steps: 100, steps per second: 155, episode reward: -13.299, mean reward: -0.133 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.260, 10.098], loss: 0.004122, mae: 0.066594, mean_q: -0.294038
  9500/100000: episode: 95, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.330, mean reward: -0.153 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.071, 10.098], loss: 0.004838, mae: 0.070608, mean_q: -0.303307
  9600/100000: episode: 96, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -17.081, mean reward: -0.171 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.701, 10.098], loss: 0.003856, mae: 0.064247, mean_q: -0.289828
  9700/100000: episode: 97, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: -16.342, mean reward: -0.163 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.897, 10.374], loss: 0.005850, mae: 0.074156, mean_q: -0.308616
  9800/100000: episode: 98, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: -15.003, mean reward: -0.150 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.319, 10.098], loss: 0.003898, mae: 0.064321, mean_q: -0.326647
  9900/100000: episode: 99, duration: 0.738s, episode steps: 100, steps per second: 136, episode reward: -15.809, mean reward: -0.158 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.417, 10.152], loss: 0.004174, mae: 0.065388, mean_q: -0.288603
 10000/100000: episode: 100, duration: 0.645s, episode steps: 100, steps per second: 155, episode reward: -15.824, mean reward: -0.158 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.871, 10.303], loss: 0.003862, mae: 0.062798, mean_q: -0.316035
 10100/100000: episode: 101, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -16.475, mean reward: -0.165 [-1.000, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.836, 10.098], loss: 0.004058, mae: 0.064954, mean_q: -0.317070
 10200/100000: episode: 102, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -9.924, mean reward: -0.099 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.981, 10.098], loss: 0.004510, mae: 0.069333, mean_q: -0.306839
 10300/100000: episode: 103, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -11.560, mean reward: -0.116 [-1.000, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.569, 10.223], loss: 0.004454, mae: 0.066552, mean_q: -0.304849
 10400/100000: episode: 104, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: -17.577, mean reward: -0.176 [-1.000, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.079, 10.098], loss: 0.003879, mae: 0.063330, mean_q: -0.277010
 10500/100000: episode: 105, duration: 0.643s, episode steps: 100, steps per second: 155, episode reward: -15.354, mean reward: -0.154 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.817, 10.098], loss: 0.004651, mae: 0.068538, mean_q: -0.294948
 10600/100000: episode: 106, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -17.221, mean reward: -0.172 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.937, 10.105], loss: 0.004494, mae: 0.070204, mean_q: -0.292546
 10700/100000: episode: 107, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: -16.240, mean reward: -0.162 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.538, 10.098], loss: 0.004155, mae: 0.067037, mean_q: -0.303756
 10800/100000: episode: 108, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: -17.904, mean reward: -0.179 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.723, 10.098], loss: 0.004290, mae: 0.067137, mean_q: -0.343173
 10900/100000: episode: 109, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: -17.156, mean reward: -0.172 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.381, 10.098], loss: 0.004292, mae: 0.067993, mean_q: -0.275629
 11000/100000: episode: 110, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: -16.663, mean reward: -0.167 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.078, 10.098], loss: 0.005736, mae: 0.075264, mean_q: -0.313356
 11100/100000: episode: 111, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: -19.050, mean reward: -0.190 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.444, 10.098], loss: 0.005065, mae: 0.072465, mean_q: -0.289823
 11200/100000: episode: 112, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -17.595, mean reward: -0.176 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.786, 10.252], loss: 0.004177, mae: 0.066465, mean_q: -0.293663
 11300/100000: episode: 113, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -20.165, mean reward: -0.202 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.170, 10.229], loss: 0.004588, mae: 0.067983, mean_q: -0.286101
 11400/100000: episode: 114, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: -17.936, mean reward: -0.179 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.675, 10.230], loss: 0.003980, mae: 0.065221, mean_q: -0.275316
 11500/100000: episode: 115, duration: 1.104s, episode steps: 100, steps per second: 91, episode reward: -19.820, mean reward: -0.198 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.199, 10.098], loss: 0.004240, mae: 0.067498, mean_q: -0.296194
 11600/100000: episode: 116, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: -18.510, mean reward: -0.185 [-1.000, 0.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.263, 10.242], loss: 0.003494, mae: 0.059983, mean_q: -0.331814
 11700/100000: episode: 117, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: -16.933, mean reward: -0.169 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.432, 10.454], loss: 0.003870, mae: 0.063606, mean_q: -0.294680
 11800/100000: episode: 118, duration: 0.682s, episode steps: 100, steps per second: 147, episode reward: -15.897, mean reward: -0.159 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.595, 10.374], loss: 0.005395, mae: 0.071451, mean_q: -0.332795
 11900/100000: episode: 119, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: -11.658, mean reward: -0.117 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.394, 10.098], loss: 0.003919, mae: 0.064672, mean_q: -0.288429
 12000/100000: episode: 120, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -17.895, mean reward: -0.179 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.490, 10.098], loss: 0.005021, mae: 0.071406, mean_q: -0.286449
 12100/100000: episode: 121, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -18.357, mean reward: -0.184 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.645, 10.098], loss: 0.004247, mae: 0.067222, mean_q: -0.303557
 12200/100000: episode: 122, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -14.677, mean reward: -0.147 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.664, 10.098], loss: 0.003821, mae: 0.062851, mean_q: -0.315088
 12300/100000: episode: 123, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: -18.642, mean reward: -0.186 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.999, 10.182], loss: 0.003444, mae: 0.060495, mean_q: -0.320021
 12400/100000: episode: 124, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -9.463, mean reward: -0.095 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.973, 10.098], loss: 0.003792, mae: 0.063876, mean_q: -0.281082
 12500/100000: episode: 125, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.893, mean reward: -0.199 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.669, 10.098], loss: 0.004172, mae: 0.066829, mean_q: -0.278899
 12600/100000: episode: 126, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: -17.815, mean reward: -0.178 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.072, 10.170], loss: 0.003917, mae: 0.064830, mean_q: -0.300093
 12700/100000: episode: 127, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.404, mean reward: -0.194 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.191, 10.098], loss: 0.003686, mae: 0.062956, mean_q: -0.320932
 12800/100000: episode: 128, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -16.818, mean reward: -0.168 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.785, 10.098], loss: 0.003974, mae: 0.065790, mean_q: -0.311485
 12900/100000: episode: 129, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.752, mean reward: -0.178 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.564, 10.098], loss: 0.003901, mae: 0.065743, mean_q: -0.327905
 13000/100000: episode: 130, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -20.182, mean reward: -0.202 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.044, 10.098], loss: 0.004712, mae: 0.071534, mean_q: -0.327671
 13100/100000: episode: 131, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -18.311, mean reward: -0.183 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.929, 10.098], loss: 0.003949, mae: 0.066130, mean_q: -0.292074
 13200/100000: episode: 132, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -19.741, mean reward: -0.197 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.505, 10.119], loss: 0.003563, mae: 0.060334, mean_q: -0.299904
 13300/100000: episode: 133, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -16.683, mean reward: -0.167 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.833, 10.387], loss: 0.004005, mae: 0.064955, mean_q: -0.301645
 13400/100000: episode: 134, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -15.512, mean reward: -0.155 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.845, 10.311], loss: 0.003861, mae: 0.064216, mean_q: -0.320506
 13500/100000: episode: 135, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -14.298, mean reward: -0.143 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.741, 10.098], loss: 0.003530, mae: 0.060167, mean_q: -0.306373
 13600/100000: episode: 136, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -19.477, mean reward: -0.195 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.570, 10.250], loss: 0.003864, mae: 0.064494, mean_q: -0.309483
 13700/100000: episode: 137, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -18.122, mean reward: -0.181 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.165, 10.242], loss: 0.004501, mae: 0.068138, mean_q: -0.268792
 13800/100000: episode: 138, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -19.649, mean reward: -0.196 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.984, 10.177], loss: 0.004148, mae: 0.066856, mean_q: -0.320727
 13900/100000: episode: 139, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -19.843, mean reward: -0.198 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.153, 10.146], loss: 0.004181, mae: 0.066129, mean_q: -0.322835
 14000/100000: episode: 140, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -15.938, mean reward: -0.159 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.308, 10.460], loss: 0.003677, mae: 0.062136, mean_q: -0.317882
 14100/100000: episode: 141, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -13.631, mean reward: -0.136 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.138, 10.346], loss: 0.003374, mae: 0.059774, mean_q: -0.315927
 14200/100000: episode: 142, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -16.155, mean reward: -0.162 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.816, 10.379], loss: 0.003311, mae: 0.059397, mean_q: -0.322976
 14300/100000: episode: 143, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.838, mean reward: -0.188 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.136, 10.276], loss: 0.003461, mae: 0.062238, mean_q: -0.298183
 14400/100000: episode: 144, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -17.251, mean reward: -0.173 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.206, 10.098], loss: 0.004744, mae: 0.066516, mean_q: -0.349683
 14500/100000: episode: 145, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -16.562, mean reward: -0.166 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.716, 10.098], loss: 0.004071, mae: 0.067503, mean_q: -0.307786
 14600/100000: episode: 146, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -18.924, mean reward: -0.189 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.814, 10.106], loss: 0.003795, mae: 0.063532, mean_q: -0.295028
 14700/100000: episode: 147, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: -20.784, mean reward: -0.208 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.443, 10.098], loss: 0.003628, mae: 0.062969, mean_q: -0.309863
 14800/100000: episode: 148, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: -16.090, mean reward: -0.161 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.719, 10.260], loss: 0.003309, mae: 0.058758, mean_q: -0.332437
 14900/100000: episode: 149, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.461, mean reward: -0.165 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.740, 10.098], loss: 0.003580, mae: 0.060720, mean_q: -0.318603
[Info] 100-TH LEVEL FOUND: 0.38924288749694824, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.938s, episode steps: 100, steps per second: 20, episode reward: -9.381, mean reward: -0.094 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.459, 10.337], loss: 0.003703, mae: 0.063334, mean_q: -0.305450
 15013/100000: episode: 151, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 2.480, mean reward: 0.191 [0.130, 0.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.798, 10.100], loss: 0.003948, mae: 0.071061, mean_q: -0.322254
 15024/100000: episode: 152, duration: 0.082s, episode steps: 11, steps per second: 135, episode reward: 2.506, mean reward: 0.228 [0.119, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.100], loss: 0.003717, mae: 0.060578, mean_q: -0.402892
 15037/100000: episode: 153, duration: 0.108s, episode steps: 13, steps per second: 120, episode reward: 3.111, mean reward: 0.239 [0.084, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.327, 10.100], loss: 0.004196, mae: 0.063913, mean_q: -0.403899
 15050/100000: episode: 154, duration: 0.116s, episode steps: 13, steps per second: 112, episode reward: 3.889, mean reward: 0.299 [0.190, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.270, 10.100], loss: 0.003699, mae: 0.064796, mean_q: -0.392239
 15062/100000: episode: 155, duration: 0.090s, episode steps: 12, steps per second: 134, episode reward: 3.218, mean reward: 0.268 [0.185, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.394, 10.100], loss: 0.003358, mae: 0.062846, mean_q: -0.143168
 15076/100000: episode: 156, duration: 0.092s, episode steps: 14, steps per second: 153, episode reward: 3.081, mean reward: 0.220 [0.149, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.067, 10.100], loss: 0.003010, mae: 0.056399, mean_q: -0.322061
 15087/100000: episode: 157, duration: 0.076s, episode steps: 11, steps per second: 145, episode reward: 2.991, mean reward: 0.272 [0.119, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.664, 10.100], loss: 0.002973, mae: 0.056150, mean_q: -0.259300
 15099/100000: episode: 158, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 4.235, mean reward: 0.353 [0.293, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.198, 10.100], loss: 0.003529, mae: 0.060628, mean_q: -0.274235
 15113/100000: episode: 159, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 5.331, mean reward: 0.381 [0.331, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.304, 10.100], loss: 0.004105, mae: 0.063244, mean_q: -0.296160
 15125/100000: episode: 160, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 4.303, mean reward: 0.359 [0.229, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.319, 10.100], loss: 0.003510, mae: 0.060364, mean_q: -0.261820
 15139/100000: episode: 161, duration: 0.100s, episode steps: 14, steps per second: 139, episode reward: 4.388, mean reward: 0.313 [0.250, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.211, 10.100], loss: 0.003528, mae: 0.059005, mean_q: -0.399419
 15151/100000: episode: 162, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 2.560, mean reward: 0.213 [0.128, 0.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.248, 10.100], loss: 0.003783, mae: 0.062947, mean_q: -0.413486
 15163/100000: episode: 163, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 3.331, mean reward: 0.278 [0.219, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.731, 10.100], loss: 0.003697, mae: 0.064333, mean_q: -0.278886
 15176/100000: episode: 164, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 3.553, mean reward: 0.273 [0.183, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.283, 10.100], loss: 0.003519, mae: 0.061714, mean_q: -0.258934
 15188/100000: episode: 165, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 4.679, mean reward: 0.390 [0.292, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.633, 10.100], loss: 0.003490, mae: 0.059430, mean_q: -0.344392
 15200/100000: episode: 166, duration: 0.083s, episode steps: 12, steps per second: 144, episode reward: 3.543, mean reward: 0.295 [0.264, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.524, 10.100], loss: 0.002711, mae: 0.056438, mean_q: -0.297695
 15212/100000: episode: 167, duration: 0.111s, episode steps: 12, steps per second: 108, episode reward: 2.892, mean reward: 0.241 [0.103, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.225, 10.100], loss: 0.004003, mae: 0.066088, mean_q: -0.370918
 15223/100000: episode: 168, duration: 0.090s, episode steps: 11, steps per second: 122, episode reward: 2.587, mean reward: 0.235 [0.076, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-1.047, 10.100], loss: 0.003684, mae: 0.063421, mean_q: -0.294387
 15236/100000: episode: 169, duration: 0.101s, episode steps: 13, steps per second: 128, episode reward: 3.226, mean reward: 0.248 [0.153, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.770, 10.100], loss: 0.005430, mae: 0.070828, mean_q: -0.260833
 15247/100000: episode: 170, duration: 0.088s, episode steps: 11, steps per second: 125, episode reward: 2.320, mean reward: 0.211 [0.114, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.180, 10.100], loss: 0.003949, mae: 0.062809, mean_q: -0.323341
 15261/100000: episode: 171, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 3.789, mean reward: 0.271 [0.207, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.365, 10.100], loss: 0.003312, mae: 0.061292, mean_q: -0.302591
 15273/100000: episode: 172, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 3.465, mean reward: 0.289 [0.207, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.257, 10.100], loss: 0.003810, mae: 0.067058, mean_q: -0.153256
 15285/100000: episode: 173, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 3.483, mean reward: 0.290 [0.199, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.288, 10.100], loss: 0.003071, mae: 0.057009, mean_q: -0.235005
 15297/100000: episode: 174, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 3.834, mean reward: 0.319 [0.248, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.232, 10.100], loss: 0.004184, mae: 0.066176, mean_q: -0.250083
 15309/100000: episode: 175, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 2.239, mean reward: 0.187 [0.136, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.089, 10.100], loss: 0.004488, mae: 0.071085, mean_q: -0.252996
 15320/100000: episode: 176, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 1.810, mean reward: 0.165 [0.062, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.100], loss: 0.005323, mae: 0.073285, mean_q: -0.275048
 15332/100000: episode: 177, duration: 0.082s, episode steps: 12, steps per second: 147, episode reward: 4.452, mean reward: 0.371 [0.270, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.294, 10.100], loss: 0.004598, mae: 0.072118, mean_q: -0.258300
 15346/100000: episode: 178, duration: 0.100s, episode steps: 14, steps per second: 140, episode reward: 4.306, mean reward: 0.308 [0.135, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.413, 10.100], loss: 0.005336, mae: 0.070801, mean_q: -0.184699
 15358/100000: episode: 179, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 3.990, mean reward: 0.333 [0.236, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.232, 10.100], loss: 0.004040, mae: 0.068242, mean_q: -0.237409
 15372/100000: episode: 180, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 3.931, mean reward: 0.281 [0.168, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.227, 10.100], loss: 0.003340, mae: 0.061238, mean_q: -0.291246
 15384/100000: episode: 181, duration: 0.078s, episode steps: 12, steps per second: 153, episode reward: 3.428, mean reward: 0.286 [0.196, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.198, 10.100], loss: 0.003789, mae: 0.063124, mean_q: -0.316009
 15398/100000: episode: 182, duration: 0.093s, episode steps: 14, steps per second: 151, episode reward: 3.822, mean reward: 0.273 [0.141, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.288, 10.100], loss: 0.003898, mae: 0.062509, mean_q: -0.209037
 15410/100000: episode: 183, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 2.874, mean reward: 0.240 [0.162, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.230, 10.100], loss: 0.003905, mae: 0.068015, mean_q: -0.211394
 15424/100000: episode: 184, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 4.373, mean reward: 0.312 [0.253, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-1.000, 10.100], loss: 0.002991, mae: 0.057474, mean_q: -0.349733
 15436/100000: episode: 185, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 3.478, mean reward: 0.290 [0.208, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.088, 10.100], loss: 0.003649, mae: 0.060168, mean_q: -0.224130
 15450/100000: episode: 186, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 4.859, mean reward: 0.347 [0.282, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.415, 10.100], loss: 0.004864, mae: 0.073032, mean_q: -0.217038
 15462/100000: episode: 187, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 4.011, mean reward: 0.334 [0.290, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.427, 10.100], loss: 0.003860, mae: 0.069242, mean_q: -0.230207
 15473/100000: episode: 188, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 3.083, mean reward: 0.280 [0.196, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.307, 10.100], loss: 0.004150, mae: 0.070217, mean_q: -0.201309
 15485/100000: episode: 189, duration: 0.081s, episode steps: 12, steps per second: 149, episode reward: 3.871, mean reward: 0.323 [0.281, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.242, 10.100], loss: 0.003862, mae: 0.070082, mean_q: -0.226195
 15499/100000: episode: 190, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 4.160, mean reward: 0.297 [0.191, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.307, 10.100], loss: 0.003403, mae: 0.063325, mean_q: -0.136233
 15512/100000: episode: 191, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 3.224, mean reward: 0.248 [0.112, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.377, 10.100], loss: 0.003598, mae: 0.062876, mean_q: -0.180626
 15524/100000: episode: 192, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 4.208, mean reward: 0.351 [0.260, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.259, 10.100], loss: 0.003982, mae: 0.067245, mean_q: -0.197703
 15538/100000: episode: 193, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 4.352, mean reward: 0.311 [0.248, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.325, 10.100], loss: 0.003392, mae: 0.061369, mean_q: -0.222715
 15551/100000: episode: 194, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 4.002, mean reward: 0.308 [0.140, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.378, 10.100], loss: 0.003342, mae: 0.060021, mean_q: -0.219069
 15562/100000: episode: 195, duration: 0.075s, episode steps: 11, steps per second: 146, episode reward: 3.586, mean reward: 0.326 [0.245, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.450, 10.100], loss: 0.003311, mae: 0.060879, mean_q: -0.232928
 15574/100000: episode: 196, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 2.823, mean reward: 0.235 [0.151, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.611, 10.100], loss: 0.003541, mae: 0.058816, mean_q: -0.292670
 15586/100000: episode: 197, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 3.227, mean reward: 0.269 [0.185, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.119, 10.100], loss: 0.003589, mae: 0.063869, mean_q: -0.068897
 15598/100000: episode: 198, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 3.392, mean reward: 0.283 [0.179, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.354, 10.100], loss: 0.003314, mae: 0.059898, mean_q: -0.276524
 15609/100000: episode: 199, duration: 0.081s, episode steps: 11, steps per second: 135, episode reward: 2.187, mean reward: 0.199 [0.101, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.103, 10.100], loss: 0.003968, mae: 0.066957, mean_q: -0.221128
 15621/100000: episode: 200, duration: 0.075s, episode steps: 12, steps per second: 159, episode reward: 1.465, mean reward: 0.122 [0.042, 0.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.201, 10.100], loss: 0.003455, mae: 0.060599, mean_q: -0.184692
 15632/100000: episode: 201, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 2.412, mean reward: 0.219 [0.155, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.224, 10.100], loss: 0.002898, mae: 0.056939, mean_q: -0.280722
 15645/100000: episode: 202, duration: 0.089s, episode steps: 13, steps per second: 145, episode reward: 2.177, mean reward: 0.167 [0.109, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.202, 10.100], loss: 0.003253, mae: 0.057772, mean_q: -0.273857
 15656/100000: episode: 203, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 2.341, mean reward: 0.213 [0.099, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.168, 10.100], loss: 0.003643, mae: 0.063725, mean_q: -0.131333
 15669/100000: episode: 204, duration: 0.094s, episode steps: 13, steps per second: 138, episode reward: 1.842, mean reward: 0.142 [0.087, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.248, 10.100], loss: 0.003743, mae: 0.060743, mean_q: -0.245118
 15680/100000: episode: 205, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 2.465, mean reward: 0.224 [0.116, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.338, 10.100], loss: 0.004193, mae: 0.065173, mean_q: -0.133967
 15693/100000: episode: 206, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 3.357, mean reward: 0.258 [0.159, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.305, 10.100], loss: 0.004225, mae: 0.067710, mean_q: -0.123938
 15705/100000: episode: 207, duration: 0.086s, episode steps: 12, steps per second: 139, episode reward: 3.169, mean reward: 0.264 [0.204, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.100], loss: 0.003243, mae: 0.057620, mean_q: -0.174673
 15719/100000: episode: 208, duration: 0.099s, episode steps: 14, steps per second: 142, episode reward: 4.111, mean reward: 0.294 [0.216, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.181, 10.100], loss: 0.003040, mae: 0.057702, mean_q: -0.268753
 15731/100000: episode: 209, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 2.753, mean reward: 0.229 [0.167, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.658, 10.100], loss: 0.003529, mae: 0.060403, mean_q: -0.215920
 15743/100000: episode: 210, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 1.526, mean reward: 0.127 [0.010, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-1.080, 10.153], loss: 0.004226, mae: 0.070226, mean_q: -0.253133
 15756/100000: episode: 211, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 3.496, mean reward: 0.269 [0.152, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.318, 10.100], loss: 0.003691, mae: 0.067837, mean_q: -0.205916
 15768/100000: episode: 212, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 2.069, mean reward: 0.172 [0.068, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.341, 10.100], loss: 0.003050, mae: 0.057952, mean_q: -0.205040
 15780/100000: episode: 213, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 2.771, mean reward: 0.231 [0.164, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.143, 10.100], loss: 0.003584, mae: 0.061626, mean_q: -0.193115
 15792/100000: episode: 214, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 4.527, mean reward: 0.377 [0.268, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.616, 10.100], loss: 0.004053, mae: 0.066152, mean_q: -0.198102
 15806/100000: episode: 215, duration: 0.103s, episode steps: 14, steps per second: 136, episode reward: 5.155, mean reward: 0.368 [0.330, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.404, 10.100], loss: 0.003080, mae: 0.057641, mean_q: -0.266950
 15819/100000: episode: 216, duration: 0.102s, episode steps: 13, steps per second: 128, episode reward: 2.079, mean reward: 0.160 [0.057, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.206, 10.100], loss: 0.003229, mae: 0.059952, mean_q: -0.145648
 15833/100000: episode: 217, duration: 0.105s, episode steps: 14, steps per second: 133, episode reward: 4.857, mean reward: 0.347 [0.307, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.120, 10.100], loss: 0.003916, mae: 0.065784, mean_q: -0.148678
 15845/100000: episode: 218, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 4.161, mean reward: 0.347 [0.265, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.217, 10.100], loss: 0.004199, mae: 0.067991, mean_q: -0.100797
 15857/100000: episode: 219, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 3.088, mean reward: 0.257 [0.150, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.216, 10.100], loss: 0.003491, mae: 0.062794, mean_q: -0.183200
 15871/100000: episode: 220, duration: 0.089s, episode steps: 14, steps per second: 157, episode reward: 4.482, mean reward: 0.320 [0.253, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.431, 10.100], loss: 0.003732, mae: 0.064612, mean_q: -0.219944
 15885/100000: episode: 221, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 3.210, mean reward: 0.229 [0.105, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.710, 10.100], loss: 0.004238, mae: 0.068302, mean_q: -0.184164
 15896/100000: episode: 222, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 2.328, mean reward: 0.212 [0.146, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.120, 10.100], loss: 0.003254, mae: 0.060276, mean_q: -0.222636
 15908/100000: episode: 223, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 3.069, mean reward: 0.256 [0.164, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.236, 10.100], loss: 0.003344, mae: 0.060720, mean_q: -0.168046
 15920/100000: episode: 224, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 3.740, mean reward: 0.312 [0.243, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.224, 10.100], loss: 0.004058, mae: 0.070668, mean_q: -0.160286
 15932/100000: episode: 225, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 3.695, mean reward: 0.308 [0.168, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-1.179, 10.100], loss: 0.004031, mae: 0.069681, mean_q: -0.190709
 15945/100000: episode: 226, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 4.124, mean reward: 0.317 [0.208, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.414, 10.100], loss: 0.003852, mae: 0.067529, mean_q: -0.138545
 15957/100000: episode: 227, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 3.256, mean reward: 0.271 [0.174, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.991, 10.100], loss: 0.003807, mae: 0.066239, mean_q: -0.063349
 15969/100000: episode: 228, duration: 0.078s, episode steps: 12, steps per second: 153, episode reward: 2.794, mean reward: 0.233 [0.168, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.100], loss: 0.004034, mae: 0.060799, mean_q: -0.134671
 15980/100000: episode: 229, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 2.598, mean reward: 0.236 [0.188, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.205, 10.100], loss: 0.005441, mae: 0.077441, mean_q: -0.138077
 15992/100000: episode: 230, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 3.235, mean reward: 0.270 [0.182, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.321, 10.100], loss: 0.005595, mae: 0.077085, mean_q: -0.150496
 16004/100000: episode: 231, duration: 0.092s, episode steps: 12, steps per second: 130, episode reward: 3.300, mean reward: 0.275 [0.235, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.933, 10.100], loss: 0.004661, mae: 0.074633, mean_q: -0.209124
 16016/100000: episode: 232, duration: 0.101s, episode steps: 12, steps per second: 118, episode reward: 2.866, mean reward: 0.239 [0.101, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.510, 10.100], loss: 0.004189, mae: 0.071379, mean_q: -0.188122
 16028/100000: episode: 233, duration: 0.094s, episode steps: 12, steps per second: 127, episode reward: 2.267, mean reward: 0.189 [0.119, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.218, 10.100], loss: 0.003611, mae: 0.066739, mean_q: -0.094246
 16039/100000: episode: 234, duration: 0.094s, episode steps: 11, steps per second: 118, episode reward: 2.769, mean reward: 0.252 [0.183, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.119, 10.100], loss: 0.003539, mae: 0.064711, mean_q: -0.116870
 16050/100000: episode: 235, duration: 0.074s, episode steps: 11, steps per second: 150, episode reward: 2.263, mean reward: 0.206 [0.036, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.107, 10.100], loss: 0.005138, mae: 0.074798, mean_q: -0.098913
 16063/100000: episode: 236, duration: 0.072s, episode steps: 13, steps per second: 179, episode reward: 3.029, mean reward: 0.233 [0.178, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.267, 10.100], loss: 0.004757, mae: 0.074573, mean_q: -0.178058
 16077/100000: episode: 237, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 5.362, mean reward: 0.383 [0.315, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.370, 10.100], loss: 0.003955, mae: 0.069122, mean_q: -0.179073
 16089/100000: episode: 238, duration: 0.065s, episode steps: 12, steps per second: 183, episode reward: 2.194, mean reward: 0.183 [0.085, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.085, 10.100], loss: 0.004941, mae: 0.075327, mean_q: -0.071386
 16101/100000: episode: 239, duration: 0.079s, episode steps: 12, steps per second: 151, episode reward: 3.490, mean reward: 0.291 [0.239, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.286, 10.100], loss: 0.003475, mae: 0.063318, mean_q: -0.056023
[Info] 200-TH LEVEL FOUND: 0.5565376281738281, Considering 10/90 traces
 16112/100000: episode: 240, duration: 4.779s, episode steps: 11, steps per second: 2, episode reward: 2.537, mean reward: 0.231 [0.170, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.254, 10.100], loss: 0.003467, mae: 0.061437, mean_q: -0.149496
 16119/100000: episode: 241, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 3.120, mean reward: 0.446 [0.388, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.460, 10.100], loss: 0.004063, mae: 0.067775, mean_q: -0.185518
 16126/100000: episode: 242, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 2.795, mean reward: 0.399 [0.336, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.410, 10.100], loss: 0.003051, mae: 0.059038, mean_q: -0.130817
 16133/100000: episode: 243, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 2.295, mean reward: 0.328 [0.284, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.370, 10.100], loss: 0.003578, mae: 0.063405, mean_q: -0.108960
 16142/100000: episode: 244, duration: 0.056s, episode steps: 9, steps per second: 159, episode reward: 2.312, mean reward: 0.257 [0.165, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.144, 10.100], loss: 0.003089, mae: 0.056673, mean_q: -0.146055
 16151/100000: episode: 245, duration: 0.059s, episode steps: 9, steps per second: 151, episode reward: 2.895, mean reward: 0.322 [0.268, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.286, 10.100], loss: 0.003503, mae: 0.059132, mean_q: -0.069047
 16161/100000: episode: 246, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 2.269, mean reward: 0.227 [0.193, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.240, 10.100], loss: 0.003981, mae: 0.068654, mean_q: -0.146172
 16169/100000: episode: 247, duration: 0.056s, episode steps: 8, steps per second: 142, episode reward: 3.236, mean reward: 0.404 [0.335, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.345, 10.100], loss: 0.003840, mae: 0.067939, mean_q: -0.134066
 16176/100000: episode: 248, duration: 0.050s, episode steps: 7, steps per second: 141, episode reward: 2.482, mean reward: 0.355 [0.321, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.419, 10.100], loss: 0.003655, mae: 0.067095, mean_q: -0.218888
 16186/100000: episode: 249, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 3.254, mean reward: 0.325 [0.276, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.483, 10.100], loss: 0.004395, mae: 0.071077, mean_q: -0.106704
 16196/100000: episode: 250, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 2.836, mean reward: 0.284 [0.211, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.295, 10.100], loss: 0.003527, mae: 0.064263, mean_q: -0.082178
 16205/100000: episode: 251, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 2.952, mean reward: 0.328 [0.225, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.367, 10.100], loss: 0.003834, mae: 0.064228, mean_q: -0.035151
 16214/100000: episode: 252, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 3.980, mean reward: 0.442 [0.338, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.335, 10.100], loss: 0.003525, mae: 0.064005, mean_q: -0.099691
 16223/100000: episode: 253, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 3.257, mean reward: 0.362 [0.262, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.451, 10.100], loss: 0.002959, mae: 0.060268, mean_q: -0.084332
 16230/100000: episode: 254, duration: 0.050s, episode steps: 7, steps per second: 141, episode reward: 2.691, mean reward: 0.384 [0.354, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.339, 10.100], loss: 0.003806, mae: 0.067849, mean_q: -0.002546
 16239/100000: episode: 255, duration: 0.070s, episode steps: 9, steps per second: 128, episode reward: 2.974, mean reward: 0.330 [0.293, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.682, 10.100], loss: 0.003016, mae: 0.061156, mean_q: -0.086014
 16249/100000: episode: 256, duration: 0.082s, episode steps: 10, steps per second: 123, episode reward: 3.111, mean reward: 0.311 [0.289, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.338, 10.100], loss: 0.003663, mae: 0.063371, mean_q: -0.130015
 16256/100000: episode: 257, duration: 0.066s, episode steps: 7, steps per second: 107, episode reward: 3.164, mean reward: 0.452 [0.420, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.454, 10.100], loss: 0.003351, mae: 0.063430, mean_q: -0.137740
 16264/100000: episode: 258, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 2.810, mean reward: 0.351 [0.299, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.438, 10.100], loss: 0.003964, mae: 0.065300, mean_q: -0.132994
 16273/100000: episode: 259, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 3.504, mean reward: 0.389 [0.339, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.277, 10.100], loss: 0.003562, mae: 0.060976, mean_q: -0.128500
 16282/100000: episode: 260, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 3.799, mean reward: 0.422 [0.334, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.783, 10.100], loss: 0.003849, mae: 0.066157, mean_q: -0.021580
 16291/100000: episode: 261, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 3.175, mean reward: 0.353 [0.307, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.314, 10.100], loss: 0.006490, mae: 0.081515, mean_q: -0.072607
 16300/100000: episode: 262, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 3.402, mean reward: 0.378 [0.327, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.393, 10.100], loss: 0.006404, mae: 0.076105, mean_q: -0.034203
 16310/100000: episode: 263, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 3.663, mean reward: 0.366 [0.183, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.537, 10.100], loss: 0.006940, mae: 0.074606, mean_q: -0.101658
 16317/100000: episode: 264, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 2.142, mean reward: 0.306 [0.245, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.168, 10.100], loss: 0.007512, mae: 0.086807, mean_q: -0.055250
 16327/100000: episode: 265, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 3.387, mean reward: 0.339 [0.279, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.337, 10.100], loss: 0.003947, mae: 0.068818, mean_q: -0.102458
 16335/100000: episode: 266, duration: 0.078s, episode steps: 8, steps per second: 102, episode reward: 3.071, mean reward: 0.384 [0.333, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.394, 10.100], loss: 0.004323, mae: 0.072636, mean_q: -0.100970
 16344/100000: episode: 267, duration: 0.088s, episode steps: 9, steps per second: 103, episode reward: 2.447, mean reward: 0.272 [0.211, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.285, 10.100], loss: 0.004200, mae: 0.065682, mean_q: -0.155582
 16352/100000: episode: 268, duration: 0.067s, episode steps: 8, steps per second: 120, episode reward: 3.019, mean reward: 0.377 [0.291, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.263, 10.100], loss: 0.003879, mae: 0.067599, mean_q: -0.072870
 16362/100000: episode: 269, duration: 0.071s, episode steps: 10, steps per second: 141, episode reward: 2.428, mean reward: 0.243 [0.124, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.394, 10.100], loss: 0.003879, mae: 0.067828, mean_q: -0.072374
 16371/100000: episode: 270, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 2.695, mean reward: 0.299 [0.166, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.115, 10.100], loss: 0.003910, mae: 0.064762, mean_q: -0.114604
 16380/100000: episode: 271, duration: 0.053s, episode steps: 9, steps per second: 168, episode reward: 3.842, mean reward: 0.427 [0.310, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.359, 10.100], loss: 0.003745, mae: 0.064047, mean_q: -0.094737
 16390/100000: episode: 272, duration: 0.066s, episode steps: 10, steps per second: 153, episode reward: 3.699, mean reward: 0.370 [0.271, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.502, 10.100], loss: 0.004174, mae: 0.070534, mean_q: -0.008639
 16400/100000: episode: 273, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 2.687, mean reward: 0.269 [0.214, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.309, 10.100], loss: 0.004163, mae: 0.070057, mean_q: -0.068501
 16407/100000: episode: 274, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 3.217, mean reward: 0.460 [0.422, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.419, 10.100], loss: 0.003871, mae: 0.067245, mean_q: -0.062270
 16417/100000: episode: 275, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 2.158, mean reward: 0.216 [0.156, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.432, 10.100], loss: 0.003669, mae: 0.063786, mean_q: -0.017321
 16426/100000: episode: 276, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 2.570, mean reward: 0.286 [0.208, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.364, 10.100], loss: 0.003649, mae: 0.065872, mean_q: 0.016275
 16435/100000: episode: 277, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 3.953, mean reward: 0.439 [0.364, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.458, 10.100], loss: 0.003309, mae: 0.060750, mean_q: -0.092934
 16444/100000: episode: 278, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 3.332, mean reward: 0.370 [0.236, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-1.026, 10.100], loss: 0.003507, mae: 0.063495, mean_q: -0.050258
 16453/100000: episode: 279, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 3.801, mean reward: 0.422 [0.356, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.423, 10.100], loss: 0.003384, mae: 0.060955, mean_q: -0.153097
 16462/100000: episode: 280, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 3.493, mean reward: 0.388 [0.328, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.294, 10.100], loss: 0.003216, mae: 0.061000, mean_q: 0.006260
 16471/100000: episode: 281, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 3.945, mean reward: 0.438 [0.345, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.193, 10.100], loss: 0.003743, mae: 0.065405, mean_q: -0.121516
 16480/100000: episode: 282, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 3.206, mean reward: 0.356 [0.289, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.194, 10.100], loss: 0.003715, mae: 0.063670, mean_q: -0.068031
 16487/100000: episode: 283, duration: 0.055s, episode steps: 7, steps per second: 128, episode reward: 2.849, mean reward: 0.407 [0.378, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.302, 10.100], loss: 0.003047, mae: 0.060809, mean_q: -0.093189
 16497/100000: episode: 284, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 3.555, mean reward: 0.356 [0.181, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.412, 10.100], loss: 0.004816, mae: 0.074078, mean_q: 0.002613
 16506/100000: episode: 285, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 3.400, mean reward: 0.378 [0.318, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.318, 10.100], loss: 0.005400, mae: 0.065155, mean_q: -0.034908
 16515/100000: episode: 286, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 3.482, mean reward: 0.387 [0.364, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.383, 10.100], loss: 0.004550, mae: 0.069231, mean_q: -0.065038
 16525/100000: episode: 287, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 3.519, mean reward: 0.352 [0.188, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.376, 10.100], loss: 0.004217, mae: 0.067875, mean_q: -0.069329
 16534/100000: episode: 288, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 3.507, mean reward: 0.390 [0.361, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.330, 10.100], loss: 0.004272, mae: 0.068871, mean_q: 0.034819
 16542/100000: episode: 289, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 2.897, mean reward: 0.362 [0.323, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.237, 10.100], loss: 0.003573, mae: 0.063048, mean_q: -0.052174
 16549/100000: episode: 290, duration: 0.039s, episode steps: 7, steps per second: 177, episode reward: 3.047, mean reward: 0.435 [0.399, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.472, 10.100], loss: 0.003979, mae: 0.065763, mean_q: -0.023463
 16558/100000: episode: 291, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 3.054, mean reward: 0.339 [0.289, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.327, 10.100], loss: 0.003821, mae: 0.063026, mean_q: 0.023080
 16567/100000: episode: 292, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 1.956, mean reward: 0.217 [0.110, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.305, 10.100], loss: 0.004124, mae: 0.063853, mean_q: -0.119179
 16576/100000: episode: 293, duration: 0.054s, episode steps: 9, steps per second: 165, episode reward: 2.963, mean reward: 0.329 [0.290, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.341, 10.100], loss: 0.004279, mae: 0.070507, mean_q: -0.061316
 16585/100000: episode: 294, duration: 0.062s, episode steps: 9, steps per second: 146, episode reward: 2.879, mean reward: 0.320 [0.238, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.471, 10.100], loss: 0.003325, mae: 0.064768, mean_q: -0.104298
 16594/100000: episode: 295, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 3.631, mean reward: 0.403 [0.329, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.343, 10.100], loss: 0.003709, mae: 0.065276, mean_q: -0.093360
 16603/100000: episode: 296, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 3.345, mean reward: 0.372 [0.334, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.498, 10.100], loss: 0.004934, mae: 0.073121, mean_q: 0.047237
 16612/100000: episode: 297, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 3.848, mean reward: 0.428 [0.379, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.379, 10.100], loss: 0.003711, mae: 0.064217, mean_q: -0.054642
 16621/100000: episode: 298, duration: 0.065s, episode steps: 9, steps per second: 138, episode reward: 3.972, mean reward: 0.441 [0.412, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.299, 10.100], loss: 0.003910, mae: 0.069535, mean_q: -0.021825
 16630/100000: episode: 299, duration: 0.069s, episode steps: 9, steps per second: 131, episode reward: 3.624, mean reward: 0.403 [0.322, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.380, 10.100], loss: 0.003942, mae: 0.067250, mean_q: -0.065877
 16640/100000: episode: 300, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 3.761, mean reward: 0.376 [0.270, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.387, 10.100], loss: 0.004294, mae: 0.070637, mean_q: -0.054519
 16649/100000: episode: 301, duration: 0.071s, episode steps: 9, steps per second: 126, episode reward: 2.486, mean reward: 0.276 [0.218, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.333, 10.100], loss: 0.003980, mae: 0.065792, mean_q: -0.055642
 16658/100000: episode: 302, duration: 0.065s, episode steps: 9, steps per second: 139, episode reward: 3.633, mean reward: 0.404 [0.294, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.527, 10.100], loss: 0.003867, mae: 0.068886, mean_q: -0.022619
 16666/100000: episode: 303, duration: 0.065s, episode steps: 8, steps per second: 124, episode reward: 2.991, mean reward: 0.374 [0.313, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.393, 10.100], loss: 0.004521, mae: 0.072752, mean_q: -0.128203
 16675/100000: episode: 304, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 3.751, mean reward: 0.417 [0.330, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.296, 10.100], loss: 0.004159, mae: 0.067092, mean_q: 0.053603
 16682/100000: episode: 305, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 2.910, mean reward: 0.416 [0.337, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.417, 10.100], loss: 0.005115, mae: 0.072686, mean_q: 0.012539
 16692/100000: episode: 306, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 3.711, mean reward: 0.371 [0.219, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.516, 10.100], loss: 0.004636, mae: 0.071259, mean_q: -0.036532
 16701/100000: episode: 307, duration: 0.064s, episode steps: 9, steps per second: 141, episode reward: 3.174, mean reward: 0.353 [0.300, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.362, 10.100], loss: 0.004187, mae: 0.075001, mean_q: -0.098484
 16711/100000: episode: 308, duration: 0.070s, episode steps: 10, steps per second: 144, episode reward: 3.030, mean reward: 0.303 [0.236, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.453, 10.100], loss: 0.004382, mae: 0.075071, mean_q: -0.138095
 16720/100000: episode: 309, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 3.765, mean reward: 0.418 [0.344, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.437, 10.100], loss: 0.003350, mae: 0.064752, mean_q: -0.062481
 16729/100000: episode: 310, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 3.255, mean reward: 0.362 [0.300, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.512, 10.100], loss: 0.003963, mae: 0.068601, mean_q: 0.006449
 16738/100000: episode: 311, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 3.485, mean reward: 0.387 [0.317, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.240, 10.100], loss: 0.003886, mae: 0.064512, mean_q: -0.088430
 16747/100000: episode: 312, duration: 0.072s, episode steps: 9, steps per second: 126, episode reward: 3.287, mean reward: 0.365 [0.262, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.241, 10.100], loss: 0.004065, mae: 0.072593, mean_q: -0.010585
 16755/100000: episode: 313, duration: 0.075s, episode steps: 8, steps per second: 107, episode reward: 3.009, mean reward: 0.376 [0.338, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.422, 10.100], loss: 0.003404, mae: 0.065456, mean_q: -0.157721
 16762/100000: episode: 314, duration: 0.068s, episode steps: 7, steps per second: 103, episode reward: 3.113, mean reward: 0.445 [0.418, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.504, 10.100], loss: 0.003832, mae: 0.064825, mean_q: 0.040739
 16772/100000: episode: 315, duration: 0.091s, episode steps: 10, steps per second: 110, episode reward: 4.127, mean reward: 0.413 [0.246, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.449, 10.100], loss: 0.005390, mae: 0.074769, mean_q: 0.009864
 16781/100000: episode: 316, duration: 0.075s, episode steps: 9, steps per second: 121, episode reward: 2.993, mean reward: 0.333 [0.306, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.379, 10.100], loss: 0.005074, mae: 0.075378, mean_q: -0.000507
 16790/100000: episode: 317, duration: 0.059s, episode steps: 9, steps per second: 154, episode reward: 3.129, mean reward: 0.348 [0.263, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-1.078, 10.100], loss: 0.003837, mae: 0.068409, mean_q: -0.112124
 16799/100000: episode: 318, duration: 0.065s, episode steps: 9, steps per second: 139, episode reward: 2.993, mean reward: 0.333 [0.279, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.401, 10.100], loss: 0.003514, mae: 0.068095, mean_q: 0.124388
 16808/100000: episode: 319, duration: 0.067s, episode steps: 9, steps per second: 135, episode reward: 3.373, mean reward: 0.375 [0.336, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.351, 10.100], loss: 0.003279, mae: 0.061173, mean_q: 0.055819
 16817/100000: episode: 320, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 3.654, mean reward: 0.406 [0.356, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.390, 10.100], loss: 0.004174, mae: 0.066963, mean_q: 0.121619
 16826/100000: episode: 321, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 2.501, mean reward: 0.278 [0.227, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.340, 10.100], loss: 0.004040, mae: 0.063245, mean_q: -0.033476
 16835/100000: episode: 322, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 3.552, mean reward: 0.395 [0.376, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.324, 10.100], loss: 0.004234, mae: 0.070175, mean_q: -0.009967
 16843/100000: episode: 323, duration: 0.066s, episode steps: 8, steps per second: 122, episode reward: 2.301, mean reward: 0.288 [0.249, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.338, 10.100], loss: 0.003552, mae: 0.064905, mean_q: 0.138284
 16852/100000: episode: 324, duration: 0.093s, episode steps: 9, steps per second: 96, episode reward: 2.332, mean reward: 0.259 [0.176, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.360, 10.100], loss: 0.003683, mae: 0.066061, mean_q: 0.001612
 16861/100000: episode: 325, duration: 0.069s, episode steps: 9, steps per second: 131, episode reward: 3.418, mean reward: 0.380 [0.356, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.398, 10.100], loss: 0.004593, mae: 0.067133, mean_q: 0.061485
 16870/100000: episode: 326, duration: 0.066s, episode steps: 9, steps per second: 137, episode reward: 3.749, mean reward: 0.417 [0.267, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.097, 10.100], loss: 0.003808, mae: 0.069449, mean_q: 0.014148
 16879/100000: episode: 327, duration: 0.067s, episode steps: 9, steps per second: 134, episode reward: 3.174, mean reward: 0.353 [0.312, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.385, 10.100], loss: 0.004052, mae: 0.066724, mean_q: 0.107876
 16889/100000: episode: 328, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 3.340, mean reward: 0.334 [0.233, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.361, 10.100], loss: 0.003873, mae: 0.065295, mean_q: -0.041722
 16898/100000: episode: 329, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 4.055, mean reward: 0.451 [0.405, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.308, 10.100], loss: 0.002977, mae: 0.059330, mean_q: -0.051466
[Info] 300-TH LEVEL FOUND: 0.7434316277503967, Considering 10/90 traces
 16907/100000: episode: 330, duration: 4.700s, episode steps: 9, steps per second: 2, episode reward: 3.620, mean reward: 0.402 [0.337, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.198, 10.100], loss: 0.003490, mae: 0.063510, mean_q: -0.011128
 16913/100000: episode: 331, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 2.124, mean reward: 0.354 [0.298, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.346, 10.100], loss: 0.003673, mae: 0.064335, mean_q: 0.042413
 16918/100000: episode: 332, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 2.126, mean reward: 0.425 [0.413, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.471, 10.100], loss: 0.003173, mae: 0.060468, mean_q: 0.033822
 16925/100000: episode: 333, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 3.044, mean reward: 0.435 [0.328, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.519, 10.100], loss: 0.003006, mae: 0.056652, mean_q: -0.007920
 16931/100000: episode: 334, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 2.244, mean reward: 0.374 [0.338, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.400, 10.100], loss: 0.003690, mae: 0.065111, mean_q: 0.128510
 16936/100000: episode: 335, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 2.525, mean reward: 0.505 [0.343, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.393, 10.100], loss: 0.004004, mae: 0.069423, mean_q: 0.140914
 16941/100000: episode: 336, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 1.897, mean reward: 0.379 [0.342, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.341, 10.100], loss: 0.003468, mae: 0.064506, mean_q: 0.244898
 16947/100000: episode: 337, duration: 0.036s, episode steps: 6, steps per second: 169, episode reward: 2.703, mean reward: 0.450 [0.358, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.351, 10.100], loss: 0.003065, mae: 0.061071, mean_q: 0.129406
 16952/100000: episode: 338, duration: 0.040s, episode steps: 5, steps per second: 125, episode reward: 1.965, mean reward: 0.393 [0.367, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.419, 10.100], loss: 0.003709, mae: 0.067774, mean_q: -0.067088
 16957/100000: episode: 339, duration: 0.036s, episode steps: 5, steps per second: 141, episode reward: 2.572, mean reward: 0.514 [0.428, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.444, 10.100], loss: 0.003756, mae: 0.064315, mean_q: 0.028993
 16962/100000: episode: 340, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 2.053, mean reward: 0.411 [0.393, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.454, 10.100], loss: 0.003853, mae: 0.063694, mean_q: -0.000107
 16968/100000: episode: 341, duration: 0.040s, episode steps: 6, steps per second: 149, episode reward: 2.002, mean reward: 0.334 [0.270, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.464, 10.100], loss: 0.003237, mae: 0.058750, mean_q: 0.019988
 16973/100000: episode: 342, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 2.381, mean reward: 0.476 [0.442, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.435, 10.100], loss: 0.003812, mae: 0.062218, mean_q: 0.029408
 16978/100000: episode: 343, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 2.085, mean reward: 0.417 [0.353, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.495, 10.100], loss: 0.003973, mae: 0.064100, mean_q: 0.073624
 16984/100000: episode: 344, duration: 0.041s, episode steps: 6, steps per second: 148, episode reward: 1.997, mean reward: 0.333 [0.267, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.451, 10.100], loss: 0.003705, mae: 0.066567, mean_q: -0.041364
 16989/100000: episode: 345, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 1.830, mean reward: 0.366 [0.337, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.521, 10.100], loss: 0.003644, mae: 0.065519, mean_q: 0.138609
 16994/100000: episode: 346, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 2.033, mean reward: 0.407 [0.372, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.523, 10.100], loss: 0.002540, mae: 0.050835, mean_q: -0.010055
 17000/100000: episode: 347, duration: 0.045s, episode steps: 6, steps per second: 132, episode reward: 2.190, mean reward: 0.365 [0.313, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.505, 10.100], loss: 0.003144, mae: 0.060094, mean_q: 0.094874
 17006/100000: episode: 348, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 2.623, mean reward: 0.437 [0.394, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.368, 10.100], loss: 0.003659, mae: 0.064404, mean_q: -0.006010
 17011/100000: episode: 349, duration: 0.037s, episode steps: 5, steps per second: 135, episode reward: 1.938, mean reward: 0.388 [0.360, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.534, 10.100], loss: 0.003938, mae: 0.066123, mean_q: 0.116145
 17016/100000: episode: 350, duration: 0.041s, episode steps: 5, steps per second: 121, episode reward: 1.863, mean reward: 0.373 [0.357, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.504, 10.100], loss: 0.003453, mae: 0.061005, mean_q: 0.083353
 17021/100000: episode: 351, duration: 0.037s, episode steps: 5, steps per second: 134, episode reward: 1.962, mean reward: 0.392 [0.304, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.475, 10.100], loss: 0.003699, mae: 0.069242, mean_q: 0.127946
 17026/100000: episode: 352, duration: 0.036s, episode steps: 5, steps per second: 137, episode reward: 2.013, mean reward: 0.403 [0.374, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.495, 10.100], loss: 0.002765, mae: 0.057304, mean_q: 0.092346
 17031/100000: episode: 353, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 1.911, mean reward: 0.382 [0.287, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.421, 10.100], loss: 0.003638, mae: 0.060015, mean_q: 0.114503
 17037/100000: episode: 354, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 1.942, mean reward: 0.324 [0.309, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.343, 10.100], loss: 0.004383, mae: 0.070384, mean_q: -0.037906
 17042/100000: episode: 355, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 1.711, mean reward: 0.342 [0.301, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.717, 10.100], loss: 0.003310, mae: 0.059723, mean_q: 0.000299
 17047/100000: episode: 356, duration: 0.040s, episode steps: 5, steps per second: 124, episode reward: 2.342, mean reward: 0.468 [0.435, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.411, 10.100], loss: 0.003238, mae: 0.062307, mean_q: 0.029139
 17052/100000: episode: 357, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 2.098, mean reward: 0.420 [0.381, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.433, 10.100], loss: 0.003816, mae: 0.067743, mean_q: -0.008542
 17057/100000: episode: 358, duration: 0.036s, episode steps: 5, steps per second: 140, episode reward: 1.764, mean reward: 0.353 [0.330, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.402, 10.100], loss: 0.003798, mae: 0.063975, mean_q: 0.054893
 17062/100000: episode: 359, duration: 0.039s, episode steps: 5, steps per second: 130, episode reward: 1.936, mean reward: 0.387 [0.368, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.467, 10.100], loss: 0.004403, mae: 0.070683, mean_q: 0.078743
 17067/100000: episode: 360, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 1.984, mean reward: 0.397 [0.373, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.466, 10.100], loss: 0.003945, mae: 0.062676, mean_q: 0.075884
 17073/100000: episode: 361, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 2.492, mean reward: 0.415 [0.322, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.491, 10.100], loss: 0.004443, mae: 0.071639, mean_q: 0.012677
 17078/100000: episode: 362, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 2.049, mean reward: 0.410 [0.362, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.425, 10.100], loss: 0.004516, mae: 0.073570, mean_q: 0.049988
 17083/100000: episode: 363, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 2.225, mean reward: 0.445 [0.402, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.399, 10.100], loss: 0.004620, mae: 0.072235, mean_q: -0.031966
 17088/100000: episode: 364, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 2.173, mean reward: 0.435 [0.366, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.365, 10.100], loss: 0.004101, mae: 0.069994, mean_q: 0.135850
 17093/100000: episode: 365, duration: 0.044s, episode steps: 5, steps per second: 115, episode reward: 1.764, mean reward: 0.353 [0.322, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.386, 10.100], loss: 0.003498, mae: 0.067420, mean_q: 0.090402
 17098/100000: episode: 366, duration: 0.036s, episode steps: 5, steps per second: 138, episode reward: 2.645, mean reward: 0.529 [0.500, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.672, 10.100], loss: 0.003369, mae: 0.060928, mean_q: 0.127257
 17103/100000: episode: 367, duration: 0.043s, episode steps: 5, steps per second: 117, episode reward: 2.249, mean reward: 0.450 [0.332, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.495, 10.100], loss: 0.004081, mae: 0.071783, mean_q: 0.183894
 17108/100000: episode: 368, duration: 0.047s, episode steps: 5, steps per second: 105, episode reward: 2.283, mean reward: 0.457 [0.413, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.496, 10.100], loss: 0.003604, mae: 0.066030, mean_q: 0.089391
 17115/100000: episode: 369, duration: 0.064s, episode steps: 7, steps per second: 110, episode reward: 2.430, mean reward: 0.347 [0.218, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.423, 10.100], loss: 0.003213, mae: 0.061447, mean_q: 0.070686
 17120/100000: episode: 370, duration: 0.051s, episode steps: 5, steps per second: 99, episode reward: 2.369, mean reward: 0.474 [0.437, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.509, 10.100], loss: 0.003715, mae: 0.066058, mean_q: 0.058585
 17127/100000: episode: 371, duration: 0.066s, episode steps: 7, steps per second: 106, episode reward: 2.274, mean reward: 0.325 [0.176, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.473, 10.100], loss: 0.004284, mae: 0.072265, mean_q: 0.084700
 17133/100000: episode: 372, duration: 0.063s, episode steps: 6, steps per second: 96, episode reward: 2.076, mean reward: 0.346 [0.295, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.451, 10.100], loss: 0.003703, mae: 0.065186, mean_q: 0.125510
 17140/100000: episode: 373, duration: 0.072s, episode steps: 7, steps per second: 97, episode reward: 1.756, mean reward: 0.251 [0.189, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.409, 10.100], loss: 0.003233, mae: 0.060925, mean_q: 0.161013
 17146/100000: episode: 374, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 2.427, mean reward: 0.405 [0.293, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.396, 10.100], loss: 0.003692, mae: 0.064941, mean_q: 0.093750
 17152/100000: episode: 375, duration: 0.070s, episode steps: 6, steps per second: 85, episode reward: 2.199, mean reward: 0.367 [0.327, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.415, 10.100], loss: 0.003602, mae: 0.063363, mean_q: 0.042512
 17158/100000: episode: 376, duration: 0.061s, episode steps: 6, steps per second: 98, episode reward: 2.725, mean reward: 0.454 [0.303, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.495, 10.100], loss: 0.004429, mae: 0.073131, mean_q: 0.070329
 17163/100000: episode: 377, duration: 0.049s, episode steps: 5, steps per second: 103, episode reward: 2.317, mean reward: 0.463 [0.419, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.315, 10.100], loss: 0.003341, mae: 0.062963, mean_q: 0.021248
 17169/100000: episode: 378, duration: 0.047s, episode steps: 6, steps per second: 127, episode reward: 2.024, mean reward: 0.337 [0.313, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.582, 10.100], loss: 0.004234, mae: 0.073702, mean_q: 0.165964
 17174/100000: episode: 379, duration: 0.043s, episode steps: 5, steps per second: 117, episode reward: 1.792, mean reward: 0.358 [0.317, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.359, 10.100], loss: 0.004171, mae: 0.069680, mean_q: 0.125881
 17179/100000: episode: 380, duration: 0.041s, episode steps: 5, steps per second: 121, episode reward: 2.428, mean reward: 0.486 [0.459, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.437, 10.100], loss: 0.004856, mae: 0.072268, mean_q: 0.053931
 17185/100000: episode: 381, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 2.708, mean reward: 0.451 [0.376, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.486, 10.100], loss: 0.005649, mae: 0.080877, mean_q: 0.061514
 17190/100000: episode: 382, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 2.229, mean reward: 0.446 [0.378, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.404, 10.100], loss: 0.004942, mae: 0.073666, mean_q: 0.153035
 17195/100000: episode: 383, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 2.108, mean reward: 0.422 [0.383, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.400, 10.100], loss: 0.004410, mae: 0.071368, mean_q: 0.114096
 17200/100000: episode: 384, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 2.334, mean reward: 0.467 [0.432, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.421, 10.100], loss: 0.003398, mae: 0.062149, mean_q: 0.043245
 17205/100000: episode: 385, duration: 0.041s, episode steps: 5, steps per second: 122, episode reward: 2.350, mean reward: 0.470 [0.449, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.356, 10.100], loss: 0.004225, mae: 0.070926, mean_q: 0.102664
 17212/100000: episode: 386, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 1.732, mean reward: 0.247 [0.170, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.369, 10.100], loss: 0.004111, mae: 0.066796, mean_q: 0.100846
 17218/100000: episode: 387, duration: 0.045s, episode steps: 6, steps per second: 132, episode reward: 2.151, mean reward: 0.359 [0.333, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.407, 10.100], loss: 0.004298, mae: 0.070215, mean_q: 0.156622
 17224/100000: episode: 388, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 2.139, mean reward: 0.357 [0.271, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.536, 10.100], loss: 0.003983, mae: 0.066678, mean_q: 0.101838
 17229/100000: episode: 389, duration: 0.038s, episode steps: 5, steps per second: 132, episode reward: 2.166, mean reward: 0.433 [0.375, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.481, 10.100], loss: 0.003591, mae: 0.067880, mean_q: 0.123545
 17235/100000: episode: 390, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 2.073, mean reward: 0.346 [0.307, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.466, 10.100], loss: 0.003345, mae: 0.060758, mean_q: 0.147535
 17240/100000: episode: 391, duration: 0.036s, episode steps: 5, steps per second: 138, episode reward: 2.032, mean reward: 0.406 [0.361, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.491, 10.100], loss: 0.004176, mae: 0.067486, mean_q: -0.038537
 17247/100000: episode: 392, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 2.718, mean reward: 0.388 [0.228, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.458, 10.100], loss: 0.003480, mae: 0.059452, mean_q: 0.074395
 17252/100000: episode: 393, duration: 0.043s, episode steps: 5, steps per second: 116, episode reward: 2.108, mean reward: 0.422 [0.341, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.521, 10.100], loss: 0.003523, mae: 0.064393, mean_q: 0.112211
 17258/100000: episode: 394, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 2.229, mean reward: 0.371 [0.322, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.454, 10.100], loss: 0.002955, mae: 0.057892, mean_q: 0.030910
 17264/100000: episode: 395, duration: 0.034s, episode steps: 6, steps per second: 176, episode reward: 2.270, mean reward: 0.378 [0.343, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.499, 10.100], loss: 0.004296, mae: 0.066762, mean_q: 0.135012
 17269/100000: episode: 396, duration: 0.038s, episode steps: 5, steps per second: 131, episode reward: 2.473, mean reward: 0.495 [0.487, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.504, 10.100], loss: 0.003943, mae: 0.066856, mean_q: 0.175055
 17274/100000: episode: 397, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 1.870, mean reward: 0.374 [0.317, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.336, 10.100], loss: 0.003995, mae: 0.070936, mean_q: 0.035385
 17281/100000: episode: 398, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 2.317, mean reward: 0.331 [0.256, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.405, 10.100], loss: 0.003419, mae: 0.064034, mean_q: 0.099352
 17286/100000: episode: 399, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 2.649, mean reward: 0.530 [0.439, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.464, 10.100], loss: 0.004072, mae: 0.066229, mean_q: 0.132617
 17292/100000: episode: 400, duration: 0.042s, episode steps: 6, steps per second: 143, episode reward: 2.254, mean reward: 0.376 [0.295, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.533, 10.100], loss: 0.003200, mae: 0.059519, mean_q: 0.099843
 17297/100000: episode: 401, duration: 0.049s, episode steps: 5, steps per second: 102, episode reward: 2.118, mean reward: 0.424 [0.396, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.465, 10.100], loss: 0.004561, mae: 0.076778, mean_q: 0.114564
 17302/100000: episode: 402, duration: 0.049s, episode steps: 5, steps per second: 102, episode reward: 2.315, mean reward: 0.463 [0.402, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.516, 10.100], loss: 0.003520, mae: 0.064193, mean_q: 0.015672
 17307/100000: episode: 403, duration: 0.049s, episode steps: 5, steps per second: 102, episode reward: 2.433, mean reward: 0.487 [0.456, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.398, 10.100], loss: 0.004911, mae: 0.072793, mean_q: 0.043373
 17312/100000: episode: 404, duration: 0.041s, episode steps: 5, steps per second: 123, episode reward: 2.313, mean reward: 0.463 [0.425, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.480, 10.100], loss: 0.004966, mae: 0.070764, mean_q: 0.102396
 17318/100000: episode: 405, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 2.417, mean reward: 0.403 [0.343, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.553, 10.100], loss: 0.003998, mae: 0.070313, mean_q: 0.144947
 17323/100000: episode: 406, duration: 0.040s, episode steps: 5, steps per second: 126, episode reward: 2.193, mean reward: 0.439 [0.374, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.293, 10.100], loss: 0.003491, mae: 0.063620, mean_q: 0.003428
 17328/100000: episode: 407, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 2.361, mean reward: 0.472 [0.425, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.360, 10.100], loss: 0.003771, mae: 0.063082, mean_q: 0.021176
 17333/100000: episode: 408, duration: 0.035s, episode steps: 5, steps per second: 144, episode reward: 2.540, mean reward: 0.508 [0.434, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.373, 10.100], loss: 0.007507, mae: 0.075769, mean_q: 0.068817
 17338/100000: episode: 409, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 2.328, mean reward: 0.466 [0.429, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.396, 10.100], loss: 0.006399, mae: 0.083209, mean_q: 0.180810
 17343/100000: episode: 410, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 1.580, mean reward: 0.316 [0.293, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.446, 10.100], loss: 0.004490, mae: 0.074723, mean_q: 0.106741
 17349/100000: episode: 411, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 1.890, mean reward: 0.315 [0.282, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.451, 10.100], loss: 0.005959, mae: 0.084894, mean_q: 0.160651
 17354/100000: episode: 412, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 2.458, mean reward: 0.492 [0.439, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.525, 10.100], loss: 0.004609, mae: 0.072832, mean_q: 0.080237
 17359/100000: episode: 413, duration: 0.038s, episode steps: 5, steps per second: 132, episode reward: 2.075, mean reward: 0.415 [0.392, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.459, 10.100], loss: 0.004232, mae: 0.070023, mean_q: 0.143750
 17364/100000: episode: 414, duration: 0.036s, episode steps: 5, steps per second: 138, episode reward: 1.858, mean reward: 0.372 [0.323, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-1.838, 10.100], loss: 0.004245, mae: 0.070507, mean_q: 0.262103
 17370/100000: episode: 415, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 2.283, mean reward: 0.381 [0.348, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.367, 10.100], loss: 0.004331, mae: 0.073685, mean_q: 0.196852
 17375/100000: episode: 416, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 2.240, mean reward: 0.448 [0.423, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.502, 10.100], loss: 0.003827, mae: 0.067380, mean_q: 0.191982
 17380/100000: episode: 417, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 2.380, mean reward: 0.476 [0.430, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.451, 10.100], loss: 0.002799, mae: 0.056535, mean_q: 0.105945
 17385/100000: episode: 418, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 2.678, mean reward: 0.536 [0.418, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.549, 10.100], loss: 0.002944, mae: 0.058426, mean_q: 0.125613
 17392/100000: episode: 419, duration: 0.049s, episode steps: 7, steps per second: 143, episode reward: 2.225, mean reward: 0.318 [0.292, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.444, 10.100], loss: 0.003460, mae: 0.063240, mean_q: 0.204434
[Info] 400-TH LEVEL FOUND: 0.8384971618652344, Considering 10/90 traces
 17397/100000: episode: 420, duration: 4.413s, episode steps: 5, steps per second: 1, episode reward: 1.921, mean reward: 0.384 [0.373, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.636, 10.100], loss: 0.004960, mae: 0.072155, mean_q: 0.108207
 17401/100000: episode: 421, duration: 0.032s, episode steps: 4, steps per second: 126, episode reward: 1.441, mean reward: 0.360 [0.300, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.423, 10.100], loss: 0.003638, mae: 0.065791, mean_q: 0.172240
 17405/100000: episode: 422, duration: 0.027s, episode steps: 4, steps per second: 149, episode reward: 1.703, mean reward: 0.426 [0.397, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.584, 10.100], loss: 0.003941, mae: 0.068068, mean_q: 0.119450
 17410/100000: episode: 423, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 2.475, mean reward: 0.495 [0.422, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.566, 10.100], loss: 0.003690, mae: 0.065121, mean_q: 0.121012
 17414/100000: episode: 424, duration: 0.032s, episode steps: 4, steps per second: 126, episode reward: 1.317, mean reward: 0.329 [0.275, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.572, 10.100], loss: 0.003827, mae: 0.068144, mean_q: 0.221296
 17418/100000: episode: 425, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 1.795, mean reward: 0.449 [0.373, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.456, 10.100], loss: 0.003776, mae: 0.066630, mean_q: 0.125062
 17422/100000: episode: 426, duration: 0.033s, episode steps: 4, steps per second: 121, episode reward: 1.915, mean reward: 0.479 [0.434, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.644, 10.100], loss: 0.002944, mae: 0.059424, mean_q: 0.095358
 17426/100000: episode: 427, duration: 0.030s, episode steps: 4, steps per second: 133, episode reward: 1.252, mean reward: 0.313 [0.287, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.430, 10.100], loss: 0.002879, mae: 0.060447, mean_q: 0.090206
 17430/100000: episode: 428, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 1.522, mean reward: 0.380 [0.363, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.500, 10.100], loss: 0.003484, mae: 0.063936, mean_q: 0.291895
 17434/100000: episode: 429, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 1.373, mean reward: 0.343 [0.301, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.668, 10.100], loss: 0.003745, mae: 0.064857, mean_q: 0.088717
 17438/100000: episode: 430, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 1.527, mean reward: 0.382 [0.361, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.503, 10.100], loss: 0.004427, mae: 0.073442, mean_q: 0.000689
 17442/100000: episode: 431, duration: 0.028s, episode steps: 4, steps per second: 142, episode reward: 1.585, mean reward: 0.396 [0.358, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.470, 10.100], loss: 0.002956, mae: 0.059521, mean_q: 0.274314
 17447/100000: episode: 432, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 2.244, mean reward: 0.449 [0.389, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.560, 10.100], loss: 0.002699, mae: 0.057852, mean_q: 0.068738
 17451/100000: episode: 433, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 1.727, mean reward: 0.432 [0.377, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.568, 10.100], loss: 0.004085, mae: 0.065343, mean_q: 0.033205
 17455/100000: episode: 434, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 1.302, mean reward: 0.326 [0.293, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.545, 10.100], loss: 0.003622, mae: 0.066616, mean_q: 0.230370
 17459/100000: episode: 435, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 1.366, mean reward: 0.341 [0.300, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.532, 10.100], loss: 0.003284, mae: 0.061648, mean_q: 0.141708
 17463/100000: episode: 436, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 1.381, mean reward: 0.345 [0.319, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.572, 10.100], loss: 0.003505, mae: 0.066026, mean_q: 0.226960
 17467/100000: episode: 437, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 1.602, mean reward: 0.401 [0.335, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.469, 10.100], loss: 0.003535, mae: 0.061777, mean_q: 0.091883
 17471/100000: episode: 438, duration: 0.027s, episode steps: 4, steps per second: 151, episode reward: 1.522, mean reward: 0.381 [0.346, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.530, 10.100], loss: 0.003412, mae: 0.063221, mean_q: 0.169754
 17475/100000: episode: 439, duration: 0.030s, episode steps: 4, steps per second: 132, episode reward: 1.606, mean reward: 0.402 [0.388, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.610, 10.100], loss: 0.003628, mae: 0.065323, mean_q: 0.129204
 17479/100000: episode: 440, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 2.141, mean reward: 0.535 [0.435, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.547, 10.100], loss: 0.005150, mae: 0.073819, mean_q: 0.183943
 17483/100000: episode: 441, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: 1.555, mean reward: 0.389 [0.363, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.413, 10.100], loss: 0.003261, mae: 0.063598, mean_q: 0.153125
 17487/100000: episode: 442, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 1.538, mean reward: 0.384 [0.306, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.474, 10.100], loss: 0.002799, mae: 0.057569, mean_q: 0.055775
 17492/100000: episode: 443, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 2.191, mean reward: 0.438 [0.391, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.510, 10.100], loss: 0.003464, mae: 0.063356, mean_q: 0.111694
 17496/100000: episode: 444, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 1.477, mean reward: 0.369 [0.307, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.555, 10.100], loss: 0.003322, mae: 0.064858, mean_q: 0.146286
 17501/100000: episode: 445, duration: 0.039s, episode steps: 5, steps per second: 128, episode reward: 2.956, mean reward: 0.591 [0.523, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.605, 10.100], loss: 0.003985, mae: 0.070207, mean_q: 0.164930
 17506/100000: episode: 446, duration: 0.037s, episode steps: 5, steps per second: 134, episode reward: 2.033, mean reward: 0.407 [0.362, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.444, 10.100], loss: 0.003939, mae: 0.066935, mean_q: 0.198247
 17511/100000: episode: 447, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 1.994, mean reward: 0.399 [0.297, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-1.672, 10.100], loss: 0.003969, mae: 0.067676, mean_q: 0.166465
 17515/100000: episode: 448, duration: 0.029s, episode steps: 4, steps per second: 137, episode reward: 2.080, mean reward: 0.520 [0.472, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.547, 10.100], loss: 0.003838, mae: 0.066583, mean_q: 0.194540
 17519/100000: episode: 449, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 1.495, mean reward: 0.374 [0.352, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.479, 10.100], loss: 0.004406, mae: 0.073261, mean_q: 0.301891
 17523/100000: episode: 450, duration: 0.024s, episode steps: 4, steps per second: 164, episode reward: 1.912, mean reward: 0.478 [0.439, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.552, 10.100], loss: 0.005872, mae: 0.083364, mean_q: 0.250667
 17527/100000: episode: 451, duration: 0.023s, episode steps: 4, steps per second: 170, episode reward: 1.530, mean reward: 0.382 [0.307, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.698, 10.100], loss: 0.004862, mae: 0.078345, mean_q: 0.201626
 17531/100000: episode: 452, duration: 0.034s, episode steps: 4, steps per second: 117, episode reward: 2.106, mean reward: 0.527 [0.472, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.557, 10.100], loss: 0.005231, mae: 0.075142, mean_q: 0.191870
 17535/100000: episode: 453, duration: 0.024s, episode steps: 4, steps per second: 164, episode reward: 1.457, mean reward: 0.364 [0.344, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.480, 10.100], loss: 0.005133, mae: 0.077708, mean_q: 0.141969
 17539/100000: episode: 454, duration: 0.030s, episode steps: 4, steps per second: 135, episode reward: 1.656, mean reward: 0.414 [0.374, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.471, 10.100], loss: 0.004395, mae: 0.070586, mean_q: 0.154944
 17543/100000: episode: 455, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 1.611, mean reward: 0.403 [0.372, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.509, 10.100], loss: 0.004920, mae: 0.076483, mean_q: 0.235678
 17547/100000: episode: 456, duration: 0.032s, episode steps: 4, steps per second: 126, episode reward: 1.309, mean reward: 0.327 [0.299, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.451, 10.100], loss: 0.004435, mae: 0.073463, mean_q: 0.303063
 17551/100000: episode: 457, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 1.424, mean reward: 0.356 [0.286, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.462, 10.100], loss: 0.003473, mae: 0.064874, mean_q: 0.186309
 17555/100000: episode: 458, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 1.511, mean reward: 0.378 [0.337, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.545, 10.100], loss: 0.003395, mae: 0.063614, mean_q: 0.197293
 17559/100000: episode: 459, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 1.858, mean reward: 0.464 [0.430, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.495, 10.100], loss: 0.003645, mae: 0.065819, mean_q: 0.250325
 17563/100000: episode: 460, duration: 0.031s, episode steps: 4, steps per second: 130, episode reward: 1.587, mean reward: 0.397 [0.303, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.467, 10.100], loss: 0.003396, mae: 0.062454, mean_q: 0.119355
 17568/100000: episode: 461, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 2.227, mean reward: 0.445 [0.384, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.799, 10.100], loss: 0.004238, mae: 0.066629, mean_q: 0.072050
 17572/100000: episode: 462, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 1.352, mean reward: 0.338 [0.287, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.498, 10.100], loss: 0.003903, mae: 0.065446, mean_q: 0.131074
 17576/100000: episode: 463, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 1.372, mean reward: 0.343 [0.256, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.424, 10.100], loss: 0.004439, mae: 0.064926, mean_q: 0.202104
 17580/100000: episode: 464, duration: 0.028s, episode steps: 4, steps per second: 140, episode reward: 1.449, mean reward: 0.362 [0.256, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.504, 10.100], loss: 0.003619, mae: 0.066201, mean_q: 0.161376
 17584/100000: episode: 465, duration: 0.029s, episode steps: 4, steps per second: 140, episode reward: 1.812, mean reward: 0.453 [0.416, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.469, 10.100], loss: 0.003825, mae: 0.063797, mean_q: 0.160828
 17588/100000: episode: 466, duration: 0.028s, episode steps: 4, steps per second: 145, episode reward: 1.457, mean reward: 0.364 [0.355, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.521, 10.100], loss: 0.003879, mae: 0.065239, mean_q: 0.105226
 17592/100000: episode: 467, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 1.696, mean reward: 0.424 [0.397, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.599, 10.100], loss: 0.003883, mae: 0.068649, mean_q: 0.224815
 17596/100000: episode: 468, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 1.557, mean reward: 0.389 [0.379, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.432, 10.100], loss: 0.003995, mae: 0.072451, mean_q: 0.192108
 17600/100000: episode: 469, duration: 0.029s, episode steps: 4, steps per second: 137, episode reward: 1.794, mean reward: 0.448 [0.434, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.521, 10.100], loss: 0.004164, mae: 0.067674, mean_q: 0.159752
 17604/100000: episode: 470, duration: 0.036s, episode steps: 4, steps per second: 110, episode reward: 1.210, mean reward: 0.303 [0.263, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.470, 10.100], loss: 0.002875, mae: 0.060549, mean_q: 0.134189
 17608/100000: episode: 471, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 1.915, mean reward: 0.479 [0.410, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.559, 10.100], loss: 0.002774, mae: 0.057527, mean_q: 0.090990
 17612/100000: episode: 472, duration: 0.025s, episode steps: 4, steps per second: 157, episode reward: 1.550, mean reward: 0.387 [0.344, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.502, 10.100], loss: 0.003480, mae: 0.065497, mean_q: 0.175092
 17616/100000: episode: 473, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 1.853, mean reward: 0.463 [0.425, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.601, 10.100], loss: 0.002902, mae: 0.058620, mean_q: 0.158922
 17620/100000: episode: 474, duration: 0.030s, episode steps: 4, steps per second: 134, episode reward: 1.675, mean reward: 0.419 [0.367, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.543, 10.100], loss: 0.003961, mae: 0.067557, mean_q: 0.013587
 17625/100000: episode: 475, duration: 0.053s, episode steps: 5, steps per second: 95, episode reward: 2.233, mean reward: 0.447 [0.381, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.537, 10.100], loss: 0.004219, mae: 0.073187, mean_q: 0.280523
 17629/100000: episode: 476, duration: 0.044s, episode steps: 4, steps per second: 91, episode reward: 2.028, mean reward: 0.507 [0.451, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.593, 10.100], loss: 0.003993, mae: 0.066624, mean_q: 0.188026
 17633/100000: episode: 477, duration: 0.042s, episode steps: 4, steps per second: 95, episode reward: 1.416, mean reward: 0.354 [0.330, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.476, 10.100], loss: 0.003316, mae: 0.062456, mean_q: 0.155353
 17637/100000: episode: 478, duration: 0.043s, episode steps: 4, steps per second: 94, episode reward: 1.645, mean reward: 0.411 [0.304, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.453, 10.100], loss: 0.003474, mae: 0.065779, mean_q: 0.231636
 17641/100000: episode: 479, duration: 0.030s, episode steps: 4, steps per second: 135, episode reward: 1.474, mean reward: 0.368 [0.308, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.461, 10.100], loss: 0.003045, mae: 0.058966, mean_q: 0.142970
 17645/100000: episode: 480, duration: 0.030s, episode steps: 4, steps per second: 134, episode reward: 1.491, mean reward: 0.373 [0.340, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.405, 10.100], loss: 0.003817, mae: 0.064646, mean_q: 0.137064
 17649/100000: episode: 481, duration: 0.037s, episode steps: 4, steps per second: 109, episode reward: 1.433, mean reward: 0.358 [0.319, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.522, 10.100], loss: 0.004102, mae: 0.067257, mean_q: 0.046265
 17654/100000: episode: 482, duration: 0.042s, episode steps: 5, steps per second: 118, episode reward: 2.181, mean reward: 0.436 [0.340, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.516, 10.100], loss: 0.003176, mae: 0.059199, mean_q: -0.005359
 17658/100000: episode: 483, duration: 0.033s, episode steps: 4, steps per second: 123, episode reward: 1.573, mean reward: 0.393 [0.269, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.460, 10.100], loss: 0.003506, mae: 0.064485, mean_q: 0.192464
 17663/100000: episode: 484, duration: 0.048s, episode steps: 5, steps per second: 105, episode reward: 1.990, mean reward: 0.398 [0.372, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.972, 10.100], loss: 0.003761, mae: 0.065227, mean_q: 0.261992
 17667/100000: episode: 485, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 1.395, mean reward: 0.349 [0.322, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.447, 10.100], loss: 0.004254, mae: 0.073564, mean_q: 0.162944
 17671/100000: episode: 486, duration: 0.042s, episode steps: 4, steps per second: 96, episode reward: 1.548, mean reward: 0.387 [0.362, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.489, 10.100], loss: 0.003376, mae: 0.064209, mean_q: 0.120704
 17675/100000: episode: 487, duration: 0.032s, episode steps: 4, steps per second: 126, episode reward: 1.826, mean reward: 0.457 [0.430, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.403, 10.100], loss: 0.004545, mae: 0.077382, mean_q: 0.266448
 17680/100000: episode: 488, duration: 0.045s, episode steps: 5, steps per second: 112, episode reward: 2.139, mean reward: 0.428 [0.309, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.537, 10.100], loss: 0.003390, mae: 0.066897, mean_q: 0.176415
 17684/100000: episode: 489, duration: 0.032s, episode steps: 4, steps per second: 126, episode reward: 1.729, mean reward: 0.432 [0.398, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.582, 10.100], loss: 0.005629, mae: 0.073595, mean_q: 0.224411
 17689/100000: episode: 490, duration: 0.040s, episode steps: 5, steps per second: 124, episode reward: 2.433, mean reward: 0.487 [0.393, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.463, 10.100], loss: 0.006139, mae: 0.072085, mean_q: 0.161417
 17693/100000: episode: 491, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 1.278, mean reward: 0.319 [0.310, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.517, 10.100], loss: 0.003876, mae: 0.069313, mean_q: 0.338662
 17698/100000: episode: 492, duration: 0.037s, episode steps: 5, steps per second: 135, episode reward: 2.002, mean reward: 0.400 [0.347, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.535, 10.100], loss: 0.003291, mae: 0.066086, mean_q: 0.265528
 17702/100000: episode: 493, duration: 0.041s, episode steps: 4, steps per second: 99, episode reward: 1.848, mean reward: 0.462 [0.441, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.476, 10.100], loss: 0.004970, mae: 0.077088, mean_q: 0.166911
 17706/100000: episode: 494, duration: 0.044s, episode steps: 4, steps per second: 91, episode reward: 1.793, mean reward: 0.448 [0.431, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.586, 10.100], loss: 0.004524, mae: 0.075319, mean_q: 0.286337
 17711/100000: episode: 495, duration: 0.041s, episode steps: 5, steps per second: 123, episode reward: 2.040, mean reward: 0.408 [0.373, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.876, 10.100], loss: 0.007440, mae: 0.074754, mean_q: 0.239161
 17716/100000: episode: 496, duration: 0.055s, episode steps: 5, steps per second: 92, episode reward: 2.301, mean reward: 0.460 [0.420, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.498, 10.100], loss: 0.004195, mae: 0.066793, mean_q: 0.175309
 17720/100000: episode: 497, duration: 0.033s, episode steps: 4, steps per second: 120, episode reward: 1.587, mean reward: 0.397 [0.386, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.532, 10.100], loss: 0.005720, mae: 0.082393, mean_q: 0.277244
 17724/100000: episode: 498, duration: 0.033s, episode steps: 4, steps per second: 121, episode reward: 1.533, mean reward: 0.383 [0.371, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.534, 10.100], loss: 0.007058, mae: 0.087906, mean_q: 0.218206
 17728/100000: episode: 499, duration: 0.031s, episode steps: 4, steps per second: 131, episode reward: 1.537, mean reward: 0.384 [0.316, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.506, 10.100], loss: 0.010045, mae: 0.089034, mean_q: 0.133615
 17732/100000: episode: 500, duration: 0.047s, episode steps: 4, steps per second: 86, episode reward: 1.356, mean reward: 0.339 [0.274, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.682, 10.100], loss: 0.004411, mae: 0.078644, mean_q: 0.284418
 17736/100000: episode: 501, duration: 0.038s, episode steps: 4, steps per second: 106, episode reward: 1.487, mean reward: 0.372 [0.314, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.479, 10.100], loss: 0.004040, mae: 0.070758, mean_q: 0.296724
 17740/100000: episode: 502, duration: 0.050s, episode steps: 4, steps per second: 80, episode reward: 1.691, mean reward: 0.423 [0.402, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.591, 10.100], loss: 0.004488, mae: 0.073882, mean_q: 0.056317
 17744/100000: episode: 503, duration: 0.044s, episode steps: 4, steps per second: 92, episode reward: 1.276, mean reward: 0.319 [0.296, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.422, 10.100], loss: 0.003675, mae: 0.066547, mean_q: 0.178752
 17749/100000: episode: 504, duration: 0.041s, episode steps: 5, steps per second: 123, episode reward: 2.258, mean reward: 0.452 [0.400, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.461, 10.100], loss: 0.003886, mae: 0.070297, mean_q: 0.239471
 17753/100000: episode: 505, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 1.546, mean reward: 0.386 [0.357, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.423, 10.100], loss: 0.003963, mae: 0.067705, mean_q: 0.209451
 17758/100000: episode: 506, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 2.008, mean reward: 0.402 [0.377, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.594, 10.100], loss: 0.002988, mae: 0.058888, mean_q: 0.190151
 17763/100000: episode: 507, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 2.041, mean reward: 0.408 [0.367, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.428, 10.100], loss: 0.003220, mae: 0.062996, mean_q: 0.222968
 17767/100000: episode: 508, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 1.878, mean reward: 0.470 [0.439, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.462, 10.100], loss: 0.002641, mae: 0.056237, mean_q: 0.256184
 17771/100000: episode: 509, duration: 0.023s, episode steps: 4, steps per second: 178, episode reward: 1.373, mean reward: 0.343 [0.322, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.458, 10.100], loss: 0.003637, mae: 0.067085, mean_q: 0.192358
[Info] 500-TH LEVEL FOUND: 0.840461790561676, Considering 10/90 traces
 17775/100000: episode: 510, duration: 4.587s, episode steps: 4, steps per second: 1, episode reward: 1.668, mean reward: 0.417 [0.383, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.494, 10.100], loss: 0.003708, mae: 0.064643, mean_q: 0.302263
 17779/100000: episode: 511, duration: 0.043s, episode steps: 4, steps per second: 94, episode reward: 2.127, mean reward: 0.532 [0.503, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.632, 10.100], loss: 0.004101, mae: 0.069217, mean_q: 0.212165
 17782/100000: episode: 512, duration: 0.028s, episode steps: 3, steps per second: 109, episode reward: 1.647, mean reward: 0.549 [0.496, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.538, 10.100], loss: 0.003996, mae: 0.066415, mean_q: 0.292517
 17786/100000: episode: 513, duration: 0.036s, episode steps: 4, steps per second: 111, episode reward: 1.835, mean reward: 0.459 [0.394, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.517, 10.100], loss: 0.003584, mae: 0.069356, mean_q: 0.291656
 17790/100000: episode: 514, duration: 0.036s, episode steps: 4, steps per second: 110, episode reward: 1.636, mean reward: 0.409 [0.372, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.541, 10.100], loss: 0.004253, mae: 0.072677, mean_q: 0.220348
 17793/100000: episode: 515, duration: 0.022s, episode steps: 3, steps per second: 137, episode reward: 1.214, mean reward: 0.405 [0.376, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.553, 10.100], loss: 0.003100, mae: 0.059535, mean_q: 0.032231
 17796/100000: episode: 516, duration: 0.030s, episode steps: 3, steps per second: 102, episode reward: 1.441, mean reward: 0.480 [0.440, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.538, 10.100], loss: 0.003196, mae: 0.058074, mean_q: 0.155104
 17800/100000: episode: 517, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 1.628, mean reward: 0.407 [0.384, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.448, 10.100], loss: 0.003014, mae: 0.060160, mean_q: 0.256888
 17804/100000: episode: 518, duration: 0.035s, episode steps: 4, steps per second: 113, episode reward: 1.874, mean reward: 0.468 [0.442, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.589, 10.100], loss: 0.004106, mae: 0.071031, mean_q: 0.157691
 17807/100000: episode: 519, duration: 0.024s, episode steps: 3, steps per second: 126, episode reward: 1.259, mean reward: 0.420 [0.393, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.603, 10.100], loss: 0.004817, mae: 0.073628, mean_q: 0.333354
 17810/100000: episode: 520, duration: 0.026s, episode steps: 3, steps per second: 114, episode reward: 1.467, mean reward: 0.489 [0.450, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.558, 10.100], loss: 0.003622, mae: 0.066402, mean_q: 0.199887
 17813/100000: episode: 521, duration: 0.021s, episode steps: 3, steps per second: 142, episode reward: 1.110, mean reward: 0.370 [0.302, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.535, 10.100], loss: 0.003550, mae: 0.064183, mean_q: 0.169425
 17816/100000: episode: 522, duration: 0.026s, episode steps: 3, steps per second: 116, episode reward: 1.174, mean reward: 0.391 [0.367, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.609, 10.100], loss: 0.003799, mae: 0.068553, mean_q: 0.254320
 17820/100000: episode: 523, duration: 0.030s, episode steps: 4, steps per second: 133, episode reward: 1.750, mean reward: 0.438 [0.390, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.521, 10.100], loss: 0.003038, mae: 0.059997, mean_q: 0.312508
 17824/100000: episode: 524, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 1.575, mean reward: 0.394 [0.376, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.497, 10.100], loss: 0.003152, mae: 0.065105, mean_q: 0.232934
 17828/100000: episode: 525, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 1.997, mean reward: 0.499 [0.470, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.537, 10.100], loss: 0.003695, mae: 0.063977, mean_q: 0.332144
 17831/100000: episode: 526, duration: 0.022s, episode steps: 3, steps per second: 134, episode reward: 1.291, mean reward: 0.430 [0.414, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.544, 10.100], loss: 0.005356, mae: 0.082821, mean_q: 0.335641
 17835/100000: episode: 527, duration: 0.025s, episode steps: 4, steps per second: 163, episode reward: 2.089, mean reward: 0.522 [0.466, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.590, 10.100], loss: 0.003837, mae: 0.062701, mean_q: 0.149881
 17838/100000: episode: 528, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 1.322, mean reward: 0.441 [0.416, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.632, 10.100], loss: 0.004458, mae: 0.074814, mean_q: 0.165890
 17842/100000: episode: 529, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 1.463, mean reward: 0.366 [0.360, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.524, 10.100], loss: 0.005970, mae: 0.073493, mean_q: 0.331446
 17845/100000: episode: 530, duration: 0.025s, episode steps: 3, steps per second: 122, episode reward: 1.555, mean reward: 0.518 [0.458, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.195, 10.100], loss: 0.002481, mae: 0.058808, mean_q: 0.317926
 17848/100000: episode: 531, duration: 0.025s, episode steps: 3, steps per second: 118, episode reward: 0.962, mean reward: 0.321 [0.297, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.528, 10.100], loss: 0.003962, mae: 0.070047, mean_q: 0.229653
 17852/100000: episode: 532, duration: 0.037s, episode steps: 4, steps per second: 109, episode reward: 1.851, mean reward: 0.463 [0.424, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.525, 10.100], loss: 0.004249, mae: 0.074297, mean_q: 0.245763
 17856/100000: episode: 533, duration: 0.034s, episode steps: 4, steps per second: 119, episode reward: 1.752, mean reward: 0.438 [0.363, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.647, 10.100], loss: 0.004328, mae: 0.077057, mean_q: 0.189855
 17859/100000: episode: 534, duration: 0.030s, episode steps: 3, steps per second: 100, episode reward: 1.507, mean reward: 0.502 [0.452, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.514, 10.100], loss: 0.003130, mae: 0.064106, mean_q: 0.453454
 17863/100000: episode: 535, duration: 0.036s, episode steps: 4, steps per second: 110, episode reward: 1.846, mean reward: 0.462 [0.428, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.567, 10.100], loss: 0.003667, mae: 0.068096, mean_q: 0.384118
 17866/100000: episode: 536, duration: 0.036s, episode steps: 3, steps per second: 83, episode reward: 1.344, mean reward: 0.448 [0.437, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.490, 10.100], loss: 0.002408, mae: 0.051395, mean_q: 0.142180
 17870/100000: episode: 537, duration: 0.043s, episode steps: 4, steps per second: 94, episode reward: 1.825, mean reward: 0.456 [0.365, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.676, 10.100], loss: 0.003585, mae: 0.066239, mean_q: 0.264698
 17873/100000: episode: 538, duration: 0.029s, episode steps: 3, steps per second: 104, episode reward: 1.749, mean reward: 0.583 [0.476, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.549, 10.100], loss: 0.003215, mae: 0.063534, mean_q: 0.095095
 17877/100000: episode: 539, duration: 0.038s, episode steps: 4, steps per second: 106, episode reward: 1.698, mean reward: 0.424 [0.363, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.473, 10.100], loss: 0.003627, mae: 0.067276, mean_q: 0.297789
 17881/100000: episode: 540, duration: 0.042s, episode steps: 4, steps per second: 95, episode reward: 1.735, mean reward: 0.434 [0.392, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.664, 10.100], loss: 0.003730, mae: 0.062456, mean_q: 0.241075
 17885/100000: episode: 541, duration: 0.033s, episode steps: 4, steps per second: 121, episode reward: 1.642, mean reward: 0.410 [0.326, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.519, 10.100], loss: 0.002957, mae: 0.061340, mean_q: 0.230902
 17889/100000: episode: 542, duration: 0.045s, episode steps: 4, steps per second: 89, episode reward: 1.595, mean reward: 0.399 [0.334, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.516, 10.100], loss: 0.003420, mae: 0.063625, mean_q: 0.234088
 17892/100000: episode: 543, duration: 0.032s, episode steps: 3, steps per second: 95, episode reward: 1.307, mean reward: 0.436 [0.421, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.545, 10.100], loss: 0.003457, mae: 0.068063, mean_q: 0.327157
 17895/100000: episode: 544, duration: 0.025s, episode steps: 3, steps per second: 121, episode reward: 1.234, mean reward: 0.411 [0.358, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.551, 10.100], loss: 0.003636, mae: 0.065771, mean_q: 0.262529
 17899/100000: episode: 545, duration: 0.026s, episode steps: 4, steps per second: 151, episode reward: 2.074, mean reward: 0.519 [0.509, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.556, 10.100], loss: 0.003291, mae: 0.061377, mean_q: 0.318376
 17902/100000: episode: 546, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 1.229, mean reward: 0.410 [0.382, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.590, 10.100], loss: 0.004774, mae: 0.075876, mean_q: 0.176128
 17906/100000: episode: 547, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 1.573, mean reward: 0.393 [0.374, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.488, 10.100], loss: 0.003850, mae: 0.064765, mean_q: 0.307879
 17910/100000: episode: 548, duration: 0.033s, episode steps: 4, steps per second: 120, episode reward: 2.282, mean reward: 0.571 [0.538, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.564, 10.100], loss: 0.003549, mae: 0.067480, mean_q: 0.131955
 17914/100000: episode: 549, duration: 0.028s, episode steps: 4, steps per second: 142, episode reward: 2.044, mean reward: 0.511 [0.445, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.536, 10.100], loss: 0.003481, mae: 0.063443, mean_q: 0.345660
 17918/100000: episode: 550, duration: 0.037s, episode steps: 4, steps per second: 108, episode reward: 1.787, mean reward: 0.447 [0.368, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.587, 10.100], loss: 0.002778, mae: 0.062581, mean_q: 0.275070
 17922/100000: episode: 551, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 1.865, mean reward: 0.466 [0.411, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.489, 10.100], loss: 0.003804, mae: 0.070750, mean_q: 0.284191
 17926/100000: episode: 552, duration: 0.044s, episode steps: 4, steps per second: 91, episode reward: 1.715, mean reward: 0.429 [0.399, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.567, 10.100], loss: 0.003738, mae: 0.070270, mean_q: 0.188708
 17930/100000: episode: 553, duration: 0.033s, episode steps: 4, steps per second: 122, episode reward: 1.515, mean reward: 0.379 [0.363, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.553, 10.100], loss: 0.003675, mae: 0.067242, mean_q: 0.279662
 17934/100000: episode: 554, duration: 0.036s, episode steps: 4, steps per second: 111, episode reward: 1.658, mean reward: 0.414 [0.399, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.625, 10.100], loss: 0.003989, mae: 0.070730, mean_q: 0.254370
 17937/100000: episode: 555, duration: 0.026s, episode steps: 3, steps per second: 114, episode reward: 1.516, mean reward: 0.505 [0.492, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.614, 10.100], loss: 0.004713, mae: 0.073526, mean_q: 0.249150
 17941/100000: episode: 556, duration: 0.031s, episode steps: 4, steps per second: 128, episode reward: 1.667, mean reward: 0.417 [0.352, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.596, 10.100], loss: 0.003409, mae: 0.063167, mean_q: 0.172007
 17945/100000: episode: 557, duration: 0.034s, episode steps: 4, steps per second: 119, episode reward: 1.701, mean reward: 0.425 [0.372, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.462, 10.100], loss: 0.004562, mae: 0.071074, mean_q: 0.336527
 17948/100000: episode: 558, duration: 0.029s, episode steps: 3, steps per second: 102, episode reward: 1.389, mean reward: 0.463 [0.432, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.584, 10.100], loss: 0.004210, mae: 0.071824, mean_q: 0.142769
 17951/100000: episode: 559, duration: 0.025s, episode steps: 3, steps per second: 118, episode reward: 1.643, mean reward: 0.548 [0.472, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.490, 10.100], loss: 0.004430, mae: 0.073723, mean_q: 0.307797
 17954/100000: episode: 560, duration: 0.031s, episode steps: 3, steps per second: 95, episode reward: 1.531, mean reward: 0.510 [0.437, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.620, 10.100], loss: 0.004084, mae: 0.072561, mean_q: 0.324672
 17957/100000: episode: 561, duration: 0.026s, episode steps: 3, steps per second: 113, episode reward: 1.388, mean reward: 0.463 [0.429, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.588, 10.100], loss: 0.005865, mae: 0.084752, mean_q: 0.302054
 17961/100000: episode: 562, duration: 0.035s, episode steps: 4, steps per second: 114, episode reward: 1.616, mean reward: 0.404 [0.383, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.498, 10.100], loss: 0.004557, mae: 0.073456, mean_q: 0.264873
 17964/100000: episode: 563, duration: 0.021s, episode steps: 3, steps per second: 144, episode reward: 1.211, mean reward: 0.404 [0.378, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.471, 10.100], loss: 0.005839, mae: 0.080848, mean_q: 0.261860
 17967/100000: episode: 564, duration: 0.022s, episode steps: 3, steps per second: 134, episode reward: 1.195, mean reward: 0.398 [0.357, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.540, 10.100], loss: 0.005858, mae: 0.080582, mean_q: 0.199246
 17970/100000: episode: 565, duration: 0.020s, episode steps: 3, steps per second: 150, episode reward: 1.261, mean reward: 0.420 [0.383, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.600, 10.100], loss: 0.002982, mae: 0.060999, mean_q: 0.363234
 17974/100000: episode: 566, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: 2.130, mean reward: 0.532 [0.469, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.557, 10.100], loss: 0.004590, mae: 0.075727, mean_q: 0.238501
 17977/100000: episode: 567, duration: 0.026s, episode steps: 3, steps per second: 117, episode reward: 1.481, mean reward: 0.494 [0.477, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.585, 10.100], loss: 0.003892, mae: 0.064347, mean_q: 0.245762
 17981/100000: episode: 568, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 2.130, mean reward: 0.532 [0.512, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.573, 10.100], loss: 0.003769, mae: 0.066970, mean_q: 0.246166
 17984/100000: episode: 569, duration: 0.022s, episode steps: 3, steps per second: 139, episode reward: 1.346, mean reward: 0.449 [0.434, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.125, 10.100], loss: 0.004284, mae: 0.070432, mean_q: 0.257902
 17988/100000: episode: 570, duration: 0.030s, episode steps: 4, steps per second: 132, episode reward: 1.605, mean reward: 0.401 [0.392, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.677, 10.100], loss: 0.003260, mae: 0.061946, mean_q: 0.226440
 17992/100000: episode: 571, duration: 0.047s, episode steps: 4, steps per second: 85, episode reward: 1.498, mean reward: 0.375 [0.314, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.589, 10.100], loss: 0.003265, mae: 0.063074, mean_q: 0.364472
 17995/100000: episode: 572, duration: 0.034s, episode steps: 3, steps per second: 88, episode reward: 1.192, mean reward: 0.397 [0.393, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.510, 10.100], loss: 0.005567, mae: 0.078920, mean_q: 0.274239
 17998/100000: episode: 573, duration: 0.046s, episode steps: 3, steps per second: 65, episode reward: 1.719, mean reward: 0.573 [0.504, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.807, 10.100], loss: 0.004070, mae: 0.072135, mean_q: 0.339414
 18001/100000: episode: 574, duration: 0.037s, episode steps: 3, steps per second: 81, episode reward: 1.315, mean reward: 0.438 [0.401, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.603, 10.100], loss: 0.003388, mae: 0.060335, mean_q: 0.257197
 18005/100000: episode: 575, duration: 0.034s, episode steps: 4, steps per second: 117, episode reward: 1.762, mean reward: 0.441 [0.414, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.595, 10.100], loss: 0.002833, mae: 0.057550, mean_q: 0.202269
 18009/100000: episode: 576, duration: 0.044s, episode steps: 4, steps per second: 90, episode reward: 1.506, mean reward: 0.376 [0.360, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.577, 10.100], loss: 0.004310, mae: 0.071827, mean_q: 0.319479
 18012/100000: episode: 577, duration: 0.026s, episode steps: 3, steps per second: 116, episode reward: 1.293, mean reward: 0.431 [0.403, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.583, 10.100], loss: 0.003732, mae: 0.069085, mean_q: 0.334697
 18016/100000: episode: 578, duration: 0.045s, episode steps: 4, steps per second: 89, episode reward: 2.327, mean reward: 0.582 [0.532, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.540, 10.100], loss: 0.002776, mae: 0.053550, mean_q: 0.274922
 18020/100000: episode: 579, duration: 0.041s, episode steps: 4, steps per second: 97, episode reward: 1.530, mean reward: 0.382 [0.360, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.538, 10.100], loss: 0.003067, mae: 0.062194, mean_q: 0.371027
 18024/100000: episode: 580, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 1.948, mean reward: 0.487 [0.412, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.579, 10.100], loss: 0.003287, mae: 0.061163, mean_q: 0.118880
 18028/100000: episode: 581, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 1.271, mean reward: 0.318 [0.292, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.545, 10.100], loss: 0.003268, mae: 0.062702, mean_q: 0.299419
 18032/100000: episode: 582, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 2.023, mean reward: 0.506 [0.483, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.657, 10.100], loss: 0.003330, mae: 0.065239, mean_q: 0.397649
 18036/100000: episode: 583, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 1.319, mean reward: 0.330 [0.306, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.541, 10.100], loss: 0.003189, mae: 0.063141, mean_q: 0.305255
 18040/100000: episode: 584, duration: 0.030s, episode steps: 4, steps per second: 135, episode reward: 1.850, mean reward: 0.462 [0.442, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.528, 10.100], loss: 0.004371, mae: 0.074807, mean_q: 0.353778
 18043/100000: episode: 585, duration: 0.018s, episode steps: 3, steps per second: 162, episode reward: 1.462, mean reward: 0.487 [0.435, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.594, 10.100], loss: 0.003602, mae: 0.066864, mean_q: 0.234954
 18047/100000: episode: 586, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 1.414, mean reward: 0.354 [0.319, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.576, 10.100], loss: 0.003275, mae: 0.064192, mean_q: 0.313981
 18051/100000: episode: 587, duration: 0.030s, episode steps: 4, steps per second: 135, episode reward: 1.766, mean reward: 0.441 [0.368, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.576, 10.100], loss: 0.002835, mae: 0.059599, mean_q: 0.228197
 18055/100000: episode: 588, duration: 0.032s, episode steps: 4, steps per second: 126, episode reward: 1.478, mean reward: 0.369 [0.343, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.433, 10.100], loss: 0.003910, mae: 0.064005, mean_q: 0.166779
 18059/100000: episode: 589, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 1.912, mean reward: 0.478 [0.442, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.552, 10.100], loss: 0.003321, mae: 0.064565, mean_q: 0.334212
 18062/100000: episode: 590, duration: 0.020s, episode steps: 3, steps per second: 151, episode reward: 1.388, mean reward: 0.463 [0.444, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.575, 10.100], loss: 0.003449, mae: 0.065214, mean_q: 0.264954
 18066/100000: episode: 591, duration: 0.034s, episode steps: 4, steps per second: 118, episode reward: 1.823, mean reward: 0.456 [0.391, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.868, 10.100], loss: 0.004038, mae: 0.067170, mean_q: 0.255981
 18069/100000: episode: 592, duration: 0.023s, episode steps: 3, steps per second: 131, episode reward: 1.534, mean reward: 0.511 [0.490, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.530, 10.100], loss: 0.003974, mae: 0.069398, mean_q: 0.212129
 18072/100000: episode: 593, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 1.288, mean reward: 0.429 [0.389, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.576, 10.100], loss: 0.003270, mae: 0.063958, mean_q: 0.436791
 18076/100000: episode: 594, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 1.872, mean reward: 0.468 [0.390, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.580, 10.100], loss: 0.003324, mae: 0.063634, mean_q: 0.312512
 18080/100000: episode: 595, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 1.761, mean reward: 0.440 [0.383, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.516, 10.100], loss: 0.004363, mae: 0.070279, mean_q: 0.362381
 18084/100000: episode: 596, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 1.563, mean reward: 0.391 [0.356, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.500, 10.100], loss: 0.003457, mae: 0.065190, mean_q: 0.315185
 18088/100000: episode: 597, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 1.647, mean reward: 0.412 [0.394, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.555, 10.100], loss: 0.003500, mae: 0.066706, mean_q: 0.344059
 18092/100000: episode: 598, duration: 0.031s, episode steps: 4, steps per second: 128, episode reward: 2.121, mean reward: 0.530 [0.475, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.534, 10.100], loss: 0.003295, mae: 0.064992, mean_q: 0.208897
 18095/100000: episode: 599, duration: 0.024s, episode steps: 3, steps per second: 123, episode reward: 1.556, mean reward: 0.519 [0.459, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.527, 10.100], loss: 0.003944, mae: 0.070649, mean_q: 0.331575
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.840461790561676
 18098/100000: episode: 600, duration: 4.131s, episode steps: 3, steps per second: 1, episode reward: 1.391, mean reward: 0.464 [0.419, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.586, 10.100], loss: 0.003085, mae: 0.063280, mean_q: 0.389171
 18198/100000: episode: 601, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.264, mean reward: -0.173 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.719, 10.181], loss: 0.003484, mae: 0.064725, mean_q: 0.333301
 18298/100000: episode: 602, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.317, mean reward: -0.193 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.535, 10.098], loss: 0.003838, mae: 0.067320, mean_q: 0.289824
 18398/100000: episode: 603, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.730, mean reward: -0.167 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.306, 10.098], loss: 0.003973, mae: 0.067775, mean_q: 0.321414
 18498/100000: episode: 604, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -16.237, mean reward: -0.162 [-1.000, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.662, 10.098], loss: 0.003490, mae: 0.065648, mean_q: 0.326608
 18598/100000: episode: 605, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: -17.672, mean reward: -0.177 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.569, 10.279], loss: 0.003768, mae: 0.067570, mean_q: 0.315718
 18698/100000: episode: 606, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: -17.093, mean reward: -0.171 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.443, 10.374], loss: 0.003378, mae: 0.064208, mean_q: 0.299369
 18798/100000: episode: 607, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: -19.283, mean reward: -0.193 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.913, 10.181], loss: 0.003232, mae: 0.061828, mean_q: 0.278044
 18898/100000: episode: 608, duration: 1.274s, episode steps: 100, steps per second: 78, episode reward: -19.569, mean reward: -0.196 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.534, 10.098], loss: 0.003210, mae: 0.062092, mean_q: 0.307644
 18998/100000: episode: 609, duration: 1.481s, episode steps: 100, steps per second: 68, episode reward: -15.540, mean reward: -0.155 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.226, 10.430], loss: 0.003269, mae: 0.062613, mean_q: 0.322407
 19098/100000: episode: 610, duration: 1.302s, episode steps: 100, steps per second: 77, episode reward: -18.779, mean reward: -0.188 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.373, 10.098], loss: 0.003455, mae: 0.063955, mean_q: 0.285418
 19198/100000: episode: 611, duration: 2.780s, episode steps: 100, steps per second: 36, episode reward: -18.532, mean reward: -0.185 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.393, 10.105], loss: 0.003530, mae: 0.064648, mean_q: 0.313180
 19298/100000: episode: 612, duration: 1.830s, episode steps: 100, steps per second: 55, episode reward: -15.844, mean reward: -0.158 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.341, 10.355], loss: 0.003772, mae: 0.064539, mean_q: 0.289903
 19398/100000: episode: 613, duration: 2.355s, episode steps: 100, steps per second: 42, episode reward: -14.238, mean reward: -0.142 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.841, 10.338], loss: 0.003296, mae: 0.063287, mean_q: 0.325505
 19498/100000: episode: 614, duration: 2.749s, episode steps: 100, steps per second: 36, episode reward: -18.050, mean reward: -0.181 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.738, 10.098], loss: 0.003581, mae: 0.064137, mean_q: 0.305246
 19598/100000: episode: 615, duration: 2.216s, episode steps: 100, steps per second: 45, episode reward: -8.344, mean reward: -0.083 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.235, 10.472], loss: 0.003139, mae: 0.061846, mean_q: 0.306589
 19698/100000: episode: 616, duration: 3.035s, episode steps: 100, steps per second: 33, episode reward: -19.460, mean reward: -0.195 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.160, 10.124], loss: 0.003626, mae: 0.065241, mean_q: 0.280124
 19798/100000: episode: 617, duration: 1.856s, episode steps: 100, steps per second: 54, episode reward: -15.689, mean reward: -0.157 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.839, 10.098], loss: 0.003293, mae: 0.060926, mean_q: 0.260179
 19898/100000: episode: 618, duration: 1.786s, episode steps: 100, steps per second: 56, episode reward: -16.684, mean reward: -0.167 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.816, 10.098], loss: 0.002958, mae: 0.060167, mean_q: 0.237180
 19998/100000: episode: 619, duration: 1.547s, episode steps: 100, steps per second: 65, episode reward: -12.445, mean reward: -0.124 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.564, 10.098], loss: 0.003272, mae: 0.062183, mean_q: 0.222651
 20098/100000: episode: 620, duration: 1.309s, episode steps: 100, steps per second: 76, episode reward: -20.991, mean reward: -0.210 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.281, 10.098], loss: 0.003311, mae: 0.063017, mean_q: 0.223021
 20198/100000: episode: 621, duration: 1.244s, episode steps: 100, steps per second: 80, episode reward: -20.643, mean reward: -0.206 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.960, 10.146], loss: 0.005196, mae: 0.071209, mean_q: 0.184568
 20298/100000: episode: 622, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: -19.087, mean reward: -0.191 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.391, 10.127], loss: 0.003259, mae: 0.062274, mean_q: 0.183210
 20398/100000: episode: 623, duration: 0.669s, episode steps: 100, steps per second: 149, episode reward: -15.182, mean reward: -0.152 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.023, 10.165], loss: 0.003131, mae: 0.060900, mean_q: 0.161095
 20498/100000: episode: 624, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.765, mean reward: -0.178 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.333, 10.098], loss: 0.003161, mae: 0.060850, mean_q: 0.128939
 20598/100000: episode: 625, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -14.594, mean reward: -0.146 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.424, 10.206], loss: 0.004248, mae: 0.066956, mean_q: 0.139516
 20698/100000: episode: 626, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: -16.648, mean reward: -0.166 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.044, 10.225], loss: 0.002906, mae: 0.057115, mean_q: 0.093470
 20798/100000: episode: 627, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -16.449, mean reward: -0.164 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.356, 10.264], loss: 0.003699, mae: 0.064243, mean_q: 0.074804
 20898/100000: episode: 628, duration: 0.761s, episode steps: 100, steps per second: 131, episode reward: -15.245, mean reward: -0.152 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.017, 10.098], loss: 0.002919, mae: 0.058610, mean_q: 0.069556
 20998/100000: episode: 629, duration: 1.182s, episode steps: 100, steps per second: 85, episode reward: -17.199, mean reward: -0.172 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.814, 10.098], loss: 0.002940, mae: 0.058193, mean_q: 0.059382
 21098/100000: episode: 630, duration: 1.253s, episode steps: 100, steps per second: 80, episode reward: -17.888, mean reward: -0.179 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.776, 10.187], loss: 0.003069, mae: 0.058828, mean_q: 0.021892
 21198/100000: episode: 631, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -16.175, mean reward: -0.162 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.939, 10.222], loss: 0.003739, mae: 0.063915, mean_q: 0.030624
 21298/100000: episode: 632, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.170, mean reward: -0.132 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.378, 10.308], loss: 0.003178, mae: 0.060210, mean_q: -0.031970
 21398/100000: episode: 633, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: -17.871, mean reward: -0.179 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.731, 10.163], loss: 0.002836, mae: 0.056256, mean_q: -0.031663
 21498/100000: episode: 634, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: -15.162, mean reward: -0.152 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.812, 10.255], loss: 0.002930, mae: 0.057566, mean_q: -0.059137
 21598/100000: episode: 635, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: -17.440, mean reward: -0.174 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.442, 10.234], loss: 0.002998, mae: 0.058398, mean_q: -0.052696
 21698/100000: episode: 636, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: -12.436, mean reward: -0.124 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.658, 10.098], loss: 0.002942, mae: 0.057579, mean_q: -0.069620
 21798/100000: episode: 637, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: -18.858, mean reward: -0.189 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.376, 10.424], loss: 0.002629, mae: 0.054217, mean_q: -0.116206
 21898/100000: episode: 638, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -20.391, mean reward: -0.204 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.611, 10.098], loss: 0.003178, mae: 0.060187, mean_q: -0.097078
 21998/100000: episode: 639, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: -17.620, mean reward: -0.176 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.686, 10.098], loss: 0.003365, mae: 0.060920, mean_q: -0.124113
 22098/100000: episode: 640, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: -18.893, mean reward: -0.189 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.504, 10.167], loss: 0.002887, mae: 0.056518, mean_q: -0.168069
 22198/100000: episode: 641, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: -18.412, mean reward: -0.184 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.090, 10.098], loss: 0.003031, mae: 0.057458, mean_q: -0.152506
 22298/100000: episode: 642, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.729, mean reward: -0.177 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.040, 10.098], loss: 0.002829, mae: 0.054930, mean_q: -0.210035
 22398/100000: episode: 643, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: -19.164, mean reward: -0.192 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.602, 10.201], loss: 0.002928, mae: 0.057013, mean_q: -0.240870
 22498/100000: episode: 644, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: -13.377, mean reward: -0.134 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.967, 10.098], loss: 0.002771, mae: 0.055548, mean_q: -0.203883
 22598/100000: episode: 645, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -19.608, mean reward: -0.196 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.097, 10.180], loss: 0.003785, mae: 0.062483, mean_q: -0.277664
 22698/100000: episode: 646, duration: 0.697s, episode steps: 100, steps per second: 143, episode reward: -13.959, mean reward: -0.140 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.314, 10.098], loss: 0.002712, mae: 0.054946, mean_q: -0.249761
 22798/100000: episode: 647, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: -16.692, mean reward: -0.167 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.080, 10.098], loss: 0.002822, mae: 0.054957, mean_q: -0.263922
 22898/100000: episode: 648, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: -15.477, mean reward: -0.155 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.041, 10.098], loss: 0.003252, mae: 0.057518, mean_q: -0.323786
 22998/100000: episode: 649, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: -19.076, mean reward: -0.191 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.471, 10.098], loss: 0.005186, mae: 0.068529, mean_q: -0.336962
 23098/100000: episode: 650, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: -16.675, mean reward: -0.167 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.781, 10.098], loss: 0.003206, mae: 0.057181, mean_q: -0.298154
 23198/100000: episode: 651, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.968, mean reward: -0.160 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.026, 10.098], loss: 0.002698, mae: 0.052759, mean_q: -0.349986
 23298/100000: episode: 652, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -12.534, mean reward: -0.125 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.033, 10.304], loss: 0.002879, mae: 0.055135, mean_q: -0.338509
 23398/100000: episode: 653, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.581, mean reward: -0.176 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.728, 10.098], loss: 0.002878, mae: 0.054752, mean_q: -0.334676
 23498/100000: episode: 654, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -18.028, mean reward: -0.180 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.387, 10.098], loss: 0.002740, mae: 0.053945, mean_q: -0.319275
 23598/100000: episode: 655, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -14.514, mean reward: -0.145 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.021, 10.252], loss: 0.002638, mae: 0.052070, mean_q: -0.335974
 23698/100000: episode: 656, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -12.829, mean reward: -0.128 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.417, 10.179], loss: 0.002546, mae: 0.051111, mean_q: -0.314609
 23798/100000: episode: 657, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -17.317, mean reward: -0.173 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.558, 10.271], loss: 0.002624, mae: 0.052164, mean_q: -0.317387
 23898/100000: episode: 658, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.536, mean reward: -0.175 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.776, 10.160], loss: 0.002911, mae: 0.054659, mean_q: -0.325439
 23998/100000: episode: 659, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -15.486, mean reward: -0.155 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.087, 10.230], loss: 0.002787, mae: 0.054271, mean_q: -0.303828
 24098/100000: episode: 660, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -10.350, mean reward: -0.104 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.672, 10.098], loss: 0.002672, mae: 0.052704, mean_q: -0.310891
 24198/100000: episode: 661, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.363, mean reward: -0.184 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.935, 10.098], loss: 0.002699, mae: 0.052806, mean_q: -0.335427
 24298/100000: episode: 662, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: -14.194, mean reward: -0.142 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.010, 10.448], loss: 0.002598, mae: 0.051763, mean_q: -0.339746
 24398/100000: episode: 663, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -17.318, mean reward: -0.173 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.413, 10.160], loss: 0.002690, mae: 0.052982, mean_q: -0.312821
 24498/100000: episode: 664, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: -12.146, mean reward: -0.121 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.273, 10.098], loss: 0.002568, mae: 0.052109, mean_q: -0.318931
 24598/100000: episode: 665, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -17.095, mean reward: -0.171 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.658, 10.098], loss: 0.002649, mae: 0.052536, mean_q: -0.301386
 24698/100000: episode: 666, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: -15.192, mean reward: -0.152 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.735, 10.098], loss: 0.002608, mae: 0.052447, mean_q: -0.297665
 24798/100000: episode: 667, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -9.828, mean reward: -0.098 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.495, 10.098], loss: 0.002696, mae: 0.052270, mean_q: -0.321223
 24898/100000: episode: 668, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: -16.315, mean reward: -0.163 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.420, 10.114], loss: 0.002852, mae: 0.052472, mean_q: -0.302047
 24998/100000: episode: 669, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: -15.980, mean reward: -0.160 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.840, 10.098], loss: 0.003776, mae: 0.066373, mean_q: -0.314014
 25098/100000: episode: 670, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.123, mean reward: -0.181 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.691, 10.328], loss: 0.004052, mae: 0.064683, mean_q: -0.304811
 25198/100000: episode: 671, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -16.825, mean reward: -0.168 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.450, 10.365], loss: 0.002619, mae: 0.051631, mean_q: -0.319908
 25298/100000: episode: 672, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.775, mean reward: -0.168 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.381, 10.098], loss: 0.002785, mae: 0.053027, mean_q: -0.293512
 25398/100000: episode: 673, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.036, mean reward: -0.160 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.490, 10.185], loss: 0.002598, mae: 0.051474, mean_q: -0.299295
 25498/100000: episode: 674, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.050, mean reward: -0.180 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.759, 10.268], loss: 0.002643, mae: 0.052175, mean_q: -0.274589
 25598/100000: episode: 675, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -15.561, mean reward: -0.156 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.832, 10.388], loss: 0.002576, mae: 0.051265, mean_q: -0.301771
 25698/100000: episode: 676, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.465, mean reward: -0.175 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.465, 10.324], loss: 0.002626, mae: 0.051835, mean_q: -0.317280
 25798/100000: episode: 677, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -13.520, mean reward: -0.135 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.226, 10.098], loss: 0.002547, mae: 0.051080, mean_q: -0.315700
 25898/100000: episode: 678, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.973, mean reward: -0.180 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.356, 10.098], loss: 0.002552, mae: 0.050519, mean_q: -0.306438
 25998/100000: episode: 679, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -16.898, mean reward: -0.169 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.766, 10.343], loss: 0.002492, mae: 0.050459, mean_q: -0.304122
 26098/100000: episode: 680, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.354, mean reward: -0.154 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.033, 10.098], loss: 0.002671, mae: 0.052966, mean_q: -0.279841
 26198/100000: episode: 681, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.550, mean reward: -0.195 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.755, 10.098], loss: 0.002808, mae: 0.052992, mean_q: -0.276173
 26298/100000: episode: 682, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.383, mean reward: -0.194 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.776, 10.098], loss: 0.002610, mae: 0.051378, mean_q: -0.312143
 26398/100000: episode: 683, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: -18.011, mean reward: -0.180 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.985, 10.285], loss: 0.002750, mae: 0.054727, mean_q: -0.332119
 26498/100000: episode: 684, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: -17.093, mean reward: -0.171 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.012, 10.098], loss: 0.002627, mae: 0.050868, mean_q: -0.327428
 26598/100000: episode: 685, duration: 1.117s, episode steps: 100, steps per second: 90, episode reward: -14.808, mean reward: -0.148 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.531, 10.167], loss: 0.004635, mae: 0.066305, mean_q: -0.316973
 26698/100000: episode: 686, duration: 0.772s, episode steps: 100, steps per second: 130, episode reward: -19.825, mean reward: -0.198 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.814, 10.098], loss: 0.002603, mae: 0.052346, mean_q: -0.312329
 26798/100000: episode: 687, duration: 0.685s, episode steps: 100, steps per second: 146, episode reward: -18.794, mean reward: -0.188 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.625, 10.098], loss: 0.002691, mae: 0.052432, mean_q: -0.283167
 26898/100000: episode: 688, duration: 0.710s, episode steps: 100, steps per second: 141, episode reward: -17.940, mean reward: -0.179 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.418, 10.098], loss: 0.002569, mae: 0.050428, mean_q: -0.342238
 26998/100000: episode: 689, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: -19.527, mean reward: -0.195 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.732, 10.098], loss: 0.002463, mae: 0.050139, mean_q: -0.322797
 27098/100000: episode: 690, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.652, mean reward: -0.187 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.659, 10.098], loss: 0.002657, mae: 0.052211, mean_q: -0.313458
 27198/100000: episode: 691, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: -9.613, mean reward: -0.096 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.665, 10.098], loss: 0.002784, mae: 0.053004, mean_q: -0.306005
 27298/100000: episode: 692, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: -14.205, mean reward: -0.142 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.329, 10.303], loss: 0.002617, mae: 0.052006, mean_q: -0.300011
 27398/100000: episode: 693, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: -13.501, mean reward: -0.135 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.832, 10.322], loss: 0.002594, mae: 0.050996, mean_q: -0.302383
 27498/100000: episode: 694, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.458, mean reward: -0.175 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.353, 10.099], loss: 0.002606, mae: 0.052446, mean_q: -0.281349
 27598/100000: episode: 695, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: -19.488, mean reward: -0.195 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.950, 10.124], loss: 0.002585, mae: 0.051406, mean_q: -0.287514
 27698/100000: episode: 696, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: -19.212, mean reward: -0.192 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.840, 10.098], loss: 0.002620, mae: 0.052060, mean_q: -0.267461
 27798/100000: episode: 697, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.256, mean reward: -0.183 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.572, 10.098], loss: 0.002571, mae: 0.051452, mean_q: -0.304617
 27898/100000: episode: 698, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.512, mean reward: -0.185 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.069, 10.098], loss: 0.002735, mae: 0.053048, mean_q: -0.316940
 27998/100000: episode: 699, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -20.186, mean reward: -0.202 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.309, 10.098], loss: 0.002615, mae: 0.050679, mean_q: -0.325843
[Info] 100-TH LEVEL FOUND: 0.5709433555603027, Considering 10/90 traces
 28098/100000: episode: 700, duration: 4.855s, episode steps: 100, steps per second: 21, episode reward: -17.632, mean reward: -0.176 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.431, 10.098], loss: 0.002591, mae: 0.050528, mean_q: -0.330463
 28114/100000: episode: 701, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 4.776, mean reward: 0.299 [0.203, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.304, 10.100], loss: 0.002575, mae: 0.052957, mean_q: -0.261901
 28128/100000: episode: 702, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 5.010, mean reward: 0.358 [0.245, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.663, 10.100], loss: 0.002447, mae: 0.050375, mean_q: -0.344015
 28155/100000: episode: 703, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 9.575, mean reward: 0.355 [0.222, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.097, 10.100], loss: 0.002512, mae: 0.052108, mean_q: -0.223379
 28170/100000: episode: 704, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 5.550, mean reward: 0.370 [0.322, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.356, 10.100], loss: 0.002361, mae: 0.051398, mean_q: -0.209360
 28186/100000: episode: 705, duration: 0.107s, episode steps: 16, steps per second: 150, episode reward: 5.134, mean reward: 0.321 [0.229, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.277, 10.100], loss: 0.003136, mae: 0.058062, mean_q: -0.333475
 28201/100000: episode: 706, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 5.902, mean reward: 0.393 [0.313, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.262, 10.100], loss: 0.010212, mae: 0.096710, mean_q: -0.250594
 28208/100000: episode: 707, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 2.472, mean reward: 0.353 [0.300, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.181, 10.100], loss: 0.010817, mae: 0.106731, mean_q: -0.453294
 28215/100000: episode: 708, duration: 0.081s, episode steps: 7, steps per second: 87, episode reward: 2.792, mean reward: 0.399 [0.349, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.421, 10.100], loss: 0.007263, mae: 0.083562, mean_q: -0.359427
 28222/100000: episode: 709, duration: 0.072s, episode steps: 7, steps per second: 98, episode reward: 2.577, mean reward: 0.368 [0.277, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.479, 10.100], loss: 0.004552, mae: 0.063696, mean_q: -0.338843
 28251/100000: episode: 710, duration: 0.282s, episode steps: 29, steps per second: 103, episode reward: 7.347, mean reward: 0.253 [0.091, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.541, 10.100], loss: 0.003188, mae: 0.059965, mean_q: -0.331491
 28280/100000: episode: 711, duration: 0.178s, episode steps: 29, steps per second: 163, episode reward: 4.526, mean reward: 0.156 [0.038, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.135, 10.100], loss: 0.002772, mae: 0.054471, mean_q: -0.285148
 28307/100000: episode: 712, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 7.307, mean reward: 0.271 [0.203, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.223, 10.100], loss: 0.002603, mae: 0.052874, mean_q: -0.266151
 28314/100000: episode: 713, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 2.477, mean reward: 0.354 [0.276, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.295, 10.100], loss: 0.002499, mae: 0.047084, mean_q: -0.318310
 28321/100000: episode: 714, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 2.151, mean reward: 0.307 [0.271, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.347, 10.100], loss: 0.002883, mae: 0.055524, mean_q: -0.220856
 28348/100000: episode: 715, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 10.434, mean reward: 0.386 [0.264, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.347, 10.100], loss: 0.002677, mae: 0.053419, mean_q: -0.263580
 28364/100000: episode: 716, duration: 0.103s, episode steps: 16, steps per second: 155, episode reward: 5.910, mean reward: 0.369 [0.313, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.332, 10.100], loss: 0.002663, mae: 0.052135, mean_q: -0.225220
 28391/100000: episode: 717, duration: 0.173s, episode steps: 27, steps per second: 156, episode reward: 8.713, mean reward: 0.323 [0.169, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.850, 10.100], loss: 0.002548, mae: 0.052083, mean_q: -0.260737
 28407/100000: episode: 718, duration: 0.132s, episode steps: 16, steps per second: 121, episode reward: 6.349, mean reward: 0.397 [0.296, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.231, 10.100], loss: 0.002612, mae: 0.052811, mean_q: -0.197482
 28422/100000: episode: 719, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 4.194, mean reward: 0.280 [0.187, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.278, 10.100], loss: 0.003010, mae: 0.054295, mean_q: -0.221599
 28451/100000: episode: 720, duration: 0.174s, episode steps: 29, steps per second: 167, episode reward: 8.834, mean reward: 0.305 [0.153, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.397, 10.100], loss: 0.003022, mae: 0.054782, mean_q: -0.277487
 28480/100000: episode: 721, duration: 0.172s, episode steps: 29, steps per second: 169, episode reward: 8.048, mean reward: 0.278 [0.162, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.375, 10.100], loss: 0.002589, mae: 0.051781, mean_q: -0.212455
 28509/100000: episode: 722, duration: 0.167s, episode steps: 29, steps per second: 174, episode reward: 7.363, mean reward: 0.254 [0.150, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.433, 10.100], loss: 0.002460, mae: 0.049219, mean_q: -0.257537
 28536/100000: episode: 723, duration: 0.195s, episode steps: 27, steps per second: 138, episode reward: 7.679, mean reward: 0.284 [0.138, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.254, 10.100], loss: 0.002596, mae: 0.052884, mean_q: -0.213907
 28552/100000: episode: 724, duration: 0.092s, episode steps: 16, steps per second: 175, episode reward: 5.573, mean reward: 0.348 [0.293, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.305, 10.100], loss: 0.002711, mae: 0.053011, mean_q: -0.277945
 28579/100000: episode: 725, duration: 0.171s, episode steps: 27, steps per second: 158, episode reward: 6.523, mean reward: 0.242 [0.096, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.685, 10.100], loss: 0.002227, mae: 0.047482, mean_q: -0.272873
 28586/100000: episode: 726, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 2.853, mean reward: 0.408 [0.392, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.444, 10.100], loss: 0.002615, mae: 0.052037, mean_q: -0.131527
 28601/100000: episode: 727, duration: 0.128s, episode steps: 15, steps per second: 117, episode reward: 5.496, mean reward: 0.366 [0.305, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.297, 10.100], loss: 0.002726, mae: 0.051580, mean_q: -0.324545
 28628/100000: episode: 728, duration: 0.180s, episode steps: 27, steps per second: 150, episode reward: 8.837, mean reward: 0.327 [0.219, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.908, 10.100], loss: 0.002487, mae: 0.049075, mean_q: -0.300463
 28655/100000: episode: 729, duration: 0.162s, episode steps: 27, steps per second: 167, episode reward: 10.067, mean reward: 0.373 [0.244, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.470, 10.100], loss: 0.002981, mae: 0.054393, mean_q: -0.202711
 28671/100000: episode: 730, duration: 0.140s, episode steps: 16, steps per second: 114, episode reward: 5.558, mean reward: 0.347 [0.287, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.413, 10.100], loss: 0.003093, mae: 0.055929, mean_q: -0.183108
 28698/100000: episode: 731, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 6.561, mean reward: 0.243 [0.132, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.536, 10.100], loss: 0.002428, mae: 0.049959, mean_q: -0.189918
 28712/100000: episode: 732, duration: 0.091s, episode steps: 14, steps per second: 153, episode reward: 3.857, mean reward: 0.275 [0.158, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.330, 10.100], loss: 0.002540, mae: 0.049484, mean_q: -0.208957
 28739/100000: episode: 733, duration: 0.186s, episode steps: 27, steps per second: 145, episode reward: 6.052, mean reward: 0.224 [0.118, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.775, 10.100], loss: 0.002718, mae: 0.052535, mean_q: -0.168416
 28755/100000: episode: 734, duration: 0.103s, episode steps: 16, steps per second: 156, episode reward: 5.382, mean reward: 0.336 [0.202, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.348, 10.100], loss: 0.003403, mae: 0.059569, mean_q: -0.184695
 28762/100000: episode: 735, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 2.672, mean reward: 0.382 [0.327, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.280, 10.100], loss: 0.003772, mae: 0.062025, mean_q: -0.080714
 28789/100000: episode: 736, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 7.308, mean reward: 0.271 [0.147, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.465, 10.100], loss: 0.003300, mae: 0.058913, mean_q: -0.172681
 28816/100000: episode: 737, duration: 0.160s, episode steps: 27, steps per second: 169, episode reward: 9.067, mean reward: 0.336 [0.238, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.534, 10.100], loss: 0.002693, mae: 0.053393, mean_q: -0.194314
 28845/100000: episode: 738, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 10.828, mean reward: 0.373 [0.242, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.720, 10.100], loss: 0.002636, mae: 0.053963, mean_q: -0.124277
 28861/100000: episode: 739, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 5.579, mean reward: 0.349 [0.274, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.672, 10.100], loss: 0.002366, mae: 0.049866, mean_q: -0.138962
 28877/100000: episode: 740, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 4.303, mean reward: 0.269 [0.221, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.755, 10.100], loss: 0.002705, mae: 0.054794, mean_q: -0.125481
 28906/100000: episode: 741, duration: 0.182s, episode steps: 29, steps per second: 159, episode reward: 13.243, mean reward: 0.457 [0.225, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.106, 10.100], loss: 0.002503, mae: 0.050777, mean_q: -0.159660
 28935/100000: episode: 742, duration: 0.298s, episode steps: 29, steps per second: 97, episode reward: 8.732, mean reward: 0.301 [0.114, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.565, 10.100], loss: 0.002859, mae: 0.054398, mean_q: -0.140768
 28964/100000: episode: 743, duration: 0.296s, episode steps: 29, steps per second: 98, episode reward: 11.559, mean reward: 0.399 [0.283, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.414, 10.100], loss: 0.002792, mae: 0.053836, mean_q: -0.102331
 28993/100000: episode: 744, duration: 0.270s, episode steps: 29, steps per second: 107, episode reward: 7.964, mean reward: 0.275 [0.065, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-1.388, 10.204], loss: 0.002523, mae: 0.050871, mean_q: -0.158021
 29000/100000: episode: 745, duration: 0.053s, episode steps: 7, steps per second: 133, episode reward: 2.603, mean reward: 0.372 [0.317, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.293, 10.100], loss: 0.002289, mae: 0.049510, mean_q: -0.190092
 29029/100000: episode: 746, duration: 0.203s, episode steps: 29, steps per second: 143, episode reward: 9.660, mean reward: 0.333 [0.130, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-1.601, 10.100], loss: 0.002733, mae: 0.053177, mean_q: -0.111600
 29051/100000: episode: 747, duration: 0.202s, episode steps: 22, steps per second: 109, episode reward: 7.288, mean reward: 0.331 [0.263, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.376, 10.133], loss: 0.002777, mae: 0.053785, mean_q: -0.119252
 29078/100000: episode: 748, duration: 0.306s, episode steps: 27, steps per second: 88, episode reward: 7.593, mean reward: 0.281 [0.163, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.295, 10.100], loss: 0.003066, mae: 0.057483, mean_q: -0.155728
 29093/100000: episode: 749, duration: 0.145s, episode steps: 15, steps per second: 104, episode reward: 5.102, mean reward: 0.340 [0.288, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.439, 10.100], loss: 0.002722, mae: 0.054255, mean_q: -0.076424
 29120/100000: episode: 750, duration: 0.224s, episode steps: 27, steps per second: 121, episode reward: 7.591, mean reward: 0.281 [0.117, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.135, 10.100], loss: 0.002606, mae: 0.052286, mean_q: -0.185447
 29134/100000: episode: 751, duration: 0.116s, episode steps: 14, steps per second: 121, episode reward: 3.651, mean reward: 0.261 [0.186, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.635, 10.100], loss: 0.002641, mae: 0.052151, mean_q: -0.128704
 29161/100000: episode: 752, duration: 0.172s, episode steps: 27, steps per second: 157, episode reward: 5.187, mean reward: 0.192 [0.057, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-1.055, 10.124], loss: 0.002651, mae: 0.052595, mean_q: -0.094528
 29183/100000: episode: 753, duration: 0.153s, episode steps: 22, steps per second: 144, episode reward: 8.041, mean reward: 0.365 [0.291, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.420], loss: 0.002408, mae: 0.050046, mean_q: -0.120646
 29190/100000: episode: 754, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 2.156, mean reward: 0.308 [0.243, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.384, 10.100], loss: 0.003077, mae: 0.058622, mean_q: -0.038468
 29219/100000: episode: 755, duration: 0.218s, episode steps: 29, steps per second: 133, episode reward: 7.425, mean reward: 0.256 [0.172, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.936, 10.100], loss: 0.002962, mae: 0.055571, mean_q: -0.115816
 29246/100000: episode: 756, duration: 0.197s, episode steps: 27, steps per second: 137, episode reward: 9.394, mean reward: 0.348 [0.191, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.557, 10.100], loss: 0.002686, mae: 0.054159, mean_q: -0.121340
 29275/100000: episode: 757, duration: 0.186s, episode steps: 29, steps per second: 156, episode reward: 8.685, mean reward: 0.299 [0.178, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.778, 10.100], loss: 0.002488, mae: 0.051458, mean_q: -0.075268
 29302/100000: episode: 758, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 9.802, mean reward: 0.363 [0.289, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.871, 10.100], loss: 0.002788, mae: 0.053991, mean_q: -0.074793
 29324/100000: episode: 759, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 7.737, mean reward: 0.352 [0.292, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.807, 10.432], loss: 0.002593, mae: 0.053313, mean_q: -0.082272
 29340/100000: episode: 760, duration: 0.100s, episode steps: 16, steps per second: 160, episode reward: 5.517, mean reward: 0.345 [0.229, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.771, 10.100], loss: 0.002808, mae: 0.054073, mean_q: -0.141704
 29356/100000: episode: 761, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 5.819, mean reward: 0.364 [0.295, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.909, 10.100], loss: 0.002863, mae: 0.054020, mean_q: -0.018645
 29383/100000: episode: 762, duration: 0.163s, episode steps: 27, steps per second: 166, episode reward: 6.742, mean reward: 0.250 [0.110, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.244, 10.100], loss: 0.002619, mae: 0.052285, mean_q: -0.088735
 29397/100000: episode: 763, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 3.472, mean reward: 0.248 [0.210, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.182, 10.100], loss: 0.002750, mae: 0.053147, mean_q: -0.066528
 29413/100000: episode: 764, duration: 0.125s, episode steps: 16, steps per second: 128, episode reward: 5.717, mean reward: 0.357 [0.310, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.116, 10.100], loss: 0.002596, mae: 0.051795, mean_q: -0.114805
 29428/100000: episode: 765, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 4.338, mean reward: 0.289 [0.190, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.227, 10.100], loss: 0.003321, mae: 0.059831, mean_q: -0.048024
 29442/100000: episode: 766, duration: 0.089s, episode steps: 14, steps per second: 157, episode reward: 4.492, mean reward: 0.321 [0.251, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.282, 10.100], loss: 0.004370, mae: 0.072871, mean_q: -0.075980
 29449/100000: episode: 767, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 2.079, mean reward: 0.297 [0.253, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.354, 10.100], loss: 0.003130, mae: 0.061970, mean_q: 0.064574
 29465/100000: episode: 768, duration: 0.099s, episode steps: 16, steps per second: 161, episode reward: 5.184, mean reward: 0.324 [0.251, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.267, 10.100], loss: 0.003239, mae: 0.062805, mean_q: 0.005214
 29492/100000: episode: 769, duration: 0.169s, episode steps: 27, steps per second: 160, episode reward: 9.070, mean reward: 0.336 [0.188, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.370, 10.100], loss: 0.002812, mae: 0.056186, mean_q: -0.011046
 29508/100000: episode: 770, duration: 0.114s, episode steps: 16, steps per second: 140, episode reward: 4.496, mean reward: 0.281 [0.211, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.870, 10.100], loss: 0.002702, mae: 0.055811, mean_q: 0.000428
 29535/100000: episode: 771, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 7.461, mean reward: 0.276 [0.215, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-1.107, 10.100], loss: 0.002726, mae: 0.054264, mean_q: -0.038009
 29564/100000: episode: 772, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 7.486, mean reward: 0.258 [0.119, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.205, 10.100], loss: 0.002927, mae: 0.055384, mean_q: -0.080238
 29578/100000: episode: 773, duration: 0.123s, episode steps: 14, steps per second: 114, episode reward: 3.361, mean reward: 0.240 [0.134, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.387, 10.100], loss: 0.002025, mae: 0.046643, mean_q: -0.094728
 29592/100000: episode: 774, duration: 0.068s, episode steps: 14, steps per second: 206, episode reward: 3.007, mean reward: 0.215 [0.071, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.323, 10.100], loss: 0.002308, mae: 0.050799, mean_q: 0.035020
 29608/100000: episode: 775, duration: 0.197s, episode steps: 16, steps per second: 81, episode reward: 4.647, mean reward: 0.290 [0.183, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.265, 10.100], loss: 0.002794, mae: 0.053073, mean_q: -0.080520
 29622/100000: episode: 776, duration: 0.150s, episode steps: 14, steps per second: 93, episode reward: 4.809, mean reward: 0.343 [0.242, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.211, 10.100], loss: 0.002834, mae: 0.055077, mean_q: -0.008085
 29638/100000: episode: 777, duration: 0.132s, episode steps: 16, steps per second: 121, episode reward: 4.944, mean reward: 0.309 [0.225, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.311, 10.100], loss: 0.003106, mae: 0.058078, mean_q: 0.036219
 29665/100000: episode: 778, duration: 0.290s, episode steps: 27, steps per second: 93, episode reward: 9.700, mean reward: 0.359 [0.246, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.585, 10.100], loss: 0.002546, mae: 0.052283, mean_q: -0.083020
 29679/100000: episode: 779, duration: 0.221s, episode steps: 14, steps per second: 63, episode reward: 3.688, mean reward: 0.263 [0.184, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.318, 10.100], loss: 0.002448, mae: 0.051025, mean_q: -0.111763
 29695/100000: episode: 780, duration: 0.211s, episode steps: 16, steps per second: 76, episode reward: 4.947, mean reward: 0.309 [0.271, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.885, 10.100], loss: 0.002267, mae: 0.047961, mean_q: -0.111243
 29717/100000: episode: 781, duration: 0.298s, episode steps: 22, steps per second: 74, episode reward: 6.737, mean reward: 0.306 [0.112, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.875, 10.289], loss: 0.002661, mae: 0.053603, mean_q: 0.061203
 29733/100000: episode: 782, duration: 0.148s, episode steps: 16, steps per second: 108, episode reward: 4.968, mean reward: 0.310 [0.192, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.465, 10.100], loss: 0.002946, mae: 0.056479, mean_q: 0.013086
 29760/100000: episode: 783, duration: 0.310s, episode steps: 27, steps per second: 87, episode reward: 9.581, mean reward: 0.355 [0.237, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-1.363, 10.100], loss: 0.002817, mae: 0.055158, mean_q: -0.050793
 29775/100000: episode: 784, duration: 0.200s, episode steps: 15, steps per second: 75, episode reward: 5.830, mean reward: 0.389 [0.297, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.804, 10.100], loss: 0.002635, mae: 0.053648, mean_q: -0.067396
 29802/100000: episode: 785, duration: 0.283s, episode steps: 27, steps per second: 95, episode reward: 7.249, mean reward: 0.268 [0.111, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.511, 10.100], loss: 0.003299, mae: 0.059519, mean_q: -0.010151
 29816/100000: episode: 786, duration: 0.161s, episode steps: 14, steps per second: 87, episode reward: 2.850, mean reward: 0.204 [0.149, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.278, 10.100], loss: 0.003101, mae: 0.056722, mean_q: -0.042508
 29843/100000: episode: 787, duration: 0.422s, episode steps: 27, steps per second: 64, episode reward: 7.170, mean reward: 0.266 [0.184, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.757, 10.100], loss: 0.002850, mae: 0.056327, mean_q: -0.011391
 29859/100000: episode: 788, duration: 0.185s, episode steps: 16, steps per second: 87, episode reward: 4.282, mean reward: 0.268 [0.210, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.238, 10.100], loss: 0.003134, mae: 0.058958, mean_q: -0.042325
 29886/100000: episode: 789, duration: 0.674s, episode steps: 27, steps per second: 40, episode reward: 9.014, mean reward: 0.334 [0.263, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-1.899, 10.100], loss: 0.002892, mae: 0.056780, mean_q: 0.007375
[Info] 200-TH LEVEL FOUND: 0.7499182820320129, Considering 10/90 traces
 29915/100000: episode: 790, duration: 5.842s, episode steps: 29, steps per second: 5, episode reward: 9.240, mean reward: 0.319 [0.200, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.102, 10.100], loss: 0.002365, mae: 0.051710, mean_q: -0.003142
 29923/100000: episode: 791, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 3.607, mean reward: 0.451 [0.399, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.461, 10.100], loss: 0.002781, mae: 0.057043, mean_q: 0.123713
 29931/100000: episode: 792, duration: 0.056s, episode steps: 8, steps per second: 142, episode reward: 2.974, mean reward: 0.372 [0.336, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.524, 10.100], loss: 0.002408, mae: 0.046307, mean_q: -0.118534
 29943/100000: episode: 793, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 5.010, mean reward: 0.418 [0.327, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.622, 10.100], loss: 0.002414, mae: 0.052680, mean_q: 0.061870
 29968/100000: episode: 794, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 9.582, mean reward: 0.383 [0.241, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.204, 10.100], loss: 0.002769, mae: 0.053718, mean_q: 0.096009
 29976/100000: episode: 795, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 3.847, mean reward: 0.481 [0.397, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.383, 10.100], loss: 0.002359, mae: 0.051794, mean_q: 0.043314
 30001/100000: episode: 796, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 8.418, mean reward: 0.337 [0.194, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.294, 10.100], loss: 0.002802, mae: 0.055124, mean_q: 0.051295
 30009/100000: episode: 797, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 3.695, mean reward: 0.462 [0.428, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.377, 10.100], loss: 0.003473, mae: 0.061721, mean_q: 0.023625
 30017/100000: episode: 798, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 3.547, mean reward: 0.443 [0.301, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.322, 10.100], loss: 0.002837, mae: 0.054944, mean_q: 0.049628
 30027/100000: episode: 799, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.347, mean reward: 0.335 [0.285, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.378, 10.100], loss: 0.002677, mae: 0.052815, mean_q: 0.094760
 30035/100000: episode: 800, duration: 0.077s, episode steps: 8, steps per second: 104, episode reward: 3.103, mean reward: 0.388 [0.314, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.240, 10.100], loss: 0.002746, mae: 0.055319, mean_q: 0.047402
 30043/100000: episode: 801, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 2.943, mean reward: 0.368 [0.299, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.483, 10.100], loss: 0.003341, mae: 0.060924, mean_q: 0.030734
 30053/100000: episode: 802, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 3.880, mean reward: 0.388 [0.354, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.328, 10.100], loss: 0.002539, mae: 0.055465, mean_q: 0.108481
 30066/100000: episode: 803, duration: 0.115s, episode steps: 13, steps per second: 113, episode reward: 5.124, mean reward: 0.394 [0.293, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.283, 10.100], loss: 0.002693, mae: 0.055649, mean_q: 0.056158
 30079/100000: episode: 804, duration: 0.143s, episode steps: 13, steps per second: 91, episode reward: 5.384, mean reward: 0.414 [0.341, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.076, 10.100], loss: 0.002807, mae: 0.055646, mean_q: -0.037480
 30098/100000: episode: 805, duration: 0.270s, episode steps: 19, steps per second: 70, episode reward: 9.059, mean reward: 0.477 [0.326, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.557, 10.100], loss: 0.002776, mae: 0.055539, mean_q: 0.073451
 30110/100000: episode: 806, duration: 0.148s, episode steps: 12, steps per second: 81, episode reward: 4.543, mean reward: 0.379 [0.328, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.334, 10.100], loss: 0.002520, mae: 0.053779, mean_q: -0.035334
 30129/100000: episode: 807, duration: 0.165s, episode steps: 19, steps per second: 115, episode reward: 7.078, mean reward: 0.373 [0.275, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.362, 10.100], loss: 0.003230, mae: 0.059383, mean_q: 0.112922
 30154/100000: episode: 808, duration: 0.148s, episode steps: 25, steps per second: 169, episode reward: 10.632, mean reward: 0.425 [0.300, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.688, 10.100], loss: 0.003382, mae: 0.059152, mean_q: 0.065898
 30173/100000: episode: 809, duration: 0.115s, episode steps: 19, steps per second: 166, episode reward: 5.745, mean reward: 0.302 [0.211, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.336, 10.100], loss: 0.002293, mae: 0.051829, mean_q: 0.085816
 30189/100000: episode: 810, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 6.044, mean reward: 0.378 [0.275, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.390, 10.100], loss: 0.002478, mae: 0.052999, mean_q: 0.133686
 30199/100000: episode: 811, duration: 0.094s, episode steps: 10, steps per second: 106, episode reward: 3.036, mean reward: 0.304 [0.263, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.278, 10.100], loss: 0.002621, mae: 0.052342, mean_q: -0.012063
 30212/100000: episode: 812, duration: 0.127s, episode steps: 13, steps per second: 103, episode reward: 4.855, mean reward: 0.373 [0.326, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.161, 10.100], loss: 0.002813, mae: 0.054595, mean_q: 0.007098
 30222/100000: episode: 813, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.502, mean reward: 0.350 [0.328, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.399, 10.100], loss: 0.002303, mae: 0.050171, mean_q: 0.047453
 30238/100000: episode: 814, duration: 0.174s, episode steps: 16, steps per second: 92, episode reward: 7.773, mean reward: 0.486 [0.437, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.446, 10.100], loss: 0.002776, mae: 0.057052, mean_q: 0.019490
 30254/100000: episode: 815, duration: 0.177s, episode steps: 16, steps per second: 91, episode reward: 5.734, mean reward: 0.358 [0.203, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.307, 10.100], loss: 0.002543, mae: 0.053190, mean_q: 0.103736
 30274/100000: episode: 816, duration: 0.146s, episode steps: 20, steps per second: 137, episode reward: 6.162, mean reward: 0.308 [0.226, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.209, 10.100], loss: 0.002786, mae: 0.056515, mean_q: 0.113699
 30286/100000: episode: 817, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 4.667, mean reward: 0.389 [0.294, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.385, 10.100], loss: 0.003136, mae: 0.057640, mean_q: 0.153590
 30296/100000: episode: 818, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 4.335, mean reward: 0.434 [0.362, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.330, 10.100], loss: 0.003119, mae: 0.055014, mean_q: 0.055091
 30315/100000: episode: 819, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 7.028, mean reward: 0.370 [0.245, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.261, 10.100], loss: 0.002485, mae: 0.052049, mean_q: 0.068899
 30328/100000: episode: 820, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 6.470, mean reward: 0.498 [0.397, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.595, 10.100], loss: 0.002396, mae: 0.053694, mean_q: 0.115592
 30347/100000: episode: 821, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 7.279, mean reward: 0.383 [0.302, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.727, 10.100], loss: 0.002803, mae: 0.055339, mean_q: 0.081310
 30366/100000: episode: 822, duration: 0.091s, episode steps: 19, steps per second: 209, episode reward: 6.580, mean reward: 0.346 [0.245, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.194, 10.100], loss: 0.002797, mae: 0.057643, mean_q: 0.105791
 30379/100000: episode: 823, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 5.528, mean reward: 0.425 [0.356, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.495, 10.100], loss: 0.002361, mae: 0.052687, mean_q: 0.134902
 30404/100000: episode: 824, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 10.212, mean reward: 0.408 [0.263, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.482, 10.100], loss: 0.002470, mae: 0.052225, mean_q: 0.101427
 30423/100000: episode: 825, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 4.886, mean reward: 0.257 [0.164, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.407, 10.100], loss: 0.002494, mae: 0.051162, mean_q: 0.073460
 30442/100000: episode: 826, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 8.236, mean reward: 0.433 [0.372, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.787, 10.100], loss: 0.002506, mae: 0.053506, mean_q: 0.151195
 30452/100000: episode: 827, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 4.066, mean reward: 0.407 [0.335, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-1.062, 10.100], loss: 0.002739, mae: 0.055260, mean_q: 0.162041
 30471/100000: episode: 828, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 6.817, mean reward: 0.359 [0.306, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.956, 10.100], loss: 0.002457, mae: 0.051900, mean_q: 0.179493
 30491/100000: episode: 829, duration: 0.159s, episode steps: 20, steps per second: 126, episode reward: 7.668, mean reward: 0.383 [0.327, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-1.096, 10.100], loss: 0.002998, mae: 0.058442, mean_q: 0.098005
 30501/100000: episode: 830, duration: 0.083s, episode steps: 10, steps per second: 120, episode reward: 4.515, mean reward: 0.452 [0.400, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.372, 10.100], loss: 0.003188, mae: 0.064441, mean_q: 0.168048
 30513/100000: episode: 831, duration: 0.091s, episode steps: 12, steps per second: 132, episode reward: 5.539, mean reward: 0.462 [0.405, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.385, 10.100], loss: 0.003128, mae: 0.062451, mean_q: 0.202787
 30532/100000: episode: 832, duration: 0.168s, episode steps: 19, steps per second: 113, episode reward: 8.604, mean reward: 0.453 [0.381, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.364, 10.100], loss: 0.003408, mae: 0.061904, mean_q: 0.169339
 30545/100000: episode: 833, duration: 0.119s, episode steps: 13, steps per second: 109, episode reward: 4.658, mean reward: 0.358 [0.306, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.382, 10.100], loss: 0.003280, mae: 0.063958, mean_q: 0.229626
 30558/100000: episode: 834, duration: 0.086s, episode steps: 13, steps per second: 150, episode reward: 4.265, mean reward: 0.328 [0.245, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.333, 10.100], loss: 0.002473, mae: 0.053879, mean_q: 0.225286
 30577/100000: episode: 835, duration: 0.137s, episode steps: 19, steps per second: 138, episode reward: 7.881, mean reward: 0.415 [0.284, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.317, 10.100], loss: 0.002672, mae: 0.055289, mean_q: 0.129013
 30596/100000: episode: 836, duration: 0.157s, episode steps: 19, steps per second: 121, episode reward: 6.516, mean reward: 0.343 [0.259, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.770, 10.100], loss: 0.003019, mae: 0.059213, mean_q: 0.200188
 30609/100000: episode: 837, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 5.022, mean reward: 0.386 [0.327, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.277, 10.100], loss: 0.002922, mae: 0.057757, mean_q: 0.219480
 30634/100000: episode: 838, duration: 0.212s, episode steps: 25, steps per second: 118, episode reward: 8.808, mean reward: 0.352 [0.221, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.256, 10.100], loss: 0.002896, mae: 0.057221, mean_q: 0.212201
 30653/100000: episode: 839, duration: 0.190s, episode steps: 19, steps per second: 100, episode reward: 6.699, mean reward: 0.353 [0.255, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.248, 10.100], loss: 0.002998, mae: 0.058494, mean_q: 0.111347
 30669/100000: episode: 840, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 6.067, mean reward: 0.379 [0.297, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.502, 10.100], loss: 0.002754, mae: 0.056235, mean_q: 0.214838
 30689/100000: episode: 841, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 7.904, mean reward: 0.395 [0.185, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.488, 10.100], loss: 0.002605, mae: 0.053870, mean_q: 0.172015
 30697/100000: episode: 842, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 3.712, mean reward: 0.464 [0.372, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.320, 10.100], loss: 0.002507, mae: 0.053396, mean_q: 0.119316
 30705/100000: episode: 843, duration: 0.060s, episode steps: 8, steps per second: 133, episode reward: 2.675, mean reward: 0.334 [0.282, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.333, 10.100], loss: 0.003153, mae: 0.060223, mean_q: 0.212405
 30713/100000: episode: 844, duration: 0.057s, episode steps: 8, steps per second: 140, episode reward: 2.985, mean reward: 0.373 [0.313, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.487, 10.100], loss: 0.002640, mae: 0.054054, mean_q: 0.244435
 30723/100000: episode: 845, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.049, mean reward: 0.405 [0.331, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.430, 10.100], loss: 0.003006, mae: 0.059805, mean_q: 0.199608
 30748/100000: episode: 846, duration: 0.340s, episode steps: 25, steps per second: 74, episode reward: 8.818, mean reward: 0.353 [0.179, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.794, 10.100], loss: 0.002683, mae: 0.056061, mean_q: 0.221321
 30764/100000: episode: 847, duration: 0.196s, episode steps: 16, steps per second: 82, episode reward: 5.497, mean reward: 0.344 [0.182, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.074, 10.100], loss: 0.002656, mae: 0.053765, mean_q: 0.152476
 30774/100000: episode: 848, duration: 0.080s, episode steps: 10, steps per second: 126, episode reward: 4.478, mean reward: 0.448 [0.369, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.591, 10.100], loss: 0.002414, mae: 0.052207, mean_q: 0.237654
 30786/100000: episode: 849, duration: 0.116s, episode steps: 12, steps per second: 103, episode reward: 4.178, mean reward: 0.348 [0.253, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.450, 10.100], loss: 0.002996, mae: 0.058605, mean_q: 0.182975
 30805/100000: episode: 850, duration: 0.203s, episode steps: 19, steps per second: 94, episode reward: 7.358, mean reward: 0.387 [0.253, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.190, 10.100], loss: 0.002778, mae: 0.056074, mean_q: 0.191435
 30824/100000: episode: 851, duration: 0.145s, episode steps: 19, steps per second: 131, episode reward: 6.286, mean reward: 0.331 [0.248, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.168, 10.100], loss: 0.002882, mae: 0.056804, mean_q: 0.191800
 30843/100000: episode: 852, duration: 0.184s, episode steps: 19, steps per second: 103, episode reward: 7.189, mean reward: 0.378 [0.208, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.354, 10.100], loss: 0.002953, mae: 0.057362, mean_q: 0.226022
 30851/100000: episode: 853, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 3.264, mean reward: 0.408 [0.321, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.554, 10.100], loss: 0.002713, mae: 0.053605, mean_q: 0.197966
 30863/100000: episode: 854, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 5.968, mean reward: 0.497 [0.406, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.428, 10.100], loss: 0.002245, mae: 0.050139, mean_q: 0.197357
 30883/100000: episode: 855, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 5.306, mean reward: 0.265 [0.177, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.100], loss: 0.002223, mae: 0.052100, mean_q: 0.295943
 30903/100000: episode: 856, duration: 0.235s, episode steps: 20, steps per second: 85, episode reward: 8.899, mean reward: 0.445 [0.369, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.388, 10.100], loss: 0.002612, mae: 0.054389, mean_q: 0.259964
 30922/100000: episode: 857, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 9.048, mean reward: 0.476 [0.360, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.455, 10.100], loss: 0.002559, mae: 0.053492, mean_q: 0.221328
 30947/100000: episode: 858, duration: 0.162s, episode steps: 25, steps per second: 155, episode reward: 10.630, mean reward: 0.425 [0.318, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.532, 10.100], loss: 0.002629, mae: 0.055349, mean_q: 0.216783
 30966/100000: episode: 859, duration: 0.199s, episode steps: 19, steps per second: 96, episode reward: 7.122, mean reward: 0.375 [0.319, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.396, 10.100], loss: 0.003482, mae: 0.061244, mean_q: 0.212664
 30979/100000: episode: 860, duration: 0.170s, episode steps: 13, steps per second: 76, episode reward: 4.299, mean reward: 0.331 [0.244, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.561, 10.100], loss: 0.002968, mae: 0.058316, mean_q: 0.263291
 31004/100000: episode: 861, duration: 0.293s, episode steps: 25, steps per second: 85, episode reward: 9.992, mean reward: 0.400 [0.271, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.284, 10.100], loss: 0.002707, mae: 0.056257, mean_q: 0.298205
 31017/100000: episode: 862, duration: 0.122s, episode steps: 13, steps per second: 106, episode reward: 6.094, mean reward: 0.469 [0.380, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.362, 10.100], loss: 0.002671, mae: 0.057210, mean_q: 0.277303
 31030/100000: episode: 863, duration: 0.088s, episode steps: 13, steps per second: 148, episode reward: 4.215, mean reward: 0.324 [0.239, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.315, 10.100], loss: 0.002514, mae: 0.053657, mean_q: 0.237863
 31040/100000: episode: 864, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 4.073, mean reward: 0.407 [0.333, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.392, 10.100], loss: 0.002448, mae: 0.052819, mean_q: 0.275647
 31053/100000: episode: 865, duration: 0.122s, episode steps: 13, steps per second: 106, episode reward: 4.525, mean reward: 0.348 [0.240, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.676, 10.100], loss: 0.002537, mae: 0.054315, mean_q: 0.240125
 31072/100000: episode: 866, duration: 0.232s, episode steps: 19, steps per second: 82, episode reward: 7.405, mean reward: 0.390 [0.295, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.589, 10.100], loss: 0.002884, mae: 0.056746, mean_q: 0.238432
 31091/100000: episode: 867, duration: 0.223s, episode steps: 19, steps per second: 85, episode reward: 7.185, mean reward: 0.378 [0.312, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.408, 10.100], loss: 0.002697, mae: 0.054934, mean_q: 0.288968
 31101/100000: episode: 868, duration: 0.073s, episode steps: 10, steps per second: 138, episode reward: 4.845, mean reward: 0.484 [0.362, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.517, 10.100], loss: 0.002751, mae: 0.058470, mean_q: 0.395911
 31114/100000: episode: 869, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 4.933, mean reward: 0.379 [0.298, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.395, 10.100], loss: 0.002775, mae: 0.057594, mean_q: 0.366500
 31133/100000: episode: 870, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 6.352, mean reward: 0.334 [0.242, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.757, 10.100], loss: 0.002766, mae: 0.056465, mean_q: 0.332389
 31149/100000: episode: 871, duration: 0.107s, episode steps: 16, steps per second: 150, episode reward: 6.460, mean reward: 0.404 [0.314, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.331, 10.100], loss: 0.002769, mae: 0.056785, mean_q: 0.277188
 31168/100000: episode: 872, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 6.188, mean reward: 0.326 [0.267, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.391, 10.100], loss: 0.002786, mae: 0.057338, mean_q: 0.261701
 31178/100000: episode: 873, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 4.399, mean reward: 0.440 [0.382, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.382, 10.100], loss: 0.005047, mae: 0.064510, mean_q: 0.341618
 31191/100000: episode: 874, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 5.454, mean reward: 0.420 [0.338, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.701, 10.100], loss: 0.006044, mae: 0.071497, mean_q: 0.293146
 31203/100000: episode: 875, duration: 0.102s, episode steps: 12, steps per second: 118, episode reward: 4.743, mean reward: 0.395 [0.334, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.388, 10.100], loss: 0.002980, mae: 0.059614, mean_q: 0.237695
 31222/100000: episode: 876, duration: 0.127s, episode steps: 19, steps per second: 150, episode reward: 7.056, mean reward: 0.371 [0.274, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.377, 10.100], loss: 0.004617, mae: 0.063164, mean_q: 0.328095
 31230/100000: episode: 877, duration: 0.053s, episode steps: 8, steps per second: 152, episode reward: 3.057, mean reward: 0.382 [0.286, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.340, 10.100], loss: 0.003290, mae: 0.060958, mean_q: 0.262571
 31238/100000: episode: 878, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 3.157, mean reward: 0.395 [0.364, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.241, 10.100], loss: 0.004581, mae: 0.071150, mean_q: 0.422270
 31250/100000: episode: 879, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 5.384, mean reward: 0.449 [0.390, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.398, 10.100], loss: 0.005005, mae: 0.068096, mean_q: 0.318478
[Info] 300-TH LEVEL FOUND: 0.8868271112442017, Considering 10/90 traces
 31258/100000: episode: 880, duration: 6.137s, episode steps: 8, steps per second: 1, episode reward: 3.734, mean reward: 0.467 [0.443, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.397, 10.100], loss: 0.002733, mae: 0.059347, mean_q: 0.294356
 31280/100000: episode: 881, duration: 0.312s, episode steps: 22, steps per second: 71, episode reward: 10.955, mean reward: 0.498 [0.393, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.965, 10.100], loss: 0.002988, mae: 0.060081, mean_q: 0.316697
 31288/100000: episode: 882, duration: 0.158s, episode steps: 8, steps per second: 51, episode reward: 4.043, mean reward: 0.505 [0.436, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.522, 10.100], loss: 0.002468, mae: 0.056039, mean_q: 0.343812
 31310/100000: episode: 883, duration: 0.253s, episode steps: 22, steps per second: 87, episode reward: 8.361, mean reward: 0.380 [0.299, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.333, 10.100], loss: 0.002752, mae: 0.056719, mean_q: 0.283049
 31332/100000: episode: 884, duration: 0.293s, episode steps: 22, steps per second: 75, episode reward: 9.426, mean reward: 0.428 [0.336, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.197, 10.100], loss: 0.002580, mae: 0.055307, mean_q: 0.329430
 31349/100000: episode: 885, duration: 0.197s, episode steps: 17, steps per second: 86, episode reward: 6.686, mean reward: 0.393 [0.283, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.432, 10.100], loss: 0.003731, mae: 0.061605, mean_q: 0.328901
[Info] FALSIFICATION!
 31352/100000: episode: 886, duration: 0.049s, episode steps: 3, steps per second: 62, episode reward: 11.279, mean reward: 3.760 [0.592, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.276, 10.020], loss: 0.004190, mae: 0.063229, mean_q: 0.347677
 31452/100000: episode: 887, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: -16.094, mean reward: -0.161 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.778, 10.098], loss: 0.003686, mae: 0.062045, mean_q: 0.355901
 31552/100000: episode: 888, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: -18.175, mean reward: -0.182 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.538, 10.104], loss: 0.016597, mae: 0.068086, mean_q: 0.360663
 31652/100000: episode: 889, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: -18.553, mean reward: -0.186 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.488, 10.382], loss: 0.003093, mae: 0.057779, mean_q: 0.342265
 31752/100000: episode: 890, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: -18.971, mean reward: -0.190 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.955, 10.155], loss: 0.004423, mae: 0.063295, mean_q: 0.352770
 31852/100000: episode: 891, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: -15.875, mean reward: -0.159 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.632, 10.098], loss: 0.016788, mae: 0.070454, mean_q: 0.337081
 31952/100000: episode: 892, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: -16.382, mean reward: -0.164 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.610, 10.098], loss: 0.003698, mae: 0.060867, mean_q: 0.343004
 32052/100000: episode: 893, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: -14.382, mean reward: -0.144 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.469, 10.238], loss: 0.002546, mae: 0.054176, mean_q: 0.334941
 32152/100000: episode: 894, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: -19.116, mean reward: -0.191 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.038, 10.098], loss: 0.002448, mae: 0.053013, mean_q: 0.302661
 32252/100000: episode: 895, duration: 0.766s, episode steps: 100, steps per second: 131, episode reward: -19.811, mean reward: -0.198 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.807, 10.098], loss: 0.015878, mae: 0.061466, mean_q: 0.326828
 32352/100000: episode: 896, duration: 0.930s, episode steps: 100, steps per second: 107, episode reward: -16.813, mean reward: -0.168 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.608, 10.293], loss: 0.030220, mae: 0.077050, mean_q: 0.343098
 32452/100000: episode: 897, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: -18.017, mean reward: -0.180 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.645, 10.232], loss: 0.003093, mae: 0.059562, mean_q: 0.364013
 32552/100000: episode: 898, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: -15.288, mean reward: -0.153 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.957, 10.098], loss: 0.015879, mae: 0.061686, mean_q: 0.348667
 32652/100000: episode: 899, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: -14.327, mean reward: -0.143 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.996, 10.098], loss: 0.002610, mae: 0.054919, mean_q: 0.325592
 32752/100000: episode: 900, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: -18.658, mean reward: -0.187 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.529, 10.323], loss: 0.002453, mae: 0.052621, mean_q: 0.340173
 32852/100000: episode: 901, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: -12.692, mean reward: -0.127 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.409, 10.098], loss: 0.002457, mae: 0.053108, mean_q: 0.342635
 32952/100000: episode: 902, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: -15.287, mean reward: -0.153 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.501, 10.098], loss: 0.015993, mae: 0.063308, mean_q: 0.348607
 33052/100000: episode: 903, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: -10.615, mean reward: -0.106 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.010, 10.098], loss: 0.015965, mae: 0.065766, mean_q: 0.331416
 33152/100000: episode: 904, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: -13.505, mean reward: -0.135 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.556, 10.259], loss: 0.003592, mae: 0.061149, mean_q: 0.332236
 33252/100000: episode: 905, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -9.047, mean reward: -0.090 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.199, 10.450], loss: 0.002588, mae: 0.054526, mean_q: 0.248778
 33352/100000: episode: 906, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -5.667, mean reward: -0.057 [-1.000, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.631, 10.535], loss: 0.016420, mae: 0.068921, mean_q: 0.283905
 33452/100000: episode: 907, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -19.215, mean reward: -0.192 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.783, 10.154], loss: 0.028904, mae: 0.070282, mean_q: 0.265779
 33552/100000: episode: 908, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: -17.342, mean reward: -0.173 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.198, 10.267], loss: 0.002545, mae: 0.054758, mean_q: 0.226644
 33652/100000: episode: 909, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: -18.334, mean reward: -0.183 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.463, 10.098], loss: 0.029043, mae: 0.071394, mean_q: 0.226962
 33752/100000: episode: 910, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: -18.892, mean reward: -0.189 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.433, 10.145], loss: 0.016077, mae: 0.065540, mean_q: 0.177293
 33852/100000: episode: 911, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -19.354, mean reward: -0.194 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.950, 10.146], loss: 0.002958, mae: 0.057748, mean_q: 0.146434
 33952/100000: episode: 912, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.029, mean reward: -0.190 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.233, 10.098], loss: 0.018287, mae: 0.075836, mean_q: 0.152209
 34052/100000: episode: 913, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -19.834, mean reward: -0.198 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.410, 10.190], loss: 0.002453, mae: 0.052643, mean_q: 0.133103
 34152/100000: episode: 914, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -11.559, mean reward: -0.116 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.222, 10.098], loss: 0.002495, mae: 0.052961, mean_q: 0.148519
 34252/100000: episode: 915, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: -18.070, mean reward: -0.181 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.766, 10.237], loss: 0.015601, mae: 0.059227, mean_q: 0.098718
 34352/100000: episode: 916, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.217, mean reward: -0.182 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.770, 10.208], loss: 0.028811, mae: 0.068042, mean_q: 0.095248
 34452/100000: episode: 917, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -19.686, mean reward: -0.197 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.647, 10.172], loss: 0.016017, mae: 0.065065, mean_q: 0.087331
 34552/100000: episode: 918, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: -19.754, mean reward: -0.198 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.381, 10.107], loss: 0.002644, mae: 0.053572, mean_q: 0.047875
 34652/100000: episode: 919, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.389, mean reward: -0.174 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.444, 10.216], loss: 0.002620, mae: 0.053482, mean_q: 0.027726
 34752/100000: episode: 920, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.651, mean reward: -0.187 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.375, 10.155], loss: 0.005172, mae: 0.066508, mean_q: 0.033335
 34852/100000: episode: 921, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -18.031, mean reward: -0.180 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.864, 10.147], loss: 0.002541, mae: 0.052127, mean_q: 0.008864
 34952/100000: episode: 922, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -17.655, mean reward: -0.177 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.435, 10.098], loss: 0.002298, mae: 0.049244, mean_q: -0.053092
 35052/100000: episode: 923, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.767, mean reward: -0.188 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.446, 10.239], loss: 0.002914, mae: 0.054742, mean_q: -0.053384
 35152/100000: episode: 924, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.925, mean reward: -0.179 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.896, 10.098], loss: 0.015799, mae: 0.059237, mean_q: -0.071237
 35252/100000: episode: 925, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -15.928, mean reward: -0.159 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.520, 10.233], loss: 0.015789, mae: 0.059919, mean_q: -0.088263
 35352/100000: episode: 926, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -17.183, mean reward: -0.172 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.802, 10.194], loss: 0.002254, mae: 0.048975, mean_q: -0.117952
 35452/100000: episode: 927, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.486, mean reward: -0.185 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.125, 10.295], loss: 0.002483, mae: 0.050907, mean_q: -0.161537
 35552/100000: episode: 928, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.236, mean reward: -0.182 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.095, 10.166], loss: 0.002317, mae: 0.048606, mean_q: -0.174956
 35652/100000: episode: 929, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.712, mean reward: -0.157 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.522, 10.265], loss: 0.002510, mae: 0.050967, mean_q: -0.188215
 35752/100000: episode: 930, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -16.814, mean reward: -0.168 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.298, 10.240], loss: 0.040557, mae: 0.067181, mean_q: -0.194001
 35852/100000: episode: 931, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -16.821, mean reward: -0.168 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.294, 10.265], loss: 0.017528, mae: 0.076458, mean_q: -0.188549
 35952/100000: episode: 932, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.038, mean reward: -0.180 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.264, 10.098], loss: 0.015505, mae: 0.060418, mean_q: -0.237861
 36052/100000: episode: 933, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.843, mean reward: -0.198 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.965, 10.098], loss: 0.015325, mae: 0.057704, mean_q: -0.239100
 36152/100000: episode: 934, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: -9.080, mean reward: -0.091 [-1.000, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.961, 10.407], loss: 0.015203, mae: 0.058297, mean_q: -0.284760
 36252/100000: episode: 935, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.848, mean reward: -0.178 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.898, 10.131], loss: 0.003579, mae: 0.056076, mean_q: -0.323086
 36352/100000: episode: 936, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: -13.906, mean reward: -0.139 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.444, 10.098], loss: 0.002508, mae: 0.051671, mean_q: -0.301476
 36452/100000: episode: 937, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: -18.232, mean reward: -0.182 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.968, 10.098], loss: 0.002507, mae: 0.051462, mean_q: -0.304534
 36552/100000: episode: 938, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: -19.858, mean reward: -0.199 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.394, 10.232], loss: 0.002413, mae: 0.048155, mean_q: -0.333072
 36652/100000: episode: 939, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -17.286, mean reward: -0.173 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.850, 10.242], loss: 0.002384, mae: 0.049122, mean_q: -0.311573
 36752/100000: episode: 940, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -18.610, mean reward: -0.186 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.541, 10.195], loss: 0.002410, mae: 0.050578, mean_q: -0.317511
 36852/100000: episode: 941, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -19.602, mean reward: -0.196 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.910, 10.268], loss: 0.002660, mae: 0.052052, mean_q: -0.296711
 36952/100000: episode: 942, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -17.143, mean reward: -0.171 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.424, 10.098], loss: 0.002473, mae: 0.050317, mean_q: -0.310066
 37052/100000: episode: 943, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -12.789, mean reward: -0.128 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.305, 10.098], loss: 0.002441, mae: 0.049757, mean_q: -0.286779
 37152/100000: episode: 944, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: -18.837, mean reward: -0.188 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.366, 10.166], loss: 0.002438, mae: 0.049326, mean_q: -0.311979
 37252/100000: episode: 945, duration: 0.662s, episode steps: 100, steps per second: 151, episode reward: -17.389, mean reward: -0.174 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.917, 10.211], loss: 0.002288, mae: 0.047969, mean_q: -0.338873
 37352/100000: episode: 946, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -13.936, mean reward: -0.139 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.109, 10.141], loss: 0.002434, mae: 0.049800, mean_q: -0.289424
 37452/100000: episode: 947, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: -10.445, mean reward: -0.104 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.158, 10.335], loss: 0.002421, mae: 0.049467, mean_q: -0.317277
 37552/100000: episode: 948, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: -3.118, mean reward: -0.031 [-1.000, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.119, 10.417], loss: 0.002404, mae: 0.049358, mean_q: -0.311890
 37652/100000: episode: 949, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: -16.681, mean reward: -0.167 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.716, 10.098], loss: 0.002444, mae: 0.049606, mean_q: -0.358280
 37752/100000: episode: 950, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -14.716, mean reward: -0.147 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.359, 10.098], loss: 0.002457, mae: 0.049912, mean_q: -0.282716
 37852/100000: episode: 951, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.070, mean reward: -0.191 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.336, 10.098], loss: 0.004550, mae: 0.061992, mean_q: -0.294497
 37952/100000: episode: 952, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -14.776, mean reward: -0.148 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.102, 10.271], loss: 0.002633, mae: 0.051504, mean_q: -0.281172
 38052/100000: episode: 953, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -18.435, mean reward: -0.184 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.645, 10.243], loss: 0.002493, mae: 0.050049, mean_q: -0.296131
 38152/100000: episode: 954, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.665, mean reward: -0.187 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.786, 10.221], loss: 0.002487, mae: 0.049661, mean_q: -0.308884
 38252/100000: episode: 955, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.542, mean reward: -0.175 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.449, 10.174], loss: 0.002343, mae: 0.048007, mean_q: -0.334510
 38352/100000: episode: 956, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: -19.052, mean reward: -0.191 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.343, 10.098], loss: 0.002279, mae: 0.047902, mean_q: -0.330823
 38452/100000: episode: 957, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -16.268, mean reward: -0.163 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.680, 10.253], loss: 0.002550, mae: 0.050207, mean_q: -0.318445
 38552/100000: episode: 958, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -19.668, mean reward: -0.197 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.733, 10.099], loss: 0.002466, mae: 0.049022, mean_q: -0.302071
 38652/100000: episode: 959, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.467, mean reward: -0.165 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.152, 10.155], loss: 0.002325, mae: 0.048191, mean_q: -0.334165
 38752/100000: episode: 960, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.194, mean reward: -0.172 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.073, 10.098], loss: 0.002507, mae: 0.049445, mean_q: -0.305992
 38852/100000: episode: 961, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -16.316, mean reward: -0.163 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.089, 10.098], loss: 0.002465, mae: 0.049938, mean_q: -0.331580
 38952/100000: episode: 962, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: -17.044, mean reward: -0.170 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.313, 10.282], loss: 0.002492, mae: 0.050226, mean_q: -0.315916
 39052/100000: episode: 963, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.223, mean reward: -0.192 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.524, 10.136], loss: 0.002422, mae: 0.049248, mean_q: -0.342097
 39152/100000: episode: 964, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.134, mean reward: -0.171 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.589, 10.205], loss: 0.002626, mae: 0.051974, mean_q: -0.327489
 39252/100000: episode: 965, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -15.813, mean reward: -0.158 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.801, 10.098], loss: 0.004968, mae: 0.064919, mean_q: -0.327652
 39352/100000: episode: 966, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.839, mean reward: -0.158 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.426, 10.098], loss: 0.003533, mae: 0.061134, mean_q: -0.312491
 39452/100000: episode: 967, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -19.736, mean reward: -0.197 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.335, 10.245], loss: 0.002776, mae: 0.054010, mean_q: -0.300319
 39552/100000: episode: 968, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -18.190, mean reward: -0.182 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.712, 10.141], loss: 0.002379, mae: 0.049350, mean_q: -0.309053
 39652/100000: episode: 969, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.138, mean reward: -0.161 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.846, 10.111], loss: 0.002656, mae: 0.052797, mean_q: -0.346222
 39752/100000: episode: 970, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: -19.009, mean reward: -0.190 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.708, 10.245], loss: 0.002412, mae: 0.050156, mean_q: -0.323057
 39852/100000: episode: 971, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.941, mean reward: -0.189 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.271, 10.153], loss: 0.002420, mae: 0.048842, mean_q: -0.338578
 39952/100000: episode: 972, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -13.827, mean reward: -0.138 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.270, 10.098], loss: 0.002203, mae: 0.045623, mean_q: -0.358243
 40052/100000: episode: 973, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.567, mean reward: -0.166 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.670, 10.098], loss: 0.002324, mae: 0.047823, mean_q: -0.312573
 40152/100000: episode: 974, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.794, mean reward: -0.168 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.791, 10.098], loss: 0.002547, mae: 0.050955, mean_q: -0.302758
 40252/100000: episode: 975, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -13.690, mean reward: -0.137 [-1.000, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.974, 10.098], loss: 0.002498, mae: 0.050537, mean_q: -0.280005
 40352/100000: episode: 976, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.326, mean reward: -0.183 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.875, 10.344], loss: 0.002428, mae: 0.049174, mean_q: -0.334838
 40452/100000: episode: 977, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -15.784, mean reward: -0.158 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.365, 10.098], loss: 0.002986, mae: 0.053640, mean_q: -0.321797
 40552/100000: episode: 978, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: -16.366, mean reward: -0.164 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.497, 10.266], loss: 0.002589, mae: 0.051166, mean_q: -0.326334
 40652/100000: episode: 979, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: -11.616, mean reward: -0.116 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.309, 10.098], loss: 0.002562, mae: 0.050367, mean_q: -0.301523
 40752/100000: episode: 980, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: -18.590, mean reward: -0.186 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.572, 10.098], loss: 0.002720, mae: 0.052598, mean_q: -0.291777
 40852/100000: episode: 981, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -17.512, mean reward: -0.175 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.466, 10.098], loss: 0.002533, mae: 0.049890, mean_q: -0.303294
 40952/100000: episode: 982, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.853, mean reward: -0.139 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.745, 10.098], loss: 0.002481, mae: 0.050408, mean_q: -0.309975
 41052/100000: episode: 983, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: -19.177, mean reward: -0.192 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.903, 10.100], loss: 0.002831, mae: 0.055026, mean_q: -0.295098
 41152/100000: episode: 984, duration: 0.656s, episode steps: 100, steps per second: 153, episode reward: -19.770, mean reward: -0.198 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.692, 10.193], loss: 0.002751, mae: 0.054823, mean_q: -0.327339
 41252/100000: episode: 985, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -18.587, mean reward: -0.186 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.863, 10.217], loss: 0.002486, mae: 0.049542, mean_q: -0.312479
[Info] 100-TH LEVEL FOUND: 0.707648754119873, Considering 10/90 traces
 41352/100000: episode: 986, duration: 5.586s, episode steps: 100, steps per second: 18, episode reward: -14.507, mean reward: -0.145 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.333, 10.364], loss: 0.002555, mae: 0.051335, mean_q: -0.290725
 41379/100000: episode: 987, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 8.928, mean reward: 0.331 [0.183, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.087, 10.100], loss: 0.002731, mae: 0.050520, mean_q: -0.327081
 41397/100000: episode: 988, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 8.072, mean reward: 0.448 [0.381, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.725, 10.100], loss: 0.002250, mae: 0.048124, mean_q: -0.322395
 41443/100000: episode: 989, duration: 0.386s, episode steps: 46, steps per second: 119, episode reward: 9.868, mean reward: 0.215 [0.066, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-1.013, 10.213], loss: 0.002320, mae: 0.047496, mean_q: -0.289906
 41461/100000: episode: 990, duration: 0.175s, episode steps: 18, steps per second: 103, episode reward: 7.654, mean reward: 0.425 [0.302, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.242, 10.100], loss: 0.001953, mae: 0.043048, mean_q: -0.368260
 41479/100000: episode: 991, duration: 0.252s, episode steps: 18, steps per second: 71, episode reward: 6.007, mean reward: 0.334 [0.237, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.162, 10.100], loss: 0.002276, mae: 0.048217, mean_q: -0.273797
 41497/100000: episode: 992, duration: 0.206s, episode steps: 18, steps per second: 87, episode reward: 7.481, mean reward: 0.416 [0.346, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.497, 10.100], loss: 0.002127, mae: 0.045989, mean_q: -0.358333
 41539/100000: episode: 993, duration: 0.493s, episode steps: 42, steps per second: 85, episode reward: 6.600, mean reward: 0.157 [0.017, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.498, 10.162], loss: 0.002570, mae: 0.051535, mean_q: -0.246250
 41579/100000: episode: 994, duration: 0.284s, episode steps: 40, steps per second: 141, episode reward: 12.527, mean reward: 0.313 [0.070, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-1.336, 10.100], loss: 0.006344, mae: 0.074948, mean_q: -0.259646
 41604/100000: episode: 995, duration: 0.196s, episode steps: 25, steps per second: 127, episode reward: 7.837, mean reward: 0.313 [0.242, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.380], loss: 0.005346, mae: 0.070803, mean_q: -0.249505
 41622/100000: episode: 996, duration: 0.148s, episode steps: 18, steps per second: 122, episode reward: 8.926, mean reward: 0.496 [0.362, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.475, 10.100], loss: 0.004874, mae: 0.064790, mean_q: -0.281233
 41668/100000: episode: 997, duration: 0.376s, episode steps: 46, steps per second: 122, episode reward: 7.911, mean reward: 0.172 [0.029, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.331, 10.198], loss: 0.003072, mae: 0.054418, mean_q: -0.281374
 41710/100000: episode: 998, duration: 0.376s, episode steps: 42, steps per second: 112, episode reward: 11.492, mean reward: 0.274 [0.107, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.877, 10.476], loss: 0.004308, mae: 0.068611, mean_q: -0.229972
 41737/100000: episode: 999, duration: 0.233s, episode steps: 27, steps per second: 116, episode reward: 6.980, mean reward: 0.259 [0.073, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.235, 10.100], loss: 0.002502, mae: 0.054845, mean_q: -0.205593
 41779/100000: episode: 1000, duration: 0.211s, episode steps: 42, steps per second: 200, episode reward: 9.571, mean reward: 0.228 [0.142, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.110, 10.292], loss: 0.002505, mae: 0.052016, mean_q: -0.247987
 41804/100000: episode: 1001, duration: 0.149s, episode steps: 25, steps per second: 168, episode reward: 9.143, mean reward: 0.366 [0.285, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.495], loss: 0.002738, mae: 0.054302, mean_q: -0.277074
 41844/100000: episode: 1002, duration: 0.232s, episode steps: 40, steps per second: 172, episode reward: 11.892, mean reward: 0.297 [0.174, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-1.059, 10.482], loss: 0.002393, mae: 0.050380, mean_q: -0.224225
 41857/100000: episode: 1003, duration: 0.098s, episode steps: 13, steps per second: 133, episode reward: 5.060, mean reward: 0.389 [0.293, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.351, 10.100], loss: 0.002350, mae: 0.048789, mean_q: -0.327580
 41884/100000: episode: 1004, duration: 0.294s, episode steps: 27, steps per second: 92, episode reward: 11.282, mean reward: 0.418 [0.325, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-1.107, 10.100], loss: 0.002539, mae: 0.050719, mean_q: -0.239328
 41902/100000: episode: 1005, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 8.044, mean reward: 0.447 [0.334, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.645, 10.100], loss: 0.002925, mae: 0.056203, mean_q: -0.167098
 41929/100000: episode: 1006, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 10.725, mean reward: 0.397 [0.261, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.416, 10.100], loss: 0.002976, mae: 0.054626, mean_q: -0.264239
 41971/100000: episode: 1007, duration: 0.246s, episode steps: 42, steps per second: 171, episode reward: 10.149, mean reward: 0.242 [0.148, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.147, 10.385], loss: 0.002884, mae: 0.055677, mean_q: -0.197348
 41984/100000: episode: 1008, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 5.908, mean reward: 0.454 [0.384, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.328, 10.100], loss: 0.003072, mae: 0.057098, mean_q: -0.175331
 42030/100000: episode: 1009, duration: 0.269s, episode steps: 46, steps per second: 171, episode reward: 12.467, mean reward: 0.271 [0.101, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.578, 10.438], loss: 0.002507, mae: 0.050668, mean_q: -0.170590
 42043/100000: episode: 1010, duration: 0.090s, episode steps: 13, steps per second: 144, episode reward: 5.185, mean reward: 0.399 [0.305, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.380, 10.100], loss: 0.002609, mae: 0.049571, mean_q: -0.232802
 42070/100000: episode: 1011, duration: 0.167s, episode steps: 27, steps per second: 162, episode reward: 13.319, mean reward: 0.493 [0.294, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-1.152, 10.100], loss: 0.002275, mae: 0.048212, mean_q: -0.172196
 42099/100000: episode: 1012, duration: 0.223s, episode steps: 29, steps per second: 130, episode reward: 8.552, mean reward: 0.295 [0.063, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.464, 10.100], loss: 0.002407, mae: 0.051007, mean_q: -0.137792
 42124/100000: episode: 1013, duration: 0.143s, episode steps: 25, steps per second: 174, episode reward: 6.478, mean reward: 0.259 [0.120, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.075, 10.307], loss: 0.002632, mae: 0.052863, mean_q: -0.211391
 42142/100000: episode: 1014, duration: 0.101s, episode steps: 18, steps per second: 177, episode reward: 7.235, mean reward: 0.402 [0.326, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.460, 10.100], loss: 0.002435, mae: 0.050994, mean_q: -0.081092
 42160/100000: episode: 1015, duration: 0.116s, episode steps: 18, steps per second: 156, episode reward: 7.423, mean reward: 0.412 [0.293, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.198, 10.100], loss: 0.002419, mae: 0.049399, mean_q: -0.184398
 42185/100000: episode: 1016, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 5.322, mean reward: 0.213 [0.070, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.605, 10.291], loss: 0.002516, mae: 0.049831, mean_q: -0.149966
 42225/100000: episode: 1017, duration: 0.290s, episode steps: 40, steps per second: 138, episode reward: 13.199, mean reward: 0.330 [0.199, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.727, 10.394], loss: 0.002732, mae: 0.052233, mean_q: -0.140026
 42254/100000: episode: 1018, duration: 0.190s, episode steps: 29, steps per second: 153, episode reward: 11.122, mean reward: 0.384 [0.186, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.881, 10.100], loss: 0.002356, mae: 0.048961, mean_q: -0.163462
 42281/100000: episode: 1019, duration: 0.183s, episode steps: 27, steps per second: 147, episode reward: 8.688, mean reward: 0.322 [0.117, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.299, 10.100], loss: 0.002884, mae: 0.055092, mean_q: -0.082023
 42331/100000: episode: 1020, duration: 0.266s, episode steps: 50, steps per second: 188, episode reward: 11.676, mean reward: 0.234 [0.034, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-1.428, 10.100], loss: 0.002372, mae: 0.049995, mean_q: -0.155653
 42349/100000: episode: 1021, duration: 0.118s, episode steps: 18, steps per second: 153, episode reward: 7.115, mean reward: 0.395 [0.315, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.245, 10.100], loss: 0.002833, mae: 0.054657, mean_q: -0.154840
 42378/100000: episode: 1022, duration: 0.196s, episode steps: 29, steps per second: 148, episode reward: 13.462, mean reward: 0.464 [0.327, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.374, 10.100], loss: 0.002532, mae: 0.051945, mean_q: -0.146378
 42405/100000: episode: 1023, duration: 0.205s, episode steps: 27, steps per second: 132, episode reward: 10.213, mean reward: 0.378 [0.282, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.122, 10.100], loss: 0.002491, mae: 0.049493, mean_q: -0.189605
 42418/100000: episode: 1024, duration: 0.094s, episode steps: 13, steps per second: 138, episode reward: 5.341, mean reward: 0.411 [0.345, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.539, 10.100], loss: 0.002764, mae: 0.053695, mean_q: -0.119072
 42431/100000: episode: 1025, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 5.020, mean reward: 0.386 [0.348, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.276, 10.100], loss: 0.002295, mae: 0.048517, mean_q: -0.063501
 42444/100000: episode: 1026, duration: 0.085s, episode steps: 13, steps per second: 153, episode reward: 5.119, mean reward: 0.394 [0.332, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.325, 10.100], loss: 0.002577, mae: 0.052995, mean_q: -0.113051
 42469/100000: episode: 1027, duration: 0.149s, episode steps: 25, steps per second: 168, episode reward: 5.582, mean reward: 0.223 [0.151, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.432, 10.398], loss: 0.002973, mae: 0.055977, mean_q: -0.105532
 42509/100000: episode: 1028, duration: 0.222s, episode steps: 40, steps per second: 180, episode reward: 14.459, mean reward: 0.361 [0.167, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.057, 10.366], loss: 0.003109, mae: 0.056531, mean_q: -0.128170
 42527/100000: episode: 1029, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 6.376, mean reward: 0.354 [0.219, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.477, 10.100], loss: 0.002760, mae: 0.053832, mean_q: -0.086941
 42540/100000: episode: 1030, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 5.476, mean reward: 0.421 [0.339, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.435, 10.100], loss: 0.002260, mae: 0.050375, mean_q: -0.138735
 42567/100000: episode: 1031, duration: 0.232s, episode steps: 27, steps per second: 116, episode reward: 8.739, mean reward: 0.324 [0.042, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-1.021, 10.164], loss: 0.002531, mae: 0.052265, mean_q: -0.136948
 42596/100000: episode: 1032, duration: 0.229s, episode steps: 29, steps per second: 127, episode reward: 10.528, mean reward: 0.363 [0.248, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.244, 10.100], loss: 0.002289, mae: 0.049298, mean_q: -0.114295
 42614/100000: episode: 1033, duration: 0.117s, episode steps: 18, steps per second: 154, episode reward: 8.898, mean reward: 0.494 [0.399, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.299, 10.100], loss: 0.002211, mae: 0.048782, mean_q: -0.098354
 42660/100000: episode: 1034, duration: 0.255s, episode steps: 46, steps per second: 180, episode reward: 8.365, mean reward: 0.182 [0.023, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.220, 10.115], loss: 0.002657, mae: 0.052763, mean_q: -0.069682
 42710/100000: episode: 1035, duration: 0.290s, episode steps: 50, steps per second: 172, episode reward: 12.659, mean reward: 0.253 [0.057, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.705, 10.148], loss: 0.002585, mae: 0.052223, mean_q: -0.030468
 42739/100000: episode: 1036, duration: 0.278s, episode steps: 29, steps per second: 104, episode reward: 8.476, mean reward: 0.292 [0.144, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.401, 10.318], loss: 0.002510, mae: 0.050372, mean_q: -0.049017
 42764/100000: episode: 1037, duration: 0.249s, episode steps: 25, steps per second: 101, episode reward: 6.487, mean reward: 0.259 [0.151, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.505, 10.272], loss: 0.002994, mae: 0.058170, mean_q: -0.058117
 42793/100000: episode: 1038, duration: 0.235s, episode steps: 29, steps per second: 123, episode reward: 11.972, mean reward: 0.413 [0.246, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.709, 10.531], loss: 0.002770, mae: 0.055593, mean_q: -0.046367
 42822/100000: episode: 1039, duration: 0.195s, episode steps: 29, steps per second: 149, episode reward: 11.086, mean reward: 0.382 [0.271, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.316, 10.100], loss: 0.002793, mae: 0.055793, mean_q: 0.006712
 42849/100000: episode: 1040, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 10.318, mean reward: 0.382 [0.233, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.083, 10.100], loss: 0.002827, mae: 0.055444, mean_q: -0.046190
 42889/100000: episode: 1041, duration: 0.322s, episode steps: 40, steps per second: 124, episode reward: 9.222, mean reward: 0.231 [0.071, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.316, 10.175], loss: 0.002602, mae: 0.053858, mean_q: -0.029015
 42935/100000: episode: 1042, duration: 0.383s, episode steps: 46, steps per second: 120, episode reward: 14.559, mean reward: 0.317 [0.184, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.273, 10.308], loss: 0.002610, mae: 0.052613, mean_q: -0.034374
 42962/100000: episode: 1043, duration: 0.163s, episode steps: 27, steps per second: 166, episode reward: 11.389, mean reward: 0.422 [0.294, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.318, 10.100], loss: 0.002690, mae: 0.055788, mean_q: -0.008515
 43008/100000: episode: 1044, duration: 0.268s, episode steps: 46, steps per second: 172, episode reward: 13.033, mean reward: 0.283 [0.106, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-1.277, 10.231], loss: 0.002848, mae: 0.054465, mean_q: -0.022343
 43037/100000: episode: 1045, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 9.310, mean reward: 0.321 [0.248, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.384, 10.410], loss: 0.002758, mae: 0.055443, mean_q: 0.031193
 43055/100000: episode: 1046, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 6.013, mean reward: 0.334 [0.247, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.088, 10.100], loss: 0.003077, mae: 0.058000, mean_q: -0.025472
 43084/100000: episode: 1047, duration: 0.176s, episode steps: 29, steps per second: 165, episode reward: 10.643, mean reward: 0.367 [0.272, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.380, 10.100], loss: 0.002478, mae: 0.052599, mean_q: -0.037661
 43126/100000: episode: 1048, duration: 0.362s, episode steps: 42, steps per second: 116, episode reward: 7.946, mean reward: 0.189 [0.026, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.979, 10.363], loss: 0.002554, mae: 0.053820, mean_q: 0.063982
 43172/100000: episode: 1049, duration: 0.234s, episode steps: 46, steps per second: 197, episode reward: 12.278, mean reward: 0.267 [0.147, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.842, 10.173], loss: 0.002517, mae: 0.052622, mean_q: 0.001689
 43214/100000: episode: 1050, duration: 0.206s, episode steps: 42, steps per second: 204, episode reward: 8.507, mean reward: 0.203 [0.053, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.493, 10.188], loss: 0.002577, mae: 0.051615, mean_q: 0.027305
 43264/100000: episode: 1051, duration: 0.244s, episode steps: 50, steps per second: 205, episode reward: 13.096, mean reward: 0.262 [0.073, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.310, 10.161], loss: 0.002726, mae: 0.054147, mean_q: 0.058274
 43291/100000: episode: 1052, duration: 0.160s, episode steps: 27, steps per second: 168, episode reward: 9.281, mean reward: 0.344 [0.206, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.373, 10.100], loss: 0.002290, mae: 0.050641, mean_q: -0.012323
 43320/100000: episode: 1053, duration: 0.211s, episode steps: 29, steps per second: 138, episode reward: 7.394, mean reward: 0.255 [0.128, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.179, 10.100], loss: 0.002994, mae: 0.057542, mean_q: 0.089657
 43366/100000: episode: 1054, duration: 0.325s, episode steps: 46, steps per second: 142, episode reward: 9.072, mean reward: 0.197 [0.004, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.335, 10.100], loss: 0.002631, mae: 0.054800, mean_q: 0.094176
 43384/100000: episode: 1055, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 7.497, mean reward: 0.416 [0.272, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.379, 10.100], loss: 0.002694, mae: 0.054781, mean_q: 0.045388
 43402/100000: episode: 1056, duration: 0.088s, episode steps: 18, steps per second: 205, episode reward: 6.615, mean reward: 0.368 [0.232, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.324, 10.100], loss: 0.002937, mae: 0.056217, mean_q: 0.125755
 43427/100000: episode: 1057, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 7.422, mean reward: 0.297 [0.180, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.338], loss: 0.003214, mae: 0.058716, mean_q: 0.064720
 43452/100000: episode: 1058, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 9.094, mean reward: 0.364 [0.237, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.655, 10.378], loss: 0.002654, mae: 0.054677, mean_q: 0.089356
 43470/100000: episode: 1059, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 7.400, mean reward: 0.411 [0.358, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.627, 10.100], loss: 0.002623, mae: 0.055935, mean_q: 0.062866
 43520/100000: episode: 1060, duration: 0.249s, episode steps: 50, steps per second: 201, episode reward: 11.169, mean reward: 0.223 [0.055, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-1.094, 10.100], loss: 0.002807, mae: 0.057364, mean_q: 0.084619
 43562/100000: episode: 1061, duration: 0.210s, episode steps: 42, steps per second: 200, episode reward: 11.309, mean reward: 0.269 [0.170, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.943, 10.344], loss: 0.003179, mae: 0.060534, mean_q: 0.087946
 43587/100000: episode: 1062, duration: 0.133s, episode steps: 25, steps per second: 189, episode reward: 6.504, mean reward: 0.260 [0.116, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.550], loss: 0.002710, mae: 0.056345, mean_q: 0.119600
 43627/100000: episode: 1063, duration: 0.233s, episode steps: 40, steps per second: 172, episode reward: 11.830, mean reward: 0.296 [0.090, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.661, 10.260], loss: 0.002889, mae: 0.055862, mean_q: 0.100745
 43667/100000: episode: 1064, duration: 0.222s, episode steps: 40, steps per second: 180, episode reward: 9.459, mean reward: 0.236 [0.085, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.411, 10.259], loss: 0.002938, mae: 0.057318, mean_q: 0.131388
 43685/100000: episode: 1065, duration: 0.087s, episode steps: 18, steps per second: 208, episode reward: 8.216, mean reward: 0.456 [0.340, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.417, 10.100], loss: 0.002932, mae: 0.058740, mean_q: 0.093091
 43714/100000: episode: 1066, duration: 0.277s, episode steps: 29, steps per second: 105, episode reward: 11.547, mean reward: 0.398 [0.186, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.836, 10.588], loss: 0.007534, mae: 0.076602, mean_q: 0.125721
 43732/100000: episode: 1067, duration: 0.150s, episode steps: 18, steps per second: 120, episode reward: 6.115, mean reward: 0.340 [0.242, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.259, 10.100], loss: 0.003623, mae: 0.063247, mean_q: 0.161312
 43782/100000: episode: 1068, duration: 0.462s, episode steps: 50, steps per second: 108, episode reward: 10.847, mean reward: 0.217 [0.039, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.316, 10.309], loss: 0.003631, mae: 0.060921, mean_q: 0.183937
 43824/100000: episode: 1069, duration: 0.262s, episode steps: 42, steps per second: 160, episode reward: 7.915, mean reward: 0.188 [0.027, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.286, 10.128], loss: 0.002729, mae: 0.055291, mean_q: 0.164040
 43874/100000: episode: 1070, duration: 0.271s, episode steps: 50, steps per second: 184, episode reward: 12.879, mean reward: 0.258 [0.087, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.333, 10.170], loss: 0.003232, mae: 0.058337, mean_q: 0.162639
 43892/100000: episode: 1071, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 7.599, mean reward: 0.422 [0.364, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.299, 10.100], loss: 0.003048, mae: 0.058172, mean_q: 0.100637
 43919/100000: episode: 1072, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 9.002, mean reward: 0.333 [0.228, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.355, 10.100], loss: 0.002791, mae: 0.056760, mean_q: 0.139825
 43948/100000: episode: 1073, duration: 0.318s, episode steps: 29, steps per second: 91, episode reward: 6.728, mean reward: 0.232 [0.116, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.035, 10.357], loss: 0.003084, mae: 0.058040, mean_q: 0.123363
 43990/100000: episode: 1074, duration: 0.305s, episode steps: 42, steps per second: 138, episode reward: 10.351, mean reward: 0.246 [0.127, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.154, 10.296], loss: 0.003771, mae: 0.064014, mean_q: 0.220585
 44040/100000: episode: 1075, duration: 0.404s, episode steps: 50, steps per second: 124, episode reward: 13.434, mean reward: 0.269 [0.057, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-1.057, 10.100], loss: 0.002756, mae: 0.055931, mean_q: 0.157943
[Info] 200-TH LEVEL FOUND: 0.933738648891449, Considering 10/90 traces
 44069/100000: episode: 1076, duration: 4.632s, episode steps: 29, steps per second: 6, episode reward: 10.276, mean reward: 0.354 [0.291, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.106, 10.100], loss: 0.002740, mae: 0.055777, mean_q: 0.197907
 44084/100000: episode: 1077, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 5.745, mean reward: 0.383 [0.281, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.237, 10.100], loss: 0.002520, mae: 0.053404, mean_q: 0.150082
 44096/100000: episode: 1078, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 6.128, mean reward: 0.511 [0.445, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.637, 10.100], loss: 0.002435, mae: 0.052472, mean_q: 0.167741
 44113/100000: episode: 1079, duration: 0.121s, episode steps: 17, steps per second: 141, episode reward: 6.866, mean reward: 0.404 [0.319, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.360, 10.100], loss: 0.002535, mae: 0.054696, mean_q: 0.185505
 44130/100000: episode: 1080, duration: 0.138s, episode steps: 17, steps per second: 123, episode reward: 7.581, mean reward: 0.446 [0.363, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.370, 10.100], loss: 0.002632, mae: 0.053403, mean_q: 0.186827
 44160/100000: episode: 1081, duration: 0.224s, episode steps: 30, steps per second: 134, episode reward: 14.953, mean reward: 0.498 [0.361, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.259, 10.527], loss: 0.002628, mae: 0.054413, mean_q: 0.177328
 44168/100000: episode: 1082, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 4.496, mean reward: 0.562 [0.510, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.627, 10.100], loss: 0.002615, mae: 0.055666, mean_q: 0.278313
 44174/100000: episode: 1083, duration: 0.052s, episode steps: 6, steps per second: 114, episode reward: 3.142, mean reward: 0.524 [0.456, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.746, 10.100], loss: 0.002596, mae: 0.054308, mean_q: 0.244503
 44187/100000: episode: 1084, duration: 0.108s, episode steps: 13, steps per second: 120, episode reward: 5.781, mean reward: 0.445 [0.350, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.330, 10.100], loss: 0.002470, mae: 0.051774, mean_q: 0.158171
 44200/100000: episode: 1085, duration: 0.085s, episode steps: 13, steps per second: 153, episode reward: 5.556, mean reward: 0.427 [0.362, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.428, 10.100], loss: 0.002385, mae: 0.052555, mean_q: 0.228866
 44217/100000: episode: 1086, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 6.112, mean reward: 0.360 [0.304, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.338, 10.100], loss: 0.002927, mae: 0.057484, mean_q: 0.187300
 44234/100000: episode: 1087, duration: 0.124s, episode steps: 17, steps per second: 137, episode reward: 9.371, mean reward: 0.551 [0.412, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.654, 10.100], loss: 0.002947, mae: 0.059389, mean_q: 0.207198
 44249/100000: episode: 1088, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 5.705, mean reward: 0.380 [0.320, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.278, 10.100], loss: 0.003218, mae: 0.058249, mean_q: 0.196634
 44266/100000: episode: 1089, duration: 0.124s, episode steps: 17, steps per second: 137, episode reward: 7.174, mean reward: 0.422 [0.363, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.372, 10.100], loss: 0.002296, mae: 0.052008, mean_q: 0.273508
 44296/100000: episode: 1090, duration: 0.192s, episode steps: 30, steps per second: 156, episode reward: 13.314, mean reward: 0.444 [0.337, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.124, 10.562], loss: 0.002650, mae: 0.054649, mean_q: 0.259849
 44326/100000: episode: 1091, duration: 0.175s, episode steps: 30, steps per second: 172, episode reward: 9.704, mean reward: 0.323 [0.241, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.734, 10.321], loss: 0.002610, mae: 0.053782, mean_q: 0.228181
 44339/100000: episode: 1092, duration: 0.089s, episode steps: 13, steps per second: 147, episode reward: 6.794, mean reward: 0.523 [0.435, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.394, 10.100], loss: 0.002325, mae: 0.052503, mean_q: 0.227448
 44369/100000: episode: 1093, duration: 0.203s, episode steps: 30, steps per second: 148, episode reward: 11.457, mean reward: 0.382 [0.204, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.137, 10.359], loss: 0.002797, mae: 0.056559, mean_q: 0.277190
 44384/100000: episode: 1094, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 6.803, mean reward: 0.454 [0.356, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.450, 10.100], loss: 0.002825, mae: 0.058511, mean_q: 0.277953
 44399/100000: episode: 1095, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 6.075, mean reward: 0.405 [0.344, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.413, 10.100], loss: 0.002721, mae: 0.054289, mean_q: 0.271904
 44416/100000: episode: 1096, duration: 0.191s, episode steps: 17, steps per second: 89, episode reward: 7.404, mean reward: 0.436 [0.303, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.414, 10.100], loss: 0.002786, mae: 0.057522, mean_q: 0.295608
 44428/100000: episode: 1097, duration: 0.094s, episode steps: 12, steps per second: 128, episode reward: 5.982, mean reward: 0.498 [0.405, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-1.112, 10.100], loss: 0.003140, mae: 0.059114, mean_q: 0.322442
 44436/100000: episode: 1098, duration: 0.042s, episode steps: 8, steps per second: 193, episode reward: 4.653, mean reward: 0.582 [0.512, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.519, 10.100], loss: 0.003060, mae: 0.058016, mean_q: 0.197594
 44448/100000: episode: 1099, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 6.876, mean reward: 0.573 [0.468, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.555, 10.100], loss: 0.002802, mae: 0.058538, mean_q: 0.311793
 44457/100000: episode: 1100, duration: 0.084s, episode steps: 9, steps per second: 107, episode reward: 4.963, mean reward: 0.551 [0.488, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.686], loss: 0.002632, mae: 0.057824, mean_q: 0.351420
 44476/100000: episode: 1101, duration: 0.215s, episode steps: 19, steps per second: 88, episode reward: 8.928, mean reward: 0.470 [0.375, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.415, 10.100], loss: 0.002813, mae: 0.056204, mean_q: 0.269439
 44491/100000: episode: 1102, duration: 0.107s, episode steps: 15, steps per second: 141, episode reward: 6.478, mean reward: 0.432 [0.367, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.515, 10.100], loss: 0.002746, mae: 0.056592, mean_q: 0.321731
 44497/100000: episode: 1103, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 2.925, mean reward: 0.488 [0.440, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.524, 10.100], loss: 0.002394, mae: 0.053054, mean_q: 0.310762
 44509/100000: episode: 1104, duration: 0.127s, episode steps: 12, steps per second: 94, episode reward: 6.755, mean reward: 0.563 [0.489, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.393, 10.100], loss: 0.003006, mae: 0.060011, mean_q: 0.288807
 44518/100000: episode: 1105, duration: 0.093s, episode steps: 9, steps per second: 97, episode reward: 5.459, mean reward: 0.607 [0.535, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.661], loss: 0.003106, mae: 0.060608, mean_q: 0.408100
 44527/100000: episode: 1106, duration: 0.118s, episode steps: 9, steps per second: 76, episode reward: 4.051, mean reward: 0.450 [0.388, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.657], loss: 0.002513, mae: 0.053365, mean_q: 0.310749
 44533/100000: episode: 1107, duration: 0.066s, episode steps: 6, steps per second: 91, episode reward: 2.595, mean reward: 0.432 [0.382, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.352, 10.100], loss: 0.002955, mae: 0.059060, mean_q: 0.312576
 44541/100000: episode: 1108, duration: 0.062s, episode steps: 8, steps per second: 129, episode reward: 3.941, mean reward: 0.493 [0.449, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.211, 10.100], loss: 0.002744, mae: 0.056724, mean_q: 0.349541
 44556/100000: episode: 1109, duration: 0.105s, episode steps: 15, steps per second: 142, episode reward: 5.384, mean reward: 0.359 [0.280, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.869, 10.100], loss: 0.003140, mae: 0.058905, mean_q: 0.304356
 44569/100000: episode: 1110, duration: 0.102s, episode steps: 13, steps per second: 127, episode reward: 5.238, mean reward: 0.403 [0.320, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.556, 10.100], loss: 0.002999, mae: 0.058708, mean_q: 0.264654
 44582/100000: episode: 1111, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 6.492, mean reward: 0.499 [0.397, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.874, 10.100], loss: 0.002400, mae: 0.054079, mean_q: 0.345602
 44596/100000: episode: 1112, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 7.721, mean reward: 0.551 [0.508, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.564, 10.100], loss: 0.002597, mae: 0.054758, mean_q: 0.331191
 44611/100000: episode: 1113, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 6.312, mean reward: 0.421 [0.374, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.536, 10.100], loss: 0.002927, mae: 0.057837, mean_q: 0.298880
 44626/100000: episode: 1114, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 6.752, mean reward: 0.450 [0.382, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.405, 10.100], loss: 0.003001, mae: 0.059175, mean_q: 0.380407
 44640/100000: episode: 1115, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 6.686, mean reward: 0.478 [0.420, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.505, 10.100], loss: 0.003315, mae: 0.062032, mean_q: 0.304804
 44657/100000: episode: 1116, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 8.594, mean reward: 0.506 [0.395, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.775, 10.100], loss: 0.003283, mae: 0.061537, mean_q: 0.318271
 44670/100000: episode: 1117, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 6.056, mean reward: 0.466 [0.387, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.441, 10.100], loss: 0.002987, mae: 0.060782, mean_q: 0.384712
 44682/100000: episode: 1118, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 5.901, mean reward: 0.492 [0.423, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.445, 10.100], loss: 0.003043, mae: 0.060154, mean_q: 0.355659
 44699/100000: episode: 1119, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 8.690, mean reward: 0.511 [0.442, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.927, 10.100], loss: 0.002509, mae: 0.054335, mean_q: 0.347428
 44716/100000: episode: 1120, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 7.618, mean reward: 0.448 [0.340, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.325, 10.100], loss: 0.003084, mae: 0.059865, mean_q: 0.383027
 44725/100000: episode: 1121, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 4.658, mean reward: 0.518 [0.463, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.580, 10.604], loss: 0.002809, mae: 0.056824, mean_q: 0.380273
 44739/100000: episode: 1122, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 5.951, mean reward: 0.425 [0.325, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.427, 10.100], loss: 0.002805, mae: 0.057526, mean_q: 0.396388
 44769/100000: episode: 1123, duration: 0.169s, episode steps: 30, steps per second: 177, episode reward: 11.629, mean reward: 0.388 [0.238, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.567, 10.581], loss: 0.002490, mae: 0.053622, mean_q: 0.371181
 44799/100000: episode: 1124, duration: 0.192s, episode steps: 30, steps per second: 157, episode reward: 10.461, mean reward: 0.349 [0.124, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.411, 10.278], loss: 0.002745, mae: 0.056823, mean_q: 0.369618
 44829/100000: episode: 1125, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 11.323, mean reward: 0.377 [0.123, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.308, 10.378], loss: 0.002665, mae: 0.055475, mean_q: 0.390275
 44846/100000: episode: 1126, duration: 0.109s, episode steps: 17, steps per second: 156, episode reward: 6.873, mean reward: 0.404 [0.326, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.768, 10.100], loss: 0.002624, mae: 0.057100, mean_q: 0.395903
 44865/100000: episode: 1127, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 9.078, mean reward: 0.478 [0.378, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.463, 10.100], loss: 0.003003, mae: 0.060599, mean_q: 0.385914
 44874/100000: episode: 1128, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 4.226, mean reward: 0.470 [0.424, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.586], loss: 0.003469, mae: 0.062762, mean_q: 0.405164
 44893/100000: episode: 1129, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 9.523, mean reward: 0.501 [0.417, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.498, 10.100], loss: 0.002499, mae: 0.054481, mean_q: 0.363758
 44902/100000: episode: 1130, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 3.666, mean reward: 0.407 [0.346, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.411], loss: 0.002802, mae: 0.059565, mean_q: 0.403707
 44908/100000: episode: 1131, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 2.963, mean reward: 0.494 [0.463, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.474, 10.100], loss: 0.002765, mae: 0.056554, mean_q: 0.380860
 44925/100000: episode: 1132, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 7.563, mean reward: 0.445 [0.356, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.382, 10.100], loss: 0.002644, mae: 0.055626, mean_q: 0.434771
 44940/100000: episode: 1133, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 6.460, mean reward: 0.431 [0.368, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.486, 10.100], loss: 0.002844, mae: 0.057983, mean_q: 0.426318
 44953/100000: episode: 1134, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 5.398, mean reward: 0.415 [0.366, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.471, 10.100], loss: 0.002359, mae: 0.052663, mean_q: 0.403868
 44961/100000: episode: 1135, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 4.076, mean reward: 0.510 [0.460, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.466, 10.100], loss: 0.002179, mae: 0.048933, mean_q: 0.395922
 44970/100000: episode: 1136, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 5.191, mean reward: 0.577 [0.508, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.582], loss: 0.002431, mae: 0.054574, mean_q: 0.357667
 44978/100000: episode: 1137, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 4.377, mean reward: 0.547 [0.496, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.429, 10.100], loss: 0.002998, mae: 0.054174, mean_q: 0.420580
[Info] FALSIFICATION!
 44983/100000: episode: 1138, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 12.104, mean reward: 2.421 [0.463, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.014, 10.775], loss: 0.003149, mae: 0.062791, mean_q: 0.396352
 45083/100000: episode: 1139, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: -19.068, mean reward: -0.191 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.177, 10.098], loss: 0.005658, mae: 0.066096, mean_q: 0.424691
 45183/100000: episode: 1140, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: -12.237, mean reward: -0.122 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.842, 10.098], loss: 0.004690, mae: 0.071432, mean_q: 0.419553
 45283/100000: episode: 1141, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: -14.083, mean reward: -0.141 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.473, 10.284], loss: 0.016069, mae: 0.062524, mean_q: 0.419938
 45383/100000: episode: 1142, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -16.654, mean reward: -0.167 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.670, 10.260], loss: 0.004326, mae: 0.068406, mean_q: 0.417742
 45483/100000: episode: 1143, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: -12.066, mean reward: -0.121 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.090, 10.261], loss: 0.030006, mae: 0.077377, mean_q: 0.419485
 45583/100000: episode: 1144, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: -17.724, mean reward: -0.177 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.516, 10.098], loss: 0.003175, mae: 0.061136, mean_q: 0.423738
 45683/100000: episode: 1145, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -19.243, mean reward: -0.192 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.937, 10.190], loss: 0.029799, mae: 0.076882, mean_q: 0.415439
 45783/100000: episode: 1146, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -11.908, mean reward: -0.119 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.432, 10.098], loss: 0.055435, mae: 0.089026, mean_q: 0.438507
 45883/100000: episode: 1147, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.807, mean reward: -0.178 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.820, 10.298], loss: 0.016306, mae: 0.070024, mean_q: 0.416390
 45983/100000: episode: 1148, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -17.725, mean reward: -0.177 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.543, 10.098], loss: 0.002992, mae: 0.059939, mean_q: 0.423698
 46083/100000: episode: 1149, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -16.080, mean reward: -0.161 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.228, 10.098], loss: 0.003370, mae: 0.060953, mean_q: 0.391801
 46183/100000: episode: 1150, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -9.722, mean reward: -0.097 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.116, 10.167], loss: 0.016260, mae: 0.067473, mean_q: 0.419108
 46283/100000: episode: 1151, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -13.344, mean reward: -0.133 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.566, 10.098], loss: 0.003022, mae: 0.057319, mean_q: 0.399911
 46383/100000: episode: 1152, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: -17.579, mean reward: -0.176 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.644, 10.098], loss: 0.015597, mae: 0.062451, mean_q: 0.389564
 46483/100000: episode: 1153, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -10.290, mean reward: -0.103 [-1.000, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.633, 10.098], loss: 0.003016, mae: 0.058704, mean_q: 0.371454
 46583/100000: episode: 1154, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.670, mean reward: -0.167 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.952, 10.098], loss: 0.002718, mae: 0.056414, mean_q: 0.369362
 46683/100000: episode: 1155, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.095, mean reward: -0.171 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.200, 10.098], loss: 0.029546, mae: 0.077279, mean_q: 0.310134
 46783/100000: episode: 1156, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.182, mean reward: -0.142 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.836, 10.118], loss: 0.002778, mae: 0.056772, mean_q: 0.313157
 46883/100000: episode: 1157, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.497, mean reward: -0.175 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.865, 10.180], loss: 0.015841, mae: 0.063141, mean_q: 0.288259
 46983/100000: episode: 1158, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -14.806, mean reward: -0.148 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.605, 10.098], loss: 0.017649, mae: 0.067804, mean_q: 0.265861
 47083/100000: episode: 1159, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -8.660, mean reward: -0.087 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.576, 10.510], loss: 0.003157, mae: 0.059820, mean_q: 0.259927
 47183/100000: episode: 1160, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -15.524, mean reward: -0.155 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.654, 10.098], loss: 0.004118, mae: 0.062217, mean_q: 0.242607
 47283/100000: episode: 1161, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.836, mean reward: -0.168 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.398, 10.098], loss: 0.002677, mae: 0.055177, mean_q: 0.236683
 47383/100000: episode: 1162, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -19.139, mean reward: -0.191 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.038, 10.109], loss: 0.015782, mae: 0.063836, mean_q: 0.226855
 47483/100000: episode: 1163, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.053, mean reward: -0.181 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.191, 10.355], loss: 0.015737, mae: 0.061974, mean_q: 0.182851
 47583/100000: episode: 1164, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -15.768, mean reward: -0.158 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.092, 10.127], loss: 0.002613, mae: 0.053961, mean_q: 0.170425
 47683/100000: episode: 1165, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.350, mean reward: -0.184 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.725, 10.098], loss: 0.030823, mae: 0.081651, mean_q: 0.153475
 47783/100000: episode: 1166, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.547, mean reward: -0.195 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.368, 10.102], loss: 0.003296, mae: 0.060378, mean_q: 0.141693
 47883/100000: episode: 1167, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -12.944, mean reward: -0.129 [-1.000, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.723, 10.098], loss: 0.002779, mae: 0.055616, mean_q: 0.099781
 47983/100000: episode: 1168, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.101, mean reward: -0.161 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.556, 10.241], loss: 0.002708, mae: 0.055029, mean_q: 0.083153
 48083/100000: episode: 1169, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.209, mean reward: -0.162 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.320, 10.098], loss: 0.015853, mae: 0.062413, mean_q: 0.073925
 48183/100000: episode: 1170, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -14.539, mean reward: -0.145 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.419, 10.098], loss: 0.015800, mae: 0.062931, mean_q: 0.049315
 48283/100000: episode: 1171, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.490, mean reward: -0.175 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.485, 10.098], loss: 0.015500, mae: 0.060706, mean_q: 0.047593
 48383/100000: episode: 1172, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.386, mean reward: -0.174 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.563, 10.229], loss: 0.002649, mae: 0.053160, mean_q: 0.000547
 48483/100000: episode: 1173, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -12.306, mean reward: -0.123 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.406, 10.098], loss: 0.017267, mae: 0.071610, mean_q: 0.039411
 48583/100000: episode: 1174, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -15.261, mean reward: -0.153 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.077, 10.098], loss: 0.019210, mae: 0.074342, mean_q: -0.007325
 48683/100000: episode: 1175, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.519, mean reward: -0.175 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.349, 10.098], loss: 0.004265, mae: 0.063833, mean_q: -0.034249
 48783/100000: episode: 1176, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -14.343, mean reward: -0.143 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.024, 10.098], loss: 0.042404, mae: 0.078034, mean_q: -0.063732
 48883/100000: episode: 1177, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.630, mean reward: -0.176 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.913, 10.117], loss: 0.028356, mae: 0.075019, mean_q: -0.042607
 48983/100000: episode: 1178, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -12.572, mean reward: -0.126 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.029, 10.098], loss: 0.003641, mae: 0.060997, mean_q: -0.086377
 49083/100000: episode: 1179, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -19.931, mean reward: -0.199 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.704, 10.141], loss: 0.003319, mae: 0.057522, mean_q: -0.089294
 49183/100000: episode: 1180, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -11.652, mean reward: -0.117 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.978, 10.529], loss: 0.016281, mae: 0.064447, mean_q: -0.097468
 49283/100000: episode: 1181, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -13.824, mean reward: -0.138 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.510, 10.100], loss: 0.003468, mae: 0.059452, mean_q: -0.111831
 49383/100000: episode: 1182, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -19.354, mean reward: -0.194 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.213, 10.159], loss: 0.016022, mae: 0.062620, mean_q: -0.169360
 49483/100000: episode: 1183, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -18.279, mean reward: -0.183 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.861, 10.120], loss: 0.003153, mae: 0.055475, mean_q: -0.196569
 49583/100000: episode: 1184, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -15.572, mean reward: -0.156 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.234, 10.098], loss: 0.002892, mae: 0.054123, mean_q: -0.236824
 49683/100000: episode: 1185, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -17.912, mean reward: -0.179 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.792, 10.116], loss: 0.016256, mae: 0.063595, mean_q: -0.205036
 49783/100000: episode: 1186, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -15.447, mean reward: -0.154 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.806, 10.098], loss: 0.002489, mae: 0.048765, mean_q: -0.301950
 49883/100000: episode: 1187, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -13.533, mean reward: -0.135 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.198, 10.098], loss: 0.002563, mae: 0.049805, mean_q: -0.287931
 49983/100000: episode: 1188, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: -19.780, mean reward: -0.198 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.788, 10.199], loss: 0.002491, mae: 0.049478, mean_q: -0.304764
 50083/100000: episode: 1189, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.813, mean reward: -0.198 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.065, 10.098], loss: 0.002647, mae: 0.050944, mean_q: -0.287532
 50183/100000: episode: 1190, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: -16.483, mean reward: -0.165 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.852, 10.098], loss: 0.002558, mae: 0.050862, mean_q: -0.282295
 50283/100000: episode: 1191, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: -17.300, mean reward: -0.173 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.835, 10.173], loss: 0.002611, mae: 0.051527, mean_q: -0.254598
 50383/100000: episode: 1192, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -18.729, mean reward: -0.187 [-1.000, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.889, 10.098], loss: 0.002602, mae: 0.050498, mean_q: -0.276517
 50483/100000: episode: 1193, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.309, mean reward: -0.173 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.826, 10.250], loss: 0.002614, mae: 0.052255, mean_q: -0.310247
 50583/100000: episode: 1194, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -12.647, mean reward: -0.126 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.244, 10.404], loss: 0.002646, mae: 0.051583, mean_q: -0.305410
 50683/100000: episode: 1195, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.700, mean reward: -0.177 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.186, 10.098], loss: 0.003008, mae: 0.055272, mean_q: -0.306406
 50783/100000: episode: 1196, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: -14.877, mean reward: -0.149 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.516, 10.157], loss: 0.002616, mae: 0.051087, mean_q: -0.291266
 50883/100000: episode: 1197, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: -16.504, mean reward: -0.165 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.038, 10.150], loss: 0.002302, mae: 0.047103, mean_q: -0.305687
 50983/100000: episode: 1198, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.598, mean reward: -0.176 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.131, 10.103], loss: 0.002385, mae: 0.048059, mean_q: -0.299320
 51083/100000: episode: 1199, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: -16.583, mean reward: -0.166 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.438, 10.251], loss: 0.002622, mae: 0.050543, mean_q: -0.310728
 51183/100000: episode: 1200, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: -18.675, mean reward: -0.187 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.706, 10.098], loss: 0.002801, mae: 0.054049, mean_q: -0.295487
 51283/100000: episode: 1201, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -17.605, mean reward: -0.176 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.234, 10.113], loss: 0.002503, mae: 0.050162, mean_q: -0.323853
 51383/100000: episode: 1202, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -14.840, mean reward: -0.148 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.697, 10.313], loss: 0.002512, mae: 0.050059, mean_q: -0.291771
 51483/100000: episode: 1203, duration: 0.681s, episode steps: 100, steps per second: 147, episode reward: -19.106, mean reward: -0.191 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.069, 10.104], loss: 0.002549, mae: 0.049774, mean_q: -0.311774
 51583/100000: episode: 1204, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: -16.026, mean reward: -0.160 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.024, 10.098], loss: 0.002499, mae: 0.049279, mean_q: -0.309974
 51683/100000: episode: 1205, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -17.183, mean reward: -0.172 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.350, 10.098], loss: 0.002543, mae: 0.050527, mean_q: -0.299705
 51783/100000: episode: 1206, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -15.729, mean reward: -0.157 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.133, 10.296], loss: 0.002428, mae: 0.049804, mean_q: -0.298952
 51883/100000: episode: 1207, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -15.446, mean reward: -0.154 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.801, 10.098], loss: 0.002382, mae: 0.048162, mean_q: -0.321273
 51983/100000: episode: 1208, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -16.101, mean reward: -0.161 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.527, 10.098], loss: 0.002515, mae: 0.050792, mean_q: -0.317974
 52083/100000: episode: 1209, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -16.006, mean reward: -0.160 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.455, 10.098], loss: 0.002573, mae: 0.053124, mean_q: -0.275949
 52183/100000: episode: 1210, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -14.020, mean reward: -0.140 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.435, 10.378], loss: 0.002508, mae: 0.050332, mean_q: -0.317788
 52283/100000: episode: 1211, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -18.359, mean reward: -0.184 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.156, 10.098], loss: 0.002353, mae: 0.048247, mean_q: -0.279981
 52383/100000: episode: 1212, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.653, mean reward: -0.187 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.447, 10.098], loss: 0.002404, mae: 0.048367, mean_q: -0.320558
 52483/100000: episode: 1213, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: -15.450, mean reward: -0.155 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.840, 10.135], loss: 0.002582, mae: 0.050060, mean_q: -0.284722
 52583/100000: episode: 1214, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -19.557, mean reward: -0.196 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.974, 10.147], loss: 0.002631, mae: 0.052639, mean_q: -0.332707
 52683/100000: episode: 1215, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -13.783, mean reward: -0.138 [-1.000, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.844, 10.098], loss: 0.002497, mae: 0.050067, mean_q: -0.314745
 52783/100000: episode: 1216, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: -18.129, mean reward: -0.181 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.610, 10.098], loss: 0.002499, mae: 0.050562, mean_q: -0.305229
 52883/100000: episode: 1217, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: -15.004, mean reward: -0.150 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.350, 10.098], loss: 0.002576, mae: 0.052878, mean_q: -0.322725
 52983/100000: episode: 1218, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -17.728, mean reward: -0.177 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.866, 10.098], loss: 0.002291, mae: 0.047358, mean_q: -0.318508
 53083/100000: episode: 1219, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.380, mean reward: -0.184 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.109, 10.149], loss: 0.002495, mae: 0.050355, mean_q: -0.340577
 53183/100000: episode: 1220, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -15.260, mean reward: -0.153 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.589, 10.391], loss: 0.002519, mae: 0.049917, mean_q: -0.316666
 53283/100000: episode: 1221, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.455, mean reward: -0.185 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.753, 10.352], loss: 0.002455, mae: 0.049142, mean_q: -0.317578
 53383/100000: episode: 1222, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: -14.475, mean reward: -0.145 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.565, 10.131], loss: 0.004728, mae: 0.068354, mean_q: -0.337323
 53483/100000: episode: 1223, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -17.774, mean reward: -0.178 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.196, 10.206], loss: 0.002649, mae: 0.051859, mean_q: -0.291156
 53583/100000: episode: 1224, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.784, mean reward: -0.178 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.571, 10.160], loss: 0.002530, mae: 0.049562, mean_q: -0.327906
 53683/100000: episode: 1225, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -18.686, mean reward: -0.187 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.435, 10.098], loss: 0.002348, mae: 0.047794, mean_q: -0.303775
 53783/100000: episode: 1226, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: -17.294, mean reward: -0.173 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.998, 10.448], loss: 0.002328, mae: 0.048163, mean_q: -0.317010
 53883/100000: episode: 1227, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -18.750, mean reward: -0.188 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.356, 10.146], loss: 0.002629, mae: 0.049839, mean_q: -0.322774
 53983/100000: episode: 1228, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.058, mean reward: -0.151 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.358, 10.098], loss: 0.002422, mae: 0.049224, mean_q: -0.305513
 54083/100000: episode: 1229, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.436, mean reward: -0.154 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.731, 10.098], loss: 0.002443, mae: 0.049573, mean_q: -0.340554
 54183/100000: episode: 1230, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.687, mean reward: -0.157 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.731, 10.098], loss: 0.002278, mae: 0.047088, mean_q: -0.335116
 54283/100000: episode: 1231, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.843, mean reward: -0.178 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.649, 10.098], loss: 0.002444, mae: 0.049312, mean_q: -0.303716
 54383/100000: episode: 1232, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.611, mean reward: -0.186 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.124, 10.098], loss: 0.002514, mae: 0.049967, mean_q: -0.287388
 54483/100000: episode: 1233, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -12.964, mean reward: -0.130 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.796, 10.098], loss: 0.002529, mae: 0.049772, mean_q: -0.313366
 54583/100000: episode: 1234, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: -16.714, mean reward: -0.167 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.674, 10.130], loss: 0.002481, mae: 0.051265, mean_q: -0.309388
 54683/100000: episode: 1235, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: -18.346, mean reward: -0.183 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.597, 10.098], loss: 0.002344, mae: 0.047900, mean_q: -0.337698
 54783/100000: episode: 1236, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: -19.319, mean reward: -0.193 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.079, 10.099], loss: 0.002275, mae: 0.047580, mean_q: -0.325235
 54883/100000: episode: 1237, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -9.139, mean reward: -0.091 [-1.000, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.995, 10.098], loss: 0.002378, mae: 0.048362, mean_q: -0.328038
[Info] 100-TH LEVEL FOUND: 0.6319605112075806, Considering 10/90 traces
 54983/100000: episode: 1238, duration: 4.729s, episode steps: 100, steps per second: 21, episode reward: -17.059, mean reward: -0.171 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.554, 10.191], loss: 0.002531, mae: 0.050545, mean_q: -0.319666
 55010/100000: episode: 1239, duration: 0.160s, episode steps: 27, steps per second: 169, episode reward: 7.821, mean reward: 0.290 [0.214, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.397, 10.100], loss: 0.002692, mae: 0.052255, mean_q: -0.243604
 55023/100000: episode: 1240, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 4.186, mean reward: 0.322 [0.240, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.461, 10.100], loss: 0.002720, mae: 0.052166, mean_q: -0.283681
 55068/100000: episode: 1241, duration: 0.244s, episode steps: 45, steps per second: 184, episode reward: 19.474, mean reward: 0.433 [0.283, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-1.884, 10.100], loss: 0.002590, mae: 0.051727, mean_q: -0.319975
 55093/100000: episode: 1242, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 6.741, mean reward: 0.270 [0.152, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.153, 10.100], loss: 0.002409, mae: 0.050944, mean_q: -0.283112
 55138/100000: episode: 1243, duration: 0.230s, episode steps: 45, steps per second: 195, episode reward: 11.769, mean reward: 0.262 [0.037, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.664, 10.100], loss: 0.002625, mae: 0.052078, mean_q: -0.277186
 55165/100000: episode: 1244, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 11.908, mean reward: 0.441 [0.332, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.324, 10.100], loss: 0.002491, mae: 0.050208, mean_q: -0.291777
 55195/100000: episode: 1245, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 10.590, mean reward: 0.353 [0.276, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.335, 10.100], loss: 0.002534, mae: 0.051549, mean_q: -0.249146
 55220/100000: episode: 1246, duration: 0.151s, episode steps: 25, steps per second: 165, episode reward: 6.718, mean reward: 0.269 [0.190, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.309, 10.100], loss: 0.002488, mae: 0.050290, mean_q: -0.252242
 55238/100000: episode: 1247, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 4.538, mean reward: 0.252 [0.193, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.611, 10.100], loss: 0.002433, mae: 0.049113, mean_q: -0.295316
 55281/100000: episode: 1248, duration: 0.240s, episode steps: 43, steps per second: 180, episode reward: 12.925, mean reward: 0.301 [0.074, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-1.946, 10.100], loss: 0.002762, mae: 0.054618, mean_q: -0.221927
 55306/100000: episode: 1249, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 8.050, mean reward: 0.322 [0.224, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.714, 10.100], loss: 0.002583, mae: 0.051509, mean_q: -0.267972
 55361/100000: episode: 1250, duration: 0.371s, episode steps: 55, steps per second: 148, episode reward: 13.342, mean reward: 0.243 [0.035, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.838 [-0.906, 10.136], loss: 0.002543, mae: 0.050113, mean_q: -0.264186
 55406/100000: episode: 1251, duration: 0.259s, episode steps: 45, steps per second: 174, episode reward: 11.385, mean reward: 0.253 [0.087, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.389, 10.100], loss: 0.002869, mae: 0.056374, mean_q: -0.203851
 55419/100000: episode: 1252, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 3.336, mean reward: 0.257 [0.195, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.172, 10.100], loss: 0.012995, mae: 0.086075, mean_q: -0.202066
 55446/100000: episode: 1253, duration: 0.158s, episode steps: 27, steps per second: 171, episode reward: 10.582, mean reward: 0.392 [0.293, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.845, 10.100], loss: 0.010081, mae: 0.089181, mean_q: -0.211017
 55476/100000: episode: 1254, duration: 0.175s, episode steps: 30, steps per second: 172, episode reward: 10.429, mean reward: 0.348 [0.274, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.761, 10.100], loss: 0.003535, mae: 0.063587, mean_q: -0.184285
 55519/100000: episode: 1255, duration: 0.291s, episode steps: 43, steps per second: 148, episode reward: 13.712, mean reward: 0.319 [0.204, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.560, 10.100], loss: 0.003035, mae: 0.057230, mean_q: -0.203608
 55546/100000: episode: 1256, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 7.878, mean reward: 0.292 [0.176, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.243, 10.100], loss: 0.003032, mae: 0.056272, mean_q: -0.214052
 55601/100000: episode: 1257, duration: 0.339s, episode steps: 55, steps per second: 162, episode reward: 18.388, mean reward: 0.334 [0.135, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-0.791, 10.291], loss: 0.002890, mae: 0.055741, mean_q: -0.225356
 55614/100000: episode: 1258, duration: 0.090s, episode steps: 13, steps per second: 144, episode reward: 3.169, mean reward: 0.244 [0.186, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.507, 10.100], loss: 0.002301, mae: 0.048969, mean_q: -0.176062
 55644/100000: episode: 1259, duration: 0.167s, episode steps: 30, steps per second: 179, episode reward: 8.357, mean reward: 0.279 [0.151, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.094, 10.100], loss: 0.002555, mae: 0.052774, mean_q: -0.132760
 55671/100000: episode: 1260, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 11.179, mean reward: 0.414 [0.292, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.682, 10.100], loss: 0.003118, mae: 0.056467, mean_q: -0.150707
 55726/100000: episode: 1261, duration: 0.358s, episode steps: 55, steps per second: 153, episode reward: 24.645, mean reward: 0.448 [0.215, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-0.442, 10.281], loss: 0.002670, mae: 0.052828, mean_q: -0.152845
 55744/100000: episode: 1262, duration: 0.117s, episode steps: 18, steps per second: 154, episode reward: 6.411, mean reward: 0.356 [0.234, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.593, 10.100], loss: 0.003179, mae: 0.057517, mean_q: -0.186215
 55757/100000: episode: 1263, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 3.462, mean reward: 0.266 [0.223, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.318, 10.100], loss: 0.002453, mae: 0.050454, mean_q: -0.124197
 55770/100000: episode: 1264, duration: 0.084s, episode steps: 13, steps per second: 156, episode reward: 3.636, mean reward: 0.280 [0.198, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.473, 10.100], loss: 0.002781, mae: 0.054831, mean_q: -0.226257
 55800/100000: episode: 1265, duration: 0.190s, episode steps: 30, steps per second: 158, episode reward: 8.547, mean reward: 0.285 [0.121, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.097, 10.100], loss: 0.002655, mae: 0.051507, mean_q: -0.130641
 55855/100000: episode: 1266, duration: 0.281s, episode steps: 55, steps per second: 196, episode reward: 18.155, mean reward: 0.330 [0.122, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-1.305, 10.344], loss: 0.002513, mae: 0.051796, mean_q: -0.137927
 55900/100000: episode: 1267, duration: 0.254s, episode steps: 45, steps per second: 177, episode reward: 13.367, mean reward: 0.297 [0.089, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.757, 10.100], loss: 0.002568, mae: 0.051929, mean_q: -0.130035
 55933/100000: episode: 1268, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 8.544, mean reward: 0.259 [0.109, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.035, 10.258], loss: 0.002678, mae: 0.052493, mean_q: -0.127336
 55963/100000: episode: 1269, duration: 0.179s, episode steps: 30, steps per second: 167, episode reward: 7.205, mean reward: 0.240 [0.115, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.381, 10.100], loss: 0.002597, mae: 0.051240, mean_q: -0.216781
 56018/100000: episode: 1270, duration: 0.323s, episode steps: 55, steps per second: 170, episode reward: 14.575, mean reward: 0.265 [0.069, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-0.664, 10.100], loss: 0.002679, mae: 0.052782, mean_q: -0.129636
 56063/100000: episode: 1271, duration: 0.259s, episode steps: 45, steps per second: 173, episode reward: 12.755, mean reward: 0.283 [0.152, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-1.015, 10.100], loss: 0.002849, mae: 0.054114, mean_q: -0.100399
 56118/100000: episode: 1272, duration: 0.331s, episode steps: 55, steps per second: 166, episode reward: 12.909, mean reward: 0.235 [0.048, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-1.448, 10.173], loss: 0.002605, mae: 0.052414, mean_q: -0.108155
 56163/100000: episode: 1273, duration: 0.228s, episode steps: 45, steps per second: 197, episode reward: 13.244, mean reward: 0.294 [0.154, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.507, 10.100], loss: 0.002718, mae: 0.054186, mean_q: -0.082154
 56206/100000: episode: 1274, duration: 0.203s, episode steps: 43, steps per second: 212, episode reward: 9.117, mean reward: 0.212 [0.031, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.359, 10.119], loss: 0.002610, mae: 0.053171, mean_q: -0.066631
 56249/100000: episode: 1275, duration: 0.272s, episode steps: 43, steps per second: 158, episode reward: 10.825, mean reward: 0.252 [0.110, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.763, 10.100], loss: 0.002452, mae: 0.050292, mean_q: -0.079736
 56274/100000: episode: 1276, duration: 0.199s, episode steps: 25, steps per second: 126, episode reward: 8.101, mean reward: 0.324 [0.188, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.399, 10.100], loss: 0.002285, mae: 0.050429, mean_q: -0.028996
 56317/100000: episode: 1277, duration: 0.281s, episode steps: 43, steps per second: 153, episode reward: 9.339, mean reward: 0.217 [0.015, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.591, 10.100], loss: 0.002456, mae: 0.050832, mean_q: -0.103485
 56362/100000: episode: 1278, duration: 0.221s, episode steps: 45, steps per second: 204, episode reward: 10.938, mean reward: 0.243 [0.021, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.820, 10.156], loss: 0.002840, mae: 0.054933, mean_q: -0.024458
 56417/100000: episode: 1279, duration: 0.278s, episode steps: 55, steps per second: 198, episode reward: 18.602, mean reward: 0.338 [0.043, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-0.809, 10.109], loss: 0.002624, mae: 0.053713, mean_q: -0.062520
 56435/100000: episode: 1280, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 4.236, mean reward: 0.235 [0.150, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.726, 10.100], loss: 0.002750, mae: 0.053245, mean_q: -0.062213
 56478/100000: episode: 1281, duration: 0.290s, episode steps: 43, steps per second: 148, episode reward: 12.019, mean reward: 0.280 [0.150, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.508, 10.100], loss: 0.002395, mae: 0.049463, mean_q: -0.101499
 56533/100000: episode: 1282, duration: 0.298s, episode steps: 55, steps per second: 185, episode reward: 19.814, mean reward: 0.360 [0.170, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-1.856, 10.380], loss: 0.002733, mae: 0.053464, mean_q: -0.059095
 56566/100000: episode: 1283, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 10.146, mean reward: 0.307 [0.167, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.210, 10.242], loss: 0.002493, mae: 0.051405, mean_q: -0.073194
 56611/100000: episode: 1284, duration: 0.257s, episode steps: 45, steps per second: 175, episode reward: 16.257, mean reward: 0.361 [0.190, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-1.299, 10.100], loss: 0.002665, mae: 0.053611, mean_q: -0.018100
 56644/100000: episode: 1285, duration: 0.200s, episode steps: 33, steps per second: 165, episode reward: 10.055, mean reward: 0.305 [0.195, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-1.346, 10.358], loss: 0.002892, mae: 0.056600, mean_q: -0.045181
 56657/100000: episode: 1286, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 3.030, mean reward: 0.233 [0.020, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.281, 10.100], loss: 0.002959, mae: 0.055690, mean_q: -0.094425
 56687/100000: episode: 1287, duration: 0.169s, episode steps: 30, steps per second: 177, episode reward: 10.113, mean reward: 0.337 [0.170, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.439, 10.100], loss: 0.004972, mae: 0.070844, mean_q: 0.025052
 56712/100000: episode: 1288, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 7.316, mean reward: 0.293 [0.162, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.510, 10.100], loss: 0.003408, mae: 0.062792, mean_q: 0.003529
 56757/100000: episode: 1289, duration: 0.239s, episode steps: 45, steps per second: 188, episode reward: 14.011, mean reward: 0.311 [0.174, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.653, 10.100], loss: 0.002667, mae: 0.055340, mean_q: 0.029364
 56775/100000: episode: 1290, duration: 0.113s, episode steps: 18, steps per second: 159, episode reward: 6.544, mean reward: 0.364 [0.239, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.771, 10.100], loss: 0.003065, mae: 0.057759, mean_q: 0.070063
 56793/100000: episode: 1291, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 6.741, mean reward: 0.374 [0.255, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.267, 10.100], loss: 0.003161, mae: 0.058252, mean_q: 0.058231
 56811/100000: episode: 1292, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 5.809, mean reward: 0.323 [0.216, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.372, 10.100], loss: 0.002558, mae: 0.051331, mean_q: -0.011973
 56854/100000: episode: 1293, duration: 0.257s, episode steps: 43, steps per second: 167, episode reward: 10.172, mean reward: 0.237 [0.058, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.869, 10.100], loss: 0.002622, mae: 0.053744, mean_q: 0.006892
 56867/100000: episode: 1294, duration: 0.085s, episode steps: 13, steps per second: 153, episode reward: 3.714, mean reward: 0.286 [0.249, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.387, 10.100], loss: 0.003092, mae: 0.058350, mean_q: 0.065577
 56880/100000: episode: 1295, duration: 0.085s, episode steps: 13, steps per second: 153, episode reward: 4.137, mean reward: 0.318 [0.205, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.235, 10.100], loss: 0.002745, mae: 0.055030, mean_q: 0.075325
 56913/100000: episode: 1296, duration: 0.191s, episode steps: 33, steps per second: 173, episode reward: 11.308, mean reward: 0.343 [0.199, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-1.113, 10.346], loss: 0.002642, mae: 0.053601, mean_q: 0.025039
 56940/100000: episode: 1297, duration: 0.167s, episode steps: 27, steps per second: 162, episode reward: 8.919, mean reward: 0.330 [0.239, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.403, 10.100], loss: 0.002717, mae: 0.054282, mean_q: 0.072818
 56953/100000: episode: 1298, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 4.063, mean reward: 0.313 [0.224, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.377, 10.100], loss: 0.002694, mae: 0.052937, mean_q: -0.014654
 56966/100000: episode: 1299, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 5.082, mean reward: 0.391 [0.271, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.520, 10.100], loss: 0.002609, mae: 0.052072, mean_q: -0.013392
 56991/100000: episode: 1300, duration: 0.144s, episode steps: 25, steps per second: 174, episode reward: 6.537, mean reward: 0.261 [0.170, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.114, 10.100], loss: 0.002664, mae: 0.054877, mean_q: 0.105101
 57004/100000: episode: 1301, duration: 0.087s, episode steps: 13, steps per second: 149, episode reward: 4.057, mean reward: 0.312 [0.264, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.294, 10.100], loss: 0.002658, mae: 0.052798, mean_q: 0.049257
 57047/100000: episode: 1302, duration: 0.251s, episode steps: 43, steps per second: 172, episode reward: 7.487, mean reward: 0.174 [0.016, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.150, 10.100], loss: 0.002592, mae: 0.053963, mean_q: 0.070989
 57065/100000: episode: 1303, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 4.825, mean reward: 0.268 [0.187, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.214, 10.100], loss: 0.002804, mae: 0.054467, mean_q: 0.056937
 57092/100000: episode: 1304, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 9.801, mean reward: 0.363 [0.261, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.393, 10.100], loss: 0.002671, mae: 0.054037, mean_q: 0.077603
 57117/100000: episode: 1305, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 8.755, mean reward: 0.350 [0.240, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.308, 10.100], loss: 0.002973, mae: 0.057406, mean_q: 0.117495
 57144/100000: episode: 1306, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 10.684, mean reward: 0.396 [0.305, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.697, 10.100], loss: 0.002846, mae: 0.056152, mean_q: 0.087056
 57157/100000: episode: 1307, duration: 0.090s, episode steps: 13, steps per second: 144, episode reward: 3.770, mean reward: 0.290 [0.232, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.605, 10.100], loss: 0.002419, mae: 0.052890, mean_q: 0.095477
 57202/100000: episode: 1308, duration: 0.254s, episode steps: 45, steps per second: 177, episode reward: 12.111, mean reward: 0.269 [0.015, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.188, 10.100], loss: 0.002854, mae: 0.056438, mean_q: 0.109964
 57235/100000: episode: 1309, duration: 0.205s, episode steps: 33, steps per second: 161, episode reward: 9.517, mean reward: 0.288 [0.178, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.264, 10.276], loss: 0.003137, mae: 0.058535, mean_q: 0.107968
 57268/100000: episode: 1310, duration: 0.191s, episode steps: 33, steps per second: 173, episode reward: 15.866, mean reward: 0.481 [0.358, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.141, 10.431], loss: 0.002406, mae: 0.051062, mean_q: 0.111232
 57301/100000: episode: 1311, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 13.222, mean reward: 0.401 [0.336, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.062, 10.517], loss: 0.002701, mae: 0.053432, mean_q: 0.090190
 57356/100000: episode: 1312, duration: 0.308s, episode steps: 55, steps per second: 179, episode reward: 14.624, mean reward: 0.266 [0.051, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-0.620, 10.162], loss: 0.002629, mae: 0.054772, mean_q: 0.154402
 57381/100000: episode: 1313, duration: 0.146s, episode steps: 25, steps per second: 172, episode reward: 7.765, mean reward: 0.311 [0.245, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.391, 10.100], loss: 0.002983, mae: 0.058079, mean_q: 0.127734
 57436/100000: episode: 1314, duration: 0.327s, episode steps: 55, steps per second: 168, episode reward: 19.756, mean reward: 0.359 [0.120, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-0.488, 10.283], loss: 0.002946, mae: 0.058480, mean_q: 0.171514
 57466/100000: episode: 1315, duration: 0.175s, episode steps: 30, steps per second: 171, episode reward: 9.554, mean reward: 0.318 [0.123, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.760, 10.100], loss: 0.002818, mae: 0.056316, mean_q: 0.108677
 57509/100000: episode: 1316, duration: 0.237s, episode steps: 43, steps per second: 182, episode reward: 12.232, mean reward: 0.284 [0.150, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.901, 10.100], loss: 0.003009, mae: 0.058762, mean_q: 0.175971
 57554/100000: episode: 1317, duration: 0.230s, episode steps: 45, steps per second: 195, episode reward: 8.828, mean reward: 0.196 [0.029, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.827, 10.123], loss: 0.002644, mae: 0.053765, mean_q: 0.159594
 57609/100000: episode: 1318, duration: 0.290s, episode steps: 55, steps per second: 190, episode reward: 16.708, mean reward: 0.304 [0.133, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-0.639, 10.309], loss: 0.002664, mae: 0.054107, mean_q: 0.134133
 57642/100000: episode: 1319, duration: 0.173s, episode steps: 33, steps per second: 190, episode reward: 11.257, mean reward: 0.341 [0.162, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.833, 10.300], loss: 0.002925, mae: 0.057323, mean_q: 0.172695
 57685/100000: episode: 1320, duration: 0.220s, episode steps: 43, steps per second: 195, episode reward: 9.868, mean reward: 0.229 [0.027, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.161, 10.176], loss: 0.002761, mae: 0.056066, mean_q: 0.227153
 57740/100000: episode: 1321, duration: 0.292s, episode steps: 55, steps per second: 189, episode reward: 17.080, mean reward: 0.311 [0.079, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-0.547, 10.124], loss: 0.002519, mae: 0.052779, mean_q: 0.176517
 57795/100000: episode: 1322, duration: 0.276s, episode steps: 55, steps per second: 199, episode reward: 12.874, mean reward: 0.234 [0.041, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-1.072, 10.279], loss: 0.002768, mae: 0.055966, mean_q: 0.229683
 57813/100000: episode: 1323, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 6.705, mean reward: 0.373 [0.213, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.327, 10.100], loss: 0.002474, mae: 0.052486, mean_q: 0.197472
 57846/100000: episode: 1324, duration: 0.162s, episode steps: 33, steps per second: 204, episode reward: 9.359, mean reward: 0.284 [0.081, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-1.938, 10.196], loss: 0.002784, mae: 0.056002, mean_q: 0.225679
 57901/100000: episode: 1325, duration: 0.275s, episode steps: 55, steps per second: 200, episode reward: 20.298, mean reward: 0.369 [0.152, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.845 [-0.516, 10.209], loss: 0.002757, mae: 0.055605, mean_q: 0.201509
 57946/100000: episode: 1326, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: 10.984, mean reward: 0.244 [0.092, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.934, 10.100], loss: 0.003035, mae: 0.059043, mean_q: 0.206135
 57973/100000: episode: 1327, duration: 0.160s, episode steps: 27, steps per second: 169, episode reward: 6.647, mean reward: 0.246 [0.183, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.437, 10.100], loss: 0.002784, mae: 0.056264, mean_q: 0.235345
[Info] 200-TH LEVEL FOUND: 0.9527632594108582, Considering 10/90 traces
 58006/100000: episode: 1328, duration: 4.296s, episode steps: 33, steps per second: 8, episode reward: 8.710, mean reward: 0.264 [0.128, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.233, 10.272], loss: 0.002894, mae: 0.057775, mean_q: 0.260013
 58058/100000: episode: 1329, duration: 0.253s, episode steps: 52, steps per second: 206, episode reward: 13.202, mean reward: 0.254 [0.057, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.547, 10.291], loss: 0.002750, mae: 0.056997, mean_q: 0.280289
 58089/100000: episode: 1330, duration: 0.151s, episode steps: 31, steps per second: 205, episode reward: 10.027, mean reward: 0.323 [0.166, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.716, 10.296], loss: 0.002603, mae: 0.054398, mean_q: 0.207872
 58118/100000: episode: 1331, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 12.365, mean reward: 0.426 [0.342, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.207, 10.517], loss: 0.002794, mae: 0.056434, mean_q: 0.231806
 58163/100000: episode: 1332, duration: 0.348s, episode steps: 45, steps per second: 129, episode reward: 13.542, mean reward: 0.301 [0.110, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.294, 10.384], loss: 0.002878, mae: 0.058470, mean_q: 0.280007
 58214/100000: episode: 1333, duration: 0.266s, episode steps: 51, steps per second: 192, episode reward: 17.731, mean reward: 0.348 [0.041, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.458, 10.181], loss: 0.002874, mae: 0.057339, mean_q: 0.290749
 58266/100000: episode: 1334, duration: 0.345s, episode steps: 52, steps per second: 151, episode reward: 17.423, mean reward: 0.335 [0.180, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.665, 10.433], loss: 0.002879, mae: 0.057651, mean_q: 0.275903
 58307/100000: episode: 1335, duration: 0.274s, episode steps: 41, steps per second: 149, episode reward: 16.192, mean reward: 0.395 [0.220, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.205, 10.484], loss: 0.002854, mae: 0.057161, mean_q: 0.285861
 58358/100000: episode: 1336, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 19.307, mean reward: 0.379 [0.113, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.809, 10.451], loss: 0.002819, mae: 0.057167, mean_q: 0.309132
 58399/100000: episode: 1337, duration: 0.258s, episode steps: 41, steps per second: 159, episode reward: 14.338, mean reward: 0.350 [0.196, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.282, 10.284], loss: 0.002894, mae: 0.059186, mean_q: 0.337662
 58450/100000: episode: 1338, duration: 0.333s, episode steps: 51, steps per second: 153, episode reward: 11.431, mean reward: 0.224 [0.029, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-1.507, 10.248], loss: 0.002694, mae: 0.056183, mean_q: 0.320065
 58479/100000: episode: 1339, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 11.339, mean reward: 0.391 [0.250, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.643, 10.411], loss: 0.003529, mae: 0.064775, mean_q: 0.349267
 58510/100000: episode: 1340, duration: 0.189s, episode steps: 31, steps per second: 164, episode reward: 11.203, mean reward: 0.361 [0.221, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.390, 10.425], loss: 0.003469, mae: 0.063121, mean_q: 0.316038
 58559/100000: episode: 1341, duration: 0.273s, episode steps: 49, steps per second: 179, episode reward: 15.652, mean reward: 0.319 [0.221, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.854, 10.405], loss: 0.002808, mae: 0.056226, mean_q: 0.313699
 58610/100000: episode: 1342, duration: 0.284s, episode steps: 51, steps per second: 179, episode reward: 20.681, mean reward: 0.406 [0.268, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.505, 10.473], loss: 0.002824, mae: 0.056996, mean_q: 0.359068
 58659/100000: episode: 1343, duration: 0.280s, episode steps: 49, steps per second: 175, episode reward: 22.050, mean reward: 0.450 [0.285, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.412, 10.546], loss: 0.002584, mae: 0.055672, mean_q: 0.372613
 58708/100000: episode: 1344, duration: 0.292s, episode steps: 49, steps per second: 168, episode reward: 17.116, mean reward: 0.349 [0.247, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.502, 10.473], loss: 0.002915, mae: 0.058937, mean_q: 0.389012
 58738/100000: episode: 1345, duration: 0.182s, episode steps: 30, steps per second: 165, episode reward: 15.507, mean reward: 0.517 [0.443, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.239, 10.100], loss: 0.002768, mae: 0.057075, mean_q: 0.378347
 58789/100000: episode: 1346, duration: 0.299s, episode steps: 51, steps per second: 171, episode reward: 18.943, mean reward: 0.371 [0.229, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.442, 10.395], loss: 0.003038, mae: 0.059728, mean_q: 0.366784
 58819/100000: episode: 1347, duration: 0.174s, episode steps: 30, steps per second: 172, episode reward: 11.354, mean reward: 0.378 [0.269, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.297, 10.100], loss: 0.002904, mae: 0.058437, mean_q: 0.401105
 58870/100000: episode: 1348, duration: 0.289s, episode steps: 51, steps per second: 177, episode reward: 25.370, mean reward: 0.497 [0.289, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.587, 10.480], loss: 0.002844, mae: 0.058628, mean_q: 0.415952
 58922/100000: episode: 1349, duration: 0.290s, episode steps: 52, steps per second: 179, episode reward: 19.126, mean reward: 0.368 [0.219, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.704, 10.494], loss: 0.002861, mae: 0.057583, mean_q: 0.427339
 58967/100000: episode: 1350, duration: 0.294s, episode steps: 45, steps per second: 153, episode reward: 20.416, mean reward: 0.454 [0.285, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.373, 10.481], loss: 0.003094, mae: 0.060765, mean_q: 0.448139
 59016/100000: episode: 1351, duration: 0.432s, episode steps: 49, steps per second: 113, episode reward: 15.021, mean reward: 0.307 [0.050, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.512, 10.225], loss: 0.003119, mae: 0.060755, mean_q: 0.444689
 59057/100000: episode: 1352, duration: 0.618s, episode steps: 41, steps per second: 66, episode reward: 16.959, mean reward: 0.414 [0.319, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.212, 10.476], loss: 0.003222, mae: 0.061957, mean_q: 0.432925
 59087/100000: episode: 1353, duration: 0.266s, episode steps: 30, steps per second: 113, episode reward: 13.238, mean reward: 0.441 [0.295, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-1.087, 10.100], loss: 0.003006, mae: 0.059226, mean_q: 0.496224
 59136/100000: episode: 1354, duration: 0.344s, episode steps: 49, steps per second: 142, episode reward: 18.003, mean reward: 0.367 [0.150, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.936, 10.292], loss: 0.005289, mae: 0.065775, mean_q: 0.497445
 59188/100000: episode: 1355, duration: 0.306s, episode steps: 52, steps per second: 170, episode reward: 13.332, mean reward: 0.256 [0.008, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.473, 10.100], loss: 0.004122, mae: 0.064803, mean_q: 0.503306
 59219/100000: episode: 1356, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 8.907, mean reward: 0.287 [0.163, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.860, 10.399], loss: 0.006049, mae: 0.068387, mean_q: 0.485336
 59264/100000: episode: 1357, duration: 0.314s, episode steps: 45, steps per second: 143, episode reward: 14.550, mean reward: 0.323 [0.084, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.304, 10.181], loss: 0.004257, mae: 0.064908, mean_q: 0.521839
 59309/100000: episode: 1358, duration: 0.294s, episode steps: 45, steps per second: 153, episode reward: 12.076, mean reward: 0.268 [0.019, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.615, 10.323], loss: 0.004093, mae: 0.066419, mean_q: 0.528910
 59358/100000: episode: 1359, duration: 0.354s, episode steps: 49, steps per second: 139, episode reward: 14.543, mean reward: 0.297 [0.072, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.407, 10.100], loss: 0.003191, mae: 0.061412, mean_q: 0.527327
 59403/100000: episode: 1360, duration: 0.327s, episode steps: 45, steps per second: 137, episode reward: 12.057, mean reward: 0.268 [0.148, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.276, 10.293], loss: 0.003227, mae: 0.062088, mean_q: 0.522023
 59448/100000: episode: 1361, duration: 0.281s, episode steps: 45, steps per second: 160, episode reward: 15.690, mean reward: 0.349 [0.260, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.813, 10.376], loss: 0.002689, mae: 0.056433, mean_q: 0.543920
 59477/100000: episode: 1362, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 13.772, mean reward: 0.475 [0.223, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.307, 10.394], loss: 0.005205, mae: 0.069160, mean_q: 0.545301
 59506/100000: episode: 1363, duration: 0.195s, episode steps: 29, steps per second: 149, episode reward: 13.446, mean reward: 0.464 [0.369, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.664, 10.622], loss: 0.003567, mae: 0.063835, mean_q: 0.546665
 59551/100000: episode: 1364, duration: 0.284s, episode steps: 45, steps per second: 159, episode reward: 16.993, mean reward: 0.378 [0.230, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-2.311, 10.544], loss: 0.004771, mae: 0.063272, mean_q: 0.556998
 59592/100000: episode: 1365, duration: 0.242s, episode steps: 41, steps per second: 169, episode reward: 13.199, mean reward: 0.322 [0.073, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.340, 10.100], loss: 0.003261, mae: 0.062833, mean_q: 0.601064
 59622/100000: episode: 1366, duration: 0.172s, episode steps: 30, steps per second: 175, episode reward: 14.555, mean reward: 0.485 [0.385, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.909, 10.100], loss: 0.003355, mae: 0.061144, mean_q: 0.570654
 59663/100000: episode: 1367, duration: 0.199s, episode steps: 41, steps per second: 206, episode reward: 11.988, mean reward: 0.292 [0.020, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.212, 10.101], loss: 0.003251, mae: 0.063282, mean_q: 0.590461
 59708/100000: episode: 1368, duration: 0.235s, episode steps: 45, steps per second: 192, episode reward: 14.361, mean reward: 0.319 [0.062, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-1.085, 10.228], loss: 0.002971, mae: 0.060984, mean_q: 0.608570
 59753/100000: episode: 1369, duration: 0.211s, episode steps: 45, steps per second: 213, episode reward: 13.481, mean reward: 0.300 [0.159, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.833, 10.354], loss: 0.002938, mae: 0.059855, mean_q: 0.604507
 59783/100000: episode: 1370, duration: 0.147s, episode steps: 30, steps per second: 204, episode reward: 12.290, mean reward: 0.410 [0.239, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.404, 10.100], loss: 0.003046, mae: 0.061219, mean_q: 0.627439
 59824/100000: episode: 1371, duration: 0.208s, episode steps: 41, steps per second: 197, episode reward: 16.000, mean reward: 0.390 [0.235, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-1.023, 10.341], loss: 0.002868, mae: 0.059291, mean_q: 0.616668
 59873/100000: episode: 1372, duration: 0.248s, episode steps: 49, steps per second: 198, episode reward: 19.310, mean reward: 0.394 [0.140, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.620, 10.324], loss: 0.003081, mae: 0.061379, mean_q: 0.624588
 59924/100000: episode: 1373, duration: 0.269s, episode steps: 51, steps per second: 190, episode reward: 23.978, mean reward: 0.470 [0.183, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.586, 10.400], loss: 0.003086, mae: 0.062385, mean_q: 0.624406
 59969/100000: episode: 1374, duration: 0.242s, episode steps: 45, steps per second: 186, episode reward: 15.414, mean reward: 0.343 [0.047, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.891, 10.167], loss: 0.003417, mae: 0.064314, mean_q: 0.623278
 60014/100000: episode: 1375, duration: 0.244s, episode steps: 45, steps per second: 184, episode reward: 14.856, mean reward: 0.330 [0.174, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.657, 10.327], loss: 0.002984, mae: 0.060533, mean_q: 0.632578
 60055/100000: episode: 1376, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 16.620, mean reward: 0.405 [0.189, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.394, 10.362], loss: 0.002888, mae: 0.059532, mean_q: 0.631738
 60100/100000: episode: 1377, duration: 0.231s, episode steps: 45, steps per second: 194, episode reward: 14.479, mean reward: 0.322 [0.152, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.491, 10.634], loss: 0.003293, mae: 0.064273, mean_q: 0.625696
 60129/100000: episode: 1378, duration: 0.141s, episode steps: 29, steps per second: 205, episode reward: 10.319, mean reward: 0.356 [0.193, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.238, 10.408], loss: 0.003505, mae: 0.065913, mean_q: 0.629703
 60160/100000: episode: 1379, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 11.154, mean reward: 0.360 [0.181, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.035, 10.297], loss: 0.002980, mae: 0.060334, mean_q: 0.631687
 60212/100000: episode: 1380, duration: 0.275s, episode steps: 52, steps per second: 189, episode reward: 14.552, mean reward: 0.280 [0.080, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.513, 10.245], loss: 0.002782, mae: 0.059185, mean_q: 0.630954
 60257/100000: episode: 1381, duration: 0.221s, episode steps: 45, steps per second: 204, episode reward: 19.540, mean reward: 0.434 [0.327, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.412, 10.415], loss: 0.002898, mae: 0.059946, mean_q: 0.623209
 60298/100000: episode: 1382, duration: 0.226s, episode steps: 41, steps per second: 181, episode reward: 11.057, mean reward: 0.270 [0.005, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.213, 10.100], loss: 0.002801, mae: 0.058816, mean_q: 0.628872
 60350/100000: episode: 1383, duration: 0.258s, episode steps: 52, steps per second: 202, episode reward: 19.691, mean reward: 0.379 [0.245, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.918, 10.489], loss: 0.002889, mae: 0.059786, mean_q: 0.636170
 60401/100000: episode: 1384, duration: 0.252s, episode steps: 51, steps per second: 202, episode reward: 19.181, mean reward: 0.376 [0.282, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.498, 10.617], loss: 0.002957, mae: 0.060496, mean_q: 0.640240
 60452/100000: episode: 1385, duration: 0.276s, episode steps: 51, steps per second: 185, episode reward: 18.137, mean reward: 0.356 [0.213, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.523, 10.350], loss: 0.003156, mae: 0.062439, mean_q: 0.643031
 60501/100000: episode: 1386, duration: 0.261s, episode steps: 49, steps per second: 188, episode reward: 19.473, mean reward: 0.397 [0.231, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-1.658, 10.347], loss: 0.002778, mae: 0.058411, mean_q: 0.639332
 60553/100000: episode: 1387, duration: 0.260s, episode steps: 52, steps per second: 200, episode reward: 12.212, mean reward: 0.235 [0.012, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.996, 10.118], loss: 0.002959, mae: 0.060237, mean_q: 0.631681
 60583/100000: episode: 1388, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 13.145, mean reward: 0.438 [0.321, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.229, 10.100], loss: 0.003015, mae: 0.060938, mean_q: 0.643757
 60628/100000: episode: 1389, duration: 0.237s, episode steps: 45, steps per second: 190, episode reward: 18.893, mean reward: 0.420 [0.260, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.313, 10.528], loss: 0.002990, mae: 0.060610, mean_q: 0.633156
 60669/100000: episode: 1390, duration: 0.201s, episode steps: 41, steps per second: 204, episode reward: 11.627, mean reward: 0.284 [0.045, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.219, 10.189], loss: 0.002804, mae: 0.058230, mean_q: 0.642241
 60698/100000: episode: 1391, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 13.234, mean reward: 0.456 [0.349, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.183, 10.442], loss: 0.003080, mae: 0.061410, mean_q: 0.636288
 60747/100000: episode: 1392, duration: 0.239s, episode steps: 49, steps per second: 205, episode reward: 15.676, mean reward: 0.320 [0.111, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.433, 10.318], loss: 0.002822, mae: 0.058716, mean_q: 0.639526
 60792/100000: episode: 1393, duration: 0.228s, episode steps: 45, steps per second: 197, episode reward: 14.359, mean reward: 0.319 [0.227, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.407, 10.388], loss: 0.003009, mae: 0.061548, mean_q: 0.640581
 60822/100000: episode: 1394, duration: 0.159s, episode steps: 30, steps per second: 188, episode reward: 13.535, mean reward: 0.451 [0.208, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.346, 10.100], loss: 0.002622, mae: 0.057219, mean_q: 0.649064
 60863/100000: episode: 1395, duration: 0.213s, episode steps: 41, steps per second: 193, episode reward: 15.190, mean reward: 0.370 [0.229, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.231, 10.442], loss: 0.002986, mae: 0.060855, mean_q: 0.636146
 60914/100000: episode: 1396, duration: 0.253s, episode steps: 51, steps per second: 202, episode reward: 22.523, mean reward: 0.442 [0.255, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.505, 10.453], loss: 0.003154, mae: 0.061249, mean_q: 0.640280
 60965/100000: episode: 1397, duration: 0.267s, episode steps: 51, steps per second: 191, episode reward: 13.784, mean reward: 0.270 [0.022, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.702, 10.162], loss: 0.002778, mae: 0.058361, mean_q: 0.652742
 61006/100000: episode: 1398, duration: 0.214s, episode steps: 41, steps per second: 192, episode reward: 14.141, mean reward: 0.345 [0.120, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.767, 10.238], loss: 0.002621, mae: 0.057487, mean_q: 0.650910
 61051/100000: episode: 1399, duration: 0.213s, episode steps: 45, steps per second: 211, episode reward: 16.279, mean reward: 0.362 [0.153, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.288, 10.383], loss: 0.002780, mae: 0.058002, mean_q: 0.643278
 61082/100000: episode: 1400, duration: 0.181s, episode steps: 31, steps per second: 172, episode reward: 16.846, mean reward: 0.543 [0.429, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.127, 10.598], loss: 0.002682, mae: 0.057407, mean_q: 0.645679
 61131/100000: episode: 1401, duration: 0.240s, episode steps: 49, steps per second: 205, episode reward: 11.812, mean reward: 0.241 [0.060, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.597, 10.227], loss: 0.002889, mae: 0.059859, mean_q: 0.649022
 61172/100000: episode: 1402, duration: 0.192s, episode steps: 41, steps per second: 213, episode reward: 19.742, mean reward: 0.482 [0.325, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.228, 10.484], loss: 0.002881, mae: 0.059282, mean_q: 0.657645
 61217/100000: episode: 1403, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: 11.920, mean reward: 0.265 [0.143, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.754, 10.294], loss: 0.002903, mae: 0.060496, mean_q: 0.658824
 61248/100000: episode: 1404, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 12.754, mean reward: 0.411 [0.178, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.447, 10.449], loss: 0.002751, mae: 0.058419, mean_q: 0.663636
 61293/100000: episode: 1405, duration: 0.223s, episode steps: 45, steps per second: 202, episode reward: 14.699, mean reward: 0.327 [0.090, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.386, 10.194], loss: 0.002962, mae: 0.060666, mean_q: 0.667685
 61324/100000: episode: 1406, duration: 0.153s, episode steps: 31, steps per second: 202, episode reward: 12.315, mean reward: 0.397 [0.275, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.708, 10.407], loss: 0.003196, mae: 0.062295, mean_q: 0.659467
 61373/100000: episode: 1407, duration: 0.256s, episode steps: 49, steps per second: 191, episode reward: 22.892, mean reward: 0.467 [0.300, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-1.531, 10.333], loss: 0.002901, mae: 0.059546, mean_q: 0.652067
 61402/100000: episode: 1408, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 14.214, mean reward: 0.490 [0.407, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.096, 10.587], loss: 0.003182, mae: 0.062708, mean_q: 0.675754
 61447/100000: episode: 1409, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: 15.385, mean reward: 0.342 [0.180, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.500, 10.339], loss: 0.002807, mae: 0.059054, mean_q: 0.668458
 61476/100000: episode: 1410, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 9.272, mean reward: 0.320 [0.025, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.082, 10.159], loss: 0.003086, mae: 0.060748, mean_q: 0.653540
 61507/100000: episode: 1411, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 10.496, mean reward: 0.339 [0.206, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.740, 10.393], loss: 0.003023, mae: 0.060528, mean_q: 0.682374
 61558/100000: episode: 1412, duration: 0.260s, episode steps: 51, steps per second: 196, episode reward: 20.122, mean reward: 0.395 [0.053, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.504, 10.364], loss: 0.002802, mae: 0.058834, mean_q: 0.662779
 61599/100000: episode: 1413, duration: 0.222s, episode steps: 41, steps per second: 184, episode reward: 18.134, mean reward: 0.442 [0.172, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.650, 10.403], loss: 0.003066, mae: 0.062101, mean_q: 0.663744
 61644/100000: episode: 1414, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 11.626, mean reward: 0.258 [0.052, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.372, 10.197], loss: 0.002866, mae: 0.059206, mean_q: 0.670968
 61696/100000: episode: 1415, duration: 0.257s, episode steps: 52, steps per second: 202, episode reward: 19.979, mean reward: 0.384 [0.199, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-1.365, 10.425], loss: 0.003273, mae: 0.063669, mean_q: 0.680761
 61745/100000: episode: 1416, duration: 0.241s, episode steps: 49, steps per second: 204, episode reward: 17.664, mean reward: 0.360 [0.185, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.425, 10.376], loss: 0.003079, mae: 0.061849, mean_q: 0.664536
 61796/100000: episode: 1417, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 22.890, mean reward: 0.449 [0.264, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-1.975, 10.411], loss: 0.002849, mae: 0.059572, mean_q: 0.671136
[Info] 300-TH LEVEL FOUND: 1.0805827379226685, Considering 10/90 traces
 61825/100000: episode: 1418, duration: 4.057s, episode steps: 29, steps per second: 7, episode reward: 14.093, mean reward: 0.486 [0.265, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.623, 10.341], loss: 0.002678, mae: 0.058566, mean_q: 0.686822
 61868/100000: episode: 1419, duration: 0.220s, episode steps: 43, steps per second: 195, episode reward: 16.831, mean reward: 0.391 [0.240, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.835, 10.395], loss: 0.003012, mae: 0.061352, mean_q: 0.654515
 61904/100000: episode: 1420, duration: 0.169s, episode steps: 36, steps per second: 213, episode reward: 14.321, mean reward: 0.398 [0.247, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.268, 10.469], loss: 0.002572, mae: 0.056522, mean_q: 0.674726
 61927/100000: episode: 1421, duration: 0.115s, episode steps: 23, steps per second: 201, episode reward: 11.211, mean reward: 0.487 [0.406, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.591], loss: 0.002787, mae: 0.058111, mean_q: 0.675553
 61965/100000: episode: 1422, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 15.554, mean reward: 0.409 [0.154, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-1.576, 10.327], loss: 0.003153, mae: 0.062507, mean_q: 0.697410
 62009/100000: episode: 1423, duration: 0.227s, episode steps: 44, steps per second: 194, episode reward: 18.405, mean reward: 0.418 [0.338, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.711, 10.486], loss: 0.002871, mae: 0.059620, mean_q: 0.680679
 62047/100000: episode: 1424, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 16.269, mean reward: 0.428 [0.143, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.959, 10.364], loss: 0.003041, mae: 0.060842, mean_q: 0.678886
 62091/100000: episode: 1425, duration: 0.234s, episode steps: 44, steps per second: 188, episode reward: 16.143, mean reward: 0.367 [0.192, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.498, 10.376], loss: 0.002610, mae: 0.056636, mean_q: 0.688121
 62114/100000: episode: 1426, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 9.332, mean reward: 0.406 [0.226, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.256, 10.340], loss: 0.003083, mae: 0.062870, mean_q: 0.696741
 62157/100000: episode: 1427, duration: 0.206s, episode steps: 43, steps per second: 208, episode reward: 20.999, mean reward: 0.488 [0.380, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.393, 10.489], loss: 0.002762, mae: 0.057969, mean_q: 0.691754
 62201/100000: episode: 1428, duration: 0.226s, episode steps: 44, steps per second: 195, episode reward: 16.441, mean reward: 0.374 [0.095, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.346, 10.493], loss: 0.002681, mae: 0.057658, mean_q: 0.681683
 62238/100000: episode: 1429, duration: 0.183s, episode steps: 37, steps per second: 203, episode reward: 11.343, mean reward: 0.307 [0.094, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.083, 10.232], loss: 0.002838, mae: 0.058914, mean_q: 0.696684
 62276/100000: episode: 1430, duration: 0.181s, episode steps: 38, steps per second: 210, episode reward: 14.597, mean reward: 0.384 [0.150, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.842, 10.214], loss: 0.002607, mae: 0.056858, mean_q: 0.690891
 62313/100000: episode: 1431, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 16.352, mean reward: 0.442 [0.339, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.427, 10.481], loss: 0.002782, mae: 0.057639, mean_q: 0.689020
 62349/100000: episode: 1432, duration: 0.174s, episode steps: 36, steps per second: 207, episode reward: 13.060, mean reward: 0.363 [0.192, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.868, 10.470], loss: 0.002653, mae: 0.057665, mean_q: 0.699309
 62385/100000: episode: 1433, duration: 0.190s, episode steps: 36, steps per second: 190, episode reward: 14.749, mean reward: 0.410 [0.220, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.148, 10.331], loss: 0.002820, mae: 0.058924, mean_q: 0.703076
 62429/100000: episode: 1434, duration: 0.235s, episode steps: 44, steps per second: 187, episode reward: 23.503, mean reward: 0.534 [0.345, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-1.206, 10.442], loss: 0.002763, mae: 0.058674, mean_q: 0.704513
 62472/100000: episode: 1435, duration: 0.210s, episode steps: 43, steps per second: 205, episode reward: 16.675, mean reward: 0.388 [0.150, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.985, 10.311], loss: 0.003209, mae: 0.063002, mean_q: 0.694235
 62515/100000: episode: 1436, duration: 0.210s, episode steps: 43, steps per second: 205, episode reward: 19.824, mean reward: 0.461 [0.342, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.421, 10.525], loss: 0.002778, mae: 0.058941, mean_q: 0.696323
 62553/100000: episode: 1437, duration: 0.179s, episode steps: 38, steps per second: 212, episode reward: 18.610, mean reward: 0.490 [0.336, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-1.146, 10.421], loss: 0.002660, mae: 0.058125, mean_q: 0.708968
 62577/100000: episode: 1438, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 12.190, mean reward: 0.508 [0.290, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.379, 10.376], loss: 0.002517, mae: 0.056616, mean_q: 0.702316
 62601/100000: episode: 1439, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 12.682, mean reward: 0.528 [0.450, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.599, 10.588], loss: 0.003051, mae: 0.060753, mean_q: 0.710888
 62638/100000: episode: 1440, duration: 0.190s, episode steps: 37, steps per second: 195, episode reward: 9.802, mean reward: 0.265 [0.082, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.096, 10.288], loss: 0.002849, mae: 0.059584, mean_q: 0.719988
[Info] FALSIFICATION!
 62642/100000: episode: 1441, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 11.960, mean reward: 2.990 [0.630, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.728 [-0.103, 8.070], loss: 0.002910, mae: 0.056705, mean_q: 0.712251
 62742/100000: episode: 1442, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -12.095, mean reward: -0.121 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.484, 10.385], loss: 0.003236, mae: 0.061482, mean_q: 0.700005
 62842/100000: episode: 1443, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -12.900, mean reward: -0.129 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.336, 10.098], loss: 0.003221, mae: 0.060331, mean_q: 0.686712
 62942/100000: episode: 1444, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -17.870, mean reward: -0.179 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.076, 10.098], loss: 0.003131, mae: 0.060772, mean_q: 0.673365
 63042/100000: episode: 1445, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -19.359, mean reward: -0.194 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.333, 10.266], loss: 0.002980, mae: 0.058293, mean_q: 0.651142
 63142/100000: episode: 1446, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -14.032, mean reward: -0.140 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.498, 10.368], loss: 0.003546, mae: 0.061034, mean_q: 0.638606
 63242/100000: episode: 1447, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.116, mean reward: -0.141 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.298, 10.437], loss: 0.003440, mae: 0.061466, mean_q: 0.611083
 63342/100000: episode: 1448, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -20.522, mean reward: -0.205 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.219, 10.182], loss: 0.003101, mae: 0.059440, mean_q: 0.593629
 63442/100000: episode: 1449, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.850, mean reward: -0.159 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.883, 10.098], loss: 0.016392, mae: 0.070437, mean_q: 0.583742
 63542/100000: episode: 1450, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.603, mean reward: -0.186 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.281, 10.183], loss: 0.003101, mae: 0.059940, mean_q: 0.553613
 63642/100000: episode: 1451, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -14.547, mean reward: -0.145 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.948, 10.098], loss: 0.016093, mae: 0.068368, mean_q: 0.555394
 63742/100000: episode: 1452, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -20.272, mean reward: -0.203 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.006, 10.098], loss: 0.042207, mae: 0.089897, mean_q: 0.519697
 63842/100000: episode: 1453, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -20.164, mean reward: -0.202 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.194, 10.184], loss: 0.015742, mae: 0.067323, mean_q: 0.480384
 63942/100000: episode: 1454, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -15.968, mean reward: -0.160 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.731, 10.404], loss: 0.015829, mae: 0.065728, mean_q: 0.484961
 64042/100000: episode: 1455, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -13.956, mean reward: -0.140 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.138, 10.098], loss: 0.002675, mae: 0.056118, mean_q: 0.418924
 64142/100000: episode: 1456, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.214, mean reward: -0.182 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.390, 10.129], loss: 0.015215, mae: 0.062050, mean_q: 0.419182
 64242/100000: episode: 1457, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.473, mean reward: -0.195 [-1.000, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.857, 10.237], loss: 0.002903, mae: 0.057652, mean_q: 0.405847
 64342/100000: episode: 1458, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.193, mean reward: -0.152 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.741, 10.098], loss: 0.002660, mae: 0.055172, mean_q: 0.369981
 64442/100000: episode: 1459, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.460, mean reward: -0.165 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.268, 10.298], loss: 0.002838, mae: 0.057029, mean_q: 0.363916
 64542/100000: episode: 1460, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -14.139, mean reward: -0.141 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.822, 10.370], loss: 0.015952, mae: 0.066995, mean_q: 0.362023
 64642/100000: episode: 1461, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -12.613, mean reward: -0.126 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.776, 10.397], loss: 0.015912, mae: 0.067078, mean_q: 0.325905
 64742/100000: episode: 1462, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.831, mean reward: -0.178 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.035, 10.098], loss: 0.003067, mae: 0.058644, mean_q: 0.285224
 64842/100000: episode: 1463, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -19.991, mean reward: -0.200 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.514, 10.098], loss: 0.002780, mae: 0.055307, mean_q: 0.284081
 64942/100000: episode: 1464, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.467, mean reward: -0.195 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.945, 10.243], loss: 0.002554, mae: 0.053941, mean_q: 0.265144
 65042/100000: episode: 1465, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.042, mean reward: -0.180 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.849, 10.098], loss: 0.015771, mae: 0.066190, mean_q: 0.239729
 65142/100000: episode: 1466, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.538, mean reward: -0.155 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.222, 10.245], loss: 0.002852, mae: 0.056131, mean_q: 0.226704
 65242/100000: episode: 1467, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.258, mean reward: -0.163 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.002, 10.098], loss: 0.016000, mae: 0.067229, mean_q: 0.209130
 65342/100000: episode: 1468, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.303, mean reward: -0.183 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.233, 10.255], loss: 0.002470, mae: 0.053136, mean_q: 0.152188
 65442/100000: episode: 1469, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.123, mean reward: -0.161 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.069, 10.098], loss: 0.002548, mae: 0.053514, mean_q: 0.145247
 65542/100000: episode: 1470, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -10.482, mean reward: -0.105 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.844, 10.098], loss: 0.015254, mae: 0.060405, mean_q: 0.132026
 65642/100000: episode: 1471, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -12.281, mean reward: -0.123 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.640, 10.277], loss: 0.002757, mae: 0.055121, mean_q: 0.119828
 65742/100000: episode: 1472, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.307, mean reward: -0.163 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.615, 10.098], loss: 0.002812, mae: 0.054835, mean_q: 0.066730
 65842/100000: episode: 1473, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -18.074, mean reward: -0.181 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.196, 10.223], loss: 0.002424, mae: 0.050727, mean_q: 0.060792
 65942/100000: episode: 1474, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.757, mean reward: -0.188 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.774, 10.130], loss: 0.028535, mae: 0.070470, mean_q: 0.049958
 66042/100000: episode: 1475, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.102, mean reward: -0.161 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.897, 10.315], loss: 0.002522, mae: 0.052119, mean_q: 0.015103
 66142/100000: episode: 1476, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -14.935, mean reward: -0.149 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.445, 10.098], loss: 0.002474, mae: 0.051055, mean_q: 0.023704
 66242/100000: episode: 1477, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -13.773, mean reward: -0.138 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.508, 10.098], loss: 0.015053, mae: 0.058038, mean_q: 0.001613
 66342/100000: episode: 1478, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.998, mean reward: -0.130 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.725, 10.115], loss: 0.003019, mae: 0.056762, mean_q: -0.043079
 66442/100000: episode: 1479, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.561, mean reward: -0.156 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.731, 10.130], loss: 0.027802, mae: 0.069293, mean_q: -0.082971
 66542/100000: episode: 1480, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.742, mean reward: -0.187 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.091, 10.213], loss: 0.003008, mae: 0.058117, mean_q: -0.041233
 66642/100000: episode: 1481, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.261, mean reward: -0.173 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.523, 10.098], loss: 0.002351, mae: 0.050193, mean_q: -0.123825
 66742/100000: episode: 1482, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -16.434, mean reward: -0.164 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.132, 10.098], loss: 0.029136, mae: 0.075810, mean_q: -0.098918
 66842/100000: episode: 1483, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.513, mean reward: -0.165 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.911, 10.312], loss: 0.014843, mae: 0.056832, mean_q: -0.148974
 66942/100000: episode: 1484, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -19.820, mean reward: -0.198 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.984, 10.144], loss: 0.014968, mae: 0.057617, mean_q: -0.154323
 67042/100000: episode: 1485, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -17.872, mean reward: -0.179 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.733, 10.098], loss: 0.002538, mae: 0.050354, mean_q: -0.197591
 67142/100000: episode: 1486, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.439, mean reward: -0.184 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.505, 10.228], loss: 0.002370, mae: 0.048879, mean_q: -0.210590
 67242/100000: episode: 1487, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -15.830, mean reward: -0.158 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.324, 10.422], loss: 0.002473, mae: 0.050273, mean_q: -0.180352
 67342/100000: episode: 1488, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -17.770, mean reward: -0.178 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.867, 10.173], loss: 0.015178, mae: 0.058872, mean_q: -0.245155
 67442/100000: episode: 1489, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.923, mean reward: -0.179 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.043, 10.098], loss: 0.014932, mae: 0.059344, mean_q: -0.248389
 67542/100000: episode: 1490, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.667, mean reward: -0.197 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.564, 10.098], loss: 0.002471, mae: 0.050347, mean_q: -0.299640
 67642/100000: episode: 1491, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.659, mean reward: -0.187 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.186, 10.098], loss: 0.004241, mae: 0.061300, mean_q: -0.296223
 67742/100000: episode: 1492, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.664, mean reward: -0.167 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.871, 10.099], loss: 0.002466, mae: 0.049955, mean_q: -0.302220
 67842/100000: episode: 1493, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -14.756, mean reward: -0.148 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.677, 10.226], loss: 0.002456, mae: 0.049673, mean_q: -0.296949
 67942/100000: episode: 1494, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.982, mean reward: -0.170 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.724, 10.098], loss: 0.002446, mae: 0.049420, mean_q: -0.299937
 68042/100000: episode: 1495, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -18.056, mean reward: -0.181 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.519, 10.279], loss: 0.003036, mae: 0.053942, mean_q: -0.295728
 68142/100000: episode: 1496, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -10.857, mean reward: -0.109 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.624, 10.098], loss: 0.002666, mae: 0.052766, mean_q: -0.344431
 68242/100000: episode: 1497, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.594, mean reward: -0.176 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.760, 10.428], loss: 0.002186, mae: 0.046541, mean_q: -0.318528
 68342/100000: episode: 1498, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -15.582, mean reward: -0.156 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.965, 10.098], loss: 0.002320, mae: 0.047167, mean_q: -0.326430
 68442/100000: episode: 1499, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -15.511, mean reward: -0.155 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.719, 10.316], loss: 0.002428, mae: 0.049681, mean_q: -0.278503
 68542/100000: episode: 1500, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.585, mean reward: -0.196 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.153, 10.098], loss: 0.002196, mae: 0.046227, mean_q: -0.309673
 68642/100000: episode: 1501, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.584, mean reward: -0.176 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.119, 10.098], loss: 0.002253, mae: 0.047575, mean_q: -0.279402
 68742/100000: episode: 1502, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.187, mean reward: -0.152 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.797, 10.254], loss: 0.002607, mae: 0.051765, mean_q: -0.276265
 68842/100000: episode: 1503, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -14.233, mean reward: -0.142 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.046, 10.422], loss: 0.002181, mae: 0.046827, mean_q: -0.303418
 68942/100000: episode: 1504, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.017, mean reward: -0.180 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.203, 10.098], loss: 0.002239, mae: 0.046655, mean_q: -0.340597
 69042/100000: episode: 1505, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -16.480, mean reward: -0.165 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.964, 10.134], loss: 0.002307, mae: 0.048420, mean_q: -0.309314
 69142/100000: episode: 1506, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.532, mean reward: -0.175 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.499, 10.135], loss: 0.002628, mae: 0.051016, mean_q: -0.313561
 69242/100000: episode: 1507, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -18.063, mean reward: -0.181 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.150, 10.143], loss: 0.002423, mae: 0.050908, mean_q: -0.308839
 69342/100000: episode: 1508, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.446, mean reward: -0.184 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.786, 10.098], loss: 0.002355, mae: 0.049055, mean_q: -0.346256
 69442/100000: episode: 1509, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.455, mean reward: -0.165 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.975, 10.375], loss: 0.002317, mae: 0.048062, mean_q: -0.310423
 69542/100000: episode: 1510, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -12.292, mean reward: -0.123 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.767, 10.098], loss: 0.002368, mae: 0.048155, mean_q: -0.300549
 69642/100000: episode: 1511, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -16.822, mean reward: -0.168 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.734, 10.213], loss: 0.002580, mae: 0.051497, mean_q: -0.283384
 69742/100000: episode: 1512, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.721, mean reward: -0.177 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.088, 10.098], loss: 0.002399, mae: 0.049428, mean_q: -0.290916
 69842/100000: episode: 1513, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.394, mean reward: -0.164 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.473, 10.098], loss: 0.002441, mae: 0.048831, mean_q: -0.297618
 69942/100000: episode: 1514, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.558, mean reward: -0.136 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.905, 10.118], loss: 0.002337, mae: 0.048121, mean_q: -0.293244
 70042/100000: episode: 1515, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -17.881, mean reward: -0.179 [-1.000, 0.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.771, 10.098], loss: 0.002231, mae: 0.047700, mean_q: -0.315958
 70142/100000: episode: 1516, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.435, mean reward: -0.184 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.678, 10.098], loss: 0.002415, mae: 0.049173, mean_q: -0.291836
 70242/100000: episode: 1517, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.220, mean reward: -0.142 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.976, 10.098], loss: 0.002268, mae: 0.047091, mean_q: -0.310185
 70342/100000: episode: 1518, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -18.091, mean reward: -0.181 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.367, 10.170], loss: 0.002345, mae: 0.047503, mean_q: -0.323825
 70442/100000: episode: 1519, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -19.894, mean reward: -0.199 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.067, 10.098], loss: 0.002395, mae: 0.049232, mean_q: -0.306881
 70542/100000: episode: 1520, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -15.929, mean reward: -0.159 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.462, 10.098], loss: 0.002260, mae: 0.047080, mean_q: -0.325815
 70642/100000: episode: 1521, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -14.631, mean reward: -0.146 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.359, 10.098], loss: 0.002407, mae: 0.049272, mean_q: -0.328383
 70742/100000: episode: 1522, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.465, mean reward: -0.185 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.536, 10.182], loss: 0.002290, mae: 0.047817, mean_q: -0.297866
 70842/100000: episode: 1523, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -19.455, mean reward: -0.195 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.512, 10.098], loss: 0.002298, mae: 0.048645, mean_q: -0.309719
 70942/100000: episode: 1524, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.329, mean reward: -0.183 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.177, 10.098], loss: 0.002342, mae: 0.048519, mean_q: -0.280778
 71042/100000: episode: 1525, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -20.344, mean reward: -0.203 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.689, 10.150], loss: 0.002441, mae: 0.049863, mean_q: -0.300243
 71142/100000: episode: 1526, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -14.147, mean reward: -0.141 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.518, 10.098], loss: 0.003460, mae: 0.056670, mean_q: -0.333304
 71242/100000: episode: 1527, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.107, mean reward: -0.181 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.240, 10.154], loss: 0.003422, mae: 0.058703, mean_q: -0.293739
 71342/100000: episode: 1528, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -15.726, mean reward: -0.157 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.468, 10.259], loss: 0.002404, mae: 0.048864, mean_q: -0.336297
 71442/100000: episode: 1529, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.914, mean reward: -0.179 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.809, 10.098], loss: 0.002205, mae: 0.046780, mean_q: -0.289108
 71542/100000: episode: 1530, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.333, mean reward: -0.163 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.258, 10.176], loss: 0.002438, mae: 0.048955, mean_q: -0.318708
 71642/100000: episode: 1531, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.407, mean reward: -0.184 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.234, 10.415], loss: 0.002320, mae: 0.046716, mean_q: -0.363098
 71742/100000: episode: 1532, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.806, mean reward: -0.188 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.301, 10.145], loss: 0.002427, mae: 0.049277, mean_q: -0.317686
 71842/100000: episode: 1533, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.487, mean reward: -0.165 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.451, 10.098], loss: 0.002283, mae: 0.047015, mean_q: -0.308752
 71942/100000: episode: 1534, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -17.732, mean reward: -0.177 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.975, 10.178], loss: 0.002328, mae: 0.047591, mean_q: -0.318427
 72042/100000: episode: 1535, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.287, mean reward: -0.143 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.632, 10.098], loss: 0.002602, mae: 0.049908, mean_q: -0.329660
 72142/100000: episode: 1536, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -13.431, mean reward: -0.134 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.552, 10.098], loss: 0.002539, mae: 0.049333, mean_q: -0.314524
 72242/100000: episode: 1537, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.810, mean reward: -0.178 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.244, 10.107], loss: 0.002385, mae: 0.048726, mean_q: -0.298670
 72342/100000: episode: 1538, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -9.681, mean reward: -0.097 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.978, 10.098], loss: 0.002368, mae: 0.047949, mean_q: -0.308752
 72442/100000: episode: 1539, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.597, mean reward: -0.176 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.633, 10.098], loss: 0.002366, mae: 0.048129, mean_q: -0.314700
 72542/100000: episode: 1540, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.160, mean reward: -0.182 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.568, 10.098], loss: 0.002398, mae: 0.048838, mean_q: -0.280966
[Info] 100-TH LEVEL FOUND: 0.6229453086853027, Considering 10/90 traces
 72642/100000: episode: 1541, duration: 4.405s, episode steps: 100, steps per second: 23, episode reward: -17.027, mean reward: -0.170 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.490, 10.214], loss: 0.002378, mae: 0.047590, mean_q: -0.313801
 72658/100000: episode: 1542, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 7.164, mean reward: 0.448 [0.348, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.792, 10.100], loss: 0.002109, mae: 0.045217, mean_q: -0.420641
 72687/100000: episode: 1543, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 8.699, mean reward: 0.300 [0.165, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.306, 10.100], loss: 0.002373, mae: 0.046906, mean_q: -0.370599
 72703/100000: episode: 1544, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 5.491, mean reward: 0.343 [0.254, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.537, 10.100], loss: 0.002488, mae: 0.049579, mean_q: -0.268799
 72712/100000: episode: 1545, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 3.540, mean reward: 0.393 [0.307, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.479, 10.100], loss: 0.002280, mae: 0.049033, mean_q: -0.320747
 72731/100000: episode: 1546, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 8.355, mean reward: 0.440 [0.348, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.427, 10.100], loss: 0.002219, mae: 0.046047, mean_q: -0.332727
 72760/100000: episode: 1547, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 8.989, mean reward: 0.310 [0.195, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-1.506, 10.100], loss: 0.002715, mae: 0.053701, mean_q: -0.348356
 72775/100000: episode: 1548, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 5.582, mean reward: 0.372 [0.269, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.473], loss: 0.005129, mae: 0.063022, mean_q: -0.325013
 72817/100000: episode: 1549, duration: 0.210s, episode steps: 42, steps per second: 200, episode reward: 7.942, mean reward: 0.189 [0.034, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-1.452, 10.100], loss: 0.004851, mae: 0.067197, mean_q: -0.282182
 72831/100000: episode: 1550, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 4.821, mean reward: 0.344 [0.239, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.404], loss: 0.003186, mae: 0.058763, mean_q: -0.215042
 72860/100000: episode: 1551, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 7.516, mean reward: 0.259 [0.066, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.368, 10.130], loss: 0.002619, mae: 0.052339, mean_q: -0.248422
 72889/100000: episode: 1552, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 7.509, mean reward: 0.259 [0.034, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.491, 10.150], loss: 0.002258, mae: 0.046667, mean_q: -0.352916
 72904/100000: episode: 1553, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 6.726, mean reward: 0.448 [0.344, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.861, 10.100], loss: 0.002124, mae: 0.046883, mean_q: -0.309224
 72919/100000: episode: 1554, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 4.675, mean reward: 0.312 [0.234, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.266, 10.100], loss: 0.002337, mae: 0.048262, mean_q: -0.259331
 72961/100000: episode: 1555, duration: 0.226s, episode steps: 42, steps per second: 186, episode reward: 13.073, mean reward: 0.311 [0.176, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.638, 10.100], loss: 0.002473, mae: 0.049469, mean_q: -0.288451
 72976/100000: episode: 1556, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 4.624, mean reward: 0.308 [0.243, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.346, 10.100], loss: 0.002350, mae: 0.048693, mean_q: -0.254361
 73020/100000: episode: 1557, duration: 0.221s, episode steps: 44, steps per second: 199, episode reward: 10.273, mean reward: 0.233 [0.074, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.307, 10.100], loss: 0.002434, mae: 0.049613, mean_q: -0.243359
 73065/100000: episode: 1558, duration: 0.231s, episode steps: 45, steps per second: 194, episode reward: 12.111, mean reward: 0.269 [0.153, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.702, 10.100], loss: 0.002644, mae: 0.051435, mean_q: -0.206519
 73109/100000: episode: 1559, duration: 0.225s, episode steps: 44, steps per second: 196, episode reward: 11.531, mean reward: 0.262 [0.139, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-1.267, 10.100], loss: 0.002431, mae: 0.048187, mean_q: -0.272536
 73128/100000: episode: 1560, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 5.867, mean reward: 0.309 [0.232, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.326, 10.100], loss: 0.002309, mae: 0.049269, mean_q: -0.199106
 73173/100000: episode: 1561, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 12.386, mean reward: 0.275 [0.155, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.889, 10.100], loss: 0.002786, mae: 0.052837, mean_q: -0.231734
 73188/100000: episode: 1562, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 4.108, mean reward: 0.274 [0.144, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.871, 10.100], loss: 0.002908, mae: 0.052617, mean_q: -0.304292
 73204/100000: episode: 1563, duration: 0.076s, episode steps: 16, steps per second: 209, episode reward: 5.079, mean reward: 0.317 [0.237, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.243, 10.100], loss: 0.003058, mae: 0.057256, mean_q: -0.170375
 73213/100000: episode: 1564, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 3.454, mean reward: 0.384 [0.308, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.270, 10.100], loss: 0.002580, mae: 0.053291, mean_q: -0.235590
 73228/100000: episode: 1565, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 6.035, mean reward: 0.402 [0.338, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.508], loss: 0.002478, mae: 0.049921, mean_q: -0.296786
 73272/100000: episode: 1566, duration: 0.242s, episode steps: 44, steps per second: 182, episode reward: 12.521, mean reward: 0.285 [0.142, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.429, 10.100], loss: 0.002547, mae: 0.050711, mean_q: -0.197364
 73301/100000: episode: 1567, duration: 0.142s, episode steps: 29, steps per second: 205, episode reward: 11.236, mean reward: 0.387 [0.317, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.745, 10.100], loss: 0.002859, mae: 0.052934, mean_q: -0.140906
 73343/100000: episode: 1568, duration: 0.208s, episode steps: 42, steps per second: 202, episode reward: 10.626, mean reward: 0.253 [0.113, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.388, 10.100], loss: 0.002279, mae: 0.048306, mean_q: -0.196888
 73358/100000: episode: 1569, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 5.317, mean reward: 0.354 [0.243, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.457, 10.100], loss: 0.002524, mae: 0.050892, mean_q: -0.178380
 73367/100000: episode: 1570, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 3.141, mean reward: 0.349 [0.298, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.544, 10.100], loss: 0.002316, mae: 0.049648, mean_q: -0.190119
 73396/100000: episode: 1571, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 8.993, mean reward: 0.310 [0.195, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.195, 10.100], loss: 0.002802, mae: 0.052710, mean_q: -0.198807
 73411/100000: episode: 1572, duration: 0.072s, episode steps: 15, steps per second: 208, episode reward: 6.706, mean reward: 0.447 [0.348, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.082, 10.482], loss: 0.002591, mae: 0.051022, mean_q: -0.241464
 73426/100000: episode: 1573, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 4.380, mean reward: 0.292 [0.209, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.757, 10.100], loss: 0.002093, mae: 0.045414, mean_q: -0.262316
 73441/100000: episode: 1574, duration: 0.073s, episode steps: 15, steps per second: 204, episode reward: 4.082, mean reward: 0.272 [0.135, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-1.077, 10.100], loss: 0.002411, mae: 0.049580, mean_q: -0.166174
 73450/100000: episode: 1575, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 3.371, mean reward: 0.375 [0.331, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.293, 10.100], loss: 0.002931, mae: 0.054668, mean_q: -0.079496
 73494/100000: episode: 1576, duration: 0.224s, episode steps: 44, steps per second: 196, episode reward: 8.740, mean reward: 0.199 [0.057, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.192, 10.100], loss: 0.002489, mae: 0.049661, mean_q: -0.159055
 73509/100000: episode: 1577, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 5.262, mean reward: 0.351 [0.277, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.975, 10.100], loss: 0.003032, mae: 0.055653, mean_q: -0.139365
 73518/100000: episode: 1578, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 3.650, mean reward: 0.406 [0.344, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.277, 10.100], loss: 0.002604, mae: 0.052709, mean_q: -0.148348
 73547/100000: episode: 1579, duration: 0.145s, episode steps: 29, steps per second: 201, episode reward: 13.335, mean reward: 0.460 [0.246, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.601, 10.100], loss: 0.002577, mae: 0.052261, mean_q: -0.134322
 73592/100000: episode: 1580, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 10.020, mean reward: 0.223 [0.047, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.144, 10.138], loss: 0.002552, mae: 0.051739, mean_q: -0.111165
 73611/100000: episode: 1581, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 6.351, mean reward: 0.334 [0.253, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.299, 10.100], loss: 0.002700, mae: 0.054357, mean_q: -0.021881
 73630/100000: episode: 1582, duration: 0.102s, episode steps: 19, steps per second: 185, episode reward: 7.388, mean reward: 0.389 [0.322, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.660, 10.100], loss: 0.002551, mae: 0.050821, mean_q: -0.178711
 73639/100000: episode: 1583, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 3.578, mean reward: 0.398 [0.327, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.367, 10.100], loss: 0.002422, mae: 0.048103, mean_q: -0.193413
 73653/100000: episode: 1584, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 4.942, mean reward: 0.353 [0.279, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.542], loss: 0.002856, mae: 0.054838, mean_q: -0.033777
 73662/100000: episode: 1585, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 4.028, mean reward: 0.448 [0.342, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.241, 10.100], loss: 0.003196, mae: 0.054756, mean_q: -0.027199
 73707/100000: episode: 1586, duration: 0.224s, episode steps: 45, steps per second: 201, episode reward: 16.841, mean reward: 0.374 [0.231, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.670, 10.100], loss: 0.002741, mae: 0.053826, mean_q: -0.134872
 73721/100000: episode: 1587, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 4.611, mean reward: 0.329 [0.259, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.433], loss: 0.003240, mae: 0.056047, mean_q: -0.191037
 73735/100000: episode: 1588, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 4.384, mean reward: 0.313 [0.261, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.353], loss: 0.002756, mae: 0.053769, mean_q: -0.117460
 73764/100000: episode: 1589, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 6.643, mean reward: 0.229 [0.079, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.426, 10.100], loss: 0.002967, mae: 0.054633, mean_q: -0.122890
 73773/100000: episode: 1590, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 3.861, mean reward: 0.429 [0.395, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.518, 10.100], loss: 0.003082, mae: 0.055519, mean_q: -0.094205
 73788/100000: episode: 1591, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 5.711, mean reward: 0.381 [0.284, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.498, 10.444], loss: 0.002665, mae: 0.052394, mean_q: -0.053700
 73830/100000: episode: 1592, duration: 0.216s, episode steps: 42, steps per second: 194, episode reward: 15.480, mean reward: 0.369 [0.179, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.652, 10.100], loss: 0.002919, mae: 0.054425, mean_q: -0.078835
 73845/100000: episode: 1593, duration: 0.072s, episode steps: 15, steps per second: 209, episode reward: 2.780, mean reward: 0.185 [0.065, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.193, 10.100], loss: 0.003118, mae: 0.056155, mean_q: -0.072872
 73859/100000: episode: 1594, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 4.422, mean reward: 0.316 [0.158, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.369], loss: 0.010092, mae: 0.081429, mean_q: -0.052074
 73878/100000: episode: 1595, duration: 0.097s, episode steps: 19, steps per second: 197, episode reward: 5.917, mean reward: 0.311 [0.187, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.264, 10.100], loss: 0.021274, mae: 0.114149, mean_q: -0.117429
 73920/100000: episode: 1596, duration: 0.200s, episode steps: 42, steps per second: 210, episode reward: 10.127, mean reward: 0.241 [0.067, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.784, 10.171], loss: 0.007693, mae: 0.078039, mean_q: -0.086459
 73962/100000: episode: 1597, duration: 0.199s, episode steps: 42, steps per second: 211, episode reward: 13.315, mean reward: 0.317 [0.193, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-1.004, 10.100], loss: 0.004518, mae: 0.063450, mean_q: -0.077654
 74007/100000: episode: 1598, duration: 0.222s, episode steps: 45, steps per second: 203, episode reward: 11.612, mean reward: 0.258 [0.043, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.736, 10.148], loss: 0.004174, mae: 0.062745, mean_q: -0.065929
 74026/100000: episode: 1599, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 6.708, mean reward: 0.353 [0.296, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.010, 10.100], loss: 0.003668, mae: 0.061990, mean_q: -0.012820
 74041/100000: episode: 1600, duration: 0.073s, episode steps: 15, steps per second: 207, episode reward: 5.578, mean reward: 0.372 [0.269, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.594, 10.100], loss: 0.003593, mae: 0.065088, mean_q: -0.050999
 74085/100000: episode: 1601, duration: 0.227s, episode steps: 44, steps per second: 194, episode reward: 10.349, mean reward: 0.235 [0.115, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.091, 10.100], loss: 0.002719, mae: 0.054689, mean_q: -0.049543
 74104/100000: episode: 1602, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 7.363, mean reward: 0.388 [0.268, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.373, 10.100], loss: 0.003922, mae: 0.063528, mean_q: -0.033441
 74149/100000: episode: 1603, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 15.167, mean reward: 0.337 [0.236, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.227, 10.100], loss: 0.002918, mae: 0.055912, mean_q: -0.067249
 74164/100000: episode: 1604, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 4.578, mean reward: 0.305 [0.229, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.358, 10.100], loss: 0.002830, mae: 0.056230, mean_q: 0.026425
 74183/100000: episode: 1605, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 5.521, mean reward: 0.291 [0.100, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.127, 10.100], loss: 0.002522, mae: 0.050402, mean_q: -0.034002
 74225/100000: episode: 1606, duration: 0.208s, episode steps: 42, steps per second: 202, episode reward: 9.786, mean reward: 0.233 [0.031, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.862, 10.100], loss: 0.002910, mae: 0.054687, mean_q: -0.026303
 74244/100000: episode: 1607, duration: 0.089s, episode steps: 19, steps per second: 214, episode reward: 6.386, mean reward: 0.336 [0.215, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.141, 10.100], loss: 0.002985, mae: 0.057246, mean_q: 0.030903
 74259/100000: episode: 1608, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 5.711, mean reward: 0.381 [0.275, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.323, 10.100], loss: 0.002675, mae: 0.052815, mean_q: -0.035181
 74304/100000: episode: 1609, duration: 0.225s, episode steps: 45, steps per second: 200, episode reward: 9.019, mean reward: 0.200 [0.026, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.170, 10.100], loss: 0.003161, mae: 0.057721, mean_q: 0.039363
 74333/100000: episode: 1610, duration: 0.144s, episode steps: 29, steps per second: 202, episode reward: 12.000, mean reward: 0.414 [0.261, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.306, 10.100], loss: 0.003257, mae: 0.058661, mean_q: 0.016953
 74352/100000: episode: 1611, duration: 0.090s, episode steps: 19, steps per second: 212, episode reward: 6.855, mean reward: 0.361 [0.300, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.213, 10.100], loss: 0.002728, mae: 0.053978, mean_q: 0.000380
 74381/100000: episode: 1612, duration: 0.135s, episode steps: 29, steps per second: 214, episode reward: 11.992, mean reward: 0.414 [0.270, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.332, 10.100], loss: 0.002544, mae: 0.052403, mean_q: 0.045182
 74397/100000: episode: 1613, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 6.338, mean reward: 0.396 [0.299, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.254, 10.100], loss: 0.002981, mae: 0.055450, mean_q: -0.003470
 74439/100000: episode: 1614, duration: 0.212s, episode steps: 42, steps per second: 198, episode reward: 7.041, mean reward: 0.168 [0.030, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.578, 10.100], loss: 0.003272, mae: 0.055132, mean_q: 0.036207
 74453/100000: episode: 1615, duration: 0.067s, episode steps: 14, steps per second: 208, episode reward: 6.181, mean reward: 0.441 [0.378, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-1.169, 10.512], loss: 0.005584, mae: 0.059014, mean_q: 0.075643
 74498/100000: episode: 1616, duration: 0.254s, episode steps: 45, steps per second: 177, episode reward: 12.487, mean reward: 0.277 [0.098, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-1.142, 10.100], loss: 0.004544, mae: 0.067515, mean_q: 0.040230
 74543/100000: episode: 1617, duration: 0.252s, episode steps: 45, steps per second: 179, episode reward: 7.872, mean reward: 0.175 [0.022, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.584, 10.120], loss: 0.003810, mae: 0.063290, mean_q: 0.033208
 74552/100000: episode: 1618, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 3.755, mean reward: 0.417 [0.362, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.400, 10.100], loss: 0.003388, mae: 0.063437, mean_q: 0.161245
 74561/100000: episode: 1619, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 3.620, mean reward: 0.402 [0.352, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.889, 10.100], loss: 0.003045, mae: 0.059997, mean_q: 0.047786
 74577/100000: episode: 1620, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 7.138, mean reward: 0.446 [0.326, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.544, 10.100], loss: 0.002945, mae: 0.059711, mean_q: 0.125720
 74596/100000: episode: 1621, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 8.040, mean reward: 0.423 [0.306, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.411, 10.100], loss: 0.003161, mae: 0.058763, mean_q: 0.003680
 74638/100000: episode: 1622, duration: 0.220s, episode steps: 42, steps per second: 191, episode reward: 12.478, mean reward: 0.297 [0.123, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.527, 10.100], loss: 0.002751, mae: 0.054890, mean_q: 0.066347
 74653/100000: episode: 1623, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 5.579, mean reward: 0.372 [0.303, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.886, 10.465], loss: 0.003194, mae: 0.058365, mean_q: 0.021303
 74667/100000: episode: 1624, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 6.070, mean reward: 0.434 [0.338, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.146, 10.490], loss: 0.003649, mae: 0.063657, mean_q: 0.094480
 74696/100000: episode: 1625, duration: 0.149s, episode steps: 29, steps per second: 194, episode reward: 5.948, mean reward: 0.205 [0.010, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.061, 10.142], loss: 0.003169, mae: 0.057059, mean_q: 0.074140
 74715/100000: episode: 1626, duration: 0.090s, episode steps: 19, steps per second: 212, episode reward: 6.097, mean reward: 0.321 [0.275, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.216, 10.100], loss: 0.002890, mae: 0.055500, mean_q: 0.091232
 74744/100000: episode: 1627, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 8.952, mean reward: 0.309 [0.226, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.944, 10.100], loss: 0.003013, mae: 0.057460, mean_q: 0.118175
 74759/100000: episode: 1628, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 4.856, mean reward: 0.324 [0.101, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.256], loss: 0.003046, mae: 0.057521, mean_q: 0.152701
 74801/100000: episode: 1629, duration: 0.202s, episode steps: 42, steps per second: 208, episode reward: 9.790, mean reward: 0.233 [0.078, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.167, 10.128], loss: 0.002786, mae: 0.054632, mean_q: 0.065142
 74846/100000: episode: 1630, duration: 0.237s, episode steps: 45, steps per second: 190, episode reward: 7.707, mean reward: 0.171 [0.020, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.183, 10.100], loss: 0.002781, mae: 0.054689, mean_q: 0.066342
[Info] 200-TH LEVEL FOUND: 0.8426347374916077, Considering 10/90 traces
 74860/100000: episode: 1631, duration: 3.988s, episode steps: 14, steps per second: 4, episode reward: 6.734, mean reward: 0.481 [0.336, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.662], loss: 0.003212, mae: 0.057386, mean_q: 0.072665
 74872/100000: episode: 1632, duration: 0.080s, episode steps: 12, steps per second: 150, episode reward: 5.150, mean reward: 0.429 [0.313, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.864, 10.100], loss: 0.002903, mae: 0.053520, mean_q: 0.071731
 74880/100000: episode: 1633, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 4.080, mean reward: 0.510 [0.386, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.417], loss: 0.003432, mae: 0.058366, mean_q: 0.103173
 74891/100000: episode: 1634, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 4.051, mean reward: 0.368 [0.318, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.428, 10.100], loss: 0.003416, mae: 0.059892, mean_q: 0.137255
 74902/100000: episode: 1635, duration: 0.083s, episode steps: 11, steps per second: 132, episode reward: 5.083, mean reward: 0.462 [0.369, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.349, 10.100], loss: 0.003119, mae: 0.058802, mean_q: 0.056213
 74924/100000: episode: 1636, duration: 0.107s, episode steps: 22, steps per second: 206, episode reward: 10.211, mean reward: 0.464 [0.306, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.227, 10.100], loss: 0.003028, mae: 0.056288, mean_q: 0.098642
 74951/100000: episode: 1637, duration: 0.158s, episode steps: 27, steps per second: 171, episode reward: 12.163, mean reward: 0.450 [0.221, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.529, 10.100], loss: 0.002924, mae: 0.055894, mean_q: 0.135625
 74975/100000: episode: 1638, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 10.478, mean reward: 0.437 [0.356, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.371, 10.100], loss: 0.002892, mae: 0.055742, mean_q: 0.090678
 74983/100000: episode: 1639, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 2.655, mean reward: 0.332 [0.256, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.327, 10.100], loss: 0.002850, mae: 0.054206, mean_q: 0.089178
 75010/100000: episode: 1640, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 8.955, mean reward: 0.332 [0.145, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.387, 10.100], loss: 0.003094, mae: 0.058519, mean_q: 0.141518
 75022/100000: episode: 1641, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 5.497, mean reward: 0.458 [0.323, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.563, 10.100], loss: 0.003205, mae: 0.058195, mean_q: 0.138678
 75032/100000: episode: 1642, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 4.707, mean reward: 0.471 [0.362, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.199, 10.100], loss: 0.002835, mae: 0.053677, mean_q: 0.172999
 75054/100000: episode: 1643, duration: 0.114s, episode steps: 22, steps per second: 192, episode reward: 10.388, mean reward: 0.472 [0.409, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.446, 10.100], loss: 0.002604, mae: 0.053907, mean_q: 0.149584
 75062/100000: episode: 1644, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 2.994, mean reward: 0.374 [0.309, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.497, 10.100], loss: 0.002851, mae: 0.055688, mean_q: 0.168242
 75070/100000: episode: 1645, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 3.748, mean reward: 0.468 [0.424, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.533], loss: 0.003117, mae: 0.057752, mean_q: 0.113934
 75081/100000: episode: 1646, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 4.484, mean reward: 0.408 [0.331, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.388, 10.100], loss: 0.002704, mae: 0.053378, mean_q: 0.163313
 75108/100000: episode: 1647, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 12.748, mean reward: 0.472 [0.394, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.015, 10.100], loss: 0.002894, mae: 0.055206, mean_q: 0.156253
 75118/100000: episode: 1648, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 4.295, mean reward: 0.430 [0.338, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.252, 10.100], loss: 0.002525, mae: 0.054021, mean_q: 0.223684
 75129/100000: episode: 1649, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 4.686, mean reward: 0.426 [0.355, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.409, 10.100], loss: 0.002841, mae: 0.056520, mean_q: 0.200806
 75138/100000: episode: 1650, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 3.729, mean reward: 0.414 [0.380, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.430, 10.100], loss: 0.003098, mae: 0.059179, mean_q: 0.195540
 75150/100000: episode: 1651, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 4.928, mean reward: 0.411 [0.346, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.378, 10.100], loss: 0.003022, mae: 0.055756, mean_q: 0.133487
 75161/100000: episode: 1652, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 4.353, mean reward: 0.396 [0.316, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.362, 10.100], loss: 0.002721, mae: 0.054412, mean_q: 0.123290
 75170/100000: episode: 1653, duration: 0.062s, episode steps: 9, steps per second: 146, episode reward: 3.554, mean reward: 0.395 [0.311, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-1.390, 10.100], loss: 0.002920, mae: 0.058789, mean_q: 0.172039
 75181/100000: episode: 1654, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 5.013, mean reward: 0.456 [0.373, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.412, 10.100], loss: 0.003261, mae: 0.059593, mean_q: 0.176667
 75189/100000: episode: 1655, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 3.811, mean reward: 0.476 [0.439, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.222, 10.601], loss: 0.003007, mae: 0.057882, mean_q: 0.187762
 75199/100000: episode: 1656, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 5.046, mean reward: 0.505 [0.386, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.418, 10.100], loss: 0.003459, mae: 0.060246, mean_q: 0.103026
 75207/100000: episode: 1657, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 4.161, mean reward: 0.520 [0.472, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.425], loss: 0.003284, mae: 0.058534, mean_q: 0.097501
 75215/100000: episode: 1658, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 4.149, mean reward: 0.519 [0.458, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.684], loss: 0.003161, mae: 0.056857, mean_q: 0.073837
 75227/100000: episode: 1659, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 5.408, mean reward: 0.451 [0.406, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.903, 10.100], loss: 0.003004, mae: 0.058948, mean_q: 0.252117
 75239/100000: episode: 1660, duration: 0.064s, episode steps: 12, steps per second: 189, episode reward: 6.104, mean reward: 0.509 [0.404, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.371, 10.100], loss: 0.002801, mae: 0.056076, mean_q: 0.220680
 75247/100000: episode: 1661, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 3.676, mean reward: 0.459 [0.419, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.582], loss: 0.003292, mae: 0.059909, mean_q: 0.198941
 75255/100000: episode: 1662, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 3.130, mean reward: 0.391 [0.363, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.312, 10.100], loss: 0.003542, mae: 0.059671, mean_q: 0.120499
 75267/100000: episode: 1663, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 4.724, mean reward: 0.394 [0.171, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.068, 10.100], loss: 0.002988, mae: 0.055501, mean_q: 0.199195
 75278/100000: episode: 1664, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 4.345, mean reward: 0.395 [0.334, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.455, 10.100], loss: 0.003175, mae: 0.058066, mean_q: 0.219806
 75302/100000: episode: 1665, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 9.293, mean reward: 0.387 [0.291, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.847, 10.100], loss: 0.002852, mae: 0.055102, mean_q: 0.187930
 75310/100000: episode: 1666, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 3.272, mean reward: 0.409 [0.310, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.405, 10.100], loss: 0.003623, mae: 0.061463, mean_q: 0.272067
[Info] FALSIFICATION!
 75323/100000: episode: 1667, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 16.663, mean reward: 1.282 [0.381, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.409, 9.911], loss: 0.003408, mae: 0.062537, mean_q: 0.218001
 75423/100000: episode: 1668, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.072, mean reward: -0.191 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.286, 10.098], loss: 0.016161, mae: 0.066829, mean_q: 0.210900
 75523/100000: episode: 1669, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -20.228, mean reward: -0.202 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.573, 10.244], loss: 0.003369, mae: 0.060126, mean_q: 0.207517
 75623/100000: episode: 1670, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.722, mean reward: -0.187 [-1.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.859, 10.122], loss: 0.015576, mae: 0.063711, mean_q: 0.210250
 75723/100000: episode: 1671, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.199, mean reward: -0.172 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.799, 10.323], loss: 0.065085, mae: 0.098423, mean_q: 0.192225
 75823/100000: episode: 1672, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.605, mean reward: -0.166 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.861, 10.155], loss: 0.015383, mae: 0.066653, mean_q: 0.224033
 75923/100000: episode: 1673, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -14.719, mean reward: -0.147 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.117, 10.098], loss: 0.003404, mae: 0.061605, mean_q: 0.230876
 76023/100000: episode: 1674, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.192, mean reward: -0.162 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.625, 10.098], loss: 0.015453, mae: 0.065536, mean_q: 0.214780
 76123/100000: episode: 1675, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -19.171, mean reward: -0.192 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.647, 10.141], loss: 0.015503, mae: 0.065568, mean_q: 0.207051
 76223/100000: episode: 1676, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.426, mean reward: -0.184 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.464, 10.130], loss: 0.027114, mae: 0.071692, mean_q: 0.209508
 76323/100000: episode: 1677, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -13.033, mean reward: -0.130 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.431, 10.456], loss: 0.003109, mae: 0.058248, mean_q: 0.193999
 76423/100000: episode: 1678, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.059, mean reward: -0.191 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.510, 10.176], loss: 0.003224, mae: 0.059770, mean_q: 0.200356
 76523/100000: episode: 1679, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.360, mean reward: -0.184 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.807, 10.098], loss: 0.003095, mae: 0.058685, mean_q: 0.196774
 76623/100000: episode: 1680, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.531, mean reward: -0.165 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.794, 10.269], loss: 0.002924, mae: 0.056297, mean_q: 0.179867
 76723/100000: episode: 1681, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.528, mean reward: -0.175 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.172, 10.259], loss: 0.002885, mae: 0.056477, mean_q: 0.222870
 76823/100000: episode: 1682, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -18.500, mean reward: -0.185 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.808, 10.098], loss: 0.002962, mae: 0.058316, mean_q: 0.208306
 76923/100000: episode: 1683, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.601, mean reward: -0.176 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.872, 10.098], loss: 0.016973, mae: 0.076159, mean_q: 0.235495
 77023/100000: episode: 1684, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -16.235, mean reward: -0.162 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.768, 10.098], loss: 0.028976, mae: 0.080884, mean_q: 0.204357
 77123/100000: episode: 1685, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.339, mean reward: -0.163 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.292, 10.168], loss: 0.003055, mae: 0.057421, mean_q: 0.179537
 77223/100000: episode: 1686, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -12.650, mean reward: -0.127 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.118, 10.360], loss: 0.027352, mae: 0.072026, mean_q: 0.219197
 77323/100000: episode: 1687, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.808, mean reward: -0.188 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.426, 10.120], loss: 0.003333, mae: 0.060725, mean_q: 0.215288
 77423/100000: episode: 1688, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.661, mean reward: -0.187 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.366, 10.137], loss: 0.027135, mae: 0.068819, mean_q: 0.210624
 77523/100000: episode: 1689, duration: 0.668s, episode steps: 100, steps per second: 150, episode reward: -16.256, mean reward: -0.163 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.634, 10.148], loss: 0.027228, mae: 0.072760, mean_q: 0.213605
 77623/100000: episode: 1690, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -13.534, mean reward: -0.135 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.796, 10.098], loss: 0.015182, mae: 0.065647, mean_q: 0.154227
 77723/100000: episode: 1691, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -12.805, mean reward: -0.128 [-1.000, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.827, 10.324], loss: 0.014819, mae: 0.062271, mean_q: 0.156642
 77823/100000: episode: 1692, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -13.864, mean reward: -0.139 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.693, 10.098], loss: 0.014745, mae: 0.062434, mean_q: 0.134000
 77923/100000: episode: 1693, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.103, mean reward: -0.161 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.826, 10.160], loss: 0.003063, mae: 0.057437, mean_q: 0.140103
 78023/100000: episode: 1694, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -14.446, mean reward: -0.144 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.449, 10.098], loss: 0.003005, mae: 0.056947, mean_q: 0.101490
 78123/100000: episode: 1695, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -13.827, mean reward: -0.138 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.177, 10.098], loss: 0.014810, mae: 0.061567, mean_q: 0.074223
 78223/100000: episode: 1696, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.346, mean reward: -0.153 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.826, 10.369], loss: 0.002909, mae: 0.055392, mean_q: 0.084422
 78323/100000: episode: 1697, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -15.605, mean reward: -0.156 [-1.000, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.893, 10.237], loss: 0.038983, mae: 0.076147, mean_q: 0.054542
 78423/100000: episode: 1698, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -14.228, mean reward: -0.142 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.467, 10.323], loss: 0.014615, mae: 0.061692, mean_q: 0.048713
 78523/100000: episode: 1699, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -20.266, mean reward: -0.203 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.098, 10.221], loss: 0.003011, mae: 0.055977, mean_q: 0.024982
 78623/100000: episode: 1700, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -19.279, mean reward: -0.193 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.622, 10.195], loss: 0.003167, mae: 0.058130, mean_q: 0.018471
 78723/100000: episode: 1701, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.863, mean reward: -0.139 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.394, 10.243], loss: 0.003050, mae: 0.056783, mean_q: -0.025761
 78823/100000: episode: 1702, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -13.625, mean reward: -0.136 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.956, 10.098], loss: 0.026812, mae: 0.067521, mean_q: -0.022712
 78923/100000: episode: 1703, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.077, mean reward: -0.181 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.543, 10.098], loss: 0.002929, mae: 0.056241, mean_q: -0.038700
 79023/100000: episode: 1704, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.777, mean reward: -0.178 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.729, 10.098], loss: 0.002775, mae: 0.053287, mean_q: -0.068273
 79123/100000: episode: 1705, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -13.624, mean reward: -0.136 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.423, 10.098], loss: 0.014925, mae: 0.061010, mean_q: -0.118763
 79223/100000: episode: 1706, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.813, mean reward: -0.188 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.454, 10.098], loss: 0.002849, mae: 0.054781, mean_q: -0.093666
 79323/100000: episode: 1707, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -10.358, mean reward: -0.104 [-1.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.749, 10.098], loss: 0.015155, mae: 0.064984, mean_q: -0.108934
 79423/100000: episode: 1708, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.989, mean reward: -0.200 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.430, 10.098], loss: 0.002538, mae: 0.050683, mean_q: -0.147797
 79523/100000: episode: 1709, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.606, mean reward: -0.166 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.950, 10.098], loss: 0.038300, mae: 0.070186, mean_q: -0.138276
 79623/100000: episode: 1710, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.843, mean reward: -0.198 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.155, 10.098], loss: 0.014692, mae: 0.063131, mean_q: -0.147654
 79723/100000: episode: 1711, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -17.765, mean reward: -0.178 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.689, 10.098], loss: 0.026302, mae: 0.069729, mean_q: -0.176677
 79823/100000: episode: 1712, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -17.875, mean reward: -0.179 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.902, 10.179], loss: 0.014468, mae: 0.059087, mean_q: -0.238199
 79923/100000: episode: 1713, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -15.372, mean reward: -0.154 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.291, 10.312], loss: 0.013473, mae: 0.052573, mean_q: -0.215779
 80023/100000: episode: 1714, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.169, mean reward: -0.182 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.975, 10.244], loss: 0.014881, mae: 0.062576, mean_q: -0.260949
 80123/100000: episode: 1715, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -16.673, mean reward: -0.167 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.654, 10.098], loss: 0.024613, mae: 0.061162, mean_q: -0.266896
 80223/100000: episode: 1716, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.056, mean reward: -0.171 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.526, 10.234], loss: 0.014662, mae: 0.064593, mean_q: -0.260717
 80323/100000: episode: 1717, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.976, mean reward: -0.150 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.586, 10.121], loss: 0.003235, mae: 0.053191, mean_q: -0.308858
 80423/100000: episode: 1718, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -12.047, mean reward: -0.120 [-1.000, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.607, 10.098], loss: 0.002416, mae: 0.047588, mean_q: -0.316218
 80523/100000: episode: 1719, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -16.497, mean reward: -0.165 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.044, 10.098], loss: 0.002473, mae: 0.048015, mean_q: -0.327403
 80623/100000: episode: 1720, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.798, mean reward: -0.168 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.643, 10.098], loss: 0.002405, mae: 0.047886, mean_q: -0.329213
 80723/100000: episode: 1721, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -13.549, mean reward: -0.135 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.948, 10.098], loss: 0.002416, mae: 0.047663, mean_q: -0.264196
 80823/100000: episode: 1722, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.017, mean reward: -0.170 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.075, 10.098], loss: 0.002258, mae: 0.046065, mean_q: -0.300096
 80923/100000: episode: 1723, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.244, mean reward: -0.182 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.105, 10.208], loss: 0.002131, mae: 0.044631, mean_q: -0.329423
 81023/100000: episode: 1724, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -16.371, mean reward: -0.164 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.659, 10.208], loss: 0.002298, mae: 0.046177, mean_q: -0.293590
 81123/100000: episode: 1725, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.120, mean reward: -0.181 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.718, 10.098], loss: 0.002264, mae: 0.046161, mean_q: -0.338363
 81223/100000: episode: 1726, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.795, mean reward: -0.168 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.201, 10.174], loss: 0.002373, mae: 0.047318, mean_q: -0.286152
 81323/100000: episode: 1727, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -15.598, mean reward: -0.156 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.914, 10.098], loss: 0.002413, mae: 0.047085, mean_q: -0.290417
 81423/100000: episode: 1728, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.743, mean reward: -0.167 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.706, 10.098], loss: 0.002399, mae: 0.047801, mean_q: -0.310569
 81523/100000: episode: 1729, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.270, mean reward: -0.173 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.447, 10.104], loss: 0.004730, mae: 0.062350, mean_q: -0.293809
 81623/100000: episode: 1730, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.645, mean reward: -0.176 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.662, 10.239], loss: 0.002651, mae: 0.051627, mean_q: -0.341395
 81723/100000: episode: 1731, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -17.628, mean reward: -0.176 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.695, 10.098], loss: 0.002383, mae: 0.047052, mean_q: -0.307544
 81823/100000: episode: 1732, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.159, mean reward: -0.172 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.121, 10.222], loss: 0.002221, mae: 0.045970, mean_q: -0.264468
 81923/100000: episode: 1733, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -20.998, mean reward: -0.210 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.250, 10.127], loss: 0.002168, mae: 0.044517, mean_q: -0.326478
 82023/100000: episode: 1734, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.957, mean reward: -0.180 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.059, 10.124], loss: 0.002275, mae: 0.046035, mean_q: -0.301010
 82123/100000: episode: 1735, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -19.363, mean reward: -0.194 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.922, 10.202], loss: 0.002317, mae: 0.047163, mean_q: -0.299394
 82223/100000: episode: 1736, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -13.728, mean reward: -0.137 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.002, 10.098], loss: 0.002179, mae: 0.045177, mean_q: -0.331672
 82323/100000: episode: 1737, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -14.200, mean reward: -0.142 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.539, 10.151], loss: 0.002158, mae: 0.045860, mean_q: -0.317010
 82423/100000: episode: 1738, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -14.812, mean reward: -0.148 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.939, 10.098], loss: 0.002275, mae: 0.046140, mean_q: -0.306317
 82523/100000: episode: 1739, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.302, mean reward: -0.173 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.712, 10.098], loss: 0.002312, mae: 0.046215, mean_q: -0.309430
 82623/100000: episode: 1740, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -14.620, mean reward: -0.146 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.872, 10.310], loss: 0.002288, mae: 0.046222, mean_q: -0.318607
 82723/100000: episode: 1741, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -10.606, mean reward: -0.106 [-1.000, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.812, 10.514], loss: 0.002220, mae: 0.045583, mean_q: -0.306887
 82823/100000: episode: 1742, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -15.690, mean reward: -0.157 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.207, 10.098], loss: 0.002288, mae: 0.045711, mean_q: -0.302867
 82923/100000: episode: 1743, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -18.749, mean reward: -0.187 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.305, 10.305], loss: 0.002086, mae: 0.044315, mean_q: -0.311323
 83023/100000: episode: 1744, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.079, mean reward: -0.161 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.413, 10.102], loss: 0.002245, mae: 0.046047, mean_q: -0.289400
 83123/100000: episode: 1745, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -11.575, mean reward: -0.116 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.889, 10.424], loss: 0.002193, mae: 0.045463, mean_q: -0.299205
 83223/100000: episode: 1746, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.707, mean reward: -0.177 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.815, 10.161], loss: 0.002328, mae: 0.046495, mean_q: -0.301531
 83323/100000: episode: 1747, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.036, mean reward: -0.170 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.204, 10.247], loss: 0.002250, mae: 0.045586, mean_q: -0.331114
 83423/100000: episode: 1748, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -15.643, mean reward: -0.156 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.544, 10.098], loss: 0.002238, mae: 0.046339, mean_q: -0.314978
 83523/100000: episode: 1749, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -16.523, mean reward: -0.165 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.115, 10.098], loss: 0.002175, mae: 0.045182, mean_q: -0.274123
 83623/100000: episode: 1750, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -13.450, mean reward: -0.135 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.501, 10.414], loss: 0.002220, mae: 0.045559, mean_q: -0.304138
 83723/100000: episode: 1751, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.821, mean reward: -0.178 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.081, 10.262], loss: 0.003577, mae: 0.054884, mean_q: -0.297747
 83823/100000: episode: 1752, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.607, mean reward: -0.156 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.429, 10.104], loss: 0.002524, mae: 0.049254, mean_q: -0.333383
 83923/100000: episode: 1753, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -7.763, mean reward: -0.078 [-1.000, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.020, 10.372], loss: 0.002253, mae: 0.046487, mean_q: -0.314179
 84023/100000: episode: 1754, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.604, mean reward: -0.176 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.820, 10.214], loss: 0.002231, mae: 0.045719, mean_q: -0.316813
 84123/100000: episode: 1755, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.570, mean reward: -0.176 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.374, 10.098], loss: 0.002232, mae: 0.045111, mean_q: -0.322394
 84223/100000: episode: 1756, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -11.876, mean reward: -0.119 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.071, 10.457], loss: 0.002390, mae: 0.047418, mean_q: -0.306254
 84323/100000: episode: 1757, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.651, mean reward: -0.187 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.920, 10.253], loss: 0.002223, mae: 0.045437, mean_q: -0.308300
 84423/100000: episode: 1758, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.933, mean reward: -0.199 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.892, 10.183], loss: 0.002380, mae: 0.047024, mean_q: -0.292867
 84523/100000: episode: 1759, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -19.497, mean reward: -0.195 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.189, 10.098], loss: 0.002441, mae: 0.047789, mean_q: -0.247742
 84623/100000: episode: 1760, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -15.138, mean reward: -0.151 [-1.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.781, 10.098], loss: 0.002334, mae: 0.046839, mean_q: -0.283888
 84723/100000: episode: 1761, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.401, mean reward: -0.174 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.410, 10.098], loss: 0.002448, mae: 0.047741, mean_q: -0.318608
 84823/100000: episode: 1762, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -19.334, mean reward: -0.193 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.256, 10.255], loss: 0.002343, mae: 0.046711, mean_q: -0.322988
 84923/100000: episode: 1763, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -13.753, mean reward: -0.138 [-1.000, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.071, 10.098], loss: 0.002349, mae: 0.046632, mean_q: -0.307725
 85023/100000: episode: 1764, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.344, mean reward: -0.143 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.653, 10.192], loss: 0.002157, mae: 0.043714, mean_q: -0.364026
 85123/100000: episode: 1765, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -19.124, mean reward: -0.191 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.620, 10.212], loss: 0.002323, mae: 0.046683, mean_q: -0.315107
 85223/100000: episode: 1766, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -13.183, mean reward: -0.132 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.737, 10.098], loss: 0.002413, mae: 0.048051, mean_q: -0.262167
[Info] 100-TH LEVEL FOUND: 0.6625675559043884, Considering 10/90 traces
 85323/100000: episode: 1767, duration: 4.367s, episode steps: 100, steps per second: 23, episode reward: -17.855, mean reward: -0.179 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.812, 10.212], loss: 0.002382, mae: 0.046446, mean_q: -0.308943
 85364/100000: episode: 1768, duration: 0.205s, episode steps: 41, steps per second: 200, episode reward: 12.378, mean reward: 0.302 [0.167, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.114, 10.389], loss: 0.002353, mae: 0.045526, mean_q: -0.295222
 85385/100000: episode: 1769, duration: 0.106s, episode steps: 21, steps per second: 199, episode reward: 9.680, mean reward: 0.461 [0.334, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.247, 10.100], loss: 0.002264, mae: 0.047461, mean_q: -0.268854
 85443/100000: episode: 1770, duration: 0.304s, episode steps: 58, steps per second: 190, episode reward: 18.147, mean reward: 0.313 [0.203, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-0.879, 10.360], loss: 0.002291, mae: 0.048801, mean_q: -0.311097
 85488/100000: episode: 1771, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 12.216, mean reward: 0.271 [0.095, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.482, 10.277], loss: 0.002299, mae: 0.046341, mean_q: -0.285376
 85533/100000: episode: 1772, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 14.241, mean reward: 0.316 [0.165, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.729, 10.247], loss: 0.002550, mae: 0.049337, mean_q: -0.225336
 85544/100000: episode: 1773, duration: 0.057s, episode steps: 11, steps per second: 191, episode reward: 3.476, mean reward: 0.316 [0.218, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.253, 10.100], loss: 0.002571, mae: 0.050060, mean_q: -0.263238
 85585/100000: episode: 1774, duration: 0.226s, episode steps: 41, steps per second: 182, episode reward: 5.868, mean reward: 0.143 [0.008, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.035, 10.115], loss: 0.002418, mae: 0.047874, mean_q: -0.263796
 85596/100000: episode: 1775, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 4.419, mean reward: 0.402 [0.320, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.643, 10.100], loss: 0.002128, mae: 0.043832, mean_q: -0.305320
 85613/100000: episode: 1776, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 4.555, mean reward: 0.268 [0.215, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.333, 10.312], loss: 0.002395, mae: 0.048431, mean_q: -0.237978
 85676/100000: episode: 1777, duration: 0.321s, episode steps: 63, steps per second: 196, episode reward: 13.637, mean reward: 0.216 [0.004, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.782 [-1.356, 10.140], loss: 0.002540, mae: 0.049770, mean_q: -0.228600
 85734/100000: episode: 1778, duration: 0.293s, episode steps: 58, steps per second: 198, episode reward: 14.241, mean reward: 0.246 [0.096, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-0.960, 10.356], loss: 0.002403, mae: 0.048733, mean_q: -0.214636
 85775/100000: episode: 1779, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 7.435, mean reward: 0.181 [0.056, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.869, 10.100], loss: 0.002593, mae: 0.050389, mean_q: -0.258507
 85833/100000: episode: 1780, duration: 0.287s, episode steps: 58, steps per second: 202, episode reward: 10.951, mean reward: 0.189 [0.038, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.833 [-0.735, 10.388], loss: 0.002489, mae: 0.049042, mean_q: -0.245128
 85891/100000: episode: 1781, duration: 0.310s, episode steps: 58, steps per second: 187, episode reward: 19.165, mean reward: 0.330 [0.123, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.820 [-0.781, 10.263], loss: 0.002264, mae: 0.046653, mean_q: -0.224928
 85912/100000: episode: 1782, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 8.394, mean reward: 0.400 [0.307, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.418, 10.100], loss: 0.002355, mae: 0.047915, mean_q: -0.178371
 85933/100000: episode: 1783, duration: 0.111s, episode steps: 21, steps per second: 190, episode reward: 5.579, mean reward: 0.266 [0.206, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.327, 10.359], loss: 0.002355, mae: 0.048036, mean_q: -0.135545
 85962/100000: episode: 1784, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 8.961, mean reward: 0.309 [0.206, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.035, 10.452], loss: 0.002353, mae: 0.048820, mean_q: -0.153135
 86007/100000: episode: 1785, duration: 0.227s, episode steps: 45, steps per second: 199, episode reward: 13.632, mean reward: 0.303 [0.201, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.268, 10.436], loss: 0.002490, mae: 0.049326, mean_q: -0.179897
 86027/100000: episode: 1786, duration: 0.095s, episode steps: 20, steps per second: 211, episode reward: 5.082, mean reward: 0.254 [0.157, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.381], loss: 0.002806, mae: 0.052668, mean_q: -0.175142
 86090/100000: episode: 1787, duration: 0.326s, episode steps: 63, steps per second: 194, episode reward: 13.109, mean reward: 0.208 [0.063, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.790 [-0.536, 10.100], loss: 0.002995, mae: 0.054766, mean_q: -0.154353
 86101/100000: episode: 1788, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 3.306, mean reward: 0.301 [0.242, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.273, 10.100], loss: 0.002226, mae: 0.047095, mean_q: -0.187703
 86112/100000: episode: 1789, duration: 0.057s, episode steps: 11, steps per second: 191, episode reward: 4.545, mean reward: 0.413 [0.358, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.441, 10.100], loss: 0.002514, mae: 0.048539, mean_q: -0.177601
 86132/100000: episode: 1790, duration: 0.104s, episode steps: 20, steps per second: 191, episode reward: 6.167, mean reward: 0.308 [0.221, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-1.582, 10.364], loss: 0.002807, mae: 0.052709, mean_q: -0.168282
 86161/100000: episode: 1791, duration: 0.166s, episode steps: 29, steps per second: 174, episode reward: 8.665, mean reward: 0.299 [0.132, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.035, 10.291], loss: 0.002613, mae: 0.050110, mean_q: -0.160739
 86224/100000: episode: 1792, duration: 0.332s, episode steps: 63, steps per second: 190, episode reward: 11.905, mean reward: 0.189 [0.049, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.791 [-1.136, 10.306], loss: 0.002623, mae: 0.051900, mean_q: -0.157785
 86241/100000: episode: 1793, duration: 0.103s, episode steps: 17, steps per second: 166, episode reward: 5.940, mean reward: 0.349 [0.269, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.151, 10.529], loss: 0.002747, mae: 0.052113, mean_q: -0.097369
 86304/100000: episode: 1794, duration: 0.320s, episode steps: 63, steps per second: 197, episode reward: 19.544, mean reward: 0.310 [0.142, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.793 [-0.924, 10.564], loss: 0.002773, mae: 0.053359, mean_q: -0.147155
 86315/100000: episode: 1795, duration: 0.054s, episode steps: 11, steps per second: 204, episode reward: 4.374, mean reward: 0.398 [0.370, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.264, 10.100], loss: 0.002483, mae: 0.051377, mean_q: -0.099500
 86336/100000: episode: 1796, duration: 0.111s, episode steps: 21, steps per second: 190, episode reward: 10.907, mean reward: 0.519 [0.285, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.456, 10.100], loss: 0.002756, mae: 0.052913, mean_q: -0.148449
 86365/100000: episode: 1797, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 7.727, mean reward: 0.266 [0.116, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.643, 10.265], loss: 0.002545, mae: 0.051130, mean_q: -0.150830
 86386/100000: episode: 1798, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 6.964, mean reward: 0.332 [0.254, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.221, 10.100], loss: 0.002637, mae: 0.052760, mean_q: -0.079910
 86415/100000: episode: 1799, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 11.019, mean reward: 0.380 [0.263, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.075, 10.507], loss: 0.002721, mae: 0.052189, mean_q: -0.143224
 86436/100000: episode: 1800, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 6.878, mean reward: 0.328 [0.186, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.252, 10.563], loss: 0.002598, mae: 0.051324, mean_q: -0.138431
 86477/100000: episode: 1801, duration: 0.212s, episode steps: 41, steps per second: 194, episode reward: 10.918, mean reward: 0.266 [0.132, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.398, 10.293], loss: 0.002903, mae: 0.054491, mean_q: -0.089814
 86497/100000: episode: 1802, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 6.448, mean reward: 0.322 [0.213, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-1.190, 10.364], loss: 0.002576, mae: 0.050863, mean_q: -0.115863
 86508/100000: episode: 1803, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 4.443, mean reward: 0.404 [0.347, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.439, 10.100], loss: 0.002871, mae: 0.054378, mean_q: -0.034723
 86549/100000: episode: 1804, duration: 0.203s, episode steps: 41, steps per second: 202, episode reward: 10.123, mean reward: 0.247 [0.120, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-1.181, 10.264], loss: 0.002600, mae: 0.052835, mean_q: -0.052027
 86578/100000: episode: 1805, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 10.307, mean reward: 0.355 [0.241, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.465, 10.361], loss: 0.002449, mae: 0.050784, mean_q: -0.022065
 86598/100000: episode: 1806, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 5.325, mean reward: 0.266 [0.192, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.216, 10.366], loss: 0.002436, mae: 0.050815, mean_q: -0.083331
 86627/100000: episode: 1807, duration: 0.146s, episode steps: 29, steps per second: 198, episode reward: 7.015, mean reward: 0.242 [0.148, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.165, 10.262], loss: 0.002500, mae: 0.051979, mean_q: -0.040832
 86648/100000: episode: 1808, duration: 0.104s, episode steps: 21, steps per second: 202, episode reward: 5.697, mean reward: 0.271 [0.158, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.106, 10.312], loss: 0.002687, mae: 0.052771, mean_q: -0.093536
 86693/100000: episode: 1809, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 9.873, mean reward: 0.219 [0.020, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.777, 10.292], loss: 0.002620, mae: 0.053227, mean_q: -0.034233
 86756/100000: episode: 1810, duration: 0.353s, episode steps: 63, steps per second: 178, episode reward: 14.614, mean reward: 0.232 [0.061, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.791 [-0.260, 10.100], loss: 0.002613, mae: 0.052571, mean_q: -0.084645
 86776/100000: episode: 1811, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 7.300, mean reward: 0.365 [0.260, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.292, 10.516], loss: 0.002461, mae: 0.050218, mean_q: -0.115830
 86787/100000: episode: 1812, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 3.861, mean reward: 0.351 [0.261, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.258, 10.100], loss: 0.002820, mae: 0.054476, mean_q: -0.028886
 86832/100000: episode: 1813, duration: 0.222s, episode steps: 45, steps per second: 202, episode reward: 11.891, mean reward: 0.264 [0.116, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-1.417, 10.192], loss: 0.002946, mae: 0.055812, mean_q: -0.008492
 86861/100000: episode: 1814, duration: 0.140s, episode steps: 29, steps per second: 207, episode reward: 10.859, mean reward: 0.374 [0.178, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.413], loss: 0.002579, mae: 0.052282, mean_q: -0.001161
 86882/100000: episode: 1815, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 7.944, mean reward: 0.378 [0.300, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.261, 10.553], loss: 0.003118, mae: 0.055589, mean_q: 0.014099
 86903/100000: episode: 1816, duration: 0.106s, episode steps: 21, steps per second: 199, episode reward: 8.032, mean reward: 0.382 [0.308, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.596, 10.100], loss: 0.002505, mae: 0.049480, mean_q: -0.047123
 86948/100000: episode: 1817, duration: 0.230s, episode steps: 45, steps per second: 196, episode reward: 12.356, mean reward: 0.275 [0.149, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.279, 10.284], loss: 0.002658, mae: 0.052783, mean_q: -0.050007
 87006/100000: episode: 1818, duration: 0.302s, episode steps: 58, steps per second: 192, episode reward: 17.855, mean reward: 0.308 [0.110, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.833 [-0.614, 10.271], loss: 0.002678, mae: 0.053364, mean_q: -0.018082
 87027/100000: episode: 1819, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 7.982, mean reward: 0.380 [0.193, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.352, 10.575], loss: 0.002578, mae: 0.052047, mean_q: -0.027722
 87038/100000: episode: 1820, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 4.222, mean reward: 0.384 [0.320, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.225, 10.100], loss: 0.002673, mae: 0.055263, mean_q: 0.089363
 87096/100000: episode: 1821, duration: 0.303s, episode steps: 58, steps per second: 191, episode reward: 10.867, mean reward: 0.187 [0.012, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.826 [-0.827, 10.174], loss: 0.002541, mae: 0.051977, mean_q: -0.015128
 87117/100000: episode: 1822, duration: 0.102s, episode steps: 21, steps per second: 206, episode reward: 9.382, mean reward: 0.447 [0.296, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.688, 10.100], loss: 0.002972, mae: 0.055894, mean_q: 0.083514
 87146/100000: episode: 1823, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 6.780, mean reward: 0.234 [0.080, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.035, 10.243], loss: 0.002484, mae: 0.051479, mean_q: -0.017539
 87157/100000: episode: 1824, duration: 0.076s, episode steps: 11, steps per second: 145, episode reward: 4.535, mean reward: 0.412 [0.349, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.375, 10.100], loss: 0.002792, mae: 0.055283, mean_q: 0.121575
 87202/100000: episode: 1825, duration: 0.235s, episode steps: 45, steps per second: 192, episode reward: 13.544, mean reward: 0.301 [0.100, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.272, 10.271], loss: 0.003351, mae: 0.058125, mean_q: 0.088025
 87243/100000: episode: 1826, duration: 0.201s, episode steps: 41, steps per second: 204, episode reward: 7.272, mean reward: 0.177 [0.051, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.104, 10.157], loss: 0.005142, mae: 0.074248, mean_q: 0.053676
 87264/100000: episode: 1827, duration: 0.111s, episode steps: 21, steps per second: 188, episode reward: 5.452, mean reward: 0.260 [0.161, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-1.330, 10.379], loss: 0.003060, mae: 0.057988, mean_q: 0.044086
 87309/100000: episode: 1828, duration: 0.228s, episode steps: 45, steps per second: 197, episode reward: 11.088, mean reward: 0.246 [0.043, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.627, 10.230], loss: 0.003069, mae: 0.059739, mean_q: 0.033624
 87338/100000: episode: 1829, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 6.925, mean reward: 0.239 [0.055, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.211, 10.127], loss: 0.002753, mae: 0.055495, mean_q: 0.008358
 87358/100000: episode: 1830, duration: 0.095s, episode steps: 20, steps per second: 210, episode reward: 6.277, mean reward: 0.314 [0.239, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.711, 10.449], loss: 0.003010, mae: 0.059142, mean_q: 0.067426
 87387/100000: episode: 1831, duration: 0.162s, episode steps: 29, steps per second: 178, episode reward: 5.039, mean reward: 0.174 [0.052, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.414, 10.100], loss: 0.002537, mae: 0.052218, mean_q: 0.044415
 87445/100000: episode: 1832, duration: 0.290s, episode steps: 58, steps per second: 200, episode reward: 22.033, mean reward: 0.380 [0.232, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.834 [-0.633, 10.421], loss: 0.002697, mae: 0.053067, mean_q: 0.000187
 87508/100000: episode: 1833, duration: 0.329s, episode steps: 63, steps per second: 192, episode reward: 13.271, mean reward: 0.211 [0.039, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.784 [-0.680, 10.156], loss: 0.002780, mae: 0.054460, mean_q: 0.077120
 87528/100000: episode: 1834, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 7.341, mean reward: 0.367 [0.283, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-1.146, 10.430], loss: 0.002368, mae: 0.049691, mean_q: -0.004922
 87569/100000: episode: 1835, duration: 0.227s, episode steps: 41, steps per second: 181, episode reward: 8.904, mean reward: 0.217 [0.034, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.805, 10.140], loss: 0.002483, mae: 0.052765, mean_q: 0.106121
 87614/100000: episode: 1836, duration: 0.224s, episode steps: 45, steps per second: 201, episode reward: 12.993, mean reward: 0.289 [0.158, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.263, 10.272], loss: 0.002759, mae: 0.056044, mean_q: 0.126016
 87635/100000: episode: 1837, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 5.474, mean reward: 0.261 [0.187, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.489, 10.352], loss: 0.002664, mae: 0.051539, mean_q: 0.123722
 87656/100000: episode: 1838, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 8.152, mean reward: 0.388 [0.301, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.414, 10.100], loss: 0.002625, mae: 0.053946, mean_q: 0.109745
 87719/100000: episode: 1839, duration: 0.341s, episode steps: 63, steps per second: 185, episode reward: 14.786, mean reward: 0.235 [0.063, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.797 [-0.546, 10.185], loss: 0.002819, mae: 0.055700, mean_q: 0.095664
 87760/100000: episode: 1840, duration: 0.196s, episode steps: 41, steps per second: 210, episode reward: 9.856, mean reward: 0.240 [0.022, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.299, 10.104], loss: 0.002382, mae: 0.052804, mean_q: 0.139642
 87771/100000: episode: 1841, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 4.176, mean reward: 0.380 [0.304, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.423, 10.100], loss: 0.002864, mae: 0.055431, mean_q: 0.029362
 87782/100000: episode: 1842, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 3.989, mean reward: 0.363 [0.304, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.569, 10.100], loss: 0.003134, mae: 0.055554, mean_q: 0.060914
 87840/100000: episode: 1843, duration: 0.288s, episode steps: 58, steps per second: 201, episode reward: 12.773, mean reward: 0.220 [0.026, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.834 [-0.315, 10.293], loss: 0.002828, mae: 0.055702, mean_q: 0.126980
 87861/100000: episode: 1844, duration: 0.102s, episode steps: 21, steps per second: 205, episode reward: 7.536, mean reward: 0.359 [0.267, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.452], loss: 0.002825, mae: 0.055428, mean_q: 0.147363
 87881/100000: episode: 1845, duration: 0.099s, episode steps: 20, steps per second: 203, episode reward: 8.249, mean reward: 0.412 [0.319, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.760, 10.484], loss: 0.002866, mae: 0.054085, mean_q: 0.127362
 87926/100000: episode: 1846, duration: 0.223s, episode steps: 45, steps per second: 202, episode reward: 9.493, mean reward: 0.211 [0.077, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.270, 10.344], loss: 0.002719, mae: 0.053822, mean_q: 0.148414
 87967/100000: episode: 1847, duration: 0.213s, episode steps: 41, steps per second: 192, episode reward: 7.148, mean reward: 0.174 [0.029, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.261, 10.295], loss: 0.002427, mae: 0.051250, mean_q: 0.141864
 87984/100000: episode: 1848, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 4.617, mean reward: 0.272 [0.212, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.226, 10.298], loss: 0.003192, mae: 0.059289, mean_q: 0.221813
 88025/100000: episode: 1849, duration: 0.229s, episode steps: 41, steps per second: 179, episode reward: 6.310, mean reward: 0.154 [0.033, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.035, 10.124], loss: 0.002864, mae: 0.055859, mean_q: 0.187211
 88070/100000: episode: 1850, duration: 0.220s, episode steps: 45, steps per second: 204, episode reward: 12.904, mean reward: 0.287 [0.173, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.492, 10.295], loss: 0.002622, mae: 0.054596, mean_q: 0.162228
 88133/100000: episode: 1851, duration: 0.310s, episode steps: 63, steps per second: 203, episode reward: 16.110, mean reward: 0.256 [0.046, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.790 [-0.914, 10.303], loss: 0.002901, mae: 0.057396, mean_q: 0.196997
 88150/100000: episode: 1852, duration: 0.082s, episode steps: 17, steps per second: 208, episode reward: 3.922, mean reward: 0.231 [0.088, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.675, 10.216], loss: 0.002997, mae: 0.058323, mean_q: 0.211503
 88208/100000: episode: 1853, duration: 0.275s, episode steps: 58, steps per second: 211, episode reward: 13.401, mean reward: 0.231 [0.083, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.834 [-0.664, 10.261], loss: 0.002930, mae: 0.057212, mean_q: 0.188040
 88225/100000: episode: 1854, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 5.627, mean reward: 0.331 [0.205, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.411], loss: 0.002384, mae: 0.052547, mean_q: 0.265689
 88242/100000: episode: 1855, duration: 0.081s, episode steps: 17, steps per second: 210, episode reward: 5.699, mean reward: 0.335 [0.264, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-1.746, 10.339], loss: 0.002406, mae: 0.052401, mean_q: 0.195775
 88300/100000: episode: 1856, duration: 0.289s, episode steps: 58, steps per second: 201, episode reward: 18.425, mean reward: 0.318 [0.116, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.831 [-0.507, 10.396], loss: 0.002584, mae: 0.054130, mean_q: 0.210445
[Info] 200-TH LEVEL FOUND: 0.8666355609893799, Considering 10/90 traces
 88311/100000: episode: 1857, duration: 3.999s, episode steps: 11, steps per second: 3, episode reward: 3.773, mean reward: 0.343 [0.249, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.333, 10.100], loss: 0.003615, mae: 0.064188, mean_q: 0.210997
 88316/100000: episode: 1858, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: 2.083, mean reward: 0.417 [0.330, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.493, 10.100], loss: 0.003167, mae: 0.060705, mean_q: 0.286312
 88334/100000: episode: 1859, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 9.240, mean reward: 0.513 [0.451, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.461, 10.100], loss: 0.003330, mae: 0.060783, mean_q: 0.228806
 88352/100000: episode: 1860, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 7.870, mean reward: 0.437 [0.354, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.327, 10.100], loss: 0.002839, mae: 0.055727, mean_q: 0.219304
 88370/100000: episode: 1861, duration: 0.106s, episode steps: 18, steps per second: 171, episode reward: 9.031, mean reward: 0.502 [0.438, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.117, 10.100], loss: 0.002458, mae: 0.051387, mean_q: 0.123013
 88384/100000: episode: 1862, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 5.667, mean reward: 0.405 [0.353, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.344, 10.100], loss: 0.003007, mae: 0.056818, mean_q: 0.188256
 88390/100000: episode: 1863, duration: 0.031s, episode steps: 6, steps per second: 193, episode reward: 2.178, mean reward: 0.363 [0.333, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.478, 10.100], loss: 0.002976, mae: 0.057453, mean_q: 0.232353
 88404/100000: episode: 1864, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 6.692, mean reward: 0.478 [0.428, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.343, 10.100], loss: 0.003100, mae: 0.060522, mean_q: 0.214263
 88423/100000: episode: 1865, duration: 0.092s, episode steps: 19, steps per second: 206, episode reward: 6.861, mean reward: 0.361 [0.241, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.186, 10.100], loss: 0.002523, mae: 0.053517, mean_q: 0.219664
 88441/100000: episode: 1866, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 7.253, mean reward: 0.403 [0.316, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.348, 10.100], loss: 0.002808, mae: 0.056008, mean_q: 0.260332
 88445/100000: episode: 1867, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 1.830, mean reward: 0.458 [0.447, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.248, 10.555], loss: 0.002295, mae: 0.052740, mean_q: 0.262319
 88451/100000: episode: 1868, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 2.561, mean reward: 0.427 [0.378, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.493, 10.100], loss: 0.003396, mae: 0.064464, mean_q: 0.301554
 88465/100000: episode: 1869, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 5.177, mean reward: 0.370 [0.314, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.314, 10.100], loss: 0.003011, mae: 0.058315, mean_q: 0.261681
 88471/100000: episode: 1870, duration: 0.043s, episode steps: 6, steps per second: 138, episode reward: 2.695, mean reward: 0.449 [0.403, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.488, 10.100], loss: 0.002888, mae: 0.055194, mean_q: 0.204067
 88490/100000: episode: 1871, duration: 0.090s, episode steps: 19, steps per second: 212, episode reward: 9.094, mean reward: 0.479 [0.351, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.907, 10.100], loss: 0.002805, mae: 0.055631, mean_q: 0.208338
 88496/100000: episode: 1872, duration: 0.031s, episode steps: 6, steps per second: 194, episode reward: 2.604, mean reward: 0.434 [0.415, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.380, 10.100], loss: 0.002303, mae: 0.052608, mean_q: 0.310329
 88500/100000: episode: 1873, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 1.800, mean reward: 0.450 [0.395, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.035, 10.555], loss: 0.002181, mae: 0.050902, mean_q: 0.282297
 88536/100000: episode: 1874, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 12.900, mean reward: 0.358 [0.235, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.012, 10.390], loss: 0.003148, mae: 0.060406, mean_q: 0.268799
 88572/100000: episode: 1875, duration: 0.177s, episode steps: 36, steps per second: 204, episode reward: 15.497, mean reward: 0.430 [0.337, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.035, 10.474], loss: 0.002777, mae: 0.057031, mean_q: 0.285883
 88590/100000: episode: 1876, duration: 0.097s, episode steps: 18, steps per second: 187, episode reward: 8.609, mean reward: 0.478 [0.406, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.418, 10.100], loss: 0.002829, mae: 0.056432, mean_q: 0.292300
 88604/100000: episode: 1877, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 5.268, mean reward: 0.376 [0.314, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.473, 10.100], loss: 0.002523, mae: 0.053995, mean_q: 0.183464
 88618/100000: episode: 1878, duration: 0.072s, episode steps: 14, steps per second: 193, episode reward: 6.249, mean reward: 0.446 [0.364, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.405, 10.100], loss: 0.003225, mae: 0.060375, mean_q: 0.330304
 88636/100000: episode: 1879, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 8.312, mean reward: 0.462 [0.397, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.376, 10.100], loss: 0.003374, mae: 0.060833, mean_q: 0.294286
[Info] FALSIFICATION!
 88650/100000: episode: 1880, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 16.649, mean reward: 1.189 [0.387, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.173, 10.046], loss: 0.002703, mae: 0.056166, mean_q: 0.302779
 88750/100000: episode: 1881, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.742, mean reward: -0.167 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.782, 10.170], loss: 0.016011, mae: 0.060796, mean_q: 0.275787
 88850/100000: episode: 1882, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.146, mean reward: -0.191 [-1.000, 0.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.919, 10.197], loss: 0.021120, mae: 0.088916, mean_q: 0.296235
 88950/100000: episode: 1883, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -18.814, mean reward: -0.188 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.872, 10.103], loss: 0.003413, mae: 0.059704, mean_q: 0.261841
 89050/100000: episode: 1884, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -10.942, mean reward: -0.109 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.306, 10.438], loss: 0.030122, mae: 0.072353, mean_q: 0.302448
 89150/100000: episode: 1885, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -14.302, mean reward: -0.143 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.778, 10.481], loss: 0.003120, mae: 0.057952, mean_q: 0.294982
 89250/100000: episode: 1886, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -18.151, mean reward: -0.182 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.396, 10.098], loss: 0.016737, mae: 0.066292, mean_q: 0.316444
 89350/100000: episode: 1887, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -11.990, mean reward: -0.120 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.002, 10.098], loss: 0.003327, mae: 0.059426, mean_q: 0.291899
 89450/100000: episode: 1888, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -13.191, mean reward: -0.132 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.468, 10.194], loss: 0.003499, mae: 0.060866, mean_q: 0.288711
 89550/100000: episode: 1889, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -14.596, mean reward: -0.146 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.290, 10.169], loss: 0.016619, mae: 0.065700, mean_q: 0.306115
 89650/100000: episode: 1890, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.674, mean reward: -0.177 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.122, 10.288], loss: 0.016577, mae: 0.066030, mean_q: 0.297914
 89750/100000: episode: 1891, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -19.317, mean reward: -0.193 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.660, 10.098], loss: 0.003207, mae: 0.058397, mean_q: 0.290217
 89850/100000: episode: 1892, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -8.280, mean reward: -0.083 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.554, 10.098], loss: 0.031588, mae: 0.082191, mean_q: 0.288116
 89950/100000: episode: 1893, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -13.398, mean reward: -0.134 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.589, 10.316], loss: 0.016750, mae: 0.068959, mean_q: 0.303939
 90050/100000: episode: 1894, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.808, mean reward: -0.168 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.913, 10.098], loss: 0.003701, mae: 0.062200, mean_q: 0.282641
 90150/100000: episode: 1895, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -10.255, mean reward: -0.103 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.837, 10.098], loss: 0.003316, mae: 0.060054, mean_q: 0.302420
 90250/100000: episode: 1896, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -17.080, mean reward: -0.171 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.655, 10.098], loss: 0.029383, mae: 0.069117, mean_q: 0.267133
 90350/100000: episode: 1897, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -18.728, mean reward: -0.187 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.469, 10.098], loss: 0.003269, mae: 0.059857, mean_q: 0.255087
 90450/100000: episode: 1898, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.426, mean reward: -0.184 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.398, 10.098], loss: 0.003748, mae: 0.063642, mean_q: 0.248495
 90550/100000: episode: 1899, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.484, mean reward: -0.185 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.735, 10.194], loss: 0.003518, mae: 0.061232, mean_q: 0.225037
 90650/100000: episode: 1900, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -14.020, mean reward: -0.140 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.069, 10.098], loss: 0.016679, mae: 0.065729, mean_q: 0.200877
 90750/100000: episode: 1901, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -15.629, mean reward: -0.156 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.214, 10.442], loss: 0.030025, mae: 0.076253, mean_q: 0.233000
 90850/100000: episode: 1902, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.977, mean reward: -0.190 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.000, 10.098], loss: 0.003320, mae: 0.058594, mean_q: 0.203573
 90950/100000: episode: 1903, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.453, mean reward: -0.175 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.362, 10.098], loss: 0.029578, mae: 0.069788, mean_q: 0.177403
 91050/100000: episode: 1904, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: -14.382, mean reward: -0.144 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.826, 10.118], loss: 0.043084, mae: 0.079122, mean_q: 0.178524
 91150/100000: episode: 1905, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.348, mean reward: -0.193 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.114, 10.254], loss: 0.003646, mae: 0.060621, mean_q: 0.132572
 91250/100000: episode: 1906, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.096, mean reward: -0.191 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.288, 10.098], loss: 0.003123, mae: 0.055927, mean_q: 0.118575
 91350/100000: episode: 1907, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -19.172, mean reward: -0.192 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.884, 10.098], loss: 0.002977, mae: 0.055598, mean_q: 0.111247
 91450/100000: episode: 1908, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.020, mean reward: -0.170 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.001, 10.335], loss: 0.016354, mae: 0.062667, mean_q: 0.080560
 91550/100000: episode: 1909, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.079, mean reward: -0.181 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.955, 10.098], loss: 0.021993, mae: 0.080691, mean_q: 0.071824
 91650/100000: episode: 1910, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.118, mean reward: -0.171 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.724, 10.390], loss: 0.042619, mae: 0.079545, mean_q: 0.028202
 91750/100000: episode: 1911, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -12.157, mean reward: -0.122 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.682, 10.098], loss: 0.003880, mae: 0.063467, mean_q: 0.046936
 91850/100000: episode: 1912, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.386, mean reward: -0.164 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.296, 10.189], loss: 0.003062, mae: 0.056111, mean_q: 0.017546
 91950/100000: episode: 1913, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -11.805, mean reward: -0.118 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.309, 10.407], loss: 0.002929, mae: 0.054657, mean_q: -0.022305
 92050/100000: episode: 1914, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.914, mean reward: -0.169 [-1.000, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.034, 10.313], loss: 0.003101, mae: 0.055551, mean_q: -0.000420
 92150/100000: episode: 1915, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -19.259, mean reward: -0.193 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.410, 10.145], loss: 0.029694, mae: 0.068108, mean_q: -0.049231
 92250/100000: episode: 1916, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.945, mean reward: -0.189 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.549, 10.233], loss: 0.002972, mae: 0.053678, mean_q: -0.056216
 92350/100000: episode: 1917, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.500, mean reward: -0.195 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.643, 10.201], loss: 0.016060, mae: 0.059047, mean_q: -0.066410
 92450/100000: episode: 1918, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -20.145, mean reward: -0.201 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.366, 10.212], loss: 0.002818, mae: 0.052497, mean_q: -0.093195
 92550/100000: episode: 1919, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.599, mean reward: -0.166 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.062, 10.098], loss: 0.002587, mae: 0.049947, mean_q: -0.155956
 92650/100000: episode: 1920, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -15.581, mean reward: -0.156 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.118, 10.098], loss: 0.002878, mae: 0.051746, mean_q: -0.132549
 92750/100000: episode: 1921, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.552, mean reward: -0.146 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.000, 10.098], loss: 0.002708, mae: 0.051200, mean_q: -0.141089
 92850/100000: episode: 1922, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.891, mean reward: -0.189 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.584, 10.215], loss: 0.002611, mae: 0.050590, mean_q: -0.155756
 92950/100000: episode: 1923, duration: 0.468s, episode steps: 100, steps per second: 214, episode reward: -12.994, mean reward: -0.130 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.064, 10.098], loss: 0.002792, mae: 0.052537, mean_q: -0.154807
 93050/100000: episode: 1924, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.615, mean reward: -0.196 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.410, 10.098], loss: 0.015867, mae: 0.055056, mean_q: -0.192582
 93150/100000: episode: 1925, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.096, mean reward: -0.171 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.617, 10.098], loss: 0.002577, mae: 0.048816, mean_q: -0.239705
 93250/100000: episode: 1926, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -15.439, mean reward: -0.154 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.019, 10.098], loss: 0.002601, mae: 0.050023, mean_q: -0.246221
 93350/100000: episode: 1927, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.291, mean reward: -0.163 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.682, 10.098], loss: 0.028889, mae: 0.059985, mean_q: -0.244353
 93450/100000: episode: 1928, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -13.375, mean reward: -0.134 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.728, 10.237], loss: 0.003076, mae: 0.053659, mean_q: -0.276411
 93550/100000: episode: 1929, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -19.140, mean reward: -0.191 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.184, 10.120], loss: 0.015606, mae: 0.053532, mean_q: -0.327328
 93650/100000: episode: 1930, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -15.402, mean reward: -0.154 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.095, 10.265], loss: 0.002450, mae: 0.049426, mean_q: -0.319214
 93750/100000: episode: 1931, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -15.780, mean reward: -0.158 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.572, 10.291], loss: 0.002222, mae: 0.045336, mean_q: -0.292260
 93850/100000: episode: 1932, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.987, mean reward: -0.180 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.630, 10.133], loss: 0.002257, mae: 0.046127, mean_q: -0.314820
 93950/100000: episode: 1933, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -19.038, mean reward: -0.190 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.972, 10.247], loss: 0.002235, mae: 0.045594, mean_q: -0.288601
 94050/100000: episode: 1934, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.211, mean reward: -0.162 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.787, 10.206], loss: 0.002443, mae: 0.047836, mean_q: -0.283877
 94150/100000: episode: 1935, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.510, mean reward: -0.175 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.273, 10.124], loss: 0.002346, mae: 0.047646, mean_q: -0.314778
 94250/100000: episode: 1936, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -17.961, mean reward: -0.180 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.232, 10.173], loss: 0.002261, mae: 0.045473, mean_q: -0.351559
 94350/100000: episode: 1937, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.197, mean reward: -0.182 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.245, 10.098], loss: 0.002320, mae: 0.047006, mean_q: -0.296341
 94450/100000: episode: 1938, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.376, mean reward: -0.184 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.743, 10.198], loss: 0.002330, mae: 0.047177, mean_q: -0.316995
 94550/100000: episode: 1939, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.494, mean reward: -0.165 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.167, 10.252], loss: 0.002572, mae: 0.049067, mean_q: -0.289728
 94650/100000: episode: 1940, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.907, mean reward: -0.179 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.636, 10.159], loss: 0.002287, mae: 0.046987, mean_q: -0.316943
 94750/100000: episode: 1941, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.822, mean reward: -0.198 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.417, 10.098], loss: 0.002374, mae: 0.047815, mean_q: -0.306911
 94850/100000: episode: 1942, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.082, mean reward: -0.181 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.987, 10.224], loss: 0.002230, mae: 0.045982, mean_q: -0.334877
 94950/100000: episode: 1943, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.473, mean reward: -0.185 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.609, 10.098], loss: 0.002366, mae: 0.047693, mean_q: -0.273275
 95050/100000: episode: 1944, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -15.580, mean reward: -0.156 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.411, 10.307], loss: 0.002305, mae: 0.046821, mean_q: -0.284209
 95150/100000: episode: 1945, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -11.283, mean reward: -0.113 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.703, 10.347], loss: 0.002211, mae: 0.045958, mean_q: -0.314484
 95250/100000: episode: 1946, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -19.775, mean reward: -0.198 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.039, 10.198], loss: 0.002237, mae: 0.045496, mean_q: -0.338279
 95350/100000: episode: 1947, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -19.422, mean reward: -0.194 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.197, 10.098], loss: 0.002206, mae: 0.045775, mean_q: -0.320861
 95450/100000: episode: 1948, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -14.834, mean reward: -0.148 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.258, 10.098], loss: 0.002204, mae: 0.046829, mean_q: -0.295893
 95550/100000: episode: 1949, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.241, mean reward: -0.182 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.896, 10.098], loss: 0.002232, mae: 0.045546, mean_q: -0.322181
 95650/100000: episode: 1950, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.713, mean reward: -0.177 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.500, 10.114], loss: 0.002307, mae: 0.047002, mean_q: -0.300583
 95750/100000: episode: 1951, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.048, mean reward: -0.180 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.891, 10.216], loss: 0.002333, mae: 0.048299, mean_q: -0.307261
 95850/100000: episode: 1952, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -15.776, mean reward: -0.158 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.319, 10.202], loss: 0.002343, mae: 0.049312, mean_q: -0.326572
 95950/100000: episode: 1953, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -11.631, mean reward: -0.116 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.749, 10.098], loss: 0.002142, mae: 0.045470, mean_q: -0.312474
 96050/100000: episode: 1954, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -15.796, mean reward: -0.158 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.413, 10.098], loss: 0.002189, mae: 0.046024, mean_q: -0.289494
 96150/100000: episode: 1955, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -13.645, mean reward: -0.136 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.639, 10.098], loss: 0.002254, mae: 0.047098, mean_q: -0.305716
 96250/100000: episode: 1956, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.238, mean reward: -0.172 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.547, 10.159], loss: 0.006162, mae: 0.068775, mean_q: -0.353605
 96350/100000: episode: 1957, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.932, mean reward: -0.179 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.914, 10.098], loss: 0.005509, mae: 0.063741, mean_q: -0.303858
 96450/100000: episode: 1958, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.876, mean reward: -0.189 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.246, 10.160], loss: 0.002507, mae: 0.049978, mean_q: -0.299158
 96550/100000: episode: 1959, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.693, mean reward: -0.177 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.390, 10.098], loss: 0.002360, mae: 0.047409, mean_q: -0.332500
 96650/100000: episode: 1960, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.806, mean reward: -0.148 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.405, 10.098], loss: 0.002351, mae: 0.047216, mean_q: -0.306950
 96750/100000: episode: 1961, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.487, mean reward: -0.185 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.715, 10.098], loss: 0.002355, mae: 0.047107, mean_q: -0.285552
 96850/100000: episode: 1962, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -19.190, mean reward: -0.192 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.530, 10.183], loss: 0.002257, mae: 0.046260, mean_q: -0.307128
 96950/100000: episode: 1963, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.154, mean reward: -0.182 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.668, 10.098], loss: 0.002268, mae: 0.046568, mean_q: -0.284464
 97050/100000: episode: 1964, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.329, mean reward: -0.173 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.356, 10.215], loss: 0.002234, mae: 0.045892, mean_q: -0.349914
 97150/100000: episode: 1965, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.716, mean reward: -0.187 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.971, 10.123], loss: 0.002263, mae: 0.046079, mean_q: -0.321461
 97250/100000: episode: 1966, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.307, mean reward: -0.153 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.733, 10.098], loss: 0.002311, mae: 0.046588, mean_q: -0.338263
 97350/100000: episode: 1967, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -14.132, mean reward: -0.141 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.462, 10.309], loss: 0.002537, mae: 0.049115, mean_q: -0.296331
 97450/100000: episode: 1968, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -13.633, mean reward: -0.136 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.815, 10.300], loss: 0.002464, mae: 0.048603, mean_q: -0.337000
 97550/100000: episode: 1969, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -18.113, mean reward: -0.181 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.654, 10.327], loss: 0.002373, mae: 0.046861, mean_q: -0.315338
 97650/100000: episode: 1970, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -19.608, mean reward: -0.196 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.171, 10.203], loss: 0.002356, mae: 0.045918, mean_q: -0.358571
 97750/100000: episode: 1971, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.225, mean reward: -0.182 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.477, 10.098], loss: 0.002334, mae: 0.047000, mean_q: -0.294786
 97850/100000: episode: 1972, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -15.314, mean reward: -0.153 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.662, 10.164], loss: 0.002346, mae: 0.048048, mean_q: -0.321902
 97950/100000: episode: 1973, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.219, mean reward: -0.172 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.991, 10.214], loss: 0.002371, mae: 0.046967, mean_q: -0.324751
 98050/100000: episode: 1974, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.817, mean reward: -0.178 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.203, 10.098], loss: 0.002392, mae: 0.047778, mean_q: -0.326168
 98150/100000: episode: 1975, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.797, mean reward: -0.198 [-1.000, 0.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.769, 10.193], loss: 0.002335, mae: 0.046382, mean_q: -0.346838
 98250/100000: episode: 1976, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -20.023, mean reward: -0.200 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.276, 10.145], loss: 0.002425, mae: 0.047245, mean_q: -0.326228
 98350/100000: episode: 1977, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.530, mean reward: -0.155 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.146, 10.098], loss: 0.002287, mae: 0.046532, mean_q: -0.324974
 98450/100000: episode: 1978, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.209, mean reward: -0.182 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.414, 10.367], loss: 0.002484, mae: 0.047723, mean_q: -0.326538
 98550/100000: episode: 1979, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.921, mean reward: -0.159 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.125, 10.309], loss: 0.002482, mae: 0.049053, mean_q: -0.326999
[Info] 100-TH LEVEL FOUND: 0.5847265720367432, Considering 10/90 traces
 98650/100000: episode: 1980, duration: 4.396s, episode steps: 100, steps per second: 23, episode reward: -20.306, mean reward: -0.203 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.600, 10.098], loss: 0.002474, mae: 0.048081, mean_q: -0.337529
 98679/100000: episode: 1981, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 6.150, mean reward: 0.212 [0.062, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.173, 10.239], loss: 0.002825, mae: 0.051730, mean_q: -0.279892
 98715/100000: episode: 1982, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 6.644, mean reward: 0.185 [0.027, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.127, 10.100], loss: 0.002614, mae: 0.049209, mean_q: -0.322114
 98761/100000: episode: 1983, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 14.318, mean reward: 0.311 [0.203, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.545, 10.100], loss: 0.002416, mae: 0.047537, mean_q: -0.353861
 98807/100000: episode: 1984, duration: 0.236s, episode steps: 46, steps per second: 195, episode reward: 14.078, mean reward: 0.306 [0.079, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.891, 10.100], loss: 0.002568, mae: 0.049235, mean_q: -0.304095
 98841/100000: episode: 1985, duration: 0.177s, episode steps: 34, steps per second: 193, episode reward: 10.385, mean reward: 0.305 [0.117, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.855, 10.237], loss: 0.002593, mae: 0.048796, mean_q: -0.305872
 98877/100000: episode: 1986, duration: 0.195s, episode steps: 36, steps per second: 185, episode reward: 9.526, mean reward: 0.265 [0.115, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.035, 10.270], loss: 0.002565, mae: 0.049567, mean_q: -0.302152
 98913/100000: episode: 1987, duration: 0.191s, episode steps: 36, steps per second: 189, episode reward: 8.297, mean reward: 0.230 [0.098, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.653, 10.176], loss: 0.002786, mae: 0.051357, mean_q: -0.302829
 98949/100000: episode: 1988, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 10.753, mean reward: 0.299 [0.103, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.760, 10.240], loss: 0.002434, mae: 0.048733, mean_q: -0.309305
 98963/100000: episode: 1989, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 4.038, mean reward: 0.288 [0.141, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.292, 10.100], loss: 0.002310, mae: 0.048630, mean_q: -0.207365
 99009/100000: episode: 1990, duration: 0.234s, episode steps: 46, steps per second: 197, episode reward: 8.620, mean reward: 0.187 [0.012, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-2.133, 10.185], loss: 0.002430, mae: 0.049053, mean_q: -0.240022
 99041/100000: episode: 1991, duration: 0.180s, episode steps: 32, steps per second: 178, episode reward: 8.307, mean reward: 0.260 [0.170, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.904, 10.100], loss: 0.002530, mae: 0.049732, mean_q: -0.261699
 99064/100000: episode: 1992, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 6.324, mean reward: 0.275 [0.196, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.350, 10.100], loss: 0.002832, mae: 0.052031, mean_q: -0.236043
 99093/100000: episode: 1993, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 7.956, mean reward: 0.274 [0.045, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.721, 10.195], loss: 0.002866, mae: 0.052564, mean_q: -0.248507
 99116/100000: episode: 1994, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 6.917, mean reward: 0.301 [0.222, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.453, 10.100], loss: 0.002463, mae: 0.049423, mean_q: -0.197723
 99139/100000: episode: 1995, duration: 0.111s, episode steps: 23, steps per second: 208, episode reward: 7.736, mean reward: 0.336 [0.234, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.281, 10.100], loss: 0.002860, mae: 0.053248, mean_q: -0.226938
 99175/100000: episode: 1996, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 6.821, mean reward: 0.189 [0.046, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.126, 10.101], loss: 0.002365, mae: 0.049000, mean_q: -0.257207
 99209/100000: episode: 1997, duration: 0.167s, episode steps: 34, steps per second: 204, episode reward: 10.670, mean reward: 0.314 [0.219, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.211, 10.434], loss: 0.002509, mae: 0.050062, mean_q: -0.245207
 99255/100000: episode: 1998, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 9.618, mean reward: 0.209 [0.063, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.254, 10.100], loss: 0.002474, mae: 0.050151, mean_q: -0.186811
 99287/100000: episode: 1999, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 11.016, mean reward: 0.344 [0.203, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.452, 10.100], loss: 0.002526, mae: 0.050689, mean_q: -0.195460
 99323/100000: episode: 2000, duration: 0.193s, episode steps: 36, steps per second: 186, episode reward: 12.222, mean reward: 0.340 [0.277, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.559, 10.508], loss: 0.002412, mae: 0.048008, mean_q: -0.246808
 99369/100000: episode: 2001, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 12.269, mean reward: 0.267 [0.078, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.947, 10.100], loss: 0.002863, mae: 0.053773, mean_q: -0.218177
 99415/100000: episode: 2002, duration: 0.226s, episode steps: 46, steps per second: 204, episode reward: 9.499, mean reward: 0.207 [0.006, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.226, 10.179], loss: 0.002581, mae: 0.050753, mean_q: -0.183363
 99461/100000: episode: 2003, duration: 0.234s, episode steps: 46, steps per second: 197, episode reward: 17.229, mean reward: 0.375 [0.198, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.655, 10.100], loss: 0.002493, mae: 0.049509, mean_q: -0.187459
 99497/100000: episode: 2004, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 9.380, mean reward: 0.261 [0.134, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.035, 10.259], loss: 0.002645, mae: 0.050858, mean_q: -0.183434
 99520/100000: episode: 2005, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 5.947, mean reward: 0.259 [0.207, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.612, 10.100], loss: 0.002593, mae: 0.053215, mean_q: -0.113252
 99552/100000: episode: 2006, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 9.305, mean reward: 0.291 [0.112, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.129, 10.100], loss: 0.002503, mae: 0.050968, mean_q: -0.173557
 99598/100000: episode: 2007, duration: 0.244s, episode steps: 46, steps per second: 188, episode reward: 12.914, mean reward: 0.281 [0.127, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.680, 10.100], loss: 0.002447, mae: 0.049444, mean_q: -0.171862
 99632/100000: episode: 2008, duration: 0.185s, episode steps: 34, steps per second: 183, episode reward: 11.925, mean reward: 0.351 [0.275, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.376, 10.461], loss: 0.002829, mae: 0.053973, mean_q: -0.112530
 99661/100000: episode: 2009, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 8.495, mean reward: 0.293 [0.173, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.088, 10.269], loss: 0.002361, mae: 0.048523, mean_q: -0.174055
 99677/100000: episode: 2010, duration: 0.076s, episode steps: 16, steps per second: 210, episode reward: 4.311, mean reward: 0.269 [0.107, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.204, 10.218], loss: 0.002697, mae: 0.050422, mean_q: -0.190986
 99711/100000: episode: 2011, duration: 0.157s, episode steps: 34, steps per second: 217, episode reward: 9.480, mean reward: 0.279 [0.090, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.170, 10.188], loss: 0.002895, mae: 0.052921, mean_q: -0.143366
 99747/100000: episode: 2012, duration: 0.188s, episode steps: 36, steps per second: 192, episode reward: 11.394, mean reward: 0.317 [0.204, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.035, 10.465], loss: 0.002594, mae: 0.052791, mean_q: -0.122051
 99781/100000: episode: 2013, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 9.124, mean reward: 0.268 [0.147, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.206, 10.273], loss: 0.002870, mae: 0.053479, mean_q: -0.116166
 99813/100000: episode: 2014, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 10.317, mean reward: 0.322 [0.219, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.398, 10.100], loss: 0.003066, mae: 0.055398, mean_q: -0.128193
 99845/100000: episode: 2015, duration: 0.186s, episode steps: 32, steps per second: 172, episode reward: 7.035, mean reward: 0.220 [0.090, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.802, 10.100], loss: 0.002599, mae: 0.051311, mean_q: -0.119050
 99881/100000: episode: 2016, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 7.192, mean reward: 0.200 [0.092, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.302, 10.136], loss: 0.002667, mae: 0.053143, mean_q: -0.089799
 99927/100000: episode: 2017, duration: 0.220s, episode steps: 46, steps per second: 209, episode reward: 9.542, mean reward: 0.207 [0.043, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.067, 10.167], loss: 0.003143, mae: 0.058778, mean_q: -0.058607
 99955/100000: episode: 2018, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 6.455, mean reward: 0.231 [0.108, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.225, 10.205], loss: 0.002320, mae: 0.048785, mean_q: -0.106049
 99987/100000: episode: 2019, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 5.754, mean reward: 0.180 [0.048, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.696, 10.100], loss: 0.002570, mae: 0.051447, mean_q: -0.107238
done, took 681.315 seconds
[Info] End Importance Splitting. Falsification occurred 5 times.
