Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.164s, episode steps: 100, steps per second: 610, episode reward: -9.749, mean reward: -0.097 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.798, 10.130], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.069s, episode steps: 100, steps per second: 1448, episode reward: -12.809, mean reward: -0.128 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.198, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.063s, episode steps: 100, steps per second: 1597, episode reward: -18.289, mean reward: -0.183 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.088, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.062s, episode steps: 100, steps per second: 1604, episode reward: -18.302, mean reward: -0.183 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.019, 10.113], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.062s, episode steps: 100, steps per second: 1606, episode reward: -18.247, mean reward: -0.182 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.651, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.170s, episode steps: 100, steps per second: 85, episode reward: -16.508, mean reward: -0.165 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.083, 10.098], loss: 0.074559, mae: 0.279221, mean_q: 0.217167
   700/100000: episode: 7, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -16.578, mean reward: -0.166 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.242, 10.098], loss: 0.025129, mae: 0.161153, mean_q: 0.028434
   800/100000: episode: 8, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.512, mean reward: -0.175 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.096, 10.259], loss: 0.018092, mae: 0.132690, mean_q: -0.084747
   900/100000: episode: 9, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.265, mean reward: -0.153 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.450, 10.334], loss: 0.016182, mae: 0.124178, mean_q: -0.161151
  1000/100000: episode: 10, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.465, mean reward: -0.165 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.178, 10.098], loss: 0.012324, mae: 0.106215, mean_q: -0.229666
  1100/100000: episode: 11, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -16.661, mean reward: -0.167 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.634, 10.098], loss: 0.010948, mae: 0.099453, mean_q: -0.273204
  1200/100000: episode: 12, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.194, mean reward: -0.182 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.360, 10.098], loss: 0.009844, mae: 0.091850, mean_q: -0.252264
  1300/100000: episode: 13, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -16.300, mean reward: -0.163 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.235, 10.365], loss: 0.008908, mae: 0.088103, mean_q: -0.333349
  1400/100000: episode: 14, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.278, mean reward: -0.173 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.273, 10.234], loss: 0.009035, mae: 0.088472, mean_q: -0.326133
  1500/100000: episode: 15, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.794, mean reward: -0.168 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.005, 10.098], loss: 0.007596, mae: 0.083264, mean_q: -0.315001
  1600/100000: episode: 16, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.659, mean reward: -0.187 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.488, 10.340], loss: 0.007893, mae: 0.082702, mean_q: -0.319899
  1700/100000: episode: 17, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -14.057, mean reward: -0.141 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.140, 10.284], loss: 0.007947, mae: 0.085158, mean_q: -0.315283
  1800/100000: episode: 18, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -15.786, mean reward: -0.158 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.774, 10.317], loss: 0.007224, mae: 0.080168, mean_q: -0.270711
  1900/100000: episode: 19, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -16.611, mean reward: -0.166 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.311, 10.098], loss: 0.007073, mae: 0.079164, mean_q: -0.330444
  2000/100000: episode: 20, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.035, mean reward: -0.150 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.708, 10.375], loss: 0.006715, mae: 0.077457, mean_q: -0.270846
  2100/100000: episode: 21, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -19.014, mean reward: -0.190 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.148, 10.098], loss: 0.006521, mae: 0.076357, mean_q: -0.334326
  2200/100000: episode: 22, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -20.364, mean reward: -0.204 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.446, 10.176], loss: 0.006590, mae: 0.077528, mean_q: -0.323272
  2300/100000: episode: 23, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.637, mean reward: -0.146 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.882, 10.098], loss: 0.005940, mae: 0.075692, mean_q: -0.305753
  2400/100000: episode: 24, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.535, mean reward: -0.165 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.045, 10.150], loss: 0.006534, mae: 0.076072, mean_q: -0.310062
  2500/100000: episode: 25, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.878, mean reward: -0.179 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.183, 10.098], loss: 0.005738, mae: 0.073412, mean_q: -0.345339
  2600/100000: episode: 26, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -16.879, mean reward: -0.169 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.626, 10.098], loss: 0.006241, mae: 0.074016, mean_q: -0.376104
  2700/100000: episode: 27, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.096, mean reward: -0.171 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.672, 10.147], loss: 0.005936, mae: 0.074277, mean_q: -0.307335
  2800/100000: episode: 28, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -15.659, mean reward: -0.157 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.785, 10.098], loss: 0.006000, mae: 0.076768, mean_q: -0.294139
  2900/100000: episode: 29, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.960, mean reward: -0.180 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.430, 10.228], loss: 0.005996, mae: 0.076635, mean_q: -0.298891
  3000/100000: episode: 30, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.519, mean reward: -0.155 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.924, 10.250], loss: 0.006479, mae: 0.077566, mean_q: -0.327247
  3100/100000: episode: 31, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: -11.227, mean reward: -0.112 [-1.000, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.916, 10.098], loss: 0.005105, mae: 0.070327, mean_q: -0.340295
  3200/100000: episode: 32, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -13.067, mean reward: -0.131 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.644, 10.440], loss: 0.005831, mae: 0.075421, mean_q: -0.334259
  3300/100000: episode: 33, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -16.381, mean reward: -0.164 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.706, 10.143], loss: 0.004943, mae: 0.070907, mean_q: -0.326574
  3400/100000: episode: 34, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -13.857, mean reward: -0.139 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.088, 10.327], loss: 0.005659, mae: 0.072405, mean_q: -0.349121
  3500/100000: episode: 35, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.234, mean reward: -0.172 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.910, 10.098], loss: 0.004883, mae: 0.070203, mean_q: -0.306640
  3600/100000: episode: 36, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.581, mean reward: -0.166 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.787, 10.098], loss: 0.005153, mae: 0.071377, mean_q: -0.321127
  3700/100000: episode: 37, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -20.107, mean reward: -0.201 [-1.000, 0.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.363, 10.098], loss: 0.005573, mae: 0.073001, mean_q: -0.305830
  3800/100000: episode: 38, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -15.137, mean reward: -0.151 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.550, 10.373], loss: 0.004679, mae: 0.067821, mean_q: -0.293487
  3900/100000: episode: 39, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.895, mean reward: -0.179 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.258, 10.208], loss: 0.004985, mae: 0.071656, mean_q: -0.314966
  4000/100000: episode: 40, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.371, mean reward: -0.154 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.872, 10.155], loss: 0.006081, mae: 0.077500, mean_q: -0.294802
  4100/100000: episode: 41, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.137, mean reward: -0.171 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.522, 10.098], loss: 0.004737, mae: 0.067826, mean_q: -0.294342
  4200/100000: episode: 42, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.584, mean reward: -0.156 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.576, 10.098], loss: 0.004657, mae: 0.069545, mean_q: -0.305295
  4300/100000: episode: 43, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.998, mean reward: -0.170 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.214, 10.098], loss: 0.004634, mae: 0.068922, mean_q: -0.327142
  4400/100000: episode: 44, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.477, mean reward: -0.145 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.588, 10.098], loss: 0.004529, mae: 0.066672, mean_q: -0.336644
  4500/100000: episode: 45, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.690, mean reward: -0.167 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.803, 10.109], loss: 0.004404, mae: 0.067134, mean_q: -0.290409
  4600/100000: episode: 46, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -14.660, mean reward: -0.147 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.815, 10.513], loss: 0.004640, mae: 0.068946, mean_q: -0.303394
  4700/100000: episode: 47, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -15.370, mean reward: -0.154 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.286, 10.098], loss: 0.004505, mae: 0.070277, mean_q: -0.294714
  4800/100000: episode: 48, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -19.110, mean reward: -0.191 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.527, 10.098], loss: 0.005105, mae: 0.071765, mean_q: -0.319924
  4900/100000: episode: 49, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.740, mean reward: -0.177 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.939, 10.240], loss: 0.004455, mae: 0.068840, mean_q: -0.308165
  5000/100000: episode: 50, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -13.895, mean reward: -0.139 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.921, 10.213], loss: 0.004479, mae: 0.068278, mean_q: -0.309685
  5100/100000: episode: 51, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -13.338, mean reward: -0.133 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.078, 10.098], loss: 0.004066, mae: 0.064532, mean_q: -0.343016
  5200/100000: episode: 52, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.945, mean reward: -0.169 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.289, 10.098], loss: 0.005817, mae: 0.072606, mean_q: -0.326526
  5300/100000: episode: 53, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -19.297, mean reward: -0.193 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.093, 10.108], loss: 0.004282, mae: 0.066665, mean_q: -0.266371
  5400/100000: episode: 54, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.005, mean reward: -0.170 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.973, 10.098], loss: 0.005116, mae: 0.070555, mean_q: -0.290943
  5500/100000: episode: 55, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.681, mean reward: -0.167 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.937, 10.120], loss: 0.004195, mae: 0.064457, mean_q: -0.319932
  5600/100000: episode: 56, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.036, mean reward: -0.160 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.802, 10.168], loss: 0.004422, mae: 0.067641, mean_q: -0.325565
  5700/100000: episode: 57, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.271, mean reward: -0.163 [-1.000, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.438, 10.098], loss: 0.005311, mae: 0.074408, mean_q: -0.301771
  5800/100000: episode: 58, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -14.577, mean reward: -0.146 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.121, 10.268], loss: 0.003978, mae: 0.064572, mean_q: -0.318228
  5900/100000: episode: 59, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.822, mean reward: -0.168 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.504, 10.238], loss: 0.003767, mae: 0.063808, mean_q: -0.316169
  6000/100000: episode: 60, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.348, mean reward: -0.183 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.379, 10.098], loss: 0.004091, mae: 0.065292, mean_q: -0.272992
  6100/100000: episode: 61, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -20.081, mean reward: -0.201 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.737, 10.098], loss: 0.004432, mae: 0.065410, mean_q: -0.328859
  6200/100000: episode: 62, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -14.332, mean reward: -0.143 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.768, 10.098], loss: 0.004819, mae: 0.067621, mean_q: -0.326107
  6300/100000: episode: 63, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.565, mean reward: -0.156 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.957, 10.269], loss: 0.004132, mae: 0.065102, mean_q: -0.305451
  6400/100000: episode: 64, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.300, mean reward: -0.163 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.377, 10.098], loss: 0.004133, mae: 0.065207, mean_q: -0.327701
  6500/100000: episode: 65, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.883, mean reward: -0.159 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.396, 10.107], loss: 0.004562, mae: 0.070687, mean_q: -0.293278
  6600/100000: episode: 66, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.038, mean reward: -0.190 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.444, 10.098], loss: 0.003937, mae: 0.063186, mean_q: -0.326740
  6700/100000: episode: 67, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.069, mean reward: -0.171 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.802, 10.127], loss: 0.004391, mae: 0.067801, mean_q: -0.297326
  6800/100000: episode: 68, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.522, mean reward: -0.195 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.524, 10.162], loss: 0.004967, mae: 0.069667, mean_q: -0.291844
  6900/100000: episode: 69, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -12.589, mean reward: -0.126 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.690, 10.390], loss: 0.007013, mae: 0.081804, mean_q: -0.308357
  7000/100000: episode: 70, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -20.826, mean reward: -0.208 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.393, 10.098], loss: 0.004227, mae: 0.067133, mean_q: -0.324471
  7100/100000: episode: 71, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.412, mean reward: -0.174 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.942, 10.181], loss: 0.004027, mae: 0.064049, mean_q: -0.306623
  7200/100000: episode: 72, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.551, mean reward: -0.196 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.523, 10.127], loss: 0.004668, mae: 0.069252, mean_q: -0.269905
  7300/100000: episode: 73, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.595, mean reward: -0.156 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.447, 10.142], loss: 0.003991, mae: 0.066573, mean_q: -0.320798
  7400/100000: episode: 74, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.289, mean reward: -0.183 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.105, 10.257], loss: 0.004220, mae: 0.066035, mean_q: -0.294361
  7500/100000: episode: 75, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.049, mean reward: -0.170 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.832, 10.098], loss: 0.004526, mae: 0.067803, mean_q: -0.331292
  7600/100000: episode: 76, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.097, mean reward: -0.191 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.200, 10.098], loss: 0.004308, mae: 0.064674, mean_q: -0.344839
  7700/100000: episode: 77, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.735, mean reward: -0.197 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.570, 10.121], loss: 0.005024, mae: 0.071193, mean_q: -0.336221
  7800/100000: episode: 78, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.195, mean reward: -0.172 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.912, 10.098], loss: 0.004866, mae: 0.068087, mean_q: -0.297044
  7900/100000: episode: 79, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.200, mean reward: -0.132 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.566, 10.394], loss: 0.004420, mae: 0.065952, mean_q: -0.303061
  8000/100000: episode: 80, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -11.794, mean reward: -0.118 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.499, 10.098], loss: 0.004298, mae: 0.066737, mean_q: -0.289367
  8100/100000: episode: 81, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.387, mean reward: -0.174 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.030, 10.098], loss: 0.004782, mae: 0.067462, mean_q: -0.332247
  8200/100000: episode: 82, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.164, mean reward: -0.162 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.378, 10.156], loss: 0.003851, mae: 0.064927, mean_q: -0.319036
  8300/100000: episode: 83, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.372, mean reward: -0.164 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.836, 10.131], loss: 0.005000, mae: 0.072500, mean_q: -0.299810
  8400/100000: episode: 84, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.487, mean reward: -0.155 [-1.000, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.507, 10.118], loss: 0.004211, mae: 0.064947, mean_q: -0.309394
  8500/100000: episode: 85, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -14.186, mean reward: -0.142 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.637, 10.098], loss: 0.004574, mae: 0.069113, mean_q: -0.301785
  8600/100000: episode: 86, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -21.112, mean reward: -0.211 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.070, 10.109], loss: 0.004235, mae: 0.067214, mean_q: -0.295338
  8700/100000: episode: 87, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -18.202, mean reward: -0.182 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.616, 10.107], loss: 0.004893, mae: 0.070022, mean_q: -0.288679
  8800/100000: episode: 88, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.812, mean reward: -0.188 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.668, 10.098], loss: 0.004159, mae: 0.064024, mean_q: -0.306398
  8900/100000: episode: 89, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.696, mean reward: -0.187 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.327, 10.131], loss: 0.003506, mae: 0.062082, mean_q: -0.290025
  9000/100000: episode: 90, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -13.102, mean reward: -0.131 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.007, 10.098], loss: 0.003974, mae: 0.064717, mean_q: -0.321923
  9100/100000: episode: 91, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.470, mean reward: -0.165 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.882, 10.098], loss: 0.004162, mae: 0.065300, mean_q: -0.317002
  9200/100000: episode: 92, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.561, mean reward: -0.176 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.870, 10.098], loss: 0.003745, mae: 0.062693, mean_q: -0.301540
  9300/100000: episode: 93, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.763, mean reward: -0.188 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.181, 10.098], loss: 0.004283, mae: 0.066593, mean_q: -0.284905
  9400/100000: episode: 94, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.832, mean reward: -0.168 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.138, 10.173], loss: 0.003912, mae: 0.063245, mean_q: -0.315891
  9500/100000: episode: 95, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.451, mean reward: -0.185 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.670, 10.130], loss: 0.004012, mae: 0.065235, mean_q: -0.310413
  9600/100000: episode: 96, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.006, mean reward: -0.180 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.547, 10.098], loss: 0.004351, mae: 0.067592, mean_q: -0.298968
  9700/100000: episode: 97, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.522, mean reward: -0.195 [-1.000, 0.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.475, 10.098], loss: 0.003681, mae: 0.062476, mean_q: -0.291641
  9800/100000: episode: 98, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.843, mean reward: -0.168 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.063, 10.098], loss: 0.004818, mae: 0.068640, mean_q: -0.319243
  9900/100000: episode: 99, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.602, mean reward: -0.166 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.653, 10.156], loss: 0.003972, mae: 0.064960, mean_q: -0.271334
 10000/100000: episode: 100, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.899, mean reward: -0.179 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.400, 10.098], loss: 0.005091, mae: 0.071211, mean_q: -0.299385
 10100/100000: episode: 101, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.858, mean reward: -0.189 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.255, 10.098], loss: 0.004030, mae: 0.064956, mean_q: -0.317867
 10200/100000: episode: 102, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.880, mean reward: -0.179 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.527, 10.098], loss: 0.003506, mae: 0.059667, mean_q: -0.336632
 10300/100000: episode: 103, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.488, mean reward: -0.165 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.294, 10.293], loss: 0.006833, mae: 0.076700, mean_q: -0.315939
 10400/100000: episode: 104, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.327, mean reward: -0.193 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.854, 10.098], loss: 0.003823, mae: 0.061940, mean_q: -0.331624
 10500/100000: episode: 105, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.340, mean reward: -0.173 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.039, 10.237], loss: 0.003725, mae: 0.063897, mean_q: -0.336238
 10600/100000: episode: 106, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.605, mean reward: -0.186 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.732, 10.098], loss: 0.003724, mae: 0.063018, mean_q: -0.324034
 10700/100000: episode: 107, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.405, mean reward: -0.154 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.904, 10.240], loss: 0.003769, mae: 0.063215, mean_q: -0.311812
 10800/100000: episode: 108, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.757, mean reward: -0.178 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.440, 10.148], loss: 0.003699, mae: 0.063211, mean_q: -0.309664
 10900/100000: episode: 109, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.691, mean reward: -0.177 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.434, 10.098], loss: 0.004849, mae: 0.069724, mean_q: -0.322730
 11000/100000: episode: 110, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.139, mean reward: -0.191 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.242, 10.098], loss: 0.003692, mae: 0.061108, mean_q: -0.300573
 11100/100000: episode: 111, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.196, mean reward: -0.172 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.396, 10.139], loss: 0.004182, mae: 0.065540, mean_q: -0.322639
 11200/100000: episode: 112, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -16.238, mean reward: -0.162 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.178, 10.136], loss: 0.003341, mae: 0.058704, mean_q: -0.328880
 11300/100000: episode: 113, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.629, mean reward: -0.146 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.772, 10.098], loss: 0.003399, mae: 0.059338, mean_q: -0.319010
 11400/100000: episode: 114, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.320, mean reward: -0.173 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.753, 10.098], loss: 0.003221, mae: 0.058074, mean_q: -0.343283
 11500/100000: episode: 115, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.843, mean reward: -0.188 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.406, 10.098], loss: 0.004022, mae: 0.063950, mean_q: -0.323370
 11600/100000: episode: 116, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -12.530, mean reward: -0.125 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.210, 10.236], loss: 0.005226, mae: 0.071823, mean_q: -0.303409
 11700/100000: episode: 117, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -11.902, mean reward: -0.119 [-1.000, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.995, 10.098], loss: 0.003548, mae: 0.062657, mean_q: -0.306622
 11800/100000: episode: 118, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.008, mean reward: -0.170 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.737, 10.098], loss: 0.003340, mae: 0.058987, mean_q: -0.334580
 11900/100000: episode: 119, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.681, mean reward: -0.167 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.334, 10.266], loss: 0.003449, mae: 0.060390, mean_q: -0.308196
 12000/100000: episode: 120, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.173, mean reward: -0.192 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.958, 10.098], loss: 0.003218, mae: 0.057421, mean_q: -0.333593
 12100/100000: episode: 121, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.737, mean reward: -0.187 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.549, 10.226], loss: 0.003108, mae: 0.056903, mean_q: -0.351959
 12200/100000: episode: 122, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.064, mean reward: -0.181 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.533, 10.161], loss: 0.003478, mae: 0.060703, mean_q: -0.328267
 12300/100000: episode: 123, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.286, mean reward: -0.163 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.410, 10.108], loss: 0.003413, mae: 0.059687, mean_q: -0.312922
 12400/100000: episode: 124, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.199, mean reward: -0.172 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.419, 10.112], loss: 0.003048, mae: 0.057325, mean_q: -0.339881
 12500/100000: episode: 125, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.008, mean reward: -0.170 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.725, 10.227], loss: 0.003493, mae: 0.060312, mean_q: -0.310890
 12600/100000: episode: 126, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.180, mean reward: -0.182 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.763, 10.201], loss: 0.003694, mae: 0.064567, mean_q: -0.297205
 12700/100000: episode: 127, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.520, mean reward: -0.195 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.277, 10.167], loss: 0.003707, mae: 0.061939, mean_q: -0.341264
 12800/100000: episode: 128, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -15.900, mean reward: -0.159 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.471, 10.098], loss: 0.003446, mae: 0.060392, mean_q: -0.325802
 12900/100000: episode: 129, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.390, mean reward: -0.174 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.778, 10.314], loss: 0.003440, mae: 0.061100, mean_q: -0.324575
 13000/100000: episode: 130, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.920, mean reward: -0.169 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.098, 10.098], loss: 0.004451, mae: 0.065866, mean_q: -0.357806
 13100/100000: episode: 131, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.944, mean reward: -0.179 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.840, 10.196], loss: 0.003329, mae: 0.061306, mean_q: -0.325292
 13200/100000: episode: 132, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.417, mean reward: -0.164 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.563, 10.098], loss: 0.002939, mae: 0.055714, mean_q: -0.290798
 13300/100000: episode: 133, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.591, mean reward: -0.176 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.716, 10.098], loss: 0.003269, mae: 0.058546, mean_q: -0.301046
 13400/100000: episode: 134, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.761, mean reward: -0.168 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.070, 10.239], loss: 0.003020, mae: 0.055368, mean_q: -0.337556
 13500/100000: episode: 135, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -17.419, mean reward: -0.174 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.513, 10.098], loss: 0.004049, mae: 0.063662, mean_q: -0.331140
 13600/100000: episode: 136, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.063, mean reward: -0.151 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.848, 10.098], loss: 0.003509, mae: 0.060066, mean_q: -0.352161
 13700/100000: episode: 137, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.924, mean reward: -0.169 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.590, 10.098], loss: 0.003077, mae: 0.058076, mean_q: -0.328894
 13800/100000: episode: 138, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.509, mean reward: -0.175 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.452, 10.099], loss: 0.004071, mae: 0.065692, mean_q: -0.340941
 13900/100000: episode: 139, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.316, mean reward: -0.193 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.194, 10.098], loss: 0.003912, mae: 0.063364, mean_q: -0.367520
 14000/100000: episode: 140, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -16.557, mean reward: -0.166 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.923, 10.098], loss: 0.003161, mae: 0.058585, mean_q: -0.314043
 14100/100000: episode: 141, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.620, mean reward: -0.156 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.687, 10.098], loss: 0.002976, mae: 0.055620, mean_q: -0.325081
 14200/100000: episode: 142, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.910, mean reward: -0.179 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.973, 10.251], loss: 0.002957, mae: 0.055575, mean_q: -0.343220
 14300/100000: episode: 143, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -8.755, mean reward: -0.088 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.752, 10.115], loss: 0.003096, mae: 0.057025, mean_q: -0.317829
 14400/100000: episode: 144, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.831, mean reward: -0.148 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.537, 10.098], loss: 0.005167, mae: 0.069316, mean_q: -0.323452
 14500/100000: episode: 145, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.638, mean reward: -0.166 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.871, 10.098], loss: 0.003420, mae: 0.059273, mean_q: -0.333165
 14600/100000: episode: 146, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.333, mean reward: -0.193 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.829, 10.202], loss: 0.003085, mae: 0.056593, mean_q: -0.313550
 14700/100000: episode: 147, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.846, mean reward: -0.178 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.887, 10.098], loss: 0.003146, mae: 0.057135, mean_q: -0.326126
 14800/100000: episode: 148, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -20.349, mean reward: -0.203 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.921, 10.098], loss: 0.003042, mae: 0.055997, mean_q: -0.323228
 14900/100000: episode: 149, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.057, mean reward: -0.171 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.547, 10.098], loss: 0.003118, mae: 0.056971, mean_q: -0.331071
 15000/100000: episode: 150, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.517, mean reward: -0.145 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.128, 10.098], loss: 0.004660, mae: 0.069819, mean_q: -0.324287
 15100/100000: episode: 151, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.030, mean reward: -0.190 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.586, 10.143], loss: 0.004180, mae: 0.066493, mean_q: -0.280514
 15200/100000: episode: 152, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.210, mean reward: -0.172 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.379, 10.303], loss: 0.004212, mae: 0.065898, mean_q: -0.305981
 15300/100000: episode: 153, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -10.002, mean reward: -0.100 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.509, 10.098], loss: 0.003458, mae: 0.060772, mean_q: -0.323711
 15400/100000: episode: 154, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -14.315, mean reward: -0.143 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.104, 10.351], loss: 0.003790, mae: 0.064795, mean_q: -0.295196
 15500/100000: episode: 155, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.691, mean reward: -0.167 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.565, 10.098], loss: 0.003302, mae: 0.058548, mean_q: -0.326639
 15600/100000: episode: 156, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.563, mean reward: -0.176 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.949, 10.206], loss: 0.003935, mae: 0.063979, mean_q: -0.310784
 15700/100000: episode: 157, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -13.727, mean reward: -0.137 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.514, 10.186], loss: 0.004210, mae: 0.069173, mean_q: -0.318299
 15800/100000: episode: 158, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.198, mean reward: -0.172 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.860, 10.098], loss: 0.003446, mae: 0.061272, mean_q: -0.321961
 15900/100000: episode: 159, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.636, mean reward: -0.176 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.640, 10.098], loss: 0.003188, mae: 0.058559, mean_q: -0.309960
 16000/100000: episode: 160, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -20.486, mean reward: -0.205 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.155, 10.169], loss: 0.003448, mae: 0.060189, mean_q: -0.287423
 16100/100000: episode: 161, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -20.954, mean reward: -0.210 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.333, 10.098], loss: 0.003334, mae: 0.058237, mean_q: -0.351014
 16200/100000: episode: 162, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.400, mean reward: -0.184 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.088, 10.366], loss: 0.003233, mae: 0.056997, mean_q: -0.341660
 16300/100000: episode: 163, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.001, mean reward: -0.180 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.798, 10.098], loss: 0.003484, mae: 0.061655, mean_q: -0.313117
 16400/100000: episode: 164, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -15.308, mean reward: -0.153 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.505, 10.098], loss: 0.003423, mae: 0.059176, mean_q: -0.340457
 16500/100000: episode: 165, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.549, mean reward: -0.185 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.704, 10.193], loss: 0.003250, mae: 0.058428, mean_q: -0.307113
 16600/100000: episode: 166, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.954, mean reward: -0.180 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.189, 10.098], loss: 0.004145, mae: 0.064783, mean_q: -0.301794
 16700/100000: episode: 167, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -18.512, mean reward: -0.185 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.575, 10.098], loss: 0.003959, mae: 0.066948, mean_q: -0.300920
 16800/100000: episode: 168, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.371, mean reward: -0.194 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.262, 10.202], loss: 0.004956, mae: 0.069846, mean_q: -0.308759
 16900/100000: episode: 169, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.006, mean reward: -0.190 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.170, 10.104], loss: 0.005823, mae: 0.074692, mean_q: -0.331600
 17000/100000: episode: 170, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.134, mean reward: -0.161 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.776, 10.305], loss: 0.003824, mae: 0.063182, mean_q: -0.328025
 17100/100000: episode: 171, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -11.898, mean reward: -0.119 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.635, 10.250], loss: 0.003385, mae: 0.060364, mean_q: -0.338576
 17200/100000: episode: 172, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.417, mean reward: -0.194 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.161, 10.145], loss: 0.003170, mae: 0.057742, mean_q: -0.345222
 17300/100000: episode: 173, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.366, mean reward: -0.174 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.742, 10.127], loss: 0.005084, mae: 0.070634, mean_q: -0.371184
 17400/100000: episode: 174, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.965, mean reward: -0.180 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.576, 10.158], loss: 0.003794, mae: 0.064618, mean_q: -0.295078
 17500/100000: episode: 175, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.275, mean reward: -0.143 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.345, 10.355], loss: 0.003246, mae: 0.059542, mean_q: -0.320088
 17600/100000: episode: 176, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.716, mean reward: -0.177 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.292, 10.330], loss: 0.003250, mae: 0.058874, mean_q: -0.322777
 17700/100000: episode: 177, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.351, mean reward: -0.154 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.197, 10.098], loss: 0.003427, mae: 0.060747, mean_q: -0.296671
 17800/100000: episode: 178, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -15.822, mean reward: -0.158 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.733, 10.159], loss: 0.003288, mae: 0.057899, mean_q: -0.336978
 17900/100000: episode: 179, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.107, mean reward: -0.191 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.974, 10.206], loss: 0.003248, mae: 0.058479, mean_q: -0.328052
 18000/100000: episode: 180, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.558, mean reward: -0.196 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.847, 10.189], loss: 0.003159, mae: 0.057455, mean_q: -0.312473
 18100/100000: episode: 181, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -12.965, mean reward: -0.130 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.086, 10.098], loss: 0.004304, mae: 0.065001, mean_q: -0.317697
 18200/100000: episode: 182, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.585, mean reward: -0.146 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.740, 10.436], loss: 0.004239, mae: 0.067993, mean_q: -0.313652
 18300/100000: episode: 183, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.612, mean reward: -0.166 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.442, 10.098], loss: 0.006289, mae: 0.069232, mean_q: -0.326272
 18400/100000: episode: 184, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.935, mean reward: -0.179 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.630, 10.114], loss: 0.005121, mae: 0.071579, mean_q: -0.350872
 18500/100000: episode: 185, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.753, mean reward: -0.188 [-1.000, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.524, 10.184], loss: 0.003397, mae: 0.061909, mean_q: -0.314416
 18600/100000: episode: 186, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.799, mean reward: -0.188 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.973, 10.098], loss: 0.003437, mae: 0.060308, mean_q: -0.289532
 18700/100000: episode: 187, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.177, mean reward: -0.162 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.157, 10.206], loss: 0.003290, mae: 0.058079, mean_q: -0.309078
 18800/100000: episode: 188, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.557, mean reward: -0.176 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.535, 10.098], loss: 0.003303, mae: 0.058805, mean_q: -0.297237
 18900/100000: episode: 189, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -13.864, mean reward: -0.139 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.268, 10.335], loss: 0.003356, mae: 0.058687, mean_q: -0.313866
 19000/100000: episode: 190, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.661, mean reward: -0.197 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.949, 10.224], loss: 0.003189, mae: 0.057118, mean_q: -0.321441
 19100/100000: episode: 191, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.560, mean reward: -0.146 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.373, 10.098], loss: 0.003388, mae: 0.059438, mean_q: -0.300655
 19200/100000: episode: 192, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.429, mean reward: -0.184 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.964, 10.282], loss: 0.003226, mae: 0.057727, mean_q: -0.311015
 19300/100000: episode: 193, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.343, mean reward: -0.183 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.708, 10.098], loss: 0.003484, mae: 0.061926, mean_q: -0.318183
 19400/100000: episode: 194, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.850, mean reward: -0.148 [-1.000, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.525, 10.098], loss: 0.004126, mae: 0.063935, mean_q: -0.333466
 19500/100000: episode: 195, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.777, mean reward: -0.158 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.551, 10.098], loss: 0.004744, mae: 0.068294, mean_q: -0.306691
 19600/100000: episode: 196, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -13.806, mean reward: -0.138 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.270, 10.098], loss: 0.003642, mae: 0.062013, mean_q: -0.309287
 19700/100000: episode: 197, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.323, mean reward: -0.193 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.453, 10.110], loss: 0.003266, mae: 0.058543, mean_q: -0.315129
 19800/100000: episode: 198, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.335, mean reward: -0.153 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.182, 10.570], loss: 0.003288, mae: 0.058225, mean_q: -0.326938
 19900/100000: episode: 199, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -13.834, mean reward: -0.138 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.164, 10.275], loss: 0.003359, mae: 0.060037, mean_q: -0.336806
 20000/100000: episode: 200, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.676, mean reward: -0.187 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.700, 10.179], loss: 0.003320, mae: 0.058883, mean_q: -0.334853
 20100/100000: episode: 201, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.309, mean reward: -0.193 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.841, 10.098], loss: 0.003866, mae: 0.065318, mean_q: -0.301314
 20200/100000: episode: 202, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.442, mean reward: -0.184 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.026, 10.098], loss: 0.003354, mae: 0.058562, mean_q: -0.295298
 20300/100000: episode: 203, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.721, mean reward: -0.187 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.165, 10.233], loss: 0.003140, mae: 0.056827, mean_q: -0.340042
 20400/100000: episode: 204, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -21.149, mean reward: -0.211 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.765, 10.129], loss: 0.003151, mae: 0.056610, mean_q: -0.332191
 20500/100000: episode: 205, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.154, mean reward: -0.172 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.733, 10.160], loss: 0.003271, mae: 0.057649, mean_q: -0.330995
 20600/100000: episode: 206, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.037, mean reward: -0.180 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.939, 10.169], loss: 0.003202, mae: 0.056688, mean_q: -0.318107
 20700/100000: episode: 207, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.833, mean reward: -0.198 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.491, 10.230], loss: 0.003167, mae: 0.056777, mean_q: -0.330861
 20800/100000: episode: 208, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.681, mean reward: -0.177 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.968, 10.111], loss: 0.003727, mae: 0.061915, mean_q: -0.344319
 20900/100000: episode: 209, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.509, mean reward: -0.175 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.665, 10.098], loss: 0.003108, mae: 0.056084, mean_q: -0.320855
 21000/100000: episode: 210, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.257, mean reward: -0.183 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.121, 10.098], loss: 0.003583, mae: 0.062386, mean_q: -0.348020
 21100/100000: episode: 211, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.369, mean reward: -0.164 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.756, 10.319], loss: 0.003061, mae: 0.055169, mean_q: -0.324159
 21200/100000: episode: 212, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -14.350, mean reward: -0.143 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.287, 10.129], loss: 0.003552, mae: 0.060655, mean_q: -0.341142
 21300/100000: episode: 213, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -7.596, mean reward: -0.076 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.082, 10.098], loss: 0.003298, mae: 0.057633, mean_q: -0.333479
 21400/100000: episode: 214, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.477, mean reward: -0.185 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.721, 10.228], loss: 0.003393, mae: 0.057830, mean_q: -0.346962
 21500/100000: episode: 215, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -13.200, mean reward: -0.132 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.956, 10.367], loss: 0.003230, mae: 0.056343, mean_q: -0.307626
 21600/100000: episode: 216, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -12.638, mean reward: -0.126 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.701, 10.098], loss: 0.003380, mae: 0.058600, mean_q: -0.323508
 21700/100000: episode: 217, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.439, mean reward: -0.164 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.041, 10.371], loss: 0.003768, mae: 0.062990, mean_q: -0.337150
 21800/100000: episode: 218, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.711, mean reward: -0.147 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.687, 10.098], loss: 0.006651, mae: 0.078530, mean_q: -0.322472
 21900/100000: episode: 219, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.474, mean reward: -0.155 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.837, 10.229], loss: 0.003464, mae: 0.059815, mean_q: -0.305472
 22000/100000: episode: 220, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.938, mean reward: -0.199 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.048, 10.098], loss: 0.003241, mae: 0.057232, mean_q: -0.331497
 22100/100000: episode: 221, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -13.830, mean reward: -0.138 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.618, 10.098], loss: 0.003217, mae: 0.057265, mean_q: -0.339639
 22200/100000: episode: 222, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.571, mean reward: -0.186 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.891, 10.109], loss: 0.003191, mae: 0.056863, mean_q: -0.304110
 22300/100000: episode: 223, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.531, mean reward: -0.155 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.623, 10.174], loss: 0.003509, mae: 0.059421, mean_q: -0.327512
 22400/100000: episode: 224, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.349, mean reward: -0.173 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.021, 10.250], loss: 0.003741, mae: 0.061786, mean_q: -0.287617
 22500/100000: episode: 225, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.058, mean reward: -0.141 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.822, 10.098], loss: 0.005176, mae: 0.070961, mean_q: -0.300032
 22600/100000: episode: 226, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.040, mean reward: -0.150 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.960, 10.098], loss: 0.003319, mae: 0.058224, mean_q: -0.313892
 22700/100000: episode: 227, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.116, mean reward: -0.181 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.640, 10.098], loss: 0.003969, mae: 0.063808, mean_q: -0.317432
 22800/100000: episode: 228, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.193, mean reward: -0.182 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.632, 10.104], loss: 0.003249, mae: 0.057935, mean_q: -0.280895
 22900/100000: episode: 229, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.712, mean reward: -0.157 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.209, 10.098], loss: 0.003189, mae: 0.056692, mean_q: -0.305135
 23000/100000: episode: 230, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -12.430, mean reward: -0.124 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.062, 10.098], loss: 0.003279, mae: 0.056827, mean_q: -0.326257
 23100/100000: episode: 231, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.029, mean reward: -0.180 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.588, 10.098], loss: 0.003742, mae: 0.060296, mean_q: -0.329843
 23200/100000: episode: 232, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.040, mean reward: -0.180 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.388, 10.255], loss: 0.005115, mae: 0.074682, mean_q: -0.332788
 23300/100000: episode: 233, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.448, mean reward: -0.184 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.106, 10.257], loss: 0.003350, mae: 0.061589, mean_q: -0.306914
 23400/100000: episode: 234, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.514, mean reward: -0.185 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.144, 10.098], loss: 0.003209, mae: 0.056797, mean_q: -0.325771
 23500/100000: episode: 235, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -8.632, mean reward: -0.086 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.938, 10.098], loss: 0.003111, mae: 0.055275, mean_q: -0.348083
 23600/100000: episode: 236, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.181, mean reward: -0.182 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.794, 10.141], loss: 0.003257, mae: 0.057184, mean_q: -0.303539
 23700/100000: episode: 237, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.257, mean reward: -0.153 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.562, 10.098], loss: 0.003284, mae: 0.057289, mean_q: -0.323601
 23800/100000: episode: 238, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.764, mean reward: -0.148 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.262, 10.369], loss: 0.003177, mae: 0.055978, mean_q: -0.316002
 23900/100000: episode: 239, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.250, mean reward: -0.162 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.352, 10.098], loss: 0.003163, mae: 0.056664, mean_q: -0.317577
 24000/100000: episode: 240, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.287, mean reward: -0.173 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.047, 10.384], loss: 0.003159, mae: 0.056385, mean_q: -0.324906
 24100/100000: episode: 241, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -20.104, mean reward: -0.201 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.833, 10.144], loss: 0.003760, mae: 0.062348, mean_q: -0.303675
 24200/100000: episode: 242, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.631, mean reward: -0.186 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.349, 10.105], loss: 0.003167, mae: 0.056245, mean_q: -0.331491
 24300/100000: episode: 243, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -12.038, mean reward: -0.120 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.927, 10.297], loss: 0.003327, mae: 0.057185, mean_q: -0.322289
 24400/100000: episode: 244, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.501, mean reward: -0.195 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.916, 10.298], loss: 0.003582, mae: 0.059240, mean_q: -0.306214
 24500/100000: episode: 245, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.447, mean reward: -0.184 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.418, 10.132], loss: 0.003520, mae: 0.059722, mean_q: -0.311274
 24600/100000: episode: 246, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.580, mean reward: -0.156 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.560, 10.098], loss: 0.003534, mae: 0.059068, mean_q: -0.323601
 24700/100000: episode: 247, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.732, mean reward: -0.177 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.262, 10.309], loss: 0.003896, mae: 0.062703, mean_q: -0.315122
 24800/100000: episode: 248, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.025, mean reward: -0.170 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.040, 10.098], loss: 0.003305, mae: 0.059163, mean_q: -0.337579
 24900/100000: episode: 249, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.954, mean reward: -0.180 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.000, 10.098], loss: 0.003238, mae: 0.056662, mean_q: -0.334191
 25000/100000: episode: 250, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.118, mean reward: -0.181 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.945, 10.156], loss: 0.003289, mae: 0.058876, mean_q: -0.279940
 25100/100000: episode: 251, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.454, mean reward: -0.165 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.753, 10.314], loss: 0.003315, mae: 0.058241, mean_q: -0.303963
 25200/100000: episode: 252, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.890, mean reward: -0.169 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.013, 10.180], loss: 0.003120, mae: 0.056540, mean_q: -0.312132
 25300/100000: episode: 253, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.546, mean reward: -0.155 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.131, 10.098], loss: 0.003958, mae: 0.062762, mean_q: -0.303248
 25400/100000: episode: 254, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -18.708, mean reward: -0.187 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.107, 10.098], loss: 0.004454, mae: 0.069802, mean_q: -0.324097
 25500/100000: episode: 255, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.065, mean reward: -0.161 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.044, 10.279], loss: 0.003395, mae: 0.058064, mean_q: -0.304702
 25600/100000: episode: 256, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.465, mean reward: -0.165 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.899, 10.099], loss: 0.003776, mae: 0.062380, mean_q: -0.294097
 25700/100000: episode: 257, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -15.889, mean reward: -0.159 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.776, 10.098], loss: 0.003200, mae: 0.056717, mean_q: -0.288142
 25800/100000: episode: 258, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.473, mean reward: -0.175 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.232, 10.098], loss: 0.003505, mae: 0.059556, mean_q: -0.310431
 25900/100000: episode: 259, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.548, mean reward: -0.165 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.495, 10.098], loss: 0.003261, mae: 0.056910, mean_q: -0.326594
 26000/100000: episode: 260, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.790, mean reward: -0.188 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.818, 10.098], loss: 0.003621, mae: 0.062153, mean_q: -0.278893
 26100/100000: episode: 261, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.839, mean reward: -0.178 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.992, 10.106], loss: 0.003576, mae: 0.062373, mean_q: -0.295870
 26200/100000: episode: 262, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.957, mean reward: -0.180 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.188, 10.098], loss: 0.003474, mae: 0.061145, mean_q: -0.292278
 26300/100000: episode: 263, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -11.634, mean reward: -0.116 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.442, 10.098], loss: 0.003484, mae: 0.061167, mean_q: -0.315792
 26400/100000: episode: 264, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.759, mean reward: -0.188 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.739, 10.196], loss: 0.003416, mae: 0.059379, mean_q: -0.317197
 26500/100000: episode: 265, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -9.050, mean reward: -0.090 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.317, 10.098], loss: 0.003593, mae: 0.062036, mean_q: -0.273928
 26600/100000: episode: 266, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.243, mean reward: -0.162 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.378, 10.138], loss: 0.003167, mae: 0.056687, mean_q: -0.307183
 26700/100000: episode: 267, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -20.128, mean reward: -0.201 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.819, 10.098], loss: 0.003110, mae: 0.056617, mean_q: -0.283540
 26800/100000: episode: 268, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -9.940, mean reward: -0.099 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.127, 10.343], loss: 0.003192, mae: 0.056918, mean_q: -0.305252
 26900/100000: episode: 269, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.832, mean reward: -0.158 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.431, 10.098], loss: 0.003285, mae: 0.057248, mean_q: -0.312729
 27000/100000: episode: 270, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.115, mean reward: -0.191 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.001, 10.103], loss: 0.005291, mae: 0.074301, mean_q: -0.319016
 27100/100000: episode: 271, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.287, mean reward: -0.163 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.744, 10.338], loss: 0.003387, mae: 0.059563, mean_q: -0.314285
 27200/100000: episode: 272, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -18.393, mean reward: -0.184 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.502, 10.098], loss: 0.003318, mae: 0.058724, mean_q: -0.273976
 27300/100000: episode: 273, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.564, mean reward: -0.196 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.613, 10.209], loss: 0.003345, mae: 0.058565, mean_q: -0.308519
 27400/100000: episode: 274, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -14.986, mean reward: -0.150 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.811, 10.269], loss: 0.003105, mae: 0.055260, mean_q: -0.328170
 27500/100000: episode: 275, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.518, mean reward: -0.185 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.928, 10.217], loss: 0.003055, mae: 0.054971, mean_q: -0.343957
 27600/100000: episode: 276, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.606, mean reward: -0.196 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.840, 10.124], loss: 0.003070, mae: 0.056597, mean_q: -0.317085
 27700/100000: episode: 277, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.752, mean reward: -0.148 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.810, 10.098], loss: 0.003156, mae: 0.057062, mean_q: -0.333236
 27800/100000: episode: 278, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.043, mean reward: -0.150 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.828, 10.098], loss: 0.003512, mae: 0.061001, mean_q: -0.298852
 27900/100000: episode: 279, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.418, mean reward: -0.164 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.614, 10.301], loss: 0.003236, mae: 0.057287, mean_q: -0.330788
 28000/100000: episode: 280, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -13.669, mean reward: -0.137 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.933, 10.292], loss: 0.003003, mae: 0.055458, mean_q: -0.319345
 28100/100000: episode: 281, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -17.844, mean reward: -0.178 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.641, 10.098], loss: 0.003228, mae: 0.058978, mean_q: -0.320328
 28200/100000: episode: 282, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -19.391, mean reward: -0.194 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.081, 10.209], loss: 0.002760, mae: 0.053568, mean_q: -0.321587
 28300/100000: episode: 283, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.377, mean reward: -0.174 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.813, 10.122], loss: 0.003277, mae: 0.057231, mean_q: -0.301539
 28400/100000: episode: 284, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.299, mean reward: -0.173 [-1.000, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.536, 10.195], loss: 0.003389, mae: 0.058581, mean_q: -0.282477
 28500/100000: episode: 285, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -10.772, mean reward: -0.108 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.978, 10.098], loss: 0.003172, mae: 0.057357, mean_q: -0.309777
 28600/100000: episode: 286, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -20.235, mean reward: -0.202 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.927, 10.101], loss: 0.003864, mae: 0.061268, mean_q: -0.288886
 28700/100000: episode: 287, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -13.347, mean reward: -0.133 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.782, 10.163], loss: 0.003420, mae: 0.061659, mean_q: -0.328860
 28800/100000: episode: 288, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -13.938, mean reward: -0.139 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.867, 10.098], loss: 0.002837, mae: 0.052982, mean_q: -0.358236
 28900/100000: episode: 289, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.851, mean reward: -0.149 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.857, 10.098], loss: 0.003113, mae: 0.056704, mean_q: -0.309355
 29000/100000: episode: 290, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -20.166, mean reward: -0.202 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.473, 10.099], loss: 0.002952, mae: 0.054839, mean_q: -0.313599
 29100/100000: episode: 291, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.094, mean reward: -0.141 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.001, 10.316], loss: 0.005606, mae: 0.070186, mean_q: -0.302498
 29200/100000: episode: 292, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.365, mean reward: -0.144 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.613, 10.256], loss: 0.003317, mae: 0.059752, mean_q: -0.328358
 29300/100000: episode: 293, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.594, mean reward: -0.186 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.736, 10.121], loss: 0.002926, mae: 0.055886, mean_q: -0.304646
 29400/100000: episode: 294, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.143, mean reward: -0.151 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.952, 10.124], loss: 0.002973, mae: 0.055228, mean_q: -0.314502
 29500/100000: episode: 295, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -20.621, mean reward: -0.206 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.452, 10.098], loss: 0.002795, mae: 0.053486, mean_q: -0.302978
 29600/100000: episode: 296, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -13.620, mean reward: -0.136 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.806, 10.098], loss: 0.002808, mae: 0.052881, mean_q: -0.304476
 29700/100000: episode: 297, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.036, mean reward: -0.190 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.984, 10.105], loss: 0.002862, mae: 0.054370, mean_q: -0.286710
 29800/100000: episode: 298, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.554, mean reward: -0.166 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.586, 10.098], loss: 0.002914, mae: 0.054375, mean_q: -0.303903
 29900/100000: episode: 299, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.812, mean reward: -0.158 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.065, 10.492], loss: 0.005142, mae: 0.067662, mean_q: -0.342466
 30000/100000: episode: 300, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.541, mean reward: -0.135 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.777, 10.106], loss: 0.002989, mae: 0.056176, mean_q: -0.308740
 30100/100000: episode: 301, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -18.449, mean reward: -0.184 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.691, 10.111], loss: 0.002993, mae: 0.056178, mean_q: -0.291203
 30200/100000: episode: 302, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.524, mean reward: -0.165 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.270, 10.098], loss: 0.002963, mae: 0.055768, mean_q: -0.311185
 30300/100000: episode: 303, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.169, mean reward: -0.182 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.887, 10.168], loss: 0.003041, mae: 0.055707, mean_q: -0.313092
 30400/100000: episode: 304, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.848, mean reward: -0.168 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.466, 10.098], loss: 0.002898, mae: 0.054201, mean_q: -0.319692
 30500/100000: episode: 305, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.323, mean reward: -0.193 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.129, 10.098], loss: 0.002876, mae: 0.054380, mean_q: -0.296766
 30600/100000: episode: 306, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -20.359, mean reward: -0.204 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.496, 10.098], loss: 0.003701, mae: 0.061157, mean_q: -0.297871
 30700/100000: episode: 307, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -14.343, mean reward: -0.143 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.009, 10.098], loss: 0.003165, mae: 0.058543, mean_q: -0.318077
 30800/100000: episode: 308, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -13.921, mean reward: -0.139 [-1.000, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.420, 10.164], loss: 0.002946, mae: 0.055363, mean_q: -0.282794
 30900/100000: episode: 309, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -13.966, mean reward: -0.140 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.939, 10.098], loss: 0.003051, mae: 0.057024, mean_q: -0.288682
 31000/100000: episode: 310, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -12.777, mean reward: -0.128 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.075, 10.098], loss: 0.003009, mae: 0.054614, mean_q: -0.307211
 31100/100000: episode: 311, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.159, mean reward: -0.162 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.727, 10.098], loss: 0.002974, mae: 0.054048, mean_q: -0.311761
 31200/100000: episode: 312, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -13.386, mean reward: -0.134 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.147, 10.098], loss: 0.003671, mae: 0.058320, mean_q: -0.315351
 31300/100000: episode: 313, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -15.488, mean reward: -0.155 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.464, 10.098], loss: 0.008188, mae: 0.084586, mean_q: -0.306655
 31400/100000: episode: 314, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.353, mean reward: -0.194 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.271, 10.118], loss: 0.004457, mae: 0.067976, mean_q: -0.323040
 31500/100000: episode: 315, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.555, mean reward: -0.166 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.031, 10.432], loss: 0.003254, mae: 0.058695, mean_q: -0.296013
 31600/100000: episode: 316, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.364, mean reward: -0.164 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.801, 10.219], loss: 0.003033, mae: 0.055311, mean_q: -0.323400
 31700/100000: episode: 317, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.753, mean reward: -0.188 [-1.000, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.306, 10.098], loss: 0.003088, mae: 0.055934, mean_q: -0.318637
 31800/100000: episode: 318, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.272, mean reward: -0.143 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.378, 10.317], loss: 0.002915, mae: 0.054262, mean_q: -0.335587
 31900/100000: episode: 319, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.171, mean reward: -0.182 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.148, 10.227], loss: 0.002781, mae: 0.052796, mean_q: -0.343108
 32000/100000: episode: 320, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -15.492, mean reward: -0.155 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.087, 10.098], loss: 0.002799, mae: 0.052833, mean_q: -0.324275
 32100/100000: episode: 321, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.948, mean reward: -0.179 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.961, 10.195], loss: 0.002959, mae: 0.054234, mean_q: -0.319231
 32200/100000: episode: 322, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.639, mean reward: -0.156 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.638, 10.098], loss: 0.002788, mae: 0.053127, mean_q: -0.309934
 32300/100000: episode: 323, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -17.604, mean reward: -0.176 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.311, 10.242], loss: 0.003036, mae: 0.055173, mean_q: -0.292492
 32400/100000: episode: 324, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.626, mean reward: -0.196 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.364, 10.230], loss: 0.002957, mae: 0.053676, mean_q: -0.322116
 32500/100000: episode: 325, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -12.672, mean reward: -0.127 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.616, 10.098], loss: 0.002930, mae: 0.054843, mean_q: -0.315106
 32600/100000: episode: 326, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.603, mean reward: -0.176 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.064, 10.464], loss: 0.002998, mae: 0.055256, mean_q: -0.289908
 32700/100000: episode: 327, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.601, mean reward: -0.156 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.923, 10.098], loss: 0.003142, mae: 0.056560, mean_q: -0.312909
 32800/100000: episode: 328, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.331, mean reward: -0.193 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.840, 10.114], loss: 0.003081, mae: 0.056706, mean_q: -0.307817
 32900/100000: episode: 329, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -14.327, mean reward: -0.143 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.565, 10.098], loss: 0.003030, mae: 0.056203, mean_q: -0.304209
 33000/100000: episode: 330, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.592, mean reward: -0.186 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.142, 10.221], loss: 0.002862, mae: 0.053732, mean_q: -0.292220
 33100/100000: episode: 331, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.931, mean reward: -0.179 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.345, 10.098], loss: 0.002988, mae: 0.054845, mean_q: -0.274313
 33200/100000: episode: 332, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.624, mean reward: -0.166 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.961, 10.098], loss: 0.002840, mae: 0.052840, mean_q: -0.319831
 33300/100000: episode: 333, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.926, mean reward: -0.139 [-1.000, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.018, 10.111], loss: 0.002998, mae: 0.055051, mean_q: -0.304796
 33400/100000: episode: 334, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.059, mean reward: -0.181 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.676, 10.098], loss: 0.003071, mae: 0.056046, mean_q: -0.305478
 33500/100000: episode: 335, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.536, mean reward: -0.165 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.941, 10.098], loss: 0.002860, mae: 0.053823, mean_q: -0.301826
 33600/100000: episode: 336, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -15.569, mean reward: -0.156 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.783, 10.330], loss: 0.003009, mae: 0.055361, mean_q: -0.294281
 33700/100000: episode: 337, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.085, mean reward: -0.171 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.854, 10.154], loss: 0.002967, mae: 0.056214, mean_q: -0.328523
 33800/100000: episode: 338, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -13.415, mean reward: -0.134 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.977, 10.098], loss: 0.002866, mae: 0.054464, mean_q: -0.284260
 33900/100000: episode: 339, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.603, mean reward: -0.176 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.136, 10.131], loss: 0.002786, mae: 0.052920, mean_q: -0.308597
 34000/100000: episode: 340, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.831, mean reward: -0.158 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.644, 10.143], loss: 0.003970, mae: 0.063101, mean_q: -0.328831
 34100/100000: episode: 341, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.499, mean reward: -0.155 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.514, 10.098], loss: 0.002965, mae: 0.054531, mean_q: -0.300256
 34200/100000: episode: 342, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.242, mean reward: -0.182 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.530, 10.255], loss: 0.003061, mae: 0.055589, mean_q: -0.306115
 34300/100000: episode: 343, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -12.897, mean reward: -0.129 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.147, 10.098], loss: 0.002932, mae: 0.054086, mean_q: -0.320341
 34400/100000: episode: 344, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.227, mean reward: -0.182 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.037, 10.179], loss: 0.002988, mae: 0.054878, mean_q: -0.276716
 34500/100000: episode: 345, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -13.783, mean reward: -0.138 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.548, 10.098], loss: 0.003061, mae: 0.055418, mean_q: -0.320063
 34600/100000: episode: 346, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.209, mean reward: -0.172 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.917, 10.098], loss: 0.002913, mae: 0.053933, mean_q: -0.323320
 34700/100000: episode: 347, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.102, mean reward: -0.181 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.174, 10.234], loss: 0.003060, mae: 0.055972, mean_q: -0.306606
 34800/100000: episode: 348, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -13.629, mean reward: -0.136 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.939, 10.176], loss: 0.003736, mae: 0.063183, mean_q: -0.327968
 34900/100000: episode: 349, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.698, mean reward: -0.137 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.483, 10.098], loss: 0.003189, mae: 0.057142, mean_q: -0.294572
 35000/100000: episode: 350, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.632, mean reward: -0.176 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.889, 10.098], loss: 0.003037, mae: 0.055197, mean_q: -0.320578
 35100/100000: episode: 351, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.014, mean reward: -0.180 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.432, 10.098], loss: 0.003151, mae: 0.056351, mean_q: -0.291949
 35200/100000: episode: 352, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.446, mean reward: -0.184 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.644, 10.234], loss: 0.003163, mae: 0.059065, mean_q: -0.290486
 35300/100000: episode: 353, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -15.799, mean reward: -0.158 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.209, 10.172], loss: 0.002809, mae: 0.053165, mean_q: -0.330799
 35400/100000: episode: 354, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.957, mean reward: -0.190 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.571, 10.098], loss: 0.003160, mae: 0.055233, mean_q: -0.309937
 35500/100000: episode: 355, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.587, mean reward: -0.196 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.470, 10.198], loss: 0.003082, mae: 0.055748, mean_q: -0.256489
 35600/100000: episode: 356, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.574, mean reward: -0.126 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.491, 10.098], loss: 0.002993, mae: 0.054811, mean_q: -0.277031
 35700/100000: episode: 357, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.262, mean reward: -0.163 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.037, 10.098], loss: 0.003049, mae: 0.054349, mean_q: -0.303206
 35800/100000: episode: 358, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -21.953, mean reward: -0.220 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.509, 10.128], loss: 0.003017, mae: 0.054546, mean_q: -0.316615
 35900/100000: episode: 359, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -14.478, mean reward: -0.145 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.858, 10.098], loss: 0.003543, mae: 0.058037, mean_q: -0.344245
 36000/100000: episode: 360, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -19.194, mean reward: -0.192 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.969, 10.098], loss: 0.005305, mae: 0.069705, mean_q: -0.283986
 36100/100000: episode: 361, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.228, mean reward: -0.162 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.147, 10.098], loss: 0.002796, mae: 0.053117, mean_q: -0.336341
 36200/100000: episode: 362, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.495, mean reward: -0.185 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.335, 10.293], loss: 0.002949, mae: 0.054001, mean_q: -0.309510
 36300/100000: episode: 363, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -21.169, mean reward: -0.212 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.040, 10.201], loss: 0.002994, mae: 0.054132, mean_q: -0.297746
 36400/100000: episode: 364, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -19.841, mean reward: -0.198 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.570, 10.098], loss: 0.003043, mae: 0.055259, mean_q: -0.327256
 36500/100000: episode: 365, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -9.825, mean reward: -0.098 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.106, 10.098], loss: 0.003160, mae: 0.058922, mean_q: -0.319160
 36600/100000: episode: 366, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.351, mean reward: -0.164 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.238, 10.217], loss: 0.002912, mae: 0.054463, mean_q: -0.329775
 36700/100000: episode: 367, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -14.058, mean reward: -0.141 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.632, 10.098], loss: 0.003041, mae: 0.055603, mean_q: -0.306602
 36800/100000: episode: 368, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.036, mean reward: -0.180 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.118, 10.098], loss: 0.002776, mae: 0.051687, mean_q: -0.328556
 36900/100000: episode: 369, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.166, mean reward: -0.162 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.566, 10.098], loss: 0.002822, mae: 0.052850, mean_q: -0.336365
 37000/100000: episode: 370, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.467, mean reward: -0.165 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.506, 10.291], loss: 0.002847, mae: 0.054043, mean_q: -0.283457
 37100/100000: episode: 371, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.075, mean reward: -0.181 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.106, 10.121], loss: 0.002746, mae: 0.051569, mean_q: -0.297918
 37200/100000: episode: 372, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -18.137, mean reward: -0.181 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.098, 10.098], loss: 0.002813, mae: 0.052921, mean_q: -0.313461
 37300/100000: episode: 373, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.485, mean reward: -0.165 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.700, 10.098], loss: 0.002753, mae: 0.051541, mean_q: -0.335592
 37400/100000: episode: 374, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.996, mean reward: -0.180 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.974, 10.183], loss: 0.002710, mae: 0.051535, mean_q: -0.297168
 37500/100000: episode: 375, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.444, mean reward: -0.154 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.822, 10.456], loss: 0.003010, mae: 0.054688, mean_q: -0.291605
 37600/100000: episode: 376, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -19.595, mean reward: -0.196 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.454, 10.111], loss: 0.003114, mae: 0.056753, mean_q: -0.311198
 37700/100000: episode: 377, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.522, mean reward: -0.135 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.376, 10.098], loss: 0.002998, mae: 0.054625, mean_q: -0.313542
 37800/100000: episode: 378, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -13.968, mean reward: -0.140 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.577, 10.148], loss: 0.002708, mae: 0.051290, mean_q: -0.337937
 37900/100000: episode: 379, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.493, mean reward: -0.175 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.712, 10.098], loss: 0.002625, mae: 0.050451, mean_q: -0.309391
 38000/100000: episode: 380, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.026, mean reward: -0.170 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.149, 10.098], loss: 0.003058, mae: 0.055695, mean_q: -0.295046
 38100/100000: episode: 381, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.866, mean reward: -0.169 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.595, 10.287], loss: 0.002749, mae: 0.051657, mean_q: -0.317665
 38200/100000: episode: 382, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.616, mean reward: -0.156 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.802, 10.098], loss: 0.002764, mae: 0.050801, mean_q: -0.337412
 38300/100000: episode: 383, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.304, mean reward: -0.173 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.328, 10.210], loss: 0.002641, mae: 0.051451, mean_q: -0.303273
 38400/100000: episode: 384, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.014, mean reward: -0.160 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.908, 10.098], loss: 0.002624, mae: 0.050821, mean_q: -0.302765
 38500/100000: episode: 385, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.938, mean reward: -0.169 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.227, 10.098], loss: 0.002649, mae: 0.052601, mean_q: -0.314774
 38600/100000: episode: 386, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.192, mean reward: -0.182 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.418, 10.136], loss: 0.002766, mae: 0.051043, mean_q: -0.327046
 38700/100000: episode: 387, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -20.881, mean reward: -0.209 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.598, 10.198], loss: 0.002631, mae: 0.051259, mean_q: -0.342842
 38800/100000: episode: 388, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -11.122, mean reward: -0.111 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.207, 10.397], loss: 0.002397, mae: 0.048028, mean_q: -0.331326
 38900/100000: episode: 389, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.725, mean reward: -0.197 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.668, 10.122], loss: 0.002684, mae: 0.051538, mean_q: -0.334853
 39000/100000: episode: 390, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.173, mean reward: -0.192 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.137, 10.166], loss: 0.003489, mae: 0.058222, mean_q: -0.322503
 39100/100000: episode: 391, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.641, mean reward: -0.186 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.626, 10.192], loss: 0.005054, mae: 0.068445, mean_q: -0.331221
 39200/100000: episode: 392, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.949, mean reward: -0.179 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.402, 10.132], loss: 0.002952, mae: 0.054324, mean_q: -0.286744
 39300/100000: episode: 393, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.333, mean reward: -0.133 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.419, 10.098], loss: 0.002600, mae: 0.050318, mean_q: -0.324594
 39400/100000: episode: 394, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -13.624, mean reward: -0.136 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.130, 10.113], loss: 0.002641, mae: 0.050418, mean_q: -0.288659
 39500/100000: episode: 395, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -20.196, mean reward: -0.202 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.552, 10.205], loss: 0.002713, mae: 0.051693, mean_q: -0.341870
 39600/100000: episode: 396, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.483, mean reward: -0.175 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.541, 10.174], loss: 0.002684, mae: 0.051018, mean_q: -0.317072
 39700/100000: episode: 397, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -14.449, mean reward: -0.144 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.724, 10.098], loss: 0.002670, mae: 0.051914, mean_q: -0.297685
 39800/100000: episode: 398, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -9.122, mean reward: -0.091 [-1.000, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.546, 10.098], loss: 0.002557, mae: 0.049583, mean_q: -0.307882
 39900/100000: episode: 399, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.027, mean reward: -0.170 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.071, 10.121], loss: 0.002493, mae: 0.049608, mean_q: -0.294521
 40000/100000: episode: 400, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -12.052, mean reward: -0.121 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.292, 10.301], loss: 0.002520, mae: 0.049263, mean_q: -0.316886
 40100/100000: episode: 401, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -19.385, mean reward: -0.194 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.092, 10.157], loss: 0.003543, mae: 0.056435, mean_q: -0.363854
 40200/100000: episode: 402, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.286, mean reward: -0.193 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.811, 10.098], loss: 0.002762, mae: 0.053677, mean_q: -0.317055
 40300/100000: episode: 403, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.160, mean reward: -0.192 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.260, 10.285], loss: 0.002619, mae: 0.049765, mean_q: -0.321454
 40400/100000: episode: 404, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.707, mean reward: -0.147 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.660, 10.345], loss: 0.002555, mae: 0.049266, mean_q: -0.332058
 40500/100000: episode: 405, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -19.053, mean reward: -0.191 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.597, 10.234], loss: 0.002659, mae: 0.050626, mean_q: -0.322993
 40600/100000: episode: 406, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.400, mean reward: -0.194 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.665, 10.098], loss: 0.002680, mae: 0.050737, mean_q: -0.339968
 40700/100000: episode: 407, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.560, mean reward: -0.186 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.768, 10.243], loss: 0.002829, mae: 0.051662, mean_q: -0.332386
 40800/100000: episode: 408, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.404, mean reward: -0.144 [-1.000, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.141, 10.332], loss: 0.002566, mae: 0.049057, mean_q: -0.306561
 40900/100000: episode: 409, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.488, mean reward: -0.185 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.809, 10.152], loss: 0.002504, mae: 0.048339, mean_q: -0.343250
 41000/100000: episode: 410, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.528, mean reward: -0.175 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.609, 10.098], loss: 0.002483, mae: 0.048916, mean_q: -0.327908
 41100/100000: episode: 411, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.359, mean reward: -0.184 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.024, 10.167], loss: 0.003048, mae: 0.053541, mean_q: -0.321340
 41200/100000: episode: 412, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.158, mean reward: -0.162 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.775, 10.169], loss: 0.002722, mae: 0.051395, mean_q: -0.331966
 41300/100000: episode: 413, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.911, mean reward: -0.189 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.368, 10.162], loss: 0.002533, mae: 0.049313, mean_q: -0.302777
 41400/100000: episode: 414, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -20.212, mean reward: -0.202 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.024, 10.350], loss: 0.002656, mae: 0.050270, mean_q: -0.331366
 41500/100000: episode: 415, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.649, mean reward: -0.166 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.841, 10.098], loss: 0.002813, mae: 0.052032, mean_q: -0.319049
 41600/100000: episode: 416, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.683, mean reward: -0.157 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.064, 10.152], loss: 0.002654, mae: 0.053986, mean_q: -0.328731
 41700/100000: episode: 417, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.226, mean reward: -0.152 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.737, 10.200], loss: 0.002663, mae: 0.050710, mean_q: -0.326706
 41800/100000: episode: 418, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.778, mean reward: -0.168 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.485, 10.173], loss: 0.002560, mae: 0.049534, mean_q: -0.333636
 41900/100000: episode: 419, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.781, mean reward: -0.158 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.013, 10.098], loss: 0.002891, mae: 0.054169, mean_q: -0.284011
 42000/100000: episode: 420, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -6.956, mean reward: -0.070 [-1.000, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.097, 10.553], loss: 0.003325, mae: 0.053976, mean_q: -0.339041
 42100/100000: episode: 421, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.557, mean reward: -0.166 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.435, 10.098], loss: 0.003853, mae: 0.060706, mean_q: -0.334042
 42200/100000: episode: 422, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.037, mean reward: -0.160 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.713, 10.098], loss: 0.002789, mae: 0.052863, mean_q: -0.305048
 42300/100000: episode: 423, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.062, mean reward: -0.171 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.470, 10.098], loss: 0.002600, mae: 0.050638, mean_q: -0.276561
 42400/100000: episode: 424, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.588, mean reward: -0.176 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.559, 10.248], loss: 0.003081, mae: 0.056500, mean_q: -0.299374
 42500/100000: episode: 425, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.916, mean reward: -0.189 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.559, 10.098], loss: 0.002654, mae: 0.051734, mean_q: -0.316904
 42600/100000: episode: 426, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.871, mean reward: -0.179 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.229, 10.123], loss: 0.002553, mae: 0.050249, mean_q: -0.313795
 42700/100000: episode: 427, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.506, mean reward: -0.175 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.356, 10.098], loss: 0.002561, mae: 0.049614, mean_q: -0.311298
 42800/100000: episode: 428, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -19.656, mean reward: -0.197 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.978, 10.199], loss: 0.002683, mae: 0.051368, mean_q: -0.307252
 42900/100000: episode: 429, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.979, mean reward: -0.160 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.505, 10.281], loss: 0.002605, mae: 0.050084, mean_q: -0.292739
 43000/100000: episode: 430, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.480, mean reward: -0.175 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.118, 10.098], loss: 0.002773, mae: 0.050859, mean_q: -0.354021
 43100/100000: episode: 431, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -19.961, mean reward: -0.200 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.961, 10.098], loss: 0.002696, mae: 0.051308, mean_q: -0.286059
 43200/100000: episode: 432, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -15.181, mean reward: -0.152 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.990, 10.273], loss: 0.002684, mae: 0.050834, mean_q: -0.297724
 43300/100000: episode: 433, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.401, mean reward: -0.194 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.930, 10.122], loss: 0.002490, mae: 0.049634, mean_q: -0.369437
 43400/100000: episode: 434, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.205, mean reward: -0.182 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.693, 10.098], loss: 0.002654, mae: 0.050866, mean_q: -0.315943
 43500/100000: episode: 435, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.357, mean reward: -0.174 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.067, 10.243], loss: 0.002697, mae: 0.050824, mean_q: -0.317765
 43600/100000: episode: 436, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -14.534, mean reward: -0.145 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.306, 10.098], loss: 0.002283, mae: 0.046740, mean_q: -0.327446
 43700/100000: episode: 437, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.896, mean reward: -0.189 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.683, 10.098], loss: 0.002563, mae: 0.049442, mean_q: -0.313954
 43800/100000: episode: 438, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -12.325, mean reward: -0.123 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.233, 10.184], loss: 0.002563, mae: 0.050678, mean_q: -0.331682
 43900/100000: episode: 439, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -14.753, mean reward: -0.148 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.490, 10.098], loss: 0.002557, mae: 0.049771, mean_q: -0.337412
 44000/100000: episode: 440, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.872, mean reward: -0.149 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.402, 10.098], loss: 0.002613, mae: 0.049921, mean_q: -0.323942
 44100/100000: episode: 441, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.951, mean reward: -0.170 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.861, 10.345], loss: 0.002707, mae: 0.049950, mean_q: -0.354001
 44200/100000: episode: 442, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.017, mean reward: -0.170 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.979, 10.137], loss: 0.002728, mae: 0.051153, mean_q: -0.305275
 44300/100000: episode: 443, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.139, mean reward: -0.191 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.746, 10.098], loss: 0.002866, mae: 0.053062, mean_q: -0.318215
 44400/100000: episode: 444, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.763, mean reward: -0.168 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.526, 10.098], loss: 0.002776, mae: 0.052034, mean_q: -0.290209
 44500/100000: episode: 445, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.313, mean reward: -0.153 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.668, 10.169], loss: 0.002705, mae: 0.051649, mean_q: -0.295009
 44600/100000: episode: 446, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -20.882, mean reward: -0.209 [-1.000, 0.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.162, 10.098], loss: 0.003009, mae: 0.054606, mean_q: -0.339308
 44700/100000: episode: 447, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.334, mean reward: -0.183 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.953, 10.103], loss: 0.002668, mae: 0.051493, mean_q: -0.317877
 44800/100000: episode: 448, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.957, mean reward: -0.180 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.209, 10.104], loss: 0.002725, mae: 0.051259, mean_q: -0.338872
 44900/100000: episode: 449, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -10.446, mean reward: -0.104 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.284, 10.170], loss: 0.003376, mae: 0.056941, mean_q: -0.326618
 45000/100000: episode: 450, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.514, mean reward: -0.185 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.753, 10.098], loss: 0.004663, mae: 0.067129, mean_q: -0.289447
 45100/100000: episode: 451, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.022, mean reward: -0.170 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.481, 10.168], loss: 0.002567, mae: 0.050553, mean_q: -0.303158
 45200/100000: episode: 452, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -13.232, mean reward: -0.132 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.915, 10.416], loss: 0.002814, mae: 0.051830, mean_q: -0.311922
 45300/100000: episode: 453, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.531, mean reward: -0.165 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.783, 10.228], loss: 0.002598, mae: 0.050005, mean_q: -0.294552
 45400/100000: episode: 454, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.877, mean reward: -0.169 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.830, 10.098], loss: 0.002584, mae: 0.049339, mean_q: -0.334342
 45500/100000: episode: 455, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -14.926, mean reward: -0.149 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.966, 10.190], loss: 0.002684, mae: 0.050223, mean_q: -0.314768
 45600/100000: episode: 456, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.275, mean reward: -0.153 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.295, 10.140], loss: 0.002769, mae: 0.051596, mean_q: -0.334615
 45700/100000: episode: 457, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -15.191, mean reward: -0.152 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.916, 10.205], loss: 0.002533, mae: 0.049013, mean_q: -0.318707
 45800/100000: episode: 458, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.645, mean reward: -0.196 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.309, 10.098], loss: 0.002652, mae: 0.050492, mean_q: -0.326574
 45900/100000: episode: 459, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.202, mean reward: -0.152 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.766, 10.251], loss: 0.002743, mae: 0.051750, mean_q: -0.324036
 46000/100000: episode: 460, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.129, mean reward: -0.171 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.859, 10.186], loss: 0.002897, mae: 0.052421, mean_q: -0.299550
 46100/100000: episode: 461, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.899, mean reward: -0.159 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.858, 10.139], loss: 0.002522, mae: 0.049089, mean_q: -0.318605
 46200/100000: episode: 462, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.865, mean reward: -0.169 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.621, 10.098], loss: 0.002537, mae: 0.050609, mean_q: -0.331070
 46300/100000: episode: 463, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.904, mean reward: -0.169 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.279, 10.098], loss: 0.003649, mae: 0.058732, mean_q: -0.298673
 46400/100000: episode: 464, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.474, mean reward: -0.195 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.655, 10.102], loss: 0.002747, mae: 0.052878, mean_q: -0.287568
 46500/100000: episode: 465, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.636, mean reward: -0.186 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.356, 10.098], loss: 0.002503, mae: 0.048486, mean_q: -0.337233
 46600/100000: episode: 466, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -13.772, mean reward: -0.138 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.779, 10.098], loss: 0.002606, mae: 0.049020, mean_q: -0.353523
 46700/100000: episode: 467, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -13.073, mean reward: -0.131 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.900, 10.382], loss: 0.002437, mae: 0.047969, mean_q: -0.322790
 46800/100000: episode: 468, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.727, mean reward: -0.187 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.274, 10.098], loss: 0.002744, mae: 0.051301, mean_q: -0.300805
 46900/100000: episode: 469, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.867, mean reward: -0.169 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.237, 10.500], loss: 0.002390, mae: 0.047642, mean_q: -0.319672
 47000/100000: episode: 470, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.295, mean reward: -0.173 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.789, 10.236], loss: 0.002578, mae: 0.050274, mean_q: -0.300416
 47100/100000: episode: 471, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.546, mean reward: -0.175 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.223, 10.229], loss: 0.004850, mae: 0.061049, mean_q: -0.318386
 47200/100000: episode: 472, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.179, mean reward: -0.172 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.563, 10.279], loss: 0.003523, mae: 0.059823, mean_q: -0.304802
 47300/100000: episode: 473, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.098, mean reward: -0.191 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.726, 10.098], loss: 0.002880, mae: 0.053242, mean_q: -0.308346
 47400/100000: episode: 474, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.320, mean reward: -0.173 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.108, 10.098], loss: 0.002691, mae: 0.050601, mean_q: -0.325962
 47500/100000: episode: 475, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -11.905, mean reward: -0.119 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.258, 10.344], loss: 0.002569, mae: 0.049024, mean_q: -0.331159
 47600/100000: episode: 476, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.383, mean reward: -0.194 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.118, 10.366], loss: 0.002523, mae: 0.048958, mean_q: -0.327301
 47700/100000: episode: 477, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.975, mean reward: -0.180 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.454, 10.268], loss: 0.002773, mae: 0.051072, mean_q: -0.333326
 47800/100000: episode: 478, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.086, mean reward: -0.191 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.439, 10.206], loss: 0.002485, mae: 0.047997, mean_q: -0.335268
 47900/100000: episode: 479, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -16.441, mean reward: -0.164 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.637, 10.105], loss: 0.002520, mae: 0.049925, mean_q: -0.308091
 48000/100000: episode: 480, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.393, mean reward: -0.164 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.616, 10.149], loss: 0.002690, mae: 0.049764, mean_q: -0.347584
 48100/100000: episode: 481, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -17.743, mean reward: -0.177 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.188, 10.098], loss: 0.002668, mae: 0.051194, mean_q: -0.345411
 48200/100000: episode: 482, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.747, mean reward: -0.197 [-1.000, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.800, 10.280], loss: 0.002507, mae: 0.048396, mean_q: -0.350430
 48300/100000: episode: 483, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -20.148, mean reward: -0.201 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.217, 10.261], loss: 0.002955, mae: 0.055084, mean_q: -0.276280
 48400/100000: episode: 484, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.714, mean reward: -0.187 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.088, 10.098], loss: 0.002704, mae: 0.050828, mean_q: -0.329849
 48500/100000: episode: 485, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.793, mean reward: -0.198 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.998, 10.214], loss: 0.002816, mae: 0.051558, mean_q: -0.316249
 48600/100000: episode: 486, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.828, mean reward: -0.188 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.770, 10.168], loss: 0.002768, mae: 0.051537, mean_q: -0.295033
 48700/100000: episode: 487, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.211, mean reward: -0.162 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.416, 10.098], loss: 0.002745, mae: 0.051447, mean_q: -0.342465
 48800/100000: episode: 488, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.788, mean reward: -0.198 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.947, 10.098], loss: 0.005228, mae: 0.065677, mean_q: -0.328052
 48900/100000: episode: 489, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.782, mean reward: -0.158 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.082, 10.305], loss: 0.002774, mae: 0.052620, mean_q: -0.324094
 49000/100000: episode: 490, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.323, mean reward: -0.183 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.528, 10.314], loss: 0.002613, mae: 0.051932, mean_q: -0.307779
 49100/100000: episode: 491, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -9.752, mean reward: -0.098 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.789, 10.371], loss: 0.003665, mae: 0.056222, mean_q: -0.319727
 49200/100000: episode: 492, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.724, mean reward: -0.197 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.153, 10.117], loss: 0.008038, mae: 0.074990, mean_q: -0.357493
 49300/100000: episode: 493, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.345, mean reward: -0.143 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.971, 10.118], loss: 0.002632, mae: 0.051786, mean_q: -0.307271
 49400/100000: episode: 494, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.289, mean reward: -0.183 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.594, 10.332], loss: 0.003132, mae: 0.055724, mean_q: -0.316953
 49500/100000: episode: 495, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.624, mean reward: -0.156 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.145, 10.426], loss: 0.002542, mae: 0.049766, mean_q: -0.298780
 49600/100000: episode: 496, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -11.790, mean reward: -0.118 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.755, 10.412], loss: 0.002355, mae: 0.047845, mean_q: -0.314123
 49700/100000: episode: 497, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.026, mean reward: -0.160 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.689, 10.098], loss: 0.002398, mae: 0.048198, mean_q: -0.297680
 49800/100000: episode: 498, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -19.791, mean reward: -0.198 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.090, 10.125], loss: 0.004452, mae: 0.061950, mean_q: -0.293692
 49900/100000: episode: 499, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -20.940, mean reward: -0.209 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.775, 10.169], loss: 0.002435, mae: 0.049324, mean_q: -0.300585
 50000/100000: episode: 500, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -19.978, mean reward: -0.200 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.158, 10.131], loss: 0.002498, mae: 0.048997, mean_q: -0.336331
 50100/100000: episode: 501, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.686, mean reward: -0.167 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.423, 10.133], loss: 0.002667, mae: 0.051188, mean_q: -0.337109
 50200/100000: episode: 502, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.923, mean reward: -0.169 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.058, 10.098], loss: 0.002481, mae: 0.048246, mean_q: -0.361319
 50300/100000: episode: 503, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.517, mean reward: -0.155 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.084, 10.166], loss: 0.002744, mae: 0.051427, mean_q: -0.334415
 50400/100000: episode: 504, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -14.003, mean reward: -0.140 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.434, 10.098], loss: 0.002477, mae: 0.049430, mean_q: -0.311799
 50500/100000: episode: 505, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.666, mean reward: -0.177 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.520, 10.098], loss: 0.002594, mae: 0.050302, mean_q: -0.297867
 50600/100000: episode: 506, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -20.393, mean reward: -0.204 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.747, 10.098], loss: 0.002283, mae: 0.047363, mean_q: -0.322493
 50700/100000: episode: 507, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.885, mean reward: -0.189 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.710, 10.098], loss: 0.002378, mae: 0.047910, mean_q: -0.350398
 50800/100000: episode: 508, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -16.702, mean reward: -0.167 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.488, 10.098], loss: 0.002548, mae: 0.049546, mean_q: -0.348691
 50900/100000: episode: 509, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.692, mean reward: -0.167 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.320, 10.098], loss: 0.004549, mae: 0.060345, mean_q: -0.348403
 51000/100000: episode: 510, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.159, mean reward: -0.172 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.597, 10.352], loss: 0.002859, mae: 0.054173, mean_q: -0.360094
 51100/100000: episode: 511, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.727, mean reward: -0.177 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.814, 10.138], loss: 0.003124, mae: 0.052469, mean_q: -0.338670
 51200/100000: episode: 512, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -20.466, mean reward: -0.205 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.341, 10.098], loss: 0.002444, mae: 0.048725, mean_q: -0.349437
 51300/100000: episode: 513, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -19.324, mean reward: -0.193 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.097, 10.151], loss: 0.002657, mae: 0.049422, mean_q: -0.362129
 51400/100000: episode: 514, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.656, mean reward: -0.177 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.365, 10.112], loss: 0.002607, mae: 0.051618, mean_q: -0.349495
 51500/100000: episode: 515, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -17.666, mean reward: -0.177 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.175, 10.098], loss: 0.002697, mae: 0.051123, mean_q: -0.325094
 51600/100000: episode: 516, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.772, mean reward: -0.178 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.534, 10.098], loss: 0.002498, mae: 0.049121, mean_q: -0.348226
 51700/100000: episode: 517, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.577, mean reward: -0.176 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.320, 10.098], loss: 0.002500, mae: 0.049220, mean_q: -0.364135
 51800/100000: episode: 518, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.452, mean reward: -0.195 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.741, 10.318], loss: 0.002567, mae: 0.050317, mean_q: -0.341146
 51900/100000: episode: 519, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.501, mean reward: -0.155 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.850, 10.098], loss: 0.002645, mae: 0.051103, mean_q: -0.332877
 52000/100000: episode: 520, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -9.503, mean reward: -0.095 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.866, 10.285], loss: 0.002660, mae: 0.050129, mean_q: -0.326950
 52100/100000: episode: 521, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -19.026, mean reward: -0.190 [-1.000, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.477, 10.140], loss: 0.002549, mae: 0.049577, mean_q: -0.321350
 52200/100000: episode: 522, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.188, mean reward: -0.162 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.686, 10.098], loss: 0.002643, mae: 0.051441, mean_q: -0.316239
 52300/100000: episode: 523, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.518, mean reward: -0.185 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.462, 10.272], loss: 0.002677, mae: 0.051342, mean_q: -0.329114
 52400/100000: episode: 524, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -20.135, mean reward: -0.201 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.827, 10.187], loss: 0.002389, mae: 0.047987, mean_q: -0.318750
 52500/100000: episode: 525, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.779, mean reward: -0.168 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.999, 10.098], loss: 0.002495, mae: 0.049376, mean_q: -0.342940
 52600/100000: episode: 526, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.234, mean reward: -0.162 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.962, 10.098], loss: 0.002422, mae: 0.048216, mean_q: -0.353813
 52700/100000: episode: 527, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.271, mean reward: -0.173 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.885, 10.098], loss: 0.002433, mae: 0.048122, mean_q: -0.331329
 52800/100000: episode: 528, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -20.354, mean reward: -0.204 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.895, 10.098], loss: 0.002469, mae: 0.050067, mean_q: -0.301119
 52900/100000: episode: 529, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.484, mean reward: -0.155 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.932, 10.098], loss: 0.002574, mae: 0.049820, mean_q: -0.336697
 53000/100000: episode: 530, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.863, mean reward: -0.179 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.073, 10.098], loss: 0.002450, mae: 0.048096, mean_q: -0.363177
 53100/100000: episode: 531, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.856, mean reward: -0.169 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.625, 10.174], loss: 0.002430, mae: 0.047969, mean_q: -0.330542
 53200/100000: episode: 532, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.805, mean reward: -0.178 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.966, 10.253], loss: 0.002704, mae: 0.052225, mean_q: -0.331261
 53300/100000: episode: 533, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.573, mean reward: -0.166 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.052, 10.142], loss: 0.002413, mae: 0.049065, mean_q: -0.302126
 53400/100000: episode: 534, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.773, mean reward: -0.158 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.721, 10.098], loss: 0.002621, mae: 0.051005, mean_q: -0.314852
 53500/100000: episode: 535, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -20.202, mean reward: -0.202 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.003, 10.215], loss: 0.002658, mae: 0.050365, mean_q: -0.358564
 53600/100000: episode: 536, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.780, mean reward: -0.178 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.354, 10.179], loss: 0.002856, mae: 0.053103, mean_q: -0.288326
 53700/100000: episode: 537, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.212, mean reward: -0.152 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.742, 10.098], loss: 0.002750, mae: 0.050731, mean_q: -0.328796
 53800/100000: episode: 538, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.345, mean reward: -0.183 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.730, 10.304], loss: 0.002878, mae: 0.053813, mean_q: -0.293701
 53900/100000: episode: 539, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.385, mean reward: -0.174 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.912, 10.098], loss: 0.002656, mae: 0.053108, mean_q: -0.304951
 54000/100000: episode: 540, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.481, mean reward: -0.185 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.240, 10.098], loss: 0.002815, mae: 0.052272, mean_q: -0.310553
 54100/100000: episode: 541, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.584, mean reward: -0.166 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.024, 10.178], loss: 0.002661, mae: 0.050435, mean_q: -0.309748
 54200/100000: episode: 542, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.009, mean reward: -0.180 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.806, 10.142], loss: 0.002609, mae: 0.049622, mean_q: -0.328785
 54300/100000: episode: 543, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.968, mean reward: -0.170 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.502, 10.098], loss: 0.002442, mae: 0.049151, mean_q: -0.325474
 54400/100000: episode: 544, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.739, mean reward: -0.177 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.736, 10.339], loss: 0.002574, mae: 0.050080, mean_q: -0.340205
 54500/100000: episode: 545, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.466, mean reward: -0.175 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.489, 10.098], loss: 0.002630, mae: 0.050179, mean_q: -0.352301
 54600/100000: episode: 546, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.118, mean reward: -0.171 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.469, 10.098], loss: 0.002650, mae: 0.050875, mean_q: -0.316890
 54700/100000: episode: 547, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.417, mean reward: -0.184 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.667, 10.098], loss: 0.002728, mae: 0.051376, mean_q: -0.327043
 54800/100000: episode: 548, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -13.348, mean reward: -0.133 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.595, 10.374], loss: 0.006543, mae: 0.071541, mean_q: -0.345257
 54900/100000: episode: 549, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -19.946, mean reward: -0.199 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.656, 10.098], loss: 0.002651, mae: 0.052763, mean_q: -0.304199
 55000/100000: episode: 550, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.063, mean reward: -0.181 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.434, 10.279], loss: 0.002810, mae: 0.053261, mean_q: -0.340252
 55100/100000: episode: 551, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.950, mean reward: -0.150 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.960, 10.142], loss: 0.002573, mae: 0.050859, mean_q: -0.320640
 55200/100000: episode: 552, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -14.184, mean reward: -0.142 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.410, 10.098], loss: 0.002611, mae: 0.050122, mean_q: -0.332959
 55300/100000: episode: 553, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -12.565, mean reward: -0.126 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.547, 10.098], loss: 0.002560, mae: 0.049457, mean_q: -0.331093
 55400/100000: episode: 554, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.918, mean reward: -0.179 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.140, 10.098], loss: 0.002658, mae: 0.050923, mean_q: -0.307587
 55500/100000: episode: 555, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.200, mean reward: -0.182 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.121, 10.196], loss: 0.002619, mae: 0.050251, mean_q: -0.294468
 55600/100000: episode: 556, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.151, mean reward: -0.172 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.962, 10.253], loss: 0.002580, mae: 0.048735, mean_q: -0.372129
 55700/100000: episode: 557, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.597, mean reward: -0.186 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.398, 10.098], loss: 0.002632, mae: 0.050153, mean_q: -0.320440
 55800/100000: episode: 558, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -10.857, mean reward: -0.109 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.548, 10.392], loss: 0.002766, mae: 0.051551, mean_q: -0.332413
 55900/100000: episode: 559, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.167, mean reward: -0.192 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.146, 10.143], loss: 0.002496, mae: 0.049562, mean_q: -0.325953
 56000/100000: episode: 560, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -20.823, mean reward: -0.208 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.623, 10.112], loss: 0.002524, mae: 0.049866, mean_q: -0.327749
 56100/100000: episode: 561, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -17.367, mean reward: -0.174 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.254, 10.232], loss: 0.002924, mae: 0.053334, mean_q: -0.315136
 56200/100000: episode: 562, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.247, mean reward: -0.182 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.573, 10.098], loss: 0.002729, mae: 0.052081, mean_q: -0.303313
 56300/100000: episode: 563, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.287, mean reward: -0.203 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.772, 10.250], loss: 0.002889, mae: 0.052408, mean_q: -0.318623
 56400/100000: episode: 564, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.208, mean reward: -0.162 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.492, 10.098], loss: 0.002583, mae: 0.049577, mean_q: -0.338021
 56500/100000: episode: 565, duration: 0.519s, episode steps: 100, steps per second: 192, episode reward: -18.926, mean reward: -0.189 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.905, 10.098], loss: 0.003195, mae: 0.054581, mean_q: -0.324595
 56600/100000: episode: 566, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -11.953, mean reward: -0.120 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.929, 10.098], loss: 0.003638, mae: 0.058335, mean_q: -0.343831
 56700/100000: episode: 567, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.507, mean reward: -0.185 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.422, 10.154], loss: 0.003362, mae: 0.056176, mean_q: -0.328589
 56800/100000: episode: 568, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.109, mean reward: -0.171 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.430, 10.098], loss: 0.002707, mae: 0.051015, mean_q: -0.283521
 56900/100000: episode: 569, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -14.807, mean reward: -0.148 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.580, 10.098], loss: 0.002543, mae: 0.048987, mean_q: -0.339310
 57000/100000: episode: 570, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.504, mean reward: -0.155 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.872, 10.098], loss: 0.002451, mae: 0.047554, mean_q: -0.356786
 57100/100000: episode: 571, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.661, mean reward: -0.137 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.089, 10.270], loss: 0.002607, mae: 0.049687, mean_q: -0.310517
 57200/100000: episode: 572, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.992, mean reward: -0.160 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.285, 10.388], loss: 0.002685, mae: 0.050881, mean_q: -0.320088
 57300/100000: episode: 573, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -11.059, mean reward: -0.111 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.662, 10.209], loss: 0.002517, mae: 0.048914, mean_q: -0.324314
 57400/100000: episode: 574, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -20.831, mean reward: -0.208 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.866, 10.098], loss: 0.002669, mae: 0.050599, mean_q: -0.318461
 57500/100000: episode: 575, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.844, mean reward: -0.158 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.814, 10.501], loss: 0.002514, mae: 0.049520, mean_q: -0.311978
 57600/100000: episode: 576, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.770, mean reward: -0.198 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.264, 10.098], loss: 0.002679, mae: 0.051365, mean_q: -0.324646
 57700/100000: episode: 577, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.766, mean reward: -0.178 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.901, 10.098], loss: 0.002518, mae: 0.049660, mean_q: -0.318537
 57800/100000: episode: 578, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.507, mean reward: -0.155 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.301, 10.098], loss: 0.002449, mae: 0.048770, mean_q: -0.331492
 57900/100000: episode: 579, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.846, mean reward: -0.168 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.513, 10.098], loss: 0.002605, mae: 0.050648, mean_q: -0.331637
 58000/100000: episode: 580, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -13.474, mean reward: -0.135 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.468, 10.098], loss: 0.002540, mae: 0.049707, mean_q: -0.300470
 58100/100000: episode: 581, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.738, mean reward: -0.187 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.406, 10.098], loss: 0.002500, mae: 0.050012, mean_q: -0.314314
 58200/100000: episode: 582, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -13.846, mean reward: -0.138 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.889, 10.098], loss: 0.002344, mae: 0.048308, mean_q: -0.339640
 58300/100000: episode: 583, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -20.839, mean reward: -0.208 [-1.000, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.673, 10.208], loss: 0.002325, mae: 0.047330, mean_q: -0.324352
 58400/100000: episode: 584, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.348, mean reward: -0.193 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.530, 10.105], loss: 0.002313, mae: 0.047471, mean_q: -0.314363
 58500/100000: episode: 585, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -9.356, mean reward: -0.094 [-1.000, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.451, 10.098], loss: 0.002394, mae: 0.048133, mean_q: -0.311442
 58600/100000: episode: 586, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.974, mean reward: -0.190 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.768, 10.098], loss: 0.002227, mae: 0.046866, mean_q: -0.327510
 58700/100000: episode: 587, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.807, mean reward: -0.158 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.967, 10.295], loss: 0.002177, mae: 0.045718, mean_q: -0.307704
 58800/100000: episode: 588, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.566, mean reward: -0.176 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.895, 10.312], loss: 0.002468, mae: 0.049468, mean_q: -0.286360
 58900/100000: episode: 589, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.576, mean reward: -0.176 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.335, 10.175], loss: 0.002472, mae: 0.048448, mean_q: -0.292490
 59000/100000: episode: 590, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -20.149, mean reward: -0.201 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.431, 10.119], loss: 0.002430, mae: 0.048129, mean_q: -0.325364
 59100/100000: episode: 591, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -20.808, mean reward: -0.208 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.837, 10.147], loss: 0.002425, mae: 0.047674, mean_q: -0.319184
 59200/100000: episode: 592, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -19.556, mean reward: -0.196 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.439, 10.098], loss: 0.002358, mae: 0.047790, mean_q: -0.327149
 59300/100000: episode: 593, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -11.754, mean reward: -0.118 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.940, 10.098], loss: 0.002324, mae: 0.047803, mean_q: -0.303383
 59400/100000: episode: 594, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.586, mean reward: -0.186 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.700, 10.206], loss: 0.002385, mae: 0.048329, mean_q: -0.311440
 59500/100000: episode: 595, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -9.731, mean reward: -0.097 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.741, 10.193], loss: 0.002330, mae: 0.047451, mean_q: -0.317004
 59600/100000: episode: 596, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -13.970, mean reward: -0.140 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.301, 10.098], loss: 0.002462, mae: 0.049399, mean_q: -0.310972
 59700/100000: episode: 597, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -15.022, mean reward: -0.150 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.956, 10.122], loss: 0.002414, mae: 0.048767, mean_q: -0.311293
 59800/100000: episode: 598, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.686, mean reward: -0.187 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.652, 10.098], loss: 0.002546, mae: 0.049361, mean_q: -0.313917
 59900/100000: episode: 599, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -11.222, mean reward: -0.112 [-1.000, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.520, 10.098], loss: 0.007807, mae: 0.075959, mean_q: -0.298599
 60000/100000: episode: 600, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.715, mean reward: -0.177 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.957, 10.151], loss: 0.004292, mae: 0.062430, mean_q: -0.329750
 60100/100000: episode: 601, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.280, mean reward: -0.183 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.865, 10.098], loss: 0.002453, mae: 0.048928, mean_q: -0.316005
 60200/100000: episode: 602, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.676, mean reward: -0.197 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.826, 10.098], loss: 0.002394, mae: 0.047204, mean_q: -0.328332
 60300/100000: episode: 603, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.545, mean reward: -0.165 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.546, 10.153], loss: 0.002460, mae: 0.048738, mean_q: -0.306701
 60400/100000: episode: 604, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.765, mean reward: -0.178 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.892, 10.150], loss: 0.002484, mae: 0.048405, mean_q: -0.305773
 60500/100000: episode: 605, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.111, mean reward: -0.191 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.456, 10.215], loss: 0.002495, mae: 0.049157, mean_q: -0.301626
 60600/100000: episode: 606, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.877, mean reward: -0.189 [-1.000, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.986, 10.098], loss: 0.002287, mae: 0.046724, mean_q: -0.324533
 60700/100000: episode: 607, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.207, mean reward: -0.162 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.889, 10.300], loss: 0.002480, mae: 0.048224, mean_q: -0.305005
 60800/100000: episode: 608, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.536, mean reward: -0.185 [-1.000, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.923, 10.130], loss: 0.002254, mae: 0.046728, mean_q: -0.301257
 60900/100000: episode: 609, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.025, mean reward: -0.170 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.984, 10.098], loss: 0.002211, mae: 0.046092, mean_q: -0.319106
 61000/100000: episode: 610, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -13.747, mean reward: -0.137 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.919, 10.107], loss: 0.002458, mae: 0.048255, mean_q: -0.313092
 61100/100000: episode: 611, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.318, mean reward: -0.183 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.689, 10.163], loss: 0.002278, mae: 0.046483, mean_q: -0.341132
 61200/100000: episode: 612, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.024, mean reward: -0.180 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.224, 10.380], loss: 0.002272, mae: 0.046421, mean_q: -0.335668
 61300/100000: episode: 613, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.121, mean reward: -0.151 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.534, 10.098], loss: 0.002369, mae: 0.047100, mean_q: -0.339240
 61400/100000: episode: 614, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -14.820, mean reward: -0.148 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.823, 10.098], loss: 0.002341, mae: 0.047481, mean_q: -0.312183
 61500/100000: episode: 615, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -18.957, mean reward: -0.190 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.510, 10.138], loss: 0.002438, mae: 0.049182, mean_q: -0.299219
 61600/100000: episode: 616, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -15.974, mean reward: -0.160 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.453, 10.098], loss: 0.002313, mae: 0.046933, mean_q: -0.340482
 61700/100000: episode: 617, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.476, mean reward: -0.135 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.671, 10.098], loss: 0.002488, mae: 0.049501, mean_q: -0.300162
 61800/100000: episode: 618, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -13.780, mean reward: -0.138 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.117, 10.266], loss: 0.002446, mae: 0.049752, mean_q: -0.301424
 61900/100000: episode: 619, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.881, mean reward: -0.189 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.388, 10.109], loss: 0.002308, mae: 0.048064, mean_q: -0.306716
 62000/100000: episode: 620, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.929, mean reward: -0.159 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.655, 10.098], loss: 0.002403, mae: 0.048013, mean_q: -0.288919
 62100/100000: episode: 621, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.151, mean reward: -0.182 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.424, 10.234], loss: 0.002393, mae: 0.047594, mean_q: -0.341437
 62200/100000: episode: 622, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.173, mean reward: -0.192 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.185, 10.155], loss: 0.002410, mae: 0.049148, mean_q: -0.345730
 62300/100000: episode: 623, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -14.804, mean reward: -0.148 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.962, 10.268], loss: 0.002519, mae: 0.049477, mean_q: -0.298261
 62400/100000: episode: 624, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.714, mean reward: -0.137 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.519, 10.201], loss: 0.002443, mae: 0.049026, mean_q: -0.300682
 62500/100000: episode: 625, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -12.476, mean reward: -0.125 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.649, 10.438], loss: 0.002514, mae: 0.049756, mean_q: -0.331662
 62600/100000: episode: 626, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -11.919, mean reward: -0.119 [-1.000, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.860, 10.463], loss: 0.002520, mae: 0.050165, mean_q: -0.304939
 62700/100000: episode: 627, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.476, mean reward: -0.155 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.026, 10.307], loss: 0.002372, mae: 0.047131, mean_q: -0.339264
 62800/100000: episode: 628, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.493, mean reward: -0.155 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.989, 10.352], loss: 0.002594, mae: 0.051294, mean_q: -0.299053
 62900/100000: episode: 629, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.011, mean reward: -0.180 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.520, 10.098], loss: 0.002455, mae: 0.047687, mean_q: -0.313373
 63000/100000: episode: 630, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -13.375, mean reward: -0.134 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.292, 10.098], loss: 0.002295, mae: 0.047237, mean_q: -0.325207
 63100/100000: episode: 631, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.183, mean reward: -0.182 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.136, 10.190], loss: 0.002539, mae: 0.049856, mean_q: -0.298286
 63200/100000: episode: 632, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.705, mean reward: -0.197 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.155, 10.098], loss: 0.002436, mae: 0.047846, mean_q: -0.329452
 63300/100000: episode: 633, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -17.855, mean reward: -0.179 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.879, 10.285], loss: 0.002441, mae: 0.048139, mean_q: -0.307543
 63400/100000: episode: 634, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.500, mean reward: -0.165 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.067, 10.098], loss: 0.002516, mae: 0.050645, mean_q: -0.291792
 63500/100000: episode: 635, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.417, mean reward: -0.154 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.376, 10.098], loss: 0.002367, mae: 0.047133, mean_q: -0.347219
 63600/100000: episode: 636, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -13.958, mean reward: -0.140 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.889, 10.151], loss: 0.002549, mae: 0.051507, mean_q: -0.302649
 63700/100000: episode: 637, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.354, mean reward: -0.184 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.043, 10.101], loss: 0.002518, mae: 0.052375, mean_q: -0.334790
 63800/100000: episode: 638, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.386, mean reward: -0.154 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.511, 10.244], loss: 0.002564, mae: 0.049241, mean_q: -0.280300
 63900/100000: episode: 639, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.840, mean reward: -0.178 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.306, 10.313], loss: 0.002427, mae: 0.048603, mean_q: -0.296467
 64000/100000: episode: 640, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.819, mean reward: -0.178 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.252, 10.206], loss: 0.002325, mae: 0.048132, mean_q: -0.317315
 64100/100000: episode: 641, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.758, mean reward: -0.148 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.089, 10.098], loss: 0.002589, mae: 0.051123, mean_q: -0.273098
 64200/100000: episode: 642, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.424, mean reward: -0.154 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.085, 10.232], loss: 0.002706, mae: 0.051671, mean_q: -0.317347
 64300/100000: episode: 643, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.027, mean reward: -0.180 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.022, 10.098], loss: 0.002578, mae: 0.050012, mean_q: -0.298488
 64400/100000: episode: 644, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -13.629, mean reward: -0.136 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.707, 10.098], loss: 0.002461, mae: 0.048697, mean_q: -0.314791
 64500/100000: episode: 645, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.668, mean reward: -0.177 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.076, 10.149], loss: 0.002591, mae: 0.049228, mean_q: -0.325266
 64600/100000: episode: 646, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.087, mean reward: -0.171 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.096, 10.098], loss: 0.002608, mae: 0.050639, mean_q: -0.300340
 64700/100000: episode: 647, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -16.515, mean reward: -0.165 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.259, 10.098], loss: 0.002549, mae: 0.050874, mean_q: -0.291465
 64800/100000: episode: 648, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.792, mean reward: -0.168 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.309, 10.098], loss: 0.003443, mae: 0.057127, mean_q: -0.295873
 64900/100000: episode: 649, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -15.039, mean reward: -0.150 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.649, 10.177], loss: 0.002592, mae: 0.051408, mean_q: -0.315350
 65000/100000: episode: 650, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.329, mean reward: -0.193 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.673, 10.098], loss: 0.002563, mae: 0.049282, mean_q: -0.308948
 65100/100000: episode: 651, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.665, mean reward: -0.167 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.094, 10.098], loss: 0.002516, mae: 0.050162, mean_q: -0.307170
 65200/100000: episode: 652, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.999, mean reward: -0.190 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.159, 10.205], loss: 0.003170, mae: 0.056191, mean_q: -0.292303
 65300/100000: episode: 653, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.579, mean reward: -0.176 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.435, 10.098], loss: 0.002493, mae: 0.048566, mean_q: -0.313705
 65400/100000: episode: 654, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -9.138, mean reward: -0.091 [-1.000, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.584, 10.260], loss: 0.002586, mae: 0.051024, mean_q: -0.290698
 65500/100000: episode: 655, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.159, mean reward: -0.182 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.697, 10.290], loss: 0.002287, mae: 0.046648, mean_q: -0.362878
 65600/100000: episode: 656, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.996, mean reward: -0.180 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.329, 10.132], loss: 0.002683, mae: 0.051234, mean_q: -0.296209
 65700/100000: episode: 657, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -20.810, mean reward: -0.208 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.664, 10.365], loss: 0.002654, mae: 0.051122, mean_q: -0.296926
 65800/100000: episode: 658, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.141, mean reward: -0.161 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.869, 10.098], loss: 0.002959, mae: 0.053691, mean_q: -0.291473
 65900/100000: episode: 659, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.646, mean reward: -0.166 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.918, 10.098], loss: 0.002569, mae: 0.049383, mean_q: -0.290322
 66000/100000: episode: 660, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.280, mean reward: -0.173 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.819, 10.199], loss: 0.002576, mae: 0.049994, mean_q: -0.308325
 66100/100000: episode: 661, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.314, mean reward: -0.153 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.655, 10.484], loss: 0.002527, mae: 0.049293, mean_q: -0.294875
 66200/100000: episode: 662, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.091, mean reward: -0.131 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.862, 10.389], loss: 0.002879, mae: 0.052936, mean_q: -0.265331
 66300/100000: episode: 663, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -19.533, mean reward: -0.195 [-1.000, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.398, 10.237], loss: 0.002493, mae: 0.048542, mean_q: -0.299544
 66400/100000: episode: 664, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -20.026, mean reward: -0.200 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.154, 10.098], loss: 0.002774, mae: 0.051216, mean_q: -0.280553
 66500/100000: episode: 665, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -13.185, mean reward: -0.132 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.642, 10.261], loss: 0.002735, mae: 0.051767, mean_q: -0.320047
 66600/100000: episode: 666, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.675, mean reward: -0.197 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.651, 10.098], loss: 0.002665, mae: 0.051339, mean_q: -0.295895
 66700/100000: episode: 667, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.416, mean reward: -0.184 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.507, 10.106], loss: 0.002776, mae: 0.054332, mean_q: -0.300585
 66800/100000: episode: 668, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.765, mean reward: -0.158 [-1.000, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.237, 10.491], loss: 0.002567, mae: 0.049986, mean_q: -0.279713
 66900/100000: episode: 669, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.009, mean reward: -0.140 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.094, 10.384], loss: 0.002661, mae: 0.050345, mean_q: -0.313849
 67000/100000: episode: 670, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.367, mean reward: -0.154 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.628, 10.231], loss: 0.002545, mae: 0.048426, mean_q: -0.292738
 67100/100000: episode: 671, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -17.787, mean reward: -0.178 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.354, 10.098], loss: 0.002508, mae: 0.049113, mean_q: -0.276746
 67200/100000: episode: 672, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -19.205, mean reward: -0.192 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.897, 10.098], loss: 0.002638, mae: 0.049395, mean_q: -0.309503
 67300/100000: episode: 673, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.595, mean reward: -0.196 [-1.000, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.111, 10.098], loss: 0.002801, mae: 0.051526, mean_q: -0.303585
 67400/100000: episode: 674, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.194, mean reward: -0.152 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.939, 10.188], loss: 0.003893, mae: 0.054294, mean_q: -0.268395
 67500/100000: episode: 675, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.896, mean reward: -0.169 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.045, 10.354], loss: 0.005397, mae: 0.071025, mean_q: -0.298614
 67600/100000: episode: 676, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.516, mean reward: -0.165 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.469, 10.098], loss: 0.002782, mae: 0.051560, mean_q: -0.317071
 67700/100000: episode: 677, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.573, mean reward: -0.156 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.875, 10.098], loss: 0.002610, mae: 0.050540, mean_q: -0.313646
 67800/100000: episode: 678, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.464, mean reward: -0.165 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.102, 10.147], loss: 0.002670, mae: 0.050742, mean_q: -0.287668
 67900/100000: episode: 679, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -17.739, mean reward: -0.177 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.765, 10.107], loss: 0.002514, mae: 0.048623, mean_q: -0.314142
 68000/100000: episode: 680, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.446, mean reward: -0.194 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.641, 10.112], loss: 0.002388, mae: 0.047369, mean_q: -0.353642
 68100/100000: episode: 681, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.040, mean reward: -0.150 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.334, 10.318], loss: 0.002661, mae: 0.050450, mean_q: -0.308631
 68200/100000: episode: 682, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.205, mean reward: -0.182 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.671, 10.151], loss: 0.002616, mae: 0.049107, mean_q: -0.314119
 68300/100000: episode: 683, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.997, mean reward: -0.180 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.657, 10.098], loss: 0.002550, mae: 0.048977, mean_q: -0.308279
 68400/100000: episode: 684, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.521, mean reward: -0.195 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.434, 10.098], loss: 0.002769, mae: 0.051731, mean_q: -0.277721
 68500/100000: episode: 685, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -17.641, mean reward: -0.176 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.767, 10.115], loss: 0.002553, mae: 0.049282, mean_q: -0.303408
 68600/100000: episode: 686, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.449, mean reward: -0.174 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.508, 10.301], loss: 0.002628, mae: 0.049436, mean_q: -0.325467
 68700/100000: episode: 687, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -16.805, mean reward: -0.168 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.140, 10.098], loss: 0.002716, mae: 0.051090, mean_q: -0.294347
 68800/100000: episode: 688, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -7.024, mean reward: -0.070 [-1.000, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.885, 10.553], loss: 0.002573, mae: 0.049666, mean_q: -0.329783
 68900/100000: episode: 689, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.269, mean reward: -0.163 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.596, 10.229], loss: 0.002759, mae: 0.051533, mean_q: -0.327061
 69000/100000: episode: 690, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.668, mean reward: -0.187 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.500, 10.098], loss: 0.002676, mae: 0.051006, mean_q: -0.302842
 69100/100000: episode: 691, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.963, mean reward: -0.180 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.222, 10.336], loss: 0.002819, mae: 0.052720, mean_q: -0.294068
 69200/100000: episode: 692, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.198, mean reward: -0.182 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.692, 10.104], loss: 0.002556, mae: 0.049314, mean_q: -0.316860
 69300/100000: episode: 693, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.969, mean reward: -0.170 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.292, 10.316], loss: 0.002688, mae: 0.050752, mean_q: -0.354830
 69400/100000: episode: 694, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -12.395, mean reward: -0.124 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.072, 10.179], loss: 0.002609, mae: 0.049583, mean_q: -0.333500
 69500/100000: episode: 695, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -14.741, mean reward: -0.147 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.279, 10.098], loss: 0.002498, mae: 0.049749, mean_q: -0.308823
 69600/100000: episode: 696, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.312, mean reward: -0.163 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.082, 10.350], loss: 0.002526, mae: 0.049325, mean_q: -0.313671
 69700/100000: episode: 697, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -14.639, mean reward: -0.146 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.918, 10.098], loss: 0.002621, mae: 0.050320, mean_q: -0.280963
 69800/100000: episode: 698, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.698, mean reward: -0.197 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.478, 10.161], loss: 0.002767, mae: 0.052706, mean_q: -0.356807
 69900/100000: episode: 699, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.060, mean reward: -0.181 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.230, 10.110], loss: 0.002591, mae: 0.050001, mean_q: -0.321053
 70000/100000: episode: 700, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.965, mean reward: -0.170 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.531, 10.098], loss: 0.002522, mae: 0.048380, mean_q: -0.328893
 70100/100000: episode: 701, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -11.521, mean reward: -0.115 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.747, 10.098], loss: 0.002571, mae: 0.050521, mean_q: -0.287062
 70200/100000: episode: 702, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.043, mean reward: -0.160 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.874, 10.098], loss: 0.002747, mae: 0.052874, mean_q: -0.318250
 70300/100000: episode: 703, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.600, mean reward: -0.176 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.541, 10.098], loss: 0.002656, mae: 0.050056, mean_q: -0.305302
 70400/100000: episode: 704, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.283, mean reward: -0.183 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.904, 10.239], loss: 0.002516, mae: 0.049551, mean_q: -0.321193
 70500/100000: episode: 705, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.328, mean reward: -0.193 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.874, 10.103], loss: 0.002593, mae: 0.051135, mean_q: -0.335395
 70600/100000: episode: 706, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.093, mean reward: -0.151 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.508, 10.355], loss: 0.002719, mae: 0.054497, mean_q: -0.305524
 70700/100000: episode: 707, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.972, mean reward: -0.170 [-1.000, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.360, 10.098], loss: 0.002603, mae: 0.050748, mean_q: -0.293970
 70800/100000: episode: 708, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -11.134, mean reward: -0.111 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.133, 10.230], loss: 0.002641, mae: 0.049488, mean_q: -0.312831
 70900/100000: episode: 709, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.114, mean reward: -0.191 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.999, 10.183], loss: 0.002668, mae: 0.050133, mean_q: -0.295072
 71000/100000: episode: 710, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.474, mean reward: -0.175 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.103, 10.115], loss: 0.002476, mae: 0.048153, mean_q: -0.307531
 71100/100000: episode: 711, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.008, mean reward: -0.180 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.635, 10.098], loss: 0.002556, mae: 0.049147, mean_q: -0.351129
 71200/100000: episode: 712, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.296, mean reward: -0.183 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.133, 10.239], loss: 0.003646, mae: 0.057662, mean_q: -0.326673
 71300/100000: episode: 713, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -19.791, mean reward: -0.198 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.042, 10.212], loss: 0.006039, mae: 0.071503, mean_q: -0.336345
 71400/100000: episode: 714, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.149, mean reward: -0.151 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.063, 10.098], loss: 0.003946, mae: 0.061258, mean_q: -0.291149
 71500/100000: episode: 715, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.542, mean reward: -0.175 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.653, 10.187], loss: 0.002630, mae: 0.050939, mean_q: -0.318252
 71600/100000: episode: 716, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.257, mean reward: -0.173 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.855, 10.098], loss: 0.002621, mae: 0.050132, mean_q: -0.288654
 71700/100000: episode: 717, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.059, mean reward: -0.181 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.428, 10.098], loss: 0.002690, mae: 0.050592, mean_q: -0.315033
 71800/100000: episode: 718, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.887, mean reward: -0.169 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.263, 10.154], loss: 0.002633, mae: 0.050100, mean_q: -0.324766
 71900/100000: episode: 719, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.460, mean reward: -0.175 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.240, 10.151], loss: 0.002478, mae: 0.049335, mean_q: -0.313781
 72000/100000: episode: 720, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.499, mean reward: -0.195 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.617, 10.183], loss: 0.002532, mae: 0.049692, mean_q: -0.273430
 72100/100000: episode: 721, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -3.287, mean reward: -0.033 [-1.000, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.722, 10.098], loss: 0.002578, mae: 0.049867, mean_q: -0.309699
 72200/100000: episode: 722, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.900, mean reward: -0.179 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.479, 10.212], loss: 0.002347, mae: 0.047131, mean_q: -0.320322
 72300/100000: episode: 723, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.983, mean reward: -0.170 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.868, 10.163], loss: 0.002730, mae: 0.050984, mean_q: -0.300969
 72400/100000: episode: 724, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.141, mean reward: -0.171 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.723, 10.098], loss: 0.002430, mae: 0.047988, mean_q: -0.311465
 72500/100000: episode: 725, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.414, mean reward: -0.184 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.605, 10.098], loss: 0.002597, mae: 0.049540, mean_q: -0.312468
 72600/100000: episode: 726, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -18.821, mean reward: -0.188 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.527, 10.098], loss: 0.002492, mae: 0.049078, mean_q: -0.320025
 72700/100000: episode: 727, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.719, mean reward: -0.157 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.246, 10.274], loss: 0.002661, mae: 0.050305, mean_q: -0.286990
 72800/100000: episode: 728, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.571, mean reward: -0.146 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-2.331, 10.320], loss: 0.002191, mae: 0.045851, mean_q: -0.317114
 72900/100000: episode: 729, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.990, mean reward: -0.170 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.846, 10.098], loss: 0.002439, mae: 0.047775, mean_q: -0.297783
 73000/100000: episode: 730, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.382, mean reward: -0.184 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.964, 10.126], loss: 0.002626, mae: 0.049287, mean_q: -0.333715
 73100/100000: episode: 731, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.401, mean reward: -0.174 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.689, 10.098], loss: 0.002945, mae: 0.053319, mean_q: -0.300503
 73200/100000: episode: 732, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -12.795, mean reward: -0.128 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.251, 10.098], loss: 0.002473, mae: 0.048194, mean_q: -0.335925
 73300/100000: episode: 733, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.339, mean reward: -0.163 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.642, 10.098], loss: 0.002473, mae: 0.049189, mean_q: -0.294822
 73400/100000: episode: 734, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.023, mean reward: -0.190 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.298, 10.098], loss: 0.002289, mae: 0.046876, mean_q: -0.314006
 73500/100000: episode: 735, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -16.337, mean reward: -0.163 [-1.000, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.864, 10.183], loss: 0.002290, mae: 0.046442, mean_q: -0.296280
 73600/100000: episode: 736, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.578, mean reward: -0.166 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.829, 10.201], loss: 0.002475, mae: 0.047653, mean_q: -0.322279
 73700/100000: episode: 737, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.191, mean reward: -0.172 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.112, 10.098], loss: 0.002601, mae: 0.049860, mean_q: -0.310105
 73800/100000: episode: 738, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.689, mean reward: -0.167 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.609, 10.098], loss: 0.002371, mae: 0.047225, mean_q: -0.344565
 73900/100000: episode: 739, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -15.427, mean reward: -0.154 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.899, 10.206], loss: 0.002546, mae: 0.049570, mean_q: -0.329779
 74000/100000: episode: 740, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.345, mean reward: -0.163 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.370, 10.098], loss: 0.002388, mae: 0.047311, mean_q: -0.315876
 74100/100000: episode: 741, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -12.567, mean reward: -0.126 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.506, 10.380], loss: 0.002506, mae: 0.049788, mean_q: -0.297239
 74200/100000: episode: 742, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.836, mean reward: -0.188 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.229, 10.196], loss: 0.002677, mae: 0.050906, mean_q: -0.308402
 74300/100000: episode: 743, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.555, mean reward: -0.176 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.530, 10.196], loss: 0.002434, mae: 0.048072, mean_q: -0.300117
 74400/100000: episode: 744, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.427, mean reward: -0.184 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.725, 10.098], loss: 0.002658, mae: 0.050233, mean_q: -0.323709
 74500/100000: episode: 745, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.382, mean reward: -0.184 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.211, 10.201], loss: 0.002387, mae: 0.047927, mean_q: -0.307509
 74600/100000: episode: 746, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -18.826, mean reward: -0.188 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.538, 10.122], loss: 0.002549, mae: 0.050802, mean_q: -0.295489
 74700/100000: episode: 747, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.853, mean reward: -0.159 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.625, 10.098], loss: 0.002520, mae: 0.048563, mean_q: -0.345195
 74800/100000: episode: 748, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.930, mean reward: -0.169 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.721, 10.212], loss: 0.004418, mae: 0.059735, mean_q: -0.319812
 74900/100000: episode: 749, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.487, mean reward: -0.175 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.526, 10.295], loss: 0.003657, mae: 0.058914, mean_q: -0.337913
 75000/100000: episode: 750, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.195, mean reward: -0.202 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.735, 10.118], loss: 0.002464, mae: 0.048487, mean_q: -0.303545
 75100/100000: episode: 751, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.583, mean reward: -0.186 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.310, 10.154], loss: 0.002398, mae: 0.047721, mean_q: -0.285003
 75200/100000: episode: 752, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.093, mean reward: -0.161 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.747, 10.098], loss: 0.002495, mae: 0.048362, mean_q: -0.318491
 75300/100000: episode: 753, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.674, mean reward: -0.167 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.361, 10.252], loss: 0.002419, mae: 0.047720, mean_q: -0.313177
 75400/100000: episode: 754, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.827, mean reward: -0.168 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.380, 10.098], loss: 0.002406, mae: 0.047520, mean_q: -0.313661
 75500/100000: episode: 755, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -11.904, mean reward: -0.119 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.415, 10.098], loss: 0.002470, mae: 0.046930, mean_q: -0.387196
 75600/100000: episode: 756, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -20.015, mean reward: -0.200 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.550, 10.098], loss: 0.002448, mae: 0.047906, mean_q: -0.311610
 75700/100000: episode: 757, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.480, mean reward: -0.165 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.082, 10.098], loss: 0.002518, mae: 0.048326, mean_q: -0.297940
 75800/100000: episode: 758, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.271, mean reward: -0.143 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.034, 10.098], loss: 0.002474, mae: 0.047911, mean_q: -0.314928
 75900/100000: episode: 759, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.873, mean reward: -0.149 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.521, 10.217], loss: 0.002523, mae: 0.049071, mean_q: -0.298459
 76000/100000: episode: 760, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.677, mean reward: -0.137 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.151, 10.098], loss: 0.002321, mae: 0.046101, mean_q: -0.330912
 76100/100000: episode: 761, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.726, mean reward: -0.197 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.847, 10.098], loss: 0.002516, mae: 0.049327, mean_q: -0.319986
 76200/100000: episode: 762, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -19.515, mean reward: -0.195 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.059, 10.098], loss: 0.002461, mae: 0.049055, mean_q: -0.278081
 76300/100000: episode: 763, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.264, mean reward: -0.153 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.842, 10.533], loss: 0.002349, mae: 0.046738, mean_q: -0.282314
 76400/100000: episode: 764, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.637, mean reward: -0.166 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.885, 10.098], loss: 0.002435, mae: 0.047829, mean_q: -0.325472
 76500/100000: episode: 765, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.910, mean reward: -0.169 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.401, 10.134], loss: 0.002390, mae: 0.047117, mean_q: -0.313067
 76600/100000: episode: 766, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.218, mean reward: -0.182 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.194, 10.215], loss: 0.003342, mae: 0.053593, mean_q: -0.327007
 76700/100000: episode: 767, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.205, mean reward: -0.172 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.219, 10.158], loss: 0.003835, mae: 0.058529, mean_q: -0.323717
 76800/100000: episode: 768, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.256, mean reward: -0.163 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.605, 10.098], loss: 0.002520, mae: 0.048724, mean_q: -0.317870
 76900/100000: episode: 769, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.071, mean reward: -0.181 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.889, 10.098], loss: 0.002422, mae: 0.047354, mean_q: -0.339623
 77000/100000: episode: 770, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.711, mean reward: -0.157 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.969, 10.212], loss: 0.002482, mae: 0.049237, mean_q: -0.313387
 77100/100000: episode: 771, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.040, mean reward: -0.170 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.299, 10.351], loss: 0.002292, mae: 0.045695, mean_q: -0.312963
 77200/100000: episode: 772, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.707, mean reward: -0.177 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.941, 10.098], loss: 0.002481, mae: 0.048841, mean_q: -0.311379
 77300/100000: episode: 773, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -20.147, mean reward: -0.201 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.700, 10.098], loss: 0.002475, mae: 0.047960, mean_q: -0.302959
 77400/100000: episode: 774, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -14.604, mean reward: -0.146 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.703, 10.312], loss: 0.002369, mae: 0.047368, mean_q: -0.316710
 77500/100000: episode: 775, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -20.499, mean reward: -0.205 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.480, 10.376], loss: 0.002465, mae: 0.048172, mean_q: -0.321275
 77600/100000: episode: 776, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.945, mean reward: -0.159 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.457, 10.243], loss: 0.002544, mae: 0.049265, mean_q: -0.331001
 77700/100000: episode: 777, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.033, mean reward: -0.170 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.035, 10.181], loss: 0.002302, mae: 0.047117, mean_q: -0.307005
 77800/100000: episode: 778, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.737, mean reward: -0.177 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.603, 10.188], loss: 0.002388, mae: 0.046968, mean_q: -0.299630
 77900/100000: episode: 779, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.464, mean reward: -0.165 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.847, 10.421], loss: 0.002480, mae: 0.048564, mean_q: -0.308207
 78000/100000: episode: 780, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.678, mean reward: -0.157 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.840, 10.098], loss: 0.002414, mae: 0.047644, mean_q: -0.325740
 78100/100000: episode: 781, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -13.515, mean reward: -0.135 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.566, 10.098], loss: 0.002510, mae: 0.049169, mean_q: -0.327219
 78200/100000: episode: 782, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.975, mean reward: -0.160 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.765, 10.098], loss: 0.002291, mae: 0.047119, mean_q: -0.301781
 78300/100000: episode: 783, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.566, mean reward: -0.176 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.762, 10.349], loss: 0.002432, mae: 0.047864, mean_q: -0.313301
 78400/100000: episode: 784, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.235, mean reward: -0.142 [-1.000, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.573, 10.235], loss: 0.002500, mae: 0.048032, mean_q: -0.327865
 78500/100000: episode: 785, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.233, mean reward: -0.162 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.750, 10.098], loss: 0.002488, mae: 0.048673, mean_q: -0.319906
 78600/100000: episode: 786, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -18.926, mean reward: -0.189 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.948, 10.098], loss: 0.002407, mae: 0.049905, mean_q: -0.282158
 78700/100000: episode: 787, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -8.744, mean reward: -0.087 [-1.000, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.014, 10.137], loss: 0.002546, mae: 0.049526, mean_q: -0.288920
 78800/100000: episode: 788, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -16.177, mean reward: -0.162 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.894, 10.150], loss: 0.002382, mae: 0.047988, mean_q: -0.312685
 78900/100000: episode: 789, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -12.936, mean reward: -0.129 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.145, 10.370], loss: 0.002258, mae: 0.045742, mean_q: -0.330444
 79000/100000: episode: 790, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.768, mean reward: -0.168 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.111, 10.098], loss: 0.002575, mae: 0.050237, mean_q: -0.278821
 79100/100000: episode: 791, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.770, mean reward: -0.148 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.628, 10.368], loss: 0.002225, mae: 0.046309, mean_q: -0.309364
 79200/100000: episode: 792, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.711, mean reward: -0.197 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.087, 10.106], loss: 0.002243, mae: 0.045613, mean_q: -0.328728
 79300/100000: episode: 793, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.008, mean reward: -0.170 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.570, 10.177], loss: 0.002464, mae: 0.048966, mean_q: -0.314740
 79400/100000: episode: 794, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -20.663, mean reward: -0.207 [-1.000, 0.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.414, 10.098], loss: 0.002166, mae: 0.046315, mean_q: -0.309728
 79500/100000: episode: 795, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -12.724, mean reward: -0.127 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.817, 10.098], loss: 0.002334, mae: 0.047655, mean_q: -0.322472
 79600/100000: episode: 796, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.701, mean reward: -0.187 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.491, 10.098], loss: 0.005563, mae: 0.066699, mean_q: -0.310780
 79700/100000: episode: 797, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.754, mean reward: -0.188 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.096, 10.098], loss: 0.002531, mae: 0.051016, mean_q: -0.302190
 79800/100000: episode: 798, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.349, mean reward: -0.193 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.187, 10.125], loss: 0.002166, mae: 0.045357, mean_q: -0.314938
 79900/100000: episode: 799, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.887, mean reward: -0.169 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.909, 10.098], loss: 0.002371, mae: 0.047972, mean_q: -0.282455
 80000/100000: episode: 800, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.775, mean reward: -0.188 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.693, 10.138], loss: 0.002258, mae: 0.046371, mean_q: -0.332889
 80100/100000: episode: 801, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.173, mean reward: -0.172 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.509, 10.169], loss: 0.002392, mae: 0.046954, mean_q: -0.334442
 80200/100000: episode: 802, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.534, mean reward: -0.175 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.605, 10.285], loss: 0.002370, mae: 0.046666, mean_q: -0.306941
 80300/100000: episode: 803, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.729, mean reward: -0.157 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.872, 10.300], loss: 0.002433, mae: 0.047510, mean_q: -0.311376
 80400/100000: episode: 804, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.155, mean reward: -0.162 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.897, 10.098], loss: 0.002284, mae: 0.046489, mean_q: -0.325882
 80500/100000: episode: 805, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.240, mean reward: -0.172 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.653, 10.253], loss: 0.002273, mae: 0.045503, mean_q: -0.331706
 80600/100000: episode: 806, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.850, mean reward: -0.148 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.396, 10.340], loss: 0.002373, mae: 0.047086, mean_q: -0.274679
 80700/100000: episode: 807, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.339, mean reward: -0.153 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.204, 10.267], loss: 0.002340, mae: 0.046621, mean_q: -0.313888
 80800/100000: episode: 808, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -10.643, mean reward: -0.106 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.737, 10.237], loss: 0.002282, mae: 0.046682, mean_q: -0.307140
 80900/100000: episode: 809, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.652, mean reward: -0.157 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.208, 10.133], loss: 0.002182, mae: 0.045095, mean_q: -0.315081
 81000/100000: episode: 810, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.505, mean reward: -0.195 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.239, 10.239], loss: 0.002358, mae: 0.047274, mean_q: -0.295271
 81100/100000: episode: 811, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -11.212, mean reward: -0.112 [-1.000, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.285, 10.185], loss: 0.002339, mae: 0.048088, mean_q: -0.286022
 81200/100000: episode: 812, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.464, mean reward: -0.175 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.456, 10.098], loss: 0.002060, mae: 0.045205, mean_q: -0.291918
 81300/100000: episode: 813, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.292, mean reward: -0.183 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.778, 10.206], loss: 0.002208, mae: 0.046786, mean_q: -0.310534
 81400/100000: episode: 814, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.759, mean reward: -0.158 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.051, 10.104], loss: 0.002328, mae: 0.047775, mean_q: -0.281962
 81500/100000: episode: 815, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.953, mean reward: -0.170 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.736, 10.251], loss: 0.002455, mae: 0.049599, mean_q: -0.281567
 81600/100000: episode: 816, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.982, mean reward: -0.190 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.038, 10.148], loss: 0.002260, mae: 0.047467, mean_q: -0.277077
 81700/100000: episode: 817, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -13.983, mean reward: -0.140 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.158, 10.338], loss: 0.002203, mae: 0.045524, mean_q: -0.328647
 81800/100000: episode: 818, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.403, mean reward: -0.164 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.183, 10.098], loss: 0.002182, mae: 0.045560, mean_q: -0.303762
 81900/100000: episode: 819, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -14.820, mean reward: -0.148 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.823, 10.233], loss: 0.002974, mae: 0.051824, mean_q: -0.285451
 82000/100000: episode: 820, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.887, mean reward: -0.159 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.932, 10.402], loss: 0.002534, mae: 0.050586, mean_q: -0.276920
 82100/100000: episode: 821, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.910, mean reward: -0.179 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.103, 10.098], loss: 0.002325, mae: 0.047064, mean_q: -0.325230
 82200/100000: episode: 822, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.657, mean reward: -0.187 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.741, 10.136], loss: 0.002261, mae: 0.046600, mean_q: -0.292538
 82300/100000: episode: 823, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.696, mean reward: -0.187 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.401, 10.098], loss: 0.002206, mae: 0.045282, mean_q: -0.316933
 82400/100000: episode: 824, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.704, mean reward: -0.187 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.387, 10.110], loss: 0.002271, mae: 0.046330, mean_q: -0.289749
 82500/100000: episode: 825, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.438, mean reward: -0.164 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.267, 10.098], loss: 0.002223, mae: 0.046158, mean_q: -0.320232
 82600/100000: episode: 826, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.253, mean reward: -0.163 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.272, 10.098], loss: 0.002168, mae: 0.045345, mean_q: -0.325661
 82700/100000: episode: 827, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.069, mean reward: -0.171 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.774, 10.158], loss: 0.002240, mae: 0.045615, mean_q: -0.335635
 82800/100000: episode: 828, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.001, mean reward: -0.150 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.462, 10.267], loss: 0.002372, mae: 0.047590, mean_q: -0.315127
 82900/100000: episode: 829, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.970, mean reward: -0.180 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.745, 10.288], loss: 0.002234, mae: 0.046198, mean_q: -0.304729
 83000/100000: episode: 830, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -13.198, mean reward: -0.132 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.513, 10.431], loss: 0.002246, mae: 0.046435, mean_q: -0.331697
 83100/100000: episode: 831, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -9.749, mean reward: -0.097 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.389, 10.098], loss: 0.002286, mae: 0.048238, mean_q: -0.331972
 83200/100000: episode: 832, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.437, mean reward: -0.164 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.120, 10.173], loss: 0.002320, mae: 0.048276, mean_q: -0.292388
 83300/100000: episode: 833, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.224, mean reward: -0.172 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.179, 10.098], loss: 0.002316, mae: 0.047520, mean_q: -0.310639
 83400/100000: episode: 834, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -8.857, mean reward: -0.089 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.045, 10.421], loss: 0.002489, mae: 0.049132, mean_q: -0.298303
 83500/100000: episode: 835, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -18.048, mean reward: -0.180 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.654, 10.234], loss: 0.002483, mae: 0.050426, mean_q: -0.323548
 83600/100000: episode: 836, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -10.424, mean reward: -0.104 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.515, 10.210], loss: 0.002633, mae: 0.052063, mean_q: -0.310446
 83700/100000: episode: 837, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.844, mean reward: -0.178 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.365, 10.102], loss: 0.002285, mae: 0.046847, mean_q: -0.302420
 83800/100000: episode: 838, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -20.319, mean reward: -0.203 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.145, 10.108], loss: 0.002412, mae: 0.048092, mean_q: -0.324722
 83900/100000: episode: 839, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -16.850, mean reward: -0.168 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.528, 10.203], loss: 0.002231, mae: 0.045420, mean_q: -0.357248
 84000/100000: episode: 840, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.178, mean reward: -0.162 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.507, 10.343], loss: 0.002355, mae: 0.048250, mean_q: -0.295485
 84100/100000: episode: 841, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.580, mean reward: -0.186 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.095, 10.369], loss: 0.002417, mae: 0.047802, mean_q: -0.308478
 84200/100000: episode: 842, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.341, mean reward: -0.163 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.907, 10.414], loss: 0.002533, mae: 0.049756, mean_q: -0.283922
 84300/100000: episode: 843, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.606, mean reward: -0.156 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.656, 10.098], loss: 0.002426, mae: 0.048148, mean_q: -0.309133
 84400/100000: episode: 844, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -19.645, mean reward: -0.196 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.668, 10.189], loss: 0.002340, mae: 0.047213, mean_q: -0.318871
 84500/100000: episode: 845, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.052, mean reward: -0.171 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.977, 10.098], loss: 0.002445, mae: 0.049128, mean_q: -0.317845
 84600/100000: episode: 846, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.396, mean reward: -0.174 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.729, 10.098], loss: 0.002374, mae: 0.047644, mean_q: -0.317592
 84700/100000: episode: 847, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.018, mean reward: -0.180 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.332, 10.104], loss: 0.002508, mae: 0.049975, mean_q: -0.260958
 84800/100000: episode: 848, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.623, mean reward: -0.176 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.180, 10.098], loss: 0.002348, mae: 0.047521, mean_q: -0.318881
 84900/100000: episode: 849, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.294, mean reward: -0.173 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.974, 10.115], loss: 0.002626, mae: 0.050023, mean_q: -0.297894
 85000/100000: episode: 850, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.558, mean reward: -0.156 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.423, 10.227], loss: 0.002394, mae: 0.047904, mean_q: -0.302538
 85100/100000: episode: 851, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.377, mean reward: -0.194 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.296, 10.098], loss: 0.002462, mae: 0.047498, mean_q: -0.316609
 85200/100000: episode: 852, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.961, mean reward: -0.170 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.291, 10.177], loss: 0.002232, mae: 0.046745, mean_q: -0.335582
 85300/100000: episode: 853, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.540, mean reward: -0.195 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.198, 10.098], loss: 0.002400, mae: 0.047965, mean_q: -0.306978
 85400/100000: episode: 854, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -15.622, mean reward: -0.156 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.807, 10.218], loss: 0.002982, mae: 0.050715, mean_q: -0.332020
 85500/100000: episode: 855, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.860, mean reward: -0.159 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.682, 10.098], loss: 0.003112, mae: 0.056034, mean_q: -0.332444
 85600/100000: episode: 856, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.625, mean reward: -0.136 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.732, 10.098], loss: 0.002273, mae: 0.046268, mean_q: -0.330577
 85700/100000: episode: 857, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -14.513, mean reward: -0.145 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.693, 10.098], loss: 0.002623, mae: 0.050267, mean_q: -0.287364
 85800/100000: episode: 858, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.493, mean reward: -0.165 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.201, 10.098], loss: 0.002496, mae: 0.048175, mean_q: -0.333498
 85900/100000: episode: 859, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -15.156, mean reward: -0.152 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.831, 10.098], loss: 0.002454, mae: 0.049269, mean_q: -0.313433
 86000/100000: episode: 860, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.247, mean reward: -0.162 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.926, 10.278], loss: 0.003730, mae: 0.059058, mean_q: -0.308662
 86100/100000: episode: 861, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.362, mean reward: -0.184 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.424, 10.098], loss: 0.002498, mae: 0.048268, mean_q: -0.289030
 86200/100000: episode: 862, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.300, mean reward: -0.153 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.961, 10.098], loss: 0.002378, mae: 0.047843, mean_q: -0.302338
 86300/100000: episode: 863, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.227, mean reward: -0.172 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.807, 10.199], loss: 0.002509, mae: 0.048905, mean_q: -0.304014
 86400/100000: episode: 864, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.535, mean reward: -0.175 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.557, 10.174], loss: 0.002566, mae: 0.049116, mean_q: -0.301468
 86500/100000: episode: 865, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.722, mean reward: -0.167 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.804, 10.098], loss: 0.002580, mae: 0.049051, mean_q: -0.303162
 86600/100000: episode: 866, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.320, mean reward: -0.183 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.746, 10.098], loss: 0.002396, mae: 0.047499, mean_q: -0.302491
 86700/100000: episode: 867, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.531, mean reward: -0.155 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.345, 10.219], loss: 0.002544, mae: 0.048564, mean_q: -0.306913
 86800/100000: episode: 868, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.833, mean reward: -0.138 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.187, 10.098], loss: 0.002453, mae: 0.047508, mean_q: -0.317868
 86900/100000: episode: 869, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.553, mean reward: -0.176 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.237, 10.098], loss: 0.002406, mae: 0.047864, mean_q: -0.307836
 87000/100000: episode: 870, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -10.569, mean reward: -0.106 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.567, 10.222], loss: 0.002486, mae: 0.047867, mean_q: -0.339680
 87100/100000: episode: 871, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -10.309, mean reward: -0.103 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.929, 10.262], loss: 0.002494, mae: 0.049508, mean_q: -0.315008
 87200/100000: episode: 872, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.059, mean reward: -0.171 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.833, 10.121], loss: 0.002603, mae: 0.050200, mean_q: -0.295325
 87300/100000: episode: 873, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -15.768, mean reward: -0.158 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.054, 10.292], loss: 0.002462, mae: 0.047585, mean_q: -0.336481
 87400/100000: episode: 874, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.602, mean reward: -0.176 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.637, 10.098], loss: 0.002359, mae: 0.047088, mean_q: -0.320281
 87500/100000: episode: 875, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.352, mean reward: -0.144 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.459, 10.098], loss: 0.002419, mae: 0.047981, mean_q: -0.262351
 87600/100000: episode: 876, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -18.245, mean reward: -0.182 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.365, 10.098], loss: 0.002640, mae: 0.050048, mean_q: -0.338588
 87700/100000: episode: 877, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -20.648, mean reward: -0.206 [-1.000, 0.232], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.893, 10.098], loss: 0.003554, mae: 0.060512, mean_q: -0.278025
 87800/100000: episode: 878, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.319, mean reward: -0.153 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.000, 10.186], loss: 0.002299, mae: 0.046184, mean_q: -0.320679
 87900/100000: episode: 879, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -7.821, mean reward: -0.078 [-1.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.598, 10.255], loss: 0.002328, mae: 0.046572, mean_q: -0.373233
 88000/100000: episode: 880, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -14.220, mean reward: -0.142 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.475, 10.232], loss: 0.002576, mae: 0.048409, mean_q: -0.302156
 88100/100000: episode: 881, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -17.516, mean reward: -0.175 [-1.000, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.655, 10.126], loss: 0.002423, mae: 0.047606, mean_q: -0.295507
 88200/100000: episode: 882, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -12.374, mean reward: -0.124 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.410 [-1.430, 10.098], loss: 0.002366, mae: 0.047281, mean_q: -0.282588
 88300/100000: episode: 883, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.776, mean reward: -0.188 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.822, 10.177], loss: 0.002487, mae: 0.048221, mean_q: -0.288846
 88400/100000: episode: 884, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.078, mean reward: -0.181 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.905, 10.098], loss: 0.002334, mae: 0.046467, mean_q: -0.286361
 88500/100000: episode: 885, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.766, mean reward: -0.188 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.720, 10.098], loss: 0.002486, mae: 0.048292, mean_q: -0.297249
 88600/100000: episode: 886, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -11.295, mean reward: -0.113 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.244, 10.542], loss: 0.002439, mae: 0.048334, mean_q: -0.311490
 88700/100000: episode: 887, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -13.113, mean reward: -0.131 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.656, 10.323], loss: 0.002431, mae: 0.047277, mean_q: -0.302125
 88800/100000: episode: 888, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.708, mean reward: -0.167 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.692, 10.309], loss: 0.002547, mae: 0.048903, mean_q: -0.301397
 88900/100000: episode: 889, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.642, mean reward: -0.166 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.607, 10.098], loss: 0.002284, mae: 0.046999, mean_q: -0.300527
 89000/100000: episode: 890, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.777, mean reward: -0.188 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.089, 10.253], loss: 0.002401, mae: 0.048452, mean_q: -0.265461
 89100/100000: episode: 891, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.662, mean reward: -0.187 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.272, 10.166], loss: 0.002242, mae: 0.046261, mean_q: -0.285387
 89200/100000: episode: 892, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -10.811, mean reward: -0.108 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.924, 10.182], loss: 0.002317, mae: 0.047120, mean_q: -0.305321
 89300/100000: episode: 893, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.040, mean reward: -0.180 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.655, 10.098], loss: 0.002403, mae: 0.047785, mean_q: -0.315500
 89400/100000: episode: 894, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.447, mean reward: -0.164 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.576, 10.126], loss: 0.002504, mae: 0.050276, mean_q: -0.319764
 89500/100000: episode: 895, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.041, mean reward: -0.190 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.330, 10.221], loss: 0.002420, mae: 0.049977, mean_q: -0.282479
 89600/100000: episode: 896, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.754, mean reward: -0.178 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.383, 10.203], loss: 0.002622, mae: 0.050132, mean_q: -0.295765
 89700/100000: episode: 897, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.149, mean reward: -0.151 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.698, 10.098], loss: 0.002313, mae: 0.047775, mean_q: -0.294181
 89800/100000: episode: 898, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.963, mean reward: -0.170 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.406, 10.098], loss: 0.002392, mae: 0.049183, mean_q: -0.265483
 89900/100000: episode: 899, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -13.557, mean reward: -0.136 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.245, 10.358], loss: 0.002205, mae: 0.045347, mean_q: -0.310136
 90000/100000: episode: 900, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -14.370, mean reward: -0.144 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.036, 10.098], loss: 0.002357, mae: 0.048049, mean_q: -0.299503
 90100/100000: episode: 901, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -9.822, mean reward: -0.098 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.908, 10.098], loss: 0.002546, mae: 0.049726, mean_q: -0.319948
 90200/100000: episode: 902, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.930, mean reward: -0.169 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.369, 10.098], loss: 0.003265, mae: 0.056314, mean_q: -0.296962
 90300/100000: episode: 903, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -12.736, mean reward: -0.127 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.880, 10.098], loss: 0.002676, mae: 0.051182, mean_q: -0.253341
 90400/100000: episode: 904, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -20.144, mean reward: -0.201 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.457, 10.260], loss: 0.002370, mae: 0.047861, mean_q: -0.289178
 90500/100000: episode: 905, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -14.976, mean reward: -0.150 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.497, 10.098], loss: 0.002496, mae: 0.049126, mean_q: -0.272345
 90600/100000: episode: 906, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.776, mean reward: -0.198 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.460, 10.098], loss: 0.002489, mae: 0.048764, mean_q: -0.309955
 90700/100000: episode: 907, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -15.291, mean reward: -0.153 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.122, 10.098], loss: 0.002544, mae: 0.048952, mean_q: -0.287351
 90800/100000: episode: 908, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.164, mean reward: -0.182 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.633, 10.098], loss: 0.002342, mae: 0.046512, mean_q: -0.296584
 90900/100000: episode: 909, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -4.845, mean reward: -0.048 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.845, 10.098], loss: 0.002501, mae: 0.048697, mean_q: -0.307490
 91000/100000: episode: 910, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.849, mean reward: -0.178 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.878, 10.098], loss: 0.002404, mae: 0.048422, mean_q: -0.297673
 91100/100000: episode: 911, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.534, mean reward: -0.185 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.281, 10.098], loss: 0.002358, mae: 0.047098, mean_q: -0.290771
 91200/100000: episode: 912, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.348, mean reward: -0.153 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.017, 10.447], loss: 0.002504, mae: 0.048695, mean_q: -0.283105
 91300/100000: episode: 913, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.722, mean reward: -0.187 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.713, 10.134], loss: 0.002308, mae: 0.046394, mean_q: -0.327681
 91400/100000: episode: 914, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -20.424, mean reward: -0.204 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.137, 10.098], loss: 0.002458, mae: 0.049109, mean_q: -0.305799
 91500/100000: episode: 915, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.417, mean reward: -0.184 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.723, 10.150], loss: 0.002097, mae: 0.045057, mean_q: -0.305126
 91600/100000: episode: 916, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.458, mean reward: -0.165 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.120, 10.098], loss: 0.002366, mae: 0.047747, mean_q: -0.301613
 91700/100000: episode: 917, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.910, mean reward: -0.179 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.728, 10.285], loss: 0.002454, mae: 0.047997, mean_q: -0.294438
 91800/100000: episode: 918, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.863, mean reward: -0.149 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.152, 10.225], loss: 0.003526, mae: 0.060187, mean_q: -0.276258
 91900/100000: episode: 919, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.666, mean reward: -0.147 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.962, 10.265], loss: 0.002393, mae: 0.049146, mean_q: -0.296917
 92000/100000: episode: 920, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.419, mean reward: -0.184 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.651, 10.270], loss: 0.002388, mae: 0.048261, mean_q: -0.318709
 92100/100000: episode: 921, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.271, mean reward: -0.193 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.245, 10.168], loss: 0.002386, mae: 0.047703, mean_q: -0.289435
 92200/100000: episode: 922, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.639, mean reward: -0.166 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.584, 10.133], loss: 0.002272, mae: 0.046447, mean_q: -0.300782
 92300/100000: episode: 923, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -16.958, mean reward: -0.170 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.785, 10.159], loss: 0.002375, mae: 0.046932, mean_q: -0.297548
 92400/100000: episode: 924, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -20.458, mean reward: -0.205 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.688, 10.098], loss: 0.002337, mae: 0.048047, mean_q: -0.302061
 92500/100000: episode: 925, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.359, mean reward: -0.184 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.001, 10.290], loss: 0.002237, mae: 0.046583, mean_q: -0.304661
 92600/100000: episode: 926, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -18.622, mean reward: -0.186 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.210, 10.268], loss: 0.002483, mae: 0.049001, mean_q: -0.289141
 92700/100000: episode: 927, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.494, mean reward: -0.165 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.158, 10.098], loss: 0.002162, mae: 0.045152, mean_q: -0.311736
 92800/100000: episode: 928, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.804, mean reward: -0.158 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.888, 10.098], loss: 0.003260, mae: 0.055887, mean_q: -0.339439
 92900/100000: episode: 929, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.187, mean reward: -0.182 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.410, 10.098], loss: 0.002968, mae: 0.056922, mean_q: -0.301178
 93000/100000: episode: 930, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.340, mean reward: -0.173 [-1.000, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.743, 10.126], loss: 0.002344, mae: 0.047288, mean_q: -0.302333
 93100/100000: episode: 931, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -10.905, mean reward: -0.109 [-1.000, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.575, 10.521], loss: 0.002333, mae: 0.046742, mean_q: -0.307698
 93200/100000: episode: 932, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.686, mean reward: -0.157 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.172, 10.098], loss: 0.002328, mae: 0.046930, mean_q: -0.330934
 93300/100000: episode: 933, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.748, mean reward: -0.177 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.134, 10.141], loss: 0.002514, mae: 0.048453, mean_q: -0.304168
 93400/100000: episode: 934, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.571, mean reward: -0.186 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.452, 10.279], loss: 0.002324, mae: 0.046635, mean_q: -0.285652
 93500/100000: episode: 935, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.671, mean reward: -0.187 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.486, 10.098], loss: 0.002292, mae: 0.046709, mean_q: -0.309891
 93600/100000: episode: 936, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.368, mean reward: -0.184 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.295, 10.098], loss: 0.002286, mae: 0.047087, mean_q: -0.290059
 93700/100000: episode: 937, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.492, mean reward: -0.175 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.629, 10.098], loss: 0.002417, mae: 0.047830, mean_q: -0.301555
 93800/100000: episode: 938, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -19.957, mean reward: -0.200 [-1.000, 0.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.505, 10.098], loss: 0.002281, mae: 0.046677, mean_q: -0.316852
 93900/100000: episode: 939, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.044, mean reward: -0.190 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.155, 10.278], loss: 0.002295, mae: 0.046577, mean_q: -0.308597
 94000/100000: episode: 940, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.405, mean reward: -0.194 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.898, 10.167], loss: 0.002212, mae: 0.046962, mean_q: -0.292656
 94100/100000: episode: 941, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.173, mean reward: -0.192 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.609, 10.098], loss: 0.002224, mae: 0.045981, mean_q: -0.312117
 94200/100000: episode: 942, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -11.048, mean reward: -0.110 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.675, 10.098], loss: 0.002287, mae: 0.046906, mean_q: -0.294449
 94300/100000: episode: 943, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.291, mean reward: -0.173 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.709, 10.103], loss: 0.002251, mae: 0.045888, mean_q: -0.315989
 94400/100000: episode: 944, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.082, mean reward: -0.151 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.508, 10.279], loss: 0.002375, mae: 0.046991, mean_q: -0.343949
 94500/100000: episode: 945, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -11.740, mean reward: -0.117 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.400, 10.098], loss: 0.002330, mae: 0.047394, mean_q: -0.291114
 94600/100000: episode: 946, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.102, mean reward: -0.181 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.640, 10.287], loss: 0.002314, mae: 0.047307, mean_q: -0.328766
 94700/100000: episode: 947, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.993, mean reward: -0.150 [-1.000, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.340, 10.098], loss: 0.002263, mae: 0.045760, mean_q: -0.318472
 94800/100000: episode: 948, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.184, mean reward: -0.152 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.955, 10.098], loss: 0.002363, mae: 0.046189, mean_q: -0.349212
 94900/100000: episode: 949, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -11.402, mean reward: -0.114 [-1.000, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.679, 10.410], loss: 0.002600, mae: 0.050458, mean_q: -0.297073
 95000/100000: episode: 950, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -15.651, mean reward: -0.157 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.779, 10.438], loss: 0.002394, mae: 0.047458, mean_q: -0.339500
 95100/100000: episode: 951, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.600, mean reward: -0.166 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.330, 10.235], loss: 0.002421, mae: 0.048938, mean_q: -0.297958
 95200/100000: episode: 952, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.323, mean reward: -0.193 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.403, 10.098], loss: 0.002341, mae: 0.047621, mean_q: -0.323687
 95300/100000: episode: 953, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.290, mean reward: -0.173 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.012, 10.149], loss: 0.002642, mae: 0.050095, mean_q: -0.317612
 95400/100000: episode: 954, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.271, mean reward: -0.183 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.753, 10.098], loss: 0.002607, mae: 0.051153, mean_q: -0.303504
 95500/100000: episode: 955, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -14.025, mean reward: -0.140 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.786, 10.217], loss: 0.002344, mae: 0.047556, mean_q: -0.303787
 95600/100000: episode: 956, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.515, mean reward: -0.175 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.186, 10.357], loss: 0.002427, mae: 0.047758, mean_q: -0.319460
 95700/100000: episode: 957, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.356, mean reward: -0.184 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.566, 10.164], loss: 0.002292, mae: 0.046041, mean_q: -0.312227
 95800/100000: episode: 958, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.881, mean reward: -0.169 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.796, 10.098], loss: 0.002104, mae: 0.045083, mean_q: -0.360443
 95900/100000: episode: 959, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.811, mean reward: -0.188 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.611, 10.098], loss: 0.002426, mae: 0.048664, mean_q: -0.302580
 96000/100000: episode: 960, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -10.035, mean reward: -0.100 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.550, 10.098], loss: 0.002462, mae: 0.048583, mean_q: -0.315393
 96100/100000: episode: 961, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.784, mean reward: -0.178 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.422, 10.169], loss: 0.002283, mae: 0.048178, mean_q: -0.282328
 96200/100000: episode: 962, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.483, mean reward: -0.175 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.866, 10.152], loss: 0.002242, mae: 0.046424, mean_q: -0.328299
 96300/100000: episode: 963, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.686, mean reward: -0.167 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.086, 10.106], loss: 0.002611, mae: 0.050711, mean_q: -0.304674
 96400/100000: episode: 964, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.881, mean reward: -0.169 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.937, 10.186], loss: 0.003452, mae: 0.057154, mean_q: -0.335044
 96500/100000: episode: 965, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.290, mean reward: -0.163 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.464, 10.098], loss: 0.002955, mae: 0.054885, mean_q: -0.273343
 96600/100000: episode: 966, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.922, mean reward: -0.179 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.434, 10.098], loss: 0.002287, mae: 0.046405, mean_q: -0.341889
 96700/100000: episode: 967, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -19.357, mean reward: -0.194 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.715, 10.130], loss: 0.002650, mae: 0.049884, mean_q: -0.297272
 96800/100000: episode: 968, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.652, mean reward: -0.157 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.691, 10.204], loss: 0.002466, mae: 0.049081, mean_q: -0.299425
 96900/100000: episode: 969, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -19.373, mean reward: -0.194 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.445, 10.098], loss: 0.002454, mae: 0.047512, mean_q: -0.330516
 97000/100000: episode: 970, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.891, mean reward: -0.169 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.190, 10.399], loss: 0.002300, mae: 0.046499, mean_q: -0.329105
 97100/100000: episode: 971, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.736, mean reward: -0.187 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.752, 10.243], loss: 0.002565, mae: 0.049610, mean_q: -0.289861
 97200/100000: episode: 972, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -4.889, mean reward: -0.049 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.130, 10.098], loss: 0.002441, mae: 0.047212, mean_q: -0.316067
 97300/100000: episode: 973, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.077, mean reward: -0.171 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.771, 10.204], loss: 0.002475, mae: 0.048633, mean_q: -0.301291
 97400/100000: episode: 974, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -12.523, mean reward: -0.125 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.137, 10.244], loss: 0.002453, mae: 0.048080, mean_q: -0.278949
 97500/100000: episode: 975, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -14.542, mean reward: -0.145 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.059, 10.243], loss: 0.002572, mae: 0.049566, mean_q: -0.300936
 97600/100000: episode: 976, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.103, mean reward: -0.131 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.791, 10.098], loss: 0.002402, mae: 0.048584, mean_q: -0.298018
 97700/100000: episode: 977, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.823, mean reward: -0.188 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.389, 10.098], loss: 0.002586, mae: 0.048920, mean_q: -0.305353
 97800/100000: episode: 978, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -7.812, mean reward: -0.078 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.787, 10.453], loss: 0.002623, mae: 0.050357, mean_q: -0.328860
 97900/100000: episode: 979, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.202, mean reward: -0.182 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.133, 10.176], loss: 0.006612, mae: 0.074861, mean_q: -0.287939
 98000/100000: episode: 980, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -16.231, mean reward: -0.162 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.713, 10.098], loss: 0.002383, mae: 0.047798, mean_q: -0.321978
 98100/100000: episode: 981, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.626, mean reward: -0.156 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.828, 10.098], loss: 0.002301, mae: 0.047128, mean_q: -0.306469
 98200/100000: episode: 982, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -14.907, mean reward: -0.149 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.527, 10.098], loss: 0.002341, mae: 0.047093, mean_q: -0.323035
 98300/100000: episode: 983, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -18.444, mean reward: -0.184 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.315, 10.098], loss: 0.002521, mae: 0.048793, mean_q: -0.300359
 98400/100000: episode: 984, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.368, mean reward: -0.164 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.692, 10.363], loss: 0.002621, mae: 0.050655, mean_q: -0.287505
 98500/100000: episode: 985, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.071, mean reward: -0.171 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.000, 10.098], loss: 0.002603, mae: 0.049494, mean_q: -0.306332
 98600/100000: episode: 986, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -19.132, mean reward: -0.191 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.810, 10.309], loss: 0.002413, mae: 0.047146, mean_q: -0.355391
 98700/100000: episode: 987, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.262, mean reward: -0.143 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.858, 10.252], loss: 0.002303, mae: 0.047374, mean_q: -0.315736
 98800/100000: episode: 988, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -19.560, mean reward: -0.196 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.686, 10.216], loss: 0.002591, mae: 0.049664, mean_q: -0.278131
 98900/100000: episode: 989, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.435, mean reward: -0.144 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.988, 10.098], loss: 0.002479, mae: 0.047985, mean_q: -0.305626
 99000/100000: episode: 990, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -12.137, mean reward: -0.121 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.090, 10.098], loss: 0.002400, mae: 0.047536, mean_q: -0.303609
 99100/100000: episode: 991, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.681, mean reward: -0.177 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.732, 10.098], loss: 0.002629, mae: 0.049461, mean_q: -0.338444
 99200/100000: episode: 992, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.992, mean reward: -0.160 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.948, 10.239], loss: 0.002295, mae: 0.047013, mean_q: -0.291435
 99300/100000: episode: 993, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.501, mean reward: -0.195 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.338, 10.238], loss: 0.002511, mae: 0.048624, mean_q: -0.304064
 99400/100000: episode: 994, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.374, mean reward: -0.184 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.756, 10.098], loss: 0.002357, mae: 0.046943, mean_q: -0.304050
 99500/100000: episode: 995, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.353, mean reward: -0.174 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.178, 10.098], loss: 0.002433, mae: 0.048166, mean_q: -0.277890
 99600/100000: episode: 996, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.332, mean reward: -0.173 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.211, 10.197], loss: 0.002438, mae: 0.047384, mean_q: -0.323615
 99700/100000: episode: 997, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -12.732, mean reward: -0.127 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.201, 10.098], loss: 0.002544, mae: 0.049121, mean_q: -0.278058
 99800/100000: episode: 998, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -20.387, mean reward: -0.204 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.654, 10.098], loss: 0.002399, mae: 0.046400, mean_q: -0.350170
 99900/100000: episode: 999, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.648, mean reward: -0.146 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.061, 10.203], loss: 0.002437, mae: 0.048170, mean_q: -0.295316
 100000/100000: episode: 1000, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.489, mean reward: -0.155 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.706, 10.171], loss: 0.002500, mae: 0.049072, mean_q: -0.319904
done, took 521.697 seconds
[Info] End Uniform Random Simulation. Falsification occurred 0 times.
