Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.184s, episode steps: 100, steps per second: 542, episode reward: -15.495, mean reward: -0.155 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.560, 10.182], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.071s, episode steps: 100, steps per second: 1403, episode reward: -13.559, mean reward: -0.136 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.039, 10.220], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.060s, episode steps: 100, steps per second: 1678, episode reward: -16.999, mean reward: -0.170 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.874, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.060s, episode steps: 100, steps per second: 1677, episode reward: -16.675, mean reward: -0.167 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.874, 10.223], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.060s, episode steps: 100, steps per second: 1665, episode reward: -19.151, mean reward: -0.192 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.835, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.232s, episode steps: 100, steps per second: 81, episode reward: -16.974, mean reward: -0.170 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.997, 10.257], loss: 0.048473, mae: 0.219456, mean_q: -0.184215
   700/100000: episode: 7, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -15.150, mean reward: -0.151 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.324, 10.209], loss: 0.014260, mae: 0.115133, mean_q: -0.210573
   800/100000: episode: 8, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.787, mean reward: -0.168 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.823, 10.098], loss: 0.011212, mae: 0.100547, mean_q: -0.266391
   900/100000: episode: 9, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.903, mean reward: -0.159 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.736, 10.098], loss: 0.011069, mae: 0.096446, mean_q: -0.296115
  1000/100000: episode: 10, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -11.156, mean reward: -0.112 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.698, 10.098], loss: 0.009199, mae: 0.090254, mean_q: -0.300691
  1100/100000: episode: 11, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -17.691, mean reward: -0.177 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.672, 10.098], loss: 0.009452, mae: 0.092004, mean_q: -0.298726
  1200/100000: episode: 12, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -19.088, mean reward: -0.191 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.842, 10.271], loss: 0.009694, mae: 0.091991, mean_q: -0.321658
  1300/100000: episode: 13, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.073, mean reward: -0.151 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.491, 10.199], loss: 0.009300, mae: 0.089833, mean_q: -0.307388
  1400/100000: episode: 14, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.637, mean reward: -0.196 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.977, 10.125], loss: 0.008331, mae: 0.083541, mean_q: -0.317327
  1500/100000: episode: 15, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.438, mean reward: -0.194 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.860, 10.098], loss: 0.007865, mae: 0.083594, mean_q: -0.337259
  1600/100000: episode: 16, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.376, mean reward: -0.174 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.149, 10.098], loss: 0.008929, mae: 0.085855, mean_q: -0.311542
  1700/100000: episode: 17, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.802, mean reward: -0.178 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.999, 10.098], loss: 0.007776, mae: 0.083011, mean_q: -0.307895
  1800/100000: episode: 18, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -11.293, mean reward: -0.113 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.526, 10.366], loss: 0.007483, mae: 0.081215, mean_q: -0.310731
  1900/100000: episode: 19, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -13.501, mean reward: -0.135 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.554, 10.098], loss: 0.007492, mae: 0.081786, mean_q: -0.326437
  2000/100000: episode: 20, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.267, mean reward: -0.173 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.537, 10.308], loss: 0.008027, mae: 0.081720, mean_q: -0.316844
  2100/100000: episode: 21, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -14.355, mean reward: -0.144 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.974, 10.098], loss: 0.007466, mae: 0.078092, mean_q: -0.300813
  2200/100000: episode: 22, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.166, mean reward: -0.192 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.279, 10.098], loss: 0.006589, mae: 0.076460, mean_q: -0.317545
  2300/100000: episode: 23, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.084, mean reward: -0.171 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.955, 10.098], loss: 0.006669, mae: 0.078722, mean_q: -0.312080
  2400/100000: episode: 24, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.698, mean reward: -0.167 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.409, 10.098], loss: 0.006200, mae: 0.075197, mean_q: -0.304079
  2500/100000: episode: 25, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -12.456, mean reward: -0.125 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.081, 10.098], loss: 0.007315, mae: 0.084449, mean_q: -0.330555
  2600/100000: episode: 26, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.537, mean reward: -0.195 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.752, 10.098], loss: 0.007635, mae: 0.081839, mean_q: -0.320071
  2700/100000: episode: 27, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.739, mean reward: -0.167 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.593, 10.098], loss: 0.007241, mae: 0.081375, mean_q: -0.320242
  2800/100000: episode: 28, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.439, mean reward: -0.174 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.106, 10.136], loss: 0.006666, mae: 0.076507, mean_q: -0.336444
  2900/100000: episode: 29, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.033, mean reward: -0.150 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.611, 10.275], loss: 0.006918, mae: 0.079729, mean_q: -0.288216
  3000/100000: episode: 30, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.315, mean reward: -0.173 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.815, 10.098], loss: 0.006219, mae: 0.075702, mean_q: -0.321345
  3100/100000: episode: 31, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: -17.833, mean reward: -0.178 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.183, 10.129], loss: 0.007482, mae: 0.083552, mean_q: -0.323193
  3200/100000: episode: 32, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -13.367, mean reward: -0.134 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.708, 10.098], loss: 0.005956, mae: 0.071977, mean_q: -0.308143
  3300/100000: episode: 33, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -20.045, mean reward: -0.200 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.356, 10.156], loss: 0.006370, mae: 0.072708, mean_q: -0.337281
  3400/100000: episode: 34, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -18.263, mean reward: -0.183 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.646, 10.098], loss: 0.007140, mae: 0.082958, mean_q: -0.297829
  3500/100000: episode: 35, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -12.865, mean reward: -0.129 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.084, 10.098], loss: 0.006542, mae: 0.076434, mean_q: -0.315613
  3600/100000: episode: 36, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -13.718, mean reward: -0.137 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.139, 10.098], loss: 0.006437, mae: 0.076091, mean_q: -0.288339
  3700/100000: episode: 37, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -17.552, mean reward: -0.176 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.469, 10.213], loss: 0.005258, mae: 0.069519, mean_q: -0.341344
  3800/100000: episode: 38, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.312, mean reward: -0.153 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.253, 10.164], loss: 0.005561, mae: 0.072037, mean_q: -0.290059
  3900/100000: episode: 39, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.848, mean reward: -0.158 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.601, 10.347], loss: 0.005696, mae: 0.073935, mean_q: -0.269824
  4000/100000: episode: 40, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.347, mean reward: -0.173 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.223, 10.222], loss: 0.005287, mae: 0.070099, mean_q: -0.292500
  4100/100000: episode: 41, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.006, mean reward: -0.180 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.633, 10.188], loss: 0.005028, mae: 0.069884, mean_q: -0.309704
  4200/100000: episode: 42, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.519, mean reward: -0.165 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.527, 10.098], loss: 0.004410, mae: 0.066497, mean_q: -0.317318
  4300/100000: episode: 43, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.721, mean reward: -0.167 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.616, 10.098], loss: 0.005705, mae: 0.073071, mean_q: -0.331850
  4400/100000: episode: 44, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.201, mean reward: -0.172 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.891, 10.116], loss: 0.005262, mae: 0.071614, mean_q: -0.336018
  4500/100000: episode: 45, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.869, mean reward: -0.189 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.758, 10.319], loss: 0.005680, mae: 0.069825, mean_q: -0.320102
  4600/100000: episode: 46, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.014, mean reward: -0.190 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.950, 10.117], loss: 0.005498, mae: 0.073293, mean_q: -0.296445
  4700/100000: episode: 47, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -14.589, mean reward: -0.146 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.147, 10.098], loss: 0.006940, mae: 0.080010, mean_q: -0.309250
  4800/100000: episode: 48, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.531, mean reward: -0.175 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.881, 10.098], loss: 0.005593, mae: 0.073548, mean_q: -0.307590
  4900/100000: episode: 49, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -13.297, mean reward: -0.133 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.544, 10.098], loss: 0.004214, mae: 0.065137, mean_q: -0.342907
  5000/100000: episode: 50, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -16.057, mean reward: -0.161 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.418, 10.120], loss: 0.005274, mae: 0.070566, mean_q: -0.329495
  5100/100000: episode: 51, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -11.472, mean reward: -0.115 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.428, 10.370], loss: 0.005673, mae: 0.074955, mean_q: -0.299788
  5200/100000: episode: 52, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.395, mean reward: -0.154 [-1.000, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.745, 10.120], loss: 0.004904, mae: 0.070040, mean_q: -0.322874
  5300/100000: episode: 53, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.820, mean reward: -0.198 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.858, 10.152], loss: 0.004423, mae: 0.064885, mean_q: -0.334203
  5400/100000: episode: 54, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.364, mean reward: -0.154 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.075, 10.218], loss: 0.008366, mae: 0.087041, mean_q: -0.313467
  5500/100000: episode: 55, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.076, mean reward: -0.191 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.587, 10.162], loss: 0.005170, mae: 0.071908, mean_q: -0.300247
  5600/100000: episode: 56, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.332, mean reward: -0.193 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.250, 10.182], loss: 0.005016, mae: 0.068032, mean_q: -0.307271
  5700/100000: episode: 57, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.225, mean reward: -0.172 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.900, 10.136], loss: 0.004908, mae: 0.068948, mean_q: -0.304471
  5800/100000: episode: 58, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.843, mean reward: -0.168 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.842, 10.098], loss: 0.005501, mae: 0.073409, mean_q: -0.312144
  5900/100000: episode: 59, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.278, mean reward: -0.153 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.158, 10.368], loss: 0.004311, mae: 0.066600, mean_q: -0.291810
  6000/100000: episode: 60, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.468, mean reward: -0.185 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.887, 10.098], loss: 0.005752, mae: 0.074627, mean_q: -0.316870
  6100/100000: episode: 61, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -12.178, mean reward: -0.122 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.491, 10.098], loss: 0.004233, mae: 0.065477, mean_q: -0.328764
  6200/100000: episode: 62, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.881, mean reward: -0.169 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.857, 10.098], loss: 0.006104, mae: 0.078435, mean_q: -0.276227
  6300/100000: episode: 63, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.925, mean reward: -0.139 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.285, 10.098], loss: 0.005022, mae: 0.071670, mean_q: -0.283981
  6400/100000: episode: 64, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.016, mean reward: -0.170 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.211, 10.224], loss: 0.005072, mae: 0.071260, mean_q: -0.307619
  6500/100000: episode: 65, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -14.921, mean reward: -0.149 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.756, 10.347], loss: 0.005487, mae: 0.073354, mean_q: -0.275911
  6600/100000: episode: 66, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -12.051, mean reward: -0.121 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.646, 10.440], loss: 0.004363, mae: 0.065915, mean_q: -0.318534
  6700/100000: episode: 67, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -8.961, mean reward: -0.090 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.044, 10.098], loss: 0.004630, mae: 0.069232, mean_q: -0.322573
  6800/100000: episode: 68, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.865, mean reward: -0.179 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.891, 10.206], loss: 0.004681, mae: 0.067664, mean_q: -0.281132
  6900/100000: episode: 69, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.329, mean reward: -0.193 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.634, 10.098], loss: 0.004565, mae: 0.067810, mean_q: -0.300384
  7000/100000: episode: 70, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.371, mean reward: -0.184 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.585, 10.173], loss: 0.004552, mae: 0.067840, mean_q: -0.293771
  7100/100000: episode: 71, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.768, mean reward: -0.178 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.848, 10.327], loss: 0.004895, mae: 0.070366, mean_q: -0.316659
  7200/100000: episode: 72, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.467, mean reward: -0.195 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.173, 10.148], loss: 0.006304, mae: 0.077173, mean_q: -0.326175
  7300/100000: episode: 73, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -19.192, mean reward: -0.192 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.315, 10.098], loss: 0.003912, mae: 0.064244, mean_q: -0.317924
  7400/100000: episode: 74, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -14.541, mean reward: -0.145 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.841, 10.098], loss: 0.004457, mae: 0.067790, mean_q: -0.309724
  7500/100000: episode: 75, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -12.412, mean reward: -0.124 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.954, 10.098], loss: 0.004862, mae: 0.072785, mean_q: -0.281628
  7600/100000: episode: 76, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -20.944, mean reward: -0.209 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.709, 10.098], loss: 0.004572, mae: 0.067436, mean_q: -0.293069
  7700/100000: episode: 77, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.109, mean reward: -0.171 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.997, 10.098], loss: 0.005726, mae: 0.076852, mean_q: -0.291150
  7800/100000: episode: 78, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.612, mean reward: -0.176 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.193, 10.098], loss: 0.004111, mae: 0.064664, mean_q: -0.303107
  7900/100000: episode: 79, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.718, mean reward: -0.187 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.028, 10.246], loss: 0.004321, mae: 0.066829, mean_q: -0.271818
  8000/100000: episode: 80, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.560, mean reward: -0.176 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.662, 10.099], loss: 0.005469, mae: 0.073824, mean_q: -0.292694
  8100/100000: episode: 81, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.263, mean reward: -0.173 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.483, 10.140], loss: 0.004788, mae: 0.069612, mean_q: -0.319711
  8200/100000: episode: 82, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.051, mean reward: -0.171 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.637, 10.098], loss: 0.006469, mae: 0.080732, mean_q: -0.329975
  8300/100000: episode: 83, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.924, mean reward: -0.199 [-1.000, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.598, 10.098], loss: 0.004858, mae: 0.069935, mean_q: -0.311994
  8400/100000: episode: 84, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.626, mean reward: -0.196 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.285, 10.116], loss: 0.004692, mae: 0.068824, mean_q: -0.327226
  8500/100000: episode: 85, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -14.137, mean reward: -0.141 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.653, 10.199], loss: 0.004206, mae: 0.065334, mean_q: -0.318079
  8600/100000: episode: 86, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.603, mean reward: -0.176 [-1.000, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.846, 10.098], loss: 0.005438, mae: 0.074454, mean_q: -0.323228
  8700/100000: episode: 87, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.819, mean reward: -0.138 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.937, 10.098], loss: 0.004242, mae: 0.065249, mean_q: -0.301425
  8800/100000: episode: 88, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.243, mean reward: -0.162 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.672, 10.193], loss: 0.003746, mae: 0.061895, mean_q: -0.311252
  8900/100000: episode: 89, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.196, mean reward: -0.192 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.414, 10.220], loss: 0.004943, mae: 0.068426, mean_q: -0.319271
  9000/100000: episode: 90, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -20.979, mean reward: -0.210 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.321, 10.098], loss: 0.004402, mae: 0.068577, mean_q: -0.322816
  9100/100000: episode: 91, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.935, mean reward: -0.179 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.931, 10.098], loss: 0.005413, mae: 0.075071, mean_q: -0.298006
  9200/100000: episode: 92, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.110, mean reward: -0.171 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.346, 10.156], loss: 0.004340, mae: 0.065621, mean_q: -0.330003
  9300/100000: episode: 93, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -12.122, mean reward: -0.121 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.744, 10.098], loss: 0.004413, mae: 0.067312, mean_q: -0.327675
  9400/100000: episode: 94, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.164, mean reward: -0.172 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.807, 10.098], loss: 0.004725, mae: 0.066094, mean_q: -0.314719
  9500/100000: episode: 95, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.234, mean reward: -0.192 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.693, 10.136], loss: 0.004488, mae: 0.068670, mean_q: -0.296332
  9600/100000: episode: 96, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.635, mean reward: -0.166 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.096, 10.134], loss: 0.004667, mae: 0.067311, mean_q: -0.306221
  9700/100000: episode: 97, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.238, mean reward: -0.172 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.511, 10.184], loss: 0.004401, mae: 0.067263, mean_q: -0.310685
  9800/100000: episode: 98, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -13.013, mean reward: -0.130 [-1.000, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.124, 10.098], loss: 0.004143, mae: 0.066091, mean_q: -0.297893
  9900/100000: episode: 99, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.881, mean reward: -0.189 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.764, 10.225], loss: 0.004234, mae: 0.065366, mean_q: -0.296069
 10000/100000: episode: 100, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.227, mean reward: -0.162 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.053, 10.257], loss: 0.005176, mae: 0.071140, mean_q: -0.302215
 10100/100000: episode: 101, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -14.707, mean reward: -0.147 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.568, 10.417], loss: 0.003846, mae: 0.065646, mean_q: -0.293452
 10200/100000: episode: 102, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.716, mean reward: -0.177 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.207, 10.098], loss: 0.004927, mae: 0.069995, mean_q: -0.289431
 10300/100000: episode: 103, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.881, mean reward: -0.179 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.666, 10.372], loss: 0.004272, mae: 0.065594, mean_q: -0.311228
 10400/100000: episode: 104, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.652, mean reward: -0.167 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.625, 10.098], loss: 0.003824, mae: 0.062988, mean_q: -0.299434
 10500/100000: episode: 105, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.019, mean reward: -0.170 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.234, 10.133], loss: 0.004298, mae: 0.066130, mean_q: -0.304222
 10600/100000: episode: 106, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.396, mean reward: -0.184 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.225, 10.198], loss: 0.003638, mae: 0.061574, mean_q: -0.308776
 10700/100000: episode: 107, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.406, mean reward: -0.184 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.624, 10.098], loss: 0.004440, mae: 0.066780, mean_q: -0.316700
 10800/100000: episode: 108, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.824, mean reward: -0.178 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.407, 10.216], loss: 0.003701, mae: 0.062925, mean_q: -0.337936
 10900/100000: episode: 109, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.410, mean reward: -0.194 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.776, 10.098], loss: 0.003627, mae: 0.062566, mean_q: -0.321040
 11000/100000: episode: 110, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.092, mean reward: -0.151 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.885, 10.247], loss: 0.003540, mae: 0.060855, mean_q: -0.304778
 11100/100000: episode: 111, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.074, mean reward: -0.181 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.418, 10.115], loss: 0.003746, mae: 0.061888, mean_q: -0.332294
 11200/100000: episode: 112, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.987, mean reward: -0.170 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.394, 10.357], loss: 0.003725, mae: 0.062836, mean_q: -0.297536
 11300/100000: episode: 113, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -13.990, mean reward: -0.140 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.403, 10.185], loss: 0.003402, mae: 0.059216, mean_q: -0.349819
 11400/100000: episode: 114, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.290, mean reward: -0.173 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.315, 10.111], loss: 0.003768, mae: 0.064006, mean_q: -0.307833
 11500/100000: episode: 115, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.671, mean reward: -0.167 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.721, 10.098], loss: 0.004105, mae: 0.065102, mean_q: -0.344810
 11600/100000: episode: 116, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.930, mean reward: -0.169 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.639, 10.216], loss: 0.003642, mae: 0.062434, mean_q: -0.332188
 11700/100000: episode: 117, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.483, mean reward: -0.155 [-1.000, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.179, 10.403], loss: 0.003624, mae: 0.062078, mean_q: -0.328200
 11800/100000: episode: 118, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.213, mean reward: -0.172 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.497, 10.098], loss: 0.003441, mae: 0.060550, mean_q: -0.309598
 11900/100000: episode: 119, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.642, mean reward: -0.176 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.686, 10.127], loss: 0.003532, mae: 0.061118, mean_q: -0.336619
 12000/100000: episode: 120, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.143, mean reward: -0.171 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.809, 10.125], loss: 0.003609, mae: 0.061178, mean_q: -0.320890
 12100/100000: episode: 121, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -8.463, mean reward: -0.085 [-1.000, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-2.288, 10.098], loss: 0.005588, mae: 0.072155, mean_q: -0.330089
 12200/100000: episode: 122, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.226, mean reward: -0.182 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.298, 10.098], loss: 0.005782, mae: 0.074479, mean_q: -0.339682
 12300/100000: episode: 123, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -14.193, mean reward: -0.142 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.936, 10.098], loss: 0.003936, mae: 0.064601, mean_q: -0.295910
 12400/100000: episode: 124, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.791, mean reward: -0.148 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.315, 10.123], loss: 0.003805, mae: 0.063420, mean_q: -0.314309
 12500/100000: episode: 125, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.949, mean reward: -0.199 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.564, 10.256], loss: 0.004506, mae: 0.068729, mean_q: -0.322940
 12600/100000: episode: 126, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.961, mean reward: -0.190 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.452, 10.098], loss: 0.004629, mae: 0.069471, mean_q: -0.267672
 12700/100000: episode: 127, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -11.931, mean reward: -0.119 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.638, 10.098], loss: 0.004040, mae: 0.065431, mean_q: -0.301957
 12800/100000: episode: 128, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.409, mean reward: -0.194 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.295, 10.098], loss: 0.003750, mae: 0.062731, mean_q: -0.343248
 12900/100000: episode: 129, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.815, mean reward: -0.178 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.476, 10.098], loss: 0.003710, mae: 0.062412, mean_q: -0.306585
 13000/100000: episode: 130, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.667, mean reward: -0.147 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.435, 10.246], loss: 0.003336, mae: 0.058865, mean_q: -0.319485
 13100/100000: episode: 131, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.055, mean reward: -0.171 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.417, 10.167], loss: 0.003879, mae: 0.064762, mean_q: -0.318419
 13200/100000: episode: 132, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.678, mean reward: -0.147 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.117, 10.098], loss: 0.004065, mae: 0.065345, mean_q: -0.319145
 13300/100000: episode: 133, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -10.163, mean reward: -0.102 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.812, 10.423], loss: 0.004438, mae: 0.067816, mean_q: -0.297855
 13400/100000: episode: 134, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.917, mean reward: -0.189 [-1.000, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.775, 10.098], loss: 0.003919, mae: 0.065466, mean_q: -0.310244
 13500/100000: episode: 135, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -14.995, mean reward: -0.150 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.578, 10.153], loss: 0.003510, mae: 0.060659, mean_q: -0.326043
 13600/100000: episode: 136, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.562, mean reward: -0.196 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.299, 10.226], loss: 0.004522, mae: 0.069197, mean_q: -0.302579
 13700/100000: episode: 137, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.686, mean reward: -0.197 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.834, 10.098], loss: 0.004188, mae: 0.065306, mean_q: -0.314884
 13800/100000: episode: 138, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -13.466, mean reward: -0.135 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.558, 10.358], loss: 0.003554, mae: 0.062462, mean_q: -0.325139
 13900/100000: episode: 139, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -11.663, mean reward: -0.117 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.646, 10.098], loss: 0.003989, mae: 0.064928, mean_q: -0.308664
 14000/100000: episode: 140, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.570, mean reward: -0.196 [-1.000, 0.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.017, 10.156], loss: 0.004149, mae: 0.067509, mean_q: -0.267251
 14100/100000: episode: 141, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.327, mean reward: -0.163 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.036, 10.098], loss: 0.004075, mae: 0.066342, mean_q: -0.292838
 14200/100000: episode: 142, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.175, mean reward: -0.162 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.168, 10.098], loss: 0.003622, mae: 0.061996, mean_q: -0.302406
 14300/100000: episode: 143, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.239, mean reward: -0.182 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.214, 10.220], loss: 0.003742, mae: 0.063668, mean_q: -0.303195
 14400/100000: episode: 144, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.284, mean reward: -0.173 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.005, 10.224], loss: 0.003797, mae: 0.063870, mean_q: -0.299855
 14500/100000: episode: 145, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.874, mean reward: -0.159 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.853, 10.098], loss: 0.003953, mae: 0.064249, mean_q: -0.342041
 14600/100000: episode: 146, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.666, mean reward: -0.197 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.332, 10.098], loss: 0.003641, mae: 0.063105, mean_q: -0.273533
 14700/100000: episode: 147, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.256, mean reward: -0.163 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.708, 10.098], loss: 0.003762, mae: 0.063953, mean_q: -0.347748
 14800/100000: episode: 148, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -19.322, mean reward: -0.193 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.779, 10.152], loss: 0.003475, mae: 0.060183, mean_q: -0.295210
 14900/100000: episode: 149, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.497, mean reward: -0.185 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.368, 10.098], loss: 0.003739, mae: 0.063875, mean_q: -0.297742
[Info] 100-TH LEVEL FOUND: 0.41115325689315796, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.413s, episode steps: 100, steps per second: 23, episode reward: -16.135, mean reward: -0.161 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.910, 10.098], loss: 0.003542, mae: 0.061649, mean_q: -0.301940
 15012/100000: episode: 151, duration: 0.070s, episode steps: 12, steps per second: 173, episode reward: 3.829, mean reward: 0.319 [0.239, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.332, 10.100], loss: 0.003704, mae: 0.063712, mean_q: -0.368015
 15082/100000: episode: 152, duration: 0.354s, episode steps: 70, steps per second: 198, episode reward: 16.694, mean reward: 0.238 [0.035, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.747 [-1.016, 10.464], loss: 0.003151, mae: 0.057441, mean_q: -0.307328
 15152/100000: episode: 153, duration: 0.350s, episode steps: 70, steps per second: 200, episode reward: 12.791, mean reward: 0.183 [0.039, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.737 [-1.945, 10.330], loss: 0.003788, mae: 0.064946, mean_q: -0.266676
 15222/100000: episode: 154, duration: 0.336s, episode steps: 70, steps per second: 209, episode reward: 10.376, mean reward: 0.148 [0.007, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.742 [-0.736, 10.100], loss: 0.003599, mae: 0.062428, mean_q: -0.296138
 15292/100000: episode: 155, duration: 0.354s, episode steps: 70, steps per second: 198, episode reward: 8.515, mean reward: 0.122 [0.021, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.735 [-0.805, 10.100], loss: 0.003601, mae: 0.062970, mean_q: -0.276389
 15362/100000: episode: 156, duration: 0.360s, episode steps: 70, steps per second: 194, episode reward: 11.518, mean reward: 0.165 [0.018, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.740 [-0.469, 10.100], loss: 0.006233, mae: 0.076534, mean_q: -0.225918
 15432/100000: episode: 157, duration: 0.349s, episode steps: 70, steps per second: 200, episode reward: 12.065, mean reward: 0.172 [0.009, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.736 [-0.282, 10.100], loss: 0.003990, mae: 0.068169, mean_q: -0.236808
 15502/100000: episode: 158, duration: 0.365s, episode steps: 70, steps per second: 192, episode reward: 16.555, mean reward: 0.237 [0.065, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.761 [-0.600, 10.533], loss: 0.004830, mae: 0.073657, mean_q: -0.237087
 15573/100000: episode: 159, duration: 0.350s, episode steps: 71, steps per second: 203, episode reward: 13.350, mean reward: 0.188 [0.011, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.730 [-1.005, 10.100], loss: 0.004305, mae: 0.070391, mean_q: -0.231976
 15589/100000: episode: 160, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 4.835, mean reward: 0.302 [0.266, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.393, 10.100], loss: 0.003758, mae: 0.063560, mean_q: -0.174601
 15659/100000: episode: 161, duration: 0.363s, episode steps: 70, steps per second: 193, episode reward: 13.823, mean reward: 0.197 [0.017, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.738 [-2.079, 10.292], loss: 0.004059, mae: 0.067162, mean_q: -0.196745
 15729/100000: episode: 162, duration: 0.374s, episode steps: 70, steps per second: 187, episode reward: 11.057, mean reward: 0.158 [0.020, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.739 [-0.326, 10.104], loss: 0.005873, mae: 0.075453, mean_q: -0.190756
 15745/100000: episode: 163, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 6.160, mean reward: 0.385 [0.281, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.233, 10.100], loss: 0.003737, mae: 0.064541, mean_q: -0.238395
 15761/100000: episode: 164, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 5.156, mean reward: 0.322 [0.234, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.351, 10.100], loss: 0.003731, mae: 0.063346, mean_q: -0.187983
 15773/100000: episode: 165, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 3.096, mean reward: 0.258 [0.164, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.453, 10.100], loss: 0.003727, mae: 0.067223, mean_q: -0.114812
 15843/100000: episode: 166, duration: 0.348s, episode steps: 70, steps per second: 201, episode reward: 15.375, mean reward: 0.220 [0.020, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.744 [-0.699, 10.129], loss: 0.003987, mae: 0.068194, mean_q: -0.174995
 15913/100000: episode: 167, duration: 0.369s, episode steps: 70, steps per second: 190, episode reward: 14.235, mean reward: 0.203 [0.041, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.730 [-0.394, 10.100], loss: 0.003829, mae: 0.065656, mean_q: -0.172772
 15983/100000: episode: 168, duration: 0.369s, episode steps: 70, steps per second: 190, episode reward: 11.632, mean reward: 0.166 [0.005, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.735 [-0.151, 10.159], loss: 0.003877, mae: 0.064894, mean_q: -0.223566
 16053/100000: episode: 169, duration: 0.347s, episode steps: 70, steps per second: 202, episode reward: 9.794, mean reward: 0.140 [0.013, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.732 [-0.661, 10.100], loss: 0.003782, mae: 0.064042, mean_q: -0.161158
 16124/100000: episode: 170, duration: 0.352s, episode steps: 71, steps per second: 202, episode reward: 13.787, mean reward: 0.194 [0.019, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.733 [-0.421, 10.260], loss: 0.004259, mae: 0.069190, mean_q: -0.154359
 16194/100000: episode: 171, duration: 0.380s, episode steps: 70, steps per second: 184, episode reward: 11.543, mean reward: 0.165 [0.018, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.728 [-0.385, 10.173], loss: 0.003926, mae: 0.067097, mean_q: -0.116262
 16264/100000: episode: 172, duration: 0.352s, episode steps: 70, steps per second: 199, episode reward: 11.075, mean reward: 0.158 [0.015, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.745 [-0.833, 10.247], loss: 0.003679, mae: 0.063877, mean_q: -0.172359
 16334/100000: episode: 173, duration: 0.349s, episode steps: 70, steps per second: 200, episode reward: 14.717, mean reward: 0.210 [0.035, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.737 [-0.999, 10.100], loss: 0.003809, mae: 0.065560, mean_q: -0.123527
 16346/100000: episode: 174, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 2.923, mean reward: 0.244 [0.165, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.177, 10.100], loss: 0.004030, mae: 0.064371, mean_q: -0.169667
 16416/100000: episode: 175, duration: 0.354s, episode steps: 70, steps per second: 198, episode reward: 13.616, mean reward: 0.195 [0.049, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.740 [-0.835, 10.100], loss: 0.003851, mae: 0.065793, mean_q: -0.097901
 16486/100000: episode: 176, duration: 0.367s, episode steps: 70, steps per second: 191, episode reward: 13.630, mean reward: 0.195 [0.059, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.733 [-1.011, 10.100], loss: 0.004140, mae: 0.068520, mean_q: -0.088522
 16498/100000: episode: 177, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 3.149, mean reward: 0.262 [0.217, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.087, 10.100], loss: 0.003607, mae: 0.063943, mean_q: -0.149368
 16568/100000: episode: 178, duration: 0.345s, episode steps: 70, steps per second: 203, episode reward: 11.041, mean reward: 0.158 [0.026, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.735 [-1.152, 10.297], loss: 0.003826, mae: 0.065269, mean_q: -0.123150
 16584/100000: episode: 179, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 6.141, mean reward: 0.384 [0.307, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.366, 10.100], loss: 0.004183, mae: 0.067873, mean_q: -0.052638
 16654/100000: episode: 180, duration: 0.369s, episode steps: 70, steps per second: 190, episode reward: 11.213, mean reward: 0.160 [0.022, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.726 [-1.609, 10.100], loss: 0.003752, mae: 0.065019, mean_q: -0.082011
 16724/100000: episode: 181, duration: 0.373s, episode steps: 70, steps per second: 188, episode reward: 12.325, mean reward: 0.176 [0.061, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.735 [-0.841, 10.100], loss: 0.003879, mae: 0.065337, mean_q: -0.097593
 16736/100000: episode: 182, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 3.709, mean reward: 0.309 [0.242, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-1.064, 10.100], loss: 0.010051, mae: 0.092132, mean_q: 0.009281
 16748/100000: episode: 183, duration: 0.061s, episode steps: 12, steps per second: 195, episode reward: 4.283, mean reward: 0.357 [0.293, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.509, 10.100], loss: 0.004056, mae: 0.070201, mean_q: -0.123182
 16818/100000: episode: 184, duration: 0.362s, episode steps: 70, steps per second: 193, episode reward: 9.209, mean reward: 0.132 [0.018, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.738 [-0.202, 10.100], loss: 0.004291, mae: 0.069651, mean_q: -0.054899
 16888/100000: episode: 185, duration: 0.380s, episode steps: 70, steps per second: 184, episode reward: 13.564, mean reward: 0.194 [0.021, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.736 [-0.350, 10.100], loss: 0.006480, mae: 0.081897, mean_q: -0.059432
 16900/100000: episode: 186, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 3.613, mean reward: 0.301 [0.244, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.427, 10.100], loss: 0.005451, mae: 0.083764, mean_q: 0.049759
 16970/100000: episode: 187, duration: 0.359s, episode steps: 70, steps per second: 195, episode reward: 9.329, mean reward: 0.133 [0.008, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.735 [-0.449, 10.100], loss: 0.004564, mae: 0.073693, mean_q: -0.047935
 17040/100000: episode: 188, duration: 0.357s, episode steps: 70, steps per second: 196, episode reward: 11.335, mean reward: 0.162 [0.019, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.727 [-0.727, 10.100], loss: 0.003977, mae: 0.068481, mean_q: -0.024135
 17110/100000: episode: 189, duration: 0.363s, episode steps: 70, steps per second: 193, episode reward: 13.244, mean reward: 0.189 [0.013, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.742 [-0.666, 10.115], loss: 0.003933, mae: 0.067294, mean_q: -0.024022
 17180/100000: episode: 190, duration: 0.371s, episode steps: 70, steps per second: 188, episode reward: 9.842, mean reward: 0.141 [0.023, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.735 [-1.069, 10.200], loss: 0.004421, mae: 0.071901, mean_q: -0.008454
 17251/100000: episode: 191, duration: 0.379s, episode steps: 71, steps per second: 187, episode reward: 17.370, mean reward: 0.245 [0.023, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.732 [-0.751, 10.109], loss: 0.004299, mae: 0.070902, mean_q: -0.005580
 17321/100000: episode: 192, duration: 0.363s, episode steps: 70, steps per second: 193, episode reward: 11.441, mean reward: 0.163 [0.016, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.736 [-0.747, 10.229], loss: 0.004082, mae: 0.069032, mean_q: 0.009483
 17333/100000: episode: 193, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 3.447, mean reward: 0.287 [0.204, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.330, 10.100], loss: 0.003776, mae: 0.065577, mean_q: -0.110035
 17403/100000: episode: 194, duration: 0.352s, episode steps: 70, steps per second: 199, episode reward: 8.750, mean reward: 0.125 [0.001, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.742 [-0.208, 10.188], loss: 0.003822, mae: 0.066708, mean_q: 0.014579
 17473/100000: episode: 195, duration: 0.384s, episode steps: 70, steps per second: 182, episode reward: 14.045, mean reward: 0.201 [0.044, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.734 [-1.028, 10.100], loss: 0.003979, mae: 0.068043, mean_q: 0.044832
 17543/100000: episode: 196, duration: 0.358s, episode steps: 70, steps per second: 195, episode reward: 11.900, mean reward: 0.170 [0.028, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.738 [-1.062, 10.100], loss: 0.004013, mae: 0.069188, mean_q: 0.039172
 17614/100000: episode: 197, duration: 0.371s, episode steps: 71, steps per second: 191, episode reward: 13.974, mean reward: 0.197 [0.036, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.730 [-1.434, 10.100], loss: 0.004370, mae: 0.073157, mean_q: 0.032152
 17684/100000: episode: 198, duration: 0.345s, episode steps: 70, steps per second: 203, episode reward: 13.885, mean reward: 0.198 [0.016, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.744 [-0.073, 10.127], loss: 0.004544, mae: 0.073701, mean_q: 0.061086
 17755/100000: episode: 199, duration: 0.356s, episode steps: 71, steps per second: 199, episode reward: 17.676, mean reward: 0.249 [0.055, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.721 [-0.643, 10.100], loss: 0.004197, mae: 0.071083, mean_q: 0.054980
 17826/100000: episode: 200, duration: 0.373s, episode steps: 71, steps per second: 190, episode reward: 14.706, mean reward: 0.207 [0.041, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.725 [-0.565, 10.100], loss: 0.004621, mae: 0.073992, mean_q: 0.074388
 17897/100000: episode: 201, duration: 0.354s, episode steps: 71, steps per second: 201, episode reward: 16.287, mean reward: 0.229 [0.098, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.718 [-0.499, 10.100], loss: 0.004096, mae: 0.070493, mean_q: 0.100078
 17967/100000: episode: 202, duration: 0.362s, episode steps: 70, steps per second: 194, episode reward: 10.643, mean reward: 0.152 [0.022, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.741 [-0.805, 10.157], loss: 0.004108, mae: 0.069944, mean_q: 0.093064
 18037/100000: episode: 203, duration: 0.366s, episode steps: 70, steps per second: 191, episode reward: 11.859, mean reward: 0.169 [0.016, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.741 [-0.499, 10.182], loss: 0.004474, mae: 0.073531, mean_q: 0.087543
 18107/100000: episode: 204, duration: 0.353s, episode steps: 70, steps per second: 198, episode reward: 12.872, mean reward: 0.184 [0.042, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.718 [-0.524, 10.100], loss: 0.003787, mae: 0.067199, mean_q: 0.120257
 18177/100000: episode: 205, duration: 0.354s, episode steps: 70, steps per second: 198, episode reward: 12.317, mean reward: 0.176 [0.020, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.731 [-0.515, 10.100], loss: 0.004308, mae: 0.072206, mean_q: 0.123426
 18247/100000: episode: 206, duration: 0.365s, episode steps: 70, steps per second: 192, episode reward: 11.919, mean reward: 0.170 [0.012, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.739 [-1.037, 10.234], loss: 0.004023, mae: 0.070635, mean_q: 0.136302
 18318/100000: episode: 207, duration: 0.377s, episode steps: 71, steps per second: 188, episode reward: 15.550, mean reward: 0.219 [0.014, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.723 [-1.091, 10.100], loss: 0.008175, mae: 0.084988, mean_q: 0.143253
 18388/100000: episode: 208, duration: 0.349s, episode steps: 70, steps per second: 201, episode reward: 11.903, mean reward: 0.170 [0.011, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.740 [-0.785, 10.264], loss: 0.004497, mae: 0.073314, mean_q: 0.145412
 18458/100000: episode: 209, duration: 0.355s, episode steps: 70, steps per second: 197, episode reward: 10.601, mean reward: 0.151 [0.019, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.744 [-1.331, 10.293], loss: 0.004279, mae: 0.073874, mean_q: 0.146916
 18470/100000: episode: 210, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 2.558, mean reward: 0.213 [0.156, 0.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.608, 10.100], loss: 0.004191, mae: 0.071990, mean_q: 0.122101
 18482/100000: episode: 211, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 5.178, mean reward: 0.432 [0.334, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.488, 10.100], loss: 0.003732, mae: 0.070592, mean_q: 0.112173
 18552/100000: episode: 212, duration: 0.356s, episode steps: 70, steps per second: 197, episode reward: 7.130, mean reward: 0.102 [0.014, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.737 [-0.387, 10.142], loss: 0.004249, mae: 0.072539, mean_q: 0.183228
 18622/100000: episode: 213, duration: 0.361s, episode steps: 70, steps per second: 194, episode reward: 13.508, mean reward: 0.193 [0.018, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.727 [-0.893, 10.101], loss: 0.004566, mae: 0.073187, mean_q: 0.182707
 18692/100000: episode: 214, duration: 0.354s, episode steps: 70, steps per second: 198, episode reward: 14.779, mean reward: 0.211 [0.018, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.747 [-0.906, 10.358], loss: 0.004055, mae: 0.071502, mean_q: 0.221279
 18763/100000: episode: 215, duration: 0.347s, episode steps: 71, steps per second: 205, episode reward: 10.135, mean reward: 0.143 [0.009, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.728 [-0.807, 10.100], loss: 0.004315, mae: 0.073346, mean_q: 0.211828
 18779/100000: episode: 216, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 5.343, mean reward: 0.334 [0.239, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.420, 10.100], loss: 0.004535, mae: 0.075976, mean_q: 0.210944
 18795/100000: episode: 217, duration: 0.076s, episode steps: 16, steps per second: 210, episode reward: 5.717, mean reward: 0.357 [0.286, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.358, 10.100], loss: 0.004060, mae: 0.073760, mean_q: 0.268132
 18808/100000: episode: 218, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 4.293, mean reward: 0.330 [0.260, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.575, 10.100], loss: 0.003852, mae: 0.069938, mean_q: 0.263783
 18824/100000: episode: 219, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 6.156, mean reward: 0.385 [0.273, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.523, 10.100], loss: 0.004002, mae: 0.070768, mean_q: 0.179602
 18894/100000: episode: 220, duration: 0.359s, episode steps: 70, steps per second: 195, episode reward: 12.519, mean reward: 0.179 [0.027, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.723 [-0.388, 10.100], loss: 0.004466, mae: 0.074291, mean_q: 0.227150
 18964/100000: episode: 221, duration: 0.360s, episode steps: 70, steps per second: 194, episode reward: 10.244, mean reward: 0.146 [0.033, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.746 [-0.120, 10.180], loss: 0.004051, mae: 0.070765, mean_q: 0.237748
 19034/100000: episode: 222, duration: 0.376s, episode steps: 70, steps per second: 186, episode reward: 14.261, mean reward: 0.204 [0.018, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.732 [-1.134, 10.100], loss: 0.004568, mae: 0.075194, mean_q: 0.230528
 19104/100000: episode: 223, duration: 0.354s, episode steps: 70, steps per second: 198, episode reward: 15.671, mean reward: 0.224 [0.038, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.730 [-1.482, 10.138], loss: 0.004339, mae: 0.072478, mean_q: 0.243396
 19174/100000: episode: 224, duration: 0.361s, episode steps: 70, steps per second: 194, episode reward: 13.828, mean reward: 0.198 [0.046, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.742 [-0.870, 10.241], loss: 0.006911, mae: 0.084887, mean_q: 0.264009
 19244/100000: episode: 225, duration: 0.368s, episode steps: 70, steps per second: 190, episode reward: 9.814, mean reward: 0.140 [0.025, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.751 [-0.561, 10.209], loss: 0.004696, mae: 0.076257, mean_q: 0.272879
 19315/100000: episode: 226, duration: 0.374s, episode steps: 71, steps per second: 190, episode reward: 10.491, mean reward: 0.148 [0.013, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.729 [-1.206, 10.147], loss: 0.004569, mae: 0.075352, mean_q: 0.274217
 19385/100000: episode: 227, duration: 0.354s, episode steps: 70, steps per second: 198, episode reward: 11.266, mean reward: 0.161 [0.010, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.736 [-0.489, 10.100], loss: 0.004497, mae: 0.074633, mean_q: 0.289015
 19397/100000: episode: 228, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 3.176, mean reward: 0.265 [0.087, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.219, 10.100], loss: 0.004030, mae: 0.070258, mean_q: 0.299466
 19468/100000: episode: 229, duration: 0.363s, episode steps: 71, steps per second: 195, episode reward: 12.459, mean reward: 0.175 [0.027, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.732 [-0.721, 10.100], loss: 0.004819, mae: 0.079053, mean_q: 0.309029
 19538/100000: episode: 230, duration: 0.375s, episode steps: 70, steps per second: 187, episode reward: 13.522, mean reward: 0.193 [0.015, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.745 [-1.169, 10.426], loss: 0.004420, mae: 0.073847, mean_q: 0.310976
 19608/100000: episode: 231, duration: 0.370s, episode steps: 70, steps per second: 189, episode reward: 10.094, mean reward: 0.144 [0.011, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.738 [-0.751, 10.100], loss: 0.004609, mae: 0.076713, mean_q: 0.326806
 19678/100000: episode: 232, duration: 0.391s, episode steps: 70, steps per second: 179, episode reward: 10.208, mean reward: 0.146 [0.044, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.740 [-0.641, 10.181], loss: 0.004146, mae: 0.071761, mean_q: 0.327845
 19748/100000: episode: 233, duration: 0.370s, episode steps: 70, steps per second: 189, episode reward: 12.520, mean reward: 0.179 [0.025, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.733 [-1.844, 10.112], loss: 0.004260, mae: 0.073443, mean_q: 0.322165
 19818/100000: episode: 234, duration: 0.389s, episode steps: 70, steps per second: 180, episode reward: 12.913, mean reward: 0.184 [0.026, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.741 [-0.458, 10.532], loss: 0.004710, mae: 0.076358, mean_q: 0.344728
 19889/100000: episode: 235, duration: 0.378s, episode steps: 71, steps per second: 188, episode reward: 12.726, mean reward: 0.179 [0.028, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.734 [-0.322, 10.138], loss: 0.004561, mae: 0.075959, mean_q: 0.358984
 19905/100000: episode: 236, duration: 0.090s, episode steps: 16, steps per second: 179, episode reward: 5.258, mean reward: 0.329 [0.247, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.152, 10.100], loss: 0.004009, mae: 0.071322, mean_q: 0.355762
 19975/100000: episode: 237, duration: 0.377s, episode steps: 70, steps per second: 186, episode reward: 25.385, mean reward: 0.363 [0.116, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.730 [-0.545, 10.100], loss: 0.004604, mae: 0.076493, mean_q: 0.359930
 20045/100000: episode: 238, duration: 0.361s, episode steps: 70, steps per second: 194, episode reward: 11.028, mean reward: 0.158 [0.002, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.734 [-1.285, 10.172], loss: 0.005016, mae: 0.079060, mean_q: 0.361578
 20061/100000: episode: 239, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 5.469, mean reward: 0.342 [0.292, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.227, 10.100], loss: 0.004994, mae: 0.079595, mean_q: 0.360955
[Info] 200-TH LEVEL FOUND: 0.6594910621643066, Considering 10/90 traces
 20131/100000: episode: 240, duration: 4.260s, episode steps: 70, steps per second: 16, episode reward: 13.455, mean reward: 0.192 [0.010, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.728 [-0.774, 10.100], loss: 0.004763, mae: 0.076496, mean_q: 0.364991
 20140/100000: episode: 241, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 3.312, mean reward: 0.368 [0.303, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.279, 10.100], loss: 0.004084, mae: 0.074187, mean_q: 0.360667
 20152/100000: episode: 242, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 4.210, mean reward: 0.351 [0.314, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.195, 10.100], loss: 0.004072, mae: 0.071025, mean_q: 0.358640
 20166/100000: episode: 243, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 5.307, mean reward: 0.379 [0.313, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.437, 10.100], loss: 0.004561, mae: 0.076372, mean_q: 0.369357
 20178/100000: episode: 244, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 4.814, mean reward: 0.401 [0.309, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.349, 10.100], loss: 0.004498, mae: 0.073426, mean_q: 0.368601
 20191/100000: episode: 245, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 3.853, mean reward: 0.296 [0.242, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.435, 10.100], loss: 0.004472, mae: 0.074087, mean_q: 0.365159
 20202/100000: episode: 246, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 3.534, mean reward: 0.321 [0.169, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.335, 10.100], loss: 0.004845, mae: 0.080547, mean_q: 0.378458
 20206/100000: episode: 247, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 1.673, mean reward: 0.418 [0.400, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.473, 10.100], loss: 0.003843, mae: 0.072009, mean_q: 0.337412
 20218/100000: episode: 248, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 4.013, mean reward: 0.334 [0.290, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.597, 10.100], loss: 0.004683, mae: 0.078018, mean_q: 0.365870
 20232/100000: episode: 249, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 4.885, mean reward: 0.349 [0.235, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.432, 10.100], loss: 0.004476, mae: 0.075165, mean_q: 0.371435
 20236/100000: episode: 250, duration: 0.031s, episode steps: 4, steps per second: 127, episode reward: 1.688, mean reward: 0.422 [0.377, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.457, 10.100], loss: 0.003208, mae: 0.065634, mean_q: 0.350571
 20248/100000: episode: 251, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 4.870, mean reward: 0.406 [0.297, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.605, 10.100], loss: 0.004493, mae: 0.077683, mean_q: 0.374809
 20262/100000: episode: 252, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 4.950, mean reward: 0.354 [0.219, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.357, 10.100], loss: 0.004928, mae: 0.078281, mean_q: 0.377156
 20275/100000: episode: 253, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 5.216, mean reward: 0.401 [0.284, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.479, 10.100], loss: 0.004295, mae: 0.072991, mean_q: 0.356992
 20287/100000: episode: 254, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 3.407, mean reward: 0.284 [0.221, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.710, 10.100], loss: 0.004128, mae: 0.074487, mean_q: 0.374271
 20298/100000: episode: 255, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 3.033, mean reward: 0.276 [0.214, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.332, 10.100], loss: 0.003614, mae: 0.067191, mean_q: 0.372778
 20309/100000: episode: 256, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 4.289, mean reward: 0.390 [0.272, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.449, 10.100], loss: 0.003752, mae: 0.070363, mean_q: 0.364628
 20323/100000: episode: 257, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 6.073, mean reward: 0.434 [0.362, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.319, 10.100], loss: 0.004141, mae: 0.072373, mean_q: 0.375260
 20335/100000: episode: 258, duration: 0.077s, episode steps: 12, steps per second: 157, episode reward: 5.049, mean reward: 0.421 [0.374, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.331, 10.100], loss: 0.003897, mae: 0.069470, mean_q: 0.376385
 20349/100000: episode: 259, duration: 0.088s, episode steps: 14, steps per second: 159, episode reward: 3.741, mean reward: 0.267 [0.206, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.375, 10.100], loss: 0.004128, mae: 0.072905, mean_q: 0.380327
 20362/100000: episode: 260, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 4.558, mean reward: 0.351 [0.223, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.332, 10.100], loss: 0.004522, mae: 0.075997, mean_q: 0.372062
 20366/100000: episode: 261, duration: 0.028s, episode steps: 4, steps per second: 142, episode reward: 2.039, mean reward: 0.510 [0.497, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.447, 10.100], loss: 0.002799, mae: 0.060190, mean_q: 0.373821
 20378/100000: episode: 262, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 4.556, mean reward: 0.380 [0.289, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.280, 10.100], loss: 0.004979, mae: 0.078864, mean_q: 0.377313
 20387/100000: episode: 263, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 2.970, mean reward: 0.330 [0.283, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.349, 10.100], loss: 0.004882, mae: 0.078964, mean_q: 0.375609
 20391/100000: episode: 264, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 1.925, mean reward: 0.481 [0.453, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.490, 10.100], loss: 0.005474, mae: 0.083543, mean_q: 0.399950
 20400/100000: episode: 265, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 3.277, mean reward: 0.364 [0.302, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.918, 10.100], loss: 0.005071, mae: 0.083535, mean_q: 0.380294
 20404/100000: episode: 266, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 1.669, mean reward: 0.417 [0.396, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.435, 10.100], loss: 0.005230, mae: 0.082263, mean_q: 0.352820
 20417/100000: episode: 267, duration: 0.069s, episode steps: 13, steps per second: 190, episode reward: 5.306, mean reward: 0.408 [0.307, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.411, 10.100], loss: 0.005283, mae: 0.081236, mean_q: 0.392162
 20425/100000: episode: 268, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 2.857, mean reward: 0.357 [0.288, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.270, 10.100], loss: 0.006007, mae: 0.086269, mean_q: 0.383774
 20438/100000: episode: 269, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 4.381, mean reward: 0.337 [0.252, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.415, 10.100], loss: 0.004515, mae: 0.076331, mean_q: 0.389966
 20447/100000: episode: 270, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 2.937, mean reward: 0.326 [0.277, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.672, 10.100], loss: 0.005611, mae: 0.084713, mean_q: 0.393593
 20459/100000: episode: 271, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 4.527, mean reward: 0.377 [0.326, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.558, 10.100], loss: 0.004435, mae: 0.071957, mean_q: 0.376597
 20472/100000: episode: 272, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 4.100, mean reward: 0.315 [0.251, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.651, 10.100], loss: 0.004846, mae: 0.078133, mean_q: 0.399848
 20476/100000: episode: 273, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 1.463, mean reward: 0.366 [0.329, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.456, 10.100], loss: 0.005244, mae: 0.080412, mean_q: 0.369012
 20480/100000: episode: 274, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 1.781, mean reward: 0.445 [0.417, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.484, 10.100], loss: 0.004383, mae: 0.075023, mean_q: 0.405030
 20491/100000: episode: 275, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 2.827, mean reward: 0.257 [0.187, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.435, 10.100], loss: 0.004013, mae: 0.072445, mean_q: 0.373307
 20495/100000: episode: 276, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 1.834, mean reward: 0.459 [0.447, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.462, 10.100], loss: 0.004548, mae: 0.076952, mean_q: 0.373487
 20507/100000: episode: 277, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 4.612, mean reward: 0.384 [0.289, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.464, 10.100], loss: 0.004557, mae: 0.077369, mean_q: 0.397428
 20515/100000: episode: 278, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 2.643, mean reward: 0.330 [0.264, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.411, 10.100], loss: 0.003602, mae: 0.066518, mean_q: 0.376081
 20528/100000: episode: 279, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 4.936, mean reward: 0.380 [0.275, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.526, 10.100], loss: 0.004535, mae: 0.076168, mean_q: 0.392605
 20537/100000: episode: 280, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 3.270, mean reward: 0.363 [0.291, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.279, 10.100], loss: 0.004062, mae: 0.069673, mean_q: 0.379014
 20549/100000: episode: 281, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 4.382, mean reward: 0.365 [0.311, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.358, 10.100], loss: 0.004885, mae: 0.077439, mean_q: 0.394192
 20563/100000: episode: 282, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 4.380, mean reward: 0.313 [0.218, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.700, 10.100], loss: 0.004773, mae: 0.075730, mean_q: 0.382395
 20567/100000: episode: 283, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 1.946, mean reward: 0.486 [0.472, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.412, 10.100], loss: 0.005236, mae: 0.080641, mean_q: 0.405718
 20581/100000: episode: 284, duration: 0.078s, episode steps: 14, steps per second: 178, episode reward: 5.781, mean reward: 0.413 [0.319, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.268, 10.100], loss: 0.004225, mae: 0.072929, mean_q: 0.404151
 20589/100000: episode: 285, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 3.580, mean reward: 0.448 [0.327, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.484, 10.100], loss: 0.003662, mae: 0.067853, mean_q: 0.383681
 20603/100000: episode: 286, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 5.796, mean reward: 0.414 [0.367, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.508, 10.100], loss: 0.004844, mae: 0.078507, mean_q: 0.394481
 20612/100000: episode: 287, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 3.094, mean reward: 0.344 [0.306, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.360, 10.100], loss: 0.006625, mae: 0.090136, mean_q: 0.413026
 20626/100000: episode: 288, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 6.022, mean reward: 0.430 [0.297, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.416, 10.100], loss: 0.004021, mae: 0.071896, mean_q: 0.392167
 20640/100000: episode: 289, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 6.304, mean reward: 0.450 [0.390, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.256, 10.100], loss: 0.004522, mae: 0.077277, mean_q: 0.406352
 20652/100000: episode: 290, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 5.837, mean reward: 0.486 [0.431, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.885, 10.100], loss: 0.005189, mae: 0.080207, mean_q: 0.391532
 20656/100000: episode: 291, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 1.798, mean reward: 0.450 [0.425, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.464, 10.100], loss: 0.005650, mae: 0.085017, mean_q: 0.405635
 20664/100000: episode: 292, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 3.110, mean reward: 0.389 [0.235, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.433, 10.100], loss: 0.004630, mae: 0.074229, mean_q: 0.429312
 20668/100000: episode: 293, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 1.548, mean reward: 0.387 [0.359, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.476, 10.100], loss: 0.004467, mae: 0.078797, mean_q: 0.365089
 20682/100000: episode: 294, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 6.492, mean reward: 0.464 [0.359, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.303, 10.100], loss: 0.003741, mae: 0.070015, mean_q: 0.400317
 20694/100000: episode: 295, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 4.293, mean reward: 0.358 [0.313, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.380, 10.100], loss: 0.004792, mae: 0.076054, mean_q: 0.390844
 20708/100000: episode: 296, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 5.063, mean reward: 0.362 [0.291, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.349, 10.100], loss: 0.004234, mae: 0.073435, mean_q: 0.409682
 20712/100000: episode: 297, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 1.749, mean reward: 0.437 [0.366, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.495, 10.100], loss: 0.003705, mae: 0.072151, mean_q: 0.375776
 20724/100000: episode: 298, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 5.599, mean reward: 0.467 [0.404, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.693, 10.100], loss: 0.004193, mae: 0.071310, mean_q: 0.413310
 20732/100000: episode: 299, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 3.747, mean reward: 0.468 [0.330, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.883, 10.100], loss: 0.004737, mae: 0.078779, mean_q: 0.405187
 20746/100000: episode: 300, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 4.816, mean reward: 0.344 [0.298, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.559, 10.100], loss: 0.004793, mae: 0.076017, mean_q: 0.400772
 20760/100000: episode: 301, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 5.955, mean reward: 0.425 [0.324, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.811, 10.100], loss: 0.005005, mae: 0.077669, mean_q: 0.417489
 20771/100000: episode: 302, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 3.272, mean reward: 0.297 [0.214, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.406, 10.100], loss: 0.005160, mae: 0.082090, mean_q: 0.412454
 20780/100000: episode: 303, duration: 0.044s, episode steps: 9, steps per second: 203, episode reward: 3.237, mean reward: 0.360 [0.303, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.937, 10.100], loss: 0.005377, mae: 0.080511, mean_q: 0.411441
 20793/100000: episode: 304, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 3.943, mean reward: 0.303 [0.240, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.329, 10.100], loss: 0.004948, mae: 0.078330, mean_q: 0.409056
 20797/100000: episode: 305, duration: 0.029s, episode steps: 4, steps per second: 138, episode reward: 2.024, mean reward: 0.506 [0.471, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.366, 10.100], loss: 0.004138, mae: 0.072069, mean_q: 0.413649
 20805/100000: episode: 306, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 2.902, mean reward: 0.363 [0.286, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.443, 10.100], loss: 0.004645, mae: 0.078973, mean_q: 0.407566
 20819/100000: episode: 307, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 5.803, mean reward: 0.414 [0.349, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.325, 10.100], loss: 0.005176, mae: 0.079615, mean_q: 0.421509
 20833/100000: episode: 308, duration: 0.092s, episode steps: 14, steps per second: 153, episode reward: 4.984, mean reward: 0.356 [0.297, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.847, 10.100], loss: 0.005545, mae: 0.083849, mean_q: 0.409491
 20837/100000: episode: 309, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 1.673, mean reward: 0.418 [0.398, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.390, 10.100], loss: 0.003955, mae: 0.071198, mean_q: 0.414685
 20846/100000: episode: 310, duration: 0.063s, episode steps: 9, steps per second: 143, episode reward: 3.296, mean reward: 0.366 [0.286, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.463, 10.100], loss: 0.004052, mae: 0.072301, mean_q: 0.423082
 20860/100000: episode: 311, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 5.050, mean reward: 0.361 [0.289, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.206, 10.100], loss: 0.004564, mae: 0.076441, mean_q: 0.417869
 20871/100000: episode: 312, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 3.238, mean reward: 0.294 [0.235, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.321, 10.100], loss: 0.004419, mae: 0.074277, mean_q: 0.403138
 20885/100000: episode: 313, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 5.014, mean reward: 0.358 [0.208, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.121, 10.100], loss: 0.005384, mae: 0.083330, mean_q: 0.416358
 20896/100000: episode: 314, duration: 0.073s, episode steps: 11, steps per second: 150, episode reward: 3.501, mean reward: 0.318 [0.192, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.719, 10.100], loss: 0.004438, mae: 0.073297, mean_q: 0.411274
 20905/100000: episode: 315, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 3.743, mean reward: 0.416 [0.339, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.542, 10.100], loss: 0.004495, mae: 0.074911, mean_q: 0.396963
 20909/100000: episode: 316, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 1.874, mean reward: 0.468 [0.392, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.378, 10.100], loss: 0.005039, mae: 0.082046, mean_q: 0.431111
 20921/100000: episode: 317, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 4.632, mean reward: 0.386 [0.320, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.584, 10.100], loss: 0.004143, mae: 0.070780, mean_q: 0.408710
 20935/100000: episode: 318, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 5.065, mean reward: 0.362 [0.298, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.271, 10.100], loss: 0.003970, mae: 0.069552, mean_q: 0.416322
 20946/100000: episode: 319, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 3.196, mean reward: 0.291 [0.187, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.356, 10.100], loss: 0.004220, mae: 0.073754, mean_q: 0.417841
 20960/100000: episode: 320, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 5.478, mean reward: 0.391 [0.332, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.316, 10.100], loss: 0.003763, mae: 0.068580, mean_q: 0.414045
 20968/100000: episode: 321, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 2.868, mean reward: 0.358 [0.324, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.351, 10.100], loss: 0.004586, mae: 0.075388, mean_q: 0.409745
 20980/100000: episode: 322, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 4.261, mean reward: 0.355 [0.302, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.400, 10.100], loss: 0.004104, mae: 0.071779, mean_q: 0.418037
 20994/100000: episode: 323, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 5.426, mean reward: 0.388 [0.311, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.232, 10.100], loss: 0.005040, mae: 0.079303, mean_q: 0.414331
 21008/100000: episode: 324, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 5.732, mean reward: 0.409 [0.328, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.886, 10.100], loss: 0.005044, mae: 0.078220, mean_q: 0.397179
 21020/100000: episode: 325, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 6.245, mean reward: 0.520 [0.437, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.298, 10.100], loss: 0.004036, mae: 0.071640, mean_q: 0.436582
 21034/100000: episode: 326, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 4.995, mean reward: 0.357 [0.281, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.301, 10.100], loss: 0.003816, mae: 0.068418, mean_q: 0.415412
 21048/100000: episode: 327, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 4.234, mean reward: 0.302 [0.223, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.131, 10.100], loss: 0.004768, mae: 0.079193, mean_q: 0.427227
 21052/100000: episode: 328, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 1.543, mean reward: 0.386 [0.350, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.399, 10.100], loss: 0.004517, mae: 0.076674, mean_q: 0.438953
 21066/100000: episode: 329, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 5.145, mean reward: 0.368 [0.273, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.637, 10.100], loss: 0.004521, mae: 0.074004, mean_q: 0.403609
[Info] 300-TH LEVEL FOUND: 0.7901124358177185, Considering 10/90 traces
 21079/100000: episode: 330, duration: 3.933s, episode steps: 13, steps per second: 3, episode reward: 4.569, mean reward: 0.351 [0.227, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.088, 10.100], loss: 0.004355, mae: 0.072820, mean_q: 0.410068
 21086/100000: episode: 331, duration: 0.049s, episode steps: 7, steps per second: 144, episode reward: 3.077, mean reward: 0.440 [0.401, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.457, 10.100], loss: 0.003967, mae: 0.069756, mean_q: 0.436017
 21089/100000: episode: 332, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 1.347, mean reward: 0.449 [0.443, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.545, 10.100], loss: 0.005567, mae: 0.084410, mean_q: 0.438572
 21096/100000: episode: 333, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 2.809, mean reward: 0.401 [0.352, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.456, 10.100], loss: 0.003888, mae: 0.068536, mean_q: 0.413391
 21103/100000: episode: 334, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 3.253, mean reward: 0.465 [0.383, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.521, 10.100], loss: 0.003994, mae: 0.071364, mean_q: 0.462512
 21110/100000: episode: 335, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 2.864, mean reward: 0.409 [0.317, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.435, 10.100], loss: 0.004341, mae: 0.075598, mean_q: 0.467643
 21113/100000: episode: 336, duration: 0.021s, episode steps: 3, steps per second: 142, episode reward: 1.664, mean reward: 0.555 [0.552, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.473, 10.100], loss: 0.005440, mae: 0.082013, mean_q: 0.440270
 21122/100000: episode: 337, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 4.719, mean reward: 0.524 [0.439, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.430, 10.100], loss: 0.004974, mae: 0.076562, mean_q: 0.418563
 21129/100000: episode: 338, duration: 0.043s, episode steps: 7, steps per second: 165, episode reward: 2.711, mean reward: 0.387 [0.353, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.292, 10.100], loss: 0.003884, mae: 0.070471, mean_q: 0.433307
 21138/100000: episode: 339, duration: 0.054s, episode steps: 9, steps per second: 165, episode reward: 4.006, mean reward: 0.445 [0.335, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.274, 10.100], loss: 0.003756, mae: 0.065687, mean_q: 0.422062
 21143/100000: episode: 340, duration: 0.036s, episode steps: 5, steps per second: 138, episode reward: 2.333, mean reward: 0.467 [0.401, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.576, 10.100], loss: 0.004526, mae: 0.073215, mean_q: 0.424594
 21150/100000: episode: 341, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 2.804, mean reward: 0.401 [0.367, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.453, 10.100], loss: 0.004613, mae: 0.076742, mean_q: 0.396019
 21159/100000: episode: 342, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 3.220, mean reward: 0.358 [0.297, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.678, 10.100], loss: 0.004193, mae: 0.072802, mean_q: 0.434827
 21166/100000: episode: 343, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 2.919, mean reward: 0.417 [0.353, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.347, 10.100], loss: 0.004672, mae: 0.074940, mean_q: 0.433383
 21175/100000: episode: 344, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 3.981, mean reward: 0.442 [0.398, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.376, 10.100], loss: 0.004332, mae: 0.073053, mean_q: 0.451212
 21182/100000: episode: 345, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 3.239, mean reward: 0.463 [0.410, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.754, 10.100], loss: 0.005182, mae: 0.081653, mean_q: 0.443685
 21189/100000: episode: 346, duration: 0.047s, episode steps: 7, steps per second: 151, episode reward: 3.145, mean reward: 0.449 [0.379, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.365, 10.100], loss: 0.004705, mae: 0.076660, mean_q: 0.433011
 21192/100000: episode: 347, duration: 0.024s, episode steps: 3, steps per second: 124, episode reward: 1.620, mean reward: 0.540 [0.536, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.517, 10.100], loss: 0.004013, mae: 0.072496, mean_q: 0.424937
 21198/100000: episode: 348, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 2.645, mean reward: 0.441 [0.404, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.537, 10.100], loss: 0.004906, mae: 0.076749, mean_q: 0.446606
 21205/100000: episode: 349, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 2.674, mean reward: 0.382 [0.345, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.476, 10.100], loss: 0.004568, mae: 0.074963, mean_q: 0.464955
 21215/100000: episode: 350, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 4.116, mean reward: 0.412 [0.328, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.452, 10.100], loss: 0.003994, mae: 0.070296, mean_q: 0.456218
 21220/100000: episode: 351, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 2.403, mean reward: 0.481 [0.422, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.533, 10.100], loss: 0.004943, mae: 0.078208, mean_q: 0.425844
 21227/100000: episode: 352, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 3.491, mean reward: 0.499 [0.438, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.416, 10.100], loss: 0.004706, mae: 0.076133, mean_q: 0.439907
 21235/100000: episode: 353, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 3.648, mean reward: 0.456 [0.348, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.892, 10.100], loss: 0.005312, mae: 0.080137, mean_q: 0.432727
 21242/100000: episode: 354, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 2.930, mean reward: 0.419 [0.361, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.535, 10.100], loss: 0.005729, mae: 0.083784, mean_q: 0.424741
 21245/100000: episode: 355, duration: 0.027s, episode steps: 3, steps per second: 113, episode reward: 1.542, mean reward: 0.514 [0.484, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.488, 10.100], loss: 0.005283, mae: 0.079145, mean_q: 0.439648
 21251/100000: episode: 356, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 2.793, mean reward: 0.466 [0.378, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.640, 10.100], loss: 0.004599, mae: 0.075560, mean_q: 0.442842
 21257/100000: episode: 357, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 2.488, mean reward: 0.415 [0.324, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.592, 10.100], loss: 0.005222, mae: 0.077481, mean_q: 0.433683
 21263/100000: episode: 358, duration: 0.031s, episode steps: 6, steps per second: 192, episode reward: 2.500, mean reward: 0.417 [0.370, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.412, 10.100], loss: 0.004381, mae: 0.076015, mean_q: 0.445753
 21270/100000: episode: 359, duration: 0.043s, episode steps: 7, steps per second: 161, episode reward: 2.797, mean reward: 0.400 [0.349, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.402, 10.100], loss: 0.004858, mae: 0.078239, mean_q: 0.455264
 21277/100000: episode: 360, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 2.791, mean reward: 0.399 [0.378, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.393, 10.100], loss: 0.004346, mae: 0.074250, mean_q: 0.464347
 21283/100000: episode: 361, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 2.668, mean reward: 0.445 [0.339, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.567, 10.100], loss: 0.005073, mae: 0.075918, mean_q: 0.453483
 21290/100000: episode: 362, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 3.220, mean reward: 0.460 [0.372, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.882, 10.100], loss: 0.004530, mae: 0.074572, mean_q: 0.444185
 21298/100000: episode: 363, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 4.013, mean reward: 0.502 [0.434, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.694, 10.100], loss: 0.005365, mae: 0.080728, mean_q: 0.446249
 21303/100000: episode: 364, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 2.092, mean reward: 0.418 [0.393, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.428, 10.100], loss: 0.004979, mae: 0.081437, mean_q: 0.449303
 21311/100000: episode: 365, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 3.332, mean reward: 0.416 [0.399, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.533, 10.100], loss: 0.005174, mae: 0.079030, mean_q: 0.472242
 21318/100000: episode: 366, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 2.790, mean reward: 0.399 [0.371, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.395, 10.100], loss: 0.005585, mae: 0.079499, mean_q: 0.432286
 21325/100000: episode: 367, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 2.865, mean reward: 0.409 [0.359, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.256, 10.100], loss: 0.004741, mae: 0.077707, mean_q: 0.418715
 21332/100000: episode: 368, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 3.180, mean reward: 0.454 [0.340, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.608, 10.100], loss: 0.004668, mae: 0.073882, mean_q: 0.450720
 21340/100000: episode: 369, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 3.272, mean reward: 0.409 [0.332, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.372, 10.100], loss: 0.004282, mae: 0.073927, mean_q: 0.444551
 21349/100000: episode: 370, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 4.090, mean reward: 0.454 [0.343, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.312, 10.100], loss: 0.004089, mae: 0.070978, mean_q: 0.476764
 21356/100000: episode: 371, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 3.376, mean reward: 0.482 [0.438, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.399, 10.100], loss: 0.005323, mae: 0.081409, mean_q: 0.440111
 21364/100000: episode: 372, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 3.807, mean reward: 0.476 [0.369, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.453, 10.100], loss: 0.004232, mae: 0.072390, mean_q: 0.456629
 21370/100000: episode: 373, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 2.424, mean reward: 0.404 [0.340, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.402, 10.100], loss: 0.005510, mae: 0.080832, mean_q: 0.461177
 21378/100000: episode: 374, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 3.269, mean reward: 0.409 [0.330, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.637, 10.100], loss: 0.004807, mae: 0.076013, mean_q: 0.451608
 21385/100000: episode: 375, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 3.325, mean reward: 0.475 [0.394, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.392, 10.100], loss: 0.004857, mae: 0.077535, mean_q: 0.464285
 21392/100000: episode: 376, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 3.605, mean reward: 0.515 [0.433, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.303, 10.100], loss: 0.003787, mae: 0.072797, mean_q: 0.453651
 21395/100000: episode: 377, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 1.630, mean reward: 0.543 [0.532, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.493, 10.100], loss: 0.004712, mae: 0.072198, mean_q: 0.425066
 21402/100000: episode: 378, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 2.992, mean reward: 0.427 [0.339, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.410, 10.100], loss: 0.004409, mae: 0.073780, mean_q: 0.455187
 21412/100000: episode: 379, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 4.422, mean reward: 0.442 [0.390, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.283, 10.100], loss: 0.004212, mae: 0.071805, mean_q: 0.476431
 21417/100000: episode: 380, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 2.316, mean reward: 0.463 [0.430, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.455, 10.100], loss: 0.002981, mae: 0.060608, mean_q: 0.443556
 21420/100000: episode: 381, duration: 0.019s, episode steps: 3, steps per second: 162, episode reward: 1.539, mean reward: 0.513 [0.489, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.593, 10.100], loss: 0.003368, mae: 0.066701, mean_q: 0.445045
 21426/100000: episode: 382, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 2.980, mean reward: 0.497 [0.384, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.405, 10.100], loss: 0.004448, mae: 0.074661, mean_q: 0.457734
 21434/100000: episode: 383, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 2.894, mean reward: 0.362 [0.307, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.448, 10.100], loss: 0.004432, mae: 0.072249, mean_q: 0.422382
 21442/100000: episode: 384, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 2.984, mean reward: 0.373 [0.336, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.530, 10.100], loss: 0.003955, mae: 0.069769, mean_q: 0.445147
 21452/100000: episode: 385, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 4.521, mean reward: 0.452 [0.397, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.438, 10.100], loss: 0.003905, mae: 0.071206, mean_q: 0.459169
 21459/100000: episode: 386, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 3.018, mean reward: 0.431 [0.382, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.510, 10.100], loss: 0.004429, mae: 0.072676, mean_q: 0.447115
 21469/100000: episode: 387, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 4.609, mean reward: 0.461 [0.357, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-1.083, 10.100], loss: 0.005438, mae: 0.080692, mean_q: 0.440957
 21476/100000: episode: 388, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 3.113, mean reward: 0.445 [0.386, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.484, 10.100], loss: 0.005314, mae: 0.081655, mean_q: 0.474314
 21479/100000: episode: 389, duration: 0.021s, episode steps: 3, steps per second: 145, episode reward: 1.524, mean reward: 0.508 [0.464, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.551, 10.100], loss: 0.004581, mae: 0.075059, mean_q: 0.417542
 21487/100000: episode: 390, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 2.924, mean reward: 0.365 [0.323, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.415, 10.100], loss: 0.005028, mae: 0.077610, mean_q: 0.455222
 21493/100000: episode: 391, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 2.622, mean reward: 0.437 [0.396, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.544, 10.100], loss: 0.005166, mae: 0.076552, mean_q: 0.476682
 21499/100000: episode: 392, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 2.167, mean reward: 0.361 [0.333, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.589, 10.100], loss: 0.005260, mae: 0.081193, mean_q: 0.475353
 21506/100000: episode: 393, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 2.915, mean reward: 0.416 [0.399, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.389, 10.100], loss: 0.004744, mae: 0.076064, mean_q: 0.453017
 21516/100000: episode: 394, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 3.916, mean reward: 0.392 [0.333, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.409, 10.100], loss: 0.004612, mae: 0.072573, mean_q: 0.466884
 21524/100000: episode: 395, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 3.609, mean reward: 0.451 [0.399, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.500, 10.100], loss: 0.006091, mae: 0.085231, mean_q: 0.462131
 21531/100000: episode: 396, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 2.593, mean reward: 0.370 [0.309, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.326, 10.100], loss: 0.004483, mae: 0.073075, mean_q: 0.517780
 21539/100000: episode: 397, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 3.830, mean reward: 0.479 [0.392, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.416, 10.100], loss: 0.004957, mae: 0.079300, mean_q: 0.457977
 21546/100000: episode: 398, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 2.986, mean reward: 0.427 [0.321, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.541, 10.100], loss: 0.005196, mae: 0.078095, mean_q: 0.452274
 21555/100000: episode: 399, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 3.971, mean reward: 0.441 [0.400, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.270, 10.100], loss: 0.004329, mae: 0.073400, mean_q: 0.465623
 21561/100000: episode: 400, duration: 0.040s, episode steps: 6, steps per second: 149, episode reward: 2.243, mean reward: 0.374 [0.333, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.475, 10.100], loss: 0.004584, mae: 0.074848, mean_q: 0.486641
 21568/100000: episode: 401, duration: 0.045s, episode steps: 7, steps per second: 154, episode reward: 2.622, mean reward: 0.375 [0.350, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.243, 10.100], loss: 0.003923, mae: 0.070092, mean_q: 0.466524
 21575/100000: episode: 402, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 2.296, mean reward: 0.328 [0.276, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.430, 10.100], loss: 0.004726, mae: 0.074348, mean_q: 0.454742
 21581/100000: episode: 403, duration: 0.046s, episode steps: 6, steps per second: 129, episode reward: 2.657, mean reward: 0.443 [0.376, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-1.090, 10.100], loss: 0.004282, mae: 0.075616, mean_q: 0.484413
 21586/100000: episode: 404, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 2.026, mean reward: 0.405 [0.385, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.426, 10.100], loss: 0.005224, mae: 0.082285, mean_q: 0.447751
 21594/100000: episode: 405, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 3.393, mean reward: 0.424 [0.360, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.430, 10.100], loss: 0.005214, mae: 0.078405, mean_q: 0.465117
 21602/100000: episode: 406, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 3.009, mean reward: 0.376 [0.349, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.326, 10.100], loss: 0.004227, mae: 0.072469, mean_q: 0.481035
 21612/100000: episode: 407, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 5.339, mean reward: 0.534 [0.466, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.435, 10.100], loss: 0.004019, mae: 0.070851, mean_q: 0.472469
 21620/100000: episode: 408, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 2.172, mean reward: 0.272 [0.203, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.507, 10.100], loss: 0.005033, mae: 0.077100, mean_q: 0.468037
 21630/100000: episode: 409, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 4.010, mean reward: 0.401 [0.339, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.574, 10.100], loss: 0.004531, mae: 0.075134, mean_q: 0.491180
 21636/100000: episode: 410, duration: 0.044s, episode steps: 6, steps per second: 135, episode reward: 2.446, mean reward: 0.408 [0.352, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.771, 10.100], loss: 0.003869, mae: 0.072982, mean_q: 0.463086
 21639/100000: episode: 411, duration: 0.027s, episode steps: 3, steps per second: 113, episode reward: 1.454, mean reward: 0.485 [0.441, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.582, 10.100], loss: 0.004465, mae: 0.074648, mean_q: 0.443295
 21646/100000: episode: 412, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 2.521, mean reward: 0.360 [0.302, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.467, 10.100], loss: 0.005125, mae: 0.078123, mean_q: 0.450512
 21651/100000: episode: 413, duration: 0.026s, episode steps: 5, steps per second: 189, episode reward: 2.338, mean reward: 0.468 [0.424, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.497, 10.100], loss: 0.003736, mae: 0.067581, mean_q: 0.475989
 21661/100000: episode: 414, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 4.232, mean reward: 0.423 [0.343, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.300, 10.100], loss: 0.005023, mae: 0.077616, mean_q: 0.474465
 21668/100000: episode: 415, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 3.020, mean reward: 0.431 [0.354, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.393, 10.100], loss: 0.005101, mae: 0.076132, mean_q: 0.485296
 21675/100000: episode: 416, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 3.243, mean reward: 0.463 [0.385, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.431, 10.100], loss: 0.004798, mae: 0.076329, mean_q: 0.499561
 21681/100000: episode: 417, duration: 0.031s, episode steps: 6, steps per second: 193, episode reward: 2.193, mean reward: 0.366 [0.324, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.612, 10.100], loss: 0.004116, mae: 0.073115, mean_q: 0.454880
 21688/100000: episode: 418, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 3.037, mean reward: 0.434 [0.366, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.406, 10.100], loss: 0.003823, mae: 0.068278, mean_q: 0.509585
 21698/100000: episode: 419, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 4.607, mean reward: 0.461 [0.412, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.468, 10.100], loss: 0.004803, mae: 0.077390, mean_q: 0.452871
[Info] 400-TH LEVEL FOUND: 0.8299064040184021, Considering 10/90 traces
 21703/100000: episode: 420, duration: 3.909s, episode steps: 5, steps per second: 1, episode reward: 2.117, mean reward: 0.423 [0.374, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.537, 10.100], loss: 0.004210, mae: 0.071789, mean_q: 0.466035
 21708/100000: episode: 421, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 2.436, mean reward: 0.487 [0.423, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.443, 10.100], loss: 0.004980, mae: 0.080467, mean_q: 0.493826
 21712/100000: episode: 422, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 1.617, mean reward: 0.404 [0.390, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.473, 10.100], loss: 0.004821, mae: 0.080397, mean_q: 0.499487
 21717/100000: episode: 423, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 2.287, mean reward: 0.457 [0.428, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.538, 10.100], loss: 0.004002, mae: 0.068169, mean_q: 0.449794
 21722/100000: episode: 424, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 2.115, mean reward: 0.423 [0.363, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.529, 10.100], loss: 0.004644, mae: 0.075754, mean_q: 0.476236
 21727/100000: episode: 425, duration: 0.034s, episode steps: 5, steps per second: 149, episode reward: 2.236, mean reward: 0.447 [0.404, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.574, 10.100], loss: 0.005019, mae: 0.073179, mean_q: 0.493697
 21732/100000: episode: 426, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 2.041, mean reward: 0.408 [0.361, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.758, 10.100], loss: 0.004655, mae: 0.072594, mean_q: 0.488297
 21734/100000: episode: 427, duration: 0.015s, episode steps: 2, steps per second: 138, episode reward: 1.121, mean reward: 0.561 [0.557, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.542, 10.100], loss: 0.004815, mae: 0.075736, mean_q: 0.442547
 21739/100000: episode: 428, duration: 0.029s, episode steps: 5, steps per second: 170, episode reward: 2.491, mean reward: 0.498 [0.465, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.574, 10.100], loss: 0.005289, mae: 0.080072, mean_q: 0.488451
 21741/100000: episode: 429, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 1.120, mean reward: 0.560 [0.538, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.540, 10.100], loss: 0.004662, mae: 0.079128, mean_q: 0.483110
 21746/100000: episode: 430, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 2.265, mean reward: 0.453 [0.365, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.417, 10.100], loss: 0.004442, mae: 0.070947, mean_q: 0.479689
 21748/100000: episode: 431, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 1.098, mean reward: 0.549 [0.542, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.532, 10.100], loss: 0.003896, mae: 0.073702, mean_q: 0.513118
 21752/100000: episode: 432, duration: 0.032s, episode steps: 4, steps per second: 126, episode reward: 1.785, mean reward: 0.446 [0.394, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.508, 10.100], loss: 0.004379, mae: 0.077188, mean_q: 0.506890
 21754/100000: episode: 433, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.151, mean reward: 0.575 [0.545, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.487, 10.100], loss: 0.004505, mae: 0.076445, mean_q: 0.431754
 21759/100000: episode: 434, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 2.018, mean reward: 0.404 [0.340, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.408, 10.100], loss: 0.005076, mae: 0.078082, mean_q: 0.466280
 21764/100000: episode: 435, duration: 0.039s, episode steps: 5, steps per second: 129, episode reward: 2.376, mean reward: 0.475 [0.403, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.279, 10.100], loss: 0.004478, mae: 0.075241, mean_q: 0.491327
 21769/100000: episode: 436, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: 2.057, mean reward: 0.411 [0.390, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.584, 10.100], loss: 0.004520, mae: 0.075089, mean_q: 0.471788
 21771/100000: episode: 437, duration: 0.013s, episode steps: 2, steps per second: 153, episode reward: 1.195, mean reward: 0.598 [0.588, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.600, 10.100], loss: 0.006242, mae: 0.091374, mean_q: 0.462556
 21776/100000: episode: 438, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 2.054, mean reward: 0.411 [0.349, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.523, 10.100], loss: 0.003733, mae: 0.067118, mean_q: 0.502908
 21778/100000: episode: 439, duration: 0.019s, episode steps: 2, steps per second: 103, episode reward: 1.048, mean reward: 0.524 [0.515, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.518, 10.100], loss: 0.005766, mae: 0.080232, mean_q: 0.456673
 21782/100000: episode: 440, duration: 0.032s, episode steps: 4, steps per second: 126, episode reward: 1.665, mean reward: 0.416 [0.381, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.582, 10.100], loss: 0.003581, mae: 0.063572, mean_q: 0.508985
 21786/100000: episode: 441, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 1.931, mean reward: 0.483 [0.428, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.637, 10.100], loss: 0.005897, mae: 0.087651, mean_q: 0.490898
 21790/100000: episode: 442, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 2.306, mean reward: 0.577 [0.518, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.559, 10.100], loss: 0.006046, mae: 0.088441, mean_q: 0.515840
 21794/100000: episode: 443, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 1.979, mean reward: 0.495 [0.439, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.942, 10.100], loss: 0.004691, mae: 0.078536, mean_q: 0.509744
 21801/100000: episode: 444, duration: 0.036s, episode steps: 7, steps per second: 197, episode reward: 2.693, mean reward: 0.385 [0.359, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.455, 10.100], loss: 0.003813, mae: 0.070690, mean_q: 0.475609
 21808/100000: episode: 445, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 3.445, mean reward: 0.492 [0.392, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.476, 10.100], loss: 0.003997, mae: 0.072499, mean_q: 0.483299
 21815/100000: episode: 446, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 3.176, mean reward: 0.454 [0.403, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.484, 10.100], loss: 0.003759, mae: 0.068170, mean_q: 0.500296
 21817/100000: episode: 447, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 1.247, mean reward: 0.623 [0.597, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.490, 10.100], loss: 0.005061, mae: 0.079951, mean_q: 0.453025
 21822/100000: episode: 448, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 2.128, mean reward: 0.426 [0.411, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.522, 10.100], loss: 0.004296, mae: 0.074505, mean_q: 0.499828
 21826/100000: episode: 449, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 1.899, mean reward: 0.475 [0.436, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.595, 10.100], loss: 0.003541, mae: 0.066474, mean_q: 0.478088
 21831/100000: episode: 450, duration: 0.027s, episode steps: 5, steps per second: 186, episode reward: 2.528, mean reward: 0.506 [0.464, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.481, 10.100], loss: 0.004416, mae: 0.073714, mean_q: 0.531383
 21836/100000: episode: 451, duration: 0.037s, episode steps: 5, steps per second: 136, episode reward: 2.059, mean reward: 0.412 [0.368, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.525, 10.100], loss: 0.005435, mae: 0.082076, mean_q: 0.470563
 21840/100000: episode: 452, duration: 0.026s, episode steps: 4, steps per second: 157, episode reward: 1.749, mean reward: 0.437 [0.411, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.534, 10.100], loss: 0.005168, mae: 0.078399, mean_q: 0.442673
 21844/100000: episode: 453, duration: 0.025s, episode steps: 4, steps per second: 157, episode reward: 1.779, mean reward: 0.445 [0.409, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.489, 10.100], loss: 0.005820, mae: 0.084564, mean_q: 0.548133
 21846/100000: episode: 454, duration: 0.013s, episode steps: 2, steps per second: 153, episode reward: 1.108, mean reward: 0.554 [0.549, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.632, 10.100], loss: 0.006753, mae: 0.090805, mean_q: 0.440865
 21851/100000: episode: 455, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 2.365, mean reward: 0.473 [0.416, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.602, 10.100], loss: 0.006454, mae: 0.085051, mean_q: 0.500108
 21856/100000: episode: 456, duration: 0.034s, episode steps: 5, steps per second: 146, episode reward: 2.204, mean reward: 0.441 [0.410, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.489, 10.100], loss: 0.004550, mae: 0.077444, mean_q: 0.502892
 21860/100000: episode: 457, duration: 0.024s, episode steps: 4, steps per second: 165, episode reward: 2.285, mean reward: 0.571 [0.528, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.963, 10.100], loss: 0.003768, mae: 0.068934, mean_q: 0.513890
 21867/100000: episode: 458, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 3.160, mean reward: 0.451 [0.393, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.466, 10.100], loss: 0.004808, mae: 0.076190, mean_q: 0.503000
 21872/100000: episode: 459, duration: 0.027s, episode steps: 5, steps per second: 189, episode reward: 2.399, mean reward: 0.480 [0.396, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.604, 10.100], loss: 0.005938, mae: 0.087154, mean_q: 0.495252
 21877/100000: episode: 460, duration: 0.033s, episode steps: 5, steps per second: 154, episode reward: 2.348, mean reward: 0.470 [0.376, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.562, 10.100], loss: 0.004702, mae: 0.075052, mean_q: 0.481607
 21881/100000: episode: 461, duration: 0.025s, episode steps: 4, steps per second: 161, episode reward: 1.915, mean reward: 0.479 [0.390, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.413, 10.100], loss: 0.004109, mae: 0.073546, mean_q: 0.518908
 21885/100000: episode: 462, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 1.894, mean reward: 0.473 [0.441, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.481, 10.100], loss: 0.004834, mae: 0.076761, mean_q: 0.443481
 21890/100000: episode: 463, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 2.385, mean reward: 0.477 [0.397, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.029, 10.100], loss: 0.005198, mae: 0.076420, mean_q: 0.478063
 21895/100000: episode: 464, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 2.030, mean reward: 0.406 [0.342, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.571, 10.100], loss: 0.005695, mae: 0.082336, mean_q: 0.528411
 21899/100000: episode: 465, duration: 0.026s, episode steps: 4, steps per second: 151, episode reward: 2.120, mean reward: 0.530 [0.507, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.521, 10.100], loss: 0.004887, mae: 0.075776, mean_q: 0.484182
 21901/100000: episode: 466, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 1.077, mean reward: 0.538 [0.538, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.504, 10.100], loss: 0.004471, mae: 0.074066, mean_q: 0.542598
 21905/100000: episode: 467, duration: 0.031s, episode steps: 4, steps per second: 131, episode reward: 1.797, mean reward: 0.449 [0.430, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.613, 10.100], loss: 0.004142, mae: 0.071144, mean_q: 0.447292
 21910/100000: episode: 468, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 2.040, mean reward: 0.408 [0.368, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.619, 10.100], loss: 0.004654, mae: 0.076091, mean_q: 0.498980
 21915/100000: episode: 469, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 2.424, mean reward: 0.485 [0.437, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.615, 10.100], loss: 0.004719, mae: 0.077391, mean_q: 0.522313
 21920/100000: episode: 470, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 2.294, mean reward: 0.459 [0.420, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.678, 10.100], loss: 0.004257, mae: 0.071470, mean_q: 0.487312
 21927/100000: episode: 471, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 2.997, mean reward: 0.428 [0.346, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.724, 10.100], loss: 0.004665, mae: 0.071280, mean_q: 0.526663
 21934/100000: episode: 472, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 2.916, mean reward: 0.417 [0.299, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.502, 10.100], loss: 0.004066, mae: 0.071269, mean_q: 0.495889
 21939/100000: episode: 473, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 1.990, mean reward: 0.398 [0.364, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.546, 10.100], loss: 0.005475, mae: 0.080410, mean_q: 0.470496
 21943/100000: episode: 474, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 1.933, mean reward: 0.483 [0.451, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.613, 10.100], loss: 0.005538, mae: 0.079346, mean_q: 0.535855
 21947/100000: episode: 475, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 1.724, mean reward: 0.431 [0.408, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.631, 10.100], loss: 0.003942, mae: 0.072104, mean_q: 0.504061
 21952/100000: episode: 476, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 2.204, mean reward: 0.441 [0.428, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.179, 10.100], loss: 0.005118, mae: 0.077405, mean_q: 0.509351
 21956/100000: episode: 477, duration: 0.026s, episode steps: 4, steps per second: 157, episode reward: 1.700, mean reward: 0.425 [0.373, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.875, 10.100], loss: 0.005118, mae: 0.077441, mean_q: 0.489292
 21958/100000: episode: 478, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.935, mean reward: 0.467 [0.456, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.577, 10.100], loss: 0.005206, mae: 0.082299, mean_q: 0.478547
 21962/100000: episode: 479, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 1.742, mean reward: 0.435 [0.415, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.542, 10.100], loss: 0.004718, mae: 0.079239, mean_q: 0.504685
 21969/100000: episode: 480, duration: 0.039s, episode steps: 7, steps per second: 177, episode reward: 2.899, mean reward: 0.414 [0.389, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.493, 10.100], loss: 0.004700, mae: 0.077898, mean_q: 0.499577
 21974/100000: episode: 481, duration: 0.036s, episode steps: 5, steps per second: 138, episode reward: 2.107, mean reward: 0.421 [0.369, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.570, 10.100], loss: 0.005231, mae: 0.077468, mean_q: 0.481638
 21981/100000: episode: 482, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 2.858, mean reward: 0.408 [0.321, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.541, 10.100], loss: 0.003696, mae: 0.067716, mean_q: 0.519971
 21985/100000: episode: 483, duration: 0.025s, episode steps: 4, steps per second: 163, episode reward: 1.975, mean reward: 0.494 [0.479, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.554, 10.100], loss: 0.004469, mae: 0.074626, mean_q: 0.509905
 21989/100000: episode: 484, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 1.709, mean reward: 0.427 [0.395, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.460, 10.100], loss: 0.005482, mae: 0.080136, mean_q: 0.485689
 21993/100000: episode: 485, duration: 0.025s, episode steps: 4, steps per second: 157, episode reward: 1.643, mean reward: 0.411 [0.387, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.528, 10.100], loss: 0.004311, mae: 0.074472, mean_q: 0.508762
 21998/100000: episode: 486, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 1.934, mean reward: 0.387 [0.355, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.473, 10.100], loss: 0.004089, mae: 0.072370, mean_q: 0.515988
 22002/100000: episode: 487, duration: 0.028s, episode steps: 4, steps per second: 142, episode reward: 2.130, mean reward: 0.532 [0.469, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.424, 10.100], loss: 0.003312, mae: 0.063172, mean_q: 0.509121
 22004/100000: episode: 488, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 1.169, mean reward: 0.584 [0.575, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.507, 10.100], loss: 0.003120, mae: 0.061058, mean_q: 0.499646
 22008/100000: episode: 489, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 2.215, mean reward: 0.554 [0.530, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.529, 10.100], loss: 0.003642, mae: 0.066235, mean_q: 0.523986
 22010/100000: episode: 490, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 1.259, mean reward: 0.629 [0.613, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.586, 10.100], loss: 0.005808, mae: 0.087424, mean_q: 0.610050
 22014/100000: episode: 491, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 2.092, mean reward: 0.523 [0.433, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.838, 10.100], loss: 0.006414, mae: 0.089413, mean_q: 0.481600
 22019/100000: episode: 492, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 1.983, mean reward: 0.397 [0.356, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.545, 10.100], loss: 0.005573, mae: 0.082152, mean_q: 0.536900
 22023/100000: episode: 493, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 1.580, mean reward: 0.395 [0.377, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.540, 10.100], loss: 0.006105, mae: 0.087259, mean_q: 0.506290
 22027/100000: episode: 494, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 2.183, mean reward: 0.546 [0.451, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.514, 10.100], loss: 0.007358, mae: 0.090219, mean_q: 0.542754
 22032/100000: episode: 495, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 2.442, mean reward: 0.488 [0.417, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.567, 10.100], loss: 0.005814, mae: 0.083245, mean_q: 0.513567
 22036/100000: episode: 496, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 1.795, mean reward: 0.449 [0.372, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.430, 10.100], loss: 0.006890, mae: 0.094762, mean_q: 0.489044
 22041/100000: episode: 497, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 2.173, mean reward: 0.435 [0.393, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.558, 10.100], loss: 0.005343, mae: 0.082422, mean_q: 0.584481
 22046/100000: episode: 498, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 1.974, mean reward: 0.395 [0.297, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.655, 10.100], loss: 0.005530, mae: 0.085457, mean_q: 0.487940
 22050/100000: episode: 499, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 2.030, mean reward: 0.507 [0.430, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.582, 10.100], loss: 0.004698, mae: 0.074396, mean_q: 0.534221
 22055/100000: episode: 500, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: 2.083, mean reward: 0.417 [0.360, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.522, 10.100], loss: 0.003905, mae: 0.070671, mean_q: 0.492134
 22059/100000: episode: 501, duration: 0.027s, episode steps: 4, steps per second: 146, episode reward: 1.834, mean reward: 0.458 [0.433, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.640, 10.100], loss: 0.003797, mae: 0.068349, mean_q: 0.508123
 22064/100000: episode: 502, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 2.079, mean reward: 0.416 [0.388, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.475, 10.100], loss: 0.007210, mae: 0.088381, mean_q: 0.506097
 22071/100000: episode: 503, duration: 0.036s, episode steps: 7, steps per second: 197, episode reward: 2.538, mean reward: 0.363 [0.277, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.526, 10.100], loss: 0.004452, mae: 0.076496, mean_q: 0.524999
 22078/100000: episode: 504, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 3.297, mean reward: 0.471 [0.425, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.421, 10.100], loss: 0.005017, mae: 0.078603, mean_q: 0.493741
 22080/100000: episode: 505, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 1.133, mean reward: 0.566 [0.547, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.688, 10.100], loss: 0.005752, mae: 0.088247, mean_q: 0.541330
 22082/100000: episode: 506, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 1.031, mean reward: 0.515 [0.507, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.539, 10.100], loss: 0.004047, mae: 0.069433, mean_q: 0.535273
 22087/100000: episode: 507, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 2.180, mean reward: 0.436 [0.403, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.499, 10.100], loss: 0.003680, mae: 0.070635, mean_q: 0.510228
 22094/100000: episode: 508, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 2.940, mean reward: 0.420 [0.369, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.899, 10.100], loss: 0.004312, mae: 0.072271, mean_q: 0.469865
 22101/100000: episode: 509, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 2.752, mean reward: 0.393 [0.330, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.413, 10.100], loss: 0.005457, mae: 0.083215, mean_q: 0.512902
[Info] 500-TH LEVEL FOUND: 0.8816442489624023, Considering 19/81 traces
 22106/100000: episode: 510, duration: 3.927s, episode steps: 5, steps per second: 1, episode reward: 2.185, mean reward: 0.437 [0.371, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.582, 10.100], loss: 0.004218, mae: 0.072902, mean_q: 0.483231
 22111/100000: episode: 511, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 2.439, mean reward: 0.488 [0.435, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.514, 10.100], loss: 0.004178, mae: 0.072347, mean_q: 0.520505
 22116/100000: episode: 512, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 2.592, mean reward: 0.518 [0.476, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.486, 10.100], loss: 0.004755, mae: 0.078324, mean_q: 0.494555
 22121/100000: episode: 513, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 2.333, mean reward: 0.467 [0.439, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.525, 10.100], loss: 0.004318, mae: 0.070285, mean_q: 0.523709
 22126/100000: episode: 514, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 2.259, mean reward: 0.452 [0.434, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.485, 10.100], loss: 0.004322, mae: 0.074678, mean_q: 0.504455
 22130/100000: episode: 515, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 1.722, mean reward: 0.430 [0.404, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.447, 10.100], loss: 0.004472, mae: 0.073719, mean_q: 0.534455
 22135/100000: episode: 516, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 2.321, mean reward: 0.464 [0.449, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.473, 10.100], loss: 0.005121, mae: 0.077939, mean_q: 0.518970
 22140/100000: episode: 517, duration: 0.031s, episode steps: 5, steps per second: 164, episode reward: 2.329, mean reward: 0.466 [0.440, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.565, 10.100], loss: 0.004482, mae: 0.074205, mean_q: 0.472789
 22145/100000: episode: 518, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 2.394, mean reward: 0.479 [0.465, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.530, 10.100], loss: 0.004715, mae: 0.077797, mean_q: 0.524736
 22150/100000: episode: 519, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 2.293, mean reward: 0.459 [0.392, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.486, 10.100], loss: 0.003985, mae: 0.070318, mean_q: 0.491555
 22155/100000: episode: 520, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 2.092, mean reward: 0.418 [0.386, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.490, 10.100], loss: 0.005174, mae: 0.079343, mean_q: 0.550373
 22160/100000: episode: 521, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 2.310, mean reward: 0.462 [0.433, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.506, 10.100], loss: 0.003684, mae: 0.067877, mean_q: 0.539207
 22165/100000: episode: 522, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 2.496, mean reward: 0.499 [0.426, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.860, 10.100], loss: 0.004441, mae: 0.078283, mean_q: 0.530366
 22169/100000: episode: 523, duration: 0.027s, episode steps: 4, steps per second: 145, episode reward: 1.473, mean reward: 0.368 [0.326, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.521, 10.100], loss: 0.004232, mae: 0.074538, mean_q: 0.491625
 22174/100000: episode: 524, duration: 0.037s, episode steps: 5, steps per second: 137, episode reward: 2.507, mean reward: 0.501 [0.458, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.504, 10.100], loss: 0.004107, mae: 0.071062, mean_q: 0.541052
 22179/100000: episode: 525, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 2.387, mean reward: 0.477 [0.389, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.503, 10.100], loss: 0.005162, mae: 0.078494, mean_q: 0.475682
 22184/100000: episode: 526, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: 2.635, mean reward: 0.527 [0.430, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.630, 10.100], loss: 0.004646, mae: 0.075497, mean_q: 0.545551
 22188/100000: episode: 527, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 1.844, mean reward: 0.461 [0.444, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.597, 10.100], loss: 0.004371, mae: 0.075138, mean_q: 0.531337
 22191/100000: episode: 528, duration: 0.022s, episode steps: 3, steps per second: 139, episode reward: 1.697, mean reward: 0.566 [0.531, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.632, 10.100], loss: 0.004945, mae: 0.077821, mean_q: 0.519586
 22195/100000: episode: 529, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 1.594, mean reward: 0.399 [0.371, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.447, 10.100], loss: 0.005605, mae: 0.086177, mean_q: 0.507529
 22200/100000: episode: 530, duration: 0.037s, episode steps: 5, steps per second: 135, episode reward: 2.286, mean reward: 0.457 [0.422, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.508, 10.100], loss: 0.003851, mae: 0.073519, mean_q: 0.532330
 22205/100000: episode: 531, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 1.979, mean reward: 0.396 [0.377, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.429, 10.100], loss: 0.004253, mae: 0.072895, mean_q: 0.538250
 22210/100000: episode: 532, duration: 0.039s, episode steps: 5, steps per second: 129, episode reward: 2.196, mean reward: 0.439 [0.419, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.385, 10.100], loss: 0.003718, mae: 0.066314, mean_q: 0.503576
 22215/100000: episode: 533, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: 2.175, mean reward: 0.435 [0.404, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.947, 10.100], loss: 0.004676, mae: 0.075892, mean_q: 0.512689
 22220/100000: episode: 534, duration: 0.034s, episode steps: 5, steps per second: 149, episode reward: 2.459, mean reward: 0.492 [0.433, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.529, 10.100], loss: 0.004910, mae: 0.078279, mean_q: 0.526156
 22225/100000: episode: 535, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 2.458, mean reward: 0.492 [0.443, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.519, 10.100], loss: 0.005234, mae: 0.079499, mean_q: 0.501400
 22230/100000: episode: 536, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: 1.943, mean reward: 0.389 [0.351, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.488, 10.100], loss: 0.004584, mae: 0.077778, mean_q: 0.503543
 22235/100000: episode: 537, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 2.109, mean reward: 0.422 [0.377, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.391, 10.100], loss: 0.004159, mae: 0.072740, mean_q: 0.504634
 22240/100000: episode: 538, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 2.403, mean reward: 0.481 [0.427, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.598, 10.100], loss: 0.004456, mae: 0.076130, mean_q: 0.523271
 22246/100000: episode: 539, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 2.797, mean reward: 0.466 [0.437, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.406, 10.100], loss: 0.004798, mae: 0.076292, mean_q: 0.516164
 22249/100000: episode: 540, duration: 0.020s, episode steps: 3, steps per second: 152, episode reward: 1.475, mean reward: 0.492 [0.457, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.876, 10.100], loss: 0.004774, mae: 0.074365, mean_q: 0.495107
 22253/100000: episode: 541, duration: 0.031s, episode steps: 4, steps per second: 128, episode reward: 1.488, mean reward: 0.372 [0.308, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.432, 10.100], loss: 0.003656, mae: 0.067766, mean_q: 0.561672
 22258/100000: episode: 542, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 1.978, mean reward: 0.396 [0.353, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.466, 10.100], loss: 0.003226, mae: 0.065191, mean_q: 0.547171
 22263/100000: episode: 543, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 2.264, mean reward: 0.453 [0.403, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.503, 10.100], loss: 0.004484, mae: 0.075012, mean_q: 0.540456
 22268/100000: episode: 544, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 2.169, mean reward: 0.434 [0.411, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.520, 10.100], loss: 0.005090, mae: 0.077007, mean_q: 0.518623
 22273/100000: episode: 545, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 2.372, mean reward: 0.474 [0.397, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.512, 10.100], loss: 0.004882, mae: 0.077695, mean_q: 0.529107
 22278/100000: episode: 546, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 2.384, mean reward: 0.477 [0.405, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.503, 10.100], loss: 0.004747, mae: 0.073749, mean_q: 0.523317
 22283/100000: episode: 547, duration: 0.038s, episode steps: 5, steps per second: 133, episode reward: 2.628, mean reward: 0.526 [0.443, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.500, 10.100], loss: 0.004451, mae: 0.076279, mean_q: 0.518628
 22288/100000: episode: 548, duration: 0.034s, episode steps: 5, steps per second: 146, episode reward: 2.264, mean reward: 0.453 [0.418, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.441, 10.100], loss: 0.004870, mae: 0.078749, mean_q: 0.566782
 22293/100000: episode: 549, duration: 0.032s, episode steps: 5, steps per second: 154, episode reward: 1.790, mean reward: 0.358 [0.322, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.533, 10.100], loss: 0.004678, mae: 0.071919, mean_q: 0.485827
 22299/100000: episode: 550, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 2.847, mean reward: 0.475 [0.441, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.476, 10.100], loss: 0.003931, mae: 0.071505, mean_q: 0.531906
 22304/100000: episode: 551, duration: 0.027s, episode steps: 5, steps per second: 186, episode reward: 2.078, mean reward: 0.416 [0.373, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.564, 10.100], loss: 0.003848, mae: 0.067636, mean_q: 0.513197
 22309/100000: episode: 552, duration: 0.027s, episode steps: 5, steps per second: 186, episode reward: 2.510, mean reward: 0.502 [0.477, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.439, 10.100], loss: 0.004035, mae: 0.071018, mean_q: 0.562935
 22313/100000: episode: 553, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 1.669, mean reward: 0.417 [0.365, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.494, 10.100], loss: 0.004042, mae: 0.070411, mean_q: 0.570020
 22318/100000: episode: 554, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 2.407, mean reward: 0.481 [0.427, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.575, 10.100], loss: 0.004877, mae: 0.076457, mean_q: 0.505210
 22321/100000: episode: 555, duration: 0.024s, episode steps: 3, steps per second: 124, episode reward: 1.525, mean reward: 0.508 [0.466, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.522, 10.100], loss: 0.005573, mae: 0.083926, mean_q: 0.536787
 22325/100000: episode: 556, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 1.923, mean reward: 0.481 [0.427, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.503, 10.100], loss: 0.003490, mae: 0.065806, mean_q: 0.525132
 22329/100000: episode: 557, duration: 0.030s, episode steps: 4, steps per second: 135, episode reward: 1.685, mean reward: 0.421 [0.400, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.552, 10.100], loss: 0.004488, mae: 0.075680, mean_q: 0.531564
 22334/100000: episode: 558, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 2.194, mean reward: 0.439 [0.381, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.503, 10.100], loss: 0.005103, mae: 0.077139, mean_q: 0.542539
 22339/100000: episode: 559, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 2.410, mean reward: 0.482 [0.429, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.529, 10.100], loss: 0.005691, mae: 0.082275, mean_q: 0.490878
 22343/100000: episode: 560, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 2.030, mean reward: 0.507 [0.469, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.481, 10.100], loss: 0.005236, mae: 0.080986, mean_q: 0.543176
 22348/100000: episode: 561, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 2.699, mean reward: 0.540 [0.490, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.469, 10.100], loss: 0.004081, mae: 0.070839, mean_q: 0.509037
 22353/100000: episode: 562, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 2.577, mean reward: 0.515 [0.465, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.526, 10.100], loss: 0.004270, mae: 0.072769, mean_q: 0.586087
 22358/100000: episode: 563, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 2.321, mean reward: 0.464 [0.398, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.618, 10.100], loss: 0.003779, mae: 0.068874, mean_q: 0.535598
 22363/100000: episode: 564, duration: 0.030s, episode steps: 5, steps per second: 164, episode reward: 2.073, mean reward: 0.415 [0.378, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.477, 10.100], loss: 0.003587, mae: 0.068583, mean_q: 0.585605
 22366/100000: episode: 565, duration: 0.026s, episode steps: 3, steps per second: 115, episode reward: 1.428, mean reward: 0.476 [0.443, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.624, 10.100], loss: 0.004594, mae: 0.076319, mean_q: 0.513565
 22371/100000: episode: 566, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 2.566, mean reward: 0.513 [0.463, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.654, 10.100], loss: 0.004058, mae: 0.068423, mean_q: 0.558108
 22376/100000: episode: 567, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 2.456, mean reward: 0.491 [0.449, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.583, 10.100], loss: 0.004028, mae: 0.069272, mean_q: 0.541161
 22381/100000: episode: 568, duration: 0.026s, episode steps: 5, steps per second: 190, episode reward: 2.626, mean reward: 0.525 [0.473, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.556, 10.100], loss: 0.004129, mae: 0.072228, mean_q: 0.552065
 22385/100000: episode: 569, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 1.626, mean reward: 0.406 [0.374, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.527, 10.100], loss: 0.004554, mae: 0.074009, mean_q: 0.507929
 22390/100000: episode: 570, duration: 0.026s, episode steps: 5, steps per second: 189, episode reward: 2.280, mean reward: 0.456 [0.424, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.911, 10.100], loss: 0.003945, mae: 0.067311, mean_q: 0.551727
 22395/100000: episode: 571, duration: 0.029s, episode steps: 5, steps per second: 170, episode reward: 2.554, mean reward: 0.511 [0.478, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.584, 10.100], loss: 0.004507, mae: 0.072449, mean_q: 0.555007
 22401/100000: episode: 572, duration: 0.035s, episode steps: 6, steps per second: 174, episode reward: 3.051, mean reward: 0.509 [0.419, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.685, 10.100], loss: 0.004132, mae: 0.068209, mean_q: 0.550952
 22407/100000: episode: 573, duration: 0.031s, episode steps: 6, steps per second: 192, episode reward: 3.045, mean reward: 0.508 [0.460, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.523, 10.100], loss: 0.005933, mae: 0.089263, mean_q: 0.493320
 22412/100000: episode: 574, duration: 0.034s, episode steps: 5, steps per second: 149, episode reward: 2.180, mean reward: 0.436 [0.422, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.391, 10.100], loss: 0.005624, mae: 0.080875, mean_q: 0.567570
 22417/100000: episode: 575, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 2.256, mean reward: 0.451 [0.417, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.509, 10.100], loss: 0.005694, mae: 0.085691, mean_q: 0.559347
 22422/100000: episode: 576, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 2.284, mean reward: 0.457 [0.407, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.556, 10.100], loss: 0.005216, mae: 0.084060, mean_q: 0.532968
 22427/100000: episode: 577, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 2.523, mean reward: 0.505 [0.423, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.464, 10.100], loss: 0.004245, mae: 0.075439, mean_q: 0.566364
 22432/100000: episode: 578, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 1.924, mean reward: 0.385 [0.323, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.493, 10.100], loss: 0.003614, mae: 0.066657, mean_q: 0.550074
 22437/100000: episode: 579, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 2.103, mean reward: 0.421 [0.373, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.486, 10.100], loss: 0.004821, mae: 0.078401, mean_q: 0.521568
 22442/100000: episode: 580, duration: 0.035s, episode steps: 5, steps per second: 144, episode reward: 1.997, mean reward: 0.399 [0.384, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.506, 10.100], loss: 0.003427, mae: 0.063860, mean_q: 0.532770
 22447/100000: episode: 581, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 2.132, mean reward: 0.426 [0.366, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.495, 10.100], loss: 0.005033, mae: 0.078456, mean_q: 0.543202
 22452/100000: episode: 582, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 2.507, mean reward: 0.501 [0.453, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.481, 10.100], loss: 0.004152, mae: 0.071129, mean_q: 0.530784
 22457/100000: episode: 583, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 2.085, mean reward: 0.417 [0.374, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.483, 10.100], loss: 0.004500, mae: 0.073946, mean_q: 0.525237
 22462/100000: episode: 584, duration: 0.036s, episode steps: 5, steps per second: 138, episode reward: 2.212, mean reward: 0.442 [0.373, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.515, 10.100], loss: 0.004374, mae: 0.075033, mean_q: 0.565746
 22467/100000: episode: 585, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 2.487, mean reward: 0.497 [0.424, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.643, 10.100], loss: 0.004386, mae: 0.074992, mean_q: 0.507674
 22471/100000: episode: 586, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 2.130, mean reward: 0.533 [0.486, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.485, 10.100], loss: 0.005307, mae: 0.080907, mean_q: 0.564739
 22476/100000: episode: 587, duration: 0.042s, episode steps: 5, steps per second: 118, episode reward: 2.386, mean reward: 0.477 [0.433, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.447, 10.100], loss: 0.004985, mae: 0.077361, mean_q: 0.538848
 22481/100000: episode: 588, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 2.486, mean reward: 0.497 [0.478, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.504, 10.100], loss: 0.004278, mae: 0.072045, mean_q: 0.495440
 22486/100000: episode: 589, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 2.233, mean reward: 0.447 [0.388, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.517, 10.100], loss: 0.003693, mae: 0.067716, mean_q: 0.582474
 22490/100000: episode: 590, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 2.057, mean reward: 0.514 [0.464, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.596, 10.100], loss: 0.004036, mae: 0.071432, mean_q: 0.569311
[Info] 600-TH LEVEL FOUND: 0.8836285471916199, Considering 10/90 traces
 22495/100000: episode: 591, duration: 3.936s, episode steps: 5, steps per second: 1, episode reward: 2.754, mean reward: 0.551 [0.441, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.568, 10.100], loss: 0.003665, mae: 0.067275, mean_q: 0.583320
 22499/100000: episode: 592, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 1.892, mean reward: 0.473 [0.413, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.516, 10.100], loss: 0.003840, mae: 0.068422, mean_q: 0.540261
 22505/100000: episode: 593, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 2.593, mean reward: 0.432 [0.388, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.644, 10.100], loss: 0.004219, mae: 0.072431, mean_q: 0.541805
 22509/100000: episode: 594, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 1.907, mean reward: 0.477 [0.456, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.573, 10.100], loss: 0.003107, mae: 0.063304, mean_q: 0.545522
 22514/100000: episode: 595, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 2.373, mean reward: 0.475 [0.422, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.591, 10.100], loss: 0.004129, mae: 0.072631, mean_q: 0.568934
 22520/100000: episode: 596, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 3.025, mean reward: 0.504 [0.464, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.485, 10.100], loss: 0.004330, mae: 0.072779, mean_q: 0.534723
 22524/100000: episode: 597, duration: 0.031s, episode steps: 4, steps per second: 129, episode reward: 2.353, mean reward: 0.588 [0.546, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.450, 10.100], loss: 0.003012, mae: 0.060369, mean_q: 0.535488
 22528/100000: episode: 598, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 1.866, mean reward: 0.466 [0.424, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.492, 10.100], loss: 0.003745, mae: 0.065008, mean_q: 0.556313
 22534/100000: episode: 599, duration: 0.040s, episode steps: 6, steps per second: 148, episode reward: 3.040, mean reward: 0.507 [0.479, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.481, 10.100], loss: 0.005001, mae: 0.079304, mean_q: 0.551956
 22539/100000: episode: 600, duration: 0.036s, episode steps: 5, steps per second: 140, episode reward: 2.168, mean reward: 0.434 [0.382, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.491, 10.100], loss: 0.004057, mae: 0.072402, mean_q: 0.603918
 22543/100000: episode: 601, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 1.812, mean reward: 0.453 [0.443, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.563, 10.100], loss: 0.004089, mae: 0.071072, mean_q: 0.554640
 22547/100000: episode: 602, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 1.846, mean reward: 0.462 [0.429, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.585, 10.100], loss: 0.003965, mae: 0.067210, mean_q: 0.534162
 22551/100000: episode: 603, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 1.977, mean reward: 0.494 [0.473, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.524, 10.100], loss: 0.003979, mae: 0.071624, mean_q: 0.583796
 22555/100000: episode: 604, duration: 0.028s, episode steps: 4, steps per second: 145, episode reward: 2.001, mean reward: 0.500 [0.482, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.510, 10.100], loss: 0.004442, mae: 0.074970, mean_q: 0.561842
 22558/100000: episode: 605, duration: 0.024s, episode steps: 3, steps per second: 125, episode reward: 1.566, mean reward: 0.522 [0.499, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.668, 10.100], loss: 0.004039, mae: 0.071009, mean_q: 0.608963
 22561/100000: episode: 606, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 1.434, mean reward: 0.478 [0.444, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.677, 10.100], loss: 0.004799, mae: 0.075042, mean_q: 0.536306
 22567/100000: episode: 607, duration: 0.031s, episode steps: 6, steps per second: 194, episode reward: 3.028, mean reward: 0.505 [0.438, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.658, 10.100], loss: 0.004958, mae: 0.080552, mean_q: 0.530745
 22570/100000: episode: 608, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 1.463, mean reward: 0.488 [0.469, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.616, 10.100], loss: 0.003627, mae: 0.070187, mean_q: 0.565682
 22573/100000: episode: 609, duration: 0.023s, episode steps: 3, steps per second: 131, episode reward: 1.654, mean reward: 0.551 [0.528, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.658, 10.100], loss: 0.004811, mae: 0.076290, mean_q: 0.560615
 22576/100000: episode: 610, duration: 0.021s, episode steps: 3, steps per second: 145, episode reward: 1.505, mean reward: 0.502 [0.481, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.608, 10.100], loss: 0.004974, mae: 0.076222, mean_q: 0.564956
 22580/100000: episode: 611, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 2.058, mean reward: 0.514 [0.461, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.540, 10.100], loss: 0.003591, mae: 0.067942, mean_q: 0.572354
 22584/100000: episode: 612, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 2.098, mean reward: 0.525 [0.487, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.461, 10.100], loss: 0.003398, mae: 0.063381, mean_q: 0.587169
 22589/100000: episode: 613, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 2.357, mean reward: 0.471 [0.434, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.505, 10.100], loss: 0.005086, mae: 0.077884, mean_q: 0.554457
 22593/100000: episode: 614, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 1.795, mean reward: 0.449 [0.428, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.574, 10.100], loss: 0.004148, mae: 0.071598, mean_q: 0.589710
 22598/100000: episode: 615, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 2.286, mean reward: 0.457 [0.437, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.539, 10.100], loss: 0.004019, mae: 0.069822, mean_q: 0.529980
 22602/100000: episode: 616, duration: 0.034s, episode steps: 4, steps per second: 119, episode reward: 1.866, mean reward: 0.467 [0.430, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.597, 10.100], loss: 0.003732, mae: 0.068820, mean_q: 0.583555
 22608/100000: episode: 617, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 2.723, mean reward: 0.454 [0.413, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.413, 10.100], loss: 0.004395, mae: 0.073769, mean_q: 0.549202
 22614/100000: episode: 618, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 2.723, mean reward: 0.454 [0.402, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.588, 10.100], loss: 0.004196, mae: 0.074178, mean_q: 0.555146
 22617/100000: episode: 619, duration: 0.020s, episode steps: 3, steps per second: 153, episode reward: 1.615, mean reward: 0.538 [0.471, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.568, 10.100], loss: 0.003633, mae: 0.069370, mean_q: 0.575729
 22623/100000: episode: 620, duration: 0.037s, episode steps: 6, steps per second: 160, episode reward: 2.887, mean reward: 0.481 [0.378, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.903, 10.100], loss: 0.003948, mae: 0.070986, mean_q: 0.574876
 22627/100000: episode: 621, duration: 0.032s, episode steps: 4, steps per second: 126, episode reward: 2.309, mean reward: 0.577 [0.561, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.477, 10.100], loss: 0.003784, mae: 0.071524, mean_q: 0.574084
 22631/100000: episode: 622, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 1.809, mean reward: 0.452 [0.419, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.613, 10.100], loss: 0.003415, mae: 0.065745, mean_q: 0.568416
 22635/100000: episode: 623, duration: 0.028s, episode steps: 4, steps per second: 142, episode reward: 1.920, mean reward: 0.480 [0.444, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.575, 10.100], loss: 0.003491, mae: 0.065474, mean_q: 0.566023
 22640/100000: episode: 624, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 2.032, mean reward: 0.406 [0.358, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.430, 10.100], loss: 0.003814, mae: 0.070646, mean_q: 0.513416
 22645/100000: episode: 625, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 1.980, mean reward: 0.396 [0.370, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.485, 10.100], loss: 0.003577, mae: 0.064320, mean_q: 0.563719
 22649/100000: episode: 626, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 2.048, mean reward: 0.512 [0.464, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.671, 10.100], loss: 0.004153, mae: 0.072557, mean_q: 0.568985
 22653/100000: episode: 627, duration: 0.029s, episode steps: 4, steps per second: 140, episode reward: 1.609, mean reward: 0.402 [0.350, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.427, 10.100], loss: 0.003232, mae: 0.064057, mean_q: 0.583837
 22656/100000: episode: 628, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 1.543, mean reward: 0.514 [0.506, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.670, 10.100], loss: 0.004250, mae: 0.072047, mean_q: 0.593449
 22660/100000: episode: 629, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 2.009, mean reward: 0.502 [0.459, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.597, 10.100], loss: 0.004494, mae: 0.077441, mean_q: 0.528705
 22664/100000: episode: 630, duration: 0.024s, episode steps: 4, steps per second: 164, episode reward: 2.035, mean reward: 0.509 [0.467, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.545, 10.100], loss: 0.004376, mae: 0.071079, mean_q: 0.571486
 22667/100000: episode: 631, duration: 0.025s, episode steps: 3, steps per second: 122, episode reward: 1.418, mean reward: 0.473 [0.425, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.697, 10.100], loss: 0.003319, mae: 0.066256, mean_q: 0.575244
 22671/100000: episode: 632, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 1.868, mean reward: 0.467 [0.433, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.502, 10.100], loss: 0.005209, mae: 0.083193, mean_q: 0.507392
 22675/100000: episode: 633, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 2.079, mean reward: 0.520 [0.470, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.490, 10.100], loss: 0.005025, mae: 0.073357, mean_q: 0.607856
 22679/100000: episode: 634, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 2.292, mean reward: 0.573 [0.495, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.563, 10.100], loss: 0.005714, mae: 0.081673, mean_q: 0.524409
 22685/100000: episode: 635, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 2.985, mean reward: 0.498 [0.441, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.451, 10.100], loss: 0.005208, mae: 0.082274, mean_q: 0.544341
 22688/100000: episode: 636, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 1.329, mean reward: 0.443 [0.427, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.646, 10.100], loss: 0.005351, mae: 0.078805, mean_q: 0.580280
 22692/100000: episode: 637, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 1.773, mean reward: 0.443 [0.419, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.561, 10.100], loss: 0.004973, mae: 0.079752, mean_q: 0.555244
 22696/100000: episode: 638, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 1.984, mean reward: 0.496 [0.484, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.639, 10.100], loss: 0.005399, mae: 0.081971, mean_q: 0.590549
 22702/100000: episode: 639, duration: 0.034s, episode steps: 6, steps per second: 176, episode reward: 2.355, mean reward: 0.392 [0.368, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.677, 10.100], loss: 0.004917, mae: 0.079486, mean_q: 0.549334
 22706/100000: episode: 640, duration: 0.025s, episode steps: 4, steps per second: 157, episode reward: 2.175, mean reward: 0.544 [0.522, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.640, 10.100], loss: 0.004489, mae: 0.073232, mean_q: 0.510099
 22710/100000: episode: 641, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 2.230, mean reward: 0.557 [0.494, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.562, 10.100], loss: 0.005090, mae: 0.078643, mean_q: 0.576410
 22716/100000: episode: 642, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 2.754, mean reward: 0.459 [0.375, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.150, 10.100], loss: 0.003870, mae: 0.068769, mean_q: 0.587570
 22721/100000: episode: 643, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 2.941, mean reward: 0.588 [0.508, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.455, 10.100], loss: 0.004149, mae: 0.070758, mean_q: 0.545229
 22724/100000: episode: 644, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 1.534, mean reward: 0.511 [0.485, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.647, 10.100], loss: 0.003297, mae: 0.064587, mean_q: 0.608758
 22728/100000: episode: 645, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 1.924, mean reward: 0.481 [0.444, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.507, 10.100], loss: 0.003712, mae: 0.072246, mean_q: 0.608214
 22731/100000: episode: 646, duration: 0.018s, episode steps: 3, steps per second: 169, episode reward: 1.682, mean reward: 0.561 [0.516, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.672, 10.100], loss: 0.003030, mae: 0.058142, mean_q: 0.594512
 22736/100000: episode: 647, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 2.525, mean reward: 0.505 [0.447, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.503, 10.100], loss: 0.003548, mae: 0.065741, mean_q: 0.536340
 22740/100000: episode: 648, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 1.868, mean reward: 0.467 [0.442, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.509, 10.100], loss: 0.003468, mae: 0.063871, mean_q: 0.591747
 22744/100000: episode: 649, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 1.667, mean reward: 0.417 [0.382, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.444, 10.100], loss: 0.004021, mae: 0.068982, mean_q: 0.582695
 22748/100000: episode: 650, duration: 0.031s, episode steps: 4, steps per second: 127, episode reward: 2.093, mean reward: 0.523 [0.498, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.596, 10.100], loss: 0.003649, mae: 0.070087, mean_q: 0.598236
 22751/100000: episode: 651, duration: 0.019s, episode steps: 3, steps per second: 156, episode reward: 1.586, mean reward: 0.529 [0.510, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.627, 10.100], loss: 0.003550, mae: 0.065730, mean_q: 0.550515
 22754/100000: episode: 652, duration: 0.018s, episode steps: 3, steps per second: 171, episode reward: 1.531, mean reward: 0.510 [0.476, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.611, 10.100], loss: 0.004237, mae: 0.075237, mean_q: 0.535611
 22759/100000: episode: 653, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 2.398, mean reward: 0.480 [0.418, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.511, 10.100], loss: 0.003854, mae: 0.068557, mean_q: 0.588743
 22765/100000: episode: 654, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 2.677, mean reward: 0.446 [0.404, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.931, 10.100], loss: 0.003361, mae: 0.064818, mean_q: 0.578035
 22769/100000: episode: 655, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 2.103, mean reward: 0.526 [0.481, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.627, 10.100], loss: 0.005249, mae: 0.085377, mean_q: 0.619459
 22774/100000: episode: 656, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 2.307, mean reward: 0.461 [0.421, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.482, 10.100], loss: 0.003721, mae: 0.064935, mean_q: 0.577564
 22779/100000: episode: 657, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 2.062, mean reward: 0.412 [0.364, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.559, 10.100], loss: 0.003569, mae: 0.068820, mean_q: 0.569652
 22783/100000: episode: 658, duration: 0.025s, episode steps: 4, steps per second: 157, episode reward: 1.830, mean reward: 0.457 [0.445, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.499, 10.100], loss: 0.004389, mae: 0.074223, mean_q: 0.619775
 22789/100000: episode: 659, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 2.946, mean reward: 0.491 [0.439, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.532, 10.100], loss: 0.003863, mae: 0.067915, mean_q: 0.571383
 22793/100000: episode: 660, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 1.963, mean reward: 0.491 [0.439, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.539, 10.100], loss: 0.003467, mae: 0.062851, mean_q: 0.532000
 22797/100000: episode: 661, duration: 0.031s, episode steps: 4, steps per second: 129, episode reward: 2.149, mean reward: 0.537 [0.506, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.458, 10.100], loss: 0.002977, mae: 0.061263, mean_q: 0.633645
 22801/100000: episode: 662, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 1.804, mean reward: 0.451 [0.421, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.566, 10.100], loss: 0.003162, mae: 0.062180, mean_q: 0.548517
 22805/100000: episode: 663, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 2.028, mean reward: 0.507 [0.477, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.696, 10.100], loss: 0.003475, mae: 0.065156, mean_q: 0.567830
 22809/100000: episode: 664, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 2.046, mean reward: 0.512 [0.492, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.660, 10.100], loss: 0.004527, mae: 0.071653, mean_q: 0.576900
 22813/100000: episode: 665, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 2.001, mean reward: 0.500 [0.446, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.546, 10.100], loss: 0.003922, mae: 0.067831, mean_q: 0.639571
 22817/100000: episode: 666, duration: 0.031s, episode steps: 4, steps per second: 128, episode reward: 1.977, mean reward: 0.494 [0.466, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.576, 10.100], loss: 0.005116, mae: 0.077522, mean_q: 0.549630
 22821/100000: episode: 667, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 1.852, mean reward: 0.463 [0.447, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.588, 10.100], loss: 0.004966, mae: 0.078193, mean_q: 0.588565
 22825/100000: episode: 668, duration: 0.029s, episode steps: 4, steps per second: 136, episode reward: 1.635, mean reward: 0.409 [0.358, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.559, 10.100], loss: 0.003453, mae: 0.064586, mean_q: 0.603486
 22829/100000: episode: 669, duration: 0.030s, episode steps: 4, steps per second: 133, episode reward: 2.190, mean reward: 0.548 [0.491, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.541, 10.100], loss: 0.003682, mae: 0.067299, mean_q: 0.571028
 22832/100000: episode: 670, duration: 0.021s, episode steps: 3, steps per second: 144, episode reward: 1.297, mean reward: 0.432 [0.408, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.622, 10.100], loss: 0.004394, mae: 0.076353, mean_q: 0.620620
 22838/100000: episode: 671, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 2.181, mean reward: 0.364 [0.319, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.498, 10.100], loss: 0.003275, mae: 0.064769, mean_q: 0.581224
 22842/100000: episode: 672, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 2.311, mean reward: 0.578 [0.534, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.536, 10.100], loss: 0.004359, mae: 0.068617, mean_q: 0.620601
 22846/100000: episode: 673, duration: 0.028s, episode steps: 4, steps per second: 142, episode reward: 1.812, mean reward: 0.453 [0.416, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.531, 10.100], loss: 0.003316, mae: 0.066364, mean_q: 0.591821
 22852/100000: episode: 674, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 2.867, mean reward: 0.478 [0.424, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.662, 10.100], loss: 0.003852, mae: 0.072308, mean_q: 0.608045
 22855/100000: episode: 675, duration: 0.021s, episode steps: 3, steps per second: 144, episode reward: 1.549, mean reward: 0.516 [0.501, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.656, 10.100], loss: 0.004370, mae: 0.074379, mean_q: 0.596480
 22861/100000: episode: 676, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 2.158, mean reward: 0.360 [0.330, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.527, 10.100], loss: 0.004115, mae: 0.070393, mean_q: 0.559478
 22865/100000: episode: 677, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 2.249, mean reward: 0.562 [0.530, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.525, 10.100], loss: 0.004253, mae: 0.069173, mean_q: 0.574182
 22869/100000: episode: 678, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 2.066, mean reward: 0.517 [0.481, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.140, 10.100], loss: 0.003485, mae: 0.066881, mean_q: 0.604013
 22872/100000: episode: 679, duration: 0.022s, episode steps: 3, steps per second: 138, episode reward: 1.642, mean reward: 0.547 [0.513, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.650, 10.100], loss: 0.003492, mae: 0.065498, mean_q: 0.601892
 22878/100000: episode: 680, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 2.713, mean reward: 0.452 [0.387, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.586, 10.100], loss: 0.004508, mae: 0.076932, mean_q: 0.580034
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.8836285471916199
 22882/100000: episode: 681, duration: 3.893s, episode steps: 4, steps per second: 1, episode reward: 2.173, mean reward: 0.543 [0.497, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.636, 10.100], loss: 0.003090, mae: 0.060511, mean_q: 0.593959
 22982/100000: episode: 682, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.235, mean reward: -0.162 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.144, 10.377], loss: 0.004162, mae: 0.070465, mean_q: 0.571540
 23082/100000: episode: 683, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.451, mean reward: -0.155 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.198, 10.116], loss: 0.004739, mae: 0.072994, mean_q: 0.564388
 23182/100000: episode: 684, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -19.420, mean reward: -0.194 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.244, 10.191], loss: 0.003896, mae: 0.069152, mean_q: 0.543499
 23282/100000: episode: 685, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.398, mean reward: -0.154 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.742, 10.126], loss: 0.004181, mae: 0.069158, mean_q: 0.526303
 23382/100000: episode: 686, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.588, mean reward: -0.166 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.484, 10.098], loss: 0.003914, mae: 0.068242, mean_q: 0.530490
 23482/100000: episode: 687, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -15.474, mean reward: -0.155 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.159, 10.098], loss: 0.004691, mae: 0.070188, mean_q: 0.492009
 23582/100000: episode: 688, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.180, mean reward: -0.142 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.960, 10.098], loss: 0.003992, mae: 0.066362, mean_q: 0.461102
 23682/100000: episode: 689, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.865, mean reward: -0.179 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.458, 10.098], loss: 0.003968, mae: 0.065551, mean_q: 0.457345
 23782/100000: episode: 690, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.348, mean reward: -0.153 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.833, 10.226], loss: 0.003523, mae: 0.065085, mean_q: 0.450486
 23882/100000: episode: 691, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -20.404, mean reward: -0.204 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.719, 10.130], loss: 0.004884, mae: 0.067830, mean_q: 0.416929
 23982/100000: episode: 692, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.085, mean reward: -0.181 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.582, 10.199], loss: 0.004169, mae: 0.068314, mean_q: 0.414187
 24082/100000: episode: 693, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -13.899, mean reward: -0.139 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.451, 10.392], loss: 0.004249, mae: 0.069362, mean_q: 0.421224
 24182/100000: episode: 694, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.036, mean reward: -0.140 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.721, 10.366], loss: 0.004254, mae: 0.067421, mean_q: 0.389304
 24282/100000: episode: 695, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -17.415, mean reward: -0.174 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.315, 10.294], loss: 0.003513, mae: 0.063357, mean_q: 0.383817
 24382/100000: episode: 696, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -13.030, mean reward: -0.130 [-1.000, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.363, 10.098], loss: 0.003726, mae: 0.064454, mean_q: 0.383683
 24482/100000: episode: 697, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.788, mean reward: -0.188 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.407, 10.098], loss: 0.003799, mae: 0.065413, mean_q: 0.358913
 24582/100000: episode: 698, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.744, mean reward: -0.167 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.726, 10.098], loss: 0.003693, mae: 0.064044, mean_q: 0.345335
 24682/100000: episode: 699, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -20.279, mean reward: -0.203 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.299, 10.110], loss: 0.003352, mae: 0.061933, mean_q: 0.306335
 24782/100000: episode: 700, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.510, mean reward: -0.155 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.722, 10.225], loss: 0.003126, mae: 0.060543, mean_q: 0.298326
 24882/100000: episode: 701, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -15.743, mean reward: -0.157 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.231, 10.378], loss: 0.002959, mae: 0.058716, mean_q: 0.248176
 24982/100000: episode: 702, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.440, mean reward: -0.184 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.773, 10.098], loss: 0.003216, mae: 0.061691, mean_q: 0.236358
 25082/100000: episode: 703, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.028, mean reward: -0.150 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.122, 10.266], loss: 0.002915, mae: 0.058038, mean_q: 0.236730
 25182/100000: episode: 704, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -17.956, mean reward: -0.180 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.910, 10.200], loss: 0.004514, mae: 0.066749, mean_q: 0.222308
 25282/100000: episode: 705, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -12.887, mean reward: -0.129 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.428, 10.098], loss: 0.003682, mae: 0.063663, mean_q: 0.181665
 25382/100000: episode: 706, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -12.358, mean reward: -0.124 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.608, 10.098], loss: 0.003100, mae: 0.058566, mean_q: 0.185123
 25482/100000: episode: 707, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -10.854, mean reward: -0.109 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.822, 10.098], loss: 0.003503, mae: 0.061798, mean_q: 0.125097
 25582/100000: episode: 708, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.063, mean reward: -0.141 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.186, 10.098], loss: 0.003045, mae: 0.058769, mean_q: 0.123438
 25682/100000: episode: 709, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -9.532, mean reward: -0.095 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.853, 10.450], loss: 0.002963, mae: 0.057207, mean_q: 0.111278
 25782/100000: episode: 710, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -15.060, mean reward: -0.151 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.122, 10.098], loss: 0.003434, mae: 0.061272, mean_q: 0.093076
 25882/100000: episode: 711, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.759, mean reward: -0.178 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.480, 10.098], loss: 0.003114, mae: 0.059025, mean_q: 0.076521
 25982/100000: episode: 712, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.799, mean reward: -0.178 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.783, 10.098], loss: 0.003232, mae: 0.059815, mean_q: 0.045784
 26082/100000: episode: 713, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -13.005, mean reward: -0.130 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.335, 10.098], loss: 0.003048, mae: 0.058240, mean_q: 0.021222
 26182/100000: episode: 714, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.158, mean reward: -0.162 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.547, 10.098], loss: 0.003479, mae: 0.061228, mean_q: -0.020514
 26282/100000: episode: 715, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.723, mean reward: -0.177 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.083, 10.326], loss: 0.003660, mae: 0.061329, mean_q: -0.017357
 26382/100000: episode: 716, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.323, mean reward: -0.193 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.293, 10.098], loss: 0.003853, mae: 0.063538, mean_q: -0.038202
 26482/100000: episode: 717, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.374, mean reward: -0.194 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.291, 10.389], loss: 0.003146, mae: 0.058339, mean_q: -0.058962
 26582/100000: episode: 718, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.105, mean reward: -0.181 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.743, 10.098], loss: 0.003042, mae: 0.057901, mean_q: -0.099249
 26682/100000: episode: 719, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -15.782, mean reward: -0.158 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.152, 10.098], loss: 0.003191, mae: 0.059731, mean_q: -0.097815
 26782/100000: episode: 720, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -15.422, mean reward: -0.154 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.238, 10.384], loss: 0.002877, mae: 0.055826, mean_q: -0.130102
 26882/100000: episode: 721, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.050, mean reward: -0.161 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.959, 10.098], loss: 0.002727, mae: 0.054870, mean_q: -0.103546
 26982/100000: episode: 722, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.915, mean reward: -0.159 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.953, 10.138], loss: 0.002903, mae: 0.055800, mean_q: -0.147310
 27082/100000: episode: 723, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.872, mean reward: -0.169 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.545, 10.284], loss: 0.002968, mae: 0.056240, mean_q: -0.153581
 27182/100000: episode: 724, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.933, mean reward: -0.179 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.762, 10.181], loss: 0.003205, mae: 0.058838, mean_q: -0.156828
 27282/100000: episode: 725, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -15.540, mean reward: -0.155 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.258, 10.098], loss: 0.002911, mae: 0.056551, mean_q: -0.174042
 27382/100000: episode: 726, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -15.872, mean reward: -0.159 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.467, 10.198], loss: 0.002632, mae: 0.053415, mean_q: -0.222610
 27482/100000: episode: 727, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: -17.492, mean reward: -0.175 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.172, 10.157], loss: 0.003425, mae: 0.058536, mean_q: -0.255136
 27582/100000: episode: 728, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.431, mean reward: -0.184 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.093, 10.098], loss: 0.002725, mae: 0.053342, mean_q: -0.250237
 27682/100000: episode: 729, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.289, mean reward: -0.153 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.142, 10.241], loss: 0.002930, mae: 0.055741, mean_q: -0.265253
 27782/100000: episode: 730, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -19.012, mean reward: -0.190 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.581, 10.180], loss: 0.002706, mae: 0.052919, mean_q: -0.304810
 27882/100000: episode: 731, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.711, mean reward: -0.187 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.338, 10.098], loss: 0.002910, mae: 0.055106, mean_q: -0.291936
 27982/100000: episode: 732, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.329, mean reward: -0.193 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.326, 10.098], loss: 0.002751, mae: 0.053699, mean_q: -0.325418
 28082/100000: episode: 733, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.276, mean reward: -0.173 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.185, 10.265], loss: 0.002860, mae: 0.056386, mean_q: -0.259795
 28182/100000: episode: 734, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.895, mean reward: -0.169 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.852, 10.098], loss: 0.002811, mae: 0.054746, mean_q: -0.294649
 28282/100000: episode: 735, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.354, mean reward: -0.194 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.772, 10.105], loss: 0.002874, mae: 0.054959, mean_q: -0.305101
 28382/100000: episode: 736, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.205, mean reward: -0.162 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.714, 10.098], loss: 0.003293, mae: 0.058034, mean_q: -0.291212
 28482/100000: episode: 737, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.455, mean reward: -0.155 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.033, 10.508], loss: 0.006312, mae: 0.070812, mean_q: -0.330965
 28582/100000: episode: 738, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.640, mean reward: -0.196 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.795, 10.174], loss: 0.002846, mae: 0.055271, mean_q: -0.285434
 28682/100000: episode: 739, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.711, mean reward: -0.187 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.714, 10.177], loss: 0.002393, mae: 0.049273, mean_q: -0.334351
 28782/100000: episode: 740, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -13.076, mean reward: -0.131 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.884, 10.302], loss: 0.002891, mae: 0.054807, mean_q: -0.283852
 28882/100000: episode: 741, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -18.702, mean reward: -0.187 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.426, 10.098], loss: 0.002661, mae: 0.051536, mean_q: -0.327761
 28982/100000: episode: 742, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.990, mean reward: -0.190 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.079, 10.098], loss: 0.002832, mae: 0.054220, mean_q: -0.315279
 29082/100000: episode: 743, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.219, mean reward: -0.192 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.557, 10.288], loss: 0.002809, mae: 0.055305, mean_q: -0.291398
 29182/100000: episode: 744, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -20.256, mean reward: -0.203 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.477, 10.285], loss: 0.002806, mae: 0.054097, mean_q: -0.309413
 29282/100000: episode: 745, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -15.693, mean reward: -0.157 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.805, 10.098], loss: 0.003197, mae: 0.058480, mean_q: -0.302187
 29382/100000: episode: 746, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.590, mean reward: -0.186 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.773, 10.180], loss: 0.002763, mae: 0.054025, mean_q: -0.362680
 29482/100000: episode: 747, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.769, mean reward: -0.158 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.207, 10.391], loss: 0.002583, mae: 0.051439, mean_q: -0.295024
 29582/100000: episode: 748, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.245, mean reward: -0.172 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.246, 10.329], loss: 0.002632, mae: 0.051691, mean_q: -0.313486
 29682/100000: episode: 749, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.364, mean reward: -0.184 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.595, 10.166], loss: 0.002502, mae: 0.051507, mean_q: -0.320283
 29782/100000: episode: 750, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -14.942, mean reward: -0.149 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.618, 10.279], loss: 0.002727, mae: 0.054029, mean_q: -0.336222
 29882/100000: episode: 751, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -14.810, mean reward: -0.148 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.343, 10.234], loss: 0.002403, mae: 0.049368, mean_q: -0.317624
 29982/100000: episode: 752, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.813, mean reward: -0.158 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.012, 10.098], loss: 0.002493, mae: 0.051142, mean_q: -0.301711
 30082/100000: episode: 753, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.942, mean reward: -0.159 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.617, 10.098], loss: 0.002547, mae: 0.053045, mean_q: -0.301269
 30182/100000: episode: 754, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.261, mean reward: -0.183 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.602, 10.098], loss: 0.002496, mae: 0.051301, mean_q: -0.322271
 30282/100000: episode: 755, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.349, mean reward: -0.153 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.298, 10.209], loss: 0.002575, mae: 0.052725, mean_q: -0.311251
 30382/100000: episode: 756, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -13.857, mean reward: -0.139 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.555, 10.098], loss: 0.002550, mae: 0.051519, mean_q: -0.323026
 30482/100000: episode: 757, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -15.746, mean reward: -0.157 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.002, 10.264], loss: 0.002434, mae: 0.050876, mean_q: -0.304438
 30582/100000: episode: 758, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -13.643, mean reward: -0.136 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.095, 10.098], loss: 0.002474, mae: 0.050684, mean_q: -0.312744
 30682/100000: episode: 759, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.606, mean reward: -0.176 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.309, 10.271], loss: 0.002321, mae: 0.048850, mean_q: -0.325747
 30782/100000: episode: 760, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.888, mean reward: -0.179 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.793, 10.319], loss: 0.002628, mae: 0.052881, mean_q: -0.287253
 30882/100000: episode: 761, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.582, mean reward: -0.166 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.256, 10.105], loss: 0.003266, mae: 0.055547, mean_q: -0.329230
 30982/100000: episode: 762, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.281, mean reward: -0.193 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.998, 10.136], loss: 0.002803, mae: 0.054591, mean_q: -0.304941
 31082/100000: episode: 763, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.997, mean reward: -0.190 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.665, 10.198], loss: 0.002292, mae: 0.048208, mean_q: -0.332072
 31182/100000: episode: 764, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.474, mean reward: -0.145 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.887, 10.207], loss: 0.002542, mae: 0.051036, mean_q: -0.302025
 31282/100000: episode: 765, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.902, mean reward: -0.169 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.013, 10.215], loss: 0.004183, mae: 0.062234, mean_q: -0.307129
 31382/100000: episode: 766, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.762, mean reward: -0.168 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.902, 10.098], loss: 0.002449, mae: 0.051429, mean_q: -0.329264
 31482/100000: episode: 767, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.293, mean reward: -0.183 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.156, 10.098], loss: 0.002228, mae: 0.047604, mean_q: -0.336152
 31582/100000: episode: 768, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.650, mean reward: -0.177 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.583, 10.098], loss: 0.002448, mae: 0.050694, mean_q: -0.313996
 31682/100000: episode: 769, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -15.016, mean reward: -0.150 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.645, 10.134], loss: 0.002335, mae: 0.049377, mean_q: -0.280868
 31782/100000: episode: 770, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -19.784, mean reward: -0.198 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.698, 10.098], loss: 0.002217, mae: 0.047555, mean_q: -0.330737
 31882/100000: episode: 771, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -15.262, mean reward: -0.153 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.508, 10.098], loss: 0.002314, mae: 0.048450, mean_q: -0.336690
 31982/100000: episode: 772, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.436, mean reward: -0.174 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.782, 10.109], loss: 0.002603, mae: 0.052698, mean_q: -0.324845
 32082/100000: episode: 773, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.561, mean reward: -0.166 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.321, 10.265], loss: 0.002694, mae: 0.053084, mean_q: -0.311593
 32182/100000: episode: 774, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.178, mean reward: -0.172 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.328, 10.197], loss: 0.002508, mae: 0.051512, mean_q: -0.327610
 32282/100000: episode: 775, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.974, mean reward: -0.180 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.500, 10.098], loss: 0.002404, mae: 0.049107, mean_q: -0.332019
 32382/100000: episode: 776, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -11.247, mean reward: -0.112 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.983, 10.352], loss: 0.002451, mae: 0.049941, mean_q: -0.308261
 32482/100000: episode: 777, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.396, mean reward: -0.164 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.333, 10.098], loss: 0.002262, mae: 0.047717, mean_q: -0.347282
 32582/100000: episode: 778, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.856, mean reward: -0.169 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.135, 10.146], loss: 0.002511, mae: 0.051761, mean_q: -0.311390
 32682/100000: episode: 779, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.078, mean reward: -0.191 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.981, 10.163], loss: 0.002514, mae: 0.050733, mean_q: -0.323097
 32782/100000: episode: 780, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -17.667, mean reward: -0.177 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.943, 10.223], loss: 0.002594, mae: 0.050902, mean_q: -0.304876
[Info] 100-TH LEVEL FOUND: 0.5207328200340271, Considering 10/90 traces
 32882/100000: episode: 781, duration: 4.574s, episode steps: 100, steps per second: 22, episode reward: -13.401, mean reward: -0.134 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.222, 10.166], loss: 0.002507, mae: 0.051263, mean_q: -0.303525
 32914/100000: episode: 782, duration: 0.167s, episode steps: 32, steps per second: 192, episode reward: 8.730, mean reward: 0.273 [0.030, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-1.079, 10.100], loss: 0.002310, mae: 0.049274, mean_q: -0.280402
 32931/100000: episode: 783, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 6.480, mean reward: 0.381 [0.321, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.248, 10.100], loss: 0.002375, mae: 0.048265, mean_q: -0.326507
 32951/100000: episode: 784, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 5.079, mean reward: 0.254 [0.206, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.261, 10.100], loss: 0.002677, mae: 0.052455, mean_q: -0.245202
 32983/100000: episode: 785, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 7.652, mean reward: 0.239 [0.061, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.048, 10.100], loss: 0.002611, mae: 0.053310, mean_q: -0.283292
 33003/100000: episode: 786, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 5.978, mean reward: 0.299 [0.207, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.639, 10.100], loss: 0.002853, mae: 0.055323, mean_q: -0.301544
 33017/100000: episode: 787, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 3.949, mean reward: 0.282 [0.216, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-1.127, 10.100], loss: 0.002799, mae: 0.053543, mean_q: -0.241401
 33034/100000: episode: 788, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 5.471, mean reward: 0.322 [0.277, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.912, 10.100], loss: 0.002917, mae: 0.056294, mean_q: -0.296764
 33064/100000: episode: 789, duration: 0.154s, episode steps: 30, steps per second: 194, episode reward: 7.329, mean reward: 0.244 [0.063, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.132, 10.100], loss: 0.004416, mae: 0.064689, mean_q: -0.300589
 33078/100000: episode: 790, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 2.486, mean reward: 0.178 [0.050, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.054, 10.100], loss: 0.004469, mae: 0.068327, mean_q: -0.289159
 33092/100000: episode: 791, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 3.329, mean reward: 0.238 [0.116, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.727, 10.100], loss: 0.003057, mae: 0.060021, mean_q: -0.282915
 33106/100000: episode: 792, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 3.843, mean reward: 0.274 [0.146, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.139, 10.100], loss: 0.003551, mae: 0.061505, mean_q: -0.263443
 33138/100000: episode: 793, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 11.453, mean reward: 0.358 [0.265, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.200, 10.100], loss: 0.002344, mae: 0.049708, mean_q: -0.295092
 33155/100000: episode: 794, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 6.098, mean reward: 0.359 [0.297, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.949, 10.100], loss: 0.002410, mae: 0.051605, mean_q: -0.261083
 33187/100000: episode: 795, duration: 0.200s, episode steps: 32, steps per second: 160, episode reward: 11.344, mean reward: 0.354 [0.166, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.514, 10.100], loss: 0.002729, mae: 0.052475, mean_q: -0.247884
 33200/100000: episode: 796, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 3.195, mean reward: 0.246 [0.166, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-1.280, 10.100], loss: 0.002423, mae: 0.051467, mean_q: -0.188050
 33230/100000: episode: 797, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 9.918, mean reward: 0.331 [0.252, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.172, 10.100], loss: 0.002713, mae: 0.054223, mean_q: -0.227278
 33253/100000: episode: 798, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 4.950, mean reward: 0.215 [0.110, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.067, 10.100], loss: 0.002762, mae: 0.055311, mean_q: -0.198727
 33276/100000: episode: 799, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 3.903, mean reward: 0.170 [0.076, 0.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.280, 10.100], loss: 0.002569, mae: 0.054208, mean_q: -0.184427
 33296/100000: episode: 800, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 6.526, mean reward: 0.326 [0.174, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.100], loss: 0.002598, mae: 0.054887, mean_q: -0.241983
 33326/100000: episode: 801, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 6.810, mean reward: 0.227 [0.072, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.074, 10.100], loss: 0.002546, mae: 0.051933, mean_q: -0.231086
 33358/100000: episode: 802, duration: 0.167s, episode steps: 32, steps per second: 191, episode reward: 9.458, mean reward: 0.296 [0.177, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.155, 10.100], loss: 0.002644, mae: 0.052605, mean_q: -0.188765
 33372/100000: episode: 803, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 4.076, mean reward: 0.291 [0.202, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.417, 10.100], loss: 0.002625, mae: 0.052930, mean_q: -0.228090
 33385/100000: episode: 804, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 4.034, mean reward: 0.310 [0.249, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.176, 10.100], loss: 0.002234, mae: 0.047041, mean_q: -0.300643
 33409/100000: episode: 805, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 6.923, mean reward: 0.288 [0.182, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.218, 10.100], loss: 0.002739, mae: 0.052273, mean_q: -0.199256
 33432/100000: episode: 806, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 6.538, mean reward: 0.284 [0.175, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.340, 10.100], loss: 0.002553, mae: 0.051888, mean_q: -0.182325
 33452/100000: episode: 807, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 6.354, mean reward: 0.318 [0.206, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.204, 10.100], loss: 0.002936, mae: 0.053216, mean_q: -0.231585
 33475/100000: episode: 808, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 5.639, mean reward: 0.245 [0.128, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.254, 10.100], loss: 0.002974, mae: 0.054869, mean_q: -0.180927
 33505/100000: episode: 809, duration: 0.142s, episode steps: 30, steps per second: 211, episode reward: 7.712, mean reward: 0.257 [0.155, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.541, 10.100], loss: 0.004956, mae: 0.067092, mean_q: -0.230499
 33518/100000: episode: 810, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 3.954, mean reward: 0.304 [0.255, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.123, 10.100], loss: 0.004444, mae: 0.072029, mean_q: -0.226799
 33550/100000: episode: 811, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 5.512, mean reward: 0.172 [0.020, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.378, 10.146], loss: 0.005802, mae: 0.067525, mean_q: -0.186598
 33582/100000: episode: 812, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 11.582, mean reward: 0.362 [0.153, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.128, 10.100], loss: 0.003948, mae: 0.062318, mean_q: -0.237117
 33606/100000: episode: 813, duration: 0.125s, episode steps: 24, steps per second: 191, episode reward: 6.960, mean reward: 0.290 [0.226, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.756, 10.100], loss: 0.007361, mae: 0.077021, mean_q: -0.177337
 33619/100000: episode: 814, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 3.273, mean reward: 0.252 [0.189, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.154, 10.100], loss: 0.010062, mae: 0.083687, mean_q: -0.179515
 33639/100000: episode: 815, duration: 0.102s, episode steps: 20, steps per second: 195, episode reward: 5.212, mean reward: 0.261 [0.190, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.140, 10.100], loss: 0.005232, mae: 0.070971, mean_q: -0.246373
 33656/100000: episode: 816, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 6.854, mean reward: 0.403 [0.281, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.822, 10.100], loss: 0.004260, mae: 0.064126, mean_q: -0.182257
 33680/100000: episode: 817, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 5.883, mean reward: 0.245 [0.118, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.231, 10.100], loss: 0.002574, mae: 0.053183, mean_q: -0.128553
 33703/100000: episode: 818, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 4.162, mean reward: 0.181 [0.033, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.630, 10.100], loss: 0.003425, mae: 0.059935, mean_q: -0.171789
 33735/100000: episode: 819, duration: 0.168s, episode steps: 32, steps per second: 191, episode reward: 8.888, mean reward: 0.278 [0.062, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.099, 10.230], loss: 0.002669, mae: 0.054666, mean_q: -0.206507
 33766/100000: episode: 820, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 12.300, mean reward: 0.397 [0.247, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.448, 10.100], loss: 0.002480, mae: 0.051520, mean_q: -0.116185
 33786/100000: episode: 821, duration: 0.115s, episode steps: 20, steps per second: 175, episode reward: 4.879, mean reward: 0.244 [0.156, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.266, 10.100], loss: 0.002403, mae: 0.050358, mean_q: -0.222970
 33816/100000: episode: 822, duration: 0.158s, episode steps: 30, steps per second: 189, episode reward: 8.658, mean reward: 0.289 [0.202, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.284, 10.100], loss: 0.002684, mae: 0.053729, mean_q: -0.110878
 33848/100000: episode: 823, duration: 0.169s, episode steps: 32, steps per second: 190, episode reward: 5.648, mean reward: 0.177 [0.030, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.853, 10.210], loss: 0.002685, mae: 0.053627, mean_q: -0.124202
 33862/100000: episode: 824, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 3.280, mean reward: 0.234 [0.134, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.598, 10.100], loss: 0.002700, mae: 0.053346, mean_q: -0.096017
 33875/100000: episode: 825, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 4.300, mean reward: 0.331 [0.281, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.343, 10.100], loss: 0.002427, mae: 0.051526, mean_q: -0.171524
 33907/100000: episode: 826, duration: 0.182s, episode steps: 32, steps per second: 176, episode reward: 9.463, mean reward: 0.296 [0.062, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.784, 10.108], loss: 0.002704, mae: 0.053610, mean_q: -0.138617
 33920/100000: episode: 827, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 2.778, mean reward: 0.214 [0.161, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.275, 10.100], loss: 0.002238, mae: 0.048624, mean_q: -0.238182
 33933/100000: episode: 828, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 4.558, mean reward: 0.351 [0.239, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.942, 10.100], loss: 0.002608, mae: 0.053324, mean_q: -0.145637
 33947/100000: episode: 829, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 3.960, mean reward: 0.283 [0.129, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.194, 10.100], loss: 0.002764, mae: 0.052656, mean_q: -0.190518
 33979/100000: episode: 830, duration: 0.187s, episode steps: 32, steps per second: 171, episode reward: 11.804, mean reward: 0.369 [0.254, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.744, 10.100], loss: 0.002532, mae: 0.051642, mean_q: -0.142114
 33996/100000: episode: 831, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 5.200, mean reward: 0.306 [0.186, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.467, 10.100], loss: 0.002419, mae: 0.050197, mean_q: -0.112008
 34010/100000: episode: 832, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 3.905, mean reward: 0.279 [0.222, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.200, 10.100], loss: 0.002204, mae: 0.048373, mean_q: -0.163683
 34040/100000: episode: 833, duration: 0.151s, episode steps: 30, steps per second: 198, episode reward: 7.316, mean reward: 0.244 [0.172, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.203, 10.100], loss: 0.002484, mae: 0.051836, mean_q: -0.099955
 34063/100000: episode: 834, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 5.058, mean reward: 0.220 [0.135, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.465, 10.100], loss: 0.002964, mae: 0.056807, mean_q: -0.057058
 34080/100000: episode: 835, duration: 0.112s, episode steps: 17, steps per second: 152, episode reward: 6.027, mean reward: 0.355 [0.230, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.368, 10.100], loss: 0.002244, mae: 0.050184, mean_q: -0.081489
 34097/100000: episode: 836, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 6.551, mean reward: 0.385 [0.323, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.803, 10.100], loss: 0.002655, mae: 0.052977, mean_q: -0.076046
 34114/100000: episode: 837, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 5.653, mean reward: 0.333 [0.278, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.580, 10.100], loss: 0.002390, mae: 0.052709, mean_q: -0.051582
 34134/100000: episode: 838, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 5.469, mean reward: 0.273 [0.160, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.171, 10.100], loss: 0.002282, mae: 0.047897, mean_q: -0.059930
 34154/100000: episode: 839, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 5.782, mean reward: 0.289 [0.194, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.782, 10.100], loss: 0.002663, mae: 0.054176, mean_q: -0.037979
 34185/100000: episode: 840, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 7.137, mean reward: 0.230 [0.116, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.636, 10.100], loss: 0.002493, mae: 0.051960, mean_q: -0.086874
 34215/100000: episode: 841, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 6.775, mean reward: 0.226 [0.140, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.200, 10.100], loss: 0.002434, mae: 0.049904, mean_q: -0.095500
 34239/100000: episode: 842, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 7.321, mean reward: 0.305 [0.182, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.200, 10.100], loss: 0.002400, mae: 0.050217, mean_q: -0.069799
 34263/100000: episode: 843, duration: 0.143s, episode steps: 24, steps per second: 167, episode reward: 6.537, mean reward: 0.272 [0.177, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.333, 10.100], loss: 0.002623, mae: 0.052950, mean_q: -0.101962
 34276/100000: episode: 844, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 2.906, mean reward: 0.224 [0.089, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.066, 10.100], loss: 0.002531, mae: 0.051611, mean_q: 0.005031
 34308/100000: episode: 845, duration: 0.180s, episode steps: 32, steps per second: 177, episode reward: 9.602, mean reward: 0.300 [0.029, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.696, 10.145], loss: 0.002833, mae: 0.056038, mean_q: -0.048319
 34340/100000: episode: 846, duration: 0.180s, episode steps: 32, steps per second: 178, episode reward: 6.724, mean reward: 0.210 [0.049, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.483, 10.100], loss: 0.002375, mae: 0.051534, mean_q: -0.039055
 34363/100000: episode: 847, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 7.171, mean reward: 0.312 [0.144, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.472, 10.100], loss: 0.002569, mae: 0.052135, mean_q: -0.050905
 34394/100000: episode: 848, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 12.725, mean reward: 0.410 [0.238, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-2.149, 10.100], loss: 0.002352, mae: 0.050132, mean_q: -0.086445
 34411/100000: episode: 849, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 3.312, mean reward: 0.195 [0.071, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.144, 10.100], loss: 0.002487, mae: 0.051721, mean_q: -0.001602
 34425/100000: episode: 850, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 3.593, mean reward: 0.257 [0.137, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.460, 10.100], loss: 0.002595, mae: 0.053448, mean_q: -0.024745
 34456/100000: episode: 851, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 12.785, mean reward: 0.412 [0.238, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.370, 10.100], loss: 0.002615, mae: 0.053351, mean_q: -0.020436
 34487/100000: episode: 852, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 9.770, mean reward: 0.315 [0.144, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.227, 10.100], loss: 0.002382, mae: 0.051759, mean_q: -0.047012
 34518/100000: episode: 853, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 11.377, mean reward: 0.367 [0.259, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.627, 10.100], loss: 0.003542, mae: 0.062519, mean_q: -0.028695
 34541/100000: episode: 854, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 6.567, mean reward: 0.286 [0.124, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.030, 10.100], loss: 0.002672, mae: 0.055482, mean_q: -0.068609
 34572/100000: episode: 855, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 7.652, mean reward: 0.247 [0.112, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.448, 10.100], loss: 0.002653, mae: 0.055351, mean_q: -0.027012
 34592/100000: episode: 856, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 3.417, mean reward: 0.171 [0.083, 0.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.047, 10.100], loss: 0.002901, mae: 0.055859, mean_q: -0.064927
 34623/100000: episode: 857, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 8.427, mean reward: 0.272 [0.143, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.152, 10.100], loss: 0.002418, mae: 0.051452, mean_q: -0.005201
 34636/100000: episode: 858, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 2.787, mean reward: 0.214 [0.161, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.467, 10.100], loss: 0.002928, mae: 0.057866, mean_q: 0.007939
 34668/100000: episode: 859, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 4.667, mean reward: 0.146 [0.028, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.186, 10.100], loss: 0.002500, mae: 0.052013, mean_q: -0.053936
 34698/100000: episode: 860, duration: 0.170s, episode steps: 30, steps per second: 177, episode reward: 8.127, mean reward: 0.271 [0.104, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.087, 10.100], loss: 0.002909, mae: 0.056398, mean_q: 0.075236
 34722/100000: episode: 861, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 7.751, mean reward: 0.323 [0.159, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.754, 10.100], loss: 0.003087, mae: 0.057420, mean_q: 0.029498
 34742/100000: episode: 862, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 6.289, mean reward: 0.314 [0.267, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.396, 10.100], loss: 0.003051, mae: 0.056964, mean_q: -0.056688
 34766/100000: episode: 863, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 8.894, mean reward: 0.371 [0.198, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.376, 10.100], loss: 0.002723, mae: 0.054814, mean_q: 0.005311
 34790/100000: episode: 864, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 8.269, mean reward: 0.345 [0.230, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.313, 10.100], loss: 0.002538, mae: 0.053123, mean_q: 0.005957
 34803/100000: episode: 865, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 3.789, mean reward: 0.291 [0.215, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.338, 10.100], loss: 0.002886, mae: 0.055407, mean_q: -0.037059
 34826/100000: episode: 866, duration: 0.118s, episode steps: 23, steps per second: 196, episode reward: 5.340, mean reward: 0.232 [0.102, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.456, 10.100], loss: 0.002586, mae: 0.052460, mean_q: 0.028117
 34858/100000: episode: 867, duration: 0.162s, episode steps: 32, steps per second: 198, episode reward: 9.751, mean reward: 0.305 [0.205, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.599, 10.100], loss: 0.003793, mae: 0.066613, mean_q: 0.051121
 34890/100000: episode: 868, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 10.553, mean reward: 0.330 [0.181, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.094, 10.100], loss: 0.002848, mae: 0.055604, mean_q: 0.056602
 34910/100000: episode: 869, duration: 0.132s, episode steps: 20, steps per second: 151, episode reward: 4.297, mean reward: 0.215 [0.117, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.755, 10.100], loss: 0.003269, mae: 0.060136, mean_q: 0.049787
 34923/100000: episode: 870, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 3.441, mean reward: 0.265 [0.069, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.133, 10.100], loss: 0.003697, mae: 0.063048, mean_q: 0.129903
[Info] 200-TH LEVEL FOUND: 0.7830104231834412, Considering 10/90 traces
 34947/100000: episode: 871, duration: 4.010s, episode steps: 24, steps per second: 6, episode reward: 9.860, mean reward: 0.411 [0.255, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.175, 10.100], loss: 0.003139, mae: 0.060971, mean_q: 0.086760
 34966/100000: episode: 872, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 7.345, mean reward: 0.387 [0.310, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.280, 10.100], loss: 0.002829, mae: 0.056090, mean_q: 0.069425
 34980/100000: episode: 873, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 5.118, mean reward: 0.366 [0.309, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.245, 10.100], loss: 0.002346, mae: 0.052109, mean_q: 0.082767
 34986/100000: episode: 874, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 2.288, mean reward: 0.381 [0.317, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.354, 10.100], loss: 0.002443, mae: 0.050691, mean_q: -0.006001
 34992/100000: episode: 875, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 2.161, mean reward: 0.360 [0.291, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.460, 10.100], loss: 0.002994, mae: 0.058000, mean_q: 0.057457
 35005/100000: episode: 876, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 4.639, mean reward: 0.357 [0.277, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.520, 10.100], loss: 0.002421, mae: 0.049254, mean_q: 0.014065
 35017/100000: episode: 877, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 4.458, mean reward: 0.372 [0.289, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.378, 10.100], loss: 0.003061, mae: 0.057449, mean_q: 0.054217
 35030/100000: episode: 878, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 5.731, mean reward: 0.441 [0.378, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.525, 10.100], loss: 0.002826, mae: 0.057137, mean_q: 0.115150
 35049/100000: episode: 879, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 8.554, mean reward: 0.450 [0.362, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.535, 10.100], loss: 0.002779, mae: 0.056416, mean_q: 0.090520
 35061/100000: episode: 880, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 5.163, mean reward: 0.430 [0.360, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.540, 10.100], loss: 0.002539, mae: 0.053844, mean_q: 0.053609
 35082/100000: episode: 881, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 9.849, mean reward: 0.469 [0.391, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.328, 10.100], loss: 0.002687, mae: 0.054881, mean_q: 0.060512
 35101/100000: episode: 882, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 6.052, mean reward: 0.319 [0.237, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.333, 10.100], loss: 0.002528, mae: 0.052745, mean_q: 0.054068
 35107/100000: episode: 883, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 2.681, mean reward: 0.447 [0.408, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-1.098, 10.100], loss: 0.002420, mae: 0.051123, mean_q: 0.182548
 35121/100000: episode: 884, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 5.664, mean reward: 0.405 [0.323, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.368, 10.100], loss: 0.002814, mae: 0.054803, mean_q: -0.024690
 35134/100000: episode: 885, duration: 0.085s, episode steps: 13, steps per second: 153, episode reward: 4.533, mean reward: 0.349 [0.236, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.846, 10.100], loss: 0.002963, mae: 0.057899, mean_q: 0.080588
 35155/100000: episode: 886, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 10.609, mean reward: 0.505 [0.413, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.685, 10.100], loss: 0.002917, mae: 0.057611, mean_q: 0.166079
 35168/100000: episode: 887, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 5.949, mean reward: 0.458 [0.393, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.397, 10.100], loss: 0.002838, mae: 0.056609, mean_q: 0.076046
 35189/100000: episode: 888, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 9.531, mean reward: 0.454 [0.355, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.280, 10.100], loss: 0.002792, mae: 0.055901, mean_q: 0.108797
 35201/100000: episode: 889, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 5.383, mean reward: 0.449 [0.414, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.385, 10.100], loss: 0.002993, mae: 0.058496, mean_q: 0.135937
 35218/100000: episode: 890, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 7.918, mean reward: 0.466 [0.390, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.761, 10.100], loss: 0.002661, mae: 0.053828, mean_q: 0.098964
 35240/100000: episode: 891, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 8.609, mean reward: 0.391 [0.290, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.395, 10.100], loss: 0.002767, mae: 0.058137, mean_q: 0.112833
 35259/100000: episode: 892, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 7.089, mean reward: 0.373 [0.280, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.709, 10.100], loss: 0.002631, mae: 0.056050, mean_q: 0.142006
 35265/100000: episode: 893, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 2.683, mean reward: 0.447 [0.407, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.532, 10.100], loss: 0.003379, mae: 0.064240, mean_q: 0.122109
 35282/100000: episode: 894, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 5.861, mean reward: 0.345 [0.162, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.482, 10.100], loss: 0.003696, mae: 0.063741, mean_q: 0.169609
 35301/100000: episode: 895, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 7.394, mean reward: 0.389 [0.339, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.345, 10.100], loss: 0.005287, mae: 0.071951, mean_q: 0.136786
 35320/100000: episode: 896, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 9.028, mean reward: 0.475 [0.344, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.168, 10.100], loss: 0.003676, mae: 0.066515, mean_q: 0.148425
 35337/100000: episode: 897, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 8.209, mean reward: 0.483 [0.428, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.476, 10.100], loss: 0.002740, mae: 0.056936, mean_q: 0.082049
 35350/100000: episode: 898, duration: 0.088s, episode steps: 13, steps per second: 147, episode reward: 5.629, mean reward: 0.433 [0.354, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.369, 10.100], loss: 0.002922, mae: 0.056914, mean_q: 0.171757
 35367/100000: episode: 899, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 7.618, mean reward: 0.448 [0.286, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.419, 10.100], loss: 0.003106, mae: 0.059046, mean_q: 0.173822
 35373/100000: episode: 900, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 2.707, mean reward: 0.451 [0.414, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.366, 10.100], loss: 0.002121, mae: 0.051432, mean_q: 0.092897
 35390/100000: episode: 901, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 6.733, mean reward: 0.396 [0.316, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.390, 10.100], loss: 0.004045, mae: 0.062514, mean_q: 0.101726
 35403/100000: episode: 902, duration: 0.062s, episode steps: 13, steps per second: 209, episode reward: 5.990, mean reward: 0.461 [0.355, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.514, 10.100], loss: 0.005101, mae: 0.062549, mean_q: 0.064794
 35409/100000: episode: 903, duration: 0.041s, episode steps: 6, steps per second: 146, episode reward: 2.834, mean reward: 0.472 [0.406, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.532, 10.100], loss: 0.004879, mae: 0.067151, mean_q: 0.195458
 35431/100000: episode: 904, duration: 0.134s, episode steps: 22, steps per second: 165, episode reward: 9.966, mean reward: 0.453 [0.306, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.187, 10.100], loss: 0.007291, mae: 0.070682, mean_q: 0.159009
 35450/100000: episode: 905, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 7.566, mean reward: 0.398 [0.347, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.268, 10.100], loss: 0.006387, mae: 0.068530, mean_q: 0.085375
 35463/100000: episode: 906, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 6.007, mean reward: 0.462 [0.413, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.354, 10.100], loss: 0.003183, mae: 0.060124, mean_q: 0.268850
 35484/100000: episode: 907, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 8.398, mean reward: 0.400 [0.224, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.100, 10.100], loss: 0.003302, mae: 0.062579, mean_q: 0.183008
 35501/100000: episode: 908, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 7.418, mean reward: 0.436 [0.349, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.253, 10.100], loss: 0.002477, mae: 0.055434, mean_q: 0.163579
 35520/100000: episode: 909, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 5.858, mean reward: 0.308 [0.204, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.716, 10.100], loss: 0.002490, mae: 0.054370, mean_q: 0.165242
 35542/100000: episode: 910, duration: 0.123s, episode steps: 22, steps per second: 180, episode reward: 7.710, mean reward: 0.350 [0.229, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.582, 10.100], loss: 0.002941, mae: 0.056069, mean_q: 0.192511
 35554/100000: episode: 911, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 3.775, mean reward: 0.315 [0.268, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.244, 10.100], loss: 0.002981, mae: 0.059550, mean_q: 0.232729
 35567/100000: episode: 912, duration: 0.092s, episode steps: 13, steps per second: 141, episode reward: 6.174, mean reward: 0.475 [0.379, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.477, 10.100], loss: 0.002373, mae: 0.052310, mean_q: 0.178927
 35580/100000: episode: 913, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 4.901, mean reward: 0.377 [0.249, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.353, 10.100], loss: 0.002461, mae: 0.053428, mean_q: 0.206726
 35593/100000: episode: 914, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 4.935, mean reward: 0.380 [0.325, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.282, 10.100], loss: 0.002851, mae: 0.056099, mean_q: 0.279588
 35610/100000: episode: 915, duration: 0.106s, episode steps: 17, steps per second: 160, episode reward: 7.362, mean reward: 0.433 [0.268, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.287, 10.100], loss: 0.002770, mae: 0.056076, mean_q: 0.208553
 35627/100000: episode: 916, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 7.151, mean reward: 0.421 [0.357, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-2.182, 10.100], loss: 0.002758, mae: 0.056980, mean_q: 0.202216
 35640/100000: episode: 917, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 5.516, mean reward: 0.424 [0.379, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.678, 10.100], loss: 0.002727, mae: 0.057779, mean_q: 0.220458
 35653/100000: episode: 918, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 7.178, mean reward: 0.552 [0.459, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.516, 10.100], loss: 0.002664, mae: 0.053343, mean_q: 0.257990
 35666/100000: episode: 919, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 5.671, mean reward: 0.436 [0.304, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.300, 10.100], loss: 0.002850, mae: 0.057390, mean_q: 0.219057
 35678/100000: episode: 920, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 4.593, mean reward: 0.383 [0.325, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.859, 10.100], loss: 0.002738, mae: 0.055583, mean_q: 0.217521
 35695/100000: episode: 921, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 6.909, mean reward: 0.406 [0.320, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.086, 10.100], loss: 0.002597, mae: 0.052068, mean_q: 0.179961
 35714/100000: episode: 922, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 7.186, mean reward: 0.378 [0.243, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.371, 10.100], loss: 0.002683, mae: 0.055079, mean_q: 0.231518
 35720/100000: episode: 923, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 2.436, mean reward: 0.406 [0.367, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.333, 10.100], loss: 0.002603, mae: 0.054080, mean_q: 0.193168
 35732/100000: episode: 924, duration: 0.077s, episode steps: 12, steps per second: 155, episode reward: 4.725, mean reward: 0.394 [0.332, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.290, 10.100], loss: 0.002519, mae: 0.055244, mean_q: 0.270750
 35738/100000: episode: 925, duration: 0.043s, episode steps: 6, steps per second: 141, episode reward: 2.515, mean reward: 0.419 [0.398, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.438, 10.100], loss: 0.002622, mae: 0.054805, mean_q: 0.219499
 35757/100000: episode: 926, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 7.605, mean reward: 0.400 [0.308, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.743, 10.100], loss: 0.002820, mae: 0.056318, mean_q: 0.222495
 35771/100000: episode: 927, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 5.808, mean reward: 0.415 [0.345, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.510, 10.100], loss: 0.002798, mae: 0.056626, mean_q: 0.224121
 35790/100000: episode: 928, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 7.316, mean reward: 0.385 [0.273, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.342, 10.100], loss: 0.002933, mae: 0.057872, mean_q: 0.282429
 35811/100000: episode: 929, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 7.935, mean reward: 0.378 [0.244, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.200, 10.100], loss: 0.002465, mae: 0.052785, mean_q: 0.224097
 35825/100000: episode: 930, duration: 0.089s, episode steps: 14, steps per second: 158, episode reward: 4.743, mean reward: 0.339 [0.282, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.257, 10.100], loss: 0.002189, mae: 0.050753, mean_q: 0.273534
 35839/100000: episode: 931, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 5.841, mean reward: 0.417 [0.296, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.487, 10.100], loss: 0.002631, mae: 0.055407, mean_q: 0.280978
 35851/100000: episode: 932, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 5.682, mean reward: 0.473 [0.322, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.570, 10.100], loss: 0.002509, mae: 0.052026, mean_q: 0.160671
 35857/100000: episode: 933, duration: 0.038s, episode steps: 6, steps per second: 160, episode reward: 2.729, mean reward: 0.455 [0.379, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.430, 10.100], loss: 0.003326, mae: 0.061499, mean_q: 0.268092
 35870/100000: episode: 934, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 5.471, mean reward: 0.421 [0.353, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.495, 10.100], loss: 0.002896, mae: 0.056720, mean_q: 0.228570
 35884/100000: episode: 935, duration: 0.084s, episode steps: 14, steps per second: 168, episode reward: 6.881, mean reward: 0.491 [0.425, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.301, 10.100], loss: 0.003016, mae: 0.058471, mean_q: 0.291158
 35901/100000: episode: 936, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 7.774, mean reward: 0.457 [0.339, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.918, 10.100], loss: 0.003252, mae: 0.063258, mean_q: 0.294047
 35915/100000: episode: 937, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 5.971, mean reward: 0.426 [0.389, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.465, 10.100], loss: 0.002833, mae: 0.058302, mean_q: 0.293279
 35934/100000: episode: 938, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 6.532, mean reward: 0.344 [0.249, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.569, 10.100], loss: 0.002459, mae: 0.053647, mean_q: 0.236646
 35948/100000: episode: 939, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 6.016, mean reward: 0.430 [0.320, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.316, 10.100], loss: 0.002592, mae: 0.055254, mean_q: 0.255952
 35961/100000: episode: 940, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 5.070, mean reward: 0.390 [0.295, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.301, 10.100], loss: 0.002621, mae: 0.055688, mean_q: 0.362408
 35973/100000: episode: 941, duration: 0.065s, episode steps: 12, steps per second: 183, episode reward: 5.742, mean reward: 0.478 [0.348, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.448, 10.100], loss: 0.003406, mae: 0.061502, mean_q: 0.279252
 35985/100000: episode: 942, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 4.485, mean reward: 0.374 [0.340, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.354, 10.100], loss: 0.002882, mae: 0.057708, mean_q: 0.348344
 35998/100000: episode: 943, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 5.570, mean reward: 0.428 [0.360, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.229, 10.100], loss: 0.002858, mae: 0.056985, mean_q: 0.221109
 36010/100000: episode: 944, duration: 0.073s, episode steps: 12, steps per second: 163, episode reward: 5.520, mean reward: 0.460 [0.380, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.549, 10.100], loss: 0.002896, mae: 0.059058, mean_q: 0.285639
 36016/100000: episode: 945, duration: 0.031s, episode steps: 6, steps per second: 194, episode reward: 2.755, mean reward: 0.459 [0.384, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.372, 10.100], loss: 0.002371, mae: 0.054388, mean_q: 0.286053
 36028/100000: episode: 946, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 4.756, mean reward: 0.396 [0.327, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.396, 10.100], loss: 0.002585, mae: 0.056012, mean_q: 0.306337
 36045/100000: episode: 947, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 7.108, mean reward: 0.418 [0.331, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.260, 10.100], loss: 0.002749, mae: 0.057691, mean_q: 0.332665
 36051/100000: episode: 948, duration: 0.043s, episode steps: 6, steps per second: 141, episode reward: 2.686, mean reward: 0.448 [0.411, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.459, 10.100], loss: 0.002656, mae: 0.056458, mean_q: 0.260661
 36073/100000: episode: 949, duration: 0.114s, episode steps: 22, steps per second: 192, episode reward: 9.418, mean reward: 0.428 [0.306, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.311, 10.100], loss: 0.003313, mae: 0.062085, mean_q: 0.283397
 36085/100000: episode: 950, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 3.834, mean reward: 0.319 [0.242, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.754, 10.100], loss: 0.003382, mae: 0.062821, mean_q: 0.332888
 36091/100000: episode: 951, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 2.280, mean reward: 0.380 [0.339, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.431, 10.100], loss: 0.004952, mae: 0.076841, mean_q: 0.288966
 36112/100000: episode: 952, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 10.681, mean reward: 0.509 [0.433, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.322, 10.100], loss: 0.003055, mae: 0.060030, mean_q: 0.309834
 36124/100000: episode: 953, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 5.063, mean reward: 0.422 [0.316, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.358, 10.100], loss: 0.002622, mae: 0.056328, mean_q: 0.323628
 36146/100000: episode: 954, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 8.297, mean reward: 0.377 [0.273, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.480, 10.100], loss: 0.002720, mae: 0.056813, mean_q: 0.306978
 36165/100000: episode: 955, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 8.107, mean reward: 0.427 [0.315, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.482, 10.100], loss: 0.002688, mae: 0.055789, mean_q: 0.342329
 36182/100000: episode: 956, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 6.120, mean reward: 0.360 [0.313, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-2.282, 10.100], loss: 0.002631, mae: 0.056463, mean_q: 0.344373
 36203/100000: episode: 957, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 7.503, mean reward: 0.357 [0.173, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.395, 10.100], loss: 0.002523, mae: 0.053540, mean_q: 0.279868
 36220/100000: episode: 958, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 8.219, mean reward: 0.483 [0.417, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.261, 10.100], loss: 0.002507, mae: 0.054692, mean_q: 0.300366
 36233/100000: episode: 959, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 5.264, mean reward: 0.405 [0.358, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.362, 10.100], loss: 0.002467, mae: 0.054075, mean_q: 0.398224
 36247/100000: episode: 960, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 5.465, mean reward: 0.390 [0.255, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.264, 10.100], loss: 0.002889, mae: 0.057847, mean_q: 0.373301
[Info] 300-TH LEVEL FOUND: 0.9634721875190735, Considering 10/90 traces
 36269/100000: episode: 961, duration: 3.975s, episode steps: 22, steps per second: 6, episode reward: 6.681, mean reward: 0.304 [0.226, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.698, 10.100], loss: 0.002389, mae: 0.053083, mean_q: 0.342587
 36278/100000: episode: 962, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 4.468, mean reward: 0.496 [0.456, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.379, 10.100], loss: 0.002854, mae: 0.057737, mean_q: 0.311953
 36289/100000: episode: 963, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 5.528, mean reward: 0.503 [0.428, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-1.640, 10.100], loss: 0.002841, mae: 0.057301, mean_q: 0.361265
 36301/100000: episode: 964, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 7.027, mean reward: 0.586 [0.534, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.343, 10.100], loss: 0.002345, mae: 0.052242, mean_q: 0.330874
 36309/100000: episode: 965, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 4.268, mean reward: 0.533 [0.436, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.389, 10.100], loss: 0.002978, mae: 0.059861, mean_q: 0.352840
 36318/100000: episode: 966, duration: 0.059s, episode steps: 9, steps per second: 151, episode reward: 4.482, mean reward: 0.498 [0.428, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.316, 10.100], loss: 0.002427, mae: 0.055332, mean_q: 0.402985
 36329/100000: episode: 967, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 6.109, mean reward: 0.555 [0.491, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.597, 10.100], loss: 0.002792, mae: 0.057149, mean_q: 0.344670
 36338/100000: episode: 968, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 4.339, mean reward: 0.482 [0.409, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.627, 10.100], loss: 0.003065, mae: 0.062142, mean_q: 0.447109
 36347/100000: episode: 969, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 4.534, mean reward: 0.504 [0.459, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.610, 10.100], loss: 0.002436, mae: 0.053923, mean_q: 0.352905
 36356/100000: episode: 970, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 4.489, mean reward: 0.499 [0.479, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.396, 10.100], loss: 0.002721, mae: 0.055265, mean_q: 0.322595
 36367/100000: episode: 971, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 4.364, mean reward: 0.397 [0.295, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.483, 10.100], loss: 0.002396, mae: 0.052386, mean_q: 0.359853
 36378/100000: episode: 972, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 4.797, mean reward: 0.436 [0.340, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.430, 10.100], loss: 0.002773, mae: 0.059619, mean_q: 0.386750
 36386/100000: episode: 973, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 3.733, mean reward: 0.467 [0.446, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.494, 10.100], loss: 0.002867, mae: 0.059577, mean_q: 0.396709
 36395/100000: episode: 974, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 4.872, mean reward: 0.541 [0.451, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.480, 10.100], loss: 0.003160, mae: 0.061245, mean_q: 0.395974
 36406/100000: episode: 975, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 5.235, mean reward: 0.476 [0.407, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.494, 10.100], loss: 0.002590, mae: 0.055130, mean_q: 0.392684
 36415/100000: episode: 976, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 4.173, mean reward: 0.464 [0.409, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.433, 10.100], loss: 0.002719, mae: 0.056545, mean_q: 0.328625
 36426/100000: episode: 977, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 4.445, mean reward: 0.404 [0.356, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.568, 10.100], loss: 0.002291, mae: 0.051999, mean_q: 0.346336
 36437/100000: episode: 978, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 5.676, mean reward: 0.516 [0.454, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.060, 10.100], loss: 0.002933, mae: 0.059346, mean_q: 0.365069
 36448/100000: episode: 979, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 3.916, mean reward: 0.356 [0.242, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.414, 10.100], loss: 0.002560, mae: 0.055247, mean_q: 0.416636
 36454/100000: episode: 980, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 3.125, mean reward: 0.521 [0.481, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.618, 10.100], loss: 0.002544, mae: 0.055737, mean_q: 0.347529
 36463/100000: episode: 981, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 4.794, mean reward: 0.533 [0.483, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.711, 10.100], loss: 0.002554, mae: 0.055156, mean_q: 0.433722
 36474/100000: episode: 982, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 5.391, mean reward: 0.490 [0.401, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.470, 10.100], loss: 0.002883, mae: 0.059682, mean_q: 0.450261
 36485/100000: episode: 983, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 4.234, mean reward: 0.385 [0.303, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.488, 10.100], loss: 0.002908, mae: 0.059319, mean_q: 0.435505
 36493/100000: episode: 984, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 3.883, mean reward: 0.485 [0.445, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.413, 10.100], loss: 0.002782, mae: 0.058331, mean_q: 0.390569
 36502/100000: episode: 985, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 4.594, mean reward: 0.510 [0.464, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.541, 10.100], loss: 0.003402, mae: 0.063373, mean_q: 0.376057
 36513/100000: episode: 986, duration: 0.063s, episode steps: 11, steps per second: 173, episode reward: 6.185, mean reward: 0.562 [0.497, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.674, 10.100], loss: 0.002532, mae: 0.055942, mean_q: 0.378085
 36524/100000: episode: 987, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 5.420, mean reward: 0.493 [0.416, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.630, 10.100], loss: 0.003382, mae: 0.065351, mean_q: 0.447598
 36530/100000: episode: 988, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 3.004, mean reward: 0.501 [0.448, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.544, 10.100], loss: 0.002486, mae: 0.057127, mean_q: 0.458882
 36541/100000: episode: 989, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 5.157, mean reward: 0.469 [0.331, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.430, 10.100], loss: 0.002416, mae: 0.053920, mean_q: 0.356910
 36552/100000: episode: 990, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 5.344, mean reward: 0.486 [0.410, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.395, 10.100], loss: 0.002640, mae: 0.055584, mean_q: 0.415535
 36563/100000: episode: 991, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 4.599, mean reward: 0.418 [0.275, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.452, 10.100], loss: 0.002561, mae: 0.055657, mean_q: 0.414195
 36574/100000: episode: 992, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 4.742, mean reward: 0.431 [0.302, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.530, 10.100], loss: 0.002725, mae: 0.057243, mean_q: 0.452974
 36585/100000: episode: 993, duration: 0.073s, episode steps: 11, steps per second: 150, episode reward: 5.574, mean reward: 0.507 [0.458, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.463, 10.100], loss: 0.002356, mae: 0.051301, mean_q: 0.403776
 36596/100000: episode: 994, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 5.737, mean reward: 0.522 [0.417, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.348, 10.100], loss: 0.002741, mae: 0.058052, mean_q: 0.428190
 36607/100000: episode: 995, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 5.751, mean reward: 0.523 [0.464, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.658, 10.100], loss: 0.002416, mae: 0.054346, mean_q: 0.489829
 36618/100000: episode: 996, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 4.955, mean reward: 0.450 [0.395, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.472, 10.100], loss: 0.002566, mae: 0.056761, mean_q: 0.450859
 36629/100000: episode: 997, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 5.384, mean reward: 0.489 [0.415, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.493, 10.100], loss: 0.003041, mae: 0.060029, mean_q: 0.462783
 36640/100000: episode: 998, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 5.396, mean reward: 0.491 [0.356, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.635, 10.100], loss: 0.003240, mae: 0.062190, mean_q: 0.475913
 36652/100000: episode: 999, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 6.472, mean reward: 0.539 [0.457, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.463, 10.100], loss: 0.003189, mae: 0.062787, mean_q: 0.465434
 36658/100000: episode: 1000, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 3.196, mean reward: 0.533 [0.515, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.512, 10.100], loss: 0.003162, mae: 0.063250, mean_q: 0.481981
[Info] FALSIFICATION!
 36659/100000: episode: 1001, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.140, 9.813], loss: 0.001772, mae: 0.049492, mean_q: 0.358974
 36759/100000: episode: 1002, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -19.443, mean reward: -0.194 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.932, 10.255], loss: 0.002899, mae: 0.057839, mean_q: 0.454800
 36859/100000: episode: 1003, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.657, mean reward: -0.167 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.156, 10.098], loss: 0.004124, mae: 0.064248, mean_q: 0.453072
 36959/100000: episode: 1004, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.492, mean reward: -0.175 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.645, 10.177], loss: 0.016342, mae: 0.069289, mean_q: 0.461283
 37059/100000: episode: 1005, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.936, mean reward: -0.169 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.855, 10.098], loss: 0.016133, mae: 0.066971, mean_q: 0.478444
 37159/100000: episode: 1006, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.520, mean reward: -0.165 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.729, 10.098], loss: 0.003155, mae: 0.061562, mean_q: 0.478796
 37259/100000: episode: 1007, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -17.229, mean reward: -0.172 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.840, 10.142], loss: 0.016029, mae: 0.064335, mean_q: 0.467763
 37359/100000: episode: 1008, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.461, mean reward: -0.165 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.773, 10.322], loss: 0.002665, mae: 0.057350, mean_q: 0.473617
 37459/100000: episode: 1009, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.256, mean reward: -0.183 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.616, 10.283], loss: 0.016302, mae: 0.067924, mean_q: 0.452711
 37559/100000: episode: 1010, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.153, mean reward: -0.182 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.604, 10.098], loss: 0.018300, mae: 0.078084, mean_q: 0.449550
 37659/100000: episode: 1011, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.882, mean reward: -0.159 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.286, 10.185], loss: 0.003316, mae: 0.061294, mean_q: 0.439221
 37759/100000: episode: 1012, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.817, mean reward: -0.178 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.973, 10.249], loss: 0.016212, mae: 0.067404, mean_q: 0.448730
 37859/100000: episode: 1013, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.989, mean reward: -0.160 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.634, 10.098], loss: 0.015529, mae: 0.059827, mean_q: 0.417038
 37959/100000: episode: 1014, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.219, mean reward: -0.172 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.459, 10.098], loss: 0.015542, mae: 0.062641, mean_q: 0.406457
 38059/100000: episode: 1015, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -20.354, mean reward: -0.204 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.057, 10.098], loss: 0.029690, mae: 0.078814, mean_q: 0.378661
 38159/100000: episode: 1016, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -21.159, mean reward: -0.212 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.847, 10.187], loss: 0.002836, mae: 0.058181, mean_q: 0.379478
 38259/100000: episode: 1017, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.122, mean reward: -0.191 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.627, 10.098], loss: 0.016183, mae: 0.067110, mean_q: 0.380337
 38359/100000: episode: 1018, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.256, mean reward: -0.163 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.584, 10.098], loss: 0.002547, mae: 0.054325, mean_q: 0.321334
 38459/100000: episode: 1019, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.598, mean reward: -0.166 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.581, 10.281], loss: 0.004567, mae: 0.063440, mean_q: 0.311224
 38559/100000: episode: 1020, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.395, mean reward: -0.194 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.897, 10.161], loss: 0.003454, mae: 0.059962, mean_q: 0.293095
 38659/100000: episode: 1021, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.222, mean reward: -0.172 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.374, 10.098], loss: 0.029431, mae: 0.074129, mean_q: 0.262238
 38759/100000: episode: 1022, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.928, mean reward: -0.159 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.343, 10.098], loss: 0.002809, mae: 0.056907, mean_q: 0.230866
 38859/100000: episode: 1023, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.894, mean reward: -0.139 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.757, 10.098], loss: 0.002724, mae: 0.055884, mean_q: 0.251121
 38959/100000: episode: 1024, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.776, mean reward: -0.178 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.677, 10.121], loss: 0.016305, mae: 0.066545, mean_q: 0.218546
 39059/100000: episode: 1025, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -4.529, mean reward: -0.045 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.062, 10.098], loss: 0.028392, mae: 0.063428, mean_q: 0.227917
 39159/100000: episode: 1026, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -13.621, mean reward: -0.136 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.121, 10.098], loss: 0.041723, mae: 0.078051, mean_q: 0.202763
 39259/100000: episode: 1027, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.416, mean reward: -0.184 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.656, 10.231], loss: 0.016718, mae: 0.067335, mean_q: 0.157028
 39359/100000: episode: 1028, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.864, mean reward: -0.169 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.293, 10.170], loss: 0.028614, mae: 0.066998, mean_q: 0.174787
 39459/100000: episode: 1029, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.309, mean reward: -0.183 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.447, 10.118], loss: 0.002487, mae: 0.052560, mean_q: 0.160379
 39559/100000: episode: 1030, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.965, mean reward: -0.160 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.397, 10.098], loss: 0.015858, mae: 0.061054, mean_q: 0.089501
 39659/100000: episode: 1031, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.665, mean reward: -0.177 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.316, 10.251], loss: 0.002594, mae: 0.053363, mean_q: 0.109116
 39759/100000: episode: 1032, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.643, mean reward: -0.146 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.008, 10.098], loss: 0.002484, mae: 0.051945, mean_q: 0.054164
 39859/100000: episode: 1033, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.761, mean reward: -0.188 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.940, 10.327], loss: 0.002467, mae: 0.051575, mean_q: 0.049547
 39959/100000: episode: 1034, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.816, mean reward: -0.138 [-1.000, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.647, 10.098], loss: 0.002550, mae: 0.052874, mean_q: 0.059421
 40059/100000: episode: 1035, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.161, mean reward: -0.172 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.134, 10.098], loss: 0.015808, mae: 0.061342, mean_q: 0.031042
 40159/100000: episode: 1036, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.275, mean reward: -0.153 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.707, 10.098], loss: 0.002732, mae: 0.054015, mean_q: 0.004311
 40259/100000: episode: 1037, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.855, mean reward: -0.179 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.400, 10.198], loss: 0.028122, mae: 0.064075, mean_q: -0.024432
 40359/100000: episode: 1038, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.145, mean reward: -0.191 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.211, 10.098], loss: 0.028986, mae: 0.070468, mean_q: -0.021829
 40459/100000: episode: 1039, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.487, mean reward: -0.165 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.891, 10.310], loss: 0.016123, mae: 0.063338, mean_q: -0.026885
 40559/100000: episode: 1040, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -19.550, mean reward: -0.196 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.180, 10.181], loss: 0.018057, mae: 0.075587, mean_q: -0.041801
 40659/100000: episode: 1041, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.661, mean reward: -0.177 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.580, 10.240], loss: 0.002649, mae: 0.053467, mean_q: -0.074125
 40759/100000: episode: 1042, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.673, mean reward: -0.187 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.498, 10.098], loss: 0.002689, mae: 0.052688, mean_q: -0.128331
 40859/100000: episode: 1043, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.855, mean reward: -0.189 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.004, 10.204], loss: 0.015574, mae: 0.057532, mean_q: -0.149240
 40959/100000: episode: 1044, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -17.808, mean reward: -0.178 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.174, 10.098], loss: 0.002554, mae: 0.051562, mean_q: -0.161239
 41059/100000: episode: 1045, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.664, mean reward: -0.177 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.085, 10.170], loss: 0.028258, mae: 0.061879, mean_q: -0.188156
 41159/100000: episode: 1046, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -17.057, mean reward: -0.171 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.737, 10.189], loss: 0.015983, mae: 0.060328, mean_q: -0.215205
 41259/100000: episode: 1047, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.550, mean reward: -0.135 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.173, 10.335], loss: 0.003265, mae: 0.058598, mean_q: -0.241257
 41359/100000: episode: 1048, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.022, mean reward: -0.180 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.332, 10.098], loss: 0.002555, mae: 0.051543, mean_q: -0.261490
 41459/100000: episode: 1049, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -14.313, mean reward: -0.143 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.933, 10.098], loss: 0.028945, mae: 0.068551, mean_q: -0.248617
 41559/100000: episode: 1050, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.519, mean reward: -0.185 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.122, 10.098], loss: 0.016770, mae: 0.065638, mean_q: -0.306635
 41659/100000: episode: 1051, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.843, mean reward: -0.158 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.850, 10.105], loss: 0.015516, mae: 0.061015, mean_q: -0.300204
 41759/100000: episode: 1052, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.899, mean reward: -0.179 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.534, 10.098], loss: 0.002321, mae: 0.047870, mean_q: -0.311031
 41859/100000: episode: 1053, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.279, mean reward: -0.173 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.493, 10.281], loss: 0.002553, mae: 0.051082, mean_q: -0.291853
 41959/100000: episode: 1054, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.880, mean reward: -0.199 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.497, 10.103], loss: 0.002425, mae: 0.049517, mean_q: -0.323014
 42059/100000: episode: 1055, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -15.920, mean reward: -0.159 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.649, 10.098], loss: 0.002367, mae: 0.048714, mean_q: -0.303674
 42159/100000: episode: 1056, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -20.258, mean reward: -0.203 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.189, 10.165], loss: 0.002408, mae: 0.048265, mean_q: -0.318918
 42259/100000: episode: 1057, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -11.895, mean reward: -0.119 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.310, 10.098], loss: 0.002580, mae: 0.049874, mean_q: -0.334132
 42359/100000: episode: 1058, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.536, mean reward: -0.155 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.914, 10.246], loss: 0.002603, mae: 0.051064, mean_q: -0.316214
 42459/100000: episode: 1059, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.125, mean reward: -0.161 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.059, 10.098], loss: 0.002445, mae: 0.049757, mean_q: -0.308490
 42559/100000: episode: 1060, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -11.847, mean reward: -0.118 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.202, 10.098], loss: 0.002537, mae: 0.049418, mean_q: -0.298339
 42659/100000: episode: 1061, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.304, mean reward: -0.193 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.259, 10.246], loss: 0.002292, mae: 0.047922, mean_q: -0.334297
 42759/100000: episode: 1062, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.123, mean reward: -0.161 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.521, 10.149], loss: 0.002402, mae: 0.049349, mean_q: -0.332331
 42859/100000: episode: 1063, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.144, mean reward: -0.191 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.417, 10.098], loss: 0.002293, mae: 0.048527, mean_q: -0.296599
 42959/100000: episode: 1064, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.764, mean reward: -0.188 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.669, 10.098], loss: 0.002279, mae: 0.046811, mean_q: -0.377511
 43059/100000: episode: 1065, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.559, mean reward: -0.186 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.808, 10.098], loss: 0.002518, mae: 0.050359, mean_q: -0.299788
 43159/100000: episode: 1066, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.385, mean reward: -0.154 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.916, 10.257], loss: 0.002553, mae: 0.051033, mean_q: -0.336416
 43259/100000: episode: 1067, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.324, mean reward: -0.173 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.745, 10.098], loss: 0.002451, mae: 0.051227, mean_q: -0.326763
 43359/100000: episode: 1068, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.782, mean reward: -0.178 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.478, 10.224], loss: 0.002308, mae: 0.048781, mean_q: -0.344069
 43459/100000: episode: 1069, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.283, mean reward: -0.183 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.281, 10.241], loss: 0.002212, mae: 0.046969, mean_q: -0.335310
 43559/100000: episode: 1070, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.284, mean reward: -0.173 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.690, 10.250], loss: 0.002534, mae: 0.050720, mean_q: -0.341081
 43659/100000: episode: 1071, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -17.940, mean reward: -0.179 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.415, 10.112], loss: 0.006839, mae: 0.075902, mean_q: -0.301639
 43759/100000: episode: 1072, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.935, mean reward: -0.179 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.688, 10.098], loss: 0.004121, mae: 0.063877, mean_q: -0.319928
 43859/100000: episode: 1073, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.913, mean reward: -0.169 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.104, 10.098], loss: 0.002617, mae: 0.051784, mean_q: -0.286393
 43959/100000: episode: 1074, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.899, mean reward: -0.189 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.121, 10.098], loss: 0.002552, mae: 0.049821, mean_q: -0.291549
 44059/100000: episode: 1075, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.366, mean reward: -0.174 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.085, 10.123], loss: 0.002304, mae: 0.047829, mean_q: -0.317153
 44159/100000: episode: 1076, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -13.107, mean reward: -0.131 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.702, 10.276], loss: 0.002597, mae: 0.050721, mean_q: -0.311285
 44259/100000: episode: 1077, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -13.898, mean reward: -0.139 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.644, 10.142], loss: 0.002571, mae: 0.050305, mean_q: -0.305991
 44359/100000: episode: 1078, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -8.469, mean reward: -0.085 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.389, 10.098], loss: 0.002528, mae: 0.050190, mean_q: -0.292652
 44459/100000: episode: 1079, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -10.220, mean reward: -0.102 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.032, 10.098], loss: 0.002549, mae: 0.049204, mean_q: -0.345656
 44559/100000: episode: 1080, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.291, mean reward: -0.183 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.842, 10.098], loss: 0.002567, mae: 0.049998, mean_q: -0.357459
 44659/100000: episode: 1081, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.796, mean reward: -0.188 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.252, 10.122], loss: 0.002605, mae: 0.050457, mean_q: -0.327742
 44759/100000: episode: 1082, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -17.924, mean reward: -0.179 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.481, 10.253], loss: 0.002738, mae: 0.053930, mean_q: -0.274919
 44859/100000: episode: 1083, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.406, mean reward: -0.174 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.777, 10.137], loss: 0.002496, mae: 0.049628, mean_q: -0.309134
 44959/100000: episode: 1084, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -17.852, mean reward: -0.179 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.351, 10.241], loss: 0.002520, mae: 0.051706, mean_q: -0.301010
 45059/100000: episode: 1085, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.199, mean reward: -0.192 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.674, 10.099], loss: 0.002261, mae: 0.048310, mean_q: -0.320644
 45159/100000: episode: 1086, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.139, mean reward: -0.181 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.595, 10.098], loss: 0.002465, mae: 0.050476, mean_q: -0.290493
 45259/100000: episode: 1087, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.862, mean reward: -0.169 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.676, 10.220], loss: 0.002517, mae: 0.049498, mean_q: -0.342184
 45359/100000: episode: 1088, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.804, mean reward: -0.148 [-1.000, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.807, 10.260], loss: 0.002448, mae: 0.049465, mean_q: -0.324089
 45459/100000: episode: 1089, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.036, mean reward: -0.190 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.185, 10.098], loss: 0.002752, mae: 0.053795, mean_q: -0.325440
 45559/100000: episode: 1090, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -11.015, mean reward: -0.110 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.467, 10.098], loss: 0.002360, mae: 0.048364, mean_q: -0.329013
 45659/100000: episode: 1091, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.264, mean reward: -0.173 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.594, 10.098], loss: 0.002582, mae: 0.051751, mean_q: -0.290399
 45759/100000: episode: 1092, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.962, mean reward: -0.150 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.512, 10.156], loss: 0.002466, mae: 0.050082, mean_q: -0.328540
 45859/100000: episode: 1093, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.420, mean reward: -0.174 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.971, 10.306], loss: 0.004197, mae: 0.060016, mean_q: -0.319773
 45959/100000: episode: 1094, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -15.999, mean reward: -0.160 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.128, 10.098], loss: 0.002599, mae: 0.052947, mean_q: -0.307409
 46059/100000: episode: 1095, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.003, mean reward: -0.190 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.951, 10.158], loss: 0.002481, mae: 0.050562, mean_q: -0.291426
 46159/100000: episode: 1096, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: -16.427, mean reward: -0.164 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.642, 10.145], loss: 0.002415, mae: 0.049330, mean_q: -0.311953
 46259/100000: episode: 1097, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.051, mean reward: -0.161 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.079, 10.098], loss: 0.002412, mae: 0.049785, mean_q: -0.321748
 46359/100000: episode: 1098, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.674, mean reward: -0.157 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.427, 10.344], loss: 0.002429, mae: 0.049976, mean_q: -0.298219
 46459/100000: episode: 1099, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.726, mean reward: -0.197 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.198, 10.241], loss: 0.002927, mae: 0.052176, mean_q: -0.333842
 46559/100000: episode: 1100, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -14.685, mean reward: -0.147 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.905, 10.303], loss: 0.005148, mae: 0.069563, mean_q: -0.297540
[Info] 100-TH LEVEL FOUND: 0.5790188312530518, Considering 10/90 traces
 46659/100000: episode: 1101, duration: 4.367s, episode steps: 100, steps per second: 23, episode reward: -15.690, mean reward: -0.157 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.165, 10.098], loss: 0.002363, mae: 0.049734, mean_q: -0.316669
 46669/100000: episode: 1102, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 2.429, mean reward: 0.243 [0.189, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.302, 10.100], loss: 0.002120, mae: 0.045826, mean_q: -0.342739
 46678/100000: episode: 1103, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 3.222, mean reward: 0.358 [0.310, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.715, 10.100], loss: 0.002464, mae: 0.048509, mean_q: -0.372328
 46707/100000: episode: 1104, duration: 0.170s, episode steps: 29, steps per second: 170, episode reward: 9.229, mean reward: 0.318 [0.143, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.714, 10.100], loss: 0.002454, mae: 0.049757, mean_q: -0.267079
 46736/100000: episode: 1105, duration: 0.139s, episode steps: 29, steps per second: 209, episode reward: 9.001, mean reward: 0.310 [0.199, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.554, 10.100], loss: 0.002445, mae: 0.049381, mean_q: -0.260555
 46750/100000: episode: 1106, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 4.672, mean reward: 0.334 [0.304, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.155, 10.100], loss: 0.002796, mae: 0.050122, mean_q: -0.299189
 46783/100000: episode: 1107, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 12.245, mean reward: 0.371 [0.264, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.523, 10.100], loss: 0.002787, mae: 0.052611, mean_q: -0.286990
 46791/100000: episode: 1108, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 2.841, mean reward: 0.355 [0.266, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.501], loss: 0.002059, mae: 0.045477, mean_q: -0.338113
 46805/100000: episode: 1109, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 4.877, mean reward: 0.348 [0.279, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.160, 10.100], loss: 0.002119, mae: 0.046886, mean_q: -0.304941
 46819/100000: episode: 1110, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 4.245, mean reward: 0.303 [0.269, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.183, 10.100], loss: 0.002604, mae: 0.050660, mean_q: -0.208388
 46829/100000: episode: 1111, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 3.122, mean reward: 0.312 [0.182, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-1.336, 10.100], loss: 0.002792, mae: 0.050986, mean_q: -0.309043
 46850/100000: episode: 1112, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 6.671, mean reward: 0.318 [0.260, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.168, 10.420], loss: 0.002823, mae: 0.052454, mean_q: -0.265682
 46864/100000: episode: 1113, duration: 0.085s, episode steps: 14, steps per second: 166, episode reward: 4.642, mean reward: 0.332 [0.241, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.040, 10.100], loss: 0.002586, mae: 0.050004, mean_q: -0.189835
 46893/100000: episode: 1114, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 9.489, mean reward: 0.327 [0.219, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.380, 10.100], loss: 0.002723, mae: 0.051097, mean_q: -0.291972
 46902/100000: episode: 1115, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 2.863, mean reward: 0.318 [0.257, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.393, 10.100], loss: 0.002335, mae: 0.047039, mean_q: -0.314143
 46916/100000: episode: 1116, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 4.734, mean reward: 0.338 [0.214, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.412, 10.100], loss: 0.002538, mae: 0.050069, mean_q: -0.307403
 46924/100000: episode: 1117, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 2.469, mean reward: 0.309 [0.261, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.447], loss: 0.002681, mae: 0.050714, mean_q: -0.214717
 46933/100000: episode: 1118, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 3.449, mean reward: 0.383 [0.261, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.228, 10.100], loss: 0.002264, mae: 0.047642, mean_q: -0.314198
 46964/100000: episode: 1119, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 10.255, mean reward: 0.331 [0.131, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.106, 10.100], loss: 0.002280, mae: 0.048015, mean_q: -0.241920
 46972/100000: episode: 1120, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 2.979, mean reward: 0.372 [0.300, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.430], loss: 0.002804, mae: 0.055534, mean_q: -0.179714
 47003/100000: episode: 1121, duration: 0.171s, episode steps: 31, steps per second: 182, episode reward: 8.174, mean reward: 0.264 [0.053, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.652, 10.100], loss: 0.002253, mae: 0.047260, mean_q: -0.239087
 47011/100000: episode: 1122, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 2.875, mean reward: 0.359 [0.267, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.275], loss: 0.002045, mae: 0.044373, mean_q: -0.258387
 47044/100000: episode: 1123, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 10.542, mean reward: 0.319 [0.175, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.235, 10.100], loss: 0.002672, mae: 0.052179, mean_q: -0.192259
 47052/100000: episode: 1124, duration: 0.057s, episode steps: 8, steps per second: 141, episode reward: 2.470, mean reward: 0.309 [0.222, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.346], loss: 0.002340, mae: 0.049525, mean_q: -0.198762
 47062/100000: episode: 1125, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 1.986, mean reward: 0.199 [0.144, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.800, 10.100], loss: 0.002156, mae: 0.046060, mean_q: -0.281490
 47082/100000: episode: 1126, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 4.423, mean reward: 0.221 [0.117, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.076, 10.270], loss: 0.002320, mae: 0.049206, mean_q: -0.193988
 47088/100000: episode: 1127, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 2.654, mean reward: 0.442 [0.417, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.426, 10.100], loss: 0.002891, mae: 0.054035, mean_q: -0.210472
 47117/100000: episode: 1128, duration: 0.149s, episode steps: 29, steps per second: 194, episode reward: 10.598, mean reward: 0.365 [0.191, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.036, 10.100], loss: 0.002741, mae: 0.052311, mean_q: -0.211414
 47125/100000: episode: 1129, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 2.668, mean reward: 0.334 [0.246, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.465], loss: 0.002286, mae: 0.049004, mean_q: -0.170913
 47135/100000: episode: 1130, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 3.075, mean reward: 0.307 [0.217, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.297, 10.100], loss: 0.002991, mae: 0.055088, mean_q: -0.161785
 47155/100000: episode: 1131, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 4.287, mean reward: 0.214 [0.101, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.089, 10.184], loss: 0.002734, mae: 0.053636, mean_q: -0.213051
 47163/100000: episode: 1132, duration: 0.040s, episode steps: 8, steps per second: 201, episode reward: 2.242, mean reward: 0.280 [0.214, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.311], loss: 0.002416, mae: 0.049264, mean_q: -0.326939
 47196/100000: episode: 1133, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 10.484, mean reward: 0.318 [0.203, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.363, 10.100], loss: 0.002419, mae: 0.049500, mean_q: -0.220212
 47202/100000: episode: 1134, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 2.611, mean reward: 0.435 [0.381, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.276, 10.100], loss: 0.002450, mae: 0.050095, mean_q: -0.220354
 47211/100000: episode: 1135, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 3.230, mean reward: 0.359 [0.290, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.307, 10.100], loss: 0.002557, mae: 0.050882, mean_q: -0.278983
 47219/100000: episode: 1136, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 1.609, mean reward: 0.201 [0.164, 0.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.266], loss: 0.002591, mae: 0.049603, mean_q: -0.261725
 47229/100000: episode: 1137, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 3.119, mean reward: 0.312 [0.206, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.318, 10.100], loss: 0.003035, mae: 0.053992, mean_q: -0.189445
 47250/100000: episode: 1138, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 5.001, mean reward: 0.238 [0.018, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.983, 10.167], loss: 0.002587, mae: 0.051245, mean_q: -0.184531
 47281/100000: episode: 1139, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 10.541, mean reward: 0.340 [0.177, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.485, 10.100], loss: 0.002596, mae: 0.051029, mean_q: -0.197933
 47290/100000: episode: 1140, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 2.886, mean reward: 0.321 [0.247, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.367, 10.100], loss: 0.002856, mae: 0.053458, mean_q: -0.196757
 47298/100000: episode: 1141, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 2.303, mean reward: 0.288 [0.198, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.336], loss: 0.002945, mae: 0.058340, mean_q: -0.133326
 47319/100000: episode: 1142, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 5.608, mean reward: 0.267 [0.171, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.275], loss: 0.002647, mae: 0.052818, mean_q: -0.198247
 47352/100000: episode: 1143, duration: 0.187s, episode steps: 33, steps per second: 177, episode reward: 8.661, mean reward: 0.262 [0.079, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.035, 10.110], loss: 0.002584, mae: 0.051835, mean_q: -0.205970
 47360/100000: episode: 1144, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 2.543, mean reward: 0.318 [0.255, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.314], loss: 0.002929, mae: 0.053297, mean_q: -0.129539
 47380/100000: episode: 1145, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 4.865, mean reward: 0.243 [0.043, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.757, 10.181], loss: 0.002460, mae: 0.049103, mean_q: -0.220543
 47401/100000: episode: 1146, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 7.024, mean reward: 0.334 [0.236, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.435, 10.417], loss: 0.002793, mae: 0.051850, mean_q: -0.279843
 47407/100000: episode: 1147, duration: 0.031s, episode steps: 6, steps per second: 194, episode reward: 2.231, mean reward: 0.372 [0.298, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.202, 10.100], loss: 0.002586, mae: 0.050626, mean_q: -0.321868
 47436/100000: episode: 1148, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 8.969, mean reward: 0.309 [0.175, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.353, 10.100], loss: 0.002289, mae: 0.048578, mean_q: -0.257579
 47465/100000: episode: 1149, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 10.223, mean reward: 0.353 [0.241, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.439, 10.100], loss: 0.002272, mae: 0.049157, mean_q: -0.191637
 47471/100000: episode: 1150, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 2.109, mean reward: 0.351 [0.321, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.219, 10.100], loss: 0.002478, mae: 0.051249, mean_q: -0.116520
 47481/100000: episode: 1151, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 4.293, mean reward: 0.429 [0.245, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.386, 10.100], loss: 0.002005, mae: 0.045824, mean_q: -0.117120
 47491/100000: episode: 1152, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 2.833, mean reward: 0.283 [0.219, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-1.582, 10.100], loss: 0.002693, mae: 0.053464, mean_q: -0.125194
 47505/100000: episode: 1153, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 5.263, mean reward: 0.376 [0.327, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.423, 10.100], loss: 0.002690, mae: 0.052296, mean_q: -0.130959
 47525/100000: episode: 1154, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 5.516, mean reward: 0.276 [0.143, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.327, 10.234], loss: 0.002678, mae: 0.052638, mean_q: -0.160088
 47539/100000: episode: 1155, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 4.866, mean reward: 0.348 [0.261, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.287, 10.100], loss: 0.002602, mae: 0.052586, mean_q: -0.144669
 47559/100000: episode: 1156, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 6.123, mean reward: 0.306 [0.217, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.195, 10.349], loss: 0.002581, mae: 0.052837, mean_q: -0.141158
 47567/100000: episode: 1157, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 2.311, mean reward: 0.289 [0.237, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.348], loss: 0.001852, mae: 0.043908, mean_q: -0.259974
 47588/100000: episode: 1158, duration: 0.101s, episode steps: 21, steps per second: 207, episode reward: 7.720, mean reward: 0.368 [0.203, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.110, 10.346], loss: 0.002413, mae: 0.050034, mean_q: -0.147255
 47596/100000: episode: 1159, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 2.275, mean reward: 0.284 [0.252, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.779, 10.360], loss: 0.002689, mae: 0.054685, mean_q: -0.148929
 47629/100000: episode: 1160, duration: 0.182s, episode steps: 33, steps per second: 182, episode reward: 12.330, mean reward: 0.374 [0.227, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.539, 10.100], loss: 0.002711, mae: 0.053510, mean_q: -0.143148
 47637/100000: episode: 1161, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 2.383, mean reward: 0.298 [0.175, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-1.021, 10.156], loss: 0.002420, mae: 0.050799, mean_q: -0.132448
 47643/100000: episode: 1162, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 2.647, mean reward: 0.441 [0.371, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.275, 10.100], loss: 0.002837, mae: 0.053505, mean_q: -0.149766
 47674/100000: episode: 1163, duration: 0.178s, episode steps: 31, steps per second: 175, episode reward: 9.189, mean reward: 0.296 [0.079, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.810, 10.100], loss: 0.003286, mae: 0.059551, mean_q: -0.110300
 47695/100000: episode: 1164, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 6.526, mean reward: 0.311 [0.157, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.493, 10.300], loss: 0.002794, mae: 0.053263, mean_q: -0.134589
 47701/100000: episode: 1165, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 2.364, mean reward: 0.394 [0.347, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.200, 10.100], loss: 0.002621, mae: 0.053150, mean_q: -0.099395
 47721/100000: episode: 1166, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 4.646, mean reward: 0.232 [0.082, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.170, 10.193], loss: 0.002454, mae: 0.051415, mean_q: -0.140024
 47752/100000: episode: 1167, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 10.761, mean reward: 0.347 [0.088, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.128, 10.100], loss: 0.002692, mae: 0.053417, mean_q: -0.094606
 47772/100000: episode: 1168, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 4.558, mean reward: 0.228 [0.119, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.626, 10.266], loss: 0.003242, mae: 0.057570, mean_q: -0.098179
 47780/100000: episode: 1169, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 1.844, mean reward: 0.230 [0.175, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.252], loss: 0.003011, mae: 0.053785, mean_q: -0.123496
 47800/100000: episode: 1170, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 6.003, mean reward: 0.300 [0.225, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.666, 10.372], loss: 0.003105, mae: 0.056117, mean_q: -0.111545
 47810/100000: episode: 1171, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 2.766, mean reward: 0.277 [0.147, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.378, 10.100], loss: 0.002999, mae: 0.057280, mean_q: -0.059527
 47816/100000: episode: 1172, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 2.455, mean reward: 0.409 [0.360, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.792, 10.100], loss: 0.002748, mae: 0.054804, mean_q: -0.117162
 47849/100000: episode: 1173, duration: 0.194s, episode steps: 33, steps per second: 170, episode reward: 14.369, mean reward: 0.435 [0.268, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.822, 10.100], loss: 0.002837, mae: 0.055531, mean_q: -0.090086
 47859/100000: episode: 1174, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 2.791, mean reward: 0.279 [0.164, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-1.110, 10.100], loss: 0.002629, mae: 0.051587, mean_q: -0.086031
 47869/100000: episode: 1175, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 3.045, mean reward: 0.305 [0.173, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.304, 10.100], loss: 0.002837, mae: 0.056145, mean_q: -0.039289
 47900/100000: episode: 1176, duration: 0.159s, episode steps: 31, steps per second: 194, episode reward: 11.961, mean reward: 0.386 [0.289, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.632, 10.100], loss: 0.002698, mae: 0.053796, mean_q: -0.072413
 47929/100000: episode: 1177, duration: 0.157s, episode steps: 29, steps per second: 184, episode reward: 11.539, mean reward: 0.398 [0.254, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.842, 10.100], loss: 0.002537, mae: 0.052474, mean_q: -0.092232
 47962/100000: episode: 1178, duration: 0.184s, episode steps: 33, steps per second: 179, episode reward: 8.332, mean reward: 0.252 [0.121, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.966, 10.100], loss: 0.003402, mae: 0.062081, mean_q: -0.025878
 47968/100000: episode: 1179, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 2.271, mean reward: 0.378 [0.347, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.250, 10.100], loss: 0.003312, mae: 0.059835, mean_q: -0.112369
 47982/100000: episode: 1180, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 4.095, mean reward: 0.292 [0.225, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.292, 10.100], loss: 0.003400, mae: 0.060877, mean_q: 0.041379
 47988/100000: episode: 1181, duration: 0.037s, episode steps: 6, steps per second: 160, episode reward: 2.228, mean reward: 0.371 [0.322, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.376, 10.100], loss: 0.002843, mae: 0.055975, mean_q: -0.065920
 48019/100000: episode: 1182, duration: 0.171s, episode steps: 31, steps per second: 182, episode reward: 9.100, mean reward: 0.294 [0.079, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.450, 10.116], loss: 0.002606, mae: 0.053062, mean_q: -0.021293
 48040/100000: episode: 1183, duration: 0.133s, episode steps: 21, steps per second: 158, episode reward: 6.142, mean reward: 0.292 [0.159, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.642, 10.240], loss: 0.002745, mae: 0.053959, mean_q: -0.075864
 48060/100000: episode: 1184, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 5.462, mean reward: 0.273 [0.168, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.141, 10.374], loss: 0.003014, mae: 0.057227, mean_q: -0.033289
 48093/100000: episode: 1185, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 11.199, mean reward: 0.339 [0.190, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.190, 10.100], loss: 0.002681, mae: 0.054465, mean_q: -0.052865
 48113/100000: episode: 1186, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 6.429, mean reward: 0.321 [0.218, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.355], loss: 0.002460, mae: 0.051905, mean_q: -0.070608
 48121/100000: episode: 1187, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 2.398, mean reward: 0.300 [0.242, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.237], loss: 0.002926, mae: 0.056784, mean_q: 0.033294
 48127/100000: episode: 1188, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 2.623, mean reward: 0.437 [0.369, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.353, 10.100], loss: 0.002783, mae: 0.056277, mean_q: 0.002230
 48148/100000: episode: 1189, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 5.556, mean reward: 0.265 [0.126, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.057, 10.272], loss: 0.003106, mae: 0.058716, mean_q: 0.005919
 48156/100000: episode: 1190, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 2.048, mean reward: 0.256 [0.170, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.390, 10.321], loss: 0.002287, mae: 0.050890, mean_q: -0.095865
[Info] 200-TH LEVEL FOUND: 0.8057529330253601, Considering 10/90 traces
 48164/100000: episode: 1191, duration: 3.940s, episode steps: 8, steps per second: 2, episode reward: 2.639, mean reward: 0.330 [0.295, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.410], loss: 0.003571, mae: 0.063555, mean_q: -0.026676
 48192/100000: episode: 1192, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 12.208, mean reward: 0.436 [0.335, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.400, 10.100], loss: 0.002545, mae: 0.051478, mean_q: -0.076294
 48216/100000: episode: 1193, duration: 0.145s, episode steps: 24, steps per second: 166, episode reward: 6.933, mean reward: 0.289 [0.108, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.392, 10.100], loss: 0.002760, mae: 0.053945, mean_q: -0.041274
 48243/100000: episode: 1194, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 10.972, mean reward: 0.406 [0.145, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.737, 10.100], loss: 0.002870, mae: 0.055393, mean_q: 0.007134
 48266/100000: episode: 1195, duration: 0.131s, episode steps: 23, steps per second: 175, episode reward: 9.237, mean reward: 0.402 [0.275, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.241, 10.100], loss: 0.002688, mae: 0.054577, mean_q: 0.032578
 48290/100000: episode: 1196, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 7.978, mean reward: 0.332 [0.174, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.259, 10.100], loss: 0.002934, mae: 0.058880, mean_q: 0.033710
 48315/100000: episode: 1197, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 10.857, mean reward: 0.434 [0.346, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.424, 10.100], loss: 0.002899, mae: 0.057444, mean_q: 0.010861
 48343/100000: episode: 1198, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 11.852, mean reward: 0.423 [0.183, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.579, 10.100], loss: 0.002760, mae: 0.054834, mean_q: -0.024333
 48371/100000: episode: 1199, duration: 0.167s, episode steps: 28, steps per second: 167, episode reward: 14.276, mean reward: 0.510 [0.371, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.372, 10.100], loss: 0.002504, mae: 0.052524, mean_q: -0.018179
 48395/100000: episode: 1200, duration: 0.127s, episode steps: 24, steps per second: 190, episode reward: 9.945, mean reward: 0.414 [0.304, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.228, 10.100], loss: 0.002740, mae: 0.053577, mean_q: 0.023813
 48407/100000: episode: 1201, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 4.793, mean reward: 0.399 [0.260, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.582, 10.100], loss: 0.002960, mae: 0.056251, mean_q: 0.011986
 48419/100000: episode: 1202, duration: 0.084s, episode steps: 12, steps per second: 143, episode reward: 4.374, mean reward: 0.365 [0.248, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.494, 10.100], loss: 0.002907, mae: 0.056652, mean_q: 0.053240
 48442/100000: episode: 1203, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 9.627, mean reward: 0.419 [0.304, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.414, 10.100], loss: 0.002849, mae: 0.056208, mean_q: -0.034319
 48466/100000: episode: 1204, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 11.130, mean reward: 0.464 [0.378, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.927, 10.100], loss: 0.003395, mae: 0.062600, mean_q: 0.071344
 48494/100000: episode: 1205, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 10.348, mean reward: 0.370 [0.267, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.372, 10.100], loss: 0.003114, mae: 0.058332, mean_q: 0.096768
 48521/100000: episode: 1206, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 9.164, mean reward: 0.339 [0.186, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-1.057, 10.100], loss: 0.003036, mae: 0.056808, mean_q: 0.032679
 48537/100000: episode: 1207, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 6.718, mean reward: 0.420 [0.379, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.470, 10.100], loss: 0.002148, mae: 0.048885, mean_q: 0.014536
 48564/100000: episode: 1208, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 8.829, mean reward: 0.327 [0.207, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.211, 10.100], loss: 0.002630, mae: 0.053130, mean_q: 0.048777
 48576/100000: episode: 1209, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 3.664, mean reward: 0.305 [0.203, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.265, 10.100], loss: 0.002623, mae: 0.053390, mean_q: 0.072829
 48600/100000: episode: 1210, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 8.827, mean reward: 0.368 [0.222, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.679, 10.100], loss: 0.002754, mae: 0.054256, mean_q: 0.065424
 48625/100000: episode: 1211, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 7.618, mean reward: 0.305 [0.142, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.561, 10.100], loss: 0.003026, mae: 0.058078, mean_q: 0.106453
 48641/100000: episode: 1212, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 7.122, mean reward: 0.445 [0.377, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.349, 10.100], loss: 0.003214, mae: 0.060777, mean_q: 0.162494
 48657/100000: episode: 1213, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 6.209, mean reward: 0.388 [0.248, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.165, 10.100], loss: 0.002874, mae: 0.058996, mean_q: 0.025077
 48681/100000: episode: 1214, duration: 0.149s, episode steps: 24, steps per second: 161, episode reward: 11.142, mean reward: 0.464 [0.337, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.668, 10.100], loss: 0.002997, mae: 0.058525, mean_q: 0.055537
 48705/100000: episode: 1215, duration: 0.131s, episode steps: 24, steps per second: 184, episode reward: 7.644, mean reward: 0.318 [0.151, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.958, 10.100], loss: 0.003010, mae: 0.058288, mean_q: 0.073071
 48729/100000: episode: 1216, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 10.482, mean reward: 0.437 [0.367, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.598, 10.100], loss: 0.003022, mae: 0.058585, mean_q: 0.157173
 48753/100000: episode: 1217, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 6.405, mean reward: 0.267 [0.012, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.035, 10.151], loss: 0.002665, mae: 0.055234, mean_q: 0.101977
 48780/100000: episode: 1218, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 10.024, mean reward: 0.371 [0.217, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.273, 10.100], loss: 0.002928, mae: 0.057299, mean_q: 0.099743
 48805/100000: episode: 1219, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 10.549, mean reward: 0.422 [0.242, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-1.196, 10.100], loss: 0.002937, mae: 0.059261, mean_q: 0.138780
 48829/100000: episode: 1220, duration: 0.129s, episode steps: 24, steps per second: 187, episode reward: 7.264, mean reward: 0.303 [0.077, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.296, 10.100], loss: 0.002513, mae: 0.053903, mean_q: 0.130529
 48857/100000: episode: 1221, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 10.389, mean reward: 0.371 [0.261, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.267, 10.100], loss: 0.002603, mae: 0.055149, mean_q: 0.154744
 48881/100000: episode: 1222, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 10.011, mean reward: 0.417 [0.267, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.406, 10.100], loss: 0.003284, mae: 0.060642, mean_q: 0.166802
 48897/100000: episode: 1223, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 6.476, mean reward: 0.405 [0.328, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.301, 10.100], loss: 0.003064, mae: 0.058100, mean_q: 0.094345
 48921/100000: episode: 1224, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 8.443, mean reward: 0.352 [0.231, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.277, 10.100], loss: 0.002519, mae: 0.054178, mean_q: 0.159984
 48937/100000: episode: 1225, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 6.945, mean reward: 0.434 [0.361, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.682, 10.100], loss: 0.002778, mae: 0.056495, mean_q: 0.135833
 48953/100000: episode: 1226, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 6.692, mean reward: 0.418 [0.337, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.371, 10.100], loss: 0.002239, mae: 0.050093, mean_q: 0.167196
 48977/100000: episode: 1227, duration: 0.147s, episode steps: 24, steps per second: 163, episode reward: 9.216, mean reward: 0.384 [0.238, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.237, 10.100], loss: 0.002440, mae: 0.053438, mean_q: 0.179759
 49001/100000: episode: 1228, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 9.952, mean reward: 0.415 [0.319, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.441, 10.100], loss: 0.002459, mae: 0.052547, mean_q: 0.161098
 49017/100000: episode: 1229, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 6.013, mean reward: 0.376 [0.308, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.309, 10.100], loss: 0.002801, mae: 0.056854, mean_q: 0.142140
 49029/100000: episode: 1230, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 5.111, mean reward: 0.426 [0.340, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.274, 10.100], loss: 0.002777, mae: 0.056114, mean_q: 0.165116
 49054/100000: episode: 1231, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 8.033, mean reward: 0.321 [0.134, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.364, 10.190], loss: 0.002779, mae: 0.056169, mean_q: 0.202710
 49066/100000: episode: 1232, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 5.640, mean reward: 0.470 [0.364, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.537, 10.100], loss: 0.003678, mae: 0.065286, mean_q: 0.169833
 49093/100000: episode: 1233, duration: 0.145s, episode steps: 27, steps per second: 187, episode reward: 10.971, mean reward: 0.406 [0.296, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.528, 10.100], loss: 0.002785, mae: 0.054962, mean_q: 0.211073
 49117/100000: episode: 1234, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 6.491, mean reward: 0.270 [0.141, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.109, 10.100], loss: 0.002686, mae: 0.055142, mean_q: 0.240898
 49129/100000: episode: 1235, duration: 0.058s, episode steps: 12, steps per second: 207, episode reward: 4.115, mean reward: 0.343 [0.272, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.456, 10.100], loss: 0.002617, mae: 0.055742, mean_q: 0.216374
 49154/100000: episode: 1236, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 12.625, mean reward: 0.505 [0.388, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.298, 10.100], loss: 0.002644, mae: 0.053376, mean_q: 0.182260
 49166/100000: episode: 1237, duration: 0.068s, episode steps: 12, steps per second: 175, episode reward: 4.061, mean reward: 0.338 [0.281, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.406, 10.100], loss: 0.002362, mae: 0.052409, mean_q: 0.134778
 49191/100000: episode: 1238, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 8.697, mean reward: 0.348 [0.180, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.260, 10.100], loss: 0.002575, mae: 0.053447, mean_q: 0.156335
 49215/100000: episode: 1239, duration: 0.120s, episode steps: 24, steps per second: 199, episode reward: 6.238, mean reward: 0.260 [0.137, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.568, 10.100], loss: 0.003286, mae: 0.059630, mean_q: 0.216468
 49231/100000: episode: 1240, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 6.048, mean reward: 0.378 [0.241, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.948, 10.100], loss: 0.002838, mae: 0.058072, mean_q: 0.196507
 49255/100000: episode: 1241, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 7.154, mean reward: 0.298 [0.144, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.335, 10.100], loss: 0.002614, mae: 0.055603, mean_q: 0.244653
 49278/100000: episode: 1242, duration: 0.112s, episode steps: 23, steps per second: 205, episode reward: 7.459, mean reward: 0.324 [0.103, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.357, 10.100], loss: 0.002415, mae: 0.053091, mean_q: 0.168155
 49294/100000: episode: 1243, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 7.342, mean reward: 0.459 [0.385, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.311, 10.100], loss: 0.002484, mae: 0.053495, mean_q: 0.237516
 49318/100000: episode: 1244, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 8.495, mean reward: 0.354 [0.162, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.193, 10.100], loss: 0.002679, mae: 0.054652, mean_q: 0.204584
 49342/100000: episode: 1245, duration: 0.122s, episode steps: 24, steps per second: 196, episode reward: 10.554, mean reward: 0.440 [0.340, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.308, 10.100], loss: 0.002686, mae: 0.056146, mean_q: 0.216249
 49370/100000: episode: 1246, duration: 0.152s, episode steps: 28, steps per second: 185, episode reward: 8.583, mean reward: 0.307 [0.057, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.935, 10.100], loss: 0.002660, mae: 0.057237, mean_q: 0.217773
 49382/100000: episode: 1247, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 4.510, mean reward: 0.376 [0.242, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.361, 10.100], loss: 0.002794, mae: 0.056360, mean_q: 0.205167
 49407/100000: episode: 1248, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 9.063, mean reward: 0.363 [0.118, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.782, 10.100], loss: 0.002527, mae: 0.054086, mean_q: 0.219757
 49419/100000: episode: 1249, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 4.806, mean reward: 0.400 [0.338, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.561, 10.100], loss: 0.003182, mae: 0.060116, mean_q: 0.261034
 49443/100000: episode: 1250, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 8.881, mean reward: 0.370 [0.256, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.378, 10.100], loss: 0.002554, mae: 0.054950, mean_q: 0.258132
 49455/100000: episode: 1251, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 4.824, mean reward: 0.402 [0.243, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.373, 10.100], loss: 0.002658, mae: 0.054359, mean_q: 0.196318
 49479/100000: episode: 1252, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 8.617, mean reward: 0.359 [0.286, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.778, 10.100], loss: 0.002490, mae: 0.053437, mean_q: 0.238622
 49503/100000: episode: 1253, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 10.019, mean reward: 0.417 [0.213, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.560, 10.100], loss: 0.002490, mae: 0.053320, mean_q: 0.245086
 49527/100000: episode: 1254, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 7.047, mean reward: 0.294 [0.081, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-1.572, 10.100], loss: 0.002912, mae: 0.058523, mean_q: 0.291890
 49539/100000: episode: 1255, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 4.646, mean reward: 0.387 [0.315, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.370, 10.100], loss: 0.002834, mae: 0.056098, mean_q: 0.286821
 49551/100000: episode: 1256, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 3.790, mean reward: 0.316 [0.273, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.300, 10.100], loss: 0.002685, mae: 0.057973, mean_q: 0.294663
 49563/100000: episode: 1257, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 4.570, mean reward: 0.381 [0.316, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.625, 10.100], loss: 0.002568, mae: 0.055661, mean_q: 0.290042
 49587/100000: episode: 1258, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 11.210, mean reward: 0.467 [0.307, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-1.161, 10.100], loss: 0.002547, mae: 0.055539, mean_q: 0.275800
 49603/100000: episode: 1259, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 6.893, mean reward: 0.431 [0.362, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.273, 10.100], loss: 0.002740, mae: 0.055949, mean_q: 0.272952
 49631/100000: episode: 1260, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 10.919, mean reward: 0.390 [0.267, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.177, 10.100], loss: 0.002988, mae: 0.059884, mean_q: 0.288678
 49655/100000: episode: 1261, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 8.372, mean reward: 0.349 [0.227, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.385, 10.100], loss: 0.002620, mae: 0.056144, mean_q: 0.335077
 49679/100000: episode: 1262, duration: 0.130s, episode steps: 24, steps per second: 184, episode reward: 6.921, mean reward: 0.288 [0.155, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.099, 10.100], loss: 0.003007, mae: 0.059385, mean_q: 0.316503
 49703/100000: episode: 1263, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 7.685, mean reward: 0.320 [0.169, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.046, 10.100], loss: 0.002691, mae: 0.055855, mean_q: 0.287736
 49727/100000: episode: 1264, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 8.014, mean reward: 0.334 [0.162, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.114, 10.100], loss: 0.002252, mae: 0.051988, mean_q: 0.343638
 49751/100000: episode: 1265, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 10.966, mean reward: 0.457 [0.370, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.415, 10.100], loss: 0.002424, mae: 0.052696, mean_q: 0.300126
 49775/100000: episode: 1266, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 10.268, mean reward: 0.428 [0.360, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.241, 10.100], loss: 0.002180, mae: 0.050778, mean_q: 0.281386
 49799/100000: episode: 1267, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 9.171, mean reward: 0.382 [0.272, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.479, 10.100], loss: 0.002742, mae: 0.057634, mean_q: 0.344401
 49823/100000: episode: 1268, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 10.075, mean reward: 0.420 [0.284, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.427, 10.100], loss: 0.002369, mae: 0.052739, mean_q: 0.304524
 49846/100000: episode: 1269, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 8.860, mean reward: 0.385 [0.230, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.333, 10.100], loss: 0.003072, mae: 0.060245, mean_q: 0.345859
 49870/100000: episode: 1270, duration: 0.131s, episode steps: 24, steps per second: 184, episode reward: 9.905, mean reward: 0.413 [0.297, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.148, 10.100], loss: 0.004208, mae: 0.060858, mean_q: 0.327203
 49894/100000: episode: 1271, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 9.904, mean reward: 0.413 [0.351, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.482, 10.100], loss: 0.002914, mae: 0.060132, mean_q: 0.352348
 49918/100000: episode: 1272, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 9.041, mean reward: 0.377 [0.262, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.394, 10.100], loss: 0.005911, mae: 0.073307, mean_q: 0.358696
 49943/100000: episode: 1273, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 10.713, mean reward: 0.429 [0.331, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.288, 10.100], loss: 0.007133, mae: 0.071406, mean_q: 0.382449
 49967/100000: episode: 1274, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 9.731, mean reward: 0.405 [0.308, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.276, 10.100], loss: 0.004959, mae: 0.069374, mean_q: 0.369997
 49991/100000: episode: 1275, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 6.636, mean reward: 0.276 [0.195, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.349, 10.100], loss: 0.002711, mae: 0.056486, mean_q: 0.355186
 50018/100000: episode: 1276, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 11.060, mean reward: 0.410 [0.323, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.451, 10.100], loss: 0.003455, mae: 0.059329, mean_q: 0.377770
 50043/100000: episode: 1277, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 9.133, mean reward: 0.365 [0.214, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.260, 10.100], loss: 0.006358, mae: 0.070437, mean_q: 0.374247
 50067/100000: episode: 1278, duration: 0.137s, episode steps: 24, steps per second: 176, episode reward: 6.848, mean reward: 0.285 [0.119, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.076, 10.100], loss: 0.002829, mae: 0.056162, mean_q: 0.340282
 50095/100000: episode: 1279, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 8.326, mean reward: 0.297 [0.169, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.287, 10.100], loss: 0.003037, mae: 0.057008, mean_q: 0.433353
 50111/100000: episode: 1280, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 6.264, mean reward: 0.391 [0.314, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.350, 10.100], loss: 0.002648, mae: 0.056859, mean_q: 0.386916
[Info] 300-TH LEVEL FOUND: 0.9832342863082886, Considering 10/90 traces
 50134/100000: episode: 1281, duration: 4.023s, episode steps: 23, steps per second: 6, episode reward: 12.378, mean reward: 0.538 [0.343, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.613, 10.100], loss: 0.003242, mae: 0.061011, mean_q: 0.427027
 50155/100000: episode: 1282, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 10.671, mean reward: 0.508 [0.395, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.359, 10.100], loss: 0.004240, mae: 0.062748, mean_q: 0.410970
 50175/100000: episode: 1283, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 10.476, mean reward: 0.524 [0.436, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.361, 10.100], loss: 0.003985, mae: 0.061142, mean_q: 0.364332
 50195/100000: episode: 1284, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 9.752, mean reward: 0.488 [0.405, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.565, 10.100], loss: 0.002892, mae: 0.058978, mean_q: 0.441845
 50218/100000: episode: 1285, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 11.130, mean reward: 0.484 [0.387, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.343, 10.100], loss: 0.002962, mae: 0.059159, mean_q: 0.430425
 50236/100000: episode: 1286, duration: 0.094s, episode steps: 18, steps per second: 190, episode reward: 7.972, mean reward: 0.443 [0.381, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.226, 10.100], loss: 0.002708, mae: 0.056228, mean_q: 0.400310
 50251/100000: episode: 1287, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 7.176, mean reward: 0.478 [0.413, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.570, 10.100], loss: 0.002568, mae: 0.055874, mean_q: 0.430295
 50274/100000: episode: 1288, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 11.615, mean reward: 0.505 [0.462, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.507, 10.100], loss: 0.002584, mae: 0.054274, mean_q: 0.400042
 50292/100000: episode: 1289, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 7.388, mean reward: 0.410 [0.264, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.789, 10.100], loss: 0.003040, mae: 0.060820, mean_q: 0.509654
 50304/100000: episode: 1290, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 5.468, mean reward: 0.456 [0.381, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.840, 10.100], loss: 0.002885, mae: 0.059147, mean_q: 0.504862
 50324/100000: episode: 1291, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 10.061, mean reward: 0.503 [0.394, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.237, 10.100], loss: 0.003662, mae: 0.063144, mean_q: 0.472998
 50345/100000: episode: 1292, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 9.168, mean reward: 0.437 [0.297, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-1.955, 10.100], loss: 0.003003, mae: 0.057334, mean_q: 0.464575
 50365/100000: episode: 1293, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 10.341, mean reward: 0.517 [0.455, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.527, 10.100], loss: 0.003414, mae: 0.060766, mean_q: 0.438807
 50388/100000: episode: 1294, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 11.490, mean reward: 0.500 [0.445, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.418, 10.100], loss: 0.003547, mae: 0.062112, mean_q: 0.464609
 50411/100000: episode: 1295, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 11.744, mean reward: 0.511 [0.460, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.443, 10.100], loss: 0.003475, mae: 0.061907, mean_q: 0.456603
 50431/100000: episode: 1296, duration: 0.117s, episode steps: 20, steps per second: 172, episode reward: 10.646, mean reward: 0.532 [0.457, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.370, 10.100], loss: 0.003291, mae: 0.059064, mean_q: 0.488492
 50451/100000: episode: 1297, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 9.842, mean reward: 0.492 [0.432, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.848, 10.100], loss: 0.003000, mae: 0.057814, mean_q: 0.462500
 50470/100000: episode: 1298, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 10.770, mean reward: 0.567 [0.516, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.445, 10.100], loss: 0.003179, mae: 0.063056, mean_q: 0.475553
 50490/100000: episode: 1299, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 9.568, mean reward: 0.478 [0.362, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.382, 10.100], loss: 0.002577, mae: 0.055223, mean_q: 0.450235
 50511/100000: episode: 1300, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 9.367, mean reward: 0.446 [0.269, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.402, 10.100], loss: 0.002684, mae: 0.057095, mean_q: 0.481373
 50532/100000: episode: 1301, duration: 0.120s, episode steps: 21, steps per second: 176, episode reward: 11.155, mean reward: 0.531 [0.465, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.731, 10.100], loss: 0.003322, mae: 0.062535, mean_q: 0.488533
 50550/100000: episode: 1302, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 8.261, mean reward: 0.459 [0.375, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.313, 10.100], loss: 0.005104, mae: 0.065609, mean_q: 0.522071
 50569/100000: episode: 1303, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 10.240, mean reward: 0.539 [0.417, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.279, 10.100], loss: 0.004059, mae: 0.066196, mean_q: 0.511740
 50581/100000: episode: 1304, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 6.220, mean reward: 0.518 [0.466, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.943, 10.100], loss: 0.004850, mae: 0.067337, mean_q: 0.573773
 50600/100000: episode: 1305, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 10.585, mean reward: 0.557 [0.483, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.931, 10.100], loss: 0.003308, mae: 0.061129, mean_q: 0.497572
 50620/100000: episode: 1306, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 9.512, mean reward: 0.476 [0.355, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.862, 10.100], loss: 0.002632, mae: 0.056423, mean_q: 0.545308
 50635/100000: episode: 1307, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 7.770, mean reward: 0.518 [0.425, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.584, 10.100], loss: 0.002504, mae: 0.055976, mean_q: 0.488805
 50655/100000: episode: 1308, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 8.369, mean reward: 0.418 [0.233, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.152, 10.100], loss: 0.002663, mae: 0.056779, mean_q: 0.520711
 50674/100000: episode: 1309, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 9.640, mean reward: 0.507 [0.364, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.230, 10.100], loss: 0.002845, mae: 0.056386, mean_q: 0.529940
[Info] FALSIFICATION!
 50683/100000: episode: 1310, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 14.467, mean reward: 1.607 [0.512, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.242, 9.629], loss: 0.004021, mae: 0.063168, mean_q: 0.547156
 50783/100000: episode: 1311, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.498, mean reward: -0.175 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.391, 10.155], loss: 0.003137, mae: 0.057520, mean_q: 0.529097
 50883/100000: episode: 1312, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -15.970, mean reward: -0.160 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.933, 10.306], loss: 0.003203, mae: 0.059741, mean_q: 0.552544
 50983/100000: episode: 1313, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.152, mean reward: -0.182 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.961, 10.098], loss: 0.015913, mae: 0.068019, mean_q: 0.540430
 51083/100000: episode: 1314, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -13.434, mean reward: -0.134 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.945, 10.209], loss: 0.016453, mae: 0.071052, mean_q: 0.552132
 51183/100000: episode: 1315, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.197, mean reward: -0.162 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.651, 10.161], loss: 0.002987, mae: 0.060636, mean_q: 0.549537
 51283/100000: episode: 1316, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.149, mean reward: -0.161 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.063, 10.527], loss: 0.003274, mae: 0.058041, mean_q: 0.555713
 51383/100000: episode: 1317, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -13.425, mean reward: -0.134 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.214, 10.098], loss: 0.015647, mae: 0.064118, mean_q: 0.533179
 51483/100000: episode: 1318, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.912, mean reward: -0.189 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.321, 10.254], loss: 0.030073, mae: 0.086521, mean_q: 0.528018
 51583/100000: episode: 1319, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -13.742, mean reward: -0.137 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.469, 10.272], loss: 0.028102, mae: 0.071792, mean_q: 0.524959
 51683/100000: episode: 1320, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -20.032, mean reward: -0.200 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.522, 10.098], loss: 0.015201, mae: 0.066499, mean_q: 0.532259
 51783/100000: episode: 1321, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.779, mean reward: -0.158 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.865, 10.239], loss: 0.015191, mae: 0.064594, mean_q: 0.489126
 51883/100000: episode: 1322, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.170, mean reward: -0.152 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.934, 10.332], loss: 0.015469, mae: 0.065831, mean_q: 0.479840
 51983/100000: episode: 1323, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.434, mean reward: -0.194 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.209, 10.180], loss: 0.002823, mae: 0.059270, mean_q: 0.472234
 52083/100000: episode: 1324, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.792, mean reward: -0.158 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.128, 10.185], loss: 0.002705, mae: 0.056873, mean_q: 0.438101
 52183/100000: episode: 1325, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.233, mean reward: -0.172 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.553, 10.199], loss: 0.015327, mae: 0.065840, mean_q: 0.420747
 52283/100000: episode: 1326, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.515, mean reward: -0.185 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.793, 10.126], loss: 0.002494, mae: 0.054130, mean_q: 0.394539
 52383/100000: episode: 1327, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.865, mean reward: -0.179 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.075, 10.098], loss: 0.015412, mae: 0.064816, mean_q: 0.374405
 52483/100000: episode: 1328, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.994, mean reward: -0.170 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.176, 10.237], loss: 0.014810, mae: 0.060492, mean_q: 0.348231
 52583/100000: episode: 1329, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -20.382, mean reward: -0.204 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.175, 10.175], loss: 0.002992, mae: 0.059078, mean_q: 0.362719
 52683/100000: episode: 1330, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.982, mean reward: -0.170 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.643, 10.098], loss: 0.016154, mae: 0.067796, mean_q: 0.320314
 52783/100000: episode: 1331, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.736, mean reward: -0.187 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.970, 10.098], loss: 0.002721, mae: 0.055877, mean_q: 0.306988
 52883/100000: episode: 1332, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.311, mean reward: -0.173 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.589, 10.098], loss: 0.002493, mae: 0.053536, mean_q: 0.267277
 52983/100000: episode: 1333, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -8.665, mean reward: -0.087 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.480, 10.098], loss: 0.002464, mae: 0.053552, mean_q: 0.267501
 53083/100000: episode: 1334, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.571, mean reward: -0.196 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.849, 10.098], loss: 0.002578, mae: 0.053892, mean_q: 0.245686
 53183/100000: episode: 1335, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.920, mean reward: -0.189 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.140, 10.236], loss: 0.002387, mae: 0.051212, mean_q: 0.208458
 53283/100000: episode: 1336, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.417, mean reward: -0.154 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.485, 10.132], loss: 0.015901, mae: 0.065251, mean_q: 0.205747
 53383/100000: episode: 1337, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.749, mean reward: -0.187 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.421, 10.176], loss: 0.015351, mae: 0.063251, mean_q: 0.176819
 53483/100000: episode: 1338, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.752, mean reward: -0.158 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.916, 10.158], loss: 0.002848, mae: 0.055723, mean_q: 0.135429
 53583/100000: episode: 1339, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.205, mean reward: -0.162 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.692, 10.276], loss: 0.002838, mae: 0.055821, mean_q: 0.131763
 53683/100000: episode: 1340, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.498, mean reward: -0.175 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.569, 10.141], loss: 0.027599, mae: 0.067800, mean_q: 0.140470
 53783/100000: episode: 1341, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.506, mean reward: -0.135 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.171, 10.100], loss: 0.016626, mae: 0.071855, mean_q: 0.084839
 53883/100000: episode: 1342, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.092, mean reward: -0.161 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.602, 10.220], loss: 0.002654, mae: 0.053710, mean_q: 0.075396
 53983/100000: episode: 1343, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -19.992, mean reward: -0.200 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.953, 10.098], loss: 0.004326, mae: 0.064462, mean_q: 0.058735
 54083/100000: episode: 1344, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -19.811, mean reward: -0.198 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.017, 10.309], loss: 0.027727, mae: 0.071565, mean_q: -0.009670
 54183/100000: episode: 1345, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -13.239, mean reward: -0.132 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.892, 10.444], loss: 0.027220, mae: 0.071291, mean_q: -0.006239
 54283/100000: episode: 1346, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.695, mean reward: -0.187 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.967, 10.250], loss: 0.015434, mae: 0.063712, mean_q: -0.004337
 54383/100000: episode: 1347, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.947, mean reward: -0.199 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.795, 10.118], loss: 0.014911, mae: 0.059999, mean_q: -0.027318
 54483/100000: episode: 1348, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.942, mean reward: -0.159 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.147, 10.098], loss: 0.027570, mae: 0.069542, mean_q: -0.051822
 54583/100000: episode: 1349, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.118, mean reward: -0.131 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.923, 10.098], loss: 0.026852, mae: 0.065344, mean_q: -0.048640
 54683/100000: episode: 1350, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.917, mean reward: -0.179 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.228, 10.236], loss: 0.016295, mae: 0.067951, mean_q: -0.088739
 54783/100000: episode: 1351, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.835, mean reward: -0.158 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.034, 10.098], loss: 0.002646, mae: 0.052695, mean_q: -0.132390
 54883/100000: episode: 1352, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.866, mean reward: -0.179 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.597, 10.098], loss: 0.002712, mae: 0.052711, mean_q: -0.105261
 54983/100000: episode: 1353, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.826, mean reward: -0.178 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.835, 10.299], loss: 0.015223, mae: 0.062247, mean_q: -0.143674
 55083/100000: episode: 1354, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -11.652, mean reward: -0.117 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.262, 10.098], loss: 0.002481, mae: 0.051779, mean_q: -0.171233
 55183/100000: episode: 1355, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.165, mean reward: -0.152 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.828, 10.262], loss: 0.002530, mae: 0.050749, mean_q: -0.211950
 55283/100000: episode: 1356, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.880, mean reward: -0.169 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.409, 10.098], loss: 0.015478, mae: 0.062458, mean_q: -0.214878
 55383/100000: episode: 1357, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -18.175, mean reward: -0.182 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.506, 10.151], loss: 0.014580, mae: 0.057517, mean_q: -0.259269
 55483/100000: episode: 1358, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.045, mean reward: -0.160 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.965, 10.098], loss: 0.002996, mae: 0.056106, mean_q: -0.258047
 55583/100000: episode: 1359, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.275, mean reward: -0.163 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.501, 10.239], loss: 0.002590, mae: 0.051020, mean_q: -0.303946
 55683/100000: episode: 1360, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.399, mean reward: -0.184 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.565, 10.098], loss: 0.002501, mae: 0.049483, mean_q: -0.318661
 55783/100000: episode: 1361, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -12.445, mean reward: -0.124 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.570, 10.098], loss: 0.002487, mae: 0.050004, mean_q: -0.328443
 55883/100000: episode: 1362, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.251, mean reward: -0.183 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.467, 10.098], loss: 0.002559, mae: 0.050353, mean_q: -0.326385
 55983/100000: episode: 1363, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.366, mean reward: -0.184 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.771, 10.110], loss: 0.002611, mae: 0.051448, mean_q: -0.293905
 56083/100000: episode: 1364, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -10.725, mean reward: -0.107 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.146, 10.098], loss: 0.002942, mae: 0.055286, mean_q: -0.309579
 56183/100000: episode: 1365, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.953, mean reward: -0.190 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.764, 10.202], loss: 0.003247, mae: 0.057605, mean_q: -0.320759
 56283/100000: episode: 1366, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -20.210, mean reward: -0.202 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.809, 10.141], loss: 0.002557, mae: 0.051027, mean_q: -0.300717
 56383/100000: episode: 1367, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.707, mean reward: -0.177 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.950, 10.400], loss: 0.002596, mae: 0.050898, mean_q: -0.300323
 56483/100000: episode: 1368, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -13.341, mean reward: -0.133 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.302, 10.226], loss: 0.002622, mae: 0.051995, mean_q: -0.294976
 56583/100000: episode: 1369, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.270, mean reward: -0.143 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.264, 10.098], loss: 0.002594, mae: 0.050652, mean_q: -0.323819
 56683/100000: episode: 1370, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.806, mean reward: -0.188 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.773, 10.245], loss: 0.002598, mae: 0.051193, mean_q: -0.286430
 56783/100000: episode: 1371, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.669, mean reward: -0.147 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.752, 10.285], loss: 0.002872, mae: 0.053179, mean_q: -0.316731
 56883/100000: episode: 1372, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -11.061, mean reward: -0.111 [-1.000, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.730, 10.463], loss: 0.002601, mae: 0.050449, mean_q: -0.334374
 56983/100000: episode: 1373, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.888, mean reward: -0.199 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.313, 10.098], loss: 0.002825, mae: 0.053886, mean_q: -0.338977
 57083/100000: episode: 1374, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.212, mean reward: -0.182 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.426, 10.098], loss: 0.002907, mae: 0.054996, mean_q: -0.314941
 57183/100000: episode: 1375, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -14.093, mean reward: -0.141 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.485, 10.378], loss: 0.002617, mae: 0.050927, mean_q: -0.315304
 57283/100000: episode: 1376, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.586, mean reward: -0.176 [-1.000, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.425, 10.098], loss: 0.002595, mae: 0.050438, mean_q: -0.329007
 57383/100000: episode: 1377, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.857, mean reward: -0.199 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.998, 10.105], loss: 0.002450, mae: 0.049047, mean_q: -0.312432
 57483/100000: episode: 1378, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.447, mean reward: -0.154 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.514, 10.172], loss: 0.002624, mae: 0.050569, mean_q: -0.323361
 57583/100000: episode: 1379, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -14.446, mean reward: -0.144 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.826, 10.251], loss: 0.003345, mae: 0.059871, mean_q: -0.314360
 57683/100000: episode: 1380, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.850, mean reward: -0.199 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.695, 10.098], loss: 0.002869, mae: 0.052914, mean_q: -0.289963
 57783/100000: episode: 1381, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -20.267, mean reward: -0.203 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.045, 10.098], loss: 0.002632, mae: 0.051007, mean_q: -0.308737
 57883/100000: episode: 1382, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -17.359, mean reward: -0.174 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.867, 10.137], loss: 0.002498, mae: 0.048882, mean_q: -0.329207
 57983/100000: episode: 1383, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.246, mean reward: -0.192 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.664, 10.098], loss: 0.002901, mae: 0.054460, mean_q: -0.289338
 58083/100000: episode: 1384, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -22.292, mean reward: -0.223 [-1.000, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.067, 10.125], loss: 0.002833, mae: 0.053121, mean_q: -0.336057
 58183/100000: episode: 1385, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.403, mean reward: -0.184 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.245, 10.185], loss: 0.002684, mae: 0.051118, mean_q: -0.324823
 58283/100000: episode: 1386, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -10.421, mean reward: -0.104 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.701, 10.362], loss: 0.002617, mae: 0.050782, mean_q: -0.313052
 58383/100000: episode: 1387, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -12.260, mean reward: -0.123 [-1.000, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.011, 10.348], loss: 0.002700, mae: 0.051419, mean_q: -0.328013
 58483/100000: episode: 1388, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.811, mean reward: -0.148 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.098, 10.497], loss: 0.002556, mae: 0.050543, mean_q: -0.277601
 58583/100000: episode: 1389, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -12.931, mean reward: -0.129 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.887, 10.098], loss: 0.002661, mae: 0.051350, mean_q: -0.290351
 58683/100000: episode: 1390, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -14.315, mean reward: -0.143 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.908, 10.098], loss: 0.002530, mae: 0.049532, mean_q: -0.325191
 58783/100000: episode: 1391, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.699, mean reward: -0.197 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.844, 10.126], loss: 0.002598, mae: 0.050560, mean_q: -0.289168
 58883/100000: episode: 1392, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.925, mean reward: -0.179 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.719, 10.249], loss: 0.002638, mae: 0.050799, mean_q: -0.322173
 58983/100000: episode: 1393, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.455, mean reward: -0.165 [-1.000, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.916, 10.098], loss: 0.002564, mae: 0.050414, mean_q: -0.313117
 59083/100000: episode: 1394, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -15.126, mean reward: -0.151 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.968, 10.098], loss: 0.002589, mae: 0.050433, mean_q: -0.272534
 59183/100000: episode: 1395, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.514, mean reward: -0.175 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.276, 10.262], loss: 0.002422, mae: 0.048556, mean_q: -0.330190
 59283/100000: episode: 1396, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.267, mean reward: -0.163 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.037, 10.381], loss: 0.002512, mae: 0.049198, mean_q: -0.333957
 59383/100000: episode: 1397, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.605, mean reward: -0.166 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.737, 10.158], loss: 0.002520, mae: 0.049842, mean_q: -0.300303
 59483/100000: episode: 1398, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.380, mean reward: -0.194 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.620, 10.184], loss: 0.003133, mae: 0.057609, mean_q: -0.322597
 59583/100000: episode: 1399, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.637, mean reward: -0.186 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.207, 10.349], loss: 0.002518, mae: 0.049565, mean_q: -0.317852
 59683/100000: episode: 1400, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.269, mean reward: -0.173 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.062, 10.098], loss: 0.002492, mae: 0.049952, mean_q: -0.285782
 59783/100000: episode: 1401, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.455, mean reward: -0.175 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.760, 10.098], loss: 0.002271, mae: 0.047895, mean_q: -0.300568
 59883/100000: episode: 1402, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.323, mean reward: -0.193 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.802, 10.098], loss: 0.002559, mae: 0.049907, mean_q: -0.309300
 59983/100000: episode: 1403, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.623, mean reward: -0.186 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.570, 10.145], loss: 0.002494, mae: 0.050130, mean_q: -0.315000
 60083/100000: episode: 1404, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.003, mean reward: -0.140 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.249, 10.203], loss: 0.002325, mae: 0.047889, mean_q: -0.285981
 60183/100000: episode: 1405, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.742, mean reward: -0.167 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.799, 10.168], loss: 0.002556, mae: 0.050079, mean_q: -0.328803
 60283/100000: episode: 1406, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.369, mean reward: -0.184 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.939, 10.279], loss: 0.002354, mae: 0.048053, mean_q: -0.335590
 60383/100000: episode: 1407, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.294, mean reward: -0.183 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.469, 10.098], loss: 0.002304, mae: 0.047090, mean_q: -0.334838
 60483/100000: episode: 1408, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -20.136, mean reward: -0.201 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.131, 10.180], loss: 0.002308, mae: 0.048002, mean_q: -0.317473
 60583/100000: episode: 1409, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.375, mean reward: -0.154 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.028, 10.318], loss: 0.002453, mae: 0.048054, mean_q: -0.328956
[Info] 100-TH LEVEL FOUND: 0.6584950089454651, Considering 10/90 traces
 60683/100000: episode: 1410, duration: 4.322s, episode steps: 100, steps per second: 23, episode reward: -19.879, mean reward: -0.199 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.524, 10.134], loss: 0.002341, mae: 0.048930, mean_q: -0.284301
 60692/100000: episode: 1411, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 3.523, mean reward: 0.391 [0.315, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.527], loss: 0.002531, mae: 0.051010, mean_q: -0.272967
 60725/100000: episode: 1412, duration: 0.177s, episode steps: 33, steps per second: 187, episode reward: 9.827, mean reward: 0.298 [0.168, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.373, 10.100], loss: 0.002568, mae: 0.048470, mean_q: -0.352190
 60733/100000: episode: 1413, duration: 0.050s, episode steps: 8, steps per second: 158, episode reward: 2.720, mean reward: 0.340 [0.257, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.400], loss: 0.002094, mae: 0.047252, mean_q: -0.166925
 60742/100000: episode: 1414, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 3.175, mean reward: 0.353 [0.290, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.524, 10.398], loss: 0.002205, mae: 0.047596, mean_q: -0.298141
 60765/100000: episode: 1415, duration: 0.113s, episode steps: 23, steps per second: 204, episode reward: 8.913, mean reward: 0.388 [0.229, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.627, 10.681], loss: 0.002404, mae: 0.047431, mean_q: -0.317108
 60798/100000: episode: 1416, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 9.598, mean reward: 0.291 [0.153, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.349, 10.100], loss: 0.004376, mae: 0.057587, mean_q: -0.319741
 60831/100000: episode: 1417, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 8.994, mean reward: 0.273 [0.097, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.484, 10.100], loss: 0.008761, mae: 0.080661, mean_q: -0.329949
 60864/100000: episode: 1418, duration: 0.192s, episode steps: 33, steps per second: 172, episode reward: 10.093, mean reward: 0.306 [0.096, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.588, 10.100], loss: 0.003449, mae: 0.060083, mean_q: -0.284287
 60887/100000: episode: 1419, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 6.271, mean reward: 0.273 [0.211, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.883, 10.401], loss: 0.002211, mae: 0.048232, mean_q: -0.239912
 60904/100000: episode: 1420, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 5.428, mean reward: 0.319 [0.205, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.245, 10.100], loss: 0.002861, mae: 0.052692, mean_q: -0.284098
 60921/100000: episode: 1421, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 7.060, mean reward: 0.415 [0.276, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.870, 10.420], loss: 0.002271, mae: 0.048609, mean_q: -0.257029
 60954/100000: episode: 1422, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 10.378, mean reward: 0.314 [0.089, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.717, 10.100], loss: 0.002422, mae: 0.050720, mean_q: -0.237899
 60977/100000: episode: 1423, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 7.618, mean reward: 0.331 [0.197, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.584, 10.410], loss: 0.002537, mae: 0.049124, mean_q: -0.260914
 61010/100000: episode: 1424, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 9.973, mean reward: 0.302 [0.164, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.580, 10.100], loss: 0.002299, mae: 0.047184, mean_q: -0.339814
 61039/100000: episode: 1425, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 7.917, mean reward: 0.273 [0.178, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-1.618, 10.365], loss: 0.002471, mae: 0.049225, mean_q: -0.275331
 61056/100000: episode: 1426, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 9.519, mean reward: 0.560 [0.485, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.656], loss: 0.002481, mae: 0.050717, mean_q: -0.153948
 61113/100000: episode: 1427, duration: 0.308s, episode steps: 57, steps per second: 185, episode reward: 15.564, mean reward: 0.273 [0.071, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.333, 10.100], loss: 0.002640, mae: 0.050983, mean_q: -0.245870
 61136/100000: episode: 1428, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 4.929, mean reward: 0.214 [0.109, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.389], loss: 0.002796, mae: 0.053364, mean_q: -0.217296
 61193/100000: episode: 1429, duration: 0.286s, episode steps: 57, steps per second: 199, episode reward: 9.989, mean reward: 0.175 [0.022, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.670, 10.106], loss: 0.002402, mae: 0.050217, mean_q: -0.194891
 61202/100000: episode: 1430, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 4.245, mean reward: 0.472 [0.311, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-1.304, 10.523], loss: 0.002608, mae: 0.047446, mean_q: -0.270635
 61211/100000: episode: 1431, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 3.800, mean reward: 0.422 [0.295, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.403, 10.517], loss: 0.002393, mae: 0.050035, mean_q: -0.175703
 61227/100000: episode: 1432, duration: 0.091s, episode steps: 16, steps per second: 177, episode reward: 5.909, mean reward: 0.369 [0.269, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.336, 10.460], loss: 0.002656, mae: 0.053153, mean_q: -0.192588
 61256/100000: episode: 1433, duration: 0.161s, episode steps: 29, steps per second: 181, episode reward: 5.890, mean reward: 0.203 [0.046, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.205, 10.100], loss: 0.002415, mae: 0.048992, mean_q: -0.229955
 61273/100000: episode: 1434, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 7.108, mean reward: 0.418 [0.368, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.841, 10.566], loss: 0.002638, mae: 0.052474, mean_q: -0.223380
 61289/100000: episode: 1435, duration: 0.081s, episode steps: 16, steps per second: 196, episode reward: 5.626, mean reward: 0.352 [0.250, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.628, 10.269], loss: 0.003031, mae: 0.054244, mean_q: -0.192707
 61318/100000: episode: 1436, duration: 0.168s, episode steps: 29, steps per second: 172, episode reward: 7.533, mean reward: 0.260 [0.067, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-1.028, 10.202], loss: 0.002912, mae: 0.055049, mean_q: -0.179528
 61352/100000: episode: 1437, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 13.374, mean reward: 0.393 [0.268, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.851, 10.100], loss: 0.002631, mae: 0.051575, mean_q: -0.206251
 61386/100000: episode: 1438, duration: 0.196s, episode steps: 34, steps per second: 173, episode reward: 12.397, mean reward: 0.365 [0.258, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.791, 10.100], loss: 0.002597, mae: 0.051053, mean_q: -0.229054
 61443/100000: episode: 1439, duration: 0.313s, episode steps: 57, steps per second: 182, episode reward: 20.065, mean reward: 0.352 [0.134, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.848 [-0.600, 10.100], loss: 0.002589, mae: 0.050958, mean_q: -0.187077
 61460/100000: episode: 1440, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 5.026, mean reward: 0.296 [0.160, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.042, 10.100], loss: 0.002664, mae: 0.052735, mean_q: -0.171381
 61477/100000: episode: 1441, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 5.845, mean reward: 0.344 [0.269, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.262, 10.100], loss: 0.002512, mae: 0.047926, mean_q: -0.236149
 61494/100000: episode: 1442, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 7.957, mean reward: 0.468 [0.356, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.144, 10.539], loss: 0.002040, mae: 0.047478, mean_q: -0.182158
 61511/100000: episode: 1443, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 5.592, mean reward: 0.329 [0.224, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.333, 10.100], loss: 0.002471, mae: 0.050987, mean_q: -0.138988
 61520/100000: episode: 1444, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 3.085, mean reward: 0.343 [0.286, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.311], loss: 0.003329, mae: 0.059105, mean_q: -0.129735
 61549/100000: episode: 1445, duration: 0.166s, episode steps: 29, steps per second: 174, episode reward: 10.047, mean reward: 0.346 [0.229, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.256, 10.398], loss: 0.003231, mae: 0.058413, mean_q: -0.221209
 61566/100000: episode: 1446, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 5.407, mean reward: 0.318 [0.199, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.045, 10.100], loss: 0.002757, mae: 0.055829, mean_q: -0.156971
 61600/100000: episode: 1447, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 8.421, mean reward: 0.248 [0.096, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-1.005, 10.100], loss: 0.002602, mae: 0.050912, mean_q: -0.140340
 61657/100000: episode: 1448, duration: 0.313s, episode steps: 57, steps per second: 182, episode reward: 17.577, mean reward: 0.308 [0.145, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.834 [-0.606, 10.100], loss: 0.002847, mae: 0.052722, mean_q: -0.160804
 61714/100000: episode: 1449, duration: 0.290s, episode steps: 57, steps per second: 196, episode reward: 16.652, mean reward: 0.292 [0.151, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-0.139, 10.100], loss: 0.002601, mae: 0.051740, mean_q: -0.120787
 61731/100000: episode: 1450, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 4.747, mean reward: 0.279 [0.134, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.047, 10.100], loss: 0.002383, mae: 0.048752, mean_q: -0.131834
 61754/100000: episode: 1451, duration: 0.133s, episode steps: 23, steps per second: 174, episode reward: 6.340, mean reward: 0.276 [0.089, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.669, 10.251], loss: 0.002279, mae: 0.048193, mean_q: -0.132546
 61771/100000: episode: 1452, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 6.169, mean reward: 0.363 [0.259, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.460], loss: 0.002456, mae: 0.050131, mean_q: -0.108167
 61794/100000: episode: 1453, duration: 0.138s, episode steps: 23, steps per second: 166, episode reward: 10.555, mean reward: 0.459 [0.248, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.670, 10.560], loss: 0.002429, mae: 0.049894, mean_q: -0.115181
 61851/100000: episode: 1454, duration: 0.298s, episode steps: 57, steps per second: 191, episode reward: 11.820, mean reward: 0.207 [0.052, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.736, 10.315], loss: 0.002622, mae: 0.052252, mean_q: -0.096711
 61908/100000: episode: 1455, duration: 0.320s, episode steps: 57, steps per second: 178, episode reward: 20.759, mean reward: 0.364 [0.142, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-0.439, 10.100], loss: 0.002862, mae: 0.054498, mean_q: -0.061048
 61925/100000: episode: 1456, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 6.433, mean reward: 0.378 [0.292, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.262, 10.477], loss: 0.002707, mae: 0.052857, mean_q: -0.036943
 61948/100000: episode: 1457, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 7.760, mean reward: 0.337 [0.237, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.400], loss: 0.002648, mae: 0.052871, mean_q: -0.021611
 61957/100000: episode: 1458, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 3.719, mean reward: 0.413 [0.398, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.041, 10.448], loss: 0.002635, mae: 0.053663, mean_q: 0.002121
 61986/100000: episode: 1459, duration: 0.166s, episode steps: 29, steps per second: 175, episode reward: 13.242, mean reward: 0.457 [0.372, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.653, 10.524], loss: 0.002735, mae: 0.053641, mean_q: -0.094001
 62020/100000: episode: 1460, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 13.514, mean reward: 0.397 [0.285, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.308, 10.100], loss: 0.002485, mae: 0.050800, mean_q: -0.115620
 62077/100000: episode: 1461, duration: 0.290s, episode steps: 57, steps per second: 197, episode reward: 22.637, mean reward: 0.397 [0.271, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.841 [-0.795, 10.100], loss: 0.002843, mae: 0.055055, mean_q: -0.052380
 62094/100000: episode: 1462, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 6.949, mean reward: 0.409 [0.343, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.289, 10.463], loss: 0.002391, mae: 0.049921, mean_q: -0.035237
 62117/100000: episode: 1463, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 6.549, mean reward: 0.285 [0.213, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.420], loss: 0.002648, mae: 0.051787, mean_q: -0.070988
 62134/100000: episode: 1464, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 6.201, mean reward: 0.365 [0.278, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.169, 10.100], loss: 0.002670, mae: 0.051601, mean_q: -0.027542
 62167/100000: episode: 1465, duration: 0.185s, episode steps: 33, steps per second: 179, episode reward: 8.005, mean reward: 0.243 [0.069, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.912, 10.130], loss: 0.004313, mae: 0.061214, mean_q: -0.011858
 62184/100000: episode: 1466, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 5.061, mean reward: 0.298 [0.200, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.546, 10.100], loss: 0.009003, mae: 0.078610, mean_q: -0.017062
 62193/100000: episode: 1467, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 2.957, mean reward: 0.329 [0.261, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.792, 10.425], loss: 0.008534, mae: 0.089094, mean_q: 0.055096
 62210/100000: episode: 1468, duration: 0.105s, episode steps: 17, steps per second: 163, episode reward: 7.350, mean reward: 0.432 [0.340, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.561], loss: 0.005371, mae: 0.070596, mean_q: -0.057046
 62226/100000: episode: 1469, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 6.170, mean reward: 0.386 [0.238, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.505], loss: 0.007765, mae: 0.078335, mean_q: 0.024662
 62243/100000: episode: 1470, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 6.794, mean reward: 0.400 [0.331, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.500, 10.100], loss: 0.007199, mae: 0.080409, mean_q: 0.068461
 62260/100000: episode: 1471, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 6.495, mean reward: 0.382 [0.292, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.887, 10.559], loss: 0.006968, mae: 0.085068, mean_q: -0.006005
 62277/100000: episode: 1472, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 7.216, mean reward: 0.424 [0.349, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.552], loss: 0.003228, mae: 0.062729, mean_q: -0.047595
 62306/100000: episode: 1473, duration: 0.147s, episode steps: 29, steps per second: 198, episode reward: 9.094, mean reward: 0.314 [0.211, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.649, 10.392], loss: 0.003706, mae: 0.062709, mean_q: -0.055089
 62314/100000: episode: 1474, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 3.140, mean reward: 0.392 [0.354, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.466], loss: 0.003117, mae: 0.062429, mean_q: -0.046079
 62371/100000: episode: 1475, duration: 0.326s, episode steps: 57, steps per second: 175, episode reward: 14.021, mean reward: 0.246 [0.057, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.513, 10.308], loss: 0.002908, mae: 0.056004, mean_q: -0.005948
 62380/100000: episode: 1476, duration: 0.059s, episode steps: 9, steps per second: 154, episode reward: 3.470, mean reward: 0.386 [0.317, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.457], loss: 0.002352, mae: 0.050621, mean_q: 0.064542
 62396/100000: episode: 1477, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 6.595, mean reward: 0.412 [0.313, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.538, 10.395], loss: 0.002420, mae: 0.053359, mean_q: -0.003080
 62419/100000: episode: 1478, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 8.681, mean reward: 0.377 [0.221, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.468], loss: 0.002569, mae: 0.053042, mean_q: -0.020016
 62476/100000: episode: 1479, duration: 0.291s, episode steps: 57, steps per second: 196, episode reward: 24.295, mean reward: 0.426 [0.252, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.834 [-0.278, 10.100], loss: 0.002743, mae: 0.055296, mean_q: 0.028262
 62493/100000: episode: 1480, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 6.139, mean reward: 0.361 [0.304, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.245, 10.100], loss: 0.002416, mae: 0.051472, mean_q: 0.061180
 62509/100000: episode: 1481, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 5.409, mean reward: 0.338 [0.266, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.236, 10.389], loss: 0.002869, mae: 0.056969, mean_q: 0.053514
 62518/100000: episode: 1482, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 3.333, mean reward: 0.370 [0.298, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.416], loss: 0.002533, mae: 0.052813, mean_q: 0.095398
 62535/100000: episode: 1483, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 5.336, mean reward: 0.314 [0.220, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.102, 10.100], loss: 0.002711, mae: 0.054522, mean_q: 0.029655
 62592/100000: episode: 1484, duration: 0.296s, episode steps: 57, steps per second: 193, episode reward: 22.924, mean reward: 0.402 [0.256, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.835 [-0.918, 10.100], loss: 0.002688, mae: 0.054500, mean_q: 0.092650
 62615/100000: episode: 1485, duration: 0.142s, episode steps: 23, steps per second: 162, episode reward: 7.417, mean reward: 0.322 [0.182, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.409], loss: 0.002633, mae: 0.052682, mean_q: 0.038901
 62672/100000: episode: 1486, duration: 0.295s, episode steps: 57, steps per second: 193, episode reward: 15.843, mean reward: 0.278 [0.100, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-0.323, 10.100], loss: 0.002902, mae: 0.056784, mean_q: 0.098409
 62701/100000: episode: 1487, duration: 0.144s, episode steps: 29, steps per second: 202, episode reward: 8.447, mean reward: 0.291 [0.110, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.283, 10.356], loss: 0.002542, mae: 0.051483, mean_q: 0.026194
 62710/100000: episode: 1488, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 3.104, mean reward: 0.345 [0.294, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.380], loss: 0.002566, mae: 0.054137, mean_q: 0.051001
 62744/100000: episode: 1489, duration: 0.194s, episode steps: 34, steps per second: 175, episode reward: 12.682, mean reward: 0.373 [0.226, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-1.140, 10.100], loss: 0.002374, mae: 0.051090, mean_q: -0.001793
 62778/100000: episode: 1490, duration: 0.203s, episode steps: 34, steps per second: 167, episode reward: 9.698, mean reward: 0.285 [0.170, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.989, 10.100], loss: 0.002539, mae: 0.054238, mean_q: 0.136195
 62795/100000: episode: 1491, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 6.005, mean reward: 0.353 [0.259, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.414, 10.437], loss: 0.002385, mae: 0.051124, mean_q: 0.098045
 62811/100000: episode: 1492, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 3.877, mean reward: 0.242 [0.125, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.908, 10.249], loss: 0.002877, mae: 0.056780, mean_q: 0.183923
 62834/100000: episode: 1493, duration: 0.125s, episode steps: 23, steps per second: 183, episode reward: 7.930, mean reward: 0.345 [0.231, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.893, 10.516], loss: 0.003084, mae: 0.059602, mean_q: 0.136028
 62863/100000: episode: 1494, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 6.764, mean reward: 0.233 [0.139, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.648, 10.356], loss: 0.002558, mae: 0.053276, mean_q: 0.144317
 62896/100000: episode: 1495, duration: 0.183s, episode steps: 33, steps per second: 181, episode reward: 11.277, mean reward: 0.342 [0.161, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.580, 10.100], loss: 0.002690, mae: 0.052976, mean_q: 0.131582
 62919/100000: episode: 1496, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 10.293, mean reward: 0.448 [0.215, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.718], loss: 0.002432, mae: 0.052282, mean_q: 0.125685
 62953/100000: episode: 1497, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 12.289, mean reward: 0.361 [0.267, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.745, 10.100], loss: 0.002549, mae: 0.053588, mean_q: 0.135778
 62986/100000: episode: 1498, duration: 0.182s, episode steps: 33, steps per second: 182, episode reward: 12.573, mean reward: 0.381 [0.140, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-1.004, 10.100], loss: 0.002688, mae: 0.054353, mean_q: 0.147463
 63003/100000: episode: 1499, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 5.667, mean reward: 0.333 [0.275, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.733, 10.100], loss: 0.002525, mae: 0.052688, mean_q: 0.192625
[Info] 200-TH LEVEL FOUND: 0.922304630279541, Considering 10/90 traces
 63060/100000: episode: 1500, duration: 4.170s, episode steps: 57, steps per second: 14, episode reward: 16.899, mean reward: 0.296 [0.017, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-0.692, 10.131], loss: 0.002596, mae: 0.053029, mean_q: 0.137740
 63074/100000: episode: 1501, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 7.195, mean reward: 0.514 [0.467, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.582], loss: 0.002929, mae: 0.056577, mean_q: 0.164130
 63115/100000: episode: 1502, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 10.760, mean reward: 0.262 [0.033, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.165, 10.149], loss: 0.002465, mae: 0.051481, mean_q: 0.132967
 63127/100000: episode: 1503, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 6.301, mean reward: 0.525 [0.478, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.926, 10.551], loss: 0.002445, mae: 0.051900, mean_q: 0.157210
 63168/100000: episode: 1504, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 18.575, mean reward: 0.453 [0.320, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.487, 10.100], loss: 0.002743, mae: 0.055075, mean_q: 0.213784
 63183/100000: episode: 1505, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 6.126, mean reward: 0.408 [0.317, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.511], loss: 0.003006, mae: 0.057509, mean_q: 0.205935
 63225/100000: episode: 1506, duration: 0.231s, episode steps: 42, steps per second: 181, episode reward: 16.327, mean reward: 0.389 [0.264, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-1.249, 10.100], loss: 0.003352, mae: 0.061006, mean_q: 0.193562
 63236/100000: episode: 1507, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 6.192, mean reward: 0.563 [0.524, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.683], loss: 0.002702, mae: 0.056359, mean_q: 0.186222
 63260/100000: episode: 1508, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 7.868, mean reward: 0.328 [0.242, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.305, 10.100], loss: 0.002512, mae: 0.052160, mean_q: 0.166525
 63287/100000: episode: 1509, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 7.945, mean reward: 0.294 [0.135, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.235, 10.100], loss: 0.002745, mae: 0.054792, mean_q: 0.160650
 63301/100000: episode: 1510, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 6.055, mean reward: 0.432 [0.358, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.470], loss: 0.002841, mae: 0.053967, mean_q: 0.149549
 63313/100000: episode: 1511, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 6.237, mean reward: 0.520 [0.462, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.561], loss: 0.002194, mae: 0.050624, mean_q: 0.254806
 63340/100000: episode: 1512, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 12.096, mean reward: 0.448 [0.312, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-1.684, 10.100], loss: 0.002449, mae: 0.051817, mean_q: 0.172945
 63354/100000: episode: 1513, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 6.210, mean reward: 0.444 [0.286, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.543], loss: 0.002495, mae: 0.053564, mean_q: 0.223335
 63395/100000: episode: 1514, duration: 0.209s, episode steps: 41, steps per second: 196, episode reward: 11.718, mean reward: 0.286 [0.086, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.123, 10.100], loss: 0.002747, mae: 0.056628, mean_q: 0.245927
 63419/100000: episode: 1515, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 10.416, mean reward: 0.434 [0.312, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.432, 10.100], loss: 0.002548, mae: 0.053943, mean_q: 0.238967
 63446/100000: episode: 1516, duration: 0.164s, episode steps: 27, steps per second: 165, episode reward: 11.390, mean reward: 0.422 [0.318, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.474, 10.100], loss: 0.002606, mae: 0.054418, mean_q: 0.213574
 63460/100000: episode: 1517, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 5.329, mean reward: 0.381 [0.314, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.481, 10.376], loss: 0.003124, mae: 0.058229, mean_q: 0.192674
 63474/100000: episode: 1518, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 6.095, mean reward: 0.435 [0.391, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.389, 10.489], loss: 0.002353, mae: 0.052353, mean_q: 0.259541
 63485/100000: episode: 1519, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 4.889, mean reward: 0.444 [0.334, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.538], loss: 0.002871, mae: 0.055895, mean_q: 0.321181
 63496/100000: episode: 1520, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 6.107, mean reward: 0.555 [0.496, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.605], loss: 0.002526, mae: 0.051380, mean_q: 0.199747
 63508/100000: episode: 1521, duration: 0.059s, episode steps: 12, steps per second: 203, episode reward: 5.465, mean reward: 0.455 [0.408, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.525], loss: 0.002603, mae: 0.053949, mean_q: 0.219253
 63523/100000: episode: 1522, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 8.696, mean reward: 0.580 [0.482, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.571], loss: 0.002247, mae: 0.050078, mean_q: 0.241377
 63535/100000: episode: 1523, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 5.831, mean reward: 0.486 [0.406, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.528], loss: 0.002364, mae: 0.052251, mean_q: 0.251089
 63559/100000: episode: 1524, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 10.541, mean reward: 0.439 [0.330, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.362, 10.100], loss: 0.002534, mae: 0.053694, mean_q: 0.232480
 63609/100000: episode: 1525, duration: 0.286s, episode steps: 50, steps per second: 175, episode reward: 13.784, mean reward: 0.276 [0.099, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.087, 10.100], loss: 0.002528, mae: 0.053093, mean_q: 0.222648
 63650/100000: episode: 1526, duration: 0.213s, episode steps: 41, steps per second: 193, episode reward: 10.513, mean reward: 0.256 [0.093, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.604, 10.100], loss: 0.002662, mae: 0.055229, mean_q: 0.229347
 63662/100000: episode: 1527, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 5.173, mean reward: 0.431 [0.342, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.538], loss: 0.002437, mae: 0.053532, mean_q: 0.276258
 63704/100000: episode: 1528, duration: 0.211s, episode steps: 42, steps per second: 199, episode reward: 22.359, mean reward: 0.532 [0.398, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.240, 10.100], loss: 0.002778, mae: 0.055029, mean_q: 0.277472
 63718/100000: episode: 1529, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 7.615, mean reward: 0.544 [0.438, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.670], loss: 0.002979, mae: 0.058901, mean_q: 0.328125
 63729/100000: episode: 1530, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 5.605, mean reward: 0.510 [0.413, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.621, 10.618], loss: 0.002548, mae: 0.053485, mean_q: 0.370450
 63770/100000: episode: 1531, duration: 0.213s, episode steps: 41, steps per second: 193, episode reward: 12.074, mean reward: 0.294 [0.024, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.624, 10.125], loss: 0.002907, mae: 0.057609, mean_q: 0.320195
 63811/100000: episode: 1532, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 15.588, mean reward: 0.380 [0.131, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.635, 10.100], loss: 0.002965, mae: 0.059041, mean_q: 0.289782
 63823/100000: episode: 1533, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 5.396, mean reward: 0.450 [0.377, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.520], loss: 0.003047, mae: 0.058064, mean_q: 0.304422
 63837/100000: episode: 1534, duration: 0.091s, episode steps: 14, steps per second: 154, episode reward: 7.777, mean reward: 0.556 [0.436, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.206, 10.569], loss: 0.002444, mae: 0.054251, mean_q: 0.288783
 63851/100000: episode: 1535, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 7.042, mean reward: 0.503 [0.447, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.542], loss: 0.002654, mae: 0.056094, mean_q: 0.235364
 63865/100000: episode: 1536, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 6.177, mean reward: 0.441 [0.270, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.540], loss: 0.002439, mae: 0.054955, mean_q: 0.356522
 63892/100000: episode: 1537, duration: 0.129s, episode steps: 27, steps per second: 210, episode reward: 10.241, mean reward: 0.379 [0.258, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.531, 10.100], loss: 0.002511, mae: 0.054359, mean_q: 0.363282
 63916/100000: episode: 1538, duration: 0.118s, episode steps: 24, steps per second: 204, episode reward: 11.456, mean reward: 0.477 [0.328, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.172, 10.100], loss: 0.002429, mae: 0.053664, mean_q: 0.311794
 63930/100000: episode: 1539, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 6.679, mean reward: 0.477 [0.401, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.358, 10.486], loss: 0.002867, mae: 0.059866, mean_q: 0.347353
 63942/100000: episode: 1540, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 5.587, mean reward: 0.466 [0.425, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.437], loss: 0.002155, mae: 0.049759, mean_q: 0.368666
 63966/100000: episode: 1541, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 11.129, mean reward: 0.464 [0.376, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.364, 10.100], loss: 0.002583, mae: 0.054092, mean_q: 0.377031
 64016/100000: episode: 1542, duration: 0.262s, episode steps: 50, steps per second: 191, episode reward: 16.713, mean reward: 0.334 [0.018, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.150, 10.114], loss: 0.002956, mae: 0.057502, mean_q: 0.362193
 64030/100000: episode: 1543, duration: 0.078s, episode steps: 14, steps per second: 178, episode reward: 7.010, mean reward: 0.501 [0.366, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.578], loss: 0.002658, mae: 0.054008, mean_q: 0.351511
 64071/100000: episode: 1544, duration: 0.231s, episode steps: 41, steps per second: 177, episode reward: 16.902, mean reward: 0.412 [0.290, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.764, 10.100], loss: 0.002798, mae: 0.057506, mean_q: 0.394002
 64082/100000: episode: 1545, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 5.110, mean reward: 0.465 [0.315, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.488], loss: 0.002461, mae: 0.053697, mean_q: 0.306050
 64132/100000: episode: 1546, duration: 0.276s, episode steps: 50, steps per second: 181, episode reward: 18.036, mean reward: 0.361 [0.111, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.348, 10.100], loss: 0.003081, mae: 0.060202, mean_q: 0.376702
 64147/100000: episode: 1547, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 8.401, mean reward: 0.560 [0.424, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.457, 10.377], loss: 0.002480, mae: 0.052612, mean_q: 0.377983
 64162/100000: episode: 1548, duration: 0.078s, episode steps: 15, steps per second: 191, episode reward: 7.215, mean reward: 0.481 [0.407, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.432, 10.566], loss: 0.002210, mae: 0.051272, mean_q: 0.404187
 64189/100000: episode: 1549, duration: 0.157s, episode steps: 27, steps per second: 171, episode reward: 9.362, mean reward: 0.347 [0.241, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.339, 10.100], loss: 0.002507, mae: 0.053809, mean_q: 0.370016
 64201/100000: episode: 1550, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 5.565, mean reward: 0.464 [0.368, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.592], loss: 0.002411, mae: 0.051737, mean_q: 0.373669
 64215/100000: episode: 1551, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 6.916, mean reward: 0.494 [0.448, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.611], loss: 0.003106, mae: 0.061145, mean_q: 0.408483
 64227/100000: episode: 1552, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 6.388, mean reward: 0.532 [0.419, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.258, 10.759], loss: 0.002624, mae: 0.055910, mean_q: 0.453679
 64268/100000: episode: 1553, duration: 0.214s, episode steps: 41, steps per second: 191, episode reward: 11.852, mean reward: 0.289 [0.081, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.359, 10.100], loss: 0.002859, mae: 0.057984, mean_q: 0.424253
 64318/100000: episode: 1554, duration: 0.270s, episode steps: 50, steps per second: 185, episode reward: 22.597, mean reward: 0.452 [0.249, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.448, 10.100], loss: 0.002870, mae: 0.058631, mean_q: 0.421974
 64329/100000: episode: 1555, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 6.095, mean reward: 0.554 [0.504, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.408, 10.626], loss: 0.002780, mae: 0.056111, mean_q: 0.397465
 64371/100000: episode: 1556, duration: 0.230s, episode steps: 42, steps per second: 183, episode reward: 16.867, mean reward: 0.402 [0.292, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-1.150, 10.100], loss: 0.003020, mae: 0.059706, mean_q: 0.437466
 64385/100000: episode: 1557, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 5.295, mean reward: 0.378 [0.311, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.129, 10.502], loss: 0.002413, mae: 0.053766, mean_q: 0.449838
 64396/100000: episode: 1558, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 4.038, mean reward: 0.367 [0.230, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.396], loss: 0.002694, mae: 0.055453, mean_q: 0.446974
 64410/100000: episode: 1559, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 7.033, mean reward: 0.502 [0.469, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.555], loss: 0.002620, mae: 0.055311, mean_q: 0.390421
 64424/100000: episode: 1560, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 6.975, mean reward: 0.498 [0.414, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.567, 10.517], loss: 0.002301, mae: 0.053581, mean_q: 0.429052
 64438/100000: episode: 1561, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 4.867, mean reward: 0.348 [0.194, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.344], loss: 0.002470, mae: 0.055038, mean_q: 0.523084
 64449/100000: episode: 1562, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 5.886, mean reward: 0.535 [0.434, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.534], loss: 0.002632, mae: 0.056563, mean_q: 0.430253
 64461/100000: episode: 1563, duration: 0.082s, episode steps: 12, steps per second: 147, episode reward: 5.250, mean reward: 0.438 [0.366, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.532], loss: 0.002591, mae: 0.054629, mean_q: 0.474167
 64502/100000: episode: 1564, duration: 0.223s, episode steps: 41, steps per second: 184, episode reward: 17.516, mean reward: 0.427 [0.338, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.670, 10.100], loss: 0.002551, mae: 0.055314, mean_q: 0.485713
 64529/100000: episode: 1565, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 8.729, mean reward: 0.323 [0.116, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.130, 10.100], loss: 0.003068, mae: 0.060685, mean_q: 0.502813
 64543/100000: episode: 1566, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 6.120, mean reward: 0.437 [0.376, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.050, 10.553], loss: 0.003665, mae: 0.065016, mean_q: 0.513745
 64593/100000: episode: 1567, duration: 0.252s, episode steps: 50, steps per second: 198, episode reward: 17.617, mean reward: 0.352 [0.028, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.679, 10.216], loss: 0.002365, mae: 0.053010, mean_q: 0.475698
 64635/100000: episode: 1568, duration: 0.236s, episode steps: 42, steps per second: 178, episode reward: 17.088, mean reward: 0.407 [0.160, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-1.068, 10.100], loss: 0.002731, mae: 0.057354, mean_q: 0.517060
 64649/100000: episode: 1569, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 6.231, mean reward: 0.445 [0.402, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.506], loss: 0.002901, mae: 0.058227, mean_q: 0.489057
 64673/100000: episode: 1570, duration: 0.122s, episode steps: 24, steps per second: 196, episode reward: 8.417, mean reward: 0.351 [0.273, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.312, 10.100], loss: 0.002837, mae: 0.058273, mean_q: 0.502802
 64684/100000: episode: 1571, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 5.726, mean reward: 0.521 [0.445, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.049, 10.565], loss: 0.002856, mae: 0.057935, mean_q: 0.539931
 64725/100000: episode: 1572, duration: 0.235s, episode steps: 41, steps per second: 175, episode reward: 15.325, mean reward: 0.374 [0.246, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.508, 10.100], loss: 0.002450, mae: 0.054875, mean_q: 0.524692
 64739/100000: episode: 1573, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 6.917, mean reward: 0.494 [0.365, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.531], loss: 0.002435, mae: 0.054634, mean_q: 0.544595
 64763/100000: episode: 1574, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 10.289, mean reward: 0.429 [0.268, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.239, 10.100], loss: 0.002651, mae: 0.057481, mean_q: 0.532554
 64777/100000: episode: 1575, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 8.264, mean reward: 0.590 [0.486, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.543, 10.576], loss: 0.002792, mae: 0.059188, mean_q: 0.549916
 64791/100000: episode: 1576, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 6.362, mean reward: 0.454 [0.392, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.453, 10.511], loss: 0.002353, mae: 0.052574, mean_q: 0.568064
 64815/100000: episode: 1577, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 8.172, mean reward: 0.341 [0.220, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.259, 10.100], loss: 0.002848, mae: 0.058709, mean_q: 0.514464
 64826/100000: episode: 1578, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 6.237, mean reward: 0.567 [0.518, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.970, 10.630], loss: 0.002635, mae: 0.057561, mean_q: 0.497745
[Info] FALSIFICATION!
 64828/100000: episode: 1579, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 10.590, mean reward: 5.295 [0.590, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-0.018, 5.420], loss: 0.002859, mae: 0.060822, mean_q: 0.637256
 64928/100000: episode: 1580, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -13.956, mean reward: -0.140 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.138, 10.207], loss: 0.016499, mae: 0.065399, mean_q: 0.540666
 65028/100000: episode: 1581, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.827, mean reward: -0.168 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.000, 10.281], loss: 0.016479, mae: 0.070039, mean_q: 0.551622
 65128/100000: episode: 1582, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.171, mean reward: -0.182 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.850, 10.098], loss: 0.016956, mae: 0.065536, mean_q: 0.565499
 65228/100000: episode: 1583, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.299, mean reward: -0.153 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.446, 10.334], loss: 0.028499, mae: 0.075633, mean_q: 0.572302
 65328/100000: episode: 1584, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.554, mean reward: -0.166 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.938, 10.098], loss: 0.002958, mae: 0.059473, mean_q: 0.552045
 65428/100000: episode: 1585, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.080, mean reward: -0.191 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.263, 10.203], loss: 0.002832, mae: 0.058385, mean_q: 0.549916
 65528/100000: episode: 1586, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -20.398, mean reward: -0.204 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.815, 10.224], loss: 0.027753, mae: 0.068382, mean_q: 0.566610
 65628/100000: episode: 1587, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -20.488, mean reward: -0.205 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.346, 10.228], loss: 0.002734, mae: 0.056707, mean_q: 0.508483
 65728/100000: episode: 1588, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.306, mean reward: -0.193 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.014, 10.221], loss: 0.003382, mae: 0.060613, mean_q: 0.528517
 65828/100000: episode: 1589, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.107, mean reward: -0.151 [-1.000, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.374, 10.098], loss: 0.002771, mae: 0.057037, mean_q: 0.475733
 65928/100000: episode: 1590, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.113, mean reward: -0.161 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.719, 10.098], loss: 0.027658, mae: 0.067371, mean_q: 0.470066
 66028/100000: episode: 1591, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -10.963, mean reward: -0.110 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.896, 10.211], loss: 0.004282, mae: 0.068344, mean_q: 0.463131
 66128/100000: episode: 1592, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.735, mean reward: -0.157 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.032, 10.312], loss: 0.015751, mae: 0.066193, mean_q: 0.449712
 66228/100000: episode: 1593, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -13.739, mean reward: -0.137 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.511, 10.098], loss: 0.004003, mae: 0.061181, mean_q: 0.427680
 66328/100000: episode: 1594, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -9.599, mean reward: -0.096 [-1.000, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.945, 10.098], loss: 0.040048, mae: 0.076217, mean_q: 0.424553
 66428/100000: episode: 1595, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -11.887, mean reward: -0.119 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.187, 10.495], loss: 0.016986, mae: 0.073529, mean_q: 0.370824
 66528/100000: episode: 1596, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.549, mean reward: -0.165 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.854, 10.240], loss: 0.015895, mae: 0.065412, mean_q: 0.369400
 66628/100000: episode: 1597, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -17.482, mean reward: -0.175 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.746, 10.098], loss: 0.002645, mae: 0.055134, mean_q: 0.363794
 66728/100000: episode: 1598, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.134, mean reward: -0.191 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.947, 10.186], loss: 0.002917, mae: 0.057633, mean_q: 0.366508
 66828/100000: episode: 1599, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.549, mean reward: -0.195 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.341, 10.098], loss: 0.015199, mae: 0.062366, mean_q: 0.319525
 66928/100000: episode: 1600, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.800, mean reward: -0.148 [-1.000, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.709, 10.098], loss: 0.002831, mae: 0.056092, mean_q: 0.291605
 67028/100000: episode: 1601, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.580, mean reward: -0.196 [-1.000, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.162, 10.129], loss: 0.015208, mae: 0.062765, mean_q: 0.252959
 67128/100000: episode: 1602, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.059, mean reward: -0.141 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.684, 10.325], loss: 0.014948, mae: 0.058564, mean_q: 0.211892
 67228/100000: episode: 1603, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -20.907, mean reward: -0.209 [-1.000, 0.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.746, 10.112], loss: 0.002680, mae: 0.054305, mean_q: 0.204097
 67328/100000: episode: 1604, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.512, mean reward: -0.195 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.130, 10.145], loss: 0.002498, mae: 0.052970, mean_q: 0.239669
 67428/100000: episode: 1605, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.063, mean reward: -0.191 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.636, 10.277], loss: 0.014984, mae: 0.059465, mean_q: 0.171798
 67528/100000: episode: 1606, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -8.667, mean reward: -0.087 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.476, 10.098], loss: 0.002764, mae: 0.055539, mean_q: 0.165396
 67628/100000: episode: 1607, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.089, mean reward: -0.161 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.009, 10.174], loss: 0.015612, mae: 0.064449, mean_q: 0.129503
 67728/100000: episode: 1608, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -17.302, mean reward: -0.173 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.592, 10.098], loss: 0.027530, mae: 0.068832, mean_q: 0.147157
 67828/100000: episode: 1609, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.185, mean reward: -0.122 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.275, 10.098], loss: 0.003000, mae: 0.057338, mean_q: 0.118610
 67928/100000: episode: 1610, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.873, mean reward: -0.189 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.623, 10.255], loss: 0.038624, mae: 0.072587, mean_q: 0.137381
 68028/100000: episode: 1611, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -15.962, mean reward: -0.160 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.004, 10.381], loss: 0.002747, mae: 0.053665, mean_q: 0.066190
 68128/100000: episode: 1612, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -10.788, mean reward: -0.108 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.964, 10.098], loss: 0.002592, mae: 0.052431, mean_q: -0.005435
 68228/100000: episode: 1613, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.411, mean reward: -0.194 [-1.000, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.458, 10.132], loss: 0.002611, mae: 0.053162, mean_q: 0.029957
 68328/100000: episode: 1614, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.179, mean reward: -0.152 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.488, 10.237], loss: 0.002488, mae: 0.051205, mean_q: 0.019439
 68428/100000: episode: 1615, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.621, mean reward: -0.166 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.284, 10.098], loss: 0.002762, mae: 0.052291, mean_q: -0.007179
 68528/100000: episode: 1616, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.572, mean reward: -0.176 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.819, 10.299], loss: 0.003716, mae: 0.059048, mean_q: -0.035320
 68628/100000: episode: 1617, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.465, mean reward: -0.175 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.406, 10.378], loss: 0.002524, mae: 0.051715, mean_q: -0.045002
 68728/100000: episode: 1618, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.504, mean reward: -0.155 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.819, 10.175], loss: 0.028118, mae: 0.071208, mean_q: -0.084984
 68828/100000: episode: 1619, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.736, mean reward: -0.177 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.373, 10.188], loss: 0.002551, mae: 0.051373, mean_q: -0.095430
 68928/100000: episode: 1620, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -15.456, mean reward: -0.155 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.983, 10.166], loss: 0.002540, mae: 0.051946, mean_q: -0.106015
 69028/100000: episode: 1621, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -14.795, mean reward: -0.148 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.895, 10.098], loss: 0.002685, mae: 0.053027, mean_q: -0.125966
 69128/100000: episode: 1622, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.139, mean reward: -0.181 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.356, 10.098], loss: 0.015001, mae: 0.058356, mean_q: -0.152457
 69228/100000: episode: 1623, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -13.988, mean reward: -0.140 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.646, 10.098], loss: 0.014479, mae: 0.053700, mean_q: -0.175355
 69328/100000: episode: 1624, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.940, mean reward: -0.179 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.682, 10.205], loss: 0.003017, mae: 0.053919, mean_q: -0.222146
 69428/100000: episode: 1625, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.395, mean reward: -0.164 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.831, 10.098], loss: 0.015452, mae: 0.060985, mean_q: -0.217431
 69528/100000: episode: 1626, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -16.318, mean reward: -0.163 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.354, 10.098], loss: 0.016034, mae: 0.066684, mean_q: -0.263892
 69628/100000: episode: 1627, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.506, mean reward: -0.195 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.325, 10.105], loss: 0.014412, mae: 0.053848, mean_q: -0.261248
 69728/100000: episode: 1628, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.160, mean reward: -0.172 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.976, 10.098], loss: 0.002650, mae: 0.050850, mean_q: -0.303794
 69828/100000: episode: 1629, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.616, mean reward: -0.166 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.060, 10.146], loss: 0.002504, mae: 0.049434, mean_q: -0.299943
 69928/100000: episode: 1630, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -15.924, mean reward: -0.159 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.477, 10.098], loss: 0.002438, mae: 0.049257, mean_q: -0.288733
 70028/100000: episode: 1631, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.535, mean reward: -0.165 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.247, 10.184], loss: 0.005851, mae: 0.065686, mean_q: -0.320692
 70128/100000: episode: 1632, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.645, mean reward: -0.156 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.738, 10.098], loss: 0.003268, mae: 0.057827, mean_q: -0.330011
 70228/100000: episode: 1633, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.768, mean reward: -0.158 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.399, 10.250], loss: 0.002491, mae: 0.049439, mean_q: -0.306244
 70328/100000: episode: 1634, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.136, mean reward: -0.171 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.801, 10.098], loss: 0.002352, mae: 0.047744, mean_q: -0.319147
 70428/100000: episode: 1635, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -14.496, mean reward: -0.145 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.720, 10.098], loss: 0.002336, mae: 0.047715, mean_q: -0.323792
 70528/100000: episode: 1636, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.582, mean reward: -0.186 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.912, 10.098], loss: 0.002412, mae: 0.047836, mean_q: -0.308335
 70628/100000: episode: 1637, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.336, mean reward: -0.163 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.941, 10.113], loss: 0.002473, mae: 0.048522, mean_q: -0.318257
 70728/100000: episode: 1638, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.915, mean reward: -0.179 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.233, 10.304], loss: 0.002341, mae: 0.048105, mean_q: -0.303629
 70828/100000: episode: 1639, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.008, mean reward: -0.150 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.731, 10.098], loss: 0.002223, mae: 0.045830, mean_q: -0.337311
 70928/100000: episode: 1640, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.338, mean reward: -0.183 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.477, 10.143], loss: 0.002388, mae: 0.046931, mean_q: -0.338868
 71028/100000: episode: 1641, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.705, mean reward: -0.167 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.541, 10.179], loss: 0.002358, mae: 0.047437, mean_q: -0.320189
 71128/100000: episode: 1642, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.202, mean reward: -0.192 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.159, 10.213], loss: 0.002527, mae: 0.049887, mean_q: -0.298535
 71228/100000: episode: 1643, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.794, mean reward: -0.178 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.747, 10.098], loss: 0.002409, mae: 0.048338, mean_q: -0.295523
 71328/100000: episode: 1644, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.449, mean reward: -0.194 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.225, 10.104], loss: 0.002634, mae: 0.050818, mean_q: -0.307525
 71428/100000: episode: 1645, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -13.148, mean reward: -0.131 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.148, 10.098], loss: 0.002468, mae: 0.049327, mean_q: -0.317000
 71528/100000: episode: 1646, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.192, mean reward: -0.162 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.756, 10.134], loss: 0.002625, mae: 0.050459, mean_q: -0.307715
 71628/100000: episode: 1647, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -14.916, mean reward: -0.149 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.743, 10.098], loss: 0.002522, mae: 0.049686, mean_q: -0.319101
 71728/100000: episode: 1648, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -12.352, mean reward: -0.124 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.630, 10.098], loss: 0.002584, mae: 0.050211, mean_q: -0.314404
 71828/100000: episode: 1649, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.688, mean reward: -0.167 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.562, 10.296], loss: 0.002394, mae: 0.047453, mean_q: -0.324955
 71928/100000: episode: 1650, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.468, mean reward: -0.185 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.529, 10.098], loss: 0.002443, mae: 0.049437, mean_q: -0.279416
 72028/100000: episode: 1651, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.032, mean reward: -0.190 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.830, 10.226], loss: 0.002516, mae: 0.049574, mean_q: -0.312686
 72128/100000: episode: 1652, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.034, mean reward: -0.160 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.941, 10.098], loss: 0.002457, mae: 0.049504, mean_q: -0.302789
 72228/100000: episode: 1653, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.088, mean reward: -0.191 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.784, 10.274], loss: 0.002604, mae: 0.051086, mean_q: -0.326332
 72328/100000: episode: 1654, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.951, mean reward: -0.180 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.425, 10.135], loss: 0.002311, mae: 0.047395, mean_q: -0.321656
 72428/100000: episode: 1655, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.117, mean reward: -0.161 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.942, 10.098], loss: 0.002346, mae: 0.048047, mean_q: -0.289156
 72528/100000: episode: 1656, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.809, mean reward: -0.168 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.730, 10.098], loss: 0.002296, mae: 0.047864, mean_q: -0.305120
 72628/100000: episode: 1657, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.150, mean reward: -0.161 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.121, 10.266], loss: 0.002513, mae: 0.049674, mean_q: -0.293628
 72728/100000: episode: 1658, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -19.616, mean reward: -0.196 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.838, 10.098], loss: 0.002332, mae: 0.048247, mean_q: -0.267977
 72828/100000: episode: 1659, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.779, mean reward: -0.208 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.098, 10.116], loss: 0.002471, mae: 0.049811, mean_q: -0.321956
 72928/100000: episode: 1660, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.199, mean reward: -0.172 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.766, 10.417], loss: 0.002490, mae: 0.050352, mean_q: -0.318855
 73028/100000: episode: 1661, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -20.552, mean reward: -0.206 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.607, 10.182], loss: 0.002385, mae: 0.048101, mean_q: -0.290264
 73128/100000: episode: 1662, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.544, mean reward: -0.185 [-1.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.250, 10.098], loss: 0.004253, mae: 0.058706, mean_q: -0.291602
 73228/100000: episode: 1663, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.551, mean reward: -0.176 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.703, 10.098], loss: 0.004667, mae: 0.063657, mean_q: -0.312896
 73328/100000: episode: 1664, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.498, mean reward: -0.155 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.626, 10.125], loss: 0.002301, mae: 0.047485, mean_q: -0.335440
 73428/100000: episode: 1665, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -14.916, mean reward: -0.149 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.736, 10.098], loss: 0.002414, mae: 0.049297, mean_q: -0.291429
 73528/100000: episode: 1666, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -14.843, mean reward: -0.148 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.409, 10.098], loss: 0.002387, mae: 0.048623, mean_q: -0.279117
 73628/100000: episode: 1667, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.715, mean reward: -0.127 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.763, 10.246], loss: 0.002382, mae: 0.048373, mean_q: -0.313999
 73728/100000: episode: 1668, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.950, mean reward: -0.189 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.376, 10.098], loss: 0.002446, mae: 0.048968, mean_q: -0.328787
 73828/100000: episode: 1669, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.663, mean reward: -0.167 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.652, 10.098], loss: 0.002531, mae: 0.050244, mean_q: -0.274581
 73928/100000: episode: 1670, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.183, mean reward: -0.152 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.809, 10.098], loss: 0.002554, mae: 0.049874, mean_q: -0.322831
 74028/100000: episode: 1671, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -19.929, mean reward: -0.199 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.719, 10.098], loss: 0.002594, mae: 0.050143, mean_q: -0.312491
 74128/100000: episode: 1672, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -11.070, mean reward: -0.111 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.876, 10.098], loss: 0.002608, mae: 0.050721, mean_q: -0.347319
 74228/100000: episode: 1673, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.072, mean reward: -0.191 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.484, 10.098], loss: 0.002267, mae: 0.047249, mean_q: -0.325066
 74328/100000: episode: 1674, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -10.656, mean reward: -0.107 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.663, 10.422], loss: 0.002248, mae: 0.047185, mean_q: -0.319945
 74428/100000: episode: 1675, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -19.689, mean reward: -0.197 [-1.000, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.005, 10.098], loss: 0.002471, mae: 0.049526, mean_q: -0.291725
 74528/100000: episode: 1676, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.970, mean reward: -0.180 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.990, 10.098], loss: 0.002952, mae: 0.053463, mean_q: -0.316039
 74628/100000: episode: 1677, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.237, mean reward: -0.182 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.705, 10.254], loss: 0.004837, mae: 0.066410, mean_q: -0.306664
 74728/100000: episode: 1678, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -13.663, mean reward: -0.137 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.098, 10.098], loss: 0.002345, mae: 0.048572, mean_q: -0.319910
[Info] 100-TH LEVEL FOUND: 0.6680830717086792, Considering 10/90 traces
 74828/100000: episode: 1679, duration: 4.357s, episode steps: 100, steps per second: 23, episode reward: -15.256, mean reward: -0.153 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.092, 10.098], loss: 0.002551, mae: 0.049888, mean_q: -0.327040
 74834/100000: episode: 1680, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 1.677, mean reward: 0.280 [0.254, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.251, 10.100], loss: 0.002350, mae: 0.046944, mean_q: -0.327072
 74851/100000: episode: 1681, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 5.965, mean reward: 0.351 [0.161, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.375, 10.141], loss: 0.002604, mae: 0.050465, mean_q: -0.329460
 74868/100000: episode: 1682, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 5.149, mean reward: 0.303 [0.246, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.255, 10.100], loss: 0.002275, mae: 0.047857, mean_q: -0.317527
 74885/100000: episode: 1683, duration: 0.109s, episode steps: 17, steps per second: 156, episode reward: 7.864, mean reward: 0.463 [0.333, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.197, 10.406], loss: 0.002819, mae: 0.053110, mean_q: -0.214563
 74891/100000: episode: 1684, duration: 0.033s, episode steps: 6, steps per second: 185, episode reward: 1.850, mean reward: 0.308 [0.186, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.348, 10.100], loss: 0.002829, mae: 0.053510, mean_q: -0.256847
 74903/100000: episode: 1685, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 4.276, mean reward: 0.356 [0.291, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.222, 10.417], loss: 0.003858, mae: 0.061175, mean_q: -0.228864
 74909/100000: episode: 1686, duration: 0.031s, episode steps: 6, steps per second: 193, episode reward: 1.270, mean reward: 0.212 [0.188, 0.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.319, 10.100], loss: 0.003208, mae: 0.057385, mean_q: -0.314359
 74922/100000: episode: 1687, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 4.878, mean reward: 0.375 [0.325, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.418, 10.100], loss: 0.002157, mae: 0.045843, mean_q: -0.392857
 74939/100000: episode: 1688, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 6.175, mean reward: 0.363 [0.296, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.239, 10.100], loss: 0.002403, mae: 0.048128, mean_q: -0.365058
 74949/100000: episode: 1689, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 3.245, mean reward: 0.325 [0.267, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.389, 10.100], loss: 0.002414, mae: 0.049905, mean_q: -0.265259
 74959/100000: episode: 1690, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 3.647, mean reward: 0.365 [0.254, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.447, 10.100], loss: 0.002626, mae: 0.051328, mean_q: -0.291363
 74971/100000: episode: 1691, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 4.169, mean reward: 0.347 [0.258, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.405], loss: 0.002676, mae: 0.053813, mean_q: -0.174035
 74984/100000: episode: 1692, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 4.876, mean reward: 0.375 [0.304, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.181, 10.100], loss: 0.002756, mae: 0.052463, mean_q: -0.326078
 75013/100000: episode: 1693, duration: 0.142s, episode steps: 29, steps per second: 204, episode reward: 7.767, mean reward: 0.268 [0.183, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.305, 10.100], loss: 0.002582, mae: 0.051099, mean_q: -0.233457
 75019/100000: episode: 1694, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 2.134, mean reward: 0.356 [0.296, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.286, 10.100], loss: 0.002320, mae: 0.049318, mean_q: -0.284515
 75036/100000: episode: 1695, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 6.094, mean reward: 0.358 [0.320, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.217, 10.100], loss: 0.002182, mae: 0.048510, mean_q: -0.240138
 75053/100000: episode: 1696, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 4.358, mean reward: 0.256 [0.174, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.938, 10.100], loss: 0.002446, mae: 0.048727, mean_q: -0.370872
 75059/100000: episode: 1697, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 1.945, mean reward: 0.324 [0.243, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.333, 10.100], loss: 0.002373, mae: 0.048098, mean_q: -0.123514
 75088/100000: episode: 1698, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 9.833, mean reward: 0.339 [0.227, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.493, 10.100], loss: 0.002505, mae: 0.050637, mean_q: -0.249078
 75095/100000: episode: 1699, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 1.291, mean reward: 0.184 [0.146, 0.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.132, 10.100], loss: 0.003189, mae: 0.057733, mean_q: -0.080700
 75105/100000: episode: 1700, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 3.468, mean reward: 0.347 [0.289, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.375, 10.100], loss: 0.002728, mae: 0.052795, mean_q: -0.262052
 75115/100000: episode: 1701, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 3.334, mean reward: 0.333 [0.311, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.310, 10.100], loss: 0.002231, mae: 0.049203, mean_q: -0.194910
 75154/100000: episode: 1702, duration: 0.204s, episode steps: 39, steps per second: 191, episode reward: 14.926, mean reward: 0.383 [0.254, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.389, 10.100], loss: 0.002431, mae: 0.049510, mean_q: -0.246529
 75161/100000: episode: 1703, duration: 0.047s, episode steps: 7, steps per second: 147, episode reward: 1.772, mean reward: 0.253 [0.223, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.233, 10.100], loss: 0.001846, mae: 0.044924, mean_q: -0.175438
 75200/100000: episode: 1704, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 15.467, mean reward: 0.397 [0.246, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.450, 10.100], loss: 0.002534, mae: 0.051884, mean_q: -0.187551
 75213/100000: episode: 1705, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 4.896, mean reward: 0.377 [0.317, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.415, 10.100], loss: 0.002340, mae: 0.049105, mean_q: -0.260250
 75219/100000: episode: 1706, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 1.806, mean reward: 0.301 [0.216, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.889, 10.100], loss: 0.002279, mae: 0.047629, mean_q: -0.280765
 75229/100000: episode: 1707, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 3.457, mean reward: 0.346 [0.238, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.738, 10.100], loss: 0.002385, mae: 0.052295, mean_q: -0.149399
 75236/100000: episode: 1708, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 2.628, mean reward: 0.375 [0.266, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.284, 10.100], loss: 0.001971, mae: 0.044896, mean_q: -0.300983
 75248/100000: episode: 1709, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 3.474, mean reward: 0.289 [0.100, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.801, 10.346], loss: 0.002281, mae: 0.048681, mean_q: -0.195802
 75254/100000: episode: 1710, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 1.974, mean reward: 0.329 [0.252, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.248, 10.100], loss: 0.002796, mae: 0.051533, mean_q: -0.303824
 75266/100000: episode: 1711, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 4.123, mean reward: 0.344 [0.273, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.498, 10.416], loss: 0.002512, mae: 0.050744, mean_q: -0.189467
 75283/100000: episode: 1712, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 4.768, mean reward: 0.280 [0.196, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.335, 10.324], loss: 0.002959, mae: 0.055023, mean_q: -0.240868
 75296/100000: episode: 1713, duration: 0.063s, episode steps: 13, steps per second: 208, episode reward: 5.135, mean reward: 0.395 [0.311, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.341, 10.100], loss: 0.002246, mae: 0.047562, mean_q: -0.248676
 75308/100000: episode: 1714, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 4.590, mean reward: 0.382 [0.316, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.512], loss: 0.002529, mae: 0.051159, mean_q: -0.234660
 75337/100000: episode: 1715, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 10.926, mean reward: 0.377 [0.183, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.192, 10.100], loss: 0.003013, mae: 0.055295, mean_q: -0.229044
 75354/100000: episode: 1716, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 6.277, mean reward: 0.369 [0.302, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.241, 10.100], loss: 0.002284, mae: 0.048384, mean_q: -0.308518
 75360/100000: episode: 1717, duration: 0.038s, episode steps: 6, steps per second: 160, episode reward: 1.575, mean reward: 0.262 [0.222, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.412, 10.100], loss: 0.002296, mae: 0.047593, mean_q: -0.259678
 75399/100000: episode: 1718, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 11.201, mean reward: 0.287 [0.058, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.090, 10.100], loss: 0.002189, mae: 0.047669, mean_q: -0.191160
 75409/100000: episode: 1719, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 3.632, mean reward: 0.363 [0.278, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.486, 10.100], loss: 0.002825, mae: 0.056074, mean_q: -0.077287
 75419/100000: episode: 1720, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 3.617, mean reward: 0.362 [0.287, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.425, 10.100], loss: 0.002253, mae: 0.045946, mean_q: -0.245356
 75436/100000: episode: 1721, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 5.122, mean reward: 0.301 [0.147, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.975, 10.323], loss: 0.002681, mae: 0.053661, mean_q: -0.204024
 75446/100000: episode: 1722, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 2.373, mean reward: 0.237 [0.155, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.198, 10.100], loss: 0.002651, mae: 0.052638, mean_q: -0.278153
 75452/100000: episode: 1723, duration: 0.031s, episode steps: 6, steps per second: 192, episode reward: 1.721, mean reward: 0.287 [0.251, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.331, 10.100], loss: 0.002529, mae: 0.054472, mean_q: -0.066617
 75458/100000: episode: 1724, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 1.755, mean reward: 0.292 [0.250, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.340, 10.100], loss: 0.002747, mae: 0.055493, mean_q: -0.101348
 75470/100000: episode: 1725, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 4.137, mean reward: 0.345 [0.286, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.681, 10.451], loss: 0.002560, mae: 0.052616, mean_q: -0.137125
 75476/100000: episode: 1726, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 1.807, mean reward: 0.301 [0.208, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.196, 10.100], loss: 0.002980, mae: 0.054044, mean_q: -0.244858
 75489/100000: episode: 1727, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 4.181, mean reward: 0.322 [0.241, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.302, 10.100], loss: 0.002105, mae: 0.046861, mean_q: -0.199864
 75502/100000: episode: 1728, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 4.102, mean reward: 0.316 [0.242, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.438, 10.100], loss: 0.002566, mae: 0.051865, mean_q: -0.140645
 75509/100000: episode: 1729, duration: 0.043s, episode steps: 7, steps per second: 165, episode reward: 1.620, mean reward: 0.231 [0.166, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.224, 10.100], loss: 0.002589, mae: 0.051834, mean_q: -0.230839
 75515/100000: episode: 1730, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 2.064, mean reward: 0.344 [0.262, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.677, 10.100], loss: 0.002646, mae: 0.053415, mean_q: -0.190979
 75532/100000: episode: 1731, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 6.266, mean reward: 0.369 [0.238, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.232, 10.100], loss: 0.001918, mae: 0.044083, mean_q: -0.252398
 75538/100000: episode: 1732, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 1.635, mean reward: 0.272 [0.232, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-1.313, 10.100], loss: 0.002252, mae: 0.045617, mean_q: -0.354075
 75555/100000: episode: 1733, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 6.062, mean reward: 0.357 [0.272, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.590, 10.475], loss: 0.002527, mae: 0.050685, mean_q: -0.169101
 75561/100000: episode: 1734, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 1.555, mean reward: 0.259 [0.188, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.100, 10.100], loss: 0.002616, mae: 0.048680, mean_q: -0.254159
 75571/100000: episode: 1735, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 3.575, mean reward: 0.357 [0.261, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.439, 10.100], loss: 0.002547, mae: 0.052437, mean_q: -0.114126
 75588/100000: episode: 1736, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 8.252, mean reward: 0.485 [0.383, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.514], loss: 0.002328, mae: 0.048525, mean_q: -0.240884
 75598/100000: episode: 1737, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 2.552, mean reward: 0.255 [0.241, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.205, 10.100], loss: 0.002590, mae: 0.049977, mean_q: -0.135326
 75627/100000: episode: 1738, duration: 0.167s, episode steps: 29, steps per second: 173, episode reward: 10.076, mean reward: 0.347 [0.230, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.785, 10.100], loss: 0.002444, mae: 0.051644, mean_q: -0.122737
 75656/100000: episode: 1739, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 9.409, mean reward: 0.324 [0.223, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.840, 10.100], loss: 0.002382, mae: 0.048245, mean_q: -0.189574
 75663/100000: episode: 1740, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 1.879, mean reward: 0.268 [0.181, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.269, 10.100], loss: 0.002652, mae: 0.051481, mean_q: -0.094950
 75680/100000: episode: 1741, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 5.996, mean reward: 0.353 [0.287, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.651, 10.100], loss: 0.002900, mae: 0.056137, mean_q: -0.115640
 75697/100000: episode: 1742, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 4.454, mean reward: 0.262 [0.155, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-1.513, 10.256], loss: 0.003031, mae: 0.055325, mean_q: -0.178120
 75714/100000: episode: 1743, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 6.962, mean reward: 0.410 [0.344, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.567, 10.100], loss: 0.002773, mae: 0.057082, mean_q: -0.107270
 75731/100000: episode: 1744, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 6.128, mean reward: 0.360 [0.308, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.346, 10.100], loss: 0.002942, mae: 0.055409, mean_q: -0.157008
 75748/100000: episode: 1745, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 4.273, mean reward: 0.251 [0.129, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.277, 10.100], loss: 0.002858, mae: 0.056749, mean_q: -0.080467
 75777/100000: episode: 1746, duration: 0.152s, episode steps: 29, steps per second: 190, episode reward: 9.038, mean reward: 0.312 [0.116, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.284, 10.100], loss: 0.002440, mae: 0.050504, mean_q: -0.161630
 75816/100000: episode: 1747, duration: 0.192s, episode steps: 39, steps per second: 203, episode reward: 16.047, mean reward: 0.411 [0.302, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.420, 10.100], loss: 0.005455, mae: 0.075470, mean_q: -0.149737
 75845/100000: episode: 1748, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 6.975, mean reward: 0.241 [0.120, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.643, 10.100], loss: 0.006431, mae: 0.070764, mean_q: -0.146582
 75851/100000: episode: 1749, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 2.179, mean reward: 0.363 [0.242, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.243, 10.100], loss: 0.003296, mae: 0.062841, mean_q: -0.054328
 75857/100000: episode: 1750, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 1.820, mean reward: 0.303 [0.262, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.925, 10.100], loss: 0.003638, mae: 0.068214, mean_q: -0.090502
 75896/100000: episode: 1751, duration: 0.217s, episode steps: 39, steps per second: 180, episode reward: 10.550, mean reward: 0.271 [0.033, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.245, 10.100], loss: 0.003110, mae: 0.058779, mean_q: -0.095373
 75909/100000: episode: 1752, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 6.025, mean reward: 0.463 [0.412, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.427, 10.100], loss: 0.002459, mae: 0.052439, mean_q: -0.094788
 75926/100000: episode: 1753, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 6.037, mean reward: 0.355 [0.275, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.838, 10.100], loss: 0.002404, mae: 0.049617, mean_q: -0.102557
 75936/100000: episode: 1754, duration: 0.069s, episode steps: 10, steps per second: 144, episode reward: 2.221, mean reward: 0.222 [0.159, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.467, 10.100], loss: 0.002465, mae: 0.050797, mean_q: -0.110542
 75953/100000: episode: 1755, duration: 0.113s, episode steps: 17, steps per second: 151, episode reward: 6.954, mean reward: 0.409 [0.364, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.627, 10.100], loss: 0.002507, mae: 0.050544, mean_q: -0.033299
 75965/100000: episode: 1756, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 3.844, mean reward: 0.320 [0.257, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.410], loss: 0.002626, mae: 0.051196, mean_q: -0.132065
 75982/100000: episode: 1757, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 5.459, mean reward: 0.321 [0.202, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.234, 10.100], loss: 0.002459, mae: 0.050774, mean_q: -0.078020
 75995/100000: episode: 1758, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 4.666, mean reward: 0.359 [0.301, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.226, 10.100], loss: 0.002701, mae: 0.052696, mean_q: -0.052626
 76001/100000: episode: 1759, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 1.696, mean reward: 0.283 [0.223, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.264, 10.100], loss: 0.001906, mae: 0.043971, mean_q: -0.206131
 76018/100000: episode: 1760, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 7.338, mean reward: 0.432 [0.355, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.451], loss: 0.002521, mae: 0.052631, mean_q: -0.007389
 76057/100000: episode: 1761, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 9.878, mean reward: 0.253 [0.055, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.035, 10.186], loss: 0.002720, mae: 0.052793, mean_q: -0.069438
 76064/100000: episode: 1762, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 1.317, mean reward: 0.188 [0.123, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.109, 10.100], loss: 0.003097, mae: 0.056800, mean_q: -0.070170
 76077/100000: episode: 1763, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 5.661, mean reward: 0.435 [0.322, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.375, 10.100], loss: 0.002424, mae: 0.051798, mean_q: -0.072178
 76094/100000: episode: 1764, duration: 0.080s, episode steps: 17, steps per second: 212, episode reward: 6.428, mean reward: 0.378 [0.257, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.353], loss: 0.002383, mae: 0.050421, mean_q: -0.054465
 76101/100000: episode: 1765, duration: 0.035s, episode steps: 7, steps per second: 200, episode reward: 1.707, mean reward: 0.244 [0.190, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.180, 10.100], loss: 0.002788, mae: 0.056160, mean_q: 0.066461
 76107/100000: episode: 1766, duration: 0.039s, episode steps: 6, steps per second: 156, episode reward: 2.062, mean reward: 0.344 [0.312, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.431, 10.100], loss: 0.002790, mae: 0.055143, mean_q: -0.098416
 76117/100000: episode: 1767, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 4.133, mean reward: 0.413 [0.267, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.457, 10.100], loss: 0.002315, mae: 0.048326, mean_q: -0.047469
 76156/100000: episode: 1768, duration: 0.195s, episode steps: 39, steps per second: 200, episode reward: 15.067, mean reward: 0.386 [0.250, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.794, 10.100], loss: 0.002574, mae: 0.051776, mean_q: -0.031940
[Info] 200-TH LEVEL FOUND: 0.8504599332809448, Considering 10/90 traces
 76166/100000: episode: 1769, duration: 3.913s, episode steps: 10, steps per second: 3, episode reward: 2.964, mean reward: 0.296 [0.264, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.297, 10.100], loss: 0.002617, mae: 0.053360, mean_q: -0.009718
 76182/100000: episode: 1770, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 5.809, mean reward: 0.363 [0.302, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.733, 10.503], loss: 0.002178, mae: 0.048114, mean_q: -0.028897
 76211/100000: episode: 1771, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 9.032, mean reward: 0.311 [0.059, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.308, 10.100], loss: 0.002633, mae: 0.051529, mean_q: -0.055369
 76243/100000: episode: 1772, duration: 0.179s, episode steps: 32, steps per second: 178, episode reward: 11.991, mean reward: 0.375 [0.166, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.206, 10.100], loss: 0.002353, mae: 0.050260, mean_q: -0.032871
 76259/100000: episode: 1773, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 6.408, mean reward: 0.401 [0.311, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.519], loss: 0.002512, mae: 0.050927, mean_q: -0.068573
 76288/100000: episode: 1774, duration: 0.152s, episode steps: 29, steps per second: 190, episode reward: 13.822, mean reward: 0.477 [0.359, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.346, 10.100], loss: 0.002565, mae: 0.051791, mean_q: -0.010831
 76308/100000: episode: 1775, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 8.727, mean reward: 0.436 [0.342, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.836, 10.100], loss: 0.002583, mae: 0.053262, mean_q: -0.037058
 76341/100000: episode: 1776, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 10.740, mean reward: 0.325 [0.144, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.300, 10.100], loss: 0.002447, mae: 0.050834, mean_q: -0.032456
 76357/100000: episode: 1777, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 8.156, mean reward: 0.510 [0.380, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.463, 10.623], loss: 0.002743, mae: 0.054036, mean_q: -0.040053
 76372/100000: episode: 1778, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 6.520, mean reward: 0.435 [0.389, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.946, 10.579], loss: 0.002247, mae: 0.050728, mean_q: -0.061717
 76392/100000: episode: 1779, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 7.444, mean reward: 0.372 [0.161, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.176, 10.100], loss: 0.002574, mae: 0.052654, mean_q: 0.039106
 76407/100000: episode: 1780, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 6.774, mean reward: 0.452 [0.271, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.554], loss: 0.002768, mae: 0.055105, mean_q: 0.044099
 76423/100000: episode: 1781, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 5.679, mean reward: 0.355 [0.294, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.457, 10.456], loss: 0.002233, mae: 0.049759, mean_q: 0.002744
 76456/100000: episode: 1782, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 15.939, mean reward: 0.483 [0.376, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.370, 10.100], loss: 0.002277, mae: 0.049513, mean_q: -0.030936
 76476/100000: episode: 1783, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 8.157, mean reward: 0.408 [0.324, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.245, 10.100], loss: 0.002283, mae: 0.049128, mean_q: -0.025838
 76491/100000: episode: 1784, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 6.006, mean reward: 0.400 [0.331, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-1.001, 10.542], loss: 0.002499, mae: 0.052942, mean_q: 0.040472
 76506/100000: episode: 1785, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 5.404, mean reward: 0.360 [0.301, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.426], loss: 0.002657, mae: 0.054073, mean_q: 0.065568
 76539/100000: episode: 1786, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 13.990, mean reward: 0.424 [0.276, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.651, 10.100], loss: 0.002334, mae: 0.050430, mean_q: 0.007336
 76555/100000: episode: 1787, duration: 0.100s, episode steps: 16, steps per second: 159, episode reward: 5.812, mean reward: 0.363 [0.278, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.487], loss: 0.002256, mae: 0.048568, mean_q: 0.032599
 76571/100000: episode: 1788, duration: 0.085s, episode steps: 16, steps per second: 187, episode reward: 5.654, mean reward: 0.353 [0.268, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.853, 10.482], loss: 0.002561, mae: 0.053199, mean_q: 0.046749
 76586/100000: episode: 1789, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 5.674, mean reward: 0.378 [0.305, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.420, 10.375], loss: 0.002891, mae: 0.056199, mean_q: 0.085882
 76602/100000: episode: 1790, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 7.144, mean reward: 0.446 [0.370, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.428, 10.429], loss: 0.002475, mae: 0.051997, mean_q: 0.042792
 76618/100000: episode: 1791, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 4.958, mean reward: 0.310 [0.206, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.319], loss: 0.002139, mae: 0.047855, mean_q: 0.026025
 76634/100000: episode: 1792, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 7.044, mean reward: 0.440 [0.350, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.642], loss: 0.002502, mae: 0.051751, mean_q: -0.052125
 76650/100000: episode: 1793, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 5.571, mean reward: 0.348 [0.264, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.545, 10.423], loss: 0.003141, mae: 0.059867, mean_q: 0.038314
 76670/100000: episode: 1794, duration: 0.098s, episode steps: 20, steps per second: 205, episode reward: 8.502, mean reward: 0.425 [0.303, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.270, 10.100], loss: 0.002524, mae: 0.052822, mean_q: 0.070777
 76690/100000: episode: 1795, duration: 0.113s, episode steps: 20, steps per second: 178, episode reward: 6.769, mean reward: 0.338 [0.216, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.311, 10.100], loss: 0.002238, mae: 0.049418, mean_q: 0.104460
 76706/100000: episode: 1796, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 5.940, mean reward: 0.371 [0.304, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.277, 10.421], loss: 0.002642, mae: 0.053980, mean_q: 0.083967
 76722/100000: episode: 1797, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 6.762, mean reward: 0.423 [0.340, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.506], loss: 0.002558, mae: 0.050792, mean_q: 0.029177
 76755/100000: episode: 1798, duration: 0.195s, episode steps: 33, steps per second: 170, episode reward: 11.500, mean reward: 0.348 [0.206, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.548, 10.100], loss: 0.002394, mae: 0.050866, mean_q: 0.101738
 76771/100000: episode: 1799, duration: 0.091s, episode steps: 16, steps per second: 177, episode reward: 6.145, mean reward: 0.384 [0.287, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.364, 10.523], loss: 0.001884, mae: 0.045259, mean_q: 0.047709
 76787/100000: episode: 1800, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 5.833, mean reward: 0.365 [0.185, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.559, 10.419], loss: 0.002437, mae: 0.053237, mean_q: 0.122134
 76820/100000: episode: 1801, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 12.995, mean reward: 0.394 [0.225, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-1.335, 10.100], loss: 0.002397, mae: 0.051541, mean_q: 0.095378
[Info] FALSIFICATION!
 76849/100000: episode: 1802, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 25.169, mean reward: 0.868 [0.358, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.701, 10.100], loss: 0.002347, mae: 0.051094, mean_q: 0.133455
 76949/100000: episode: 1803, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.263, mean reward: -0.193 [-1.000, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.006, 10.334], loss: 0.018558, mae: 0.070983, mean_q: 0.097021
 77049/100000: episode: 1804, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.285, mean reward: -0.163 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.573, 10.098], loss: 0.017304, mae: 0.068802, mean_q: 0.087644
 77149/100000: episode: 1805, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -12.482, mean reward: -0.125 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-2.021, 10.450], loss: 0.003030, mae: 0.055189, mean_q: 0.085847
 77249/100000: episode: 1806, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.345, mean reward: -0.193 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.179, 10.288], loss: 0.003159, mae: 0.056722, mean_q: 0.069176
 77349/100000: episode: 1807, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.581, mean reward: -0.146 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.477, 10.098], loss: 0.002952, mae: 0.054878, mean_q: 0.094228
 77449/100000: episode: 1808, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -18.640, mean reward: -0.186 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.551, 10.224], loss: 0.031357, mae: 0.080747, mean_q: 0.109259
 77549/100000: episode: 1809, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.256, mean reward: -0.173 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.672, 10.098], loss: 0.017935, mae: 0.073374, mean_q: 0.113605
 77649/100000: episode: 1810, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.970, mean reward: -0.200 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.103, 10.098], loss: 0.018162, mae: 0.075063, mean_q: 0.103159
 77749/100000: episode: 1811, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.262, mean reward: -0.193 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.631, 10.153], loss: 0.017427, mae: 0.072831, mean_q: 0.107038
 77849/100000: episode: 1812, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.469, mean reward: -0.175 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.741, 10.098], loss: 0.016384, mae: 0.063070, mean_q: 0.090181
 77949/100000: episode: 1813, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.167, mean reward: -0.142 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.187, 10.098], loss: 0.016324, mae: 0.062469, mean_q: 0.105717
 78049/100000: episode: 1814, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -10.984, mean reward: -0.110 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.408, 10.098], loss: 0.003035, mae: 0.055415, mean_q: 0.098200
 78149/100000: episode: 1815, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.861, mean reward: -0.159 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.566, 10.098], loss: 0.016066, mae: 0.060570, mean_q: 0.079869
 78249/100000: episode: 1816, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -19.329, mean reward: -0.193 [-1.000, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.755, 10.158], loss: 0.003081, mae: 0.056842, mean_q: 0.088041
 78349/100000: episode: 1817, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -17.162, mean reward: -0.172 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.149, 10.287], loss: 0.002902, mae: 0.055352, mean_q: 0.100395
 78449/100000: episode: 1818, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.612, mean reward: -0.176 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.127, 10.098], loss: 0.002845, mae: 0.055023, mean_q: 0.093861
 78549/100000: episode: 1819, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.275, mean reward: -0.163 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.110, 10.098], loss: 0.029981, mae: 0.071618, mean_q: 0.122543
 78649/100000: episode: 1820, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.921, mean reward: -0.189 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.874, 10.125], loss: 0.003041, mae: 0.056547, mean_q: 0.075237
 78749/100000: episode: 1821, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.844, mean reward: -0.178 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.271, 10.098], loss: 0.002980, mae: 0.056512, mean_q: 0.122504
 78849/100000: episode: 1822, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.179, mean reward: -0.162 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.325, 10.389], loss: 0.031099, mae: 0.081666, mean_q: 0.117954
 78949/100000: episode: 1823, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.098, mean reward: -0.181 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.219, 10.098], loss: 0.017826, mae: 0.075908, mean_q: 0.088679
 79049/100000: episode: 1824, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.993, mean reward: -0.180 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.306, 10.098], loss: 0.003266, mae: 0.059207, mean_q: 0.097977
 79149/100000: episode: 1825, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.668, mean reward: -0.157 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.960, 10.098], loss: 0.003152, mae: 0.059515, mean_q: 0.100490
 79249/100000: episode: 1826, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.943, mean reward: -0.189 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.184, 10.144], loss: 0.002787, mae: 0.054248, mean_q: 0.092419
 79349/100000: episode: 1827, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.280, mean reward: -0.163 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.722, 10.200], loss: 0.003229, mae: 0.057639, mean_q: 0.098033
 79449/100000: episode: 1828, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.285, mean reward: -0.193 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.724, 10.098], loss: 0.029830, mae: 0.077963, mean_q: 0.106587
 79549/100000: episode: 1829, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -17.773, mean reward: -0.178 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.576, 10.098], loss: 0.030623, mae: 0.077218, mean_q: 0.114940
 79649/100000: episode: 1830, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.491, mean reward: -0.195 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.800, 10.134], loss: 0.004353, mae: 0.066999, mean_q: 0.055983
 79749/100000: episode: 1831, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.454, mean reward: -0.175 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.623, 10.240], loss: 0.003371, mae: 0.061318, mean_q: 0.090653
 79849/100000: episode: 1832, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.911, mean reward: -0.179 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.940, 10.098], loss: 0.002821, mae: 0.053554, mean_q: 0.039726
 79949/100000: episode: 1833, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.131, mean reward: -0.181 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.000, 10.098], loss: 0.015944, mae: 0.062360, mean_q: 0.055385
 80049/100000: episode: 1834, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.514, mean reward: -0.195 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.274, 10.098], loss: 0.016226, mae: 0.064400, mean_q: 0.049343
 80149/100000: episode: 1835, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -20.626, mean reward: -0.206 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.080, 10.125], loss: 0.002693, mae: 0.053087, mean_q: 0.015830
 80249/100000: episode: 1836, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.670, mean reward: -0.197 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.526, 10.098], loss: 0.002822, mae: 0.053697, mean_q: -0.003658
 80349/100000: episode: 1837, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.543, mean reward: -0.175 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.608, 10.098], loss: 0.002782, mae: 0.053355, mean_q: -0.013833
 80449/100000: episode: 1838, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.777, mean reward: -0.178 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.870, 10.205], loss: 0.002948, mae: 0.055665, mean_q: -0.050620
 80549/100000: episode: 1839, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.564, mean reward: -0.196 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.583, 10.098], loss: 0.003108, mae: 0.058317, mean_q: -0.069401
 80649/100000: episode: 1840, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -10.808, mean reward: -0.108 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.878, 10.165], loss: 0.002806, mae: 0.055049, mean_q: -0.088350
 80749/100000: episode: 1841, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.071, mean reward: -0.181 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.381, 10.242], loss: 0.015685, mae: 0.057593, mean_q: -0.092866
 80849/100000: episode: 1842, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -18.409, mean reward: -0.184 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.040, 10.098], loss: 0.003619, mae: 0.060868, mean_q: -0.122819
 80949/100000: episode: 1843, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.333, mean reward: -0.173 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.771, 10.098], loss: 0.002555, mae: 0.049784, mean_q: -0.161258
 81049/100000: episode: 1844, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.369, mean reward: -0.194 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.062, 10.098], loss: 0.002879, mae: 0.053121, mean_q: -0.199226
 81149/100000: episode: 1845, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.792, mean reward: -0.148 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.124, 10.241], loss: 0.002614, mae: 0.051411, mean_q: -0.160847
 81249/100000: episode: 1846, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.818, mean reward: -0.168 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.531, 10.098], loss: 0.002491, mae: 0.050055, mean_q: -0.199463
 81349/100000: episode: 1847, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.482, mean reward: -0.185 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.384, 10.098], loss: 0.002486, mae: 0.049630, mean_q: -0.219559
 81449/100000: episode: 1848, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -18.257, mean reward: -0.183 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.649, 10.128], loss: 0.015888, mae: 0.060488, mean_q: -0.212163
 81549/100000: episode: 1849, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.839, mean reward: -0.178 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.075, 10.215], loss: 0.015103, mae: 0.057044, mean_q: -0.262255
 81649/100000: episode: 1850, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -15.608, mean reward: -0.156 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.592, 10.098], loss: 0.002831, mae: 0.053590, mean_q: -0.284806
 81749/100000: episode: 1851, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.125, mean reward: -0.161 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.459, 10.306], loss: 0.015614, mae: 0.061673, mean_q: -0.282690
 81849/100000: episode: 1852, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.948, mean reward: -0.199 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.924, 10.139], loss: 0.002684, mae: 0.051463, mean_q: -0.342596
 81949/100000: episode: 1853, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.816, mean reward: -0.188 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.758, 10.229], loss: 0.005487, mae: 0.066831, mean_q: -0.318825
 82049/100000: episode: 1854, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -20.623, mean reward: -0.206 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.659, 10.253], loss: 0.002730, mae: 0.052693, mean_q: -0.337438
 82149/100000: episode: 1855, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -18.052, mean reward: -0.181 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.061, 10.098], loss: 0.002531, mae: 0.049867, mean_q: -0.337951
 82249/100000: episode: 1856, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.053, mean reward: -0.181 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.582, 10.098], loss: 0.002531, mae: 0.049340, mean_q: -0.336586
 82349/100000: episode: 1857, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.939, mean reward: -0.189 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.741, 10.153], loss: 0.002554, mae: 0.049596, mean_q: -0.318615
 82449/100000: episode: 1858, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.938, mean reward: -0.169 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.483, 10.125], loss: 0.002386, mae: 0.048102, mean_q: -0.327543
 82549/100000: episode: 1859, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.976, mean reward: -0.180 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.832, 10.217], loss: 0.002387, mae: 0.047605, mean_q: -0.336305
 82649/100000: episode: 1860, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.339, mean reward: -0.193 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.212, 10.166], loss: 0.002446, mae: 0.049074, mean_q: -0.320346
 82749/100000: episode: 1861, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -15.678, mean reward: -0.157 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.706, 10.270], loss: 0.002496, mae: 0.049645, mean_q: -0.304761
 82849/100000: episode: 1862, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -17.750, mean reward: -0.177 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.921, 10.195], loss: 0.002497, mae: 0.049253, mean_q: -0.332727
 82949/100000: episode: 1863, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.077, mean reward: -0.161 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.531, 10.098], loss: 0.002540, mae: 0.050691, mean_q: -0.322470
 83049/100000: episode: 1864, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.082, mean reward: -0.181 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.904, 10.277], loss: 0.002479, mae: 0.049021, mean_q: -0.323895
 83149/100000: episode: 1865, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -18.690, mean reward: -0.187 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.063, 10.098], loss: 0.002209, mae: 0.046030, mean_q: -0.360959
 83249/100000: episode: 1866, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -13.245, mean reward: -0.132 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.803, 10.265], loss: 0.002480, mae: 0.049014, mean_q: -0.338159
 83349/100000: episode: 1867, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.265, mean reward: -0.163 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.011, 10.098], loss: 0.002401, mae: 0.048299, mean_q: -0.351817
 83449/100000: episode: 1868, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.488, mean reward: -0.155 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.262, 10.235], loss: 0.002423, mae: 0.048216, mean_q: -0.341459
 83549/100000: episode: 1869, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.337, mean reward: -0.183 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.237, 10.369], loss: 0.002530, mae: 0.049725, mean_q: -0.320776
 83649/100000: episode: 1870, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.814, mean reward: -0.158 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.982, 10.098], loss: 0.002409, mae: 0.048512, mean_q: -0.366144
 83749/100000: episode: 1871, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.751, mean reward: -0.168 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.195, 10.098], loss: 0.002443, mae: 0.049018, mean_q: -0.360719
 83849/100000: episode: 1872, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.553, mean reward: -0.186 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.908, 10.147], loss: 0.002610, mae: 0.050753, mean_q: -0.307762
 83949/100000: episode: 1873, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.067, mean reward: -0.161 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.461, 10.199], loss: 0.002518, mae: 0.049471, mean_q: -0.298287
 84049/100000: episode: 1874, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.377, mean reward: -0.144 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.387, 10.098], loss: 0.002504, mae: 0.050395, mean_q: -0.365969
 84149/100000: episode: 1875, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.062, mean reward: -0.181 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.916, 10.098], loss: 0.002574, mae: 0.050892, mean_q: -0.326252
 84249/100000: episode: 1876, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -20.875, mean reward: -0.209 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.713, 10.098], loss: 0.003394, mae: 0.056248, mean_q: -0.324023
 84349/100000: episode: 1877, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.936, mean reward: -0.199 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.078, 10.179], loss: 0.004871, mae: 0.067608, mean_q: -0.326710
 84449/100000: episode: 1878, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.367, mean reward: -0.184 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.844, 10.098], loss: 0.002570, mae: 0.051089, mean_q: -0.328245
 84549/100000: episode: 1879, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.577, mean reward: -0.156 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.265, 10.098], loss: 0.002605, mae: 0.050827, mean_q: -0.320242
 84649/100000: episode: 1880, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.305, mean reward: -0.153 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.777, 10.326], loss: 0.002653, mae: 0.051256, mean_q: -0.324333
 84749/100000: episode: 1881, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.175, mean reward: -0.182 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.442, 10.150], loss: 0.002396, mae: 0.047880, mean_q: -0.350982
 84849/100000: episode: 1882, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -16.255, mean reward: -0.163 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.835, 10.236], loss: 0.002478, mae: 0.048923, mean_q: -0.336709
 84949/100000: episode: 1883, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -20.754, mean reward: -0.208 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.365, 10.098], loss: 0.002624, mae: 0.050626, mean_q: -0.328983
 85049/100000: episode: 1884, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.507, mean reward: -0.185 [-1.000, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.452, 10.098], loss: 0.002407, mae: 0.048289, mean_q: -0.335536
 85149/100000: episode: 1885, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.840, mean reward: -0.178 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.302, 10.098], loss: 0.002717, mae: 0.050950, mean_q: -0.330675
 85249/100000: episode: 1886, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.822, mean reward: -0.198 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.251, 10.098], loss: 0.002475, mae: 0.049167, mean_q: -0.338019
 85349/100000: episode: 1887, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -20.362, mean reward: -0.204 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.878, 10.098], loss: 0.002582, mae: 0.049689, mean_q: -0.332257
 85449/100000: episode: 1888, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -13.518, mean reward: -0.135 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.791, 10.403], loss: 0.002490, mae: 0.049403, mean_q: -0.336387
 85549/100000: episode: 1889, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -20.021, mean reward: -0.200 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.922, 10.098], loss: 0.002720, mae: 0.051367, mean_q: -0.304785
 85649/100000: episode: 1890, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.774, mean reward: -0.188 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.207, 10.176], loss: 0.002531, mae: 0.049479, mean_q: -0.314988
 85749/100000: episode: 1891, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -12.268, mean reward: -0.123 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.128, 10.098], loss: 0.002615, mae: 0.049562, mean_q: -0.354290
 85849/100000: episode: 1892, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.533, mean reward: -0.175 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.880, 10.144], loss: 0.002704, mae: 0.051058, mean_q: -0.327158
 85949/100000: episode: 1893, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.290, mean reward: -0.163 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.110, 10.205], loss: 0.002589, mae: 0.050065, mean_q: -0.315138
 86049/100000: episode: 1894, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -15.992, mean reward: -0.160 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.727, 10.098], loss: 0.002616, mae: 0.049442, mean_q: -0.338551
 86149/100000: episode: 1895, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -11.559, mean reward: -0.116 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.073, 10.528], loss: 0.002989, mae: 0.057043, mean_q: -0.314171
 86249/100000: episode: 1896, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.078, mean reward: -0.181 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.566, 10.195], loss: 0.002743, mae: 0.051087, mean_q: -0.341428
 86349/100000: episode: 1897, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.451, mean reward: -0.195 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.345, 10.127], loss: 0.002560, mae: 0.049418, mean_q: -0.330665
 86449/100000: episode: 1898, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.463, mean reward: -0.145 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.800, 10.336], loss: 0.003154, mae: 0.056281, mean_q: -0.345456
 86549/100000: episode: 1899, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.043, mean reward: -0.150 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.027, 10.201], loss: 0.002715, mae: 0.053323, mean_q: -0.318794
 86649/100000: episode: 1900, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.226, mean reward: -0.162 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.902, 10.098], loss: 0.002741, mae: 0.051488, mean_q: -0.303750
 86749/100000: episode: 1901, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.142, mean reward: -0.161 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.519, 10.098], loss: 0.002523, mae: 0.048490, mean_q: -0.356409
[Info] 100-TH LEVEL FOUND: 0.5370914340019226, Considering 10/90 traces
 86849/100000: episode: 1902, duration: 4.368s, episode steps: 100, steps per second: 23, episode reward: -13.876, mean reward: -0.139 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.356, 10.098], loss: 0.002746, mae: 0.051525, mean_q: -0.312737
 86864/100000: episode: 1903, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 4.605, mean reward: 0.307 [0.257, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.334, 10.100], loss: 0.002353, mae: 0.046150, mean_q: -0.255463
 86926/100000: episode: 1904, duration: 0.340s, episode steps: 62, steps per second: 183, episode reward: 16.749, mean reward: 0.270 [0.109, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.798 [-1.006, 10.299], loss: 0.002597, mae: 0.050130, mean_q: -0.301069
 86979/100000: episode: 1905, duration: 0.267s, episode steps: 53, steps per second: 198, episode reward: 13.120, mean reward: 0.248 [0.023, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.873, 10.353], loss: 0.002643, mae: 0.051731, mean_q: -0.278601
 86997/100000: episode: 1906, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 3.811, mean reward: 0.212 [0.093, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.178, 10.100], loss: 0.002955, mae: 0.053202, mean_q: -0.255048
 87007/100000: episode: 1907, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 3.957, mean reward: 0.396 [0.328, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.951, 10.534], loss: 0.002813, mae: 0.050428, mean_q: -0.258132
 87032/100000: episode: 1908, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 7.477, mean reward: 0.299 [0.162, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.270, 10.343], loss: 0.002534, mae: 0.048666, mean_q: -0.363582
 87050/100000: episode: 1909, duration: 0.091s, episode steps: 18, steps per second: 199, episode reward: 5.051, mean reward: 0.281 [0.185, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.147, 10.100], loss: 0.002533, mae: 0.049033, mean_q: -0.287398
 87106/100000: episode: 1910, duration: 0.312s, episode steps: 56, steps per second: 180, episode reward: 14.071, mean reward: 0.251 [0.060, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-1.002, 10.100], loss: 0.003044, mae: 0.054374, mean_q: -0.258738
 87162/100000: episode: 1911, duration: 0.282s, episode steps: 56, steps per second: 199, episode reward: 15.442, mean reward: 0.276 [0.028, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-0.986, 10.100], loss: 0.002627, mae: 0.050667, mean_q: -0.257418
 87215/100000: episode: 1912, duration: 0.295s, episode steps: 53, steps per second: 180, episode reward: 10.923, mean reward: 0.206 [0.010, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.764, 10.123], loss: 0.002614, mae: 0.050334, mean_q: -0.272395
 87230/100000: episode: 1913, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 4.680, mean reward: 0.312 [0.237, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.684, 10.100], loss: 0.002736, mae: 0.050516, mean_q: -0.295194
 87296/100000: episode: 1914, duration: 0.361s, episode steps: 66, steps per second: 183, episode reward: 17.030, mean reward: 0.258 [0.026, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.767 [-0.503, 10.101], loss: 0.002929, mae: 0.053367, mean_q: -0.247679
 87306/100000: episode: 1915, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 2.841, mean reward: 0.284 [0.255, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.715, 10.377], loss: 0.002189, mae: 0.047368, mean_q: -0.280591
 87321/100000: episode: 1916, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 5.155, mean reward: 0.344 [0.246, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.470], loss: 0.002952, mae: 0.052576, mean_q: -0.232887
 87377/100000: episode: 1917, duration: 0.292s, episode steps: 56, steps per second: 191, episode reward: 18.652, mean reward: 0.333 [0.222, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-0.730, 10.100], loss: 0.002567, mae: 0.050291, mean_q: -0.258535
 87402/100000: episode: 1918, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 8.105, mean reward: 0.324 [0.192, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-1.152, 10.471], loss: 0.002675, mae: 0.051639, mean_q: -0.236772
 87455/100000: episode: 1919, duration: 0.292s, episode steps: 53, steps per second: 182, episode reward: 10.030, mean reward: 0.189 [0.033, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.332, 10.179], loss: 0.002836, mae: 0.052907, mean_q: -0.230098
 87480/100000: episode: 1920, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 8.647, mean reward: 0.346 [0.195, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.420], loss: 0.002405, mae: 0.048074, mean_q: -0.244408
 87529/100000: episode: 1921, duration: 0.266s, episode steps: 49, steps per second: 184, episode reward: 9.020, mean reward: 0.184 [0.022, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.544, 10.220], loss: 0.002893, mae: 0.052927, mean_q: -0.234716
 87595/100000: episode: 1922, duration: 0.347s, episode steps: 66, steps per second: 190, episode reward: 12.328, mean reward: 0.187 [0.024, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.766 [-1.386, 10.123], loss: 0.003047, mae: 0.054726, mean_q: -0.206858
 87648/100000: episode: 1923, duration: 0.290s, episode steps: 53, steps per second: 183, episode reward: 11.418, mean reward: 0.215 [0.067, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.501, 10.284], loss: 0.002993, mae: 0.054917, mean_q: -0.160770
 87710/100000: episode: 1924, duration: 0.318s, episode steps: 62, steps per second: 195, episode reward: 16.332, mean reward: 0.263 [0.036, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.814 [-0.601, 10.455], loss: 0.002991, mae: 0.054868, mean_q: -0.150689
 87759/100000: episode: 1925, duration: 0.256s, episode steps: 49, steps per second: 191, episode reward: 15.418, mean reward: 0.315 [0.106, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.290, 10.448], loss: 0.002966, mae: 0.054982, mean_q: -0.121220
 87812/100000: episode: 1926, duration: 0.289s, episode steps: 53, steps per second: 183, episode reward: 10.654, mean reward: 0.201 [0.026, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-1.670, 10.100], loss: 0.002798, mae: 0.053112, mean_q: -0.159571
 87837/100000: episode: 1927, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 7.768, mean reward: 0.311 [0.210, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.524, 10.346], loss: 0.002912, mae: 0.055529, mean_q: -0.134693
 87893/100000: episode: 1928, duration: 0.310s, episode steps: 56, steps per second: 181, episode reward: 17.410, mean reward: 0.311 [0.209, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.700, 10.100], loss: 0.003042, mae: 0.055285, mean_q: -0.149319
 87911/100000: episode: 1929, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 5.092, mean reward: 0.283 [0.218, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.760, 10.100], loss: 0.003985, mae: 0.063003, mean_q: -0.153561
 87973/100000: episode: 1930, duration: 0.324s, episode steps: 62, steps per second: 191, episode reward: 11.478, mean reward: 0.185 [0.043, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.790 [-0.905, 10.158], loss: 0.003133, mae: 0.057984, mean_q: -0.092201
 87991/100000: episode: 1931, duration: 0.088s, episode steps: 18, steps per second: 205, episode reward: 4.288, mean reward: 0.238 [0.200, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.894, 10.100], loss: 0.002958, mae: 0.055953, mean_q: -0.123349
 88006/100000: episode: 1932, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 5.498, mean reward: 0.367 [0.233, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.645, 10.100], loss: 0.003077, mae: 0.057980, mean_q: -0.109887
 88068/100000: episode: 1933, duration: 0.323s, episode steps: 62, steps per second: 192, episode reward: 17.057, mean reward: 0.275 [0.062, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.790 [-1.303, 10.188], loss: 0.003008, mae: 0.056004, mean_q: -0.157587
 88130/100000: episode: 1934, duration: 0.327s, episode steps: 62, steps per second: 190, episode reward: 15.494, mean reward: 0.250 [0.051, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.794 [-0.969, 10.100], loss: 0.002901, mae: 0.055277, mean_q: -0.133893
 88183/100000: episode: 1935, duration: 0.296s, episode steps: 53, steps per second: 179, episode reward: 12.476, mean reward: 0.235 [0.032, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.425, 10.169], loss: 0.003353, mae: 0.060198, mean_q: -0.100816
 88232/100000: episode: 1936, duration: 0.272s, episode steps: 49, steps per second: 180, episode reward: 10.730, mean reward: 0.219 [0.098, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.570, 10.241], loss: 0.002773, mae: 0.052972, mean_q: -0.124533
 88281/100000: episode: 1937, duration: 0.265s, episode steps: 49, steps per second: 185, episode reward: 12.529, mean reward: 0.256 [0.103, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.265, 10.351], loss: 0.003014, mae: 0.057128, mean_q: -0.065441
 88347/100000: episode: 1938, duration: 0.356s, episode steps: 66, steps per second: 185, episode reward: 14.263, mean reward: 0.216 [0.018, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.751 [-0.654, 10.170], loss: 0.002920, mae: 0.055604, mean_q: -0.059386
 88362/100000: episode: 1939, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 4.263, mean reward: 0.284 [0.149, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.614, 10.100], loss: 0.002935, mae: 0.055074, mean_q: -0.067261
 88411/100000: episode: 1940, duration: 0.252s, episode steps: 49, steps per second: 195, episode reward: 9.745, mean reward: 0.199 [0.037, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-1.450, 10.132], loss: 0.003119, mae: 0.056901, mean_q: -0.048939
 88426/100000: episode: 1941, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 4.521, mean reward: 0.301 [0.261, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.173, 10.100], loss: 0.003435, mae: 0.059303, mean_q: -0.053624
 88482/100000: episode: 1942, duration: 0.308s, episode steps: 56, steps per second: 182, episode reward: 12.341, mean reward: 0.220 [0.104, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-1.427, 10.349], loss: 0.003006, mae: 0.056021, mean_q: -0.031431
 88500/100000: episode: 1943, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 4.715, mean reward: 0.262 [0.162, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.274, 10.100], loss: 0.003419, mae: 0.058787, mean_q: -0.065585
 88515/100000: episode: 1944, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 5.272, mean reward: 0.351 [0.274, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.508, 10.607], loss: 0.003256, mae: 0.060434, mean_q: -0.042388
 88540/100000: episode: 1945, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 7.176, mean reward: 0.287 [0.155, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.100, 10.376], loss: 0.002886, mae: 0.056044, mean_q: -0.044571
 88555/100000: episode: 1946, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 4.320, mean reward: 0.288 [0.155, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.160, 10.100], loss: 0.003090, mae: 0.056940, mean_q: -0.031122
 88611/100000: episode: 1947, duration: 0.296s, episode steps: 56, steps per second: 189, episode reward: 14.718, mean reward: 0.263 [0.074, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.845 [-0.275, 10.100], loss: 0.002905, mae: 0.056479, mean_q: -0.036875
 88636/100000: episode: 1948, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 9.692, mean reward: 0.388 [0.240, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.493], loss: 0.003147, mae: 0.058748, mean_q: 0.028899
 88698/100000: episode: 1949, duration: 0.323s, episode steps: 62, steps per second: 192, episode reward: 16.168, mean reward: 0.261 [0.047, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.801 [-0.265, 10.504], loss: 0.003243, mae: 0.060145, mean_q: 0.017086
 88764/100000: episode: 1950, duration: 0.350s, episode steps: 66, steps per second: 189, episode reward: 19.630, mean reward: 0.297 [0.160, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.757 [-0.769, 10.408], loss: 0.002953, mae: 0.056026, mean_q: 0.004937
 88820/100000: episode: 1951, duration: 0.320s, episode steps: 56, steps per second: 175, episode reward: 9.703, mean reward: 0.173 [0.032, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.853 [-1.109, 10.122], loss: 0.002874, mae: 0.056738, mean_q: 0.004665
 88882/100000: episode: 1952, duration: 0.321s, episode steps: 62, steps per second: 193, episode reward: 15.796, mean reward: 0.255 [0.092, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.798 [-0.896, 10.128], loss: 0.002918, mae: 0.055708, mean_q: -0.007572
 88897/100000: episode: 1953, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 3.900, mean reward: 0.260 [0.114, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.179, 10.396], loss: 0.002776, mae: 0.056644, mean_q: 0.062273
 88915/100000: episode: 1954, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 5.226, mean reward: 0.290 [0.119, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.155, 10.100], loss: 0.003027, mae: 0.055959, mean_q: 0.016890
 88968/100000: episode: 1955, duration: 0.319s, episode steps: 53, steps per second: 166, episode reward: 11.862, mean reward: 0.224 [0.037, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.912, 10.100], loss: 0.002966, mae: 0.057402, mean_q: 0.037730
 88983/100000: episode: 1956, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 3.655, mean reward: 0.244 [0.138, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.520, 10.336], loss: 0.003577, mae: 0.061685, mean_q: 0.100522
 89008/100000: episode: 1957, duration: 0.150s, episode steps: 25, steps per second: 167, episode reward: 6.369, mean reward: 0.255 [0.090, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.249, 10.152], loss: 0.003348, mae: 0.059948, mean_q: 0.020957
 89023/100000: episode: 1958, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 3.196, mean reward: 0.213 [0.065, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.232, 10.242], loss: 0.003098, mae: 0.058593, mean_q: 0.056292
 89048/100000: episode: 1959, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 7.188, mean reward: 0.288 [0.192, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.231, 10.313], loss: 0.002939, mae: 0.057228, mean_q: 0.091258
 89101/100000: episode: 1960, duration: 0.281s, episode steps: 53, steps per second: 189, episode reward: 12.874, mean reward: 0.243 [0.014, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.866 [-0.331, 10.100], loss: 0.003042, mae: 0.058449, mean_q: 0.051832
 89154/100000: episode: 1961, duration: 0.291s, episode steps: 53, steps per second: 182, episode reward: 15.416, mean reward: 0.291 [0.125, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.964, 10.267], loss: 0.007292, mae: 0.070674, mean_q: 0.038886
 89210/100000: episode: 1962, duration: 0.302s, episode steps: 56, steps per second: 185, episode reward: 19.065, mean reward: 0.340 [0.142, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.180, 10.100], loss: 0.005311, mae: 0.072309, mean_q: 0.061434
 89259/100000: episode: 1963, duration: 0.272s, episode steps: 49, steps per second: 180, episode reward: 9.017, mean reward: 0.184 [0.056, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.671, 10.100], loss: 0.004345, mae: 0.068870, mean_q: 0.087565
 89284/100000: episode: 1964, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 7.065, mean reward: 0.283 [0.233, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.372], loss: 0.003577, mae: 0.065568, mean_q: 0.062763
 89337/100000: episode: 1965, duration: 0.291s, episode steps: 53, steps per second: 182, episode reward: 12.396, mean reward: 0.234 [0.114, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.729, 10.368], loss: 0.003109, mae: 0.058447, mean_q: 0.087271
 89355/100000: episode: 1966, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 4.095, mean reward: 0.228 [0.140, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.666, 10.100], loss: 0.002755, mae: 0.055883, mean_q: 0.118199
 89421/100000: episode: 1967, duration: 0.350s, episode steps: 66, steps per second: 189, episode reward: 15.590, mean reward: 0.236 [0.073, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.758 [-0.456, 10.242], loss: 0.003198, mae: 0.060337, mean_q: 0.091522
 89470/100000: episode: 1968, duration: 0.258s, episode steps: 49, steps per second: 190, episode reward: 13.872, mean reward: 0.283 [0.176, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.823, 10.356], loss: 0.003187, mae: 0.059706, mean_q: 0.085404
 89523/100000: episode: 1969, duration: 0.275s, episode steps: 53, steps per second: 193, episode reward: 12.031, mean reward: 0.227 [0.062, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-1.829, 10.220], loss: 0.003408, mae: 0.062247, mean_q: 0.127337
 89533/100000: episode: 1970, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 3.405, mean reward: 0.341 [0.288, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.472], loss: 0.003024, mae: 0.059232, mean_q: 0.075500
 89551/100000: episode: 1971, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 5.090, mean reward: 0.283 [0.195, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.150, 10.100], loss: 0.003492, mae: 0.063128, mean_q: 0.172260
 89561/100000: episode: 1972, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 3.731, mean reward: 0.373 [0.276, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.570, 10.430], loss: 0.003799, mae: 0.065775, mean_q: 0.136810
 89623/100000: episode: 1973, duration: 0.327s, episode steps: 62, steps per second: 190, episode reward: 9.796, mean reward: 0.158 [0.027, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.797 [-0.339, 10.100], loss: 0.003192, mae: 0.060131, mean_q: 0.146921
 89672/100000: episode: 1974, duration: 0.257s, episode steps: 49, steps per second: 190, episode reward: 11.458, mean reward: 0.234 [0.106, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.614, 10.132], loss: 0.002847, mae: 0.056522, mean_q: 0.167643
 89738/100000: episode: 1975, duration: 0.340s, episode steps: 66, steps per second: 194, episode reward: 13.096, mean reward: 0.198 [0.057, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.768 [-0.250, 10.250], loss: 0.003073, mae: 0.058577, mean_q: 0.191939
 89753/100000: episode: 1976, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 4.336, mean reward: 0.289 [0.204, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.146, 10.100], loss: 0.003041, mae: 0.059019, mean_q: 0.184204
 89809/100000: episode: 1977, duration: 0.306s, episode steps: 56, steps per second: 183, episode reward: 11.046, mean reward: 0.197 [0.008, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.164, 10.100], loss: 0.003018, mae: 0.058944, mean_q: 0.199970
 89824/100000: episode: 1978, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 5.990, mean reward: 0.399 [0.279, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.335, 10.542], loss: 0.003314, mae: 0.060561, mean_q: 0.203852
 89849/100000: episode: 1979, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 7.397, mean reward: 0.296 [0.166, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.017, 10.380], loss: 0.003464, mae: 0.062854, mean_q: 0.190200
 89864/100000: episode: 1980, duration: 0.097s, episode steps: 15, steps per second: 155, episode reward: 4.648, mean reward: 0.310 [0.202, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.902, 10.100], loss: 0.003115, mae: 0.060321, mean_q: 0.182974
 89913/100000: episode: 1981, duration: 0.254s, episode steps: 49, steps per second: 193, episode reward: 13.548, mean reward: 0.276 [0.170, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.927, 10.408], loss: 0.003029, mae: 0.058273, mean_q: 0.176242
 89975/100000: episode: 1982, duration: 0.322s, episode steps: 62, steps per second: 193, episode reward: 11.240, mean reward: 0.181 [0.017, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.805 [-0.473, 10.129], loss: 0.003185, mae: 0.059908, mean_q: 0.172352
 90037/100000: episode: 1983, duration: 0.323s, episode steps: 62, steps per second: 192, episode reward: 16.472, mean reward: 0.266 [0.073, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.799 [-0.364, 10.227], loss: 0.003104, mae: 0.058774, mean_q: 0.232435
 90062/100000: episode: 1984, duration: 0.148s, episode steps: 25, steps per second: 169, episode reward: 7.832, mean reward: 0.313 [0.134, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-1.195, 10.296], loss: 0.002882, mae: 0.056549, mean_q: 0.226484
 90115/100000: episode: 1985, duration: 0.282s, episode steps: 53, steps per second: 188, episode reward: 8.892, mean reward: 0.168 [0.013, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.325, 10.150], loss: 0.002871, mae: 0.057116, mean_q: 0.197700
 90140/100000: episode: 1986, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 6.321, mean reward: 0.253 [0.121, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.232], loss: 0.003129, mae: 0.059114, mean_q: 0.219219
 90158/100000: episode: 1987, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 6.007, mean reward: 0.334 [0.261, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.447, 10.100], loss: 0.003360, mae: 0.061661, mean_q: 0.245611
 90207/100000: episode: 1988, duration: 0.249s, episode steps: 49, steps per second: 197, episode reward: 9.101, mean reward: 0.186 [0.016, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.931, 10.239], loss: 0.003151, mae: 0.059877, mean_q: 0.241874
 90222/100000: episode: 1989, duration: 0.071s, episode steps: 15, steps per second: 211, episode reward: 4.187, mean reward: 0.279 [0.206, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.162, 10.100], loss: 0.003020, mae: 0.059461, mean_q: 0.193759
 90240/100000: episode: 1990, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 2.913, mean reward: 0.162 [0.066, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.341, 10.100], loss: 0.002883, mae: 0.056355, mean_q: 0.226697
 90255/100000: episode: 1991, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 4.778, mean reward: 0.319 [0.234, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.816, 10.394], loss: 0.002845, mae: 0.058606, mean_q: 0.284283
[Info] 200-TH LEVEL FOUND: 0.7786059379577637, Considering 10/90 traces
 90270/100000: episode: 1992, duration: 3.993s, episode steps: 15, steps per second: 4, episode reward: 4.108, mean reward: 0.274 [0.163, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.436, 10.278], loss: 0.002805, mae: 0.055353, mean_q: 0.228709
 90280/100000: episode: 1993, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 4.776, mean reward: 0.478 [0.357, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.600], loss: 0.003346, mae: 0.063859, mean_q: 0.344636
 90327/100000: episode: 1994, duration: 0.250s, episode steps: 47, steps per second: 188, episode reward: 14.483, mean reward: 0.308 [0.059, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.325, 10.108], loss: 0.003142, mae: 0.060737, mean_q: 0.268336
 90384/100000: episode: 1995, duration: 0.316s, episode steps: 57, steps per second: 181, episode reward: 20.090, mean reward: 0.352 [0.123, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.824 [-0.408, 10.299], loss: 0.002807, mae: 0.056668, mean_q: 0.258594
 90397/100000: episode: 1996, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 4.162, mean reward: 0.320 [0.248, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.471], loss: 0.002550, mae: 0.054051, mean_q: 0.273608
 90412/100000: episode: 1997, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 7.851, mean reward: 0.523 [0.379, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.592], loss: 0.003380, mae: 0.063048, mean_q: 0.332429
 90461/100000: episode: 1998, duration: 0.263s, episode steps: 49, steps per second: 186, episode reward: 13.914, mean reward: 0.284 [0.035, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.556, 10.219], loss: 0.003205, mae: 0.059891, mean_q: 0.270530
 90474/100000: episode: 1999, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 5.318, mean reward: 0.409 [0.311, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-1.308, 10.548], loss: 0.002745, mae: 0.055640, mean_q: 0.256695
 90489/100000: episode: 2000, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 6.432, mean reward: 0.429 [0.316, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.351, 10.544], loss: 0.003092, mae: 0.060157, mean_q: 0.292488
 90502/100000: episode: 2001, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 4.640, mean reward: 0.357 [0.282, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.463], loss: 0.003637, mae: 0.064964, mean_q: 0.333697
 90515/100000: episode: 2002, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 4.691, mean reward: 0.361 [0.291, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.125, 10.451], loss: 0.003003, mae: 0.058466, mean_q: 0.294642
 90566/100000: episode: 2003, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 13.325, mean reward: 0.261 [0.084, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.500, 10.379], loss: 0.003405, mae: 0.062922, mean_q: 0.337011
 90617/100000: episode: 2004, duration: 0.285s, episode steps: 51, steps per second: 179, episode reward: 16.126, mean reward: 0.316 [0.127, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.681, 10.564], loss: 0.003052, mae: 0.059739, mean_q: 0.308286
 90668/100000: episode: 2005, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 17.467, mean reward: 0.342 [0.143, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-1.437, 10.274], loss: 0.003365, mae: 0.061785, mean_q: 0.309214
 90678/100000: episode: 2006, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 4.125, mean reward: 0.412 [0.350, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.264, 10.494], loss: 0.003238, mae: 0.062740, mean_q: 0.335748
 90693/100000: episode: 2007, duration: 0.101s, episode steps: 15, steps per second: 149, episode reward: 7.054, mean reward: 0.470 [0.374, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.537], loss: 0.003430, mae: 0.065428, mean_q: 0.345573
 90742/100000: episode: 2008, duration: 0.255s, episode steps: 49, steps per second: 193, episode reward: 9.771, mean reward: 0.199 [0.064, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.311, 10.306], loss: 0.003693, mae: 0.064536, mean_q: 0.322511
 90757/100000: episode: 2009, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 6.645, mean reward: 0.443 [0.346, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.545], loss: 0.003342, mae: 0.062753, mean_q: 0.301172
 90770/100000: episode: 2010, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 4.706, mean reward: 0.362 [0.289, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.544], loss: 0.003691, mae: 0.064848, mean_q: 0.343834
 90817/100000: episode: 2011, duration: 0.251s, episode steps: 47, steps per second: 187, episode reward: 16.941, mean reward: 0.360 [0.255, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-1.273, 10.393], loss: 0.003230, mae: 0.061186, mean_q: 0.343773
 90818/100000: episode: 2012, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.468, mean reward: 0.468 [0.468, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.035, 10.562], loss: 0.003058, mae: 0.061848, mean_q: 0.274673
 90875/100000: episode: 2013, duration: 0.293s, episode steps: 57, steps per second: 194, episode reward: 23.622, mean reward: 0.414 [0.199, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.831 [-0.708, 10.333], loss: 0.003478, mae: 0.063684, mean_q: 0.377942
 90876/100000: episode: 2014, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.494, mean reward: 0.494 [0.494, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.574], loss: 0.004083, mae: 0.072739, mean_q: 0.409921
 90889/100000: episode: 2015, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 4.773, mean reward: 0.367 [0.286, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.548], loss: 0.002921, mae: 0.059596, mean_q: 0.336895
 90902/100000: episode: 2016, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 5.856, mean reward: 0.450 [0.298, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.372, 10.560], loss: 0.003270, mae: 0.061977, mean_q: 0.396689
 90917/100000: episode: 2017, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 5.540, mean reward: 0.369 [0.281, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.566, 10.457], loss: 0.003051, mae: 0.059769, mean_q: 0.320101
 90966/100000: episode: 2018, duration: 0.251s, episode steps: 49, steps per second: 195, episode reward: 14.912, mean reward: 0.304 [0.142, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-1.093, 10.318], loss: 0.003439, mae: 0.063347, mean_q: 0.386361
 90981/100000: episode: 2019, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 5.881, mean reward: 0.392 [0.349, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.201, 10.477], loss: 0.003475, mae: 0.064322, mean_q: 0.383906
 91030/100000: episode: 2020, duration: 0.271s, episode steps: 49, steps per second: 181, episode reward: 11.870, mean reward: 0.242 [0.067, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.990, 10.100], loss: 0.003372, mae: 0.063470, mean_q: 0.402150
 91077/100000: episode: 2021, duration: 0.251s, episode steps: 47, steps per second: 187, episode reward: 14.770, mean reward: 0.314 [0.160, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.319, 10.288], loss: 0.003397, mae: 0.063402, mean_q: 0.415956
 91090/100000: episode: 2022, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 3.813, mean reward: 0.293 [0.241, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.424], loss: 0.003629, mae: 0.067541, mean_q: 0.490585
 91139/100000: episode: 2023, duration: 0.255s, episode steps: 49, steps per second: 193, episode reward: 9.048, mean reward: 0.185 [0.029, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.305, 10.100], loss: 0.003277, mae: 0.062524, mean_q: 0.430685
 91149/100000: episode: 2024, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 4.151, mean reward: 0.415 [0.370, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.529], loss: 0.003247, mae: 0.059885, mean_q: 0.412582
 91164/100000: episode: 2025, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 5.174, mean reward: 0.345 [0.265, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.529, 10.490], loss: 0.003659, mae: 0.065635, mean_q: 0.451351
 91174/100000: episode: 2026, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 3.865, mean reward: 0.386 [0.291, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.426], loss: 0.003151, mae: 0.060619, mean_q: 0.430729
 91187/100000: episode: 2027, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 5.454, mean reward: 0.420 [0.336, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.608, 10.553], loss: 0.002879, mae: 0.058182, mean_q: 0.396999
 91202/100000: episode: 2028, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 7.388, mean reward: 0.493 [0.429, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.890, 10.573], loss: 0.002656, mae: 0.055613, mean_q: 0.417868
 91203/100000: episode: 2029, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 0.496, mean reward: 0.496 [0.496, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.587], loss: 0.003597, mae: 0.064911, mean_q: 0.459630
 91216/100000: episode: 2030, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 3.837, mean reward: 0.295 [0.226, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.329], loss: 0.003246, mae: 0.061228, mean_q: 0.421536
 91231/100000: episode: 2031, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 6.015, mean reward: 0.401 [0.338, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.583], loss: 0.003178, mae: 0.061507, mean_q: 0.450864
 91246/100000: episode: 2032, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 4.932, mean reward: 0.329 [0.225, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.360], loss: 0.003075, mae: 0.060834, mean_q: 0.455127
 91297/100000: episode: 2033, duration: 0.274s, episode steps: 51, steps per second: 186, episode reward: 15.429, mean reward: 0.303 [0.088, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.733, 10.273], loss: 0.003154, mae: 0.061070, mean_q: 0.453071
 91312/100000: episode: 2034, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 6.708, mean reward: 0.447 [0.378, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.828, 10.476], loss: 0.003743, mae: 0.065846, mean_q: 0.452222
 91327/100000: episode: 2035, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 5.181, mean reward: 0.345 [0.308, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.275, 10.395], loss: 0.002859, mae: 0.058905, mean_q: 0.454987
 91342/100000: episode: 2036, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 6.054, mean reward: 0.404 [0.343, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.491, 10.358], loss: 0.003172, mae: 0.061097, mean_q: 0.466337
 91343/100000: episode: 2037, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.456, mean reward: 0.456 [0.456, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.035, 10.553], loss: 0.002856, mae: 0.063706, mean_q: 0.552426
 91356/100000: episode: 2038, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 4.605, mean reward: 0.354 [0.251, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.440], loss: 0.003205, mae: 0.062168, mean_q: 0.485739
 91366/100000: episode: 2039, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 4.826, mean reward: 0.483 [0.415, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.577], loss: 0.004381, mae: 0.071775, mean_q: 0.452736
 91376/100000: episode: 2040, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 3.750, mean reward: 0.375 [0.324, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.045, 10.384], loss: 0.003931, mae: 0.071344, mean_q: 0.450121
 91389/100000: episode: 2041, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 4.830, mean reward: 0.372 [0.342, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.893, 10.489], loss: 0.003874, mae: 0.069098, mean_q: 0.426095
 91399/100000: episode: 2042, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 4.344, mean reward: 0.434 [0.395, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.183, 10.532], loss: 0.004283, mae: 0.074625, mean_q: 0.480448
 91412/100000: episode: 2043, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 6.116, mean reward: 0.470 [0.357, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.620], loss: 0.003240, mae: 0.063385, mean_q: 0.477731
 91461/100000: episode: 2044, duration: 0.250s, episode steps: 49, steps per second: 196, episode reward: 9.298, mean reward: 0.190 [0.054, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-1.383, 10.100], loss: 0.003447, mae: 0.064099, mean_q: 0.486424
 91508/100000: episode: 2045, duration: 0.249s, episode steps: 47, steps per second: 189, episode reward: 13.516, mean reward: 0.288 [0.076, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.510, 10.260], loss: 0.003171, mae: 0.062345, mean_q: 0.482084
 91523/100000: episode: 2046, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 3.813, mean reward: 0.254 [0.120, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.273], loss: 0.004315, mae: 0.072186, mean_q: 0.504086
 91570/100000: episode: 2047, duration: 0.264s, episode steps: 47, steps per second: 178, episode reward: 19.572, mean reward: 0.416 [0.299, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.344, 10.455], loss: 0.003236, mae: 0.062719, mean_q: 0.499393
 91621/100000: episode: 2048, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 17.055, mean reward: 0.334 [0.153, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.392, 10.332], loss: 0.003641, mae: 0.066277, mean_q: 0.514268
 91634/100000: episode: 2049, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 5.042, mean reward: 0.388 [0.315, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.569], loss: 0.004248, mae: 0.070668, mean_q: 0.527621
 91644/100000: episode: 2050, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 3.719, mean reward: 0.372 [0.340, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.465], loss: 0.003641, mae: 0.066545, mean_q: 0.523729
 91659/100000: episode: 2051, duration: 0.071s, episode steps: 15, steps per second: 210, episode reward: 7.268, mean reward: 0.485 [0.430, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.324, 10.444], loss: 0.003419, mae: 0.065139, mean_q: 0.527052
 91660/100000: episode: 2052, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.473, mean reward: 0.473 [0.473, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.567], loss: 0.003871, mae: 0.069875, mean_q: 0.591206
 91717/100000: episode: 2053, duration: 0.306s, episode steps: 57, steps per second: 187, episode reward: 21.981, mean reward: 0.386 [0.186, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.826 [-0.597, 10.233], loss: 0.003239, mae: 0.062870, mean_q: 0.536006
 91730/100000: episode: 2054, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 5.535, mean reward: 0.426 [0.268, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.489], loss: 0.002907, mae: 0.059268, mean_q: 0.509823
 91743/100000: episode: 2055, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 4.123, mean reward: 0.317 [0.262, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.337], loss: 0.003736, mae: 0.066588, mean_q: 0.535617
 91794/100000: episode: 2056, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 18.450, mean reward: 0.362 [0.062, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.402, 10.210], loss: 0.003276, mae: 0.062296, mean_q: 0.528975
 91795/100000: episode: 2057, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.457, mean reward: 0.457 [0.457, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.528], loss: 0.004804, mae: 0.064842, mean_q: 0.485733
 91808/100000: episode: 2058, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 4.146, mean reward: 0.319 [0.275, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.388], loss: 0.003401, mae: 0.063646, mean_q: 0.528582
 91821/100000: episode: 2059, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 4.769, mean reward: 0.367 [0.304, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.531], loss: 0.003148, mae: 0.061618, mean_q: 0.521555
 91831/100000: episode: 2060, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 4.126, mean reward: 0.413 [0.349, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.162, 10.451], loss: 0.002946, mae: 0.058731, mean_q: 0.550559
 91846/100000: episode: 2061, duration: 0.093s, episode steps: 15, steps per second: 162, episode reward: 7.116, mean reward: 0.474 [0.334, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.552], loss: 0.002976, mae: 0.061693, mean_q: 0.544366
 91893/100000: episode: 2062, duration: 0.236s, episode steps: 47, steps per second: 199, episode reward: 13.874, mean reward: 0.295 [0.099, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.519, 10.186], loss: 0.003106, mae: 0.061471, mean_q: 0.531895
 91903/100000: episode: 2063, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 4.512, mean reward: 0.451 [0.394, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.560], loss: 0.002653, mae: 0.057601, mean_q: 0.536120
[Info] FALSIFICATION!
 91913/100000: episode: 2064, duration: 0.067s, episode steps: 10, steps per second: 149, episode reward: 14.362, mean reward: 1.436 [0.381, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.169, 10.784], loss: 0.003030, mae: 0.060166, mean_q: 0.535511
 92013/100000: episode: 2065, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.290, mean reward: -0.163 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.470, 10.098], loss: 0.043290, mae: 0.094066, mean_q: 0.541084
 92113/100000: episode: 2066, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.305, mean reward: -0.153 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.982, 10.098], loss: 0.003689, mae: 0.066339, mean_q: 0.507214
 92213/100000: episode: 2067, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.653, mean reward: -0.177 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.847, 10.209], loss: 0.017645, mae: 0.079969, mean_q: 0.503661
 92313/100000: episode: 2068, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.414, mean reward: -0.174 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.640, 10.240], loss: 0.004037, mae: 0.065254, mean_q: 0.490016
 92413/100000: episode: 2069, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.081, mean reward: -0.181 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.827, 10.098], loss: 0.029604, mae: 0.084994, mean_q: 0.477238
 92513/100000: episode: 2070, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.490, mean reward: -0.185 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.916, 10.144], loss: 0.004210, mae: 0.065160, mean_q: 0.451041
 92613/100000: episode: 2071, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.225, mean reward: -0.172 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.582, 10.098], loss: 0.003532, mae: 0.063748, mean_q: 0.432473
 92713/100000: episode: 2072, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.316, mean reward: -0.173 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.009, 10.160], loss: 0.003582, mae: 0.063082, mean_q: 0.418949
 92813/100000: episode: 2073, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -17.219, mean reward: -0.172 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.040, 10.449], loss: 0.016212, mae: 0.070336, mean_q: 0.395483
 92913/100000: episode: 2074, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.011, mean reward: -0.170 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.778, 10.098], loss: 0.003757, mae: 0.066026, mean_q: 0.367751
 93013/100000: episode: 2075, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -13.705, mean reward: -0.137 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.274, 10.098], loss: 0.004727, mae: 0.068646, mean_q: 0.346327
 93113/100000: episode: 2076, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -14.378, mean reward: -0.144 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.328, 10.354], loss: 0.003508, mae: 0.062751, mean_q: 0.339768
 93213/100000: episode: 2077, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.499, mean reward: -0.185 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.542, 10.098], loss: 0.003093, mae: 0.059759, mean_q: 0.313127
 93313/100000: episode: 2078, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.556, mean reward: -0.186 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.764, 10.328], loss: 0.028413, mae: 0.077922, mean_q: 0.300028
 93413/100000: episode: 2079, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -14.903, mean reward: -0.149 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.792, 10.233], loss: 0.003281, mae: 0.061050, mean_q: 0.306342
 93513/100000: episode: 2080, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.909, mean reward: -0.169 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.262, 10.354], loss: 0.028530, mae: 0.078155, mean_q: 0.298680
 93613/100000: episode: 2081, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -10.670, mean reward: -0.107 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.184, 10.202], loss: 0.003924, mae: 0.065804, mean_q: 0.267094
 93713/100000: episode: 2082, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -12.204, mean reward: -0.122 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.488, 10.098], loss: 0.015942, mae: 0.069389, mean_q: 0.253988
 93813/100000: episode: 2083, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.250, mean reward: -0.183 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.987, 10.098], loss: 0.016459, mae: 0.071676, mean_q: 0.219001
 93913/100000: episode: 2084, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -13.640, mean reward: -0.136 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.429, 10.370], loss: 0.003211, mae: 0.058623, mean_q: 0.214855
 94013/100000: episode: 2085, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -8.953, mean reward: -0.090 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.017, 10.226], loss: 0.002921, mae: 0.057083, mean_q: 0.206530
 94113/100000: episode: 2086, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.511, mean reward: -0.175 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.680, 10.098], loss: 0.016632, mae: 0.072594, mean_q: 0.197745
 94213/100000: episode: 2087, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -13.087, mean reward: -0.131 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.077, 10.374], loss: 0.041385, mae: 0.087046, mean_q: 0.183214
 94313/100000: episode: 2088, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.865, mean reward: -0.179 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.436, 10.278], loss: 0.016422, mae: 0.071949, mean_q: 0.140616
 94413/100000: episode: 2089, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.440, mean reward: -0.174 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.576, 10.098], loss: 0.017386, mae: 0.072174, mean_q: 0.161617
 94513/100000: episode: 2090, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.205, mean reward: -0.182 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.090, 10.138], loss: 0.005179, mae: 0.069315, mean_q: 0.125156
 94613/100000: episode: 2091, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.331, mean reward: -0.193 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.868, 10.206], loss: 0.003285, mae: 0.059680, mean_q: 0.105889
 94713/100000: episode: 2092, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.044, mean reward: -0.180 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.042, 10.098], loss: 0.003176, mae: 0.058280, mean_q: 0.073114
 94813/100000: episode: 2093, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -14.854, mean reward: -0.149 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.393, 10.098], loss: 0.003106, mae: 0.057654, mean_q: 0.105749
 94913/100000: episode: 2094, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -19.592, mean reward: -0.196 [-1.000, 0.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.485, 10.098], loss: 0.016333, mae: 0.072473, mean_q: 0.052022
 95013/100000: episode: 2095, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -16.631, mean reward: -0.166 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.766, 10.098], loss: 0.016152, mae: 0.071118, mean_q: 0.058470
 95113/100000: episode: 2096, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -13.548, mean reward: -0.135 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.777, 10.166], loss: 0.003037, mae: 0.056550, mean_q: 0.038116
 95213/100000: episode: 2097, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -15.264, mean reward: -0.153 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.989, 10.098], loss: 0.003117, mae: 0.056040, mean_q: 0.002005
 95313/100000: episode: 2098, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -11.287, mean reward: -0.113 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.035, 10.254], loss: 0.003016, mae: 0.055695, mean_q: -0.007091
 95413/100000: episode: 2099, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -18.385, mean reward: -0.184 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.810, 10.317], loss: 0.042193, mae: 0.081728, mean_q: -0.020413
 95513/100000: episode: 2100, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -20.921, mean reward: -0.209 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.919, 10.098], loss: 0.017056, mae: 0.078071, mean_q: -0.015791
 95613/100000: episode: 2101, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.712, mean reward: -0.177 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.780, 10.098], loss: 0.002696, mae: 0.052397, mean_q: -0.044966
 95713/100000: episode: 2102, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.595, mean reward: -0.176 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.040, 10.098], loss: 0.002759, mae: 0.053027, mean_q: -0.091292
 95813/100000: episode: 2103, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -20.502, mean reward: -0.205 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.919, 10.217], loss: 0.002953, mae: 0.054072, mean_q: -0.100464
 95913/100000: episode: 2104, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.959, mean reward: -0.200 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.002, 10.098], loss: 0.002705, mae: 0.052263, mean_q: -0.131778
 96013/100000: episode: 2105, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.980, mean reward: -0.170 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.429, 10.105], loss: 0.002661, mae: 0.052412, mean_q: -0.151498
 96113/100000: episode: 2106, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.182, mean reward: -0.162 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.501, 10.282], loss: 0.002709, mae: 0.052331, mean_q: -0.160652
 96213/100000: episode: 2107, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.377, mean reward: -0.184 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.536, 10.286], loss: 0.015391, mae: 0.060893, mean_q: -0.164844
 96313/100000: episode: 2108, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -13.608, mean reward: -0.136 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.766, 10.330], loss: 0.043380, mae: 0.098911, mean_q: -0.202973
 96413/100000: episode: 2109, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.445, mean reward: -0.174 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.549, 10.098], loss: 0.004659, mae: 0.066259, mean_q: -0.229775
 96513/100000: episode: 2110, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -15.134, mean reward: -0.151 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.446, 10.134], loss: 0.002677, mae: 0.051245, mean_q: -0.233028
 96613/100000: episode: 2111, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.059, mean reward: -0.161 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.572, 10.098], loss: 0.014107, mae: 0.053227, mean_q: -0.274257
 96713/100000: episode: 2112, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.033, mean reward: -0.140 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.417, 10.201], loss: 0.026387, mae: 0.073965, mean_q: -0.287924
 96813/100000: episode: 2113, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.760, mean reward: -0.188 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.553, 10.175], loss: 0.002622, mae: 0.050738, mean_q: -0.293312
 96913/100000: episode: 2114, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -20.357, mean reward: -0.204 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.001, 10.216], loss: 0.014683, mae: 0.061362, mean_q: -0.315556
 97013/100000: episode: 2115, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.022, mean reward: -0.180 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.010, 10.190], loss: 0.002401, mae: 0.048175, mean_q: -0.315164
 97113/100000: episode: 2116, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.498, mean reward: -0.185 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.548, 10.098], loss: 0.002449, mae: 0.048607, mean_q: -0.314015
 97213/100000: episode: 2117, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -20.027, mean reward: -0.200 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.155, 10.098], loss: 0.002494, mae: 0.049799, mean_q: -0.308174
 97313/100000: episode: 2118, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -15.908, mean reward: -0.159 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.637, 10.098], loss: 0.002546, mae: 0.049509, mean_q: -0.310267
 97413/100000: episode: 2119, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.690, mean reward: -0.197 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.168, 10.098], loss: 0.002585, mae: 0.049940, mean_q: -0.285803
 97513/100000: episode: 2120, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.875, mean reward: -0.179 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.625, 10.098], loss: 0.002477, mae: 0.049045, mean_q: -0.305463
 97613/100000: episode: 2121, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.143, mean reward: -0.161 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.265, 10.098], loss: 0.002565, mae: 0.050119, mean_q: -0.311166
 97713/100000: episode: 2122, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -19.150, mean reward: -0.191 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.623, 10.181], loss: 0.002378, mae: 0.047797, mean_q: -0.344861
 97813/100000: episode: 2123, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.765, mean reward: -0.178 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.029, 10.225], loss: 0.002305, mae: 0.047932, mean_q: -0.339287
 97913/100000: episode: 2124, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.248, mean reward: -0.172 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.564, 10.098], loss: 0.002511, mae: 0.050132, mean_q: -0.291981
 98013/100000: episode: 2125, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.988, mean reward: -0.170 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.641, 10.160], loss: 0.002506, mae: 0.050146, mean_q: -0.303835
 98113/100000: episode: 2126, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -18.689, mean reward: -0.187 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.879, 10.102], loss: 0.002357, mae: 0.048532, mean_q: -0.325366
 98213/100000: episode: 2127, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.438, mean reward: -0.184 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.284, 10.098], loss: 0.002320, mae: 0.047409, mean_q: -0.340555
 98313/100000: episode: 2128, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.963, mean reward: -0.200 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.896, 10.103], loss: 0.002545, mae: 0.050506, mean_q: -0.295684
 98413/100000: episode: 2129, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.837, mean reward: -0.178 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.080, 10.098], loss: 0.004007, mae: 0.062684, mean_q: -0.322911
 98513/100000: episode: 2130, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.701, mean reward: -0.177 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.922, 10.098], loss: 0.002436, mae: 0.049887, mean_q: -0.317360
 98613/100000: episode: 2131, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.341, mean reward: -0.173 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.847, 10.122], loss: 0.002591, mae: 0.050213, mean_q: -0.330296
 98713/100000: episode: 2132, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.101, mean reward: -0.181 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.821, 10.146], loss: 0.002461, mae: 0.049611, mean_q: -0.336699
 98813/100000: episode: 2133, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -19.102, mean reward: -0.191 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.110, 10.098], loss: 0.002512, mae: 0.049307, mean_q: -0.303391
 98913/100000: episode: 2134, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.274, mean reward: -0.173 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.370, 10.098], loss: 0.002537, mae: 0.049154, mean_q: -0.348092
 99013/100000: episode: 2135, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -16.734, mean reward: -0.167 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.742, 10.204], loss: 0.002645, mae: 0.050652, mean_q: -0.330803
 99113/100000: episode: 2136, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.568, mean reward: -0.156 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.467, 10.098], loss: 0.002547, mae: 0.049629, mean_q: -0.321975
 99213/100000: episode: 2137, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.303, mean reward: -0.193 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.165, 10.147], loss: 0.002453, mae: 0.048336, mean_q: -0.336426
 99313/100000: episode: 2138, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.499, mean reward: -0.185 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.616, 10.098], loss: 0.002501, mae: 0.049656, mean_q: -0.296496
 99413/100000: episode: 2139, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -13.883, mean reward: -0.139 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.846, 10.279], loss: 0.002568, mae: 0.049253, mean_q: -0.318879
 99513/100000: episode: 2140, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.652, mean reward: -0.177 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.278, 10.247], loss: 0.002393, mae: 0.048102, mean_q: -0.331308
 99613/100000: episode: 2141, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.638, mean reward: -0.156 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.952, 10.098], loss: 0.002502, mae: 0.048748, mean_q: -0.349565
 99713/100000: episode: 2142, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -9.541, mean reward: -0.095 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.528, 10.513], loss: 0.002391, mae: 0.048264, mean_q: -0.312860
 99813/100000: episode: 2143, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.527, mean reward: -0.175 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.709, 10.098], loss: 0.002594, mae: 0.049489, mean_q: -0.334015
 99913/100000: episode: 2144, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.590, mean reward: -0.186 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.949, 10.098], loss: 0.002364, mae: 0.047209, mean_q: -0.333516
done, took 607.996 seconds
[Info] End Importance Splitting. Falsification occurred 5 times.
