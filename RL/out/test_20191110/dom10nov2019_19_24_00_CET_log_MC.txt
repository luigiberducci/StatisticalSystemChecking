Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.179s, episode steps: 100, steps per second: 558, episode reward: -14.066, mean reward: -0.141 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.623, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.069s, episode steps: 100, steps per second: 1459, episode reward: -17.859, mean reward: -0.179 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.856, 10.237], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.059s, episode steps: 100, steps per second: 1689, episode reward: -20.748, mean reward: -0.207 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.869, 10.115], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.059s, episode steps: 100, steps per second: 1704, episode reward: -16.666, mean reward: -0.167 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.538, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.059s, episode steps: 100, steps per second: 1693, episode reward: -18.506, mean reward: -0.185 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.708, 10.137], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.237s, episode steps: 100, steps per second: 81, episode reward: -15.619, mean reward: -0.156 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.085, 10.117], loss: 0.043700, mae: 0.202917, mean_q: -0.049072
   700/100000: episode: 7, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -12.646, mean reward: -0.126 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.484, 10.295], loss: 0.017650, mae: 0.131405, mean_q: -0.169116
   800/100000: episode: 8, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -15.576, mean reward: -0.156 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.912, 10.204], loss: 0.013977, mae: 0.111897, mean_q: -0.220210
   900/100000: episode: 9, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -14.834, mean reward: -0.148 [-1.000, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.983, 10.186], loss: 0.012824, mae: 0.106552, mean_q: -0.265498
  1000/100000: episode: 10, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -14.168, mean reward: -0.142 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.730, 10.220], loss: 0.011007, mae: 0.096195, mean_q: -0.296412
  1100/100000: episode: 11, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.971, mean reward: -0.160 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.209, 10.226], loss: 0.009777, mae: 0.090375, mean_q: -0.309463
  1200/100000: episode: 12, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -13.735, mean reward: -0.137 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.670, 10.098], loss: 0.009444, mae: 0.088113, mean_q: -0.290483
  1300/100000: episode: 13, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -17.291, mean reward: -0.173 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.200, 10.204], loss: 0.010336, mae: 0.090491, mean_q: -0.329690
  1400/100000: episode: 14, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.540, mean reward: -0.175 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.274, 10.136], loss: 0.009496, mae: 0.088717, mean_q: -0.311906
  1500/100000: episode: 15, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -9.645, mean reward: -0.096 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.587, 10.098], loss: 0.009994, mae: 0.090380, mean_q: -0.330633
  1600/100000: episode: 16, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.184, mean reward: -0.192 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.901, 10.243], loss: 0.009785, mae: 0.090172, mean_q: -0.331834
  1700/100000: episode: 17, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -16.525, mean reward: -0.165 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.980, 10.098], loss: 0.008923, mae: 0.084795, mean_q: -0.301182
  1800/100000: episode: 18, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -14.912, mean reward: -0.149 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.900, 10.098], loss: 0.008843, mae: 0.084673, mean_q: -0.322319
  1900/100000: episode: 19, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -15.443, mean reward: -0.154 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.354, 10.400], loss: 0.007757, mae: 0.079795, mean_q: -0.311098
  2000/100000: episode: 20, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -14.424, mean reward: -0.144 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.907, 10.098], loss: 0.006788, mae: 0.075688, mean_q: -0.326319
  2100/100000: episode: 21, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -15.213, mean reward: -0.152 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.383, 10.098], loss: 0.007188, mae: 0.076486, mean_q: -0.330881
  2200/100000: episode: 22, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -19.956, mean reward: -0.200 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.814, 10.149], loss: 0.006555, mae: 0.073650, mean_q: -0.292217
  2300/100000: episode: 23, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.904, mean reward: -0.169 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.660, 10.120], loss: 0.007350, mae: 0.079953, mean_q: -0.293778
  2400/100000: episode: 24, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.284, mean reward: -0.183 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.636, 10.098], loss: 0.006628, mae: 0.077506, mean_q: -0.269553
  2500/100000: episode: 25, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.397, mean reward: -0.184 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.921, 10.098], loss: 0.007327, mae: 0.075731, mean_q: -0.326158
  2600/100000: episode: 26, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -7.173, mean reward: -0.072 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.417, 10.480], loss: 0.007094, mae: 0.076702, mean_q: -0.306285
  2700/100000: episode: 27, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.546, mean reward: -0.195 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.547, 10.098], loss: 0.006310, mae: 0.073723, mean_q: -0.326015
  2800/100000: episode: 28, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.167, mean reward: -0.162 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.316, 10.125], loss: 0.006512, mae: 0.074453, mean_q: -0.309414
  2900/100000: episode: 29, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -18.461, mean reward: -0.185 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.273, 10.155], loss: 0.006855, mae: 0.072369, mean_q: -0.353923
  3000/100000: episode: 30, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.353, mean reward: -0.184 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.425, 10.098], loss: 0.006925, mae: 0.076344, mean_q: -0.299014
  3100/100000: episode: 31, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -18.314, mean reward: -0.183 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.352, 10.157], loss: 0.006266, mae: 0.072261, mean_q: -0.334715
  3200/100000: episode: 32, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -15.243, mean reward: -0.152 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.278, 10.101], loss: 0.005662, mae: 0.069730, mean_q: -0.320197
  3300/100000: episode: 33, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.300, mean reward: -0.173 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.583, 10.098], loss: 0.006159, mae: 0.072058, mean_q: -0.293225
  3400/100000: episode: 34, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.357, mean reward: -0.174 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.608, 10.182], loss: 0.006561, mae: 0.073453, mean_q: -0.308654
  3500/100000: episode: 35, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.287, mean reward: -0.183 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.887, 10.098], loss: 0.007625, mae: 0.076294, mean_q: -0.364838
  3600/100000: episode: 36, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.403, mean reward: -0.164 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.928, 10.183], loss: 0.007540, mae: 0.076632, mean_q: -0.313457
  3700/100000: episode: 37, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.281, mean reward: -0.173 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.363, 10.324], loss: 0.004386, mae: 0.063769, mean_q: -0.296383
  3800/100000: episode: 38, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -15.753, mean reward: -0.158 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.502, 10.309], loss: 0.005419, mae: 0.068538, mean_q: -0.314038
  3900/100000: episode: 39, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.417, mean reward: -0.164 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.951, 10.279], loss: 0.005966, mae: 0.071674, mean_q: -0.331151
  4000/100000: episode: 40, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.820, mean reward: -0.188 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.952, 10.252], loss: 0.005440, mae: 0.068530, mean_q: -0.314067
  4100/100000: episode: 41, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.570, mean reward: -0.196 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.937, 10.098], loss: 0.004814, mae: 0.066695, mean_q: -0.283471
  4200/100000: episode: 42, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.895, mean reward: -0.179 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.472, 10.098], loss: 0.006445, mae: 0.072592, mean_q: -0.331339
  4300/100000: episode: 43, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -15.040, mean reward: -0.150 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.461, 10.194], loss: 0.005143, mae: 0.063934, mean_q: -0.310408
  4400/100000: episode: 44, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -12.158, mean reward: -0.122 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.841, 10.098], loss: 0.005172, mae: 0.067622, mean_q: -0.271637
  4500/100000: episode: 45, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.138, mean reward: -0.171 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.007, 10.098], loss: 0.005050, mae: 0.065434, mean_q: -0.306419
  4600/100000: episode: 46, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.928, mean reward: -0.189 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.917, 10.098], loss: 0.006014, mae: 0.070958, mean_q: -0.306519
  4700/100000: episode: 47, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.779, mean reward: -0.178 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.104, 10.459], loss: 0.004719, mae: 0.065675, mean_q: -0.343804
  4800/100000: episode: 48, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -18.347, mean reward: -0.183 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.211, 10.282], loss: 0.005382, mae: 0.067928, mean_q: -0.288758
  4900/100000: episode: 49, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.313, mean reward: -0.153 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.807, 10.098], loss: 0.004590, mae: 0.065487, mean_q: -0.291518
  5000/100000: episode: 50, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -16.778, mean reward: -0.168 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.761, 10.098], loss: 0.005737, mae: 0.070407, mean_q: -0.301815
  5100/100000: episode: 51, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -17.326, mean reward: -0.173 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.468, 10.098], loss: 0.004297, mae: 0.064270, mean_q: -0.297309
  5200/100000: episode: 52, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -10.068, mean reward: -0.101 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.689, 10.447], loss: 0.006150, mae: 0.072365, mean_q: -0.301308
  5300/100000: episode: 53, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.492, mean reward: -0.175 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.641, 10.153], loss: 0.004311, mae: 0.064660, mean_q: -0.287712
  5400/100000: episode: 54, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -19.971, mean reward: -0.200 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.772, 10.131], loss: 0.004327, mae: 0.064420, mean_q: -0.308399
  5500/100000: episode: 55, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -12.777, mean reward: -0.128 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.839, 10.333], loss: 0.004403, mae: 0.063969, mean_q: -0.279810
  5600/100000: episode: 56, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.787, mean reward: -0.178 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.919, 10.098], loss: 0.004303, mae: 0.064187, mean_q: -0.285418
  5700/100000: episode: 57, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.652, mean reward: -0.187 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.547, 10.120], loss: 0.004452, mae: 0.064404, mean_q: -0.297542
  5800/100000: episode: 58, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.446, mean reward: -0.184 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.346, 10.232], loss: 0.004136, mae: 0.061927, mean_q: -0.320540
  5900/100000: episode: 59, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -14.297, mean reward: -0.143 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.625, 10.242], loss: 0.004906, mae: 0.067112, mean_q: -0.300344
  6000/100000: episode: 60, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.502, mean reward: -0.165 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.235, 10.245], loss: 0.004806, mae: 0.067581, mean_q: -0.292251
  6100/100000: episode: 61, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.697, mean reward: -0.157 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.830, 10.366], loss: 0.004563, mae: 0.066065, mean_q: -0.278864
  6200/100000: episode: 62, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -15.779, mean reward: -0.158 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.490, 10.412], loss: 0.003625, mae: 0.060018, mean_q: -0.300805
  6300/100000: episode: 63, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -8.201, mean reward: -0.082 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.967, 10.393], loss: 0.004309, mae: 0.065631, mean_q: -0.311676
  6400/100000: episode: 64, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -11.226, mean reward: -0.112 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.770, 10.098], loss: 0.004112, mae: 0.063416, mean_q: -0.325249
  6500/100000: episode: 65, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.292, mean reward: -0.183 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.862, 10.189], loss: 0.003874, mae: 0.062338, mean_q: -0.283396
  6600/100000: episode: 66, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -19.890, mean reward: -0.199 [-1.000, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.318, 10.270], loss: 0.004712, mae: 0.066891, mean_q: -0.282606
  6700/100000: episode: 67, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.211, mean reward: -0.162 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.488, 10.195], loss: 0.005212, mae: 0.066785, mean_q: -0.344715
  6800/100000: episode: 68, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.447, mean reward: -0.174 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.043, 10.098], loss: 0.004058, mae: 0.063635, mean_q: -0.304190
  6900/100000: episode: 69, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -19.179, mean reward: -0.192 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.777, 10.098], loss: 0.004397, mae: 0.065473, mean_q: -0.307948
  7000/100000: episode: 70, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.239, mean reward: -0.182 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.248, 10.098], loss: 0.004574, mae: 0.065788, mean_q: -0.301093
  7100/100000: episode: 71, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.293, mean reward: -0.173 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.900, 10.179], loss: 0.004198, mae: 0.063665, mean_q: -0.306051
  7200/100000: episode: 72, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -11.701, mean reward: -0.117 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.346, 10.137], loss: 0.003858, mae: 0.062279, mean_q: -0.316105
  7300/100000: episode: 73, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.133, mean reward: -0.181 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.427, 10.098], loss: 0.004641, mae: 0.064931, mean_q: -0.316194
  7400/100000: episode: 74, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.934, mean reward: -0.179 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.768, 10.098], loss: 0.004414, mae: 0.065337, mean_q: -0.336520
  7500/100000: episode: 75, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.636, mean reward: -0.186 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.742, 10.124], loss: 0.003751, mae: 0.060789, mean_q: -0.304887
  7600/100000: episode: 76, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.364, mean reward: -0.194 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.644, 10.098], loss: 0.004261, mae: 0.064019, mean_q: -0.297488
  7700/100000: episode: 77, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.647, mean reward: -0.186 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.924, 10.098], loss: 0.004829, mae: 0.066082, mean_q: -0.283719
  7800/100000: episode: 78, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -12.855, mean reward: -0.129 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.743, 10.098], loss: 0.003908, mae: 0.062534, mean_q: -0.328292
  7900/100000: episode: 79, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.833, mean reward: -0.168 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.781, 10.098], loss: 0.004321, mae: 0.064834, mean_q: -0.285433
  8000/100000: episode: 80, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.651, mean reward: -0.147 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.161, 10.098], loss: 0.003864, mae: 0.061192, mean_q: -0.328570
  8100/100000: episode: 81, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -20.462, mean reward: -0.205 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.613, 10.160], loss: 0.003613, mae: 0.060089, mean_q: -0.322926
  8200/100000: episode: 82, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -19.137, mean reward: -0.191 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.576, 10.208], loss: 0.004344, mae: 0.062997, mean_q: -0.290780
  8300/100000: episode: 83, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.053, mean reward: -0.181 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.289, 10.098], loss: 0.005667, mae: 0.069433, mean_q: -0.313910
  8400/100000: episode: 84, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -13.745, mean reward: -0.137 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.296, 10.422], loss: 0.004259, mae: 0.063273, mean_q: -0.295396
  8500/100000: episode: 85, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.003, mean reward: -0.150 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.651, 10.285], loss: 0.003607, mae: 0.060388, mean_q: -0.304583
  8600/100000: episode: 86, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -12.951, mean reward: -0.130 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.914, 10.432], loss: 0.003798, mae: 0.061702, mean_q: -0.300946
  8700/100000: episode: 87, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.467, mean reward: -0.155 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.323, 10.351], loss: 0.003643, mae: 0.060596, mean_q: -0.307596
  8800/100000: episode: 88, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.986, mean reward: -0.160 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.323, 10.098], loss: 0.005382, mae: 0.070064, mean_q: -0.310328
  8900/100000: episode: 89, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.745, mean reward: -0.177 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.501, 10.285], loss: 0.004825, mae: 0.067920, mean_q: -0.317614
  9000/100000: episode: 90, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.300, mean reward: -0.183 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.991, 10.098], loss: 0.004164, mae: 0.063518, mean_q: -0.309169
  9100/100000: episode: 91, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.948, mean reward: -0.169 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.346, 10.098], loss: 0.003412, mae: 0.058717, mean_q: -0.347975
  9200/100000: episode: 92, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -11.278, mean reward: -0.113 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.702, 10.098], loss: 0.003585, mae: 0.061066, mean_q: -0.307496
  9300/100000: episode: 93, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.234, mean reward: -0.182 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.397, 10.262], loss: 0.004651, mae: 0.064224, mean_q: -0.317364
  9400/100000: episode: 94, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -13.190, mean reward: -0.132 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.268, 10.272], loss: 0.005742, mae: 0.072094, mean_q: -0.279372
  9500/100000: episode: 95, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.399, mean reward: -0.144 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.941, 10.347], loss: 0.004145, mae: 0.064095, mean_q: -0.320826
  9600/100000: episode: 96, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.453, mean reward: -0.165 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.498, 10.176], loss: 0.003550, mae: 0.059983, mean_q: -0.334422
  9700/100000: episode: 97, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.854, mean reward: -0.179 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.068, 10.325], loss: 0.003644, mae: 0.061553, mean_q: -0.296271
  9800/100000: episode: 98, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -12.288, mean reward: -0.123 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.652, 10.098], loss: 0.003934, mae: 0.062876, mean_q: -0.294026
  9900/100000: episode: 99, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -16.426, mean reward: -0.164 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.650, 10.176], loss: 0.003519, mae: 0.059510, mean_q: -0.307282
 10000/100000: episode: 100, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.172, mean reward: -0.172 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.758, 10.098], loss: 0.004694, mae: 0.066767, mean_q: -0.292434
 10100/100000: episode: 101, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.113, mean reward: -0.171 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.124, 10.311], loss: 0.003461, mae: 0.059665, mean_q: -0.306602
 10200/100000: episode: 102, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -13.505, mean reward: -0.135 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.328, 10.098], loss: 0.003806, mae: 0.062328, mean_q: -0.323135
 10300/100000: episode: 103, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.649, mean reward: -0.146 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.516, 10.221], loss: 0.003921, mae: 0.063832, mean_q: -0.277455
 10400/100000: episode: 104, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -7.534, mean reward: -0.075 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.348, 10.098], loss: 0.003926, mae: 0.062116, mean_q: -0.350050
 10500/100000: episode: 105, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -16.255, mean reward: -0.163 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.840, 10.098], loss: 0.004375, mae: 0.065225, mean_q: -0.298197
 10600/100000: episode: 106, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.048, mean reward: -0.170 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.707, 10.098], loss: 0.004012, mae: 0.064447, mean_q: -0.296942
 10700/100000: episode: 107, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.095, mean reward: -0.151 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.465, 10.362], loss: 0.004158, mae: 0.064689, mean_q: -0.319117
 10800/100000: episode: 108, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -14.664, mean reward: -0.147 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.164, 10.098], loss: 0.003776, mae: 0.063672, mean_q: -0.272613
 10900/100000: episode: 109, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -13.134, mean reward: -0.131 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.049, 10.098], loss: 0.003692, mae: 0.061128, mean_q: -0.279077
 11000/100000: episode: 110, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.963, mean reward: -0.150 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.338, 10.098], loss: 0.003280, mae: 0.057676, mean_q: -0.339392
 11100/100000: episode: 111, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -20.582, mean reward: -0.206 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.874, 10.125], loss: 0.004555, mae: 0.064990, mean_q: -0.274361
 11200/100000: episode: 112, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.807, mean reward: -0.198 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.171, 10.098], loss: 0.003383, mae: 0.057944, mean_q: -0.327042
 11300/100000: episode: 113, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -19.070, mean reward: -0.191 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.574, 10.098], loss: 0.003911, mae: 0.062670, mean_q: -0.304166
 11400/100000: episode: 114, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -11.746, mean reward: -0.117 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.618, 10.463], loss: 0.003545, mae: 0.059799, mean_q: -0.290263
 11500/100000: episode: 115, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.913, mean reward: -0.179 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.361, 10.263], loss: 0.003437, mae: 0.059155, mean_q: -0.287323
 11600/100000: episode: 116, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.257, mean reward: -0.153 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.181, 10.098], loss: 0.004340, mae: 0.065150, mean_q: -0.306707
 11700/100000: episode: 117, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.913, mean reward: -0.189 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.886, 10.098], loss: 0.003944, mae: 0.063951, mean_q: -0.270611
 11800/100000: episode: 118, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.864, mean reward: -0.189 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.793, 10.098], loss: 0.004209, mae: 0.065058, mean_q: -0.306338
 11900/100000: episode: 119, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -14.542, mean reward: -0.145 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.495, 10.098], loss: 0.003781, mae: 0.060336, mean_q: -0.326906
 12000/100000: episode: 120, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.965, mean reward: -0.180 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.676, 10.098], loss: 0.003554, mae: 0.060845, mean_q: -0.271035
 12100/100000: episode: 121, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -20.152, mean reward: -0.202 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.984, 10.098], loss: 0.004074, mae: 0.065036, mean_q: -0.273919
 12200/100000: episode: 122, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -12.765, mean reward: -0.128 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.012, 10.240], loss: 0.003954, mae: 0.062226, mean_q: -0.305654
 12300/100000: episode: 123, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -19.668, mean reward: -0.197 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.027, 10.230], loss: 0.004449, mae: 0.066137, mean_q: -0.277194
 12400/100000: episode: 124, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.858, mean reward: -0.179 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.767, 10.098], loss: 0.005981, mae: 0.071743, mean_q: -0.314092
 12500/100000: episode: 125, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.477, mean reward: -0.185 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.811, 10.098], loss: 0.004793, mae: 0.064591, mean_q: -0.295003
 12600/100000: episode: 126, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.525, mean reward: -0.185 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.557, 10.224], loss: 0.003356, mae: 0.058180, mean_q: -0.322938
 12700/100000: episode: 127, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -18.589, mean reward: -0.186 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.231, 10.278], loss: 0.003770, mae: 0.061405, mean_q: -0.304331
 12800/100000: episode: 128, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -11.974, mean reward: -0.120 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.785, 10.098], loss: 0.004751, mae: 0.066562, mean_q: -0.280858
 12900/100000: episode: 129, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.536, mean reward: -0.195 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.033, 10.170], loss: 0.004336, mae: 0.063919, mean_q: -0.262625
 13000/100000: episode: 130, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.699, mean reward: -0.177 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.821, 10.374], loss: 0.004020, mae: 0.062005, mean_q: -0.279856
 13100/100000: episode: 131, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -9.977, mean reward: -0.100 [-1.000, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.472, 10.098], loss: 0.004023, mae: 0.063933, mean_q: -0.270957
 13200/100000: episode: 132, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.654, mean reward: -0.187 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.942, 10.131], loss: 0.003769, mae: 0.062035, mean_q: -0.306418
 13300/100000: episode: 133, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -12.122, mean reward: -0.121 [-1.000, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.346, 10.235], loss: 0.003592, mae: 0.060413, mean_q: -0.306543
 13400/100000: episode: 134, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.489, mean reward: -0.155 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.862, 10.186], loss: 0.003748, mae: 0.061893, mean_q: -0.298745
 13500/100000: episode: 135, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.995, mean reward: -0.160 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.632, 10.249], loss: 0.003766, mae: 0.061948, mean_q: -0.309077
 13600/100000: episode: 136, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -10.906, mean reward: -0.109 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.590, 10.098], loss: 0.003667, mae: 0.058899, mean_q: -0.322629
 13700/100000: episode: 137, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.186, mean reward: -0.172 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.926, 10.098], loss: 0.003918, mae: 0.063784, mean_q: -0.287281
 13800/100000: episode: 138, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.338, mean reward: -0.183 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.753, 10.123], loss: 0.004023, mae: 0.064154, mean_q: -0.304055
 13900/100000: episode: 139, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -14.695, mean reward: -0.147 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.754, 10.475], loss: 0.003842, mae: 0.061662, mean_q: -0.292893
 14000/100000: episode: 140, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.761, mean reward: -0.168 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.832, 10.098], loss: 0.004441, mae: 0.063621, mean_q: -0.297203
 14100/100000: episode: 141, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.155, mean reward: -0.182 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.306, 10.186], loss: 0.004098, mae: 0.062458, mean_q: -0.309456
 14200/100000: episode: 142, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.949, mean reward: -0.189 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.459, 10.114], loss: 0.003659, mae: 0.061851, mean_q: -0.253500
 14300/100000: episode: 143, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.316, mean reward: -0.183 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.862, 10.098], loss: 0.003798, mae: 0.060487, mean_q: -0.280493
 14400/100000: episode: 144, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.523, mean reward: -0.185 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.394, 10.098], loss: 0.004308, mae: 0.064615, mean_q: -0.314472
 14500/100000: episode: 145, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -20.200, mean reward: -0.202 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.350, 10.152], loss: 0.003557, mae: 0.059687, mean_q: -0.288280
 14600/100000: episode: 146, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.905, mean reward: -0.179 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.281, 10.114], loss: 0.003653, mae: 0.060152, mean_q: -0.330444
 14700/100000: episode: 147, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.058, mean reward: -0.171 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.427, 10.276], loss: 0.003686, mae: 0.060847, mean_q: -0.282388
 14800/100000: episode: 148, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.609, mean reward: -0.176 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.631, 10.107], loss: 0.004530, mae: 0.065805, mean_q: -0.296009
 14900/100000: episode: 149, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -17.274, mean reward: -0.173 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.708, 10.115], loss: 0.003479, mae: 0.059833, mean_q: -0.308191
 15000/100000: episode: 150, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.276, mean reward: -0.193 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.725, 10.297], loss: 0.003838, mae: 0.061694, mean_q: -0.308403
 15100/100000: episode: 151, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.170, mean reward: -0.182 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.475, 10.098], loss: 0.003144, mae: 0.056308, mean_q: -0.314066
 15200/100000: episode: 152, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.335, mean reward: -0.183 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.629, 10.224], loss: 0.003191, mae: 0.057412, mean_q: -0.311243
 15300/100000: episode: 153, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.721, mean reward: -0.177 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.175, 10.098], loss: 0.003146, mae: 0.056848, mean_q: -0.318509
 15400/100000: episode: 154, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -12.923, mean reward: -0.129 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.347, 10.262], loss: 0.003290, mae: 0.058027, mean_q: -0.299702
 15500/100000: episode: 155, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -12.219, mean reward: -0.122 [-1.000, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.841, 10.450], loss: 0.003368, mae: 0.057953, mean_q: -0.295590
 15600/100000: episode: 156, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -10.886, mean reward: -0.109 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.453, 10.421], loss: 0.003507, mae: 0.057972, mean_q: -0.336504
 15700/100000: episode: 157, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -20.219, mean reward: -0.202 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.511, 10.098], loss: 0.003448, mae: 0.058384, mean_q: -0.329323
 15800/100000: episode: 158, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.679, mean reward: -0.187 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.190, 10.098], loss: 0.003386, mae: 0.058290, mean_q: -0.324133
 15900/100000: episode: 159, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.359, mean reward: -0.154 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.391, 10.098], loss: 0.003102, mae: 0.055870, mean_q: -0.317336
 16000/100000: episode: 160, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.832, mean reward: -0.168 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.853, 10.256], loss: 0.003184, mae: 0.056492, mean_q: -0.315778
 16100/100000: episode: 161, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -9.636, mean reward: -0.096 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.708, 10.098], loss: 0.003607, mae: 0.062737, mean_q: -0.304199
 16200/100000: episode: 162, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.543, mean reward: -0.165 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.831, 10.196], loss: 0.003500, mae: 0.059006, mean_q: -0.330453
 16300/100000: episode: 163, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.216, mean reward: -0.172 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.607, 10.360], loss: 0.004435, mae: 0.063969, mean_q: -0.308176
 16400/100000: episode: 164, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -16.995, mean reward: -0.170 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.245, 10.122], loss: 0.003510, mae: 0.059790, mean_q: -0.286326
 16500/100000: episode: 165, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -13.847, mean reward: -0.138 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.898, 10.098], loss: 0.003319, mae: 0.057537, mean_q: -0.324515
 16600/100000: episode: 166, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.217, mean reward: -0.182 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.273, 10.098], loss: 0.002991, mae: 0.055831, mean_q: -0.291173
 16700/100000: episode: 167, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.088, mean reward: -0.161 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.523, 10.250], loss: 0.003062, mae: 0.055846, mean_q: -0.316073
 16800/100000: episode: 168, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -15.804, mean reward: -0.158 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.232, 10.098], loss: 0.003075, mae: 0.055253, mean_q: -0.312634
 16900/100000: episode: 169, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.618, mean reward: -0.156 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.818, 10.098], loss: 0.003521, mae: 0.059720, mean_q: -0.294499
 17000/100000: episode: 170, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.641, mean reward: -0.186 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.965, 10.125], loss: 0.005961, mae: 0.067016, mean_q: -0.311396
 17100/100000: episode: 171, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.402, mean reward: -0.194 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.623, 10.098], loss: 0.003102, mae: 0.056471, mean_q: -0.308181
 17200/100000: episode: 172, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.437, mean reward: -0.164 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.641, 10.296], loss: 0.003412, mae: 0.057344, mean_q: -0.327075
 17300/100000: episode: 173, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.628, mean reward: -0.156 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.984, 10.252], loss: 0.004002, mae: 0.061271, mean_q: -0.286261
 17400/100000: episode: 174, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.702, mean reward: -0.147 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.670, 10.378], loss: 0.003084, mae: 0.055711, mean_q: -0.331195
 17500/100000: episode: 175, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.329, mean reward: -0.173 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.929, 10.098], loss: 0.003273, mae: 0.057626, mean_q: -0.317452
 17600/100000: episode: 176, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.648, mean reward: -0.186 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.429, 10.098], loss: 0.002947, mae: 0.054786, mean_q: -0.315221
 17700/100000: episode: 177, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.321, mean reward: -0.143 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.234, 10.098], loss: 0.003286, mae: 0.057325, mean_q: -0.294592
 17800/100000: episode: 178, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.295, mean reward: -0.153 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.585, 10.098], loss: 0.003352, mae: 0.058559, mean_q: -0.297196
 17900/100000: episode: 179, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.951, mean reward: -0.160 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.005, 10.377], loss: 0.003003, mae: 0.054891, mean_q: -0.320391
 18000/100000: episode: 180, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -13.973, mean reward: -0.140 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.698, 10.098], loss: 0.003155, mae: 0.057035, mean_q: -0.315373
 18100/100000: episode: 181, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.074, mean reward: -0.181 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.159, 10.098], loss: 0.002935, mae: 0.054264, mean_q: -0.325001
 18200/100000: episode: 182, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.596, mean reward: -0.166 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.548, 10.176], loss: 0.003250, mae: 0.057757, mean_q: -0.291272
 18300/100000: episode: 183, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.030, mean reward: -0.170 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.005, 10.182], loss: 0.003548, mae: 0.058012, mean_q: -0.310762
 18400/100000: episode: 184, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.552, mean reward: -0.186 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.539, 10.240], loss: 0.003076, mae: 0.057049, mean_q: -0.295113
 18500/100000: episode: 185, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -12.359, mean reward: -0.124 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.866, 10.098], loss: 0.002936, mae: 0.054440, mean_q: -0.313342
 18600/100000: episode: 186, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -12.411, mean reward: -0.124 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.427, 10.226], loss: 0.003028, mae: 0.056297, mean_q: -0.308251
 18700/100000: episode: 187, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.483, mean reward: -0.165 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.427, 10.098], loss: 0.003064, mae: 0.055940, mean_q: -0.305391
 18800/100000: episode: 188, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.812, mean reward: -0.158 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.807, 10.241], loss: 0.003128, mae: 0.055863, mean_q: -0.309103
 18900/100000: episode: 189, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.458, mean reward: -0.165 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.893, 10.176], loss: 0.002984, mae: 0.054594, mean_q: -0.337300
 19000/100000: episode: 190, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -17.197, mean reward: -0.172 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.625, 10.098], loss: 0.003002, mae: 0.055155, mean_q: -0.335105
 19100/100000: episode: 191, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.701, mean reward: -0.167 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.686, 10.098], loss: 0.002888, mae: 0.053779, mean_q: -0.317346
 19200/100000: episode: 192, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -11.588, mean reward: -0.116 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.449, 10.434], loss: 0.002870, mae: 0.053591, mean_q: -0.347953
 19300/100000: episode: 193, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.604, mean reward: -0.186 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.268, 10.280], loss: 0.002818, mae: 0.053551, mean_q: -0.339516
 19400/100000: episode: 194, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.286, mean reward: -0.193 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.759, 10.098], loss: 0.003014, mae: 0.055627, mean_q: -0.314614
 19500/100000: episode: 195, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.317, mean reward: -0.193 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.640, 10.098], loss: 0.003014, mae: 0.055225, mean_q: -0.288912
 19600/100000: episode: 196, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -16.892, mean reward: -0.169 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.004, 10.225], loss: 0.002835, mae: 0.054073, mean_q: -0.321493
 19700/100000: episode: 197, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -5.999, mean reward: -0.060 [-1.000, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.895, 10.098], loss: 0.003361, mae: 0.058251, mean_q: -0.309998
 19800/100000: episode: 198, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -17.791, mean reward: -0.178 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.979, 10.338], loss: 0.003089, mae: 0.056403, mean_q: -0.286319
 19900/100000: episode: 199, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.295, mean reward: -0.173 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.000, 10.298], loss: 0.002895, mae: 0.053713, mean_q: -0.307763
 20000/100000: episode: 200, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.319, mean reward: -0.163 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.671, 10.098], loss: 0.003003, mae: 0.055120, mean_q: -0.310485
 20100/100000: episode: 201, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.110, mean reward: -0.181 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.346, 10.109], loss: 0.003258, mae: 0.057106, mean_q: -0.312666
 20200/100000: episode: 202, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -16.431, mean reward: -0.164 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.236, 10.267], loss: 0.002920, mae: 0.054089, mean_q: -0.311932
 20300/100000: episode: 203, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.916, mean reward: -0.199 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.163, 10.171], loss: 0.004983, mae: 0.062665, mean_q: -0.300320
 20400/100000: episode: 204, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -13.594, mean reward: -0.136 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.824, 10.098], loss: 0.003417, mae: 0.058726, mean_q: -0.303419
 20500/100000: episode: 205, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.911, mean reward: -0.179 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.608, 10.107], loss: 0.002797, mae: 0.053347, mean_q: -0.291526
 20600/100000: episode: 206, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -12.084, mean reward: -0.121 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.885, 10.422], loss: 0.003011, mae: 0.055011, mean_q: -0.296031
 20700/100000: episode: 207, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -18.462, mean reward: -0.185 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.703, 10.138], loss: 0.002989, mae: 0.054152, mean_q: -0.295990
 20800/100000: episode: 208, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.690, mean reward: -0.157 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.507, 10.098], loss: 0.003482, mae: 0.059239, mean_q: -0.279539
 20900/100000: episode: 209, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.626, mean reward: -0.176 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.495, 10.098], loss: 0.002861, mae: 0.053439, mean_q: -0.290729
 21000/100000: episode: 210, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -16.007, mean reward: -0.160 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.221, 10.098], loss: 0.002913, mae: 0.054586, mean_q: -0.292188
 21100/100000: episode: 211, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.074, mean reward: -0.171 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.919, 10.098], loss: 0.002985, mae: 0.054363, mean_q: -0.319770
 21200/100000: episode: 212, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.412, mean reward: -0.164 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.622, 10.214], loss: 0.003034, mae: 0.055205, mean_q: -0.313460
 21300/100000: episode: 213, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.275, mean reward: -0.183 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.410, 10.098], loss: 0.002700, mae: 0.052424, mean_q: -0.326855
 21400/100000: episode: 214, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.413, mean reward: -0.164 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.427, 10.098], loss: 0.002971, mae: 0.054640, mean_q: -0.278514
 21500/100000: episode: 215, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -19.711, mean reward: -0.197 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.676, 10.209], loss: 0.004747, mae: 0.064308, mean_q: -0.306414
 21600/100000: episode: 216, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.801, mean reward: -0.178 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.614, 10.133], loss: 0.005540, mae: 0.066564, mean_q: -0.305334
 21700/100000: episode: 217, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.139, mean reward: -0.191 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.768, 10.151], loss: 0.002737, mae: 0.052055, mean_q: -0.305742
 21800/100000: episode: 218, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.535, mean reward: -0.135 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.856, 10.098], loss: 0.004606, mae: 0.061557, mean_q: -0.319455
 21900/100000: episode: 219, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -16.789, mean reward: -0.168 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.843, 10.098], loss: 0.002868, mae: 0.052954, mean_q: -0.351304
 22000/100000: episode: 220, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.406, mean reward: -0.154 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.480, 10.297], loss: 0.002862, mae: 0.053886, mean_q: -0.311546
 22100/100000: episode: 221, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -14.667, mean reward: -0.147 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.384, 10.238], loss: 0.002793, mae: 0.053639, mean_q: -0.299693
 22200/100000: episode: 222, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.110, mean reward: -0.191 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.550, 10.098], loss: 0.003124, mae: 0.056447, mean_q: -0.288156
 22300/100000: episode: 223, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.109, mean reward: -0.181 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.920, 10.098], loss: 0.002948, mae: 0.053422, mean_q: -0.305312
 22400/100000: episode: 224, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.632, mean reward: -0.176 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.494, 10.098], loss: 0.003182, mae: 0.056544, mean_q: -0.309562
 22500/100000: episode: 225, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.265, mean reward: -0.163 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.597, 10.224], loss: 0.002736, mae: 0.052190, mean_q: -0.288243
 22600/100000: episode: 226, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.847, mean reward: -0.188 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.292, 10.177], loss: 0.002904, mae: 0.054716, mean_q: -0.305149
 22700/100000: episode: 227, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -10.091, mean reward: -0.101 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.544, 10.098], loss: 0.003069, mae: 0.056420, mean_q: -0.290117
 22800/100000: episode: 228, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -19.164, mean reward: -0.192 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.097, 10.269], loss: 0.002991, mae: 0.055973, mean_q: -0.290547
 22900/100000: episode: 229, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -15.942, mean reward: -0.159 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.722, 10.098], loss: 0.002888, mae: 0.054132, mean_q: -0.321837
 23000/100000: episode: 230, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.299, mean reward: -0.163 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.912, 10.112], loss: 0.003017, mae: 0.055699, mean_q: -0.311370
 23100/100000: episode: 231, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -14.905, mean reward: -0.149 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.712, 10.331], loss: 0.003154, mae: 0.054639, mean_q: -0.311143
 23200/100000: episode: 232, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.352, mean reward: -0.154 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.616, 10.275], loss: 0.011828, mae: 0.086822, mean_q: -0.321905
 23300/100000: episode: 233, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.252, mean reward: -0.163 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.978, 10.100], loss: 0.003754, mae: 0.060892, mean_q: -0.303748
 23400/100000: episode: 234, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.830, mean reward: -0.188 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.133, 10.106], loss: 0.003062, mae: 0.056262, mean_q: -0.296481
 23500/100000: episode: 235, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.006, mean reward: -0.170 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.664, 10.098], loss: 0.003015, mae: 0.054672, mean_q: -0.319673
 23600/100000: episode: 236, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.307, mean reward: -0.173 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.616, 10.149], loss: 0.002855, mae: 0.053591, mean_q: -0.270350
 23700/100000: episode: 237, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.468, mean reward: -0.195 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.659, 10.184], loss: 0.002934, mae: 0.054983, mean_q: -0.319086
 23800/100000: episode: 238, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.660, mean reward: -0.177 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.821, 10.098], loss: 0.003513, mae: 0.056079, mean_q: -0.329119
 23900/100000: episode: 239, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.872, mean reward: -0.159 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.399, 10.098], loss: 0.007525, mae: 0.071684, mean_q: -0.287979
 24000/100000: episode: 240, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.969, mean reward: -0.160 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.691, 10.102], loss: 0.003675, mae: 0.059754, mean_q: -0.303531
 24100/100000: episode: 241, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.382, mean reward: -0.164 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.675, 10.098], loss: 0.002976, mae: 0.054758, mean_q: -0.317556
 24200/100000: episode: 242, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.018, mean reward: -0.180 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.402, 10.098], loss: 0.002871, mae: 0.053379, mean_q: -0.295702
 24300/100000: episode: 243, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.525, mean reward: -0.185 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.678, 10.151], loss: 0.002940, mae: 0.055254, mean_q: -0.288712
 24400/100000: episode: 244, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.961, mean reward: -0.190 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.124, 10.159], loss: 0.003081, mae: 0.055239, mean_q: -0.336267
 24500/100000: episode: 245, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.048, mean reward: -0.160 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.755, 10.462], loss: 0.002885, mae: 0.053152, mean_q: -0.328201
 24600/100000: episode: 246, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.463, mean reward: -0.185 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.767, 10.180], loss: 0.002781, mae: 0.052719, mean_q: -0.327456
 24700/100000: episode: 247, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.249, mean reward: -0.172 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.112, 10.133], loss: 0.002791, mae: 0.053638, mean_q: -0.300811
 24800/100000: episode: 248, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.074, mean reward: -0.181 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.493, 10.098], loss: 0.002528, mae: 0.050765, mean_q: -0.329665
 24900/100000: episode: 249, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.708, mean reward: -0.177 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.352, 10.123], loss: 0.002896, mae: 0.053911, mean_q: -0.337299
 25000/100000: episode: 250, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.810, mean reward: -0.198 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.402, 10.175], loss: 0.002893, mae: 0.053787, mean_q: -0.337083
 25100/100000: episode: 251, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -12.750, mean reward: -0.127 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.282, 10.278], loss: 0.003079, mae: 0.057038, mean_q: -0.287708
 25200/100000: episode: 252, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.194, mean reward: -0.192 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.514, 10.098], loss: 0.002697, mae: 0.052123, mean_q: -0.348572
 25300/100000: episode: 253, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.953, mean reward: -0.180 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.941, 10.245], loss: 0.002870, mae: 0.054641, mean_q: -0.309747
 25400/100000: episode: 254, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.345, mean reward: -0.183 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.867, 10.220], loss: 0.002655, mae: 0.051461, mean_q: -0.330488
 25500/100000: episode: 255, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.043, mean reward: -0.160 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.061, 10.117], loss: 0.002744, mae: 0.054448, mean_q: -0.316353
 25600/100000: episode: 256, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.009, mean reward: -0.160 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.837, 10.291], loss: 0.002826, mae: 0.053752, mean_q: -0.305420
 25700/100000: episode: 257, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.753, mean reward: -0.178 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.953, 10.167], loss: 0.002648, mae: 0.051636, mean_q: -0.330768
 25800/100000: episode: 258, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -15.419, mean reward: -0.154 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.003, 10.098], loss: 0.007724, mae: 0.073426, mean_q: -0.311305
 25900/100000: episode: 259, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -20.412, mean reward: -0.204 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.142, 10.270], loss: 0.003829, mae: 0.060495, mean_q: -0.313698
 26000/100000: episode: 260, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.884, mean reward: -0.179 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.586, 10.098], loss: 0.002638, mae: 0.052217, mean_q: -0.343077
 26100/100000: episode: 261, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.180, mean reward: -0.162 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.580, 10.252], loss: 0.002607, mae: 0.052015, mean_q: -0.323045
 26200/100000: episode: 262, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.688, mean reward: -0.187 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.077, 10.098], loss: 0.002748, mae: 0.052899, mean_q: -0.314582
 26300/100000: episode: 263, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -13.806, mean reward: -0.138 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.840, 10.164], loss: 0.002566, mae: 0.051488, mean_q: -0.340573
 26400/100000: episode: 264, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.158, mean reward: -0.182 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.900, 10.098], loss: 0.002815, mae: 0.053060, mean_q: -0.310792
 26500/100000: episode: 265, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.010, mean reward: -0.160 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.728, 10.098], loss: 0.002810, mae: 0.053489, mean_q: -0.320878
 26600/100000: episode: 266, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.413, mean reward: -0.174 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.894, 10.098], loss: 0.002769, mae: 0.053393, mean_q: -0.338283
 26700/100000: episode: 267, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.422, mean reward: -0.154 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.071, 10.121], loss: 0.002575, mae: 0.051894, mean_q: -0.340712
 26800/100000: episode: 268, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -14.192, mean reward: -0.142 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.712, 10.477], loss: 0.002749, mae: 0.053328, mean_q: -0.320859
 26900/100000: episode: 269, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.096, mean reward: -0.151 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.657, 10.098], loss: 0.002554, mae: 0.052761, mean_q: -0.322958
 27000/100000: episode: 270, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.733, mean reward: -0.157 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.467, 10.160], loss: 0.002823, mae: 0.054759, mean_q: -0.297362
 27100/100000: episode: 271, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -12.276, mean reward: -0.123 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.520, 10.098], loss: 0.002890, mae: 0.054912, mean_q: -0.297282
 27200/100000: episode: 272, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.802, mean reward: -0.188 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.024, 10.196], loss: 0.002844, mae: 0.053030, mean_q: -0.347568
 27300/100000: episode: 273, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.057, mean reward: -0.171 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.919, 10.154], loss: 0.005501, mae: 0.065592, mean_q: -0.338038
 27400/100000: episode: 274, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.860, mean reward: -0.169 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.474, 10.098], loss: 0.002716, mae: 0.052877, mean_q: -0.313541
 27500/100000: episode: 275, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.777, mean reward: -0.178 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.562, 10.098], loss: 0.002712, mae: 0.052558, mean_q: -0.330994
 27600/100000: episode: 276, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -12.270, mean reward: -0.123 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.997, 10.366], loss: 0.002376, mae: 0.048466, mean_q: -0.355339
 27700/100000: episode: 277, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.350, mean reward: -0.173 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.032, 10.229], loss: 0.002679, mae: 0.052364, mean_q: -0.290951
 27800/100000: episode: 278, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -10.438, mean reward: -0.104 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.088, 10.295], loss: 0.002616, mae: 0.050968, mean_q: -0.345574
 27900/100000: episode: 279, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: -15.982, mean reward: -0.160 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.980, 10.330], loss: 0.002761, mae: 0.052188, mean_q: -0.302606
 28000/100000: episode: 280, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.319, mean reward: -0.193 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.427, 10.205], loss: 0.002770, mae: 0.052970, mean_q: -0.325244
 28100/100000: episode: 281, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -19.326, mean reward: -0.193 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.614, 10.114], loss: 0.002662, mae: 0.051833, mean_q: -0.314727
 28200/100000: episode: 282, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -13.556, mean reward: -0.136 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.729, 10.098], loss: 0.002909, mae: 0.055298, mean_q: -0.295182
 28300/100000: episode: 283, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.492, mean reward: -0.175 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.458, 10.162], loss: 0.002750, mae: 0.053017, mean_q: -0.313534
 28400/100000: episode: 284, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.033, mean reward: -0.180 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.692, 10.098], loss: 0.002734, mae: 0.053907, mean_q: -0.313219
 28500/100000: episode: 285, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.871, mean reward: -0.179 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.842, 10.339], loss: 0.002659, mae: 0.053041, mean_q: -0.316348
 28600/100000: episode: 286, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.839, mean reward: -0.178 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.414, 10.158], loss: 0.002559, mae: 0.051053, mean_q: -0.323662
 28700/100000: episode: 287, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.603, mean reward: -0.166 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.946, 10.098], loss: 0.002692, mae: 0.052120, mean_q: -0.307100
 28800/100000: episode: 288, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.345, mean reward: -0.143 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.184, 10.098], loss: 0.002723, mae: 0.052567, mean_q: -0.294181
 28900/100000: episode: 289, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.324, mean reward: -0.153 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.222, 10.129], loss: 0.002834, mae: 0.053150, mean_q: -0.340262
 29000/100000: episode: 290, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -15.444, mean reward: -0.154 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.799, 10.269], loss: 0.002608, mae: 0.051277, mean_q: -0.329241
 29100/100000: episode: 291, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.022, mean reward: -0.190 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.785, 10.161], loss: 0.003994, mae: 0.056945, mean_q: -0.298999
 29200/100000: episode: 292, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.489, mean reward: -0.185 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.615, 10.098], loss: 0.009150, mae: 0.080538, mean_q: -0.304574
 29300/100000: episode: 293, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.543, mean reward: -0.165 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.839, 10.221], loss: 0.002704, mae: 0.052645, mean_q: -0.317903
 29400/100000: episode: 294, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.479, mean reward: -0.175 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.637, 10.098], loss: 0.002705, mae: 0.052835, mean_q: -0.313290
 29500/100000: episode: 295, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.356, mean reward: -0.194 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.561, 10.098], loss: 0.002666, mae: 0.051398, mean_q: -0.342597
 29600/100000: episode: 296, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -14.552, mean reward: -0.146 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.023, 10.098], loss: 0.002658, mae: 0.050942, mean_q: -0.321394
 29700/100000: episode: 297, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -20.499, mean reward: -0.205 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.867, 10.098], loss: 0.002761, mae: 0.052856, mean_q: -0.285027
 29800/100000: episode: 298, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -14.116, mean reward: -0.141 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.844, 10.219], loss: 0.002914, mae: 0.054158, mean_q: -0.298526
 29900/100000: episode: 299, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -15.313, mean reward: -0.153 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.791, 10.348], loss: 0.002698, mae: 0.051793, mean_q: -0.291400
 30000/100000: episode: 300, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.560, mean reward: -0.186 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.177, 10.098], loss: 0.002844, mae: 0.053972, mean_q: -0.291706
 30100/100000: episode: 301, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.670, mean reward: -0.167 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.843, 10.098], loss: 0.002636, mae: 0.051432, mean_q: -0.306214
 30200/100000: episode: 302, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.279, mean reward: -0.183 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.143, 10.098], loss: 0.002425, mae: 0.049371, mean_q: -0.293590
 30300/100000: episode: 303, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.100, mean reward: -0.161 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.233, 10.098], loss: 0.002650, mae: 0.051770, mean_q: -0.320630
 30400/100000: episode: 304, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -14.725, mean reward: -0.147 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.190, 10.111], loss: 0.006292, mae: 0.070368, mean_q: -0.316455
 30500/100000: episode: 305, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.088, mean reward: -0.171 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.957, 10.440], loss: 0.002613, mae: 0.051810, mean_q: -0.309548
 30600/100000: episode: 306, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -13.889, mean reward: -0.139 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.751, 10.098], loss: 0.002697, mae: 0.052433, mean_q: -0.293183
 30700/100000: episode: 307, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.450, mean reward: -0.175 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.706, 10.110], loss: 0.002705, mae: 0.052408, mean_q: -0.309438
 30800/100000: episode: 308, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.657, mean reward: -0.157 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.951, 10.540], loss: 0.002522, mae: 0.050259, mean_q: -0.327950
 30900/100000: episode: 309, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.607, mean reward: -0.196 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.777, 10.098], loss: 0.002560, mae: 0.050319, mean_q: -0.324729
 31000/100000: episode: 310, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.397, mean reward: -0.154 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.634, 10.098], loss: 0.002521, mae: 0.050039, mean_q: -0.332361
 31100/100000: episode: 311, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.620, mean reward: -0.196 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.170, 10.098], loss: 0.002675, mae: 0.052721, mean_q: -0.322092
 31200/100000: episode: 312, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.291, mean reward: -0.173 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.445, 10.104], loss: 0.002966, mae: 0.054835, mean_q: -0.295079
 31300/100000: episode: 313, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -13.731, mean reward: -0.137 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.146, 10.107], loss: 0.004009, mae: 0.059539, mean_q: -0.306533
 31400/100000: episode: 314, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -19.161, mean reward: -0.192 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.119, 10.098], loss: 0.002785, mae: 0.053709, mean_q: -0.307516
 31500/100000: episode: 315, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.746, mean reward: -0.187 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.696, 10.279], loss: 0.002650, mae: 0.051741, mean_q: -0.292362
 31600/100000: episode: 316, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.893, mean reward: -0.159 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.173, 10.415], loss: 0.002719, mae: 0.052809, mean_q: -0.277863
 31700/100000: episode: 317, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.688, mean reward: -0.187 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.505, 10.265], loss: 0.002656, mae: 0.050590, mean_q: -0.326988
 31800/100000: episode: 318, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.224, mean reward: -0.152 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.788, 10.361], loss: 0.002742, mae: 0.052996, mean_q: -0.263691
 31900/100000: episode: 319, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -11.549, mean reward: -0.115 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.359, 10.598], loss: 0.002559, mae: 0.049901, mean_q: -0.335875
 32000/100000: episode: 320, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -19.185, mean reward: -0.192 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.816, 10.098], loss: 0.002775, mae: 0.052793, mean_q: -0.317019
 32100/100000: episode: 321, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.278, mean reward: -0.153 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.453, 10.098], loss: 0.002720, mae: 0.052115, mean_q: -0.309655
 32200/100000: episode: 322, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.980, mean reward: -0.190 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.959, 10.098], loss: 0.002691, mae: 0.051845, mean_q: -0.302690
 32300/100000: episode: 323, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -14.964, mean reward: -0.150 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.768, 10.098], loss: 0.002668, mae: 0.052036, mean_q: -0.302743
 32400/100000: episode: 324, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -12.725, mean reward: -0.127 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.884, 10.241], loss: 0.002632, mae: 0.051898, mean_q: -0.310013
 32500/100000: episode: 325, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.765, mean reward: -0.188 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.285, 10.146], loss: 0.002713, mae: 0.051865, mean_q: -0.293364
 32600/100000: episode: 326, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -9.658, mean reward: -0.097 [-1.000, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.850, 10.401], loss: 0.002757, mae: 0.052935, mean_q: -0.324383
 32700/100000: episode: 327, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -16.362, mean reward: -0.164 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.715, 10.098], loss: 0.002484, mae: 0.049837, mean_q: -0.327661
 32800/100000: episode: 328, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -19.993, mean reward: -0.200 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.027, 10.245], loss: 0.002745, mae: 0.053049, mean_q: -0.324054
 32900/100000: episode: 329, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.521, mean reward: -0.145 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.418, 10.098], loss: 0.003043, mae: 0.055799, mean_q: -0.305666
 33000/100000: episode: 330, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.035, mean reward: -0.180 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.865, 10.164], loss: 0.003717, mae: 0.056407, mean_q: -0.341930
 33100/100000: episode: 331, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.019, mean reward: -0.160 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.145, 10.124], loss: 0.002829, mae: 0.053776, mean_q: -0.300959
 33200/100000: episode: 332, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.320, mean reward: -0.193 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.867, 10.098], loss: 0.002583, mae: 0.051070, mean_q: -0.328059
 33300/100000: episode: 333, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -17.467, mean reward: -0.175 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.057, 10.098], loss: 0.002538, mae: 0.050258, mean_q: -0.309055
 33400/100000: episode: 334, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.703, mean reward: -0.197 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.654, 10.258], loss: 0.002643, mae: 0.051301, mean_q: -0.295133
 33500/100000: episode: 335, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -12.928, mean reward: -0.129 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.881, 10.285], loss: 0.002717, mae: 0.052282, mean_q: -0.292858
 33600/100000: episode: 336, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.885, mean reward: -0.179 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.525, 10.116], loss: 0.002478, mae: 0.049444, mean_q: -0.323437
 33700/100000: episode: 337, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.786, mean reward: -0.168 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.453, 10.399], loss: 0.003405, mae: 0.057367, mean_q: -0.333729
 33800/100000: episode: 338, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.537, mean reward: -0.175 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.416, 10.098], loss: 0.002526, mae: 0.050094, mean_q: -0.346598
 33900/100000: episode: 339, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.692, mean reward: -0.177 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.969, 10.208], loss: 0.002754, mae: 0.053511, mean_q: -0.291300
 34000/100000: episode: 340, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.694, mean reward: -0.157 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.184, 10.270], loss: 0.002500, mae: 0.050473, mean_q: -0.319298
 34100/100000: episode: 341, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.627, mean reward: -0.176 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.349, 10.098], loss: 0.002564, mae: 0.050604, mean_q: -0.308054
 34200/100000: episode: 342, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -16.058, mean reward: -0.161 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.328, 10.324], loss: 0.002574, mae: 0.050720, mean_q: -0.320049
 34300/100000: episode: 343, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.170, mean reward: -0.182 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.723, 10.098], loss: 0.002484, mae: 0.049300, mean_q: -0.331164
 34400/100000: episode: 344, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.393, mean reward: -0.184 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.963, 10.213], loss: 0.002762, mae: 0.052991, mean_q: -0.293347
 34500/100000: episode: 345, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.558, mean reward: -0.146 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.027, 10.236], loss: 0.002771, mae: 0.053328, mean_q: -0.335541
 34600/100000: episode: 346, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.418, mean reward: -0.164 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.545, 10.315], loss: 0.002675, mae: 0.051860, mean_q: -0.271221
 34700/100000: episode: 347, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.770, mean reward: -0.158 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.053, 10.098], loss: 0.003857, mae: 0.057631, mean_q: -0.308494
 34800/100000: episode: 348, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.925, mean reward: -0.189 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.329, 10.307], loss: 0.002651, mae: 0.051558, mean_q: -0.328165
 34900/100000: episode: 349, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.696, mean reward: -0.187 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.455, 10.243], loss: 0.002418, mae: 0.049501, mean_q: -0.314821
 35000/100000: episode: 350, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.970, mean reward: -0.190 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.150, 10.098], loss: 0.002704, mae: 0.052132, mean_q: -0.310313
 35100/100000: episode: 351, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -17.026, mean reward: -0.170 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.195, 10.324], loss: 0.002552, mae: 0.050448, mean_q: -0.296947
 35200/100000: episode: 352, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.437, mean reward: -0.184 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.992, 10.098], loss: 0.002453, mae: 0.049185, mean_q: -0.308608
 35300/100000: episode: 353, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.194, mean reward: -0.172 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.868, 10.098], loss: 0.002786, mae: 0.053136, mean_q: -0.323816
 35400/100000: episode: 354, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -22.210, mean reward: -0.222 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.618, 10.098], loss: 0.002731, mae: 0.052186, mean_q: -0.303601
 35500/100000: episode: 355, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -13.234, mean reward: -0.132 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.199, 10.098], loss: 0.002673, mae: 0.051099, mean_q: -0.320061
 35600/100000: episode: 356, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -19.305, mean reward: -0.193 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.677, 10.098], loss: 0.003081, mae: 0.057068, mean_q: -0.322670
 35700/100000: episode: 357, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.488, mean reward: -0.185 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.184, 10.098], loss: 0.002491, mae: 0.050262, mean_q: -0.296736
 35800/100000: episode: 358, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -20.085, mean reward: -0.201 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.537, 10.098], loss: 0.002614, mae: 0.050212, mean_q: -0.321235
 35900/100000: episode: 359, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -14.299, mean reward: -0.143 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.529, 10.098], loss: 0.002637, mae: 0.050785, mean_q: -0.309061
 36000/100000: episode: 360, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.205, mean reward: -0.182 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.645, 10.130], loss: 0.002468, mae: 0.049772, mean_q: -0.300700
 36100/100000: episode: 361, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.083, mean reward: -0.171 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.873, 10.098], loss: 0.002495, mae: 0.049548, mean_q: -0.298999
 36200/100000: episode: 362, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.180, mean reward: -0.192 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.394, 10.137], loss: 0.002548, mae: 0.051059, mean_q: -0.305173
 36300/100000: episode: 363, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.793, mean reward: -0.168 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.784, 10.098], loss: 0.002586, mae: 0.051210, mean_q: -0.334925
 36400/100000: episode: 364, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.596, mean reward: -0.186 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.707, 10.098], loss: 0.002559, mae: 0.050569, mean_q: -0.327420
 36500/100000: episode: 365, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -13.872, mean reward: -0.139 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.892, 10.098], loss: 0.002408, mae: 0.048815, mean_q: -0.331384
 36600/100000: episode: 366, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.543, mean reward: -0.165 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.996, 10.122], loss: 0.002786, mae: 0.053139, mean_q: -0.302695
 36700/100000: episode: 367, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.219, mean reward: -0.192 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.883, 10.139], loss: 0.002801, mae: 0.053632, mean_q: -0.328179
 36800/100000: episode: 368, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.763, mean reward: -0.178 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.254, 10.191], loss: 0.002578, mae: 0.050046, mean_q: -0.342886
 36900/100000: episode: 369, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -10.813, mean reward: -0.108 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.696, 10.518], loss: 0.002616, mae: 0.050453, mean_q: -0.342363
 37000/100000: episode: 370, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.344, mean reward: -0.193 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.809, 10.100], loss: 0.002619, mae: 0.052042, mean_q: -0.329016
 37100/100000: episode: 371, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -17.814, mean reward: -0.178 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.680, 10.098], loss: 0.002508, mae: 0.049503, mean_q: -0.326915
 37200/100000: episode: 372, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.282, mean reward: -0.183 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.511, 10.203], loss: 0.003121, mae: 0.055135, mean_q: -0.318202
 37300/100000: episode: 373, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.456, mean reward: -0.175 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.466, 10.143], loss: 0.002751, mae: 0.053115, mean_q: -0.298107
 37400/100000: episode: 374, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.304, mean reward: -0.183 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.483, 10.123], loss: 0.002604, mae: 0.051953, mean_q: -0.338003
 37500/100000: episode: 375, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.813, mean reward: -0.178 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.207, 10.098], loss: 0.002561, mae: 0.050967, mean_q: -0.343264
 37600/100000: episode: 376, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.981, mean reward: -0.180 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.044, 10.150], loss: 0.006039, mae: 0.065026, mean_q: -0.311523
 37700/100000: episode: 377, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -12.075, mean reward: -0.121 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.925, 10.098], loss: 0.002739, mae: 0.054089, mean_q: -0.325431
 37800/100000: episode: 378, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -16.877, mean reward: -0.169 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.870, 10.098], loss: 0.002370, mae: 0.048466, mean_q: -0.343789
 37900/100000: episode: 379, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -17.156, mean reward: -0.172 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.099, 10.293], loss: 0.002409, mae: 0.048373, mean_q: -0.351768
 38000/100000: episode: 380, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -20.014, mean reward: -0.200 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.780, 10.102], loss: 0.002547, mae: 0.049674, mean_q: -0.320937
 38100/100000: episode: 381, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.605, mean reward: -0.176 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.182, 10.098], loss: 0.002484, mae: 0.048938, mean_q: -0.329960
 38200/100000: episode: 382, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.470, mean reward: -0.145 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.005, 10.117], loss: 0.002684, mae: 0.051337, mean_q: -0.334228
 38300/100000: episode: 383, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.368, mean reward: -0.184 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.255, 10.221], loss: 0.002668, mae: 0.052737, mean_q: -0.310180
 38400/100000: episode: 384, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.610, mean reward: -0.166 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.389, 10.266], loss: 0.002600, mae: 0.050515, mean_q: -0.336807
 38500/100000: episode: 385, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.305, mean reward: -0.193 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.374, 10.210], loss: 0.002764, mae: 0.052159, mean_q: -0.329961
 38600/100000: episode: 386, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.994, mean reward: -0.160 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.365, 10.321], loss: 0.002571, mae: 0.050610, mean_q: -0.321532
 38700/100000: episode: 387, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.801, mean reward: -0.188 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.690, 10.098], loss: 0.002457, mae: 0.048999, mean_q: -0.371844
 38800/100000: episode: 388, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.031, mean reward: -0.190 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.340, 10.190], loss: 0.002399, mae: 0.047755, mean_q: -0.350567
 38900/100000: episode: 389, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -11.774, mean reward: -0.118 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.836, 10.354], loss: 0.002786, mae: 0.051729, mean_q: -0.360629
 39000/100000: episode: 390, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.712, mean reward: -0.167 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.052, 10.098], loss: 0.002628, mae: 0.050718, mean_q: -0.335991
 39100/100000: episode: 391, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.427, mean reward: -0.184 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.843, 10.098], loss: 0.002704, mae: 0.051106, mean_q: -0.334039
 39200/100000: episode: 392, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.750, mean reward: -0.177 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.838, 10.201], loss: 0.002506, mae: 0.049834, mean_q: -0.333825
 39300/100000: episode: 393, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.033, mean reward: -0.170 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.868, 10.480], loss: 0.002775, mae: 0.052632, mean_q: -0.308032
 39400/100000: episode: 394, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -13.832, mean reward: -0.138 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.652, 10.098], loss: 0.002824, mae: 0.053507, mean_q: -0.331105
 39500/100000: episode: 395, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.356, mean reward: -0.164 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.817, 10.142], loss: 0.002508, mae: 0.050793, mean_q: -0.326956
 39600/100000: episode: 396, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.151, mean reward: -0.152 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.751, 10.123], loss: 0.002702, mae: 0.052100, mean_q: -0.336244
 39700/100000: episode: 397, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -11.635, mean reward: -0.116 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.780, 10.098], loss: 0.002704, mae: 0.052915, mean_q: -0.314487
 39800/100000: episode: 398, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.324, mean reward: -0.153 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.546, 10.386], loss: 0.002527, mae: 0.050502, mean_q: -0.341218
 39900/100000: episode: 399, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -11.493, mean reward: -0.115 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.591, 10.266], loss: 0.002691, mae: 0.052298, mean_q: -0.344106
 40000/100000: episode: 400, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.131, mean reward: -0.161 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.115, 10.255], loss: 0.002787, mae: 0.053103, mean_q: -0.333460
 40100/100000: episode: 401, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -18.408, mean reward: -0.184 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.815, 10.168], loss: 0.002741, mae: 0.052485, mean_q: -0.308793
 40200/100000: episode: 402, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.933, mean reward: -0.159 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.283, 10.152], loss: 0.002730, mae: 0.052010, mean_q: -0.331969
 40300/100000: episode: 403, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -15.007, mean reward: -0.150 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.439, 10.242], loss: 0.003078, mae: 0.054948, mean_q: -0.334718
 40400/100000: episode: 404, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.697, mean reward: -0.177 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.596, 10.429], loss: 0.002748, mae: 0.052552, mean_q: -0.313403
 40500/100000: episode: 405, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.202, mean reward: -0.162 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.321, 10.351], loss: 0.003007, mae: 0.056009, mean_q: -0.284446
 40600/100000: episode: 406, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.798, mean reward: -0.178 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.204, 10.236], loss: 0.002725, mae: 0.051314, mean_q: -0.333721
 40700/100000: episode: 407, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.911, mean reward: -0.159 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.600, 10.098], loss: 0.002825, mae: 0.053951, mean_q: -0.293316
 40800/100000: episode: 408, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.428, mean reward: -0.164 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.152, 10.194], loss: 0.002732, mae: 0.052689, mean_q: -0.312051
 40900/100000: episode: 409, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -12.377, mean reward: -0.124 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.773, 10.363], loss: 0.002782, mae: 0.053740, mean_q: -0.298788
 41000/100000: episode: 410, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.045, mean reward: -0.190 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.983, 10.098], loss: 0.002682, mae: 0.051805, mean_q: -0.269932
 41100/100000: episode: 411, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.742, mean reward: -0.177 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.064, 10.098], loss: 0.002519, mae: 0.049856, mean_q: -0.348421
 41200/100000: episode: 412, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.890, mean reward: -0.169 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.721, 10.098], loss: 0.002594, mae: 0.051364, mean_q: -0.305633
 41300/100000: episode: 413, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.627, mean reward: -0.166 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.746, 10.203], loss: 0.002650, mae: 0.051890, mean_q: -0.301284
 41400/100000: episode: 414, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.873, mean reward: -0.179 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.892, 10.117], loss: 0.002847, mae: 0.054035, mean_q: -0.295650
 41500/100000: episode: 415, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.168, mean reward: -0.162 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.458, 10.222], loss: 0.004480, mae: 0.061195, mean_q: -0.327249
 41600/100000: episode: 416, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.286, mean reward: -0.143 [-1.000, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.119, 10.327], loss: 0.003046, mae: 0.056095, mean_q: -0.345870
 41700/100000: episode: 417, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.652, mean reward: -0.147 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.222, 10.106], loss: 0.002757, mae: 0.052905, mean_q: -0.324759
 41800/100000: episode: 418, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.491, mean reward: -0.195 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.274, 10.098], loss: 0.002615, mae: 0.051772, mean_q: -0.281559
 41900/100000: episode: 419, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.856, mean reward: -0.189 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.077, 10.098], loss: 0.002521, mae: 0.050426, mean_q: -0.317761
 42000/100000: episode: 420, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.658, mean reward: -0.187 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.601, 10.127], loss: 0.002636, mae: 0.051440, mean_q: -0.292562
 42100/100000: episode: 421, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.276, mean reward: -0.173 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.282, 10.098], loss: 0.002721, mae: 0.051786, mean_q: -0.318165
 42200/100000: episode: 422, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.218, mean reward: -0.182 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.979, 10.098], loss: 0.002725, mae: 0.053033, mean_q: -0.313615
 42300/100000: episode: 423, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.278, mean reward: -0.173 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.127, 10.098], loss: 0.002867, mae: 0.054769, mean_q: -0.278559
 42400/100000: episode: 424, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.405, mean reward: -0.184 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.634, 10.115], loss: 0.002688, mae: 0.051657, mean_q: -0.310482
 42500/100000: episode: 425, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -14.331, mean reward: -0.143 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.046, 10.098], loss: 0.002767, mae: 0.053519, mean_q: -0.306641
 42600/100000: episode: 426, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.170, mean reward: -0.172 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.227, 10.208], loss: 0.002588, mae: 0.052232, mean_q: -0.309541
 42700/100000: episode: 427, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -20.282, mean reward: -0.203 [-1.000, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.471, 10.208], loss: 0.002626, mae: 0.052249, mean_q: -0.351801
 42800/100000: episode: 428, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.188, mean reward: -0.182 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.659, 10.130], loss: 0.002715, mae: 0.052594, mean_q: -0.307702
 42900/100000: episode: 429, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -17.756, mean reward: -0.178 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.947, 10.271], loss: 0.002459, mae: 0.050213, mean_q: -0.312826
 43000/100000: episode: 430, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -15.603, mean reward: -0.156 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.210, 10.173], loss: 0.002733, mae: 0.052778, mean_q: -0.296976
 43100/100000: episode: 431, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.213, mean reward: -0.182 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.218, 10.191], loss: 0.002563, mae: 0.051012, mean_q: -0.305959
 43200/100000: episode: 432, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.561, mean reward: -0.166 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.026, 10.416], loss: 0.002691, mae: 0.052622, mean_q: -0.328510
 43300/100000: episode: 433, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.745, mean reward: -0.167 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.814, 10.098], loss: 0.002744, mae: 0.052364, mean_q: -0.321365
 43400/100000: episode: 434, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.095, mean reward: -0.181 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.485, 10.375], loss: 0.002579, mae: 0.050900, mean_q: -0.353664
 43500/100000: episode: 435, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.506, mean reward: -0.175 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.673, 10.140], loss: 0.002504, mae: 0.049895, mean_q: -0.345747
 43600/100000: episode: 436, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -5.923, mean reward: -0.059 [-1.000, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.856, 10.366], loss: 0.002680, mae: 0.052024, mean_q: -0.312685
 43700/100000: episode: 437, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.038, mean reward: -0.190 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.681, 10.359], loss: 0.002757, mae: 0.052878, mean_q: -0.316541
 43800/100000: episode: 438, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -20.008, mean reward: -0.200 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.377, 10.202], loss: 0.002755, mae: 0.053078, mean_q: -0.326324
 43900/100000: episode: 439, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -13.795, mean reward: -0.138 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.146, 10.098], loss: 0.002682, mae: 0.052480, mean_q: -0.324562
 44000/100000: episode: 440, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.580, mean reward: -0.166 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.053, 10.098], loss: 0.002555, mae: 0.050915, mean_q: -0.321286
 44100/100000: episode: 441, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -17.085, mean reward: -0.171 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.085, 10.184], loss: 0.002502, mae: 0.050060, mean_q: -0.327171
 44200/100000: episode: 442, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.761, mean reward: -0.188 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.266, 10.222], loss: 0.005034, mae: 0.066844, mean_q: -0.291993
 44300/100000: episode: 443, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.070, mean reward: -0.181 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.844, 10.153], loss: 0.002655, mae: 0.052688, mean_q: -0.284879
 44400/100000: episode: 444, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -19.112, mean reward: -0.191 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.657, 10.122], loss: 0.002803, mae: 0.053194, mean_q: -0.312578
 44500/100000: episode: 445, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -11.555, mean reward: -0.116 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.901, 10.098], loss: 0.002633, mae: 0.051134, mean_q: -0.296627
 44600/100000: episode: 446, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -14.591, mean reward: -0.146 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.282, 10.399], loss: 0.002684, mae: 0.052834, mean_q: -0.279921
 44700/100000: episode: 447, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -18.222, mean reward: -0.182 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.270, 10.313], loss: 0.002745, mae: 0.053281, mean_q: -0.317387
 44800/100000: episode: 448, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.031, mean reward: -0.190 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.704, 10.098], loss: 0.002547, mae: 0.049841, mean_q: -0.321730
 44900/100000: episode: 449, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.355, mean reward: -0.154 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.700, 10.197], loss: 0.002571, mae: 0.050164, mean_q: -0.312380
 45000/100000: episode: 450, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -15.378, mean reward: -0.154 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.779, 10.377], loss: 0.002439, mae: 0.049174, mean_q: -0.333962
 45100/100000: episode: 451, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.449, mean reward: -0.164 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.163, 10.098], loss: 0.002734, mae: 0.052412, mean_q: -0.324537
 45200/100000: episode: 452, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.123, mean reward: -0.181 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.922, 10.098], loss: 0.002671, mae: 0.051784, mean_q: -0.324399
 45300/100000: episode: 453, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.939, mean reward: -0.169 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.250, 10.399], loss: 0.002534, mae: 0.049631, mean_q: -0.311242
 45400/100000: episode: 454, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.241, mean reward: -0.172 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.807, 10.261], loss: 0.002773, mae: 0.051836, mean_q: -0.284987
 45500/100000: episode: 455, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -16.874, mean reward: -0.169 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.975, 10.098], loss: 0.002715, mae: 0.052195, mean_q: -0.290135
 45600/100000: episode: 456, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.745, mean reward: -0.187 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.126, 10.244], loss: 0.002709, mae: 0.051752, mean_q: -0.335143
 45700/100000: episode: 457, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -15.648, mean reward: -0.156 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.386, 10.116], loss: 0.002512, mae: 0.049108, mean_q: -0.336978
 45800/100000: episode: 458, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -14.635, mean reward: -0.146 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.612, 10.098], loss: 0.002718, mae: 0.051680, mean_q: -0.327444
 45900/100000: episode: 459, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.582, mean reward: -0.176 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.018, 10.265], loss: 0.002790, mae: 0.052136, mean_q: -0.299323
 46000/100000: episode: 460, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -14.723, mean reward: -0.147 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.218, 10.234], loss: 0.002776, mae: 0.053275, mean_q: -0.316175
 46100/100000: episode: 461, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.251, mean reward: -0.173 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.654, 10.098], loss: 0.002961, mae: 0.054472, mean_q: -0.336068
 46200/100000: episode: 462, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.540, mean reward: -0.145 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.489, 10.433], loss: 0.002780, mae: 0.053247, mean_q: -0.300766
 46300/100000: episode: 463, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.953, mean reward: -0.190 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.783, 10.271], loss: 0.002633, mae: 0.052060, mean_q: -0.294909
 46400/100000: episode: 464, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.806, mean reward: -0.188 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.394, 10.110], loss: 0.002825, mae: 0.053208, mean_q: -0.315685
 46500/100000: episode: 465, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.412, mean reward: -0.194 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.784, 10.098], loss: 0.003007, mae: 0.054565, mean_q: -0.322739
 46600/100000: episode: 466, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.938, mean reward: -0.199 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.481, 10.098], loss: 0.002504, mae: 0.049062, mean_q: -0.329728
 46700/100000: episode: 467, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.488, mean reward: -0.175 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.590, 10.098], loss: 0.002829, mae: 0.053210, mean_q: -0.319950
 46800/100000: episode: 468, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.140, mean reward: -0.171 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.302, 10.098], loss: 0.002919, mae: 0.054562, mean_q: -0.294592
 46900/100000: episode: 469, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.901, mean reward: -0.159 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.548, 10.190], loss: 0.002603, mae: 0.050991, mean_q: -0.346290
 47000/100000: episode: 470, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.495, mean reward: -0.165 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.161, 10.098], loss: 0.002512, mae: 0.050155, mean_q: -0.315814
 47100/100000: episode: 471, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.642, mean reward: -0.176 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.768, 10.098], loss: 0.003040, mae: 0.054379, mean_q: -0.331876
 47200/100000: episode: 472, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.310, mean reward: -0.183 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.241, 10.351], loss: 0.002542, mae: 0.050760, mean_q: -0.305821
 47300/100000: episode: 473, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.503, mean reward: -0.185 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.748, 10.213], loss: 0.002645, mae: 0.051498, mean_q: -0.321242
 47400/100000: episode: 474, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.961, mean reward: -0.180 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.559, 10.150], loss: 0.002667, mae: 0.051605, mean_q: -0.303121
 47500/100000: episode: 475, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -11.577, mean reward: -0.116 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.516, 10.098], loss: 0.002760, mae: 0.052132, mean_q: -0.333224
 47600/100000: episode: 476, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.665, mean reward: -0.167 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.993, 10.098], loss: 0.002624, mae: 0.051114, mean_q: -0.298807
 47700/100000: episode: 477, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.251, mean reward: -0.183 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.871, 10.098], loss: 0.002627, mae: 0.051485, mean_q: -0.341760
 47800/100000: episode: 478, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.798, mean reward: -0.178 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.315, 10.098], loss: 0.002718, mae: 0.051966, mean_q: -0.323925
 47900/100000: episode: 479, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.795, mean reward: -0.168 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.549, 10.287], loss: 0.005784, mae: 0.067850, mean_q: -0.324608
 48000/100000: episode: 480, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.497, mean reward: -0.185 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.685, 10.098], loss: 0.002762, mae: 0.051952, mean_q: -0.349471
 48100/100000: episode: 481, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.287, mean reward: -0.173 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.435, 10.098], loss: 0.002735, mae: 0.052027, mean_q: -0.311671
 48200/100000: episode: 482, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.490, mean reward: -0.185 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.829, 10.098], loss: 0.002529, mae: 0.050420, mean_q: -0.289845
 48300/100000: episode: 483, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -15.782, mean reward: -0.158 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.363, 10.125], loss: 0.002746, mae: 0.051952, mean_q: -0.300050
 48400/100000: episode: 484, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.779, mean reward: -0.188 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.883, 10.098], loss: 0.002719, mae: 0.052662, mean_q: -0.340061
 48500/100000: episode: 485, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.004, mean reward: -0.180 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.736, 10.098], loss: 0.002591, mae: 0.050414, mean_q: -0.327850
 48600/100000: episode: 486, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.774, mean reward: -0.178 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.067, 10.118], loss: 0.002637, mae: 0.051142, mean_q: -0.298661
 48700/100000: episode: 487, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -11.011, mean reward: -0.110 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.286, 10.490], loss: 0.002514, mae: 0.050132, mean_q: -0.341426
 48800/100000: episode: 488, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.254, mean reward: -0.183 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.758, 10.257], loss: 0.002649, mae: 0.051330, mean_q: -0.331547
 48900/100000: episode: 489, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.135, mean reward: -0.181 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.001, 10.172], loss: 0.002745, mae: 0.052373, mean_q: -0.305611
 49000/100000: episode: 490, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.164, mean reward: -0.172 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.782, 10.098], loss: 0.002604, mae: 0.050860, mean_q: -0.302817
 49100/100000: episode: 491, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.180, mean reward: -0.172 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.251, 10.230], loss: 0.002590, mae: 0.050584, mean_q: -0.331984
 49200/100000: episode: 492, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -15.592, mean reward: -0.156 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.688, 10.098], loss: 0.002493, mae: 0.049221, mean_q: -0.313841
 49300/100000: episode: 493, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.714, mean reward: -0.167 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.810, 10.107], loss: 0.002710, mae: 0.051454, mean_q: -0.294398
 49400/100000: episode: 494, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.940, mean reward: -0.199 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.396, 10.107], loss: 0.002611, mae: 0.050509, mean_q: -0.328502
 49500/100000: episode: 495, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.638, mean reward: -0.166 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.065, 10.098], loss: 0.002572, mae: 0.049815, mean_q: -0.346383
 49600/100000: episode: 496, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.671, mean reward: -0.167 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.496, 10.105], loss: 0.002764, mae: 0.051867, mean_q: -0.309069
 49700/100000: episode: 497, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -11.461, mean reward: -0.115 [-1.000, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.845, 10.098], loss: 0.002562, mae: 0.050562, mean_q: -0.306159
 49800/100000: episode: 498, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.916, mean reward: -0.179 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.869, 10.196], loss: 0.002633, mae: 0.051233, mean_q: -0.315887
 49900/100000: episode: 499, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.621, mean reward: -0.176 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.846, 10.098], loss: 0.002697, mae: 0.051269, mean_q: -0.346276
 50000/100000: episode: 500, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.124, mean reward: -0.181 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.336, 10.098], loss: 0.002538, mae: 0.049237, mean_q: -0.350278
 50100/100000: episode: 501, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -21.196, mean reward: -0.212 [-1.000, 0.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.305, 10.098], loss: 0.002347, mae: 0.048324, mean_q: -0.328897
 50200/100000: episode: 502, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.759, mean reward: -0.198 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.022, 10.098], loss: 0.002586, mae: 0.051939, mean_q: -0.306699
 50300/100000: episode: 503, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.426, mean reward: -0.154 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.946, 10.098], loss: 0.002624, mae: 0.051298, mean_q: -0.313673
 50400/100000: episode: 504, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.757, mean reward: -0.158 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.323, 10.332], loss: 0.002490, mae: 0.049856, mean_q: -0.292834
 50500/100000: episode: 505, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -12.293, mean reward: -0.123 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.461, 10.098], loss: 0.002674, mae: 0.051744, mean_q: -0.348979
 50600/100000: episode: 506, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.339, mean reward: -0.183 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.128, 10.218], loss: 0.002566, mae: 0.051991, mean_q: -0.293739
 50700/100000: episode: 507, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -17.607, mean reward: -0.176 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.285, 10.129], loss: 0.002541, mae: 0.050401, mean_q: -0.292248
 50800/100000: episode: 508, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -14.880, mean reward: -0.149 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.609, 10.098], loss: 0.002591, mae: 0.049836, mean_q: -0.321572
 50900/100000: episode: 509, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.204, mean reward: -0.162 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.227, 10.350], loss: 0.002746, mae: 0.051873, mean_q: -0.332052
 51000/100000: episode: 510, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -20.102, mean reward: -0.201 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.737, 10.155], loss: 0.005263, mae: 0.059917, mean_q: -0.344614
 51100/100000: episode: 511, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.576, mean reward: -0.166 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.809, 10.098], loss: 0.004035, mae: 0.058194, mean_q: -0.305094
 51200/100000: episode: 512, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.528, mean reward: -0.195 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.734, 10.146], loss: 0.002637, mae: 0.049328, mean_q: -0.359499
 51300/100000: episode: 513, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.273, mean reward: -0.163 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.201, 10.221], loss: 0.002514, mae: 0.049933, mean_q: -0.313578
 51400/100000: episode: 514, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.654, mean reward: -0.187 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.185, 10.098], loss: 0.002417, mae: 0.047831, mean_q: -0.312046
 51500/100000: episode: 515, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.858, mean reward: -0.179 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.435, 10.240], loss: 0.002457, mae: 0.048076, mean_q: -0.321648
 51600/100000: episode: 516, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.712, mean reward: -0.197 [-1.000, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.517, 10.171], loss: 0.002466, mae: 0.049314, mean_q: -0.355213
 51700/100000: episode: 517, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.887, mean reward: -0.159 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.763, 10.173], loss: 0.002390, mae: 0.049029, mean_q: -0.285133
 51800/100000: episode: 518, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.457, mean reward: -0.185 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.367, 10.199], loss: 0.002484, mae: 0.049338, mean_q: -0.323819
 51900/100000: episode: 519, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.361, mean reward: -0.184 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.788, 10.202], loss: 0.002607, mae: 0.050830, mean_q: -0.317261
 52000/100000: episode: 520, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.740, mean reward: -0.177 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.624, 10.144], loss: 0.002462, mae: 0.048865, mean_q: -0.345200
 52100/100000: episode: 521, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -14.832, mean reward: -0.148 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.295, 10.308], loss: 0.002358, mae: 0.047500, mean_q: -0.369653
 52200/100000: episode: 522, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.850, mean reward: -0.199 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.629, 10.098], loss: 0.002399, mae: 0.048047, mean_q: -0.343193
 52300/100000: episode: 523, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.184, mean reward: -0.182 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.109, 10.098], loss: 0.002303, mae: 0.047878, mean_q: -0.360428
 52400/100000: episode: 524, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.179, mean reward: -0.152 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.163, 10.098], loss: 0.002511, mae: 0.049397, mean_q: -0.308544
 52500/100000: episode: 525, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.908, mean reward: -0.189 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.909, 10.098], loss: 0.002484, mae: 0.049115, mean_q: -0.335535
 52600/100000: episode: 526, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.690, mean reward: -0.157 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.188, 10.098], loss: 0.002516, mae: 0.050004, mean_q: -0.310812
 52700/100000: episode: 527, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.159, mean reward: -0.162 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.427, 10.098], loss: 0.002434, mae: 0.049522, mean_q: -0.341431
 52800/100000: episode: 528, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -19.228, mean reward: -0.192 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.940, 10.260], loss: 0.002556, mae: 0.049718, mean_q: -0.304586
 52900/100000: episode: 529, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.111, mean reward: -0.161 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.820, 10.153], loss: 0.002605, mae: 0.050715, mean_q: -0.335803
 53000/100000: episode: 530, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.458, mean reward: -0.135 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.942, 10.364], loss: 0.002561, mae: 0.049568, mean_q: -0.325148
 53100/100000: episode: 531, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.177, mean reward: -0.192 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.023, 10.158], loss: 0.002478, mae: 0.049442, mean_q: -0.306489
 53200/100000: episode: 532, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.334, mean reward: -0.153 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.004, 10.098], loss: 0.002210, mae: 0.046873, mean_q: -0.340980
 53300/100000: episode: 533, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.557, mean reward: -0.176 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.690, 10.187], loss: 0.002514, mae: 0.050018, mean_q: -0.342874
 53400/100000: episode: 534, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -20.089, mean reward: -0.201 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.367, 10.111], loss: 0.002432, mae: 0.049251, mean_q: -0.334649
 53500/100000: episode: 535, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.849, mean reward: -0.188 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.659, 10.113], loss: 0.002580, mae: 0.050128, mean_q: -0.308207
 53600/100000: episode: 536, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.919, mean reward: -0.169 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.966, 10.098], loss: 0.004957, mae: 0.059681, mean_q: -0.347884
 53700/100000: episode: 537, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.029, mean reward: -0.170 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.202, 10.098], loss: 0.002663, mae: 0.054224, mean_q: -0.336821
 53800/100000: episode: 538, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -21.354, mean reward: -0.214 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.707, 10.186], loss: 0.002429, mae: 0.048751, mean_q: -0.324498
 53900/100000: episode: 539, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.625, mean reward: -0.186 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.648, 10.220], loss: 0.002313, mae: 0.047531, mean_q: -0.364419
 54000/100000: episode: 540, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -14.158, mean reward: -0.142 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.149, 10.136], loss: 0.002383, mae: 0.048528, mean_q: -0.351947
 54100/100000: episode: 541, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.652, mean reward: -0.157 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.654, 10.098], loss: 0.002500, mae: 0.049369, mean_q: -0.367208
 54200/100000: episode: 542, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -14.816, mean reward: -0.148 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.281, 10.098], loss: 0.002498, mae: 0.049473, mean_q: -0.313173
 54300/100000: episode: 543, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.866, mean reward: -0.189 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.180, 10.221], loss: 0.002486, mae: 0.049156, mean_q: -0.352065
 54400/100000: episode: 544, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.757, mean reward: -0.168 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.944, 10.098], loss: 0.002279, mae: 0.047271, mean_q: -0.344874
 54500/100000: episode: 545, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.644, mean reward: -0.146 [-1.000, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.053, 10.585], loss: 0.002383, mae: 0.048523, mean_q: -0.326297
 54600/100000: episode: 546, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.005, mean reward: -0.160 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.931, 10.098], loss: 0.002400, mae: 0.048219, mean_q: -0.349656
 54700/100000: episode: 547, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.170, mean reward: -0.192 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.706, 10.098], loss: 0.002450, mae: 0.049407, mean_q: -0.335364
 54800/100000: episode: 548, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -16.691, mean reward: -0.167 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.922, 10.135], loss: 0.002454, mae: 0.047563, mean_q: -0.368390
 54900/100000: episode: 549, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -19.129, mean reward: -0.191 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.612, 10.223], loss: 0.002415, mae: 0.048860, mean_q: -0.318711
 55000/100000: episode: 550, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.268, mean reward: -0.163 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.608, 10.098], loss: 0.002360, mae: 0.049147, mean_q: -0.345670
 55100/100000: episode: 551, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.713, mean reward: -0.167 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.256, 10.098], loss: 0.003062, mae: 0.054607, mean_q: -0.300696
 55200/100000: episode: 552, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.687, mean reward: -0.177 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.056, 10.109], loss: 0.002935, mae: 0.051867, mean_q: -0.343187
 55300/100000: episode: 553, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.540, mean reward: -0.185 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.080, 10.142], loss: 0.002612, mae: 0.052838, mean_q: -0.315974
 55400/100000: episode: 554, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.123, mean reward: -0.161 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.938, 10.098], loss: 0.002518, mae: 0.049326, mean_q: -0.328966
 55500/100000: episode: 555, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -19.184, mean reward: -0.192 [-1.000, 0.229], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.297, 10.263], loss: 0.002218, mae: 0.045805, mean_q: -0.362870
 55600/100000: episode: 556, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.178, mean reward: -0.172 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.768, 10.098], loss: 0.002282, mae: 0.046790, mean_q: -0.351772
 55700/100000: episode: 557, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.635, mean reward: -0.146 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.103, 10.151], loss: 0.002174, mae: 0.047568, mean_q: -0.322609
 55800/100000: episode: 558, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.289, mean reward: -0.163 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.302, 10.401], loss: 0.002321, mae: 0.048656, mean_q: -0.318208
 55900/100000: episode: 559, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.502, mean reward: -0.185 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.556, 10.098], loss: 0.002386, mae: 0.047931, mean_q: -0.348124
 56000/100000: episode: 560, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.013, mean reward: -0.160 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.941, 10.156], loss: 0.002462, mae: 0.049147, mean_q: -0.327935
 56100/100000: episode: 561, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.494, mean reward: -0.165 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.264, 10.124], loss: 0.002332, mae: 0.047499, mean_q: -0.351314
 56200/100000: episode: 562, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.004, mean reward: -0.170 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.403, 10.098], loss: 0.002324, mae: 0.048296, mean_q: -0.323170
 56300/100000: episode: 563, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.764, mean reward: -0.178 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.952, 10.373], loss: 0.002088, mae: 0.044317, mean_q: -0.344957
 56400/100000: episode: 564, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -15.933, mean reward: -0.159 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.139, 10.098], loss: 0.002208, mae: 0.045331, mean_q: -0.344618
 56500/100000: episode: 565, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -19.386, mean reward: -0.194 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.599, 10.143], loss: 0.002233, mae: 0.046666, mean_q: -0.332312
 56600/100000: episode: 566, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -14.974, mean reward: -0.150 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.764, 10.334], loss: 0.002223, mae: 0.047310, mean_q: -0.322927
 56700/100000: episode: 567, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.937, mean reward: -0.189 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.876, 10.179], loss: 0.002425, mae: 0.048555, mean_q: -0.320969
 56800/100000: episode: 568, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: -18.178, mean reward: -0.182 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.190, 10.258], loss: 0.002276, mae: 0.047811, mean_q: -0.293419
 56900/100000: episode: 569, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.837, mean reward: -0.188 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.731, 10.099], loss: 0.002442, mae: 0.049708, mean_q: -0.317712
 57000/100000: episode: 570, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.679, mean reward: -0.157 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.961, 10.098], loss: 0.002221, mae: 0.047439, mean_q: -0.329121
 57100/100000: episode: 571, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -13.275, mean reward: -0.133 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.065, 10.306], loss: 0.002271, mae: 0.047705, mean_q: -0.337613
 57200/100000: episode: 572, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -17.671, mean reward: -0.177 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.678, 10.098], loss: 0.002348, mae: 0.048508, mean_q: -0.302196
 57300/100000: episode: 573, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.497, mean reward: -0.175 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.611, 10.098], loss: 0.002372, mae: 0.047949, mean_q: -0.342102
 57400/100000: episode: 574, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.074, mean reward: -0.161 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.842, 10.228], loss: 0.002429, mae: 0.049381, mean_q: -0.324155
 57500/100000: episode: 575, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -14.793, mean reward: -0.148 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.985, 10.238], loss: 0.002409, mae: 0.048520, mean_q: -0.328967
 57600/100000: episode: 576, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -19.287, mean reward: -0.193 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.982, 10.197], loss: 0.006409, mae: 0.060733, mean_q: -0.325525
 57700/100000: episode: 577, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.404, mean reward: -0.174 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.827, 10.227], loss: 0.004869, mae: 0.062387, mean_q: -0.308904
 57800/100000: episode: 578, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -20.179, mean reward: -0.202 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.554, 10.167], loss: 0.002304, mae: 0.048496, mean_q: -0.345140
 57900/100000: episode: 579, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -20.422, mean reward: -0.204 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.205, 10.098], loss: 0.002483, mae: 0.049610, mean_q: -0.321521
 58000/100000: episode: 580, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.621, mean reward: -0.186 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.846, 10.164], loss: 0.002306, mae: 0.048124, mean_q: -0.325276
 58100/100000: episode: 581, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -15.956, mean reward: -0.160 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.131, 10.100], loss: 0.002484, mae: 0.049662, mean_q: -0.326661
 58200/100000: episode: 582, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.326, mean reward: -0.183 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.636, 10.145], loss: 0.002455, mae: 0.049257, mean_q: -0.318190
 58300/100000: episode: 583, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.342, mean reward: -0.193 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.414, 10.098], loss: 0.002374, mae: 0.048382, mean_q: -0.339302
 58400/100000: episode: 584, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -13.860, mean reward: -0.139 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.488, 10.098], loss: 0.002329, mae: 0.047167, mean_q: -0.357207
 58500/100000: episode: 585, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.214, mean reward: -0.182 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.200, 10.241], loss: 0.002351, mae: 0.047411, mean_q: -0.324420
 58600/100000: episode: 586, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.960, mean reward: -0.190 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.081, 10.163], loss: 0.002402, mae: 0.048730, mean_q: -0.349712
 58700/100000: episode: 587, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -7.207, mean reward: -0.072 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.269, 10.439], loss: 0.002435, mae: 0.048580, mean_q: -0.359159
 58800/100000: episode: 588, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.016, mean reward: -0.190 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.538, 10.098], loss: 0.002425, mae: 0.049172, mean_q: -0.340857
 58900/100000: episode: 589, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -16.187, mean reward: -0.162 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.751, 10.098], loss: 0.002367, mae: 0.047832, mean_q: -0.317765
 59000/100000: episode: 590, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -15.793, mean reward: -0.158 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.823, 10.098], loss: 0.002206, mae: 0.046160, mean_q: -0.362192
 59100/100000: episode: 591, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.638, mean reward: -0.166 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.933, 10.180], loss: 0.002348, mae: 0.048211, mean_q: -0.327116
 59200/100000: episode: 592, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.271, mean reward: -0.193 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.917, 10.098], loss: 0.002588, mae: 0.051499, mean_q: -0.307892
 59300/100000: episode: 593, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -15.766, mean reward: -0.158 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.391, 10.098], loss: 0.002522, mae: 0.051129, mean_q: -0.294216
 59400/100000: episode: 594, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -14.524, mean reward: -0.145 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.629, 10.209], loss: 0.002464, mae: 0.048416, mean_q: -0.332631
 59500/100000: episode: 595, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.810, mean reward: -0.188 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.687, 10.098], loss: 0.002448, mae: 0.048811, mean_q: -0.318784
 59600/100000: episode: 596, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -20.676, mean reward: -0.207 [-1.000, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.362, 10.182], loss: 0.002468, mae: 0.049094, mean_q: -0.326228
 59700/100000: episode: 597, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.103, mean reward: -0.181 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.923, 10.098], loss: 0.002408, mae: 0.049014, mean_q: -0.311358
 59800/100000: episode: 598, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.516, mean reward: -0.165 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.348, 10.298], loss: 0.002374, mae: 0.048393, mean_q: -0.313541
 59900/100000: episode: 599, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -10.470, mean reward: -0.105 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.401, 10.441], loss: 0.002479, mae: 0.050398, mean_q: -0.331001
 60000/100000: episode: 600, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.222, mean reward: -0.182 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.368, 10.098], loss: 0.002652, mae: 0.050701, mean_q: -0.321155
 60100/100000: episode: 601, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.429, mean reward: -0.174 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.529, 10.098], loss: 0.002407, mae: 0.049064, mean_q: -0.341398
 60200/100000: episode: 602, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -20.987, mean reward: -0.210 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.723, 10.118], loss: 0.003377, mae: 0.057151, mean_q: -0.298106
 60300/100000: episode: 603, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -19.517, mean reward: -0.195 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.740, 10.111], loss: 0.002597, mae: 0.050438, mean_q: -0.325260
 60400/100000: episode: 604, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.520, mean reward: -0.165 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.437, 10.098], loss: 0.002614, mae: 0.049530, mean_q: -0.326603
 60500/100000: episode: 605, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.062, mean reward: -0.151 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.704, 10.248], loss: 0.002654, mae: 0.050605, mean_q: -0.322028
 60600/100000: episode: 606, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.188, mean reward: -0.192 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.219, 10.122], loss: 0.007694, mae: 0.068386, mean_q: -0.314341
 60700/100000: episode: 607, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.806, mean reward: -0.178 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.484, 10.098], loss: 0.004049, mae: 0.058011, mean_q: -0.362884
 60800/100000: episode: 608, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -19.370, mean reward: -0.194 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.485, 10.117], loss: 0.002538, mae: 0.051008, mean_q: -0.339984
 60900/100000: episode: 609, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -8.870, mean reward: -0.089 [-1.000, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.648, 10.098], loss: 0.002443, mae: 0.049183, mean_q: -0.336904
 61000/100000: episode: 610, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -15.103, mean reward: -0.151 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.831, 10.121], loss: 0.002456, mae: 0.048590, mean_q: -0.351031
 61100/100000: episode: 611, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.241, mean reward: -0.172 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.520, 10.239], loss: 0.002582, mae: 0.050096, mean_q: -0.336553
 61200/100000: episode: 612, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.641, mean reward: -0.176 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.885, 10.209], loss: 0.002619, mae: 0.050824, mean_q: -0.326828
 61300/100000: episode: 613, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -20.226, mean reward: -0.202 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.216, 10.157], loss: 0.002795, mae: 0.052260, mean_q: -0.366868
 61400/100000: episode: 614, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -3.076, mean reward: -0.031 [-1.000, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.767, 10.428], loss: 0.002613, mae: 0.050990, mean_q: -0.278574
 61500/100000: episode: 615, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.037, mean reward: -0.190 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.326, 10.244], loss: 0.002777, mae: 0.052449, mean_q: -0.315962
 61600/100000: episode: 616, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.919, mean reward: -0.189 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.643, 10.240], loss: 0.002612, mae: 0.050835, mean_q: -0.350189
 61700/100000: episode: 617, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -13.708, mean reward: -0.137 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.299, 10.300], loss: 0.002620, mae: 0.050663, mean_q: -0.346699
 61800/100000: episode: 618, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -16.242, mean reward: -0.162 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.848, 10.435], loss: 0.002540, mae: 0.049389, mean_q: -0.342433
 61900/100000: episode: 619, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -9.760, mean reward: -0.098 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.769, 10.098], loss: 0.002562, mae: 0.049331, mean_q: -0.331500
 62000/100000: episode: 620, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -17.828, mean reward: -0.178 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.071, 10.098], loss: 0.002565, mae: 0.050789, mean_q: -0.331924
 62100/100000: episode: 621, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.241, mean reward: -0.182 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.993, 10.237], loss: 0.002560, mae: 0.050029, mean_q: -0.330717
 62200/100000: episode: 622, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.866, mean reward: -0.169 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.486, 10.159], loss: 0.002599, mae: 0.050827, mean_q: -0.329145
 62300/100000: episode: 623, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.067, mean reward: -0.171 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.599, 10.098], loss: 0.002434, mae: 0.048438, mean_q: -0.326265
 62400/100000: episode: 624, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.647, mean reward: -0.186 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.899, 10.126], loss: 0.002612, mae: 0.050188, mean_q: -0.327245
 62500/100000: episode: 625, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.830, mean reward: -0.178 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.178, 10.098], loss: 0.002488, mae: 0.049591, mean_q: -0.340794
 62600/100000: episode: 626, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -11.746, mean reward: -0.117 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.874, 10.098], loss: 0.002508, mae: 0.049124, mean_q: -0.313571
 62700/100000: episode: 627, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.569, mean reward: -0.166 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.825, 10.268], loss: 0.002753, mae: 0.052939, mean_q: -0.320691
 62800/100000: episode: 628, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -12.530, mean reward: -0.125 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.799, 10.442], loss: 0.002518, mae: 0.049575, mean_q: -0.299565
 62900/100000: episode: 629, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.005, mean reward: -0.180 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.314, 10.098], loss: 0.002453, mae: 0.048654, mean_q: -0.302834
 63000/100000: episode: 630, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -14.989, mean reward: -0.150 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.733, 10.315], loss: 0.002465, mae: 0.048712, mean_q: -0.345739
 63100/100000: episode: 631, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -17.096, mean reward: -0.171 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.980, 10.326], loss: 0.002437, mae: 0.049677, mean_q: -0.291137
 63200/100000: episode: 632, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -13.579, mean reward: -0.136 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.901, 10.098], loss: 0.002380, mae: 0.048465, mean_q: -0.304176
 63300/100000: episode: 633, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.877, mean reward: -0.169 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.809, 10.098], loss: 0.002257, mae: 0.047007, mean_q: -0.319353
 63400/100000: episode: 634, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.099, mean reward: -0.171 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.869, 10.246], loss: 0.002510, mae: 0.049784, mean_q: -0.288646
 63500/100000: episode: 635, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -13.209, mean reward: -0.132 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.492, 10.536], loss: 0.002501, mae: 0.050455, mean_q: -0.284770
 63600/100000: episode: 636, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.921, mean reward: -0.189 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.995, 10.098], loss: 0.002588, mae: 0.050650, mean_q: -0.306104
 63700/100000: episode: 637, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -12.044, mean reward: -0.120 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.150, 10.098], loss: 0.002541, mae: 0.049535, mean_q: -0.299775
 63800/100000: episode: 638, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.025, mean reward: -0.180 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.870, 10.111], loss: 0.002481, mae: 0.049607, mean_q: -0.316282
 63900/100000: episode: 639, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -12.620, mean reward: -0.126 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.888, 10.198], loss: 0.005471, mae: 0.061881, mean_q: -0.294749
 64000/100000: episode: 640, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.545, mean reward: -0.185 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.424, 10.098], loss: 0.005532, mae: 0.063629, mean_q: -0.304147
 64100/100000: episode: 641, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.630, mean reward: -0.156 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.094, 10.218], loss: 0.002588, mae: 0.050947, mean_q: -0.354102
 64200/100000: episode: 642, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.262, mean reward: -0.163 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.790, 10.363], loss: 0.002321, mae: 0.047870, mean_q: -0.322495
 64300/100000: episode: 643, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.683, mean reward: -0.177 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.996, 10.098], loss: 0.002545, mae: 0.050099, mean_q: -0.314866
 64400/100000: episode: 644, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.893, mean reward: -0.169 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.532, 10.281], loss: 0.002407, mae: 0.048643, mean_q: -0.331563
 64500/100000: episode: 645, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -17.736, mean reward: -0.177 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.651, 10.130], loss: 0.002548, mae: 0.049654, mean_q: -0.323155
 64600/100000: episode: 646, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.869, mean reward: -0.159 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.561, 10.126], loss: 0.002652, mae: 0.050243, mean_q: -0.322513
 64700/100000: episode: 647, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.787, mean reward: -0.198 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.992, 10.253], loss: 0.002590, mae: 0.051188, mean_q: -0.300606
 64800/100000: episode: 648, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.666, mean reward: -0.167 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.108, 10.098], loss: 0.002780, mae: 0.052704, mean_q: -0.284596
 64900/100000: episode: 649, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.773, mean reward: -0.178 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.628, 10.401], loss: 0.002595, mae: 0.049610, mean_q: -0.310264
 65000/100000: episode: 650, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.813, mean reward: -0.178 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.686, 10.098], loss: 0.002552, mae: 0.050204, mean_q: -0.305771
 65100/100000: episode: 651, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -20.426, mean reward: -0.204 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.581, 10.319], loss: 0.002775, mae: 0.051931, mean_q: -0.298575
 65200/100000: episode: 652, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -20.678, mean reward: -0.207 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.860, 10.098], loss: 0.002590, mae: 0.050793, mean_q: -0.276397
 65300/100000: episode: 653, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.794, mean reward: -0.178 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.608, 10.098], loss: 0.002434, mae: 0.048145, mean_q: -0.317579
 65400/100000: episode: 654, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.354, mean reward: -0.184 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.663, 10.098], loss: 0.002413, mae: 0.048685, mean_q: -0.305365
 65500/100000: episode: 655, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.303, mean reward: -0.173 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.695, 10.098], loss: 0.002522, mae: 0.049897, mean_q: -0.305792
 65600/100000: episode: 656, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.342, mean reward: -0.173 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.561, 10.098], loss: 0.002385, mae: 0.048118, mean_q: -0.318187
 65700/100000: episode: 657, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.258, mean reward: -0.153 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.913, 10.098], loss: 0.002475, mae: 0.049204, mean_q: -0.300803
 65800/100000: episode: 658, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.351, mean reward: -0.174 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.818, 10.098], loss: 0.002487, mae: 0.049772, mean_q: -0.292657
 65900/100000: episode: 659, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -15.677, mean reward: -0.157 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.510, 10.302], loss: 0.002559, mae: 0.049634, mean_q: -0.298858
 66000/100000: episode: 660, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.377, mean reward: -0.194 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.402, 10.144], loss: 0.002272, mae: 0.048162, mean_q: -0.289894
 66100/100000: episode: 661, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.990, mean reward: -0.200 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.451, 10.098], loss: 0.002468, mae: 0.049014, mean_q: -0.332413
 66200/100000: episode: 662, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -14.186, mean reward: -0.142 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.744, 10.098], loss: 0.004083, mae: 0.058077, mean_q: -0.324693
 66300/100000: episode: 663, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.884, mean reward: -0.179 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.782, 10.114], loss: 0.002803, mae: 0.051696, mean_q: -0.316846
 66400/100000: episode: 664, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.113, mean reward: -0.181 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.943, 10.205], loss: 0.002442, mae: 0.049290, mean_q: -0.329906
 66500/100000: episode: 665, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.437, mean reward: -0.174 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.387, 10.098], loss: 0.002435, mae: 0.049314, mean_q: -0.321798
 66600/100000: episode: 666, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.474, mean reward: -0.175 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.682, 10.130], loss: 0.002299, mae: 0.046564, mean_q: -0.295401
 66700/100000: episode: 667, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.486, mean reward: -0.175 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.707, 10.098], loss: 0.002086, mae: 0.045333, mean_q: -0.351891
 66800/100000: episode: 668, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.113, mean reward: -0.151 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.860, 10.098], loss: 0.002608, mae: 0.050387, mean_q: -0.315179
 66900/100000: episode: 669, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.538, mean reward: -0.175 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.557, 10.098], loss: 0.002559, mae: 0.050297, mean_q: -0.289307
 67000/100000: episode: 670, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.910, mean reward: -0.189 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.523, 10.098], loss: 0.002422, mae: 0.048654, mean_q: -0.296071
 67100/100000: episode: 671, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -9.839, mean reward: -0.098 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.439, 10.098], loss: 0.002711, mae: 0.051189, mean_q: -0.312888
 67200/100000: episode: 672, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.603, mean reward: -0.176 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.290, 10.098], loss: 0.002425, mae: 0.048851, mean_q: -0.345999
 67300/100000: episode: 673, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.026, mean reward: -0.170 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.631, 10.277], loss: 0.002517, mae: 0.050376, mean_q: -0.289875
 67400/100000: episode: 674, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -14.468, mean reward: -0.145 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.654, 10.170], loss: 0.002304, mae: 0.047027, mean_q: -0.345507
 67500/100000: episode: 675, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.604, mean reward: -0.166 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.833, 10.101], loss: 0.002775, mae: 0.051864, mean_q: -0.331009
 67600/100000: episode: 676, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.094, mean reward: -0.141 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.799, 10.098], loss: 0.002418, mae: 0.048513, mean_q: -0.330551
 67700/100000: episode: 677, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.465, mean reward: -0.155 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.812, 10.098], loss: 0.002431, mae: 0.049061, mean_q: -0.315026
 67800/100000: episode: 678, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.751, mean reward: -0.178 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.557, 10.098], loss: 0.002349, mae: 0.047858, mean_q: -0.351056
 67900/100000: episode: 679, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -14.746, mean reward: -0.147 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.753, 10.098], loss: 0.002571, mae: 0.050584, mean_q: -0.315627
 68000/100000: episode: 680, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.636, mean reward: -0.176 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.840, 10.190], loss: 0.002672, mae: 0.052094, mean_q: -0.343168
 68100/100000: episode: 681, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.102, mean reward: -0.191 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.235, 10.098], loss: 0.002405, mae: 0.049899, mean_q: -0.292602
 68200/100000: episode: 682, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -7.557, mean reward: -0.076 [-1.000, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.418, 10.504], loss: 0.002487, mae: 0.048808, mean_q: -0.314413
 68300/100000: episode: 683, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.695, mean reward: -0.177 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.463, 10.255], loss: 0.002291, mae: 0.047953, mean_q: -0.344570
 68400/100000: episode: 684, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.040, mean reward: -0.190 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.050, 10.098], loss: 0.002364, mae: 0.048631, mean_q: -0.303329
 68500/100000: episode: 685, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.064, mean reward: -0.171 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.849, 10.146], loss: 0.002393, mae: 0.048751, mean_q: -0.314672
 68600/100000: episode: 686, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.711, mean reward: -0.167 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.861, 10.098], loss: 0.002434, mae: 0.049538, mean_q: -0.318807
 68700/100000: episode: 687, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.851, mean reward: -0.169 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.390, 10.246], loss: 0.002422, mae: 0.049975, mean_q: -0.343545
 68800/100000: episode: 688, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -11.848, mean reward: -0.118 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.236, 10.292], loss: 0.002413, mae: 0.048877, mean_q: -0.318765
 68900/100000: episode: 689, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -8.527, mean reward: -0.085 [-1.000, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.267, 10.363], loss: 0.002514, mae: 0.049834, mean_q: -0.319550
 69000/100000: episode: 690, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.196, mean reward: -0.182 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.296, 10.098], loss: 0.002484, mae: 0.050027, mean_q: -0.296902
 69100/100000: episode: 691, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.570, mean reward: -0.166 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.718, 10.192], loss: 0.002417, mae: 0.049485, mean_q: -0.281229
 69200/100000: episode: 692, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -18.435, mean reward: -0.184 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.328, 10.098], loss: 0.002977, mae: 0.053527, mean_q: -0.294713
 69300/100000: episode: 693, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.082, mean reward: -0.181 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.032, 10.109], loss: 0.003097, mae: 0.055301, mean_q: -0.303794
 69400/100000: episode: 694, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.407, mean reward: -0.184 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.472, 10.098], loss: 0.002414, mae: 0.048620, mean_q: -0.328967
 69500/100000: episode: 695, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -15.621, mean reward: -0.156 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.525, 10.267], loss: 0.002348, mae: 0.047677, mean_q: -0.342177
 69600/100000: episode: 696, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.788, mean reward: -0.198 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.470, 10.144], loss: 0.002354, mae: 0.048031, mean_q: -0.297950
 69700/100000: episode: 697, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.890, mean reward: -0.179 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.570, 10.145], loss: 0.002412, mae: 0.048838, mean_q: -0.346354
 69800/100000: episode: 698, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.582, mean reward: -0.146 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.826, 10.136], loss: 0.002468, mae: 0.049086, mean_q: -0.310008
 69900/100000: episode: 699, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.156, mean reward: -0.152 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.367, 10.380], loss: 0.002652, mae: 0.051260, mean_q: -0.320193
 70000/100000: episode: 700, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -9.952, mean reward: -0.100 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.575, 10.098], loss: 0.002407, mae: 0.048850, mean_q: -0.293772
 70100/100000: episode: 701, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.771, mean reward: -0.188 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.138, 10.098], loss: 0.002430, mae: 0.048454, mean_q: -0.312155
 70200/100000: episode: 702, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.538, mean reward: -0.175 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.752, 10.098], loss: 0.002332, mae: 0.047758, mean_q: -0.344137
 70300/100000: episode: 703, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -15.868, mean reward: -0.159 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.401, 10.272], loss: 0.002658, mae: 0.051018, mean_q: -0.286760
 70400/100000: episode: 704, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.456, mean reward: -0.175 [-1.000, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.512, 10.186], loss: 0.002626, mae: 0.051191, mean_q: -0.288656
 70500/100000: episode: 705, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.154, mean reward: -0.132 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.543, 10.207], loss: 0.002554, mae: 0.050463, mean_q: -0.299605
 70600/100000: episode: 706, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -19.782, mean reward: -0.198 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.578, 10.136], loss: 0.002337, mae: 0.047180, mean_q: -0.351942
 70700/100000: episode: 707, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -20.186, mean reward: -0.202 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.659, 10.098], loss: 0.002541, mae: 0.050174, mean_q: -0.259059
 70800/100000: episode: 708, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.129, mean reward: -0.151 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.234, 10.098], loss: 0.002574, mae: 0.050241, mean_q: -0.304657
 70900/100000: episode: 709, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.231, mean reward: -0.162 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.239, 10.225], loss: 0.002987, mae: 0.053641, mean_q: -0.329089
 71000/100000: episode: 710, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.274, mean reward: -0.143 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.807, 10.098], loss: 0.002575, mae: 0.049872, mean_q: -0.315436
 71100/100000: episode: 711, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.335, mean reward: -0.193 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.406, 10.098], loss: 0.002426, mae: 0.047089, mean_q: -0.334247
 71200/100000: episode: 712, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.786, mean reward: -0.168 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.111, 10.262], loss: 0.002426, mae: 0.048992, mean_q: -0.296149
 71300/100000: episode: 713, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.177, mean reward: -0.162 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.703, 10.246], loss: 0.002503, mae: 0.049631, mean_q: -0.315968
 71400/100000: episode: 714, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -21.033, mean reward: -0.210 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.146, 10.208], loss: 0.002694, mae: 0.050781, mean_q: -0.297517
 71500/100000: episode: 715, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.652, mean reward: -0.177 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.104, 10.228], loss: 0.002349, mae: 0.047674, mean_q: -0.338889
 71600/100000: episode: 716, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -11.061, mean reward: -0.111 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.297, 10.298], loss: 0.002522, mae: 0.049763, mean_q: -0.284304
 71700/100000: episode: 717, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -13.737, mean reward: -0.137 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.893, 10.098], loss: 0.002594, mae: 0.051292, mean_q: -0.286960
 71800/100000: episode: 718, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -14.465, mean reward: -0.145 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.789, 10.098], loss: 0.002464, mae: 0.049922, mean_q: -0.319136
 71900/100000: episode: 719, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.310, mean reward: -0.183 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.185, 10.174], loss: 0.002524, mae: 0.049965, mean_q: -0.294640
 72000/100000: episode: 720, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.676, mean reward: -0.177 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.955, 10.127], loss: 0.002651, mae: 0.050391, mean_q: -0.305252
 72100/100000: episode: 721, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.972, mean reward: -0.170 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.981, 10.321], loss: 0.002491, mae: 0.049599, mean_q: -0.307782
 72200/100000: episode: 722, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -13.563, mean reward: -0.136 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.447, 10.199], loss: 0.002428, mae: 0.048222, mean_q: -0.326133
 72300/100000: episode: 723, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -14.366, mean reward: -0.144 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.627, 10.246], loss: 0.003136, mae: 0.051345, mean_q: -0.305555
 72400/100000: episode: 724, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.212, mean reward: -0.182 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.709, 10.098], loss: 0.005930, mae: 0.066375, mean_q: -0.278633
 72500/100000: episode: 725, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.931, mean reward: -0.149 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.910, 10.098], loss: 0.002777, mae: 0.050274, mean_q: -0.317993
 72600/100000: episode: 726, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.735, mean reward: -0.177 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.709, 10.098], loss: 0.002424, mae: 0.048758, mean_q: -0.286794
 72700/100000: episode: 727, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -19.996, mean reward: -0.200 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.746, 10.112], loss: 0.002682, mae: 0.050083, mean_q: -0.300398
 72800/100000: episode: 728, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -13.878, mean reward: -0.139 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.536, 10.098], loss: 0.002542, mae: 0.049081, mean_q: -0.333397
 72900/100000: episode: 729, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -14.320, mean reward: -0.143 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.876, 10.120], loss: 0.002591, mae: 0.049523, mean_q: -0.321933
 73000/100000: episode: 730, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.896, mean reward: -0.179 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.905, 10.098], loss: 0.002694, mae: 0.050916, mean_q: -0.335395
 73100/100000: episode: 731, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.796, mean reward: -0.188 [-1.000, 0.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.862, 10.162], loss: 0.002492, mae: 0.049076, mean_q: -0.302397
 73200/100000: episode: 732, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.490, mean reward: -0.185 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.831, 10.098], loss: 0.002526, mae: 0.049087, mean_q: -0.311549
 73300/100000: episode: 733, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.973, mean reward: -0.160 [-1.000, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.084, 10.283], loss: 0.002564, mae: 0.049065, mean_q: -0.321528
 73400/100000: episode: 734, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.800, mean reward: -0.168 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.481, 10.183], loss: 0.002547, mae: 0.048789, mean_q: -0.322569
 73500/100000: episode: 735, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.724, mean reward: -0.177 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.574, 10.282], loss: 0.002530, mae: 0.048656, mean_q: -0.317463
 73600/100000: episode: 736, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.363, mean reward: -0.164 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.672, 10.328], loss: 0.002713, mae: 0.050165, mean_q: -0.314876
 73700/100000: episode: 737, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.647, mean reward: -0.146 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.769, 10.098], loss: 0.002874, mae: 0.052220, mean_q: -0.292980
 73800/100000: episode: 738, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.920, mean reward: -0.149 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.848, 10.135], loss: 0.002587, mae: 0.049402, mean_q: -0.316204
 73900/100000: episode: 739, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.217, mean reward: -0.182 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.869, 10.098], loss: 0.002828, mae: 0.052321, mean_q: -0.307180
 74000/100000: episode: 740, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -16.437, mean reward: -0.164 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.814, 10.207], loss: 0.002685, mae: 0.051083, mean_q: -0.316592
 74100/100000: episode: 741, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -20.381, mean reward: -0.204 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.062, 10.111], loss: 0.002593, mae: 0.048981, mean_q: -0.324106
 74200/100000: episode: 742, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.729, mean reward: -0.167 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.494, 10.174], loss: 0.002736, mae: 0.051710, mean_q: -0.338099
 74300/100000: episode: 743, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -19.567, mean reward: -0.196 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.355, 10.240], loss: 0.002765, mae: 0.052004, mean_q: -0.337847
 74400/100000: episode: 744, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -7.967, mean reward: -0.080 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.880, 10.098], loss: 0.002519, mae: 0.049731, mean_q: -0.330823
 74500/100000: episode: 745, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.247, mean reward: -0.182 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.536, 10.098], loss: 0.002523, mae: 0.049301, mean_q: -0.315217
 74600/100000: episode: 746, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.314, mean reward: -0.183 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.167, 10.098], loss: 0.002848, mae: 0.052180, mean_q: -0.273727
 74700/100000: episode: 747, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.703, mean reward: -0.177 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.679, 10.236], loss: 0.002564, mae: 0.050337, mean_q: -0.314615
 74800/100000: episode: 748, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -15.825, mean reward: -0.158 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.992, 10.264], loss: 0.002465, mae: 0.048680, mean_q: -0.336566
 74900/100000: episode: 749, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -20.284, mean reward: -0.203 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.763, 10.098], loss: 0.002283, mae: 0.046907, mean_q: -0.327674
 75000/100000: episode: 750, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.756, mean reward: -0.158 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-3.371, 10.098], loss: 0.002417, mae: 0.048409, mean_q: -0.289210
 75100/100000: episode: 751, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.739, mean reward: -0.187 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.152, 10.098], loss: 0.002599, mae: 0.051381, mean_q: -0.295691
 75200/100000: episode: 752, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.071, mean reward: -0.171 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.632, 10.215], loss: 0.002713, mae: 0.052210, mean_q: -0.297888
 75300/100000: episode: 753, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -12.099, mean reward: -0.121 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.587, 10.516], loss: 0.002501, mae: 0.049336, mean_q: -0.299049
 75400/100000: episode: 754, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.680, mean reward: -0.157 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.892, 10.098], loss: 0.002511, mae: 0.049261, mean_q: -0.304889
 75500/100000: episode: 755, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.412, mean reward: -0.164 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.817, 10.149], loss: 0.002356, mae: 0.047680, mean_q: -0.343275
 75600/100000: episode: 756, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -14.425, mean reward: -0.144 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.475, 10.261], loss: 0.002529, mae: 0.049211, mean_q: -0.325558
 75700/100000: episode: 757, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.061, mean reward: -0.181 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.016, 10.098], loss: 0.002307, mae: 0.047069, mean_q: -0.311905
 75800/100000: episode: 758, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -14.271, mean reward: -0.143 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.540, 10.509], loss: 0.002497, mae: 0.049760, mean_q: -0.318209
 75900/100000: episode: 759, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.183, mean reward: -0.192 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.580, 10.194], loss: 0.002728, mae: 0.049845, mean_q: -0.325219
 76000/100000: episode: 760, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.871, mean reward: -0.149 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.178, 10.098], loss: 0.005201, mae: 0.063572, mean_q: -0.303875
 76100/100000: episode: 761, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.257, mean reward: -0.173 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.897, 10.098], loss: 0.004528, mae: 0.059676, mean_q: -0.301782
 76200/100000: episode: 762, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.914, mean reward: -0.159 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.644, 10.246], loss: 0.002384, mae: 0.048595, mean_q: -0.276245
 76300/100000: episode: 763, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.893, mean reward: -0.189 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.262, 10.187], loss: 0.002285, mae: 0.046442, mean_q: -0.341653
 76400/100000: episode: 764, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -14.991, mean reward: -0.150 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.280, 10.331], loss: 0.002328, mae: 0.047977, mean_q: -0.303591
 76500/100000: episode: 765, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -12.418, mean reward: -0.124 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.166, 10.098], loss: 0.002246, mae: 0.046893, mean_q: -0.321141
 76600/100000: episode: 766, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -21.167, mean reward: -0.212 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.374, 10.118], loss: 0.002191, mae: 0.045442, mean_q: -0.295523
 76700/100000: episode: 767, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -19.431, mean reward: -0.194 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.353, 10.205], loss: 0.002404, mae: 0.047644, mean_q: -0.303217
 76800/100000: episode: 768, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.724, mean reward: -0.197 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.753, 10.129], loss: 0.002513, mae: 0.048501, mean_q: -0.327757
 76900/100000: episode: 769, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.838, mean reward: -0.188 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.692, 10.234], loss: 0.002395, mae: 0.047793, mean_q: -0.298924
 77000/100000: episode: 770, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -15.769, mean reward: -0.158 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.312, 10.136], loss: 0.002433, mae: 0.048701, mean_q: -0.323945
 77100/100000: episode: 771, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.844, mean reward: -0.158 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.082, 10.341], loss: 0.002353, mae: 0.047566, mean_q: -0.320443
 77200/100000: episode: 772, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.814, mean reward: -0.158 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.087, 10.264], loss: 0.002301, mae: 0.046697, mean_q: -0.308810
 77300/100000: episode: 773, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.272, mean reward: -0.173 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.645, 10.098], loss: 0.002314, mae: 0.047475, mean_q: -0.325807
 77400/100000: episode: 774, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.358, mean reward: -0.204 [-1.000, 0.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.998, 10.112], loss: 0.002404, mae: 0.047357, mean_q: -0.331401
 77500/100000: episode: 775, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.721, mean reward: -0.167 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.894, 10.098], loss: 0.002447, mae: 0.048401, mean_q: -0.336710
 77600/100000: episode: 776, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.868, mean reward: -0.169 [-1.000, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.716, 10.098], loss: 0.002572, mae: 0.050179, mean_q: -0.312111
 77700/100000: episode: 777, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -12.867, mean reward: -0.129 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.299, 10.098], loss: 0.002803, mae: 0.052018, mean_q: -0.326673
 77800/100000: episode: 778, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -14.377, mean reward: -0.144 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.656, 10.159], loss: 0.002554, mae: 0.049953, mean_q: -0.296558
 77900/100000: episode: 779, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.700, mean reward: -0.177 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.308, 10.328], loss: 0.002420, mae: 0.047769, mean_q: -0.343579
 78000/100000: episode: 780, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.704, mean reward: -0.167 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.576, 10.098], loss: 0.002600, mae: 0.050511, mean_q: -0.338129
 78100/100000: episode: 781, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -20.824, mean reward: -0.208 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.647, 10.139], loss: 0.002570, mae: 0.049470, mean_q: -0.341875
 78200/100000: episode: 782, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.820, mean reward: -0.158 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.583, 10.098], loss: 0.002429, mae: 0.049066, mean_q: -0.326797
 78300/100000: episode: 783, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -20.486, mean reward: -0.205 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.669, 10.108], loss: 0.002414, mae: 0.049386, mean_q: -0.320372
 78400/100000: episode: 784, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -19.638, mean reward: -0.196 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.602, 10.098], loss: 0.002487, mae: 0.049549, mean_q: -0.309914
 78500/100000: episode: 785, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -14.398, mean reward: -0.144 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.284, 10.197], loss: 0.002406, mae: 0.048223, mean_q: -0.331753
 78600/100000: episode: 786, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.215, mean reward: -0.182 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.705, 10.215], loss: 0.002543, mae: 0.049053, mean_q: -0.337125
 78700/100000: episode: 787, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.202, mean reward: -0.172 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.949, 10.205], loss: 0.002773, mae: 0.051737, mean_q: -0.317984
 78800/100000: episode: 788, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.607, mean reward: -0.146 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.041, 10.098], loss: 0.002338, mae: 0.048124, mean_q: -0.330861
 78900/100000: episode: 789, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -20.012, mean reward: -0.200 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.975, 10.159], loss: 0.002385, mae: 0.048305, mean_q: -0.352157
 79000/100000: episode: 790, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -11.738, mean reward: -0.117 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.887, 10.098], loss: 0.002429, mae: 0.048536, mean_q: -0.313504
 79100/100000: episode: 791, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.499, mean reward: -0.175 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.364, 10.166], loss: 0.002588, mae: 0.050928, mean_q: -0.345009
 79200/100000: episode: 792, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.149, mean reward: -0.191 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.517, 10.098], loss: 0.002525, mae: 0.049457, mean_q: -0.317458
 79300/100000: episode: 793, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.498, mean reward: -0.185 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.160, 10.098], loss: 0.002517, mae: 0.050101, mean_q: -0.322956
 79400/100000: episode: 794, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -19.202, mean reward: -0.192 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.652, 10.098], loss: 0.002974, mae: 0.055385, mean_q: -0.301907
 79500/100000: episode: 795, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.604, mean reward: -0.176 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.007, 10.128], loss: 0.002501, mae: 0.049683, mean_q: -0.292008
 79600/100000: episode: 796, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.876, mean reward: -0.129 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.381, 10.098], loss: 0.002582, mae: 0.050269, mean_q: -0.329215
 79700/100000: episode: 797, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -19.255, mean reward: -0.193 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.558, 10.098], loss: 0.002544, mae: 0.050640, mean_q: -0.303410
 79800/100000: episode: 798, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.566, mean reward: -0.146 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.633, 10.317], loss: 0.002529, mae: 0.048685, mean_q: -0.375105
 79900/100000: episode: 799, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.140, mean reward: -0.141 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.615, 10.198], loss: 0.002673, mae: 0.051269, mean_q: -0.301273
 80000/100000: episode: 800, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.499, mean reward: -0.165 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.908, 10.353], loss: 0.002608, mae: 0.050675, mean_q: -0.299202
 80100/100000: episode: 801, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.302, mean reward: -0.163 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.172, 10.098], loss: 0.002688, mae: 0.051682, mean_q: -0.303652
 80200/100000: episode: 802, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.914, mean reward: -0.199 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.092, 10.122], loss: 0.002615, mae: 0.051291, mean_q: -0.300954
 80300/100000: episode: 803, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -20.141, mean reward: -0.201 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.225, 10.098], loss: 0.002660, mae: 0.051320, mean_q: -0.298927
 80400/100000: episode: 804, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.214, mean reward: -0.162 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.102, 10.356], loss: 0.002766, mae: 0.052845, mean_q: -0.320087
 80500/100000: episode: 805, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.499, mean reward: -0.175 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.948, 10.098], loss: 0.002658, mae: 0.050958, mean_q: -0.320352
 80600/100000: episode: 806, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.636, mean reward: -0.176 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.395, 10.335], loss: 0.002930, mae: 0.053686, mean_q: -0.276838
 80700/100000: episode: 807, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.699, mean reward: -0.187 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.196, 10.241], loss: 0.002693, mae: 0.051358, mean_q: -0.324122
 80800/100000: episode: 808, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -19.396, mean reward: -0.194 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.287, 10.213], loss: 0.002468, mae: 0.049322, mean_q: -0.331412
 80900/100000: episode: 809, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.061, mean reward: -0.161 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.963, 10.098], loss: 0.002494, mae: 0.049672, mean_q: -0.383923
 81000/100000: episode: 810, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -20.281, mean reward: -0.203 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.558, 10.136], loss: 0.002585, mae: 0.050570, mean_q: -0.293956
 81100/100000: episode: 811, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.329, mean reward: -0.173 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.662, 10.098], loss: 0.002610, mae: 0.051159, mean_q: -0.318528
 81200/100000: episode: 812, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.634, mean reward: -0.186 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.298, 10.106], loss: 0.002711, mae: 0.052163, mean_q: -0.312575
 81300/100000: episode: 813, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.549, mean reward: -0.145 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.805, 10.393], loss: 0.005055, mae: 0.063503, mean_q: -0.316161
 81400/100000: episode: 814, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.762, mean reward: -0.178 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.625, 10.272], loss: 0.002833, mae: 0.054521, mean_q: -0.317517
 81500/100000: episode: 815, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.019, mean reward: -0.190 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.283, 10.217], loss: 0.002643, mae: 0.051598, mean_q: -0.340647
 81600/100000: episode: 816, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -8.897, mean reward: -0.089 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.639, 10.197], loss: 0.002416, mae: 0.049063, mean_q: -0.355687
 81700/100000: episode: 817, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.129, mean reward: -0.171 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.390, 10.241], loss: 0.002670, mae: 0.050589, mean_q: -0.351213
 81800/100000: episode: 818, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -16.460, mean reward: -0.165 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.780, 10.278], loss: 0.002319, mae: 0.047306, mean_q: -0.336439
 81900/100000: episode: 819, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.012, mean reward: -0.160 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.236, 10.098], loss: 0.002519, mae: 0.050399, mean_q: -0.320745
 82000/100000: episode: 820, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: -16.079, mean reward: -0.161 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.889, 10.098], loss: 0.002503, mae: 0.048815, mean_q: -0.350271
 82100/100000: episode: 821, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.929, mean reward: -0.199 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.325, 10.149], loss: 0.002377, mae: 0.047804, mean_q: -0.343293
 82200/100000: episode: 822, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -7.669, mean reward: -0.077 [-1.000, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.962, 10.098], loss: 0.002591, mae: 0.051813, mean_q: -0.296405
 82300/100000: episode: 823, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.699, mean reward: -0.177 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.758, 10.162], loss: 0.002367, mae: 0.048965, mean_q: -0.334165
 82400/100000: episode: 824, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.698, mean reward: -0.147 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.991, 10.098], loss: 0.002628, mae: 0.051760, mean_q: -0.307889
 82500/100000: episode: 825, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -13.205, mean reward: -0.132 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.544, 10.098], loss: 0.002547, mae: 0.049951, mean_q: -0.327750
 82600/100000: episode: 826, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.130, mean reward: -0.191 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.135, 10.176], loss: 0.002530, mae: 0.050119, mean_q: -0.332652
 82700/100000: episode: 827, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -13.333, mean reward: -0.133 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.602, 10.098], loss: 0.002495, mae: 0.050094, mean_q: -0.291885
 82800/100000: episode: 828, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.016, mean reward: -0.170 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.106, 10.139], loss: 0.002531, mae: 0.050052, mean_q: -0.333270
 82900/100000: episode: 829, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.881, mean reward: -0.169 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.970, 10.098], loss: 0.002456, mae: 0.049634, mean_q: -0.331038
 83000/100000: episode: 830, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.820, mean reward: -0.128 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-2.037, 10.346], loss: 0.002689, mae: 0.051645, mean_q: -0.281939
 83100/100000: episode: 831, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.197, mean reward: -0.152 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.998, 10.219], loss: 0.002414, mae: 0.048350, mean_q: -0.313238
 83200/100000: episode: 832, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -11.616, mean reward: -0.116 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.026, 10.098], loss: 0.002524, mae: 0.049637, mean_q: -0.327181
 83300/100000: episode: 833, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -19.704, mean reward: -0.197 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.146, 10.241], loss: 0.002478, mae: 0.050258, mean_q: -0.308728
 83400/100000: episode: 834, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -11.893, mean reward: -0.119 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.862, 10.098], loss: 0.002666, mae: 0.052329, mean_q: -0.310775
 83500/100000: episode: 835, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.708, mean reward: -0.177 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.529, 10.098], loss: 0.002468, mae: 0.049418, mean_q: -0.302419
 83600/100000: episode: 836, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.641, mean reward: -0.196 [-1.000, 0.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.802, 10.098], loss: 0.002341, mae: 0.047834, mean_q: -0.345063
 83700/100000: episode: 837, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -12.072, mean reward: -0.121 [-1.000, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.549, 10.098], loss: 0.002467, mae: 0.048781, mean_q: -0.295206
 83800/100000: episode: 838, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.302, mean reward: -0.163 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.146, 10.280], loss: 0.002455, mae: 0.048660, mean_q: -0.317182
 83900/100000: episode: 839, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -13.969, mean reward: -0.140 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.459, 10.278], loss: 0.003796, mae: 0.056563, mean_q: -0.303442
 84000/100000: episode: 840, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.034, mean reward: -0.160 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.228, 10.098], loss: 0.004915, mae: 0.062286, mean_q: -0.331093
 84100/100000: episode: 841, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.958, mean reward: -0.140 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.121, 10.098], loss: 0.002523, mae: 0.050316, mean_q: -0.317613
 84200/100000: episode: 842, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.942, mean reward: -0.169 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.716, 10.185], loss: 0.002494, mae: 0.050642, mean_q: -0.284769
 84300/100000: episode: 843, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.280, mean reward: -0.183 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.981, 10.122], loss: 0.002418, mae: 0.048040, mean_q: -0.334412
 84400/100000: episode: 844, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.853, mean reward: -0.179 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.669, 10.098], loss: 0.002438, mae: 0.048411, mean_q: -0.324627
 84500/100000: episode: 845, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.560, mean reward: -0.196 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.430, 10.308], loss: 0.002412, mae: 0.049513, mean_q: -0.299077
 84600/100000: episode: 846, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.232, mean reward: -0.182 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.637, 10.217], loss: 0.002571, mae: 0.050044, mean_q: -0.319118
 84700/100000: episode: 847, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -12.030, mean reward: -0.120 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.407, 10.098], loss: 0.002370, mae: 0.047806, mean_q: -0.347677
 84800/100000: episode: 848, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.584, mean reward: -0.176 [-1.000, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.563, 10.195], loss: 0.002453, mae: 0.048957, mean_q: -0.314363
 84900/100000: episode: 849, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.003, mean reward: -0.130 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.013, 10.098], loss: 0.002225, mae: 0.045627, mean_q: -0.331950
 85000/100000: episode: 850, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.909, mean reward: -0.189 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.891, 10.098], loss: 0.002329, mae: 0.047208, mean_q: -0.302634
 85100/100000: episode: 851, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -11.889, mean reward: -0.119 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.413, 10.196], loss: 0.002385, mae: 0.048296, mean_q: -0.278488
 85200/100000: episode: 852, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.639, mean reward: -0.166 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.842, 10.098], loss: 0.002534, mae: 0.049263, mean_q: -0.332681
 85300/100000: episode: 853, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -21.069, mean reward: -0.211 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.787, 10.098], loss: 0.002369, mae: 0.048106, mean_q: -0.293768
 85400/100000: episode: 854, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.388, mean reward: -0.164 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.365, 10.098], loss: 0.002476, mae: 0.048546, mean_q: -0.280544
 85500/100000: episode: 855, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -13.567, mean reward: -0.136 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.001, 10.407], loss: 0.002192, mae: 0.045385, mean_q: -0.333119
 85600/100000: episode: 856, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -13.174, mean reward: -0.132 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.118, 10.417], loss: 0.002370, mae: 0.046977, mean_q: -0.324341
 85700/100000: episode: 857, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -11.209, mean reward: -0.112 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.660, 10.127], loss: 0.002392, mae: 0.047309, mean_q: -0.322760
 85800/100000: episode: 858, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.353, mean reward: -0.174 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.676, 10.098], loss: 0.002623, mae: 0.049589, mean_q: -0.276068
 85900/100000: episode: 859, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.633, mean reward: -0.186 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.234, 10.154], loss: 0.002376, mae: 0.047653, mean_q: -0.288725
 86000/100000: episode: 860, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.236, mean reward: -0.192 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.989, 10.098], loss: 0.002278, mae: 0.046689, mean_q: -0.287252
 86100/100000: episode: 861, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.999, mean reward: -0.180 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.407, 10.098], loss: 0.002503, mae: 0.049651, mean_q: -0.289350
 86200/100000: episode: 862, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -13.685, mean reward: -0.137 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.912, 10.098], loss: 0.003291, mae: 0.052356, mean_q: -0.319463
 86300/100000: episode: 863, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.107, mean reward: -0.151 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.934, 10.212], loss: 0.005305, mae: 0.061588, mean_q: -0.308629
 86400/100000: episode: 864, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.612, mean reward: -0.166 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.854, 10.196], loss: 0.002441, mae: 0.048212, mean_q: -0.297957
 86500/100000: episode: 865, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.953, mean reward: -0.190 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.933, 10.098], loss: 0.002635, mae: 0.049486, mean_q: -0.308175
 86600/100000: episode: 866, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -10.450, mean reward: -0.104 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.464, 10.406], loss: 0.002457, mae: 0.048572, mean_q: -0.275844
 86700/100000: episode: 867, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -14.120, mean reward: -0.141 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.778, 10.098], loss: 0.002542, mae: 0.050059, mean_q: -0.264595
 86800/100000: episode: 868, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -14.082, mean reward: -0.141 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.645, 10.287], loss: 0.002438, mae: 0.048311, mean_q: -0.304345
 86900/100000: episode: 869, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.000, mean reward: -0.130 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.532, 10.098], loss: 0.002328, mae: 0.047099, mean_q: -0.313766
 87000/100000: episode: 870, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.250, mean reward: -0.162 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.476, 10.098], loss: 0.002413, mae: 0.048397, mean_q: -0.268400
 87100/100000: episode: 871, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.925, mean reward: -0.179 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.173, 10.098], loss: 0.002596, mae: 0.049966, mean_q: -0.280058
 87200/100000: episode: 872, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.543, mean reward: -0.175 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.881, 10.098], loss: 0.002253, mae: 0.046778, mean_q: -0.306977
 87300/100000: episode: 873, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.553, mean reward: -0.186 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.847, 10.098], loss: 0.002503, mae: 0.049661, mean_q: -0.281616
 87400/100000: episode: 874, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -14.519, mean reward: -0.145 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.609, 10.100], loss: 0.002434, mae: 0.048791, mean_q: -0.283754
 87500/100000: episode: 875, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.325, mean reward: -0.143 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.479, 10.492], loss: 0.002337, mae: 0.047115, mean_q: -0.294700
 87600/100000: episode: 876, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -16.831, mean reward: -0.168 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.411, 10.098], loss: 0.002386, mae: 0.048024, mean_q: -0.307954
 87700/100000: episode: 877, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -18.746, mean reward: -0.187 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.286, 10.136], loss: 0.002489, mae: 0.049080, mean_q: -0.268059
 87800/100000: episode: 878, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.622, mean reward: -0.176 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.035, 10.238], loss: 0.002489, mae: 0.049399, mean_q: -0.314570
 87900/100000: episode: 879, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.069, mean reward: -0.181 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.958, 10.133], loss: 0.002613, mae: 0.049525, mean_q: -0.307007
 88000/100000: episode: 880, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.023, mean reward: -0.170 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.426, 10.098], loss: 0.002664, mae: 0.051055, mean_q: -0.281611
 88100/100000: episode: 881, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.368, mean reward: -0.194 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.147, 10.098], loss: 0.002423, mae: 0.049224, mean_q: -0.280820
 88200/100000: episode: 882, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.519, mean reward: -0.185 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.502, 10.108], loss: 0.002838, mae: 0.051173, mean_q: -0.287149
 88300/100000: episode: 883, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.451, mean reward: -0.175 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.639, 10.098], loss: 0.003784, mae: 0.058473, mean_q: -0.320043
 88400/100000: episode: 884, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.258, mean reward: -0.173 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.194, 10.098], loss: 0.002498, mae: 0.050781, mean_q: -0.294676
 88500/100000: episode: 885, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.600, mean reward: -0.166 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.476, 10.098], loss: 0.002588, mae: 0.049751, mean_q: -0.315879
 88600/100000: episode: 886, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.925, mean reward: -0.199 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.481, 10.098], loss: 0.002288, mae: 0.047218, mean_q: -0.312195
 88700/100000: episode: 887, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.236, mean reward: -0.162 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.825, 10.241], loss: 0.002544, mae: 0.049123, mean_q: -0.327406
 88800/100000: episode: 888, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.052, mean reward: -0.161 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.051, 10.098], loss: 0.002294, mae: 0.046764, mean_q: -0.313149
 88900/100000: episode: 889, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -17.101, mean reward: -0.171 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.387, 10.098], loss: 0.002504, mae: 0.049382, mean_q: -0.290789
 89000/100000: episode: 890, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.750, mean reward: -0.178 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.532, 10.286], loss: 0.002501, mae: 0.049971, mean_q: -0.302399
 89100/100000: episode: 891, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.085, mean reward: -0.181 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.809, 10.270], loss: 0.002514, mae: 0.048957, mean_q: -0.333439
 89200/100000: episode: 892, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.941, mean reward: -0.179 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.805, 10.207], loss: 0.002676, mae: 0.050679, mean_q: -0.321797
 89300/100000: episode: 893, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -14.716, mean reward: -0.147 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.814, 10.327], loss: 0.002580, mae: 0.051434, mean_q: -0.313543
 89400/100000: episode: 894, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.882, mean reward: -0.149 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.908, 10.121], loss: 0.002517, mae: 0.049751, mean_q: -0.309042
 89500/100000: episode: 895, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.108, mean reward: -0.181 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.146, 10.098], loss: 0.002299, mae: 0.047093, mean_q: -0.304434
 89600/100000: episode: 896, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.369, mean reward: -0.184 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.782, 10.098], loss: 0.002457, mae: 0.048868, mean_q: -0.331711
 89700/100000: episode: 897, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.213, mean reward: -0.192 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.558, 10.227], loss: 0.002643, mae: 0.050580, mean_q: -0.289292
 89800/100000: episode: 898, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.251, mean reward: -0.173 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.058, 10.239], loss: 0.004732, mae: 0.059673, mean_q: -0.282755
 89900/100000: episode: 899, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -12.050, mean reward: -0.121 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.999, 10.098], loss: 0.002528, mae: 0.050436, mean_q: -0.315678
 90000/100000: episode: 900, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.054, mean reward: -0.171 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.391, 10.098], loss: 0.002541, mae: 0.049384, mean_q: -0.309421
 90100/100000: episode: 901, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.980, mean reward: -0.180 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.876, 10.122], loss: 0.002340, mae: 0.047676, mean_q: -0.310420
 90200/100000: episode: 902, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.648, mean reward: -0.186 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.832, 10.119], loss: 0.002407, mae: 0.047559, mean_q: -0.300877
 90300/100000: episode: 903, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -15.056, mean reward: -0.151 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.557, 10.098], loss: 0.002480, mae: 0.048262, mean_q: -0.330362
 90400/100000: episode: 904, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.887, mean reward: -0.179 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.811, 10.146], loss: 0.002525, mae: 0.048724, mean_q: -0.324004
 90500/100000: episode: 905, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -15.693, mean reward: -0.157 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.413, 10.247], loss: 0.002363, mae: 0.047406, mean_q: -0.336096
 90600/100000: episode: 906, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -13.652, mean reward: -0.137 [-1.000, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.027, 10.098], loss: 0.002499, mae: 0.048371, mean_q: -0.329426
 90700/100000: episode: 907, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.980, mean reward: -0.200 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.723, 10.194], loss: 0.002376, mae: 0.048444, mean_q: -0.331311
 90800/100000: episode: 908, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.524, mean reward: -0.185 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.312, 10.195], loss: 0.002353, mae: 0.048230, mean_q: -0.322663
 90900/100000: episode: 909, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.479, mean reward: -0.165 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.682, 10.098], loss: 0.002474, mae: 0.049384, mean_q: -0.310634
 91000/100000: episode: 910, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.818, mean reward: -0.178 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.643, 10.321], loss: 0.002347, mae: 0.047032, mean_q: -0.311995
 91100/100000: episode: 911, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.034, mean reward: -0.160 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.937, 10.394], loss: 0.002344, mae: 0.047693, mean_q: -0.333164
 91200/100000: episode: 912, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -12.970, mean reward: -0.130 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.183, 10.265], loss: 0.002311, mae: 0.047170, mean_q: -0.309265
 91300/100000: episode: 913, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.297, mean reward: -0.193 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.422, 10.107], loss: 0.002198, mae: 0.046145, mean_q: -0.328017
 91400/100000: episode: 914, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -20.640, mean reward: -0.206 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.855, 10.182], loss: 0.002363, mae: 0.048000, mean_q: -0.305515
 91500/100000: episode: 915, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.238, mean reward: -0.162 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.580, 10.098], loss: 0.002252, mae: 0.046266, mean_q: -0.335003
 91600/100000: episode: 916, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.392, mean reward: -0.194 [-1.000, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.545, 10.098], loss: 0.002504, mae: 0.048889, mean_q: -0.302488
 91700/100000: episode: 917, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.555, mean reward: -0.186 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.243, 10.098], loss: 0.002422, mae: 0.047670, mean_q: -0.341040
 91800/100000: episode: 918, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.290, mean reward: -0.193 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.295, 10.098], loss: 0.002432, mae: 0.048068, mean_q: -0.311723
 91900/100000: episode: 919, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.109, mean reward: -0.171 [-1.000, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.696, 10.098], loss: 0.002390, mae: 0.048324, mean_q: -0.310031
 92000/100000: episode: 920, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -11.688, mean reward: -0.117 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.974, 10.254], loss: 0.002474, mae: 0.048652, mean_q: -0.341399
 92100/100000: episode: 921, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -18.646, mean reward: -0.186 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.404, 10.098], loss: 0.002525, mae: 0.049318, mean_q: -0.334307
 92200/100000: episode: 922, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.573, mean reward: -0.176 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.759, 10.098], loss: 0.002305, mae: 0.047388, mean_q: -0.337247
 92300/100000: episode: 923, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.867, mean reward: -0.169 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.399, 10.146], loss: 0.002653, mae: 0.050591, mean_q: -0.317587
 92400/100000: episode: 924, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.025, mean reward: -0.190 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.904, 10.098], loss: 0.002568, mae: 0.050767, mean_q: -0.289136
 92500/100000: episode: 925, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.659, mean reward: -0.157 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.860, 10.295], loss: 0.002516, mae: 0.049752, mean_q: -0.308860
 92600/100000: episode: 926, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.583, mean reward: -0.176 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.731, 10.109], loss: 0.002280, mae: 0.045870, mean_q: -0.327664
 92700/100000: episode: 927, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -13.573, mean reward: -0.136 [-1.000, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.554, 10.160], loss: 0.002158, mae: 0.046514, mean_q: -0.298774
 92800/100000: episode: 928, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.549, mean reward: -0.165 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.996, 10.341], loss: 0.002480, mae: 0.049961, mean_q: -0.286818
 92900/100000: episode: 929, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.846, mean reward: -0.188 [-1.000, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.659, 10.098], loss: 0.002428, mae: 0.048184, mean_q: -0.313688
 93000/100000: episode: 930, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.565, mean reward: -0.166 [-1.000, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.212, 10.371], loss: 0.002481, mae: 0.049240, mean_q: -0.301968
 93100/100000: episode: 931, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.293, mean reward: -0.173 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.594, 10.214], loss: 0.002478, mae: 0.048629, mean_q: -0.334182
 93200/100000: episode: 932, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -15.937, mean reward: -0.159 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.252, 10.389], loss: 0.002328, mae: 0.048216, mean_q: -0.323280
 93300/100000: episode: 933, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.403, mean reward: -0.184 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.308, 10.136], loss: 0.002367, mae: 0.046669, mean_q: -0.335073
 93400/100000: episode: 934, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -19.633, mean reward: -0.196 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.613, 10.320], loss: 0.002353, mae: 0.047547, mean_q: -0.319980
 93500/100000: episode: 935, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.264, mean reward: -0.153 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.201, 10.098], loss: 0.002607, mae: 0.050383, mean_q: -0.334435
 93600/100000: episode: 936, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.864, mean reward: -0.159 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.294, 10.098], loss: 0.002316, mae: 0.048064, mean_q: -0.298071
 93700/100000: episode: 937, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -20.904, mean reward: -0.209 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.311, 10.200], loss: 0.004428, mae: 0.054675, mean_q: -0.381967
 93800/100000: episode: 938, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.057, mean reward: -0.151 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.282, 10.118], loss: 0.004114, mae: 0.058559, mean_q: -0.357139
 93900/100000: episode: 939, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.101, mean reward: -0.191 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.930, 10.192], loss: 0.002373, mae: 0.048771, mean_q: -0.320461
 94000/100000: episode: 940, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.953, mean reward: -0.170 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.003, 10.142], loss: 0.002312, mae: 0.046609, mean_q: -0.329803
 94100/100000: episode: 941, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.478, mean reward: -0.195 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.567, 10.098], loss: 0.002251, mae: 0.046647, mean_q: -0.317497
 94200/100000: episode: 942, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.394, mean reward: -0.194 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.029, 10.197], loss: 0.002481, mae: 0.048355, mean_q: -0.354345
 94300/100000: episode: 943, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.778, mean reward: -0.188 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.600, 10.098], loss: 0.002274, mae: 0.046367, mean_q: -0.329886
 94400/100000: episode: 944, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.457, mean reward: -0.165 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.530, 10.098], loss: 0.002293, mae: 0.046954, mean_q: -0.335826
 94500/100000: episode: 945, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.081, mean reward: -0.181 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.049, 10.098], loss: 0.002435, mae: 0.048012, mean_q: -0.315536
 94600/100000: episode: 946, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.755, mean reward: -0.168 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.500, 10.098], loss: 0.002414, mae: 0.048747, mean_q: -0.306093
 94700/100000: episode: 947, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.980, mean reward: -0.180 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.781, 10.098], loss: 0.002475, mae: 0.048921, mean_q: -0.299480
 94800/100000: episode: 948, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.794, mean reward: -0.168 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.643, 10.098], loss: 0.002469, mae: 0.049578, mean_q: -0.320391
 94900/100000: episode: 949, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.477, mean reward: -0.175 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.662, 10.305], loss: 0.002247, mae: 0.046601, mean_q: -0.351222
 95000/100000: episode: 950, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.867, mean reward: -0.169 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.397, 10.261], loss: 0.002421, mae: 0.049228, mean_q: -0.272708
 95100/100000: episode: 951, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -13.145, mean reward: -0.131 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.160, 10.490], loss: 0.002361, mae: 0.048730, mean_q: -0.305605
 95200/100000: episode: 952, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.441, mean reward: -0.164 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.511, 10.248], loss: 0.002406, mae: 0.048614, mean_q: -0.310821
 95300/100000: episode: 953, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.118, mean reward: -0.171 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.253, 10.098], loss: 0.002328, mae: 0.047926, mean_q: -0.345678
 95400/100000: episode: 954, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -12.807, mean reward: -0.128 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.955, 10.098], loss: 0.002424, mae: 0.047977, mean_q: -0.323479
 95500/100000: episode: 955, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.855, mean reward: -0.179 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.909, 10.098], loss: 0.002360, mae: 0.048060, mean_q: -0.332901
 95600/100000: episode: 956, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.124, mean reward: -0.191 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.785, 10.173], loss: 0.002265, mae: 0.046870, mean_q: -0.341163
 95700/100000: episode: 957, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -15.667, mean reward: -0.157 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.738, 10.331], loss: 0.002274, mae: 0.048162, mean_q: -0.324785
 95800/100000: episode: 958, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.179, mean reward: -0.172 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.667, 10.100], loss: 0.002358, mae: 0.048385, mean_q: -0.332921
 95900/100000: episode: 959, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -20.275, mean reward: -0.203 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.629, 10.143], loss: 0.002378, mae: 0.048732, mean_q: -0.323902
 96000/100000: episode: 960, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.140, mean reward: -0.171 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.050, 10.098], loss: 0.002335, mae: 0.047886, mean_q: -0.340972
 96100/100000: episode: 961, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.869, mean reward: -0.189 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.061, 10.210], loss: 0.002348, mae: 0.047678, mean_q: -0.328475
 96200/100000: episode: 962, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -9.710, mean reward: -0.097 [-1.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.976, 10.386], loss: 0.002483, mae: 0.048954, mean_q: -0.350646
 96300/100000: episode: 963, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.929, mean reward: -0.189 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.414, 10.179], loss: 0.002322, mae: 0.047579, mean_q: -0.353289
 96400/100000: episode: 964, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -15.597, mean reward: -0.156 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.827, 10.151], loss: 0.002109, mae: 0.046343, mean_q: -0.296633
 96500/100000: episode: 965, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.307, mean reward: -0.183 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.706, 10.248], loss: 0.003170, mae: 0.054426, mean_q: -0.324561
 96600/100000: episode: 966, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -12.748, mean reward: -0.127 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.243, 10.098], loss: 0.002480, mae: 0.049046, mean_q: -0.360608
 96700/100000: episode: 967, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.107, mean reward: -0.151 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.410 [-1.509, 10.098], loss: 0.002470, mae: 0.049065, mean_q: -0.316182
 96800/100000: episode: 968, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.682, mean reward: -0.177 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.333, 10.134], loss: 0.002211, mae: 0.045836, mean_q: -0.342140
 96900/100000: episode: 969, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.472, mean reward: -0.185 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.485, 10.112], loss: 0.002375, mae: 0.047907, mean_q: -0.321800
 97000/100000: episode: 970, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.382, mean reward: -0.184 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.788, 10.153], loss: 0.002250, mae: 0.046464, mean_q: -0.324168
 97100/100000: episode: 971, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.997, mean reward: -0.170 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.255, 10.098], loss: 0.002614, mae: 0.050719, mean_q: -0.319163
 97200/100000: episode: 972, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -20.676, mean reward: -0.207 [-1.000, 0.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.672, 10.098], loss: 0.002412, mae: 0.047853, mean_q: -0.334592
 97300/100000: episode: 973, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -18.240, mean reward: -0.182 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.634, 10.098], loss: 0.002295, mae: 0.047410, mean_q: -0.318837
 97400/100000: episode: 974, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.914, mean reward: -0.189 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.707, 10.205], loss: 0.002383, mae: 0.047966, mean_q: -0.333922
 97500/100000: episode: 975, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -12.613, mean reward: -0.126 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.268, 10.098], loss: 0.002340, mae: 0.048141, mean_q: -0.295629
 97600/100000: episode: 976, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.043, mean reward: -0.170 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.299, 10.098], loss: 0.002479, mae: 0.049433, mean_q: -0.319938
 97700/100000: episode: 977, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.518, mean reward: -0.175 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.642, 10.098], loss: 0.002458, mae: 0.050466, mean_q: -0.295602
 97800/100000: episode: 978, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.091, mean reward: -0.181 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.593, 10.226], loss: 0.002499, mae: 0.049637, mean_q: -0.328547
 97900/100000: episode: 979, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.428, mean reward: -0.164 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.349, 10.098], loss: 0.002547, mae: 0.049262, mean_q: -0.311147
 98000/100000: episode: 980, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -14.183, mean reward: -0.142 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.108, 10.294], loss: 0.002407, mae: 0.048613, mean_q: -0.346276
 98100/100000: episode: 981, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -20.151, mean reward: -0.202 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.818, 10.190], loss: 0.002632, mae: 0.050811, mean_q: -0.321217
 98200/100000: episode: 982, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.608, mean reward: -0.166 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.164, 10.098], loss: 0.002354, mae: 0.048643, mean_q: -0.330655
 98300/100000: episode: 983, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.472, mean reward: -0.175 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.858, 10.098], loss: 0.002416, mae: 0.048173, mean_q: -0.334862
 98400/100000: episode: 984, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.748, mean reward: -0.177 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.966, 10.098], loss: 0.002345, mae: 0.047807, mean_q: -0.336982
 98500/100000: episode: 985, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.124, mean reward: -0.181 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.827, 10.143], loss: 0.002718, mae: 0.051014, mean_q: -0.299530
 98600/100000: episode: 986, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.266, mean reward: -0.173 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.064, 10.231], loss: 0.002616, mae: 0.050968, mean_q: -0.319598
 98700/100000: episode: 987, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -12.233, mean reward: -0.122 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.771, 10.409], loss: 0.002288, mae: 0.046667, mean_q: -0.334261
 98800/100000: episode: 988, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.414, mean reward: -0.164 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.829, 10.343], loss: 0.002346, mae: 0.047651, mean_q: -0.317025
 98900/100000: episode: 989, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -13.754, mean reward: -0.138 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.535, 10.446], loss: 0.002653, mae: 0.051537, mean_q: -0.310336
 99000/100000: episode: 990, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.635, mean reward: -0.166 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.974, 10.098], loss: 0.004990, mae: 0.060515, mean_q: -0.322446
 99100/100000: episode: 991, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.474, mean reward: -0.185 [-1.000, 0.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.164, 10.098], loss: 0.002397, mae: 0.048036, mean_q: -0.329466
 99200/100000: episode: 992, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.467, mean reward: -0.175 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.451, 10.137], loss: 0.002389, mae: 0.047817, mean_q: -0.327577
 99300/100000: episode: 993, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.491, mean reward: -0.155 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.093, 10.098], loss: 0.002462, mae: 0.048310, mean_q: -0.329517
 99400/100000: episode: 994, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -7.727, mean reward: -0.077 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.012, 10.349], loss: 0.002500, mae: 0.049564, mean_q: -0.305186
 99500/100000: episode: 995, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.615, mean reward: -0.166 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.796, 10.190], loss: 0.002418, mae: 0.048111, mean_q: -0.317128
 99600/100000: episode: 996, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.886, mean reward: -0.189 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.741, 10.098], loss: 0.002401, mae: 0.048248, mean_q: -0.290482
 99700/100000: episode: 997, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -12.484, mean reward: -0.125 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.309, 10.098], loss: 0.002680, mae: 0.050107, mean_q: -0.270338
 99800/100000: episode: 998, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -12.357, mean reward: -0.124 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.797, 10.098], loss: 0.002660, mae: 0.050767, mean_q: -0.290402
 99900/100000: episode: 999, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -14.067, mean reward: -0.141 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.744, 10.412], loss: 0.002516, mae: 0.049926, mean_q: -0.300874
 100000/100000: episode: 1000, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -18.216, mean reward: -0.182 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.464, 10.115], loss: 0.002492, mae: 0.048010, mean_q: -0.309607
done, took 520.986 seconds
[Info] End Uniform Random Simulation. Falsification occurred 0 times.
