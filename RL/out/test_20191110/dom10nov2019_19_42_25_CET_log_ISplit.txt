Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.166s, episode steps: 100, steps per second: 603, episode reward: -16.204, mean reward: -0.162 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.862, 10.233], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.061s, episode steps: 100, steps per second: 1631, episode reward: -20.393, mean reward: -0.204 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.616, 10.217], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.065s, episode steps: 100, steps per second: 1539, episode reward: -14.801, mean reward: -0.148 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.616, 10.130], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.062s, episode steps: 100, steps per second: 1614, episode reward: -17.908, mean reward: -0.179 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.769, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.062s, episode steps: 100, steps per second: 1619, episode reward: -17.198, mean reward: -0.172 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.339, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: -12.737, mean reward: -0.127 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.540, 10.098], loss: 0.050387, mae: 0.233587, mean_q: -0.436733
   700/100000: episode: 7, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -19.844, mean reward: -0.198 [-1.000, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.162, 10.249], loss: 0.016182, mae: 0.126111, mean_q: -0.400176
   800/100000: episode: 8, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -17.750, mean reward: -0.178 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.075, 10.098], loss: 0.014131, mae: 0.116559, mean_q: -0.393859
   900/100000: episode: 9, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.432, mean reward: -0.184 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.815, 10.098], loss: 0.012767, mae: 0.108398, mean_q: -0.364297
  1000/100000: episode: 10, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -18.130, mean reward: -0.181 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.186, 10.378], loss: 0.010163, mae: 0.097402, mean_q: -0.359057
  1100/100000: episode: 11, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.237, mean reward: -0.162 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.511, 10.098], loss: 0.009290, mae: 0.091493, mean_q: -0.373705
  1200/100000: episode: 12, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -16.374, mean reward: -0.164 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.769, 10.189], loss: 0.009042, mae: 0.091568, mean_q: -0.355009
  1300/100000: episode: 13, duration: 0.473s, episode steps: 100, steps per second: 212, episode reward: -14.441, mean reward: -0.144 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.982, 10.328], loss: 0.008496, mae: 0.088201, mean_q: -0.314814
  1400/100000: episode: 14, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.427, mean reward: -0.184 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.525, 10.260], loss: 0.008960, mae: 0.090265, mean_q: -0.331159
  1500/100000: episode: 15, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.708, mean reward: -0.167 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.822, 10.131], loss: 0.009489, mae: 0.091042, mean_q: -0.368670
  1600/100000: episode: 16, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -15.829, mean reward: -0.158 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.685, 10.311], loss: 0.009384, mae: 0.094115, mean_q: -0.364086
  1700/100000: episode: 17, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -11.510, mean reward: -0.115 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.338, 10.098], loss: 0.008993, mae: 0.091464, mean_q: -0.338686
  1800/100000: episode: 18, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -11.952, mean reward: -0.120 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.471, 10.098], loss: 0.007427, mae: 0.086318, mean_q: -0.305066
  1900/100000: episode: 19, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.377, mean reward: -0.164 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.993, 10.325], loss: 0.008066, mae: 0.086566, mean_q: -0.301884
  2000/100000: episode: 20, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.767, mean reward: -0.188 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.099, 10.168], loss: 0.006584, mae: 0.078604, mean_q: -0.337708
  2100/100000: episode: 21, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -15.277, mean reward: -0.153 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.263, 10.098], loss: 0.007616, mae: 0.086528, mean_q: -0.351691
  2200/100000: episode: 22, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.258, mean reward: -0.173 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.859, 10.119], loss: 0.007178, mae: 0.080845, mean_q: -0.324867
  2300/100000: episode: 23, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.814, mean reward: -0.168 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.362, 10.119], loss: 0.006650, mae: 0.078392, mean_q: -0.329112
  2400/100000: episode: 24, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -11.967, mean reward: -0.120 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.723, 10.321], loss: 0.006532, mae: 0.075995, mean_q: -0.307743
  2500/100000: episode: 25, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -16.425, mean reward: -0.164 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.842, 10.318], loss: 0.007070, mae: 0.082405, mean_q: -0.353972
  2600/100000: episode: 26, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.635, mean reward: -0.176 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.303, 10.108], loss: 0.006500, mae: 0.080955, mean_q: -0.298309
  2700/100000: episode: 27, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -17.662, mean reward: -0.177 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.826, 10.335], loss: 0.006129, mae: 0.076934, mean_q: -0.310493
  2800/100000: episode: 28, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.286, mean reward: -0.173 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.630, 10.253], loss: 0.006450, mae: 0.078103, mean_q: -0.310813
  2900/100000: episode: 29, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.577, mean reward: -0.186 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.832, 10.098], loss: 0.005981, mae: 0.075032, mean_q: -0.334240
  3000/100000: episode: 30, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.834, mean reward: -0.178 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.514, 10.098], loss: 0.006486, mae: 0.078931, mean_q: -0.310172
  3100/100000: episode: 31, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -18.252, mean reward: -0.183 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.996, 10.150], loss: 0.005703, mae: 0.073865, mean_q: -0.340248
  3200/100000: episode: 32, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -20.368, mean reward: -0.204 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.466, 10.149], loss: 0.006267, mae: 0.078104, mean_q: -0.317457
  3300/100000: episode: 33, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.071, mean reward: -0.171 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.490, 10.098], loss: 0.006128, mae: 0.078912, mean_q: -0.329745
  3400/100000: episode: 34, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.505, mean reward: -0.175 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.890, 10.118], loss: 0.005680, mae: 0.075365, mean_q: -0.299238
  3500/100000: episode: 35, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -20.537, mean reward: -0.205 [-1.000, 0.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.046, 10.126], loss: 0.004776, mae: 0.068214, mean_q: -0.303505
  3600/100000: episode: 36, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.398, mean reward: -0.174 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.995, 10.223], loss: 0.005307, mae: 0.071131, mean_q: -0.343274
  3700/100000: episode: 37, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -11.460, mean reward: -0.115 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.154, 10.098], loss: 0.005948, mae: 0.077238, mean_q: -0.332904
  3800/100000: episode: 38, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.773, mean reward: -0.168 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.858, 10.271], loss: 0.005956, mae: 0.077274, mean_q: -0.313565
  3900/100000: episode: 39, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -20.340, mean reward: -0.203 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.457, 10.100], loss: 0.005668, mae: 0.073648, mean_q: -0.317356
  4000/100000: episode: 40, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.895, mean reward: -0.199 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.335, 10.098], loss: 0.005499, mae: 0.074270, mean_q: -0.308957
  4100/100000: episode: 41, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -20.040, mean reward: -0.200 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.887, 10.098], loss: 0.004292, mae: 0.065847, mean_q: -0.326326
  4200/100000: episode: 42, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.731, mean reward: -0.177 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.463, 10.098], loss: 0.005339, mae: 0.074491, mean_q: -0.351647
  4300/100000: episode: 43, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.697, mean reward: -0.167 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.918, 10.098], loss: 0.004951, mae: 0.070101, mean_q: -0.316026
  4400/100000: episode: 44, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.487, mean reward: -0.175 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.861, 10.286], loss: 0.005155, mae: 0.072911, mean_q: -0.309066
  4500/100000: episode: 45, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -13.304, mean reward: -0.133 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.409, 10.274], loss: 0.005181, mae: 0.071097, mean_q: -0.321324
  4600/100000: episode: 46, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -19.590, mean reward: -0.196 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.691, 10.098], loss: 0.005052, mae: 0.071102, mean_q: -0.327498
  4700/100000: episode: 47, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -10.616, mean reward: -0.106 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.371, 10.098], loss: 0.004991, mae: 0.072030, mean_q: -0.352139
  4800/100000: episode: 48, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -5.817, mean reward: -0.058 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.231, 10.098], loss: 0.005182, mae: 0.072205, mean_q: -0.326906
  4900/100000: episode: 49, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -15.690, mean reward: -0.157 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.378, 10.098], loss: 0.005197, mae: 0.073020, mean_q: -0.306936
  5000/100000: episode: 50, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -13.470, mean reward: -0.135 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.587, 10.192], loss: 0.004570, mae: 0.068930, mean_q: -0.313298
  5100/100000: episode: 51, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.413, mean reward: -0.174 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.622, 10.098], loss: 0.005204, mae: 0.071478, mean_q: -0.312057
  5200/100000: episode: 52, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -15.216, mean reward: -0.152 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.667, 10.098], loss: 0.005089, mae: 0.073242, mean_q: -0.274541
  5300/100000: episode: 53, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.294, mean reward: -0.183 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.602, 10.220], loss: 0.004874, mae: 0.071424, mean_q: -0.315641
  5400/100000: episode: 54, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -10.541, mean reward: -0.105 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.196, 10.359], loss: 0.004470, mae: 0.069706, mean_q: -0.323134
  5500/100000: episode: 55, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -17.722, mean reward: -0.177 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.538, 10.181], loss: 0.005503, mae: 0.074177, mean_q: -0.308953
  5600/100000: episode: 56, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -17.287, mean reward: -0.173 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.844, 10.098], loss: 0.004879, mae: 0.071578, mean_q: -0.307500
  5700/100000: episode: 57, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -18.173, mean reward: -0.182 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.756, 10.342], loss: 0.005043, mae: 0.070942, mean_q: -0.294842
  5800/100000: episode: 58, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -16.687, mean reward: -0.167 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.959, 10.244], loss: 0.004585, mae: 0.071527, mean_q: -0.300552
  5900/100000: episode: 59, duration: 0.466s, episode steps: 100, steps per second: 215, episode reward: -15.200, mean reward: -0.152 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.937, 10.148], loss: 0.005041, mae: 0.074147, mean_q: -0.309321
  6000/100000: episode: 60, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -19.022, mean reward: -0.190 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.493, 10.103], loss: 0.005157, mae: 0.073569, mean_q: -0.299721
  6100/100000: episode: 61, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -18.102, mean reward: -0.181 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.420, 10.098], loss: 0.004943, mae: 0.072075, mean_q: -0.309208
  6200/100000: episode: 62, duration: 0.473s, episode steps: 100, steps per second: 212, episode reward: -19.081, mean reward: -0.191 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.815, 10.197], loss: 0.004670, mae: 0.069483, mean_q: -0.309281
  6300/100000: episode: 63, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -7.894, mean reward: -0.079 [-1.000, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.901, 10.539], loss: 0.005190, mae: 0.074195, mean_q: -0.289253
  6400/100000: episode: 64, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -18.701, mean reward: -0.187 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.777, 10.109], loss: 0.005014, mae: 0.071350, mean_q: -0.317078
  6500/100000: episode: 65, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -18.355, mean reward: -0.184 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.595, 10.098], loss: 0.004875, mae: 0.070578, mean_q: -0.299800
  6600/100000: episode: 66, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -15.867, mean reward: -0.159 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.164, 10.098], loss: 0.005291, mae: 0.073252, mean_q: -0.297996
  6700/100000: episode: 67, duration: 0.466s, episode steps: 100, steps per second: 215, episode reward: -15.241, mean reward: -0.152 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.248, 10.179], loss: 0.005044, mae: 0.071848, mean_q: -0.315088
  6800/100000: episode: 68, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -14.611, mean reward: -0.146 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.527, 10.203], loss: 0.005087, mae: 0.073439, mean_q: -0.292272
  6900/100000: episode: 69, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -15.683, mean reward: -0.157 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.635, 10.179], loss: 0.005615, mae: 0.075616, mean_q: -0.306109
  7000/100000: episode: 70, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -10.006, mean reward: -0.100 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.475, 10.098], loss: 0.005568, mae: 0.075088, mean_q: -0.311773
  7100/100000: episode: 71, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -17.778, mean reward: -0.178 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.515, 10.205], loss: 0.005038, mae: 0.072352, mean_q: -0.297519
  7200/100000: episode: 72, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -16.513, mean reward: -0.165 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.605, 10.098], loss: 0.004649, mae: 0.069644, mean_q: -0.331807
  7300/100000: episode: 73, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -18.414, mean reward: -0.184 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.398, 10.188], loss: 0.004975, mae: 0.071990, mean_q: -0.315887
  7400/100000: episode: 74, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -18.262, mean reward: -0.183 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.170, 10.107], loss: 0.005013, mae: 0.072847, mean_q: -0.318858
  7500/100000: episode: 75, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -18.345, mean reward: -0.183 [-1.000, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.911, 10.098], loss: 0.006695, mae: 0.081535, mean_q: -0.275122
  7600/100000: episode: 76, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -15.278, mean reward: -0.153 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.629, 10.098], loss: 0.007390, mae: 0.085982, mean_q: -0.279248
  7700/100000: episode: 77, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -13.065, mean reward: -0.131 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.729, 10.292], loss: 0.005157, mae: 0.073667, mean_q: -0.308849
  7800/100000: episode: 78, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -18.548, mean reward: -0.185 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.366, 10.208], loss: 0.004426, mae: 0.067257, mean_q: -0.316546
  7900/100000: episode: 79, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -16.587, mean reward: -0.166 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.496, 10.098], loss: 0.004706, mae: 0.072017, mean_q: -0.293203
  8000/100000: episode: 80, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.163, mean reward: -0.182 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.713, 10.098], loss: 0.005554, mae: 0.076201, mean_q: -0.356375
  8100/100000: episode: 81, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -14.128, mean reward: -0.141 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.854, 10.226], loss: 0.004446, mae: 0.067692, mean_q: -0.302636
  8200/100000: episode: 82, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -4.953, mean reward: -0.050 [-1.000, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.949, 10.098], loss: 0.004121, mae: 0.065405, mean_q: -0.304412
  8300/100000: episode: 83, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -14.536, mean reward: -0.145 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.593, 10.169], loss: 0.004500, mae: 0.067932, mean_q: -0.315456
  8400/100000: episode: 84, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -19.245, mean reward: -0.192 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.842, 10.098], loss: 0.004419, mae: 0.067977, mean_q: -0.270966
  8500/100000: episode: 85, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -17.178, mean reward: -0.172 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.822, 10.098], loss: 0.004296, mae: 0.067465, mean_q: -0.312891
  8600/100000: episode: 86, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -19.371, mean reward: -0.194 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.602, 10.145], loss: 0.004687, mae: 0.069636, mean_q: -0.289088
  8700/100000: episode: 87, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.234, mean reward: -0.152 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.638, 10.146], loss: 0.005057, mae: 0.073823, mean_q: -0.339134
  8800/100000: episode: 88, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -19.827, mean reward: -0.198 [-1.000, 0.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.731, 10.103], loss: 0.004613, mae: 0.070562, mean_q: -0.324852
  8900/100000: episode: 89, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -14.581, mean reward: -0.146 [-1.000, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.341, 10.176], loss: 0.005021, mae: 0.073038, mean_q: -0.313324
  9000/100000: episode: 90, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -16.497, mean reward: -0.165 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.126, 10.098], loss: 0.005116, mae: 0.074112, mean_q: -0.304677
  9100/100000: episode: 91, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -12.237, mean reward: -0.122 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.588, 10.098], loss: 0.005375, mae: 0.074436, mean_q: -0.305906
  9200/100000: episode: 92, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -12.483, mean reward: -0.125 [-1.000, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.806, 10.098], loss: 0.004415, mae: 0.067494, mean_q: -0.325213
  9300/100000: episode: 93, duration: 0.462s, episode steps: 100, steps per second: 217, episode reward: -16.843, mean reward: -0.168 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.038, 10.098], loss: 0.005236, mae: 0.076147, mean_q: -0.301185
  9400/100000: episode: 94, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -17.103, mean reward: -0.171 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.489, 10.206], loss: 0.004511, mae: 0.070325, mean_q: -0.266636
  9500/100000: episode: 95, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -18.979, mean reward: -0.190 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.465, 10.172], loss: 0.004272, mae: 0.067847, mean_q: -0.290302
  9600/100000: episode: 96, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -17.389, mean reward: -0.174 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.644, 10.264], loss: 0.004099, mae: 0.065803, mean_q: -0.302175
  9700/100000: episode: 97, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -18.427, mean reward: -0.184 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.734, 10.182], loss: 0.004455, mae: 0.068297, mean_q: -0.292004
  9800/100000: episode: 98, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -19.681, mean reward: -0.197 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.426, 10.113], loss: 0.003879, mae: 0.064271, mean_q: -0.303277
  9900/100000: episode: 99, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -16.576, mean reward: -0.166 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.795, 10.138], loss: 0.004015, mae: 0.064093, mean_q: -0.287807
 10000/100000: episode: 100, duration: 0.465s, episode steps: 100, steps per second: 215, episode reward: -18.970, mean reward: -0.190 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.915, 10.252], loss: 0.004516, mae: 0.067916, mean_q: -0.289855
 10100/100000: episode: 101, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -18.437, mean reward: -0.184 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.781, 10.239], loss: 0.004158, mae: 0.064492, mean_q: -0.278687
 10200/100000: episode: 102, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.166, mean reward: -0.182 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.292, 10.258], loss: 0.004055, mae: 0.065046, mean_q: -0.280751
 10300/100000: episode: 103, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -11.010, mean reward: -0.110 [-1.000, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.774, 10.098], loss: 0.003904, mae: 0.063755, mean_q: -0.316789
 10400/100000: episode: 104, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -17.339, mean reward: -0.173 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.803, 10.166], loss: 0.003952, mae: 0.064739, mean_q: -0.318347
 10500/100000: episode: 105, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -18.954, mean reward: -0.190 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.348, 10.098], loss: 0.004275, mae: 0.065393, mean_q: -0.300549
 10600/100000: episode: 106, duration: 0.480s, episode steps: 100, steps per second: 209, episode reward: -11.754, mean reward: -0.118 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.859, 10.098], loss: 0.004275, mae: 0.065670, mean_q: -0.332367
 10700/100000: episode: 107, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -20.351, mean reward: -0.204 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.383, 10.098], loss: 0.003782, mae: 0.063736, mean_q: -0.292084
 10800/100000: episode: 108, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -18.380, mean reward: -0.184 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.054, 10.288], loss: 0.005224, mae: 0.071952, mean_q: -0.320570
 10900/100000: episode: 109, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -18.645, mean reward: -0.186 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.497, 10.171], loss: 0.003825, mae: 0.064112, mean_q: -0.331000
 11000/100000: episode: 110, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -20.445, mean reward: -0.204 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.645, 10.098], loss: 0.004305, mae: 0.065267, mean_q: -0.303569
 11100/100000: episode: 111, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -14.064, mean reward: -0.141 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.591, 10.098], loss: 0.005622, mae: 0.076252, mean_q: -0.323599
 11200/100000: episode: 112, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -15.147, mean reward: -0.151 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.217, 10.098], loss: 0.004198, mae: 0.067996, mean_q: -0.320593
 11300/100000: episode: 113, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -8.000, mean reward: -0.080 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.328, 10.357], loss: 0.003276, mae: 0.058836, mean_q: -0.282219
 11400/100000: episode: 114, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.439, mean reward: -0.184 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.899, 10.488], loss: 0.003929, mae: 0.064727, mean_q: -0.299558
 11500/100000: episode: 115, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.897, mean reward: -0.179 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.096, 10.119], loss: 0.003818, mae: 0.063621, mean_q: -0.291251
 11600/100000: episode: 116, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -14.796, mean reward: -0.148 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.765, 10.098], loss: 0.004017, mae: 0.065634, mean_q: -0.303669
 11700/100000: episode: 117, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -17.235, mean reward: -0.172 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.492, 10.263], loss: 0.004442, mae: 0.069099, mean_q: -0.280519
 11800/100000: episode: 118, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -19.774, mean reward: -0.198 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.893, 10.249], loss: 0.003699, mae: 0.062707, mean_q: -0.308025
 11900/100000: episode: 119, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -13.722, mean reward: -0.137 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.787, 10.270], loss: 0.003969, mae: 0.063657, mean_q: -0.307017
 12000/100000: episode: 120, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -10.400, mean reward: -0.104 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.634, 10.413], loss: 0.003900, mae: 0.064874, mean_q: -0.295461
 12100/100000: episode: 121, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -15.950, mean reward: -0.160 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.747, 10.207], loss: 0.003631, mae: 0.062735, mean_q: -0.303985
 12200/100000: episode: 122, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.370, mean reward: -0.174 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.530, 10.098], loss: 0.003774, mae: 0.063390, mean_q: -0.331761
 12300/100000: episode: 123, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -16.920, mean reward: -0.169 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.291, 10.098], loss: 0.004156, mae: 0.067485, mean_q: -0.285198
 12400/100000: episode: 124, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -15.429, mean reward: -0.154 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.545, 10.353], loss: 0.004799, mae: 0.070842, mean_q: -0.317346
 12500/100000: episode: 125, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.549, mean reward: -0.185 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.012, 10.098], loss: 0.003924, mae: 0.065991, mean_q: -0.304500
 12600/100000: episode: 126, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -14.930, mean reward: -0.149 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.779, 10.142], loss: 0.003883, mae: 0.064631, mean_q: -0.271601
 12700/100000: episode: 127, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.454, mean reward: -0.175 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.522, 10.098], loss: 0.003823, mae: 0.063303, mean_q: -0.304792
 12800/100000: episode: 128, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.637, mean reward: -0.176 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.159, 10.098], loss: 0.004136, mae: 0.066755, mean_q: -0.285607
 12900/100000: episode: 129, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -18.579, mean reward: -0.186 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.573, 10.260], loss: 0.003683, mae: 0.062910, mean_q: -0.309848
 13000/100000: episode: 130, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: -18.895, mean reward: -0.189 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.306, 10.179], loss: 0.004450, mae: 0.068435, mean_q: -0.327255
 13100/100000: episode: 131, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -18.621, mean reward: -0.186 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.808, 10.169], loss: 0.004004, mae: 0.065811, mean_q: -0.281204
 13200/100000: episode: 132, duration: 0.480s, episode steps: 100, steps per second: 209, episode reward: -13.310, mean reward: -0.133 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.274, 10.403], loss: 0.003148, mae: 0.056993, mean_q: -0.324490
 13300/100000: episode: 133, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -11.336, mean reward: -0.113 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.712, 10.176], loss: 0.003718, mae: 0.061625, mean_q: -0.337575
 13400/100000: episode: 134, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -18.456, mean reward: -0.185 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.454, 10.215], loss: 0.003598, mae: 0.061248, mean_q: -0.321545
 13500/100000: episode: 135, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -19.416, mean reward: -0.194 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.823, 10.130], loss: 0.004492, mae: 0.069965, mean_q: -0.301981
 13600/100000: episode: 136, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -15.255, mean reward: -0.153 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.994, 10.123], loss: 0.004526, mae: 0.069470, mean_q: -0.318269
 13700/100000: episode: 137, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.268, mean reward: -0.183 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.263, 10.288], loss: 0.003596, mae: 0.062471, mean_q: -0.311722
 13800/100000: episode: 138, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -15.635, mean reward: -0.156 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.563, 10.315], loss: 0.003392, mae: 0.059778, mean_q: -0.300527
 13900/100000: episode: 139, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -17.462, mean reward: -0.175 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.054, 10.098], loss: 0.003753, mae: 0.063585, mean_q: -0.300499
 14000/100000: episode: 140, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -20.513, mean reward: -0.205 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.118, 10.101], loss: 0.003359, mae: 0.059539, mean_q: -0.305535
 14100/100000: episode: 141, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -16.880, mean reward: -0.169 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.472, 10.098], loss: 0.003236, mae: 0.058431, mean_q: -0.351520
 14200/100000: episode: 142, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.453, mean reward: -0.185 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.872, 10.222], loss: 0.003507, mae: 0.061046, mean_q: -0.310428
 14300/100000: episode: 143, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -14.679, mean reward: -0.147 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.560, 10.247], loss: 0.003257, mae: 0.057577, mean_q: -0.344352
 14400/100000: episode: 144, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -13.550, mean reward: -0.135 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.899, 10.098], loss: 0.003289, mae: 0.059140, mean_q: -0.301501
 14500/100000: episode: 145, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.829, mean reward: -0.158 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.869, 10.300], loss: 0.003611, mae: 0.063403, mean_q: -0.308150
 14600/100000: episode: 146, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -18.750, mean reward: -0.188 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.214, 10.098], loss: 0.003683, mae: 0.061982, mean_q: -0.302891
 14700/100000: episode: 147, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -16.999, mean reward: -0.170 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.565, 10.215], loss: 0.003306, mae: 0.060195, mean_q: -0.298486
 14800/100000: episode: 148, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -14.484, mean reward: -0.145 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.765, 10.098], loss: 0.003527, mae: 0.061643, mean_q: -0.318290
 14900/100000: episode: 149, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -17.363, mean reward: -0.174 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.269, 10.426], loss: 0.003015, mae: 0.057367, mean_q: -0.271850
[Info] 100-TH LEVEL FOUND: 0.5582639575004578, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.255s, episode steps: 100, steps per second: 24, episode reward: -16.968, mean reward: -0.170 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.574, 10.098], loss: 0.003128, mae: 0.057272, mean_q: -0.336659
 15020/100000: episode: 151, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 7.286, mean reward: 0.364 [0.303, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.233, 10.100], loss: 0.003385, mae: 0.058012, mean_q: -0.437787
 15033/100000: episode: 152, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 4.129, mean reward: 0.318 [0.261, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.522, 10.100], loss: 0.005250, mae: 0.069274, mean_q: -0.360070
 15041/100000: episode: 153, duration: 0.040s, episode steps: 8, steps per second: 201, episode reward: 3.135, mean reward: 0.392 [0.327, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.350, 10.100], loss: 0.004755, mae: 0.073762, mean_q: -0.311760
 15070/100000: episode: 154, duration: 0.157s, episode steps: 29, steps per second: 184, episode reward: 8.229, mean reward: 0.284 [0.095, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.169, 10.100], loss: 0.004723, mae: 0.071826, mean_q: -0.278158
 15093/100000: episode: 155, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 7.157, mean reward: 0.311 [0.210, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.321], loss: 0.003733, mae: 0.061454, mean_q: -0.264331
 15101/100000: episode: 156, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 2.900, mean reward: 0.363 [0.313, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.262, 10.100], loss: 0.003415, mae: 0.060097, mean_q: -0.292838
 15118/100000: episode: 157, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 8.067, mean reward: 0.475 [0.355, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.907, 10.100], loss: 0.004666, mae: 0.074346, mean_q: -0.209663
 15128/100000: episode: 158, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 2.644, mean reward: 0.264 [0.196, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.342, 10.100], loss: 0.003836, mae: 0.070980, mean_q: -0.183609
 15141/100000: episode: 159, duration: 0.064s, episode steps: 13, steps per second: 205, episode reward: 4.491, mean reward: 0.345 [0.288, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.176, 10.100], loss: 0.003323, mae: 0.059830, mean_q: -0.287160
 15151/100000: episode: 160, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 2.347, mean reward: 0.235 [0.186, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.297, 10.100], loss: 0.003818, mae: 0.064996, mean_q: -0.302992
 15160/100000: episode: 161, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 2.940, mean reward: 0.327 [0.275, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.301, 10.100], loss: 0.003661, mae: 0.064157, mean_q: -0.265925
 15173/100000: episode: 162, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 3.812, mean reward: 0.293 [0.237, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.232, 10.100], loss: 0.003975, mae: 0.061239, mean_q: -0.257074
 15182/100000: episode: 163, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 2.352, mean reward: 0.261 [0.235, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.265, 10.100], loss: 0.003042, mae: 0.059983, mean_q: -0.265107
 15199/100000: episode: 164, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 5.626, mean reward: 0.331 [0.181, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.500, 10.100], loss: 0.003714, mae: 0.066154, mean_q: -0.152033
 15222/100000: episode: 165, duration: 0.125s, episode steps: 23, steps per second: 183, episode reward: 8.860, mean reward: 0.385 [0.241, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.954, 10.397], loss: 0.003250, mae: 0.058827, mean_q: -0.287478
 15231/100000: episode: 166, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 2.811, mean reward: 0.312 [0.250, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.319, 10.100], loss: 0.004097, mae: 0.066400, mean_q: -0.281469
 15260/100000: episode: 167, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 7.975, mean reward: 0.275 [0.145, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.669, 10.100], loss: 0.003852, mae: 0.068525, mean_q: -0.238728
 15283/100000: episode: 168, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 8.993, mean reward: 0.391 [0.263, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.555], loss: 0.002906, mae: 0.056831, mean_q: -0.268852
 15291/100000: episode: 169, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 3.223, mean reward: 0.403 [0.325, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.330, 10.100], loss: 0.003215, mae: 0.059284, mean_q: -0.245934
 15308/100000: episode: 170, duration: 0.080s, episode steps: 17, steps per second: 213, episode reward: 5.829, mean reward: 0.343 [0.260, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.379, 10.100], loss: 0.003160, mae: 0.058675, mean_q: -0.267194
 15337/100000: episode: 171, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 9.764, mean reward: 0.337 [0.230, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.253, 10.100], loss: 0.003323, mae: 0.059185, mean_q: -0.219195
 15350/100000: episode: 172, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 3.664, mean reward: 0.282 [0.125, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.123, 10.100], loss: 0.003577, mae: 0.063388, mean_q: -0.237868
 15359/100000: episode: 173, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 2.230, mean reward: 0.248 [0.204, 0.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.227, 10.100], loss: 0.003720, mae: 0.064910, mean_q: -0.213221
 15367/100000: episode: 174, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 3.169, mean reward: 0.396 [0.316, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.411, 10.100], loss: 0.003899, mae: 0.064333, mean_q: -0.133659
 15390/100000: episode: 175, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 10.546, mean reward: 0.459 [0.325, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.121, 10.496], loss: 0.004353, mae: 0.067062, mean_q: -0.285403
 15403/100000: episode: 176, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 4.152, mean reward: 0.319 [0.277, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.502, 10.100], loss: 0.004013, mae: 0.063487, mean_q: -0.214650
 15420/100000: episode: 177, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 5.946, mean reward: 0.350 [0.312, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.364, 10.100], loss: 0.004651, mae: 0.070929, mean_q: -0.246038
 15437/100000: episode: 178, duration: 0.081s, episode steps: 17, steps per second: 210, episode reward: 5.210, mean reward: 0.306 [0.192, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.150, 10.100], loss: 0.003863, mae: 0.063444, mean_q: -0.221803
 15445/100000: episode: 179, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 2.420, mean reward: 0.302 [0.220, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.202, 10.100], loss: 0.003243, mae: 0.060690, mean_q: -0.268302
 15462/100000: episode: 180, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 5.944, mean reward: 0.350 [0.223, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.258, 10.100], loss: 0.003057, mae: 0.059739, mean_q: -0.172156
 15485/100000: episode: 181, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 10.752, mean reward: 0.467 [0.383, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.470], loss: 0.003348, mae: 0.058933, mean_q: -0.214893
 15495/100000: episode: 182, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 3.472, mean reward: 0.347 [0.286, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.357, 10.100], loss: 0.004074, mae: 0.064656, mean_q: -0.203207
 15524/100000: episode: 183, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 6.797, mean reward: 0.234 [0.096, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-1.393, 10.100], loss: 0.003215, mae: 0.057715, mean_q: -0.252877
 15534/100000: episode: 184, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 2.609, mean reward: 0.261 [0.201, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.207, 10.100], loss: 0.002888, mae: 0.055633, mean_q: -0.119372
 15543/100000: episode: 185, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 2.749, mean reward: 0.305 [0.248, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.327, 10.100], loss: 0.002911, mae: 0.057456, mean_q: -0.207343
 15553/100000: episode: 186, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 3.069, mean reward: 0.307 [0.223, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.389, 10.100], loss: 0.003283, mae: 0.056906, mean_q: -0.253257
 15582/100000: episode: 187, duration: 0.144s, episode steps: 29, steps per second: 201, episode reward: 9.160, mean reward: 0.316 [0.125, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.885, 10.100], loss: 0.003801, mae: 0.064060, mean_q: -0.218152
 15595/100000: episode: 188, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 4.699, mean reward: 0.361 [0.228, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.209, 10.100], loss: 0.003531, mae: 0.064323, mean_q: -0.132861
 15605/100000: episode: 189, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 3.638, mean reward: 0.364 [0.209, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.224, 10.100], loss: 0.003382, mae: 0.060449, mean_q: -0.119960
 15625/100000: episode: 190, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 8.486, mean reward: 0.424 [0.272, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.331, 10.100], loss: 0.003268, mae: 0.056977, mean_q: -0.292456
 15634/100000: episode: 191, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 3.833, mean reward: 0.426 [0.352, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.316, 10.100], loss: 0.003335, mae: 0.059601, mean_q: -0.212913
 15651/100000: episode: 192, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 6.642, mean reward: 0.391 [0.292, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.259, 10.100], loss: 0.003434, mae: 0.059476, mean_q: -0.173705
 15668/100000: episode: 193, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 6.939, mean reward: 0.408 [0.356, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.706, 10.100], loss: 0.003300, mae: 0.058404, mean_q: -0.162721
 15676/100000: episode: 194, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 3.180, mean reward: 0.398 [0.348, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-1.167, 10.100], loss: 0.003897, mae: 0.063702, mean_q: -0.135551
 15705/100000: episode: 195, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 9.584, mean reward: 0.330 [0.258, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.707, 10.100], loss: 0.003571, mae: 0.061873, mean_q: -0.143762
 15725/100000: episode: 196, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 7.523, mean reward: 0.376 [0.277, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.430, 10.100], loss: 0.003625, mae: 0.062387, mean_q: -0.149549
 15738/100000: episode: 197, duration: 0.062s, episode steps: 13, steps per second: 208, episode reward: 4.555, mean reward: 0.350 [0.271, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.222, 10.100], loss: 0.003879, mae: 0.065472, mean_q: -0.232065
 15748/100000: episode: 198, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 3.539, mean reward: 0.354 [0.272, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.303, 10.100], loss: 0.003083, mae: 0.059347, mean_q: -0.126535
 15771/100000: episode: 199, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 6.568, mean reward: 0.286 [0.098, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.062, 10.201], loss: 0.007720, mae: 0.083673, mean_q: -0.122878
 15781/100000: episode: 200, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 3.430, mean reward: 0.343 [0.295, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.372, 10.100], loss: 0.005989, mae: 0.080959, mean_q: -0.108066
 15801/100000: episode: 201, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 8.319, mean reward: 0.416 [0.318, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.931, 10.100], loss: 0.004621, mae: 0.069915, mean_q: -0.109011
 15811/100000: episode: 202, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 3.252, mean reward: 0.325 [0.213, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.343, 10.100], loss: 0.004445, mae: 0.068459, mean_q: -0.088352
 15819/100000: episode: 203, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 3.014, mean reward: 0.377 [0.344, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.303, 10.100], loss: 0.003739, mae: 0.069377, mean_q: -0.154970
 15828/100000: episode: 204, duration: 0.063s, episode steps: 9, steps per second: 142, episode reward: 2.720, mean reward: 0.302 [0.256, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.383, 10.100], loss: 0.004461, mae: 0.075537, mean_q: -0.150467
 15857/100000: episode: 205, duration: 0.144s, episode steps: 29, steps per second: 201, episode reward: 11.142, mean reward: 0.384 [0.302, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.749, 10.100], loss: 0.006084, mae: 0.079808, mean_q: -0.120264
 15874/100000: episode: 206, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 6.693, mean reward: 0.394 [0.335, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.872, 10.100], loss: 0.004392, mae: 0.072795, mean_q: -0.128505
 15884/100000: episode: 207, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 2.232, mean reward: 0.223 [0.157, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.230, 10.100], loss: 0.004167, mae: 0.069618, mean_q: -0.038818
 15901/100000: episode: 208, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 6.209, mean reward: 0.365 [0.283, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.675, 10.100], loss: 0.003180, mae: 0.058657, mean_q: -0.117558
 15909/100000: episode: 209, duration: 0.042s, episode steps: 8, steps per second: 193, episode reward: 2.943, mean reward: 0.368 [0.332, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.490, 10.100], loss: 0.003608, mae: 0.064435, mean_q: -0.040777
 15917/100000: episode: 210, duration: 0.061s, episode steps: 8, steps per second: 131, episode reward: 3.219, mean reward: 0.402 [0.375, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.428, 10.100], loss: 0.003348, mae: 0.057857, mean_q: -0.066904
 15927/100000: episode: 211, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 3.719, mean reward: 0.372 [0.264, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.438, 10.100], loss: 0.003930, mae: 0.064917, mean_q: -0.153751
 15956/100000: episode: 212, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 9.315, mean reward: 0.321 [0.144, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.088, 10.100], loss: 0.004005, mae: 0.066117, mean_q: -0.165529
 15965/100000: episode: 213, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 2.776, mean reward: 0.308 [0.261, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.329, 10.100], loss: 0.004548, mae: 0.072001, mean_q: -0.044674
 15978/100000: episode: 214, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 5.066, mean reward: 0.390 [0.322, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.358, 10.100], loss: 0.003838, mae: 0.065726, mean_q: -0.095884
 15986/100000: episode: 215, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 2.351, mean reward: 0.294 [0.257, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.169, 10.100], loss: 0.003439, mae: 0.061836, mean_q: -0.124766
 16015/100000: episode: 216, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 11.666, mean reward: 0.402 [0.306, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.260, 10.100], loss: 0.003720, mae: 0.063523, mean_q: -0.127075
 16038/100000: episode: 217, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 11.663, mean reward: 0.507 [0.391, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.269, 10.632], loss: 0.003550, mae: 0.062887, mean_q: -0.049261
 16046/100000: episode: 218, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 3.352, mean reward: 0.419 [0.350, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.425, 10.100], loss: 0.004955, mae: 0.074074, mean_q: -0.076038
 16055/100000: episode: 219, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 2.292, mean reward: 0.255 [0.213, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.495, 10.100], loss: 0.003510, mae: 0.064090, mean_q: -0.165459
 16078/100000: episode: 220, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 10.309, mean reward: 0.448 [0.291, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.925, 10.439], loss: 0.004603, mae: 0.067380, mean_q: -0.050444
 16088/100000: episode: 221, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 3.134, mean reward: 0.313 [0.213, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.257, 10.100], loss: 0.006493, mae: 0.085649, mean_q: -0.114863
 16105/100000: episode: 222, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 7.059, mean reward: 0.415 [0.302, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.274, 10.100], loss: 0.004180, mae: 0.074021, mean_q: -0.102861
 16114/100000: episode: 223, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 2.373, mean reward: 0.264 [0.201, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.187, 10.100], loss: 0.003768, mae: 0.064364, mean_q: -0.194737
 16124/100000: episode: 224, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 3.392, mean reward: 0.339 [0.246, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.378, 10.100], loss: 0.005875, mae: 0.068185, mean_q: -0.161849
 16137/100000: episode: 225, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 5.027, mean reward: 0.387 [0.306, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.216, 10.100], loss: 0.006953, mae: 0.081718, mean_q: -0.059665
 16147/100000: episode: 226, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 2.680, mean reward: 0.268 [0.194, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.250, 10.100], loss: 0.004550, mae: 0.074036, mean_q: -0.099554
 16155/100000: episode: 227, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 2.448, mean reward: 0.306 [0.252, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.423, 10.100], loss: 0.004005, mae: 0.072738, mean_q: -0.076932
 16163/100000: episode: 228, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 3.150, mean reward: 0.394 [0.270, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.868, 10.100], loss: 0.003588, mae: 0.063851, mean_q: -0.122237
 16172/100000: episode: 229, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 2.784, mean reward: 0.309 [0.205, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.333, 10.100], loss: 0.003289, mae: 0.061781, mean_q: -0.011964
 16181/100000: episode: 230, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 3.255, mean reward: 0.362 [0.291, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.843, 10.100], loss: 0.003469, mae: 0.063143, mean_q: -0.054501
 16189/100000: episode: 231, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 2.829, mean reward: 0.354 [0.305, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.287, 10.100], loss: 0.003240, mae: 0.061218, mean_q: -0.020834
 16198/100000: episode: 232, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 3.143, mean reward: 0.349 [0.296, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.544, 10.100], loss: 0.003503, mae: 0.063473, mean_q: -0.114746
 16207/100000: episode: 233, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 2.588, mean reward: 0.288 [0.262, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.868, 10.100], loss: 0.004262, mae: 0.066394, mean_q: -0.070299
 16215/100000: episode: 234, duration: 0.040s, episode steps: 8, steps per second: 202, episode reward: 2.689, mean reward: 0.336 [0.211, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.206, 10.100], loss: 0.003521, mae: 0.064017, mean_q: -0.011378
 16228/100000: episode: 235, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 4.247, mean reward: 0.327 [0.254, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.224, 10.100], loss: 0.003340, mae: 0.062177, mean_q: -0.147074
 16248/100000: episode: 236, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 8.234, mean reward: 0.412 [0.272, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.406, 10.100], loss: 0.003887, mae: 0.064468, mean_q: -0.060125
 16257/100000: episode: 237, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 2.332, mean reward: 0.259 [0.217, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.239, 10.100], loss: 0.003249, mae: 0.059384, mean_q: -0.101288
 16274/100000: episode: 238, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 6.950, mean reward: 0.409 [0.335, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.227, 10.100], loss: 0.003560, mae: 0.061380, mean_q: -0.028451
 16291/100000: episode: 239, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 6.378, mean reward: 0.375 [0.286, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.318, 10.100], loss: 0.003491, mae: 0.060417, mean_q: -0.071539
[Info] 200-TH LEVEL FOUND: 0.7826618552207947, Considering 10/90 traces
 16320/100000: episode: 240, duration: 3.999s, episode steps: 29, steps per second: 7, episode reward: 12.059, mean reward: 0.416 [0.306, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-1.762, 10.100], loss: 0.004020, mae: 0.067623, mean_q: 0.004671
 16331/100000: episode: 241, duration: 0.062s, episode steps: 11, steps per second: 179, episode reward: 5.327, mean reward: 0.484 [0.404, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.319, 10.100], loss: 0.003412, mae: 0.062474, mean_q: -0.132860
 16344/100000: episode: 242, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 6.041, mean reward: 0.465 [0.326, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.575, 10.100], loss: 0.003790, mae: 0.064726, mean_q: 0.071981
 16357/100000: episode: 243, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 6.835, mean reward: 0.526 [0.449, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.456, 10.100], loss: 0.004040, mae: 0.064569, mean_q: -0.009278
 16370/100000: episode: 244, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 6.234, mean reward: 0.480 [0.440, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.442, 10.100], loss: 0.002830, mae: 0.058511, mean_q: -0.063336
 16383/100000: episode: 245, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 5.017, mean reward: 0.386 [0.279, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.209, 10.100], loss: 0.004221, mae: 0.067665, mean_q: -0.007308
 16405/100000: episode: 246, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 9.662, mean reward: 0.439 [0.346, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.567, 10.100], loss: 0.007014, mae: 0.090018, mean_q: -0.057803
 16425/100000: episode: 247, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 8.789, mean reward: 0.439 [0.347, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.729, 10.100], loss: 0.003383, mae: 0.062203, mean_q: -0.018596
 16436/100000: episode: 248, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 4.832, mean reward: 0.439 [0.388, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.448, 10.100], loss: 0.003039, mae: 0.056891, mean_q: -0.103157
 16449/100000: episode: 249, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 6.485, mean reward: 0.499 [0.431, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.764, 10.100], loss: 0.003733, mae: 0.061972, mean_q: -0.058882
 16462/100000: episode: 250, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 5.186, mean reward: 0.399 [0.308, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.265, 10.100], loss: 0.003362, mae: 0.060145, mean_q: -0.102952
 16482/100000: episode: 251, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 9.696, mean reward: 0.485 [0.362, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.537, 10.100], loss: 0.003637, mae: 0.063501, mean_q: 0.026285
 16502/100000: episode: 252, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 7.830, mean reward: 0.392 [0.331, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.323, 10.100], loss: 0.003096, mae: 0.058860, mean_q: 0.009629
 16516/100000: episode: 253, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 6.970, mean reward: 0.498 [0.360, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.470, 10.100], loss: 0.003043, mae: 0.057545, mean_q: -0.028073
 16529/100000: episode: 254, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 5.949, mean reward: 0.458 [0.392, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.432, 10.100], loss: 0.003105, mae: 0.055643, mean_q: -0.098436
 16538/100000: episode: 255, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 4.470, mean reward: 0.497 [0.420, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.393, 10.100], loss: 0.002973, mae: 0.060003, mean_q: 0.067856
 16547/100000: episode: 256, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 4.155, mean reward: 0.462 [0.440, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.415, 10.100], loss: 0.003676, mae: 0.062240, mean_q: 0.009473
 16560/100000: episode: 257, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 5.445, mean reward: 0.419 [0.337, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.520, 10.100], loss: 0.003524, mae: 0.064792, mean_q: 0.077403
 16571/100000: episode: 258, duration: 0.053s, episode steps: 11, steps per second: 206, episode reward: 4.667, mean reward: 0.424 [0.366, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.233, 10.100], loss: 0.003407, mae: 0.062815, mean_q: -0.020052
 16584/100000: episode: 259, duration: 0.062s, episode steps: 13, steps per second: 210, episode reward: 5.218, mean reward: 0.401 [0.297, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.288, 10.100], loss: 0.003038, mae: 0.059426, mean_q: 0.059672
 16597/100000: episode: 260, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 6.294, mean reward: 0.484 [0.413, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.399, 10.100], loss: 0.003227, mae: 0.058841, mean_q: 0.011922
 16608/100000: episode: 261, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 4.515, mean reward: 0.410 [0.331, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.353, 10.100], loss: 0.002696, mae: 0.056258, mean_q: 0.009387
 16630/100000: episode: 262, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 7.670, mean reward: 0.349 [0.214, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.225, 10.100], loss: 0.003275, mae: 0.060100, mean_q: 0.010302
 16641/100000: episode: 263, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 4.768, mean reward: 0.433 [0.379, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.362, 10.100], loss: 0.003966, mae: 0.064516, mean_q: 0.027902
 16654/100000: episode: 264, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 6.509, mean reward: 0.501 [0.377, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.654, 10.100], loss: 0.004702, mae: 0.070831, mean_q: 0.029110
 16667/100000: episode: 265, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 5.764, mean reward: 0.443 [0.409, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.328, 10.100], loss: 0.003911, mae: 0.066578, mean_q: -0.013488
 16678/100000: episode: 266, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 4.928, mean reward: 0.448 [0.389, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.447, 10.100], loss: 0.002773, mae: 0.054508, mean_q: -0.042157
 16687/100000: episode: 267, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 4.159, mean reward: 0.462 [0.375, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.400, 10.100], loss: 0.003403, mae: 0.061265, mean_q: 0.067512
 16700/100000: episode: 268, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 5.081, mean reward: 0.391 [0.340, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.314, 10.100], loss: 0.003210, mae: 0.060030, mean_q: -0.029399
 16713/100000: episode: 269, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 6.307, mean reward: 0.485 [0.396, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.385, 10.100], loss: 0.003459, mae: 0.061431, mean_q: 0.027044
 16726/100000: episode: 270, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 4.546, mean reward: 0.350 [0.289, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.463, 10.100], loss: 0.003316, mae: 0.060643, mean_q: -0.012810
 16748/100000: episode: 271, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 8.622, mean reward: 0.392 [0.305, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.353, 10.100], loss: 0.003244, mae: 0.061582, mean_q: 0.017847
 16759/100000: episode: 272, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 3.888, mean reward: 0.353 [0.279, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.379, 10.100], loss: 0.003567, mae: 0.062196, mean_q: -0.002195
 16770/100000: episode: 273, duration: 0.062s, episode steps: 11, steps per second: 179, episode reward: 4.333, mean reward: 0.394 [0.340, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.478, 10.100], loss: 0.004265, mae: 0.071058, mean_q: 0.061076
 16792/100000: episode: 274, duration: 0.109s, episode steps: 22, steps per second: 203, episode reward: 6.663, mean reward: 0.303 [0.065, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.332, 10.100], loss: 0.004600, mae: 0.072429, mean_q: 0.104769
 16801/100000: episode: 275, duration: 0.052s, episode steps: 9, steps per second: 175, episode reward: 2.818, mean reward: 0.313 [0.258, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.363, 10.100], loss: 0.004013, mae: 0.065541, mean_q: 0.061869
 16812/100000: episode: 276, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 3.826, mean reward: 0.348 [0.257, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.490, 10.100], loss: 0.003136, mae: 0.060244, mean_q: 0.086654
 16821/100000: episode: 277, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 3.602, mean reward: 0.400 [0.370, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.437, 10.100], loss: 0.003353, mae: 0.061084, mean_q: 0.144523
 16835/100000: episode: 278, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 4.958, mean reward: 0.354 [0.263, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.441, 10.100], loss: 0.003800, mae: 0.065612, mean_q: 0.110515
 16848/100000: episode: 279, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 6.223, mean reward: 0.479 [0.396, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.259, 10.100], loss: 0.003513, mae: 0.060710, mean_q: -0.015797
 16868/100000: episode: 280, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 9.825, mean reward: 0.491 [0.356, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.807, 10.100], loss: 0.004005, mae: 0.068295, mean_q: 0.162051
 16881/100000: episode: 281, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 5.983, mean reward: 0.460 [0.410, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.253, 10.100], loss: 0.003475, mae: 0.063346, mean_q: 0.038092
 16894/100000: episode: 282, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 5.184, mean reward: 0.399 [0.332, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.192, 10.100], loss: 0.004191, mae: 0.068380, mean_q: 0.060606
 16907/100000: episode: 283, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 6.454, mean reward: 0.496 [0.434, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.534, 10.100], loss: 0.005322, mae: 0.075958, mean_q: 0.029372
 16918/100000: episode: 284, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 3.967, mean reward: 0.361 [0.267, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.400, 10.100], loss: 0.004075, mae: 0.065263, mean_q: 0.045478
 16940/100000: episode: 285, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 9.257, mean reward: 0.421 [0.339, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.660, 10.100], loss: 0.003950, mae: 0.065948, mean_q: 0.097003
 16951/100000: episode: 286, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 4.594, mean reward: 0.418 [0.352, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.384, 10.100], loss: 0.003492, mae: 0.064690, mean_q: 0.140276
 16973/100000: episode: 287, duration: 0.117s, episode steps: 22, steps per second: 187, episode reward: 7.800, mean reward: 0.355 [0.298, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.288, 10.100], loss: 0.003515, mae: 0.061807, mean_q: 0.099035
 16987/100000: episode: 288, duration: 0.067s, episode steps: 14, steps per second: 210, episode reward: 4.747, mean reward: 0.339 [0.278, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.699, 10.100], loss: 0.003096, mae: 0.057428, mean_q: 0.108530
 17000/100000: episode: 289, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 5.016, mean reward: 0.386 [0.297, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-1.187, 10.100], loss: 0.003437, mae: 0.062638, mean_q: 0.021070
 17014/100000: episode: 290, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 6.212, mean reward: 0.444 [0.403, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.345, 10.100], loss: 0.003872, mae: 0.064522, mean_q: 0.165066
 17028/100000: episode: 291, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 6.565, mean reward: 0.469 [0.382, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.424, 10.100], loss: 0.004226, mae: 0.069821, mean_q: 0.123033
 17042/100000: episode: 292, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 6.323, mean reward: 0.452 [0.390, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.349, 10.100], loss: 0.003484, mae: 0.063406, mean_q: 0.131066
 17055/100000: episode: 293, duration: 0.063s, episode steps: 13, steps per second: 208, episode reward: 6.002, mean reward: 0.462 [0.373, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.205, 10.100], loss: 0.003877, mae: 0.067069, mean_q: 0.161607
 17068/100000: episode: 294, duration: 0.063s, episode steps: 13, steps per second: 206, episode reward: 4.989, mean reward: 0.384 [0.214, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.392, 10.100], loss: 0.003511, mae: 0.062116, mean_q: 0.082335
 17081/100000: episode: 295, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 5.732, mean reward: 0.441 [0.377, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.453, 10.100], loss: 0.003249, mae: 0.061915, mean_q: 0.115075
 17103/100000: episode: 296, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 12.365, mean reward: 0.562 [0.454, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.622, 10.100], loss: 0.003868, mae: 0.066058, mean_q: 0.134317
 17116/100000: episode: 297, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 5.813, mean reward: 0.447 [0.363, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.301, 10.100], loss: 0.003737, mae: 0.064628, mean_q: 0.104474
 17127/100000: episode: 298, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 4.381, mean reward: 0.398 [0.377, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.323, 10.100], loss: 0.004352, mae: 0.068300, mean_q: 0.140143
 17140/100000: episode: 299, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 6.808, mean reward: 0.524 [0.459, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.055, 10.100], loss: 0.005124, mae: 0.075094, mean_q: 0.148105
 17162/100000: episode: 300, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 9.592, mean reward: 0.436 [0.351, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.460, 10.100], loss: 0.006792, mae: 0.089151, mean_q: 0.121201
 17182/100000: episode: 301, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 5.827, mean reward: 0.291 [0.204, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.997, 10.100], loss: 0.003703, mae: 0.067763, mean_q: 0.187605
 17204/100000: episode: 302, duration: 0.105s, episode steps: 22, steps per second: 210, episode reward: 8.184, mean reward: 0.372 [0.288, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.256, 10.100], loss: 0.002966, mae: 0.059353, mean_q: 0.153283
 17217/100000: episode: 303, duration: 0.079s, episode steps: 13, steps per second: 166, episode reward: 6.147, mean reward: 0.473 [0.386, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.591, 10.100], loss: 0.002907, mae: 0.058515, mean_q: 0.133151
 17228/100000: episode: 304, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 4.555, mean reward: 0.414 [0.356, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.421, 10.100], loss: 0.003324, mae: 0.062924, mean_q: 0.164499
 17241/100000: episode: 305, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 5.317, mean reward: 0.409 [0.315, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.381, 10.100], loss: 0.003198, mae: 0.060950, mean_q: 0.152482
 17254/100000: episode: 306, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 6.617, mean reward: 0.509 [0.421, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.593, 10.100], loss: 0.002952, mae: 0.057875, mean_q: 0.219538
 17276/100000: episode: 307, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 11.653, mean reward: 0.530 [0.425, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.388, 10.100], loss: 0.003250, mae: 0.061938, mean_q: 0.251860
 17287/100000: episode: 308, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 5.286, mean reward: 0.481 [0.395, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.985, 10.100], loss: 0.003344, mae: 0.061051, mean_q: 0.188645
 17309/100000: episode: 309, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 8.755, mean reward: 0.398 [0.331, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.123, 10.100], loss: 0.003385, mae: 0.062943, mean_q: 0.248127
 17320/100000: episode: 310, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 3.955, mean reward: 0.360 [0.290, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.289, 10.100], loss: 0.003277, mae: 0.061116, mean_q: 0.082967
 17334/100000: episode: 311, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 6.352, mean reward: 0.454 [0.387, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.467, 10.100], loss: 0.003237, mae: 0.061621, mean_q: 0.240892
 17345/100000: episode: 312, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 4.227, mean reward: 0.384 [0.311, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.449, 10.100], loss: 0.003226, mae: 0.060952, mean_q: 0.209998
 17365/100000: episode: 313, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 9.007, mean reward: 0.450 [0.373, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.442, 10.100], loss: 0.004757, mae: 0.075256, mean_q: 0.242324
 17385/100000: episode: 314, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 8.321, mean reward: 0.416 [0.351, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.795, 10.100], loss: 0.003542, mae: 0.065156, mean_q: 0.263981
 17405/100000: episode: 315, duration: 0.121s, episode steps: 20, steps per second: 166, episode reward: 7.234, mean reward: 0.362 [0.255, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.401, 10.100], loss: 0.003190, mae: 0.062011, mean_q: 0.212386
 17416/100000: episode: 316, duration: 0.053s, episode steps: 11, steps per second: 208, episode reward: 4.921, mean reward: 0.447 [0.373, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.499, 10.100], loss: 0.004289, mae: 0.069740, mean_q: 0.250489
 17429/100000: episode: 317, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 5.209, mean reward: 0.401 [0.218, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.240, 10.100], loss: 0.007902, mae: 0.084464, mean_q: 0.281067
 17442/100000: episode: 318, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 5.448, mean reward: 0.419 [0.356, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.528, 10.100], loss: 0.003333, mae: 0.064421, mean_q: 0.243886
 17455/100000: episode: 319, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 6.190, mean reward: 0.476 [0.326, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.424, 10.100], loss: 0.003930, mae: 0.066955, mean_q: 0.203011
 17469/100000: episode: 320, duration: 0.086s, episode steps: 14, steps per second: 164, episode reward: 6.781, mean reward: 0.484 [0.359, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.362, 10.100], loss: 0.003960, mae: 0.066463, mean_q: 0.191846
 17482/100000: episode: 321, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 6.908, mean reward: 0.531 [0.454, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.485, 10.100], loss: 0.003828, mae: 0.068817, mean_q: 0.283467
 17493/100000: episode: 322, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 4.955, mean reward: 0.450 [0.399, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.232, 10.100], loss: 0.004391, mae: 0.073799, mean_q: 0.219234
 17506/100000: episode: 323, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 5.795, mean reward: 0.446 [0.401, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.271, 10.100], loss: 0.003579, mae: 0.065695, mean_q: 0.257556
 17519/100000: episode: 324, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 5.047, mean reward: 0.388 [0.297, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.405, 10.100], loss: 0.002410, mae: 0.053428, mean_q: 0.167494
 17528/100000: episode: 325, duration: 0.044s, episode steps: 9, steps per second: 204, episode reward: 4.108, mean reward: 0.456 [0.400, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.412, 10.100], loss: 0.002878, mae: 0.056900, mean_q: 0.238646
 17542/100000: episode: 326, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 6.235, mean reward: 0.445 [0.356, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.491, 10.100], loss: 0.003098, mae: 0.060259, mean_q: 0.338098
 17556/100000: episode: 327, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 6.457, mean reward: 0.461 [0.373, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.450, 10.100], loss: 0.003517, mae: 0.064474, mean_q: 0.298026
 17576/100000: episode: 328, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 7.573, mean reward: 0.379 [0.262, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.241, 10.100], loss: 0.003441, mae: 0.065299, mean_q: 0.176812
 17589/100000: episode: 329, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 5.332, mean reward: 0.410 [0.381, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.222, 10.100], loss: 0.003583, mae: 0.065975, mean_q: 0.227639
[Info] 300-TH LEVEL FOUND: 1.0414961576461792, Considering 10/90 traces
 17600/100000: episode: 330, duration: 3.856s, episode steps: 11, steps per second: 3, episode reward: 4.597, mean reward: 0.418 [0.294, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.220, 10.100], loss: 0.003141, mae: 0.061207, mean_q: 0.216227
 17607/100000: episode: 331, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 3.424, mean reward: 0.489 [0.443, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.408, 10.100], loss: 0.004304, mae: 0.070115, mean_q: 0.279267
 17621/100000: episode: 332, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 7.144, mean reward: 0.510 [0.440, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.664, 10.100], loss: 0.003320, mae: 0.063177, mean_q: 0.265223
 17633/100000: episode: 333, duration: 0.058s, episode steps: 12, steps per second: 207, episode reward: 6.476, mean reward: 0.540 [0.465, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.387, 10.100], loss: 0.002561, mae: 0.055213, mean_q: 0.203334
 17642/100000: episode: 334, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 5.251, mean reward: 0.583 [0.521, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.361, 10.100], loss: 0.002754, mae: 0.053541, mean_q: 0.240256
 17648/100000: episode: 335, duration: 0.045s, episode steps: 6, steps per second: 134, episode reward: 3.614, mean reward: 0.602 [0.523, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.416, 10.100], loss: 0.003024, mae: 0.057871, mean_q: 0.379622
 17657/100000: episode: 336, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 4.151, mean reward: 0.461 [0.384, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.492, 10.100], loss: 0.002802, mae: 0.056774, mean_q: 0.289247
 17663/100000: episode: 337, duration: 0.030s, episode steps: 6, steps per second: 197, episode reward: 3.232, mean reward: 0.539 [0.492, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.335, 10.100], loss: 0.002890, mae: 0.058102, mean_q: 0.271373
 17669/100000: episode: 338, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 3.135, mean reward: 0.522 [0.494, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.543, 10.100], loss: 0.002716, mae: 0.057325, mean_q: 0.246249
 17678/100000: episode: 339, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 4.703, mean reward: 0.523 [0.473, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.407, 10.100], loss: 0.003158, mae: 0.059014, mean_q: 0.298558
 17684/100000: episode: 340, duration: 0.031s, episode steps: 6, steps per second: 195, episode reward: 2.970, mean reward: 0.495 [0.438, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.468, 10.100], loss: 0.002920, mae: 0.058949, mean_q: 0.331521
 17691/100000: episode: 341, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 3.743, mean reward: 0.535 [0.484, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.641, 10.100], loss: 0.003244, mae: 0.061419, mean_q: 0.247544
 17700/100000: episode: 342, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 4.766, mean reward: 0.530 [0.498, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.680, 10.100], loss: 0.002928, mae: 0.055767, mean_q: 0.282957
 17706/100000: episode: 343, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 3.284, mean reward: 0.547 [0.528, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.370, 10.100], loss: 0.002770, mae: 0.057619, mean_q: 0.172478
 17715/100000: episode: 344, duration: 0.044s, episode steps: 9, steps per second: 205, episode reward: 4.915, mean reward: 0.546 [0.483, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.194, 10.100], loss: 0.003382, mae: 0.062741, mean_q: 0.279815
[Info] FALSIFICATION!
 17722/100000: episode: 345, duration: 0.035s, episode steps: 7, steps per second: 197, episode reward: 12.892, mean reward: 1.842 [0.442, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.321, 10.082], loss: 0.004112, mae: 0.070966, mean_q: 0.304616
 17822/100000: episode: 346, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.962, mean reward: -0.180 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.643, 10.332], loss: 0.019750, mae: 0.087357, mean_q: 0.302814
 17922/100000: episode: 347, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -11.213, mean reward: -0.112 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.472, 10.098], loss: 0.004055, mae: 0.065928, mean_q: 0.308803
 18022/100000: episode: 348, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.678, mean reward: -0.177 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.408, 10.119], loss: 0.003885, mae: 0.064344, mean_q: 0.288404
 18122/100000: episode: 349, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -16.814, mean reward: -0.168 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.298, 10.098], loss: 0.019404, mae: 0.083978, mean_q: 0.299283
 18222/100000: episode: 350, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -19.754, mean reward: -0.198 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.488, 10.098], loss: 0.005613, mae: 0.075365, mean_q: 0.287968
 18322/100000: episode: 351, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.230, mean reward: -0.182 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.243, 10.225], loss: 0.033996, mae: 0.096751, mean_q: 0.299664
 18422/100000: episode: 352, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -2.305, mean reward: -0.023 [-1.000, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.458, 10.438], loss: 0.048941, mae: 0.110472, mean_q: 0.312148
 18522/100000: episode: 353, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -14.851, mean reward: -0.149 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.997, 10.098], loss: 0.018444, mae: 0.077028, mean_q: 0.317813
 18622/100000: episode: 354, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.989, mean reward: -0.190 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.340, 10.157], loss: 0.018291, mae: 0.076738, mean_q: 0.322869
 18722/100000: episode: 355, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -13.191, mean reward: -0.132 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.755, 10.221], loss: 0.004533, mae: 0.068430, mean_q: 0.328586
 18822/100000: episode: 356, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.579, mean reward: -0.166 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.732, 10.363], loss: 0.019102, mae: 0.083174, mean_q: 0.316620
 18922/100000: episode: 357, duration: 0.480s, episode steps: 100, steps per second: 209, episode reward: -17.855, mean reward: -0.179 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.532, 10.191], loss: 0.018568, mae: 0.079589, mean_q: 0.322633
 19022/100000: episode: 358, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.932, mean reward: -0.189 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.848, 10.098], loss: 0.017656, mae: 0.072643, mean_q: 0.290452
 19122/100000: episode: 359, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.757, mean reward: -0.168 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.272, 10.246], loss: 0.003731, mae: 0.063728, mean_q: 0.288087
 19222/100000: episode: 360, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -14.022, mean reward: -0.140 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.447, 10.098], loss: 0.033222, mae: 0.093101, mean_q: 0.332172
 19322/100000: episode: 361, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -11.964, mean reward: -0.120 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.853, 10.354], loss: 0.018819, mae: 0.080333, mean_q: 0.295372
 19422/100000: episode: 362, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -15.958, mean reward: -0.160 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.774, 10.098], loss: 0.032671, mae: 0.090875, mean_q: 0.302450
 19522/100000: episode: 363, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -14.085, mean reward: -0.141 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.329, 10.287], loss: 0.021282, mae: 0.095242, mean_q: 0.335985
 19622/100000: episode: 364, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -17.598, mean reward: -0.176 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.457, 10.098], loss: 0.004745, mae: 0.070480, mean_q: 0.313024
 19722/100000: episode: 365, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.591, mean reward: -0.186 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.992, 10.139], loss: 0.017300, mae: 0.072013, mean_q: 0.311029
 19822/100000: episode: 366, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.183, mean reward: -0.182 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.588, 10.155], loss: 0.003916, mae: 0.065230, mean_q: 0.293542
 19922/100000: episode: 367, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -16.520, mean reward: -0.165 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.631, 10.098], loss: 0.018827, mae: 0.082346, mean_q: 0.274379
 20022/100000: episode: 368, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -11.592, mean reward: -0.116 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.596, 10.140], loss: 0.003888, mae: 0.065709, mean_q: 0.245739
 20122/100000: episode: 369, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.157, mean reward: -0.182 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.788, 10.206], loss: 0.018881, mae: 0.081352, mean_q: 0.250434
 20222/100000: episode: 370, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -19.110, mean reward: -0.191 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.782, 10.137], loss: 0.033164, mae: 0.094143, mean_q: 0.208978
 20322/100000: episode: 371, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.234, mean reward: -0.192 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.953, 10.098], loss: 0.004467, mae: 0.069460, mean_q: 0.193869
 20422/100000: episode: 372, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.560, mean reward: -0.176 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.466, 10.260], loss: 0.018502, mae: 0.082178, mean_q: 0.182364
 20522/100000: episode: 373, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.512, mean reward: -0.185 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.507, 10.144], loss: 0.018120, mae: 0.077826, mean_q: 0.150812
 20622/100000: episode: 374, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.988, mean reward: -0.170 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.576, 10.184], loss: 0.030604, mae: 0.077828, mean_q: 0.161260
 20722/100000: episode: 375, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.121, mean reward: -0.181 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.613, 10.125], loss: 0.003335, mae: 0.059985, mean_q: 0.126754
 20822/100000: episode: 376, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.630, mean reward: -0.176 [-1.000, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.157, 10.098], loss: 0.003830, mae: 0.064642, mean_q: 0.113014
 20922/100000: episode: 377, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.158, mean reward: -0.192 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.560, 10.098], loss: 0.003159, mae: 0.059226, mean_q: 0.063689
 21022/100000: episode: 378, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.375, mean reward: -0.164 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.578, 10.295], loss: 0.003148, mae: 0.058823, mean_q: 0.065544
 21122/100000: episode: 379, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.616, mean reward: -0.186 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.905, 10.098], loss: 0.016909, mae: 0.066545, mean_q: 0.045662
 21222/100000: episode: 380, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.812, mean reward: -0.168 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.146, 10.274], loss: 0.004371, mae: 0.070096, mean_q: -0.024164
 21322/100000: episode: 381, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -21.644, mean reward: -0.216 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.643, 10.098], loss: 0.003555, mae: 0.058633, mean_q: -0.022337
 21422/100000: episode: 382, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.815, mean reward: -0.178 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.541, 10.098], loss: 0.003294, mae: 0.057385, mean_q: -0.037771
 21522/100000: episode: 383, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -19.985, mean reward: -0.200 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.467, 10.098], loss: 0.018517, mae: 0.077557, mean_q: -0.059182
 21622/100000: episode: 384, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -20.626, mean reward: -0.206 [-1.000, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.511, 10.188], loss: 0.004078, mae: 0.063099, mean_q: -0.091689
 21722/100000: episode: 385, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.916, mean reward: -0.159 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.581, 10.098], loss: 0.016655, mae: 0.064458, mean_q: -0.110854
 21822/100000: episode: 386, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -15.357, mean reward: -0.154 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.220, 10.335], loss: 0.003709, mae: 0.061628, mean_q: -0.136033
 21922/100000: episode: 387, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -12.732, mean reward: -0.127 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.922, 10.098], loss: 0.020453, mae: 0.088138, mean_q: -0.142205
 22022/100000: episode: 388, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.656, mean reward: -0.177 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.455, 10.273], loss: 0.004170, mae: 0.064476, mean_q: -0.148997
 22122/100000: episode: 389, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -14.014, mean reward: -0.140 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.399, 10.098], loss: 0.018457, mae: 0.078141, mean_q: -0.211060
 22222/100000: episode: 390, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.463, mean reward: -0.165 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.109, 10.142], loss: 0.017006, mae: 0.068911, mean_q: -0.187421
 22322/100000: episode: 391, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.543, mean reward: -0.175 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.669, 10.098], loss: 0.016985, mae: 0.067847, mean_q: -0.242321
 22422/100000: episode: 392, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -13.688, mean reward: -0.137 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.566, 10.098], loss: 0.002997, mae: 0.055371, mean_q: -0.243470
 22522/100000: episode: 393, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -18.198, mean reward: -0.182 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.638, 10.098], loss: 0.017205, mae: 0.069836, mean_q: -0.233963
 22622/100000: episode: 394, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: -19.991, mean reward: -0.200 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.403, 10.164], loss: 0.016615, mae: 0.066821, mean_q: -0.321353
 22722/100000: episode: 395, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -16.425, mean reward: -0.164 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.045, 10.098], loss: 0.003478, mae: 0.060153, mean_q: -0.328452
 22822/100000: episode: 396, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -17.714, mean reward: -0.177 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.393, 10.098], loss: 0.002804, mae: 0.053304, mean_q: -0.312767
 22922/100000: episode: 397, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -17.938, mean reward: -0.179 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.324, 10.098], loss: 0.002677, mae: 0.052888, mean_q: -0.304379
 23022/100000: episode: 398, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -19.580, mean reward: -0.196 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.766, 10.256], loss: 0.002706, mae: 0.052711, mean_q: -0.319009
 23122/100000: episode: 399, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.101, mean reward: -0.151 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.221, 10.102], loss: 0.002649, mae: 0.052122, mean_q: -0.295413
 23222/100000: episode: 400, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -13.642, mean reward: -0.136 [-1.000, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.988, 10.182], loss: 0.002578, mae: 0.051820, mean_q: -0.307454
 23322/100000: episode: 401, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -6.612, mean reward: -0.066 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.411, 10.505], loss: 0.002924, mae: 0.054641, mean_q: -0.290445
 23422/100000: episode: 402, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -13.231, mean reward: -0.132 [-1.000, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.705, 10.274], loss: 0.003446, mae: 0.058975, mean_q: -0.313560
 23522/100000: episode: 403, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -15.285, mean reward: -0.153 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.589, 10.256], loss: 0.003020, mae: 0.056145, mean_q: -0.330451
 23622/100000: episode: 404, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.712, mean reward: -0.167 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.364, 10.112], loss: 0.003092, mae: 0.056300, mean_q: -0.300862
 23722/100000: episode: 405, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.885, mean reward: -0.179 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.730, 10.318], loss: 0.003488, mae: 0.061063, mean_q: -0.311852
 23822/100000: episode: 406, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -12.068, mean reward: -0.121 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.472, 10.390], loss: 0.002787, mae: 0.053059, mean_q: -0.316109
 23922/100000: episode: 407, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.698, mean reward: -0.177 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.974, 10.163], loss: 0.002936, mae: 0.054846, mean_q: -0.302825
 24022/100000: episode: 408, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -16.943, mean reward: -0.169 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.624, 10.098], loss: 0.002941, mae: 0.054939, mean_q: -0.318002
 24122/100000: episode: 409, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.964, mean reward: -0.160 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.955, 10.098], loss: 0.003122, mae: 0.058051, mean_q: -0.319524
 24222/100000: episode: 410, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.447, mean reward: -0.184 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.414, 10.098], loss: 0.002848, mae: 0.054361, mean_q: -0.297818
 24322/100000: episode: 411, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.637, mean reward: -0.176 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.539, 10.187], loss: 0.002819, mae: 0.054091, mean_q: -0.296907
 24422/100000: episode: 412, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -11.098, mean reward: -0.111 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.637, 10.329], loss: 0.002832, mae: 0.054020, mean_q: -0.307661
 24522/100000: episode: 413, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -17.006, mean reward: -0.170 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.216, 10.313], loss: 0.002985, mae: 0.055373, mean_q: -0.285030
 24622/100000: episode: 414, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -12.520, mean reward: -0.125 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.092, 10.098], loss: 0.003091, mae: 0.056479, mean_q: -0.291634
 24722/100000: episode: 415, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -13.513, mean reward: -0.135 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.695, 10.098], loss: 0.003223, mae: 0.059612, mean_q: -0.285222
 24822/100000: episode: 416, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -14.367, mean reward: -0.144 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.860, 10.125], loss: 0.002857, mae: 0.054053, mean_q: -0.314019
 24922/100000: episode: 417, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.467, mean reward: -0.185 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.021, 10.098], loss: 0.002972, mae: 0.054401, mean_q: -0.313563
 25022/100000: episode: 418, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -18.092, mean reward: -0.181 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.492, 10.098], loss: 0.002937, mae: 0.054059, mean_q: -0.303480
 25122/100000: episode: 419, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -17.746, mean reward: -0.177 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.574, 10.246], loss: 0.002925, mae: 0.055041, mean_q: -0.325170
 25222/100000: episode: 420, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.837, mean reward: -0.178 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.080, 10.238], loss: 0.002999, mae: 0.055690, mean_q: -0.298120
 25322/100000: episode: 421, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -16.986, mean reward: -0.170 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.481, 10.107], loss: 0.003076, mae: 0.056865, mean_q: -0.279050
 25422/100000: episode: 422, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -10.236, mean reward: -0.102 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.888, 10.327], loss: 0.002675, mae: 0.052013, mean_q: -0.334707
 25522/100000: episode: 423, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.036, mean reward: -0.180 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.777, 10.236], loss: 0.003072, mae: 0.057978, mean_q: -0.294251
 25622/100000: episode: 424, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.659, mean reward: -0.167 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.738, 10.214], loss: 0.003019, mae: 0.055667, mean_q: -0.273838
 25722/100000: episode: 425, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -19.219, mean reward: -0.192 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.451, 10.144], loss: 0.002897, mae: 0.054091, mean_q: -0.340667
 25822/100000: episode: 426, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.441, mean reward: -0.164 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.846, 10.098], loss: 0.003016, mae: 0.055900, mean_q: -0.304623
 25922/100000: episode: 427, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -10.716, mean reward: -0.107 [-1.000, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.969, 10.098], loss: 0.003667, mae: 0.062192, mean_q: -0.303446
 26022/100000: episode: 428, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.042, mean reward: -0.170 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.128, 10.098], loss: 0.002943, mae: 0.055682, mean_q: -0.298416
 26122/100000: episode: 429, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -12.351, mean reward: -0.124 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.538, 10.351], loss: 0.003076, mae: 0.057956, mean_q: -0.303395
 26222/100000: episode: 430, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -15.582, mean reward: -0.156 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.197, 10.224], loss: 0.002932, mae: 0.055210, mean_q: -0.314223
 26322/100000: episode: 431, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.993, mean reward: -0.170 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.976, 10.150], loss: 0.003082, mae: 0.056690, mean_q: -0.296568
 26422/100000: episode: 432, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.666, mean reward: -0.187 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.787, 10.113], loss: 0.002942, mae: 0.054528, mean_q: -0.314131
 26522/100000: episode: 433, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -15.155, mean reward: -0.152 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.164, 10.098], loss: 0.002957, mae: 0.055476, mean_q: -0.320156
 26622/100000: episode: 434, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.071, mean reward: -0.171 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.286, 10.147], loss: 0.002909, mae: 0.054875, mean_q: -0.290009
 26722/100000: episode: 435, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.713, mean reward: -0.177 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.805, 10.123], loss: 0.003606, mae: 0.061556, mean_q: -0.280228
 26822/100000: episode: 436, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -11.761, mean reward: -0.118 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.527, 10.098], loss: 0.002983, mae: 0.055220, mean_q: -0.272501
 26922/100000: episode: 437, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -19.714, mean reward: -0.197 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.273, 10.109], loss: 0.003113, mae: 0.055698, mean_q: -0.278883
 27022/100000: episode: 438, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -18.965, mean reward: -0.190 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.563, 10.098], loss: 0.003011, mae: 0.054820, mean_q: -0.327836
 27122/100000: episode: 439, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -16.680, mean reward: -0.167 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.908, 10.367], loss: 0.002891, mae: 0.054411, mean_q: -0.329296
 27222/100000: episode: 440, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -18.252, mean reward: -0.183 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.909, 10.212], loss: 0.003310, mae: 0.059367, mean_q: -0.288914
 27322/100000: episode: 441, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -16.626, mean reward: -0.166 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.841, 10.098], loss: 0.003032, mae: 0.056829, mean_q: -0.312387
 27422/100000: episode: 442, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -15.198, mean reward: -0.152 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.521, 10.105], loss: 0.003092, mae: 0.057076, mean_q: -0.330491
 27522/100000: episode: 443, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -15.015, mean reward: -0.150 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.062, 10.223], loss: 0.002878, mae: 0.053942, mean_q: -0.304659
 27622/100000: episode: 444, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -18.893, mean reward: -0.189 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.179, 10.122], loss: 0.002783, mae: 0.053274, mean_q: -0.280841
[Info] 100-TH LEVEL FOUND: 0.5804765820503235, Considering 10/90 traces
 27722/100000: episode: 445, duration: 4.294s, episode steps: 100, steps per second: 23, episode reward: -14.335, mean reward: -0.143 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.019, 10.366], loss: 0.004016, mae: 0.060763, mean_q: -0.310831
 27754/100000: episode: 446, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 9.241, mean reward: 0.289 [0.023, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-1.024, 10.138], loss: 0.004524, mae: 0.070401, mean_q: -0.232853
 27774/100000: episode: 447, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 6.375, mean reward: 0.319 [0.225, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.121, 10.100], loss: 0.003221, mae: 0.063846, mean_q: -0.273244
 27839/100000: episode: 448, duration: 0.348s, episode steps: 65, steps per second: 187, episode reward: 7.768, mean reward: 0.120 [0.021, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.768 [-1.167, 10.144], loss: 0.002927, mae: 0.055611, mean_q: -0.267076
 27872/100000: episode: 449, duration: 0.192s, episode steps: 33, steps per second: 172, episode reward: 13.074, mean reward: 0.396 [0.232, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.252, 10.100], loss: 0.003279, mae: 0.060242, mean_q: -0.276410
 27892/100000: episode: 450, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 7.063, mean reward: 0.353 [0.187, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.172, 10.100], loss: 0.003370, mae: 0.060621, mean_q: -0.281114
 27909/100000: episode: 451, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 5.626, mean reward: 0.331 [0.215, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.557, 10.100], loss: 0.003261, mae: 0.058285, mean_q: -0.260195
 27929/100000: episode: 452, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 6.636, mean reward: 0.332 [0.261, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.425, 10.100], loss: 0.003063, mae: 0.056566, mean_q: -0.327721
 27961/100000: episode: 453, duration: 0.154s, episode steps: 32, steps per second: 207, episode reward: 10.537, mean reward: 0.329 [0.209, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.035, 10.329], loss: 0.002974, mae: 0.055299, mean_q: -0.243684
 28025/100000: episode: 454, duration: 0.330s, episode steps: 64, steps per second: 194, episode reward: 16.371, mean reward: 0.256 [0.061, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.781 [-0.716, 10.353], loss: 0.003075, mae: 0.056010, mean_q: -0.257781
 28090/100000: episode: 455, duration: 0.332s, episode steps: 65, steps per second: 196, episode reward: 12.872, mean reward: 0.198 [0.039, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.764 [-0.464, 10.100], loss: 0.002971, mae: 0.055074, mean_q: -0.215997
 28154/100000: episode: 456, duration: 0.339s, episode steps: 64, steps per second: 189, episode reward: 9.968, mean reward: 0.156 [0.020, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.779 [-0.458, 10.100], loss: 0.004886, mae: 0.070868, mean_q: -0.204356
 28183/100000: episode: 457, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 11.259, mean reward: 0.388 [0.202, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.035, 10.399], loss: 0.003088, mae: 0.057312, mean_q: -0.179065
 28247/100000: episode: 458, duration: 0.318s, episode steps: 64, steps per second: 201, episode reward: 11.473, mean reward: 0.179 [0.022, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.779 [-1.075, 10.100], loss: 0.003129, mae: 0.056306, mean_q: -0.216603
 28264/100000: episode: 459, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 4.450, mean reward: 0.262 [0.097, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.100], loss: 0.003082, mae: 0.057900, mean_q: -0.163577
 28297/100000: episode: 460, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 12.962, mean reward: 0.393 [0.195, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-1.120, 10.100], loss: 0.002813, mae: 0.054519, mean_q: -0.200972
 28356/100000: episode: 461, duration: 0.291s, episode steps: 59, steps per second: 203, episode reward: 9.483, mean reward: 0.161 [0.023, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.833 [-0.327, 10.151], loss: 0.003072, mae: 0.056379, mean_q: -0.198023
 28376/100000: episode: 462, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 6.830, mean reward: 0.341 [0.250, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.236, 10.100], loss: 0.002895, mae: 0.055107, mean_q: -0.225441
 28396/100000: episode: 463, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 8.352, mean reward: 0.418 [0.290, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.358, 10.100], loss: 0.002952, mae: 0.056981, mean_q: -0.159336
 28428/100000: episode: 464, duration: 0.166s, episode steps: 32, steps per second: 192, episode reward: 11.191, mean reward: 0.350 [0.204, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.044, 10.322], loss: 0.003171, mae: 0.056774, mean_q: -0.188223
 28487/100000: episode: 465, duration: 0.307s, episode steps: 59, steps per second: 192, episode reward: 12.334, mean reward: 0.209 [0.019, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.826 [-0.447, 10.351], loss: 0.003168, mae: 0.057050, mean_q: -0.189473
 28551/100000: episode: 466, duration: 0.344s, episode steps: 64, steps per second: 186, episode reward: 20.539, mean reward: 0.321 [0.044, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.774 [-1.066, 10.226], loss: 0.003687, mae: 0.062882, mean_q: -0.126826
 28583/100000: episode: 467, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 10.473, mean reward: 0.327 [0.215, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.745, 10.355], loss: 0.003363, mae: 0.060391, mean_q: -0.159050
 28615/100000: episode: 468, duration: 0.158s, episode steps: 32, steps per second: 203, episode reward: 6.843, mean reward: 0.214 [0.026, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.785, 10.100], loss: 0.003309, mae: 0.060020, mean_q: -0.150200
 28651/100000: episode: 469, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 13.806, mean reward: 0.384 [0.203, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.035, 10.508], loss: 0.004003, mae: 0.066075, mean_q: -0.144401
 28668/100000: episode: 470, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 5.500, mean reward: 0.324 [0.264, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.403, 10.100], loss: 0.003477, mae: 0.060591, mean_q: -0.146791
 28688/100000: episode: 471, duration: 0.098s, episode steps: 20, steps per second: 205, episode reward: 7.374, mean reward: 0.369 [0.307, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.640, 10.100], loss: 0.003947, mae: 0.065649, mean_q: -0.106260
 28753/100000: episode: 472, duration: 0.339s, episode steps: 65, steps per second: 192, episode reward: 16.916, mean reward: 0.260 [0.088, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.775 [-0.947, 10.286], loss: 0.003542, mae: 0.062843, mean_q: -0.096023
 28817/100000: episode: 473, duration: 0.331s, episode steps: 64, steps per second: 193, episode reward: 12.910, mean reward: 0.202 [0.014, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.794 [-0.844, 10.371], loss: 0.003038, mae: 0.056952, mean_q: -0.125423
 28853/100000: episode: 474, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 14.896, mean reward: 0.414 [0.233, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.526, 10.404], loss: 0.003410, mae: 0.061076, mean_q: -0.068016
 28912/100000: episode: 475, duration: 0.286s, episode steps: 59, steps per second: 206, episode reward: 13.434, mean reward: 0.228 [0.118, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.836 [-0.314, 10.329], loss: 0.003230, mae: 0.059152, mean_q: -0.098722
 28948/100000: episode: 476, duration: 0.177s, episode steps: 36, steps per second: 203, episode reward: 13.327, mean reward: 0.370 [0.264, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.035, 10.558], loss: 0.003279, mae: 0.059076, mean_q: -0.099328
 28980/100000: episode: 477, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 9.898, mean reward: 0.309 [0.239, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-1.357, 10.428], loss: 0.003054, mae: 0.057441, mean_q: -0.070184
 28997/100000: episode: 478, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 4.262, mean reward: 0.251 [0.116, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.388, 10.100], loss: 0.003585, mae: 0.062789, mean_q: -0.114149
 29061/100000: episode: 479, duration: 0.317s, episode steps: 64, steps per second: 202, episode reward: 14.736, mean reward: 0.230 [0.041, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.781 [-1.274, 10.239], loss: 0.003265, mae: 0.059674, mean_q: -0.092132
 29120/100000: episode: 480, duration: 0.281s, episode steps: 59, steps per second: 210, episode reward: 17.959, mean reward: 0.304 [0.108, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.817 [-0.272, 10.205], loss: 0.003116, mae: 0.057056, mean_q: -0.121020
 29179/100000: episode: 481, duration: 0.295s, episode steps: 59, steps per second: 200, episode reward: 11.706, mean reward: 0.198 [0.047, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.822 [-1.486, 10.270], loss: 0.003477, mae: 0.061339, mean_q: -0.077027
 29238/100000: episode: 482, duration: 0.307s, episode steps: 59, steps per second: 192, episode reward: 12.030, mean reward: 0.204 [0.021, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.818 [-0.306, 10.100], loss: 0.003436, mae: 0.061359, mean_q: -0.022766
 29297/100000: episode: 483, duration: 0.298s, episode steps: 59, steps per second: 198, episode reward: 10.844, mean reward: 0.184 [0.018, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.821 [-0.892, 10.100], loss: 0.003070, mae: 0.058953, mean_q: -0.062201
 29326/100000: episode: 484, duration: 0.142s, episode steps: 29, steps per second: 204, episode reward: 7.375, mean reward: 0.254 [0.149, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.325], loss: 0.003577, mae: 0.061777, mean_q: -0.060944
 29362/100000: episode: 485, duration: 0.203s, episode steps: 36, steps per second: 177, episode reward: 10.019, mean reward: 0.278 [0.100, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.795, 10.267], loss: 0.004141, mae: 0.066847, mean_q: -0.060622
 29421/100000: episode: 486, duration: 0.297s, episode steps: 59, steps per second: 199, episode reward: 16.202, mean reward: 0.275 [0.055, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.818 [-0.632, 10.232], loss: 0.003287, mae: 0.060495, mean_q: 0.001022
 29485/100000: episode: 487, duration: 0.316s, episode steps: 64, steps per second: 203, episode reward: 9.297, mean reward: 0.145 [0.006, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.789 [-0.495, 10.100], loss: 0.003093, mae: 0.058032, mean_q: -0.034417
 29518/100000: episode: 488, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 11.711, mean reward: 0.355 [0.208, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.965, 10.100], loss: 0.003291, mae: 0.059313, mean_q: -0.025676
 29538/100000: episode: 489, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 8.755, mean reward: 0.438 [0.291, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.538, 10.100], loss: 0.003151, mae: 0.059655, mean_q: -0.071693
 29570/100000: episode: 490, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 9.748, mean reward: 0.305 [0.207, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.035, 10.339], loss: 0.003519, mae: 0.063077, mean_q: -0.034033
 29603/100000: episode: 491, duration: 0.161s, episode steps: 33, steps per second: 206, episode reward: 12.388, mean reward: 0.375 [0.264, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-1.101, 10.100], loss: 0.003495, mae: 0.063014, mean_q: -0.003406
 29623/100000: episode: 492, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 6.994, mean reward: 0.350 [0.287, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.618, 10.100], loss: 0.003624, mae: 0.064922, mean_q: 0.043042
 29687/100000: episode: 493, duration: 0.338s, episode steps: 64, steps per second: 190, episode reward: 11.801, mean reward: 0.184 [0.030, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.782 [-0.453, 10.227], loss: 0.003223, mae: 0.059540, mean_q: 0.018403
 29746/100000: episode: 494, duration: 0.287s, episode steps: 59, steps per second: 206, episode reward: 13.929, mean reward: 0.236 [0.043, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.820 [-1.039, 10.398], loss: 0.003364, mae: 0.061900, mean_q: 0.046984
 29779/100000: episode: 495, duration: 0.169s, episode steps: 33, steps per second: 196, episode reward: 8.216, mean reward: 0.249 [0.005, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.484, 10.100], loss: 0.005021, mae: 0.076031, mean_q: 0.068302
 29808/100000: episode: 496, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 11.219, mean reward: 0.387 [0.316, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.221, 10.398], loss: 0.005794, mae: 0.078243, mean_q: 0.075411
 29828/100000: episode: 497, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 4.362, mean reward: 0.218 [0.064, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.126, 10.118], loss: 0.003806, mae: 0.065094, mean_q: 0.089913
 29887/100000: episode: 498, duration: 0.307s, episode steps: 59, steps per second: 192, episode reward: 10.246, mean reward: 0.174 [0.010, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.825 [-0.258, 10.100], loss: 0.003975, mae: 0.068608, mean_q: 0.025150
 29946/100000: episode: 499, duration: 0.297s, episode steps: 59, steps per second: 199, episode reward: 15.871, mean reward: 0.269 [0.113, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.821 [-1.275, 10.225], loss: 0.003919, mae: 0.067679, mean_q: 0.063613
 30011/100000: episode: 500, duration: 0.333s, episode steps: 65, steps per second: 195, episode reward: 18.064, mean reward: 0.278 [0.067, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.761 [-0.968, 10.190], loss: 0.003862, mae: 0.066812, mean_q: 0.065595
 30075/100000: episode: 501, duration: 0.320s, episode steps: 64, steps per second: 200, episode reward: 13.875, mean reward: 0.217 [0.028, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.797 [-1.573, 10.333], loss: 0.003584, mae: 0.063753, mean_q: 0.086424
 30140/100000: episode: 502, duration: 0.320s, episode steps: 65, steps per second: 203, episode reward: 13.302, mean reward: 0.205 [0.036, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.765 [-1.138, 10.100], loss: 0.003772, mae: 0.065242, mean_q: 0.079880
 30176/100000: episode: 503, duration: 0.177s, episode steps: 36, steps per second: 204, episode reward: 7.906, mean reward: 0.220 [0.029, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-1.452, 10.126], loss: 0.003754, mae: 0.064828, mean_q: 0.101031
 30241/100000: episode: 504, duration: 0.335s, episode steps: 65, steps per second: 194, episode reward: 10.766, mean reward: 0.166 [0.054, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.766 [-1.116, 10.100], loss: 0.003805, mae: 0.065971, mean_q: 0.111864
 30261/100000: episode: 505, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 5.850, mean reward: 0.293 [0.243, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.458, 10.100], loss: 0.004280, mae: 0.069165, mean_q: 0.118811
 30326/100000: episode: 506, duration: 0.341s, episode steps: 65, steps per second: 191, episode reward: 13.714, mean reward: 0.211 [0.013, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.774 [-0.485, 10.100], loss: 0.003668, mae: 0.065600, mean_q: 0.123227
 30362/100000: episode: 507, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 11.443, mean reward: 0.318 [0.149, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.472, 10.244], loss: 0.003414, mae: 0.062151, mean_q: 0.088304
 30394/100000: episode: 508, duration: 0.168s, episode steps: 32, steps per second: 191, episode reward: 8.853, mean reward: 0.277 [0.170, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.489, 10.276], loss: 0.003233, mae: 0.061341, mean_q: 0.130567
 30430/100000: episode: 509, duration: 0.198s, episode steps: 36, steps per second: 181, episode reward: 15.509, mean reward: 0.431 [0.247, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-1.147, 10.591], loss: 0.003618, mae: 0.064384, mean_q: 0.105367
 30494/100000: episode: 510, duration: 0.318s, episode steps: 64, steps per second: 201, episode reward: 11.904, mean reward: 0.186 [0.049, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.780 [-0.855, 10.100], loss: 0.003562, mae: 0.064431, mean_q: 0.143644
 30511/100000: episode: 511, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 5.389, mean reward: 0.317 [0.228, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.161, 10.100], loss: 0.002944, mae: 0.057799, mean_q: 0.143079
 30528/100000: episode: 512, duration: 0.081s, episode steps: 17, steps per second: 210, episode reward: 7.112, mean reward: 0.418 [0.296, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.600, 10.100], loss: 0.003420, mae: 0.063404, mean_q: 0.185142
 30587/100000: episode: 513, duration: 0.292s, episode steps: 59, steps per second: 202, episode reward: 11.004, mean reward: 0.187 [0.036, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.823 [-0.616, 10.219], loss: 0.003573, mae: 0.065122, mean_q: 0.193491
 30646/100000: episode: 514, duration: 0.278s, episode steps: 59, steps per second: 212, episode reward: 9.417, mean reward: 0.160 [0.020, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.821 [-1.131, 10.148], loss: 0.003659, mae: 0.064414, mean_q: 0.133622
 30663/100000: episode: 515, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 6.578, mean reward: 0.387 [0.281, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.493, 10.100], loss: 0.003384, mae: 0.061796, mean_q: 0.165530
 30696/100000: episode: 516, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 10.541, mean reward: 0.319 [0.193, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.790, 10.100], loss: 0.003952, mae: 0.068935, mean_q: 0.208247
 30728/100000: episode: 517, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 8.169, mean reward: 0.255 [0.051, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.035, 10.213], loss: 0.003997, mae: 0.070446, mean_q: 0.259754
 30745/100000: episode: 518, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 5.761, mean reward: 0.339 [0.253, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.552, 10.100], loss: 0.003996, mae: 0.067404, mean_q: 0.225973
 30804/100000: episode: 519, duration: 0.292s, episode steps: 59, steps per second: 202, episode reward: 14.135, mean reward: 0.240 [0.058, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.825 [-0.751, 10.207], loss: 0.003954, mae: 0.068510, mean_q: 0.218879
 30840/100000: episode: 520, duration: 0.189s, episode steps: 36, steps per second: 191, episode reward: 13.048, mean reward: 0.362 [0.253, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.097, 10.469], loss: 0.004150, mae: 0.070215, mean_q: 0.214823
 30872/100000: episode: 521, duration: 0.149s, episode steps: 32, steps per second: 215, episode reward: 8.035, mean reward: 0.251 [0.053, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.035, 10.198], loss: 0.003964, mae: 0.067674, mean_q: 0.238642
 30889/100000: episode: 522, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 6.713, mean reward: 0.395 [0.296, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.268, 10.100], loss: 0.004648, mae: 0.075326, mean_q: 0.277851
 30906/100000: episode: 523, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 5.963, mean reward: 0.351 [0.280, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.461, 10.100], loss: 0.003675, mae: 0.064351, mean_q: 0.174605
 30926/100000: episode: 524, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 6.813, mean reward: 0.341 [0.211, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.217, 10.100], loss: 0.004887, mae: 0.071061, mean_q: 0.216745
 30958/100000: episode: 525, duration: 0.164s, episode steps: 32, steps per second: 196, episode reward: 14.512, mean reward: 0.453 [0.267, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.952, 10.505], loss: 0.005296, mae: 0.077199, mean_q: 0.182851
 31017/100000: episode: 526, duration: 0.292s, episode steps: 59, steps per second: 202, episode reward: 19.468, mean reward: 0.330 [0.224, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.813 [-0.855, 10.366], loss: 0.004230, mae: 0.070039, mean_q: 0.249146
 31050/100000: episode: 527, duration: 0.157s, episode steps: 33, steps per second: 211, episode reward: 9.430, mean reward: 0.286 [0.196, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.748, 10.100], loss: 0.004003, mae: 0.066601, mean_q: 0.257207
 31083/100000: episode: 528, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 11.034, mean reward: 0.334 [0.104, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.194, 10.100], loss: 0.003513, mae: 0.066205, mean_q: 0.278496
 31112/100000: episode: 529, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 11.239, mean reward: 0.388 [0.313, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.987, 10.462], loss: 0.003885, mae: 0.066990, mean_q: 0.248936
 31176/100000: episode: 530, duration: 0.335s, episode steps: 64, steps per second: 191, episode reward: 12.011, mean reward: 0.188 [0.042, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.783 [-0.827, 10.297], loss: 0.003747, mae: 0.066157, mean_q: 0.272590
 31212/100000: episode: 531, duration: 0.172s, episode steps: 36, steps per second: 210, episode reward: 14.100, mean reward: 0.392 [0.208, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.439, 10.527], loss: 0.003508, mae: 0.065319, mean_q: 0.260367
 31276/100000: episode: 532, duration: 0.318s, episode steps: 64, steps per second: 201, episode reward: 12.226, mean reward: 0.191 [0.004, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.781 [-0.338, 10.167], loss: 0.004150, mae: 0.070694, mean_q: 0.281676
 31309/100000: episode: 533, duration: 0.160s, episode steps: 33, steps per second: 207, episode reward: 10.068, mean reward: 0.305 [0.220, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.262, 10.100], loss: 0.003610, mae: 0.065938, mean_q: 0.260039
 31326/100000: episode: 534, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 6.310, mean reward: 0.371 [0.252, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.262, 10.100], loss: 0.002942, mae: 0.061074, mean_q: 0.309893
[Info] 200-TH LEVEL FOUND: 0.7988053560256958, Considering 10/90 traces
 31390/100000: episode: 535, duration: 4.197s, episode steps: 64, steps per second: 15, episode reward: 17.606, mean reward: 0.275 [0.030, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.777 [-0.858, 10.119], loss: 0.003582, mae: 0.064834, mean_q: 0.286459
 31414/100000: episode: 536, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 8.860, mean reward: 0.369 [0.289, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-1.051, 10.422], loss: 0.003712, mae: 0.067070, mean_q: 0.296830
 31446/100000: episode: 537, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 10.819, mean reward: 0.338 [0.162, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.592, 10.355], loss: 0.003939, mae: 0.068333, mean_q: 0.326303
 31478/100000: episode: 538, duration: 0.155s, episode steps: 32, steps per second: 207, episode reward: 15.206, mean reward: 0.475 [0.387, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.035, 10.579], loss: 0.003636, mae: 0.066165, mean_q: 0.328810
 31510/100000: episode: 539, duration: 0.163s, episode steps: 32, steps per second: 197, episode reward: 13.630, mean reward: 0.426 [0.300, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.035, 10.476], loss: 0.004531, mae: 0.073446, mean_q: 0.340612
 31534/100000: episode: 540, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 9.811, mean reward: 0.409 [0.215, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.966, 10.310], loss: 0.003903, mae: 0.067806, mean_q: 0.331245
 31563/100000: episode: 541, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 10.646, mean reward: 0.367 [0.225, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.198, 10.458], loss: 0.003682, mae: 0.066369, mean_q: 0.359340
 31595/100000: episode: 542, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 15.618, mean reward: 0.488 [0.378, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.138, 10.582], loss: 0.004126, mae: 0.070717, mean_q: 0.345130
 31624/100000: episode: 543, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 10.661, mean reward: 0.368 [0.168, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.738, 10.100], loss: 0.003786, mae: 0.066877, mean_q: 0.361445
 31653/100000: episode: 544, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 15.697, mean reward: 0.541 [0.410, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.543, 10.100], loss: 0.005453, mae: 0.075841, mean_q: 0.390943
 31682/100000: episode: 545, duration: 0.139s, episode steps: 29, steps per second: 209, episode reward: 11.802, mean reward: 0.407 [0.148, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.295, 10.310], loss: 0.004201, mae: 0.073966, mean_q: 0.397191
 31712/100000: episode: 546, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 10.954, mean reward: 0.365 [0.269, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.043, 10.440], loss: 0.004171, mae: 0.071021, mean_q: 0.330883
 31744/100000: episode: 547, duration: 0.166s, episode steps: 32, steps per second: 192, episode reward: 10.706, mean reward: 0.335 [0.221, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.449, 10.393], loss: 0.004665, mae: 0.072378, mean_q: 0.360561
 31768/100000: episode: 548, duration: 0.138s, episode steps: 24, steps per second: 173, episode reward: 8.469, mean reward: 0.353 [0.196, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.313], loss: 0.004492, mae: 0.071379, mean_q: 0.355716
 31797/100000: episode: 549, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 14.740, mean reward: 0.508 [0.442, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.509, 10.100], loss: 0.003727, mae: 0.067329, mean_q: 0.400124
 31819/100000: episode: 550, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 7.566, mean reward: 0.344 [0.180, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.316], loss: 0.003427, mae: 0.064672, mean_q: 0.420445
 31843/100000: episode: 551, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 8.268, mean reward: 0.345 [0.191, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.208, 10.355], loss: 0.003491, mae: 0.064969, mean_q: 0.432414
 31867/100000: episode: 552, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 8.850, mean reward: 0.369 [0.287, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.441, 10.100], loss: 0.003634, mae: 0.066098, mean_q: 0.410400
 31891/100000: episode: 553, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 8.401, mean reward: 0.350 [0.289, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.338, 10.100], loss: 0.003634, mae: 0.066800, mean_q: 0.436196
 31921/100000: episode: 554, duration: 0.148s, episode steps: 30, steps per second: 202, episode reward: 11.777, mean reward: 0.393 [0.279, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.303, 10.361], loss: 0.003833, mae: 0.067560, mean_q: 0.414063
 31950/100000: episode: 555, duration: 0.162s, episode steps: 29, steps per second: 180, episode reward: 12.573, mean reward: 0.434 [0.340, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.471, 10.485], loss: 0.004197, mae: 0.069790, mean_q: 0.449139
 31969/100000: episode: 556, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 8.342, mean reward: 0.439 [0.325, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-1.145, 10.454], loss: 0.003583, mae: 0.065224, mean_q: 0.426056
 31998/100000: episode: 557, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 13.656, mean reward: 0.471 [0.393, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-1.204, 10.100], loss: 0.003895, mae: 0.069390, mean_q: 0.457980
 32027/100000: episode: 558, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 10.439, mean reward: 0.360 [0.223, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.414, 10.100], loss: 0.003899, mae: 0.068244, mean_q: 0.456958
 32059/100000: episode: 559, duration: 0.162s, episode steps: 32, steps per second: 198, episode reward: 13.508, mean reward: 0.422 [0.290, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.229, 10.431], loss: 0.003517, mae: 0.064211, mean_q: 0.436898
 32078/100000: episode: 560, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 8.687, mean reward: 0.457 [0.365, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.703, 10.481], loss: 0.003932, mae: 0.068638, mean_q: 0.440283
 32107/100000: episode: 561, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 11.935, mean reward: 0.412 [0.303, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.538, 10.531], loss: 0.004174, mae: 0.071297, mean_q: 0.463712
 32129/100000: episode: 562, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 9.812, mean reward: 0.446 [0.398, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.529, 10.524], loss: 0.004121, mae: 0.071563, mean_q: 0.497452
 32158/100000: episode: 563, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 11.285, mean reward: 0.389 [0.266, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.359, 10.100], loss: 0.003440, mae: 0.065391, mean_q: 0.488279
 32187/100000: episode: 564, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 9.774, mean reward: 0.337 [0.254, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.267, 10.100], loss: 0.004232, mae: 0.070920, mean_q: 0.512918
 32206/100000: episode: 565, duration: 0.089s, episode steps: 19, steps per second: 212, episode reward: 7.964, mean reward: 0.419 [0.370, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.085, 10.444], loss: 0.003532, mae: 0.066207, mean_q: 0.496145
 32236/100000: episode: 566, duration: 0.151s, episode steps: 30, steps per second: 198, episode reward: 14.204, mean reward: 0.473 [0.315, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.853, 10.444], loss: 0.004217, mae: 0.069591, mean_q: 0.482693
 32258/100000: episode: 567, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 6.975, mean reward: 0.317 [0.181, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.385, 10.370], loss: 0.004208, mae: 0.071033, mean_q: 0.502881
 32290/100000: episode: 568, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 13.150, mean reward: 0.411 [0.274, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.181, 10.421], loss: 0.004106, mae: 0.070050, mean_q: 0.498963
 32314/100000: episode: 569, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 8.332, mean reward: 0.347 [0.278, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.377], loss: 0.004585, mae: 0.070814, mean_q: 0.525630
 32333/100000: episode: 570, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 7.583, mean reward: 0.399 [0.298, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.933, 10.461], loss: 0.004997, mae: 0.068900, mean_q: 0.505936
 32365/100000: episode: 571, duration: 0.179s, episode steps: 32, steps per second: 179, episode reward: 16.294, mean reward: 0.509 [0.354, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.200, 10.537], loss: 0.003795, mae: 0.068265, mean_q: 0.541272
 32387/100000: episode: 572, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 8.738, mean reward: 0.397 [0.295, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.410], loss: 0.003810, mae: 0.067386, mean_q: 0.519359
 32411/100000: episode: 573, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 7.946, mean reward: 0.331 [0.192, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.266], loss: 0.003543, mae: 0.065351, mean_q: 0.524049
 32441/100000: episode: 574, duration: 0.173s, episode steps: 30, steps per second: 173, episode reward: 9.677, mean reward: 0.323 [0.132, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.035, 10.274], loss: 0.003537, mae: 0.065529, mean_q: 0.545612
 32465/100000: episode: 575, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 8.245, mean reward: 0.344 [0.221, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.211, 10.100], loss: 0.004306, mae: 0.072613, mean_q: 0.539568
 32489/100000: episode: 576, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 10.893, mean reward: 0.454 [0.351, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.783, 10.100], loss: 0.003474, mae: 0.065401, mean_q: 0.566033
 32513/100000: episode: 577, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 9.591, mean reward: 0.400 [0.272, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.645, 10.100], loss: 0.004029, mae: 0.069002, mean_q: 0.546065
 32545/100000: episode: 578, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 10.925, mean reward: 0.341 [0.086, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.405, 10.313], loss: 0.003742, mae: 0.066959, mean_q: 0.575800
 32574/100000: episode: 579, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 13.175, mean reward: 0.454 [0.316, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.526, 10.100], loss: 0.004275, mae: 0.070815, mean_q: 0.580673
 32593/100000: episode: 580, duration: 0.091s, episode steps: 19, steps per second: 210, episode reward: 7.793, mean reward: 0.410 [0.340, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.555], loss: 0.003706, mae: 0.067819, mean_q: 0.574893
 32617/100000: episode: 581, duration: 0.125s, episode steps: 24, steps per second: 191, episode reward: 6.737, mean reward: 0.281 [0.090, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.189, 10.328], loss: 0.003530, mae: 0.066183, mean_q: 0.587117
 32639/100000: episode: 582, duration: 0.108s, episode steps: 22, steps per second: 203, episode reward: 9.453, mean reward: 0.430 [0.297, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.562], loss: 0.003590, mae: 0.065341, mean_q: 0.587354
 32668/100000: episode: 583, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 14.576, mean reward: 0.503 [0.404, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.298, 10.100], loss: 0.003534, mae: 0.065354, mean_q: 0.569054
 32700/100000: episode: 584, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 12.711, mean reward: 0.397 [0.271, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.197, 10.421], loss: 0.004002, mae: 0.069506, mean_q: 0.570701
 32724/100000: episode: 585, duration: 0.115s, episode steps: 24, steps per second: 209, episode reward: 10.651, mean reward: 0.444 [0.324, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.977, 10.502], loss: 0.003480, mae: 0.065187, mean_q: 0.588230
 32753/100000: episode: 586, duration: 0.146s, episode steps: 29, steps per second: 198, episode reward: 14.823, mean reward: 0.511 [0.342, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.649, 10.100], loss: 0.004134, mae: 0.070465, mean_q: 0.585223
 32782/100000: episode: 587, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 11.927, mean reward: 0.411 [0.303, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.486, 10.100], loss: 0.003492, mae: 0.064367, mean_q: 0.578376
 32812/100000: episode: 588, duration: 0.142s, episode steps: 30, steps per second: 211, episode reward: 13.329, mean reward: 0.444 [0.292, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.709, 10.518], loss: 0.003556, mae: 0.066578, mean_q: 0.583789
 32842/100000: episode: 589, duration: 0.144s, episode steps: 30, steps per second: 208, episode reward: 8.935, mean reward: 0.298 [0.149, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.733, 10.217], loss: 0.004220, mae: 0.071836, mean_q: 0.592062
 32871/100000: episode: 590, duration: 0.144s, episode steps: 29, steps per second: 201, episode reward: 8.351, mean reward: 0.288 [0.134, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.435, 10.100], loss: 0.004158, mae: 0.071019, mean_q: 0.587452
 32903/100000: episode: 591, duration: 0.156s, episode steps: 32, steps per second: 205, episode reward: 11.242, mean reward: 0.351 [0.209, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.524, 10.397], loss: 0.003857, mae: 0.068383, mean_q: 0.591952
 32932/100000: episode: 592, duration: 0.168s, episode steps: 29, steps per second: 172, episode reward: 10.441, mean reward: 0.360 [0.231, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.283, 10.100], loss: 0.003960, mae: 0.069534, mean_q: 0.583362
 32964/100000: episode: 593, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 13.757, mean reward: 0.430 [0.144, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.552, 10.398], loss: 0.003616, mae: 0.066280, mean_q: 0.591129
 32996/100000: episode: 594, duration: 0.156s, episode steps: 32, steps per second: 205, episode reward: 12.823, mean reward: 0.401 [0.309, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.675, 10.513], loss: 0.003950, mae: 0.067546, mean_q: 0.596431
 33028/100000: episode: 595, duration: 0.168s, episode steps: 32, steps per second: 191, episode reward: 18.396, mean reward: 0.575 [0.417, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.841, 10.513], loss: 0.003431, mae: 0.064820, mean_q: 0.624746
 33052/100000: episode: 596, duration: 0.112s, episode steps: 24, steps per second: 214, episode reward: 7.754, mean reward: 0.323 [0.189, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.319], loss: 0.004387, mae: 0.073083, mean_q: 0.593043
 33081/100000: episode: 597, duration: 0.141s, episode steps: 29, steps per second: 206, episode reward: 9.002, mean reward: 0.310 [0.169, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.302, 10.100], loss: 0.004672, mae: 0.075249, mean_q: 0.600840
 33110/100000: episode: 598, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 11.013, mean reward: 0.380 [0.206, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.446, 10.308], loss: 0.003726, mae: 0.067738, mean_q: 0.606195
 33140/100000: episode: 599, duration: 0.159s, episode steps: 30, steps per second: 188, episode reward: 10.817, mean reward: 0.361 [0.191, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-1.281, 10.356], loss: 0.003586, mae: 0.067605, mean_q: 0.611717
 33164/100000: episode: 600, duration: 0.111s, episode steps: 24, steps per second: 216, episode reward: 9.628, mean reward: 0.401 [0.224, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.307, 10.402], loss: 0.003758, mae: 0.066831, mean_q: 0.609635
 33193/100000: episode: 601, duration: 0.137s, episode steps: 29, steps per second: 211, episode reward: 14.354, mean reward: 0.495 [0.412, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-1.186, 10.100], loss: 0.003449, mae: 0.065084, mean_q: 0.610128
 33225/100000: episode: 602, duration: 0.187s, episode steps: 32, steps per second: 171, episode reward: 12.096, mean reward: 0.378 [0.241, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.795, 10.399], loss: 0.003346, mae: 0.064529, mean_q: 0.618654
 33257/100000: episode: 603, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 11.544, mean reward: 0.361 [0.264, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.444, 10.523], loss: 0.003302, mae: 0.062741, mean_q: 0.605045
 33286/100000: episode: 604, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 13.406, mean reward: 0.462 [0.284, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.357, 10.100], loss: 0.003693, mae: 0.066333, mean_q: 0.617761
 33308/100000: episode: 605, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 8.840, mean reward: 0.402 [0.322, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.325, 10.514], loss: 0.003590, mae: 0.066159, mean_q: 0.619678
 33330/100000: episode: 606, duration: 0.139s, episode steps: 22, steps per second: 158, episode reward: 8.989, mean reward: 0.409 [0.308, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.506], loss: 0.003427, mae: 0.064351, mean_q: 0.622419
 33354/100000: episode: 607, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 8.457, mean reward: 0.352 [0.239, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.976, 10.506], loss: 0.003441, mae: 0.063763, mean_q: 0.615701
 33386/100000: episode: 608, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 16.412, mean reward: 0.513 [0.260, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.405, 10.456], loss: 0.003669, mae: 0.066462, mean_q: 0.623421
 33418/100000: episode: 609, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 13.766, mean reward: 0.430 [0.351, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.239, 10.548], loss: 0.003867, mae: 0.068904, mean_q: 0.618488
 33450/100000: episode: 610, duration: 0.166s, episode steps: 32, steps per second: 192, episode reward: 14.278, mean reward: 0.446 [0.342, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.809, 10.601], loss: 0.003997, mae: 0.069001, mean_q: 0.627287
 33479/100000: episode: 611, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 10.508, mean reward: 0.362 [0.231, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.478, 10.100], loss: 0.003632, mae: 0.066512, mean_q: 0.635304
 33509/100000: episode: 612, duration: 0.147s, episode steps: 30, steps per second: 204, episode reward: 11.637, mean reward: 0.388 [0.268, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.035, 10.432], loss: 0.003599, mae: 0.066275, mean_q: 0.645391
 33538/100000: episode: 613, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 11.141, mean reward: 0.384 [0.232, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.103, 10.100], loss: 0.004738, mae: 0.075425, mean_q: 0.621997
 33567/100000: episode: 614, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 9.809, mean reward: 0.338 [0.148, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.905, 10.339], loss: 0.003328, mae: 0.063241, mean_q: 0.627416
 33596/100000: episode: 615, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 14.915, mean reward: 0.514 [0.387, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.049, 10.100], loss: 0.003684, mae: 0.066731, mean_q: 0.629830
 33618/100000: episode: 616, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 8.406, mean reward: 0.382 [0.227, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.045, 10.356], loss: 0.004090, mae: 0.070736, mean_q: 0.639870
 33647/100000: episode: 617, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 9.533, mean reward: 0.329 [0.211, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.430, 10.100], loss: 0.003477, mae: 0.065292, mean_q: 0.634215
 33669/100000: episode: 618, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 8.813, mean reward: 0.401 [0.251, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.406, 10.430], loss: 0.003669, mae: 0.065945, mean_q: 0.647620
 33701/100000: episode: 619, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 13.080, mean reward: 0.409 [0.276, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.120, 10.477], loss: 0.004043, mae: 0.070291, mean_q: 0.642473
 33725/100000: episode: 620, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 8.900, mean reward: 0.371 [0.154, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.473, 10.100], loss: 0.004187, mae: 0.071534, mean_q: 0.640407
 33749/100000: episode: 621, duration: 0.118s, episode steps: 24, steps per second: 204, episode reward: 11.269, mean reward: 0.470 [0.277, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.302, 10.100], loss: 0.004029, mae: 0.070172, mean_q: 0.631210
 33781/100000: episode: 622, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 12.885, mean reward: 0.403 [0.284, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.035, 10.494], loss: 0.003600, mae: 0.065863, mean_q: 0.641662
 33811/100000: episode: 623, duration: 0.142s, episode steps: 30, steps per second: 212, episode reward: 9.090, mean reward: 0.303 [0.135, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.844, 10.292], loss: 0.003612, mae: 0.065672, mean_q: 0.645006
 33840/100000: episode: 624, duration: 0.141s, episode steps: 29, steps per second: 205, episode reward: 12.649, mean reward: 0.436 [0.297, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.282, 10.100], loss: 0.003658, mae: 0.066404, mean_q: 0.651720
[Info] 300-TH LEVEL FOUND: 0.9894189834594727, Considering 10/90 traces
 33872/100000: episode: 625, duration: 3.988s, episode steps: 32, steps per second: 8, episode reward: 13.322, mean reward: 0.416 [0.320, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.390, 10.390], loss: 0.004423, mae: 0.074182, mean_q: 0.635487
 33888/100000: episode: 626, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 8.720, mean reward: 0.545 [0.491, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.551, 10.100], loss: 0.003357, mae: 0.062769, mean_q: 0.642757
 33911/100000: episode: 627, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 11.697, mean reward: 0.509 [0.408, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.340, 10.640], loss: 0.003482, mae: 0.066633, mean_q: 0.656810
[Info] FALSIFICATION!
 33919/100000: episode: 628, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 14.201, mean reward: 1.775 [0.540, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.205, 9.861], loss: 0.003950, mae: 0.071734, mean_q: 0.632158
 34019/100000: episode: 629, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.015, mean reward: -0.180 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.889, 10.098], loss: 0.003733, mae: 0.067497, mean_q: 0.638732
 34119/100000: episode: 630, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.021, mean reward: -0.180 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.565, 10.098], loss: 0.042620, mae: 0.091089, mean_q: 0.634407
 34219/100000: episode: 631, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.598, mean reward: -0.166 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.475, 10.098], loss: 0.005323, mae: 0.074587, mean_q: 0.616800
 34319/100000: episode: 632, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -19.156, mean reward: -0.192 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.674, 10.146], loss: 0.004128, mae: 0.068147, mean_q: 0.594637
 34419/100000: episode: 633, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -16.334, mean reward: -0.163 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.081, 10.362], loss: 0.003640, mae: 0.065054, mean_q: 0.574196
 34519/100000: episode: 634, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -13.930, mean reward: -0.139 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.976, 10.194], loss: 0.017065, mae: 0.075945, mean_q: 0.545247
 34619/100000: episode: 635, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.964, mean reward: -0.180 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.560, 10.195], loss: 0.003752, mae: 0.066257, mean_q: 0.560848
 34719/100000: episode: 636, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.207, mean reward: -0.182 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.721, 10.212], loss: 0.017221, mae: 0.075337, mean_q: 0.532629
 34819/100000: episode: 637, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.553, mean reward: -0.156 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.027, 10.363], loss: 0.016697, mae: 0.076423, mean_q: 0.519945
 34919/100000: episode: 638, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.690, mean reward: -0.177 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.733, 10.153], loss: 0.003832, mae: 0.066424, mean_q: 0.462903
 35019/100000: episode: 639, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.184, mean reward: -0.172 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.334, 10.135], loss: 0.003760, mae: 0.066076, mean_q: 0.463667
 35119/100000: episode: 640, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -17.003, mean reward: -0.170 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.838, 10.098], loss: 0.042362, mae: 0.086368, mean_q: 0.463320
 35219/100000: episode: 641, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -18.300, mean reward: -0.183 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.951, 10.375], loss: 0.004351, mae: 0.071355, mean_q: 0.452330
 35319/100000: episode: 642, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -14.215, mean reward: -0.142 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.070, 10.133], loss: 0.016175, mae: 0.070442, mean_q: 0.423679
 35419/100000: episode: 643, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.372, mean reward: -0.184 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.483, 10.098], loss: 0.003568, mae: 0.064382, mean_q: 0.398328
 35519/100000: episode: 644, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -17.544, mean reward: -0.175 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.197, 10.110], loss: 0.016508, mae: 0.072446, mean_q: 0.388674
 35619/100000: episode: 645, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -19.561, mean reward: -0.196 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.173, 10.098], loss: 0.016623, mae: 0.073425, mean_q: 0.385303
 35719/100000: episode: 646, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -12.209, mean reward: -0.122 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.180, 10.098], loss: 0.003631, mae: 0.064905, mean_q: 0.375649
 35819/100000: episode: 647, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -17.189, mean reward: -0.172 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.643, 10.098], loss: 0.003166, mae: 0.059962, mean_q: 0.322908
 35919/100000: episode: 648, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.374, mean reward: -0.184 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.687, 10.098], loss: 0.016896, mae: 0.072323, mean_q: 0.309770
 36019/100000: episode: 649, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.615, mean reward: -0.176 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.314, 10.237], loss: 0.003304, mae: 0.062088, mean_q: 0.276378
 36119/100000: episode: 650, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.122, mean reward: -0.161 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.643, 10.098], loss: 0.042374, mae: 0.088950, mean_q: 0.297940
 36219/100000: episode: 651, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -18.608, mean reward: -0.186 [-1.000, 0.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.448, 10.204], loss: 0.016376, mae: 0.071740, mean_q: 0.271728
 36319/100000: episode: 652, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -12.085, mean reward: -0.121 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.710, 10.232], loss: 0.016734, mae: 0.073540, mean_q: 0.286283
 36419/100000: episode: 653, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -16.507, mean reward: -0.165 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.381, 10.098], loss: 0.016096, mae: 0.068170, mean_q: 0.203514
 36519/100000: episode: 654, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -20.167, mean reward: -0.202 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.594, 10.098], loss: 0.003455, mae: 0.062619, mean_q: 0.216061
 36619/100000: episode: 655, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.176, mean reward: -0.152 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.278, 10.306], loss: 0.016196, mae: 0.068637, mean_q: 0.185860
 36719/100000: episode: 656, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -16.728, mean reward: -0.167 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.037, 10.098], loss: 0.018660, mae: 0.080566, mean_q: 0.157058
 36819/100000: episode: 657, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.518, mean reward: -0.185 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.367, 10.098], loss: 0.042008, mae: 0.085510, mean_q: 0.144629
 36919/100000: episode: 658, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -16.966, mean reward: -0.170 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.939, 10.204], loss: 0.003289, mae: 0.059655, mean_q: 0.067716
 37019/100000: episode: 659, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -13.300, mean reward: -0.133 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.200, 10.506], loss: 0.003333, mae: 0.060209, mean_q: 0.092894
 37119/100000: episode: 660, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -15.746, mean reward: -0.157 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.978, 10.401], loss: 0.003082, mae: 0.056478, mean_q: 0.065434
 37219/100000: episode: 661, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -21.061, mean reward: -0.211 [-1.000, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.520, 10.098], loss: 0.016329, mae: 0.069653, mean_q: 0.039190
 37319/100000: episode: 662, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.210, mean reward: -0.162 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.551, 10.114], loss: 0.003039, mae: 0.057092, mean_q: 0.038334
 37419/100000: episode: 663, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -16.859, mean reward: -0.169 [-1.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.473, 10.098], loss: 0.002982, mae: 0.055994, mean_q: -0.015812
 37519/100000: episode: 664, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -11.437, mean reward: -0.114 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.513, 10.444], loss: 0.003334, mae: 0.060803, mean_q: -0.024618
 37619/100000: episode: 665, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -17.309, mean reward: -0.173 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.616, 10.308], loss: 0.003039, mae: 0.056760, mean_q: -0.049847
 37719/100000: episode: 666, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -18.845, mean reward: -0.188 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.293, 10.192], loss: 0.016322, mae: 0.068982, mean_q: -0.053698
 37819/100000: episode: 667, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -15.490, mean reward: -0.155 [-1.000, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.504, 10.098], loss: 0.002805, mae: 0.054013, mean_q: -0.103803
 37919/100000: episode: 668, duration: 0.459s, episode steps: 100, steps per second: 218, episode reward: -15.511, mean reward: -0.155 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.912, 10.098], loss: 0.004214, mae: 0.066244, mean_q: -0.124418
 38019/100000: episode: 669, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -17.694, mean reward: -0.177 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.783, 10.098], loss: 0.015600, mae: 0.061457, mean_q: -0.147470
 38119/100000: episode: 670, duration: 0.471s, episode steps: 100, steps per second: 213, episode reward: -11.849, mean reward: -0.118 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.339, 10.098], loss: 0.003445, mae: 0.060300, mean_q: -0.131221
 38219/100000: episode: 671, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.877, mean reward: -0.199 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.891, 10.225], loss: 0.017014, mae: 0.073457, mean_q: -0.140702
 38319/100000: episode: 672, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -16.218, mean reward: -0.162 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.523, 10.098], loss: 0.002855, mae: 0.054233, mean_q: -0.179323
 38419/100000: episode: 673, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -15.261, mean reward: -0.153 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.865, 10.098], loss: 0.002882, mae: 0.054861, mean_q: -0.208361
 38519/100000: episode: 674, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -15.052, mean reward: -0.151 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.695, 10.415], loss: 0.015941, mae: 0.066439, mean_q: -0.249875
 38619/100000: episode: 675, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -16.669, mean reward: -0.167 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.288, 10.098], loss: 0.016904, mae: 0.072328, mean_q: -0.213642
 38719/100000: episode: 676, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.346, mean reward: -0.183 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.478, 10.198], loss: 0.003114, mae: 0.057574, mean_q: -0.255291
 38819/100000: episode: 677, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.124, mean reward: -0.161 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.391, 10.520], loss: 0.002711, mae: 0.052227, mean_q: -0.327831
 38919/100000: episode: 678, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.713, mean reward: -0.157 [-1.000, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.655, 10.373], loss: 0.030416, mae: 0.080968, mean_q: -0.331294
 39019/100000: episode: 679, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -12.515, mean reward: -0.125 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.924, 10.098], loss: 0.003004, mae: 0.055595, mean_q: -0.315414
 39119/100000: episode: 680, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.143, mean reward: -0.171 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.337, 10.098], loss: 0.002938, mae: 0.054425, mean_q: -0.321266
 39219/100000: episode: 681, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -17.861, mean reward: -0.179 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.857, 10.347], loss: 0.002702, mae: 0.052384, mean_q: -0.320021
 39319/100000: episode: 682, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -19.178, mean reward: -0.192 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.884, 10.098], loss: 0.003007, mae: 0.055920, mean_q: -0.326075
 39419/100000: episode: 683, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.064, mean reward: -0.171 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.516, 10.098], loss: 0.002710, mae: 0.052382, mean_q: -0.326483
 39519/100000: episode: 684, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.509, mean reward: -0.175 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.514, 10.098], loss: 0.002717, mae: 0.051779, mean_q: -0.332937
 39619/100000: episode: 685, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -14.244, mean reward: -0.142 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.992, 10.121], loss: 0.002799, mae: 0.053064, mean_q: -0.342248
 39719/100000: episode: 686, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -21.177, mean reward: -0.212 [-1.000, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.422, 10.173], loss: 0.003022, mae: 0.056270, mean_q: -0.299665
 39819/100000: episode: 687, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.597, mean reward: -0.166 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.440, 10.098], loss: 0.002839, mae: 0.053016, mean_q: -0.316589
 39919/100000: episode: 688, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.115, mean reward: -0.161 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.371, 10.098], loss: 0.002680, mae: 0.050756, mean_q: -0.328758
 40019/100000: episode: 689, duration: 0.464s, episode steps: 100, steps per second: 216, episode reward: -18.116, mean reward: -0.181 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.144, 10.098], loss: 0.002543, mae: 0.050130, mean_q: -0.320218
 40119/100000: episode: 690, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -17.030, mean reward: -0.170 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.363, 10.098], loss: 0.003434, mae: 0.058993, mean_q: -0.306309
 40219/100000: episode: 691, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.017, mean reward: -0.180 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.068, 10.098], loss: 0.002949, mae: 0.054787, mean_q: -0.279832
 40319/100000: episode: 692, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.152, mean reward: -0.192 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.514, 10.098], loss: 0.002824, mae: 0.053849, mean_q: -0.292648
 40419/100000: episode: 693, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -15.735, mean reward: -0.157 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.829, 10.281], loss: 0.002807, mae: 0.052624, mean_q: -0.287135
 40519/100000: episode: 694, duration: 0.468s, episode steps: 100, steps per second: 214, episode reward: -21.261, mean reward: -0.213 [-1.000, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.141, 10.111], loss: 0.003082, mae: 0.055880, mean_q: -0.304950
 40619/100000: episode: 695, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -14.611, mean reward: -0.146 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.320, 10.098], loss: 0.003279, mae: 0.058223, mean_q: -0.292708
 40719/100000: episode: 696, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -17.530, mean reward: -0.175 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.870, 10.098], loss: 0.003045, mae: 0.054412, mean_q: -0.331654
 40819/100000: episode: 697, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.898, mean reward: -0.169 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.968, 10.189], loss: 0.003219, mae: 0.057610, mean_q: -0.288451
 40919/100000: episode: 698, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -18.655, mean reward: -0.187 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.858, 10.098], loss: 0.002795, mae: 0.052986, mean_q: -0.318530
 41019/100000: episode: 699, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -19.125, mean reward: -0.191 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.682, 10.098], loss: 0.002907, mae: 0.053477, mean_q: -0.332688
 41119/100000: episode: 700, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -13.927, mean reward: -0.139 [-1.000, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.923, 10.098], loss: 0.002573, mae: 0.050243, mean_q: -0.321029
 41219/100000: episode: 701, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.334, mean reward: -0.163 [-1.000, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.257, 10.128], loss: 0.002717, mae: 0.052779, mean_q: -0.317563
 41319/100000: episode: 702, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.230, mean reward: -0.192 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.707, 10.144], loss: 0.002942, mae: 0.053941, mean_q: -0.337359
 41419/100000: episode: 703, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -13.985, mean reward: -0.140 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.777, 10.125], loss: 0.005008, mae: 0.067587, mean_q: -0.317515
 41519/100000: episode: 704, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -17.010, mean reward: -0.170 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.561, 10.100], loss: 0.003926, mae: 0.062828, mean_q: -0.319490
 41619/100000: episode: 705, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -8.934, mean reward: -0.089 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.610, 10.449], loss: 0.002838, mae: 0.053907, mean_q: -0.310980
 41719/100000: episode: 706, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.807, mean reward: -0.188 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.177, 10.098], loss: 0.002656, mae: 0.052970, mean_q: -0.306145
 41819/100000: episode: 707, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -18.994, mean reward: -0.190 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.733, 10.098], loss: 0.002919, mae: 0.055102, mean_q: -0.324088
 41919/100000: episode: 708, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -12.982, mean reward: -0.130 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.042, 10.369], loss: 0.002758, mae: 0.053299, mean_q: -0.302011
 42019/100000: episode: 709, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -19.133, mean reward: -0.191 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.750, 10.201], loss: 0.002629, mae: 0.051489, mean_q: -0.308997
 42119/100000: episode: 710, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -19.466, mean reward: -0.195 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.493, 10.152], loss: 0.002676, mae: 0.052602, mean_q: -0.313982
 42219/100000: episode: 711, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -12.279, mean reward: -0.123 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.229, 10.098], loss: 0.002711, mae: 0.052715, mean_q: -0.327510
 42319/100000: episode: 712, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -16.190, mean reward: -0.162 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.932, 10.098], loss: 0.002891, mae: 0.054180, mean_q: -0.298579
 42419/100000: episode: 713, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -17.702, mean reward: -0.177 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.872, 10.098], loss: 0.002631, mae: 0.052964, mean_q: -0.307864
 42519/100000: episode: 714, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.142, mean reward: -0.161 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.466, 10.098], loss: 0.002615, mae: 0.050442, mean_q: -0.347166
 42619/100000: episode: 715, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -11.348, mean reward: -0.113 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.797, 10.328], loss: 0.002862, mae: 0.054620, mean_q: -0.303941
 42719/100000: episode: 716, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -18.281, mean reward: -0.183 [-1.000, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.786, 10.098], loss: 0.002723, mae: 0.053275, mean_q: -0.316275
 42819/100000: episode: 717, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.495, mean reward: -0.175 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.674, 10.098], loss: 0.002633, mae: 0.053651, mean_q: -0.290910
 42919/100000: episode: 718, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.139, mean reward: -0.171 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.317, 10.098], loss: 0.003567, mae: 0.062207, mean_q: -0.302219
 43019/100000: episode: 719, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -8.212, mean reward: -0.082 [-1.000, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.417, 10.098], loss: 0.002655, mae: 0.053578, mean_q: -0.263217
 43119/100000: episode: 720, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.857, mean reward: -0.159 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.203, 10.108], loss: 0.002310, mae: 0.048800, mean_q: -0.340888
 43219/100000: episode: 721, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -12.175, mean reward: -0.122 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.703, 10.372], loss: 0.002492, mae: 0.050471, mean_q: -0.295189
 43319/100000: episode: 722, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -18.765, mean reward: -0.188 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.529, 10.107], loss: 0.003128, mae: 0.056670, mean_q: -0.285109
 43419/100000: episode: 723, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.775, mean reward: -0.168 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.546, 10.164], loss: 0.002864, mae: 0.055379, mean_q: -0.314031
 43519/100000: episode: 724, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.189, mean reward: -0.182 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.931, 10.098], loss: 0.002518, mae: 0.050876, mean_q: -0.305497
 43619/100000: episode: 725, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.106, mean reward: -0.151 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.397, 10.098], loss: 0.002634, mae: 0.052877, mean_q: -0.328831
 43719/100000: episode: 726, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.552, mean reward: -0.176 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.260, 10.192], loss: 0.002814, mae: 0.054874, mean_q: -0.301411
 43819/100000: episode: 727, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.355, mean reward: -0.184 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.626, 10.311], loss: 0.002508, mae: 0.050558, mean_q: -0.303891
[Info] 100-TH LEVEL FOUND: 0.608380138874054, Considering 10/90 traces
 43919/100000: episode: 728, duration: 4.318s, episode steps: 100, steps per second: 23, episode reward: -20.660, mean reward: -0.207 [-1.000, 0.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.281, 10.231], loss: 0.002401, mae: 0.049454, mean_q: -0.370797
 43933/100000: episode: 729, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 3.991, mean reward: 0.285 [0.220, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.318, 10.335], loss: 0.002588, mae: 0.049958, mean_q: -0.365326
 43959/100000: episode: 730, duration: 0.120s, episode steps: 26, steps per second: 217, episode reward: 10.185, mean reward: 0.392 [0.323, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.035, 10.531], loss: 0.002512, mae: 0.051606, mean_q: -0.354411
 43973/100000: episode: 731, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 4.729, mean reward: 0.338 [0.283, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.075, 10.408], loss: 0.002442, mae: 0.053073, mean_q: -0.280902
 44005/100000: episode: 732, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 9.177, mean reward: 0.287 [0.102, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.109, 10.423], loss: 0.002109, mae: 0.047108, mean_q: -0.307955
 44019/100000: episode: 733, duration: 0.067s, episode steps: 14, steps per second: 208, episode reward: 6.612, mean reward: 0.472 [0.305, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.553], loss: 0.002239, mae: 0.047610, mean_q: -0.362841
 44051/100000: episode: 734, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 14.885, mean reward: 0.465 [0.386, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.778, 10.564], loss: 0.002491, mae: 0.052656, mean_q: -0.321920
 44070/100000: episode: 735, duration: 0.106s, episode steps: 19, steps per second: 178, episode reward: 6.433, mean reward: 0.339 [0.233, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.082, 10.348], loss: 0.002380, mae: 0.049117, mean_q: -0.251148
 44101/100000: episode: 736, duration: 0.155s, episode steps: 31, steps per second: 200, episode reward: 13.201, mean reward: 0.426 [0.286, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.134, 10.705], loss: 0.002773, mae: 0.052805, mean_q: -0.210853
 44115/100000: episode: 737, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 5.415, mean reward: 0.387 [0.263, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.713, 10.536], loss: 0.003180, mae: 0.060362, mean_q: -0.300618
 44135/100000: episode: 738, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 8.252, mean reward: 0.413 [0.274, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.624, 10.100], loss: 0.002884, mae: 0.055827, mean_q: -0.265672
 44149/100000: episode: 739, duration: 0.066s, episode steps: 14, steps per second: 211, episode reward: 4.019, mean reward: 0.287 [0.242, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.218, 10.297], loss: 0.004969, mae: 0.067519, mean_q: -0.286704
 44163/100000: episode: 740, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 5.584, mean reward: 0.399 [0.323, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.484], loss: 0.004740, mae: 0.075019, mean_q: -0.303101
 44174/100000: episode: 741, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 3.174, mean reward: 0.289 [0.216, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.378], loss: 0.003276, mae: 0.063367, mean_q: -0.246984
 44206/100000: episode: 742, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 14.077, mean reward: 0.440 [0.286, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.194, 10.446], loss: 0.002791, mae: 0.053526, mean_q: -0.285887
 44238/100000: episode: 743, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 8.920, mean reward: 0.279 [0.182, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-1.915, 10.281], loss: 0.003091, mae: 0.058895, mean_q: -0.303470
 44250/100000: episode: 744, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 4.501, mean reward: 0.375 [0.266, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.214, 10.100], loss: 0.003102, mae: 0.060015, mean_q: -0.255838
 44269/100000: episode: 745, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 7.275, mean reward: 0.383 [0.296, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.431], loss: 0.002545, mae: 0.054403, mean_q: -0.209428
 44288/100000: episode: 746, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 5.190, mean reward: 0.273 [0.073, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-1.010, 10.207], loss: 0.002720, mae: 0.052428, mean_q: -0.211331
 44319/100000: episode: 747, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 9.644, mean reward: 0.311 [0.170, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.785, 10.386], loss: 0.002340, mae: 0.048577, mean_q: -0.263140
 44351/100000: episode: 748, duration: 0.181s, episode steps: 32, steps per second: 177, episode reward: 12.095, mean reward: 0.378 [0.282, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.320, 10.500], loss: 0.002505, mae: 0.049947, mean_q: -0.231642
 44365/100000: episode: 749, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 3.840, mean reward: 0.274 [0.184, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-1.422, 10.369], loss: 0.002887, mae: 0.053406, mean_q: -0.220359
 44397/100000: episode: 750, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 15.754, mean reward: 0.492 [0.346, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.831, 10.741], loss: 0.002401, mae: 0.050155, mean_q: -0.220962
 44423/100000: episode: 751, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 8.806, mean reward: 0.339 [0.252, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.507], loss: 0.002748, mae: 0.053512, mean_q: -0.243297
 44449/100000: episode: 752, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 11.595, mean reward: 0.446 [0.256, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.277, 10.100], loss: 0.002695, mae: 0.052967, mean_q: -0.193273
 44475/100000: episode: 753, duration: 0.123s, episode steps: 26, steps per second: 212, episode reward: 9.561, mean reward: 0.368 [0.268, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.452], loss: 0.002821, mae: 0.055048, mean_q: -0.190740
 44495/100000: episode: 754, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 6.050, mean reward: 0.303 [0.196, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.179, 10.100], loss: 0.002788, mae: 0.054421, mean_q: -0.210425
 44521/100000: episode: 755, duration: 0.123s, episode steps: 26, steps per second: 211, episode reward: 8.921, mean reward: 0.343 [0.267, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.215, 10.437], loss: 0.003050, mae: 0.056173, mean_q: -0.212929
 44541/100000: episode: 756, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 5.333, mean reward: 0.267 [0.190, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.675, 10.100], loss: 0.002839, mae: 0.056795, mean_q: -0.171743
 44555/100000: episode: 757, duration: 0.067s, episode steps: 14, steps per second: 209, episode reward: 6.028, mean reward: 0.431 [0.326, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.178, 10.525], loss: 0.002690, mae: 0.053954, mean_q: -0.182184
 44587/100000: episode: 758, duration: 0.157s, episode steps: 32, steps per second: 204, episode reward: 11.573, mean reward: 0.362 [0.247, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.035, 10.464], loss: 0.002900, mae: 0.054835, mean_q: -0.155578
 44606/100000: episode: 759, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 5.821, mean reward: 0.306 [0.248, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.861, 10.422], loss: 0.003493, mae: 0.060580, mean_q: -0.139915
 44625/100000: episode: 760, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 6.309, mean reward: 0.332 [0.213, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-2.243, 10.340], loss: 0.003925, mae: 0.065559, mean_q: -0.146541
 44644/100000: episode: 761, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 5.822, mean reward: 0.306 [0.092, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.295, 10.381], loss: 0.003480, mae: 0.061517, mean_q: -0.257531
 44658/100000: episode: 762, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 6.585, mean reward: 0.470 [0.396, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.523], loss: 0.002613, mae: 0.053059, mean_q: -0.186149
 44690/100000: episode: 763, duration: 0.167s, episode steps: 32, steps per second: 191, episode reward: 11.126, mean reward: 0.348 [0.224, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.241, 10.526], loss: 0.002750, mae: 0.051993, mean_q: -0.205424
 44701/100000: episode: 764, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 4.416, mean reward: 0.401 [0.354, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.576], loss: 0.002913, mae: 0.054449, mean_q: -0.138388
 44715/100000: episode: 765, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 5.390, mean reward: 0.385 [0.320, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.388, 10.481], loss: 0.003522, mae: 0.061167, mean_q: -0.160284
 44729/100000: episode: 766, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 4.770, mean reward: 0.341 [0.243, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.402], loss: 0.003284, mae: 0.059656, mean_q: -0.119171
 44740/100000: episode: 767, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 3.819, mean reward: 0.347 [0.237, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.377], loss: 0.003442, mae: 0.057444, mean_q: -0.110713
 44760/100000: episode: 768, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 6.739, mean reward: 0.337 [0.200, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.314, 10.100], loss: 0.002379, mae: 0.050028, mean_q: -0.210436
 44780/100000: episode: 769, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 5.516, mean reward: 0.276 [0.185, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.272, 10.100], loss: 0.002436, mae: 0.049470, mean_q: -0.129881
 44806/100000: episode: 770, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 7.870, mean reward: 0.303 [0.228, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.330, 10.383], loss: 0.003162, mae: 0.058148, mean_q: -0.154295
 44838/100000: episode: 771, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 13.975, mean reward: 0.437 [0.300, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.035, 10.585], loss: 0.002626, mae: 0.052503, mean_q: -0.138482
 44864/100000: episode: 772, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 7.262, mean reward: 0.279 [0.076, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.431, 10.100], loss: 0.002563, mae: 0.051794, mean_q: -0.089909
 44890/100000: episode: 773, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 9.499, mean reward: 0.365 [0.222, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.254, 10.100], loss: 0.002389, mae: 0.050667, mean_q: -0.146570
 44922/100000: episode: 774, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 10.845, mean reward: 0.339 [0.250, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-1.220, 10.358], loss: 0.002461, mae: 0.051165, mean_q: -0.073483
 44934/100000: episode: 775, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 3.618, mean reward: 0.302 [0.259, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.232, 10.100], loss: 0.003209, mae: 0.058010, mean_q: -0.075849
 44960/100000: episode: 776, duration: 0.123s, episode steps: 26, steps per second: 211, episode reward: 9.428, mean reward: 0.363 [0.242, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.044, 10.407], loss: 0.002816, mae: 0.053729, mean_q: -0.062599
 44974/100000: episode: 777, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 4.459, mean reward: 0.319 [0.216, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.445], loss: 0.002367, mae: 0.049602, mean_q: -0.114804
 44993/100000: episode: 778, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 6.466, mean reward: 0.340 [0.239, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.332], loss: 0.002589, mae: 0.052396, mean_q: -0.088053
 45004/100000: episode: 779, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 4.293, mean reward: 0.390 [0.295, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.565], loss: 0.002540, mae: 0.050701, mean_q: -0.121258
 45015/100000: episode: 780, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 4.408, mean reward: 0.401 [0.320, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.445, 10.482], loss: 0.003945, mae: 0.059669, mean_q: -0.106792
 45041/100000: episode: 781, duration: 0.159s, episode steps: 26, steps per second: 164, episode reward: 9.310, mean reward: 0.358 [0.253, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.035, 10.464], loss: 0.002829, mae: 0.058076, mean_q: -0.064681
 45061/100000: episode: 782, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 6.201, mean reward: 0.310 [0.212, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.183, 10.100], loss: 0.002993, mae: 0.054855, mean_q: -0.119712
 45087/100000: episode: 783, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 6.722, mean reward: 0.259 [0.161, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.319], loss: 0.002686, mae: 0.053218, mean_q: -0.062242
 45101/100000: episode: 784, duration: 0.068s, episode steps: 14, steps per second: 207, episode reward: 4.267, mean reward: 0.305 [0.229, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.142, 10.389], loss: 0.002901, mae: 0.056950, mean_q: -0.097778
 45127/100000: episode: 785, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 10.219, mean reward: 0.393 [0.217, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.501], loss: 0.005223, mae: 0.069060, mean_q: -0.090948
 45138/100000: episode: 786, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 4.723, mean reward: 0.429 [0.325, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.757, 10.608], loss: 0.013427, mae: 0.091984, mean_q: -0.191826
 45164/100000: episode: 787, duration: 0.132s, episode steps: 26, steps per second: 196, episode reward: 12.006, mean reward: 0.462 [0.353, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.529, 10.514], loss: 0.005262, mae: 0.071778, mean_q: -0.044736
 45190/100000: episode: 788, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 8.932, mean reward: 0.344 [0.192, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.542, 10.100], loss: 0.003449, mae: 0.059275, mean_q: -0.039382
 45210/100000: episode: 789, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 5.156, mean reward: 0.258 [0.117, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.098, 10.100], loss: 0.004050, mae: 0.065344, mean_q: -0.019392
 45221/100000: episode: 790, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 3.671, mean reward: 0.334 [0.260, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.460], loss: 0.002967, mae: 0.056626, mean_q: -0.008776
 45235/100000: episode: 791, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 5.047, mean reward: 0.360 [0.278, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-1.110, 10.435], loss: 0.003205, mae: 0.056441, mean_q: -0.089374
 45254/100000: episode: 792, duration: 0.098s, episode steps: 19, steps per second: 195, episode reward: 5.759, mean reward: 0.303 [0.231, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.473, 10.316], loss: 0.003089, mae: 0.056043, mean_q: -0.086659
 45265/100000: episode: 793, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 3.648, mean reward: 0.332 [0.292, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.345], loss: 0.004298, mae: 0.066466, mean_q: 0.042246
 45291/100000: episode: 794, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 8.087, mean reward: 0.311 [0.224, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.464, 10.100], loss: 0.003760, mae: 0.063907, mean_q: -0.010088
 45302/100000: episode: 795, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 3.934, mean reward: 0.358 [0.307, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.372, 10.456], loss: 0.003217, mae: 0.055637, mean_q: -0.048852
 45316/100000: episode: 796, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 5.527, mean reward: 0.395 [0.304, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.452], loss: 0.002712, mae: 0.052818, mean_q: -0.084948
 45327/100000: episode: 797, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 3.765, mean reward: 0.342 [0.297, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.465], loss: 0.003163, mae: 0.058572, mean_q: -0.085638
 45347/100000: episode: 798, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 5.188, mean reward: 0.259 [0.113, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.912, 10.100], loss: 0.003082, mae: 0.056316, mean_q: -0.063403
 45358/100000: episode: 799, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 3.441, mean reward: 0.313 [0.266, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.379], loss: 0.003411, mae: 0.061283, mean_q: 0.004084
 45384/100000: episode: 800, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 8.203, mean reward: 0.316 [0.121, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.668, 10.251], loss: 0.004287, mae: 0.066548, mean_q: 0.059498
 45396/100000: episode: 801, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 4.465, mean reward: 0.372 [0.270, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.623, 10.100], loss: 0.004539, mae: 0.067783, mean_q: 0.015444
 45416/100000: episode: 802, duration: 0.094s, episode steps: 20, steps per second: 213, episode reward: 5.109, mean reward: 0.255 [0.121, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.212, 10.100], loss: 0.003769, mae: 0.064023, mean_q: 0.054311
 45427/100000: episode: 803, duration: 0.071s, episode steps: 11, steps per second: 156, episode reward: 4.218, mean reward: 0.383 [0.321, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.429], loss: 0.005066, mae: 0.067984, mean_q: -0.035945
 45459/100000: episode: 804, duration: 0.154s, episode steps: 32, steps per second: 208, episode reward: 13.306, mean reward: 0.416 [0.311, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.572, 10.499], loss: 0.003667, mae: 0.060685, mean_q: -0.014504
 45485/100000: episode: 805, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 6.316, mean reward: 0.243 [0.138, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.281], loss: 0.003543, mae: 0.062018, mean_q: 0.022734
 45499/100000: episode: 806, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 5.236, mean reward: 0.374 [0.283, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.433], loss: 0.003822, mae: 0.064245, mean_q: 0.024620
 45510/100000: episode: 807, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 3.644, mean reward: 0.331 [0.215, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.499], loss: 0.002839, mae: 0.055487, mean_q: -0.020296
 45524/100000: episode: 808, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 4.309, mean reward: 0.308 [0.232, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.339], loss: 0.002998, mae: 0.058142, mean_q: 0.058696
 45535/100000: episode: 809, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 4.597, mean reward: 0.418 [0.361, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-1.389, 10.472], loss: 0.003155, mae: 0.056482, mean_q: 0.051854
 45555/100000: episode: 810, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 6.294, mean reward: 0.315 [0.246, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.191, 10.100], loss: 0.003872, mae: 0.063816, mean_q: 0.065958
 45569/100000: episode: 811, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 4.802, mean reward: 0.343 [0.257, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.440], loss: 0.003589, mae: 0.065125, mean_q: 0.008891
 45581/100000: episode: 812, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 3.686, mean reward: 0.307 [0.256, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.303, 10.100], loss: 0.003681, mae: 0.061130, mean_q: 0.014007
 45593/100000: episode: 813, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 4.110, mean reward: 0.342 [0.287, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.598, 10.100], loss: 0.002845, mae: 0.054626, mean_q: 0.019818
 45607/100000: episode: 814, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 4.976, mean reward: 0.355 [0.271, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.909, 10.449], loss: 0.004432, mae: 0.066847, mean_q: 0.015701
 45627/100000: episode: 815, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 5.816, mean reward: 0.291 [0.222, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.194, 10.100], loss: 0.004304, mae: 0.063362, mean_q: -0.045513
 45646/100000: episode: 816, duration: 0.092s, episode steps: 19, steps per second: 206, episode reward: 6.783, mean reward: 0.357 [0.233, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.302, 10.430], loss: 0.003975, mae: 0.064603, mean_q: 0.042117
 45657/100000: episode: 817, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 3.382, mean reward: 0.307 [0.227, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.411], loss: 0.004322, mae: 0.065677, mean_q: 0.024294
[Info] 200-TH LEVEL FOUND: 0.860201358795166, Considering 10/90 traces
 45683/100000: episode: 818, duration: 4.006s, episode steps: 26, steps per second: 6, episode reward: 10.416, mean reward: 0.401 [0.288, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.333, 10.100], loss: 0.003395, mae: 0.060973, mean_q: -0.003606
 45696/100000: episode: 819, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 6.017, mean reward: 0.463 [0.329, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.500], loss: 0.002542, mae: 0.052255, mean_q: 0.011323
 45714/100000: episode: 820, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 7.554, mean reward: 0.420 [0.184, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-1.101, 10.327], loss: 0.002681, mae: 0.052720, mean_q: -0.004949
 45724/100000: episode: 821, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 4.266, mean reward: 0.427 [0.382, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.285, 10.556], loss: 0.002844, mae: 0.054038, mean_q: 0.086844
 45742/100000: episode: 822, duration: 0.084s, episode steps: 18, steps per second: 213, episode reward: 8.083, mean reward: 0.449 [0.387, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.332, 10.562], loss: 0.003206, mae: 0.058464, mean_q: 0.030538
 45764/100000: episode: 823, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 10.604, mean reward: 0.482 [0.360, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.301, 10.446], loss: 0.003506, mae: 0.061452, mean_q: 0.049640
 45770/100000: episode: 824, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 3.357, mean reward: 0.559 [0.526, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.552], loss: 0.003298, mae: 0.059621, mean_q: 0.114415
 45776/100000: episode: 825, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 3.287, mean reward: 0.548 [0.526, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.557], loss: 0.003324, mae: 0.059063, mean_q: 0.040992
 45794/100000: episode: 826, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 6.478, mean reward: 0.360 [0.265, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.123, 10.432], loss: 0.003303, mae: 0.060735, mean_q: 0.056482
 45804/100000: episode: 827, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 5.215, mean reward: 0.522 [0.414, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.691], loss: 0.002934, mae: 0.056963, mean_q: -0.010129
 45824/100000: episode: 828, duration: 0.117s, episode steps: 20, steps per second: 170, episode reward: 8.189, mean reward: 0.409 [0.246, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.344], loss: 0.003218, mae: 0.059967, mean_q: 0.115244
 45830/100000: episode: 829, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 3.251, mean reward: 0.542 [0.515, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.413, 10.552], loss: 0.003507, mae: 0.064304, mean_q: 0.082982
 45840/100000: episode: 830, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 5.436, mean reward: 0.544 [0.459, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.549, 10.568], loss: 0.002814, mae: 0.052836, mean_q: 0.054014
 45859/100000: episode: 831, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 10.255, mean reward: 0.540 [0.395, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.434, 10.642], loss: 0.003434, mae: 0.062336, mean_q: 0.139095
 45872/100000: episode: 832, duration: 0.069s, episode steps: 13, steps per second: 190, episode reward: 5.614, mean reward: 0.432 [0.382, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.536], loss: 0.003053, mae: 0.056368, mean_q: 0.006604
 45892/100000: episode: 833, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 10.133, mean reward: 0.507 [0.413, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.536, 10.100], loss: 0.003200, mae: 0.056527, mean_q: 0.143922
 45911/100000: episode: 834, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 9.308, mean reward: 0.490 [0.358, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.500, 10.512], loss: 0.003252, mae: 0.060393, mean_q: 0.075106
 45933/100000: episode: 835, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 6.735, mean reward: 0.306 [0.149, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.059, 10.303], loss: 0.003245, mae: 0.059660, mean_q: 0.111742
 45943/100000: episode: 836, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 4.570, mean reward: 0.457 [0.378, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.523], loss: 0.003316, mae: 0.058989, mean_q: 0.103947
 45957/100000: episode: 837, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 6.551, mean reward: 0.468 [0.343, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.270, 10.474], loss: 0.002981, mae: 0.055254, mean_q: 0.090510
 45976/100000: episode: 838, duration: 0.089s, episode steps: 19, steps per second: 214, episode reward: 8.286, mean reward: 0.436 [0.247, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.287], loss: 0.003023, mae: 0.058291, mean_q: 0.077671
 45989/100000: episode: 839, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 6.118, mean reward: 0.471 [0.380, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.514], loss: 0.003961, mae: 0.065517, mean_q: 0.120977
 46009/100000: episode: 840, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 10.238, mean reward: 0.512 [0.431, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.356, 10.100], loss: 0.002637, mae: 0.054269, mean_q: 0.164527
 46019/100000: episode: 841, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 4.552, mean reward: 0.455 [0.416, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.587], loss: 0.002601, mae: 0.054392, mean_q: 0.163949
 46033/100000: episode: 842, duration: 0.085s, episode steps: 14, steps per second: 166, episode reward: 6.683, mean reward: 0.477 [0.421, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.213, 10.582], loss: 0.003151, mae: 0.059536, mean_q: 0.103260
 46052/100000: episode: 843, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 7.931, mean reward: 0.417 [0.360, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.503], loss: 0.003040, mae: 0.056436, mean_q: 0.166235
 46066/100000: episode: 844, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 6.661, mean reward: 0.476 [0.433, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.058, 10.543], loss: 0.003721, mae: 0.064791, mean_q: 0.137683
 46076/100000: episode: 845, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 5.107, mean reward: 0.511 [0.459, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.814, 10.528], loss: 0.004479, mae: 0.071769, mean_q: 0.160425
 46094/100000: episode: 846, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 6.140, mean reward: 0.341 [0.232, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-1.029, 10.332], loss: 0.002797, mae: 0.055937, mean_q: 0.096674
 46114/100000: episode: 847, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 9.755, mean reward: 0.488 [0.398, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.373, 10.474], loss: 0.003567, mae: 0.061565, mean_q: 0.189353
 46124/100000: episode: 848, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 4.202, mean reward: 0.420 [0.392, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.521], loss: 0.003131, mae: 0.057312, mean_q: 0.195145
 46137/100000: episode: 849, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 7.591, mean reward: 0.584 [0.533, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.053, 10.611], loss: 0.002951, mae: 0.057405, mean_q: 0.195739
 46159/100000: episode: 850, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 7.548, mean reward: 0.343 [0.260, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.354, 10.292], loss: 0.003391, mae: 0.060608, mean_q: 0.152697
 46165/100000: episode: 851, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 3.039, mean reward: 0.507 [0.487, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.617], loss: 0.003455, mae: 0.058146, mean_q: 0.237592
 46183/100000: episode: 852, duration: 0.084s, episode steps: 18, steps per second: 214, episode reward: 7.313, mean reward: 0.406 [0.267, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.356, 10.493], loss: 0.003454, mae: 0.062281, mean_q: 0.209427
 46202/100000: episode: 853, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 7.217, mean reward: 0.380 [0.282, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.064, 10.436], loss: 0.004646, mae: 0.070588, mean_q: 0.177141
 46215/100000: episode: 854, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 4.427, mean reward: 0.341 [0.235, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.245, 10.340], loss: 0.004411, mae: 0.070967, mean_q: 0.113434
 46237/100000: episode: 855, duration: 0.117s, episode steps: 22, steps per second: 187, episode reward: 9.038, mean reward: 0.411 [0.318, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.822, 10.443], loss: 0.003476, mae: 0.061851, mean_q: 0.210027
 46257/100000: episode: 856, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 8.695, mean reward: 0.435 [0.325, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.066, 10.427], loss: 0.003121, mae: 0.058913, mean_q: 0.154891
 46279/100000: episode: 857, duration: 0.105s, episode steps: 22, steps per second: 209, episode reward: 8.739, mean reward: 0.397 [0.307, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.197, 10.434], loss: 0.003128, mae: 0.059195, mean_q: 0.126171
 46289/100000: episode: 858, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 4.260, mean reward: 0.426 [0.336, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.490], loss: 0.002688, mae: 0.053749, mean_q: 0.172366
 46311/100000: episode: 859, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 9.120, mean reward: 0.415 [0.353, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.121, 10.496], loss: 0.002735, mae: 0.055662, mean_q: 0.256717
 46329/100000: episode: 860, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 8.206, mean reward: 0.456 [0.376, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.102, 10.479], loss: 0.004099, mae: 0.067661, mean_q: 0.189123
 46351/100000: episode: 861, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 9.142, mean reward: 0.416 [0.338, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.452], loss: 0.003862, mae: 0.065301, mean_q: 0.201666
 46365/100000: episode: 862, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 6.794, mean reward: 0.485 [0.448, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.609], loss: 0.005227, mae: 0.076240, mean_q: 0.215917
 46375/100000: episode: 863, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 3.788, mean reward: 0.379 [0.255, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.450], loss: 0.003066, mae: 0.059476, mean_q: 0.190108
 46393/100000: episode: 864, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 8.512, mean reward: 0.473 [0.336, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.933, 10.502], loss: 0.003283, mae: 0.061169, mean_q: 0.248211
 46403/100000: episode: 865, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 4.225, mean reward: 0.423 [0.395, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.522], loss: 0.003727, mae: 0.063543, mean_q: 0.224453
 46413/100000: episode: 866, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 4.576, mean reward: 0.458 [0.419, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.069, 10.510], loss: 0.003704, mae: 0.063202, mean_q: 0.260019
 46431/100000: episode: 867, duration: 0.088s, episode steps: 18, steps per second: 205, episode reward: 7.003, mean reward: 0.389 [0.259, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.434], loss: 0.003029, mae: 0.057274, mean_q: 0.200236
 46450/100000: episode: 868, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 8.107, mean reward: 0.427 [0.355, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.549, 10.419], loss: 0.002739, mae: 0.055499, mean_q: 0.157580
 46470/100000: episode: 869, duration: 0.113s, episode steps: 20, steps per second: 178, episode reward: 8.227, mean reward: 0.411 [0.300, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.326, 10.100], loss: 0.003271, mae: 0.059149, mean_q: 0.217160
 46480/100000: episode: 870, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 5.441, mean reward: 0.544 [0.467, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-1.008, 10.600], loss: 0.002656, mae: 0.051754, mean_q: 0.192026
 46494/100000: episode: 871, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 5.795, mean reward: 0.414 [0.378, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.486], loss: 0.003150, mae: 0.055707, mean_q: 0.155705
 46504/100000: episode: 872, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 4.894, mean reward: 0.489 [0.442, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.185, 10.595], loss: 0.002999, mae: 0.060024, mean_q: 0.221526
 46510/100000: episode: 873, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 3.384, mean reward: 0.564 [0.512, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.036, 10.596], loss: 0.003027, mae: 0.058745, mean_q: 0.145957
 46532/100000: episode: 874, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 9.889, mean reward: 0.449 [0.388, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.413, 10.447], loss: 0.003387, mae: 0.060531, mean_q: 0.193570
[Info] FALSIFICATION!
 46536/100000: episode: 875, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 11.644, mean reward: 2.911 [0.493, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.014, 10.736], loss: 0.003194, mae: 0.061912, mean_q: 0.258928
 46636/100000: episode: 876, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.678, mean reward: -0.147 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.431, 10.137], loss: 0.016317, mae: 0.066843, mean_q: 0.258566
 46736/100000: episode: 877, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -13.511, mean reward: -0.135 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.049, 10.262], loss: 0.003323, mae: 0.060949, mean_q: 0.256193
 46836/100000: episode: 878, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -12.544, mean reward: -0.125 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.737, 10.104], loss: 0.016985, mae: 0.071752, mean_q: 0.255492
 46936/100000: episode: 879, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -16.835, mean reward: -0.168 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.864, 10.098], loss: 0.042273, mae: 0.080455, mean_q: 0.297429
 47036/100000: episode: 880, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.964, mean reward: -0.160 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.595, 10.098], loss: 0.016924, mae: 0.073084, mean_q: 0.236853
 47136/100000: episode: 881, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -12.922, mean reward: -0.129 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.949, 10.098], loss: 0.002901, mae: 0.056084, mean_q: 0.237007
 47236/100000: episode: 882, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.819, mean reward: -0.188 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.613, 10.151], loss: 0.002991, mae: 0.056339, mean_q: 0.261118
 47336/100000: episode: 883, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -20.142, mean reward: -0.201 [-1.000, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.446, 10.239], loss: 0.016233, mae: 0.065682, mean_q: 0.239248
 47436/100000: episode: 884, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -15.500, mean reward: -0.155 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.390, 10.363], loss: 0.002944, mae: 0.056573, mean_q: 0.244608
 47536/100000: episode: 885, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -18.575, mean reward: -0.186 [-1.000, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.018, 10.211], loss: 0.002898, mae: 0.056328, mean_q: 0.273163
 47636/100000: episode: 886, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -15.264, mean reward: -0.153 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.390, 10.241], loss: 0.029302, mae: 0.073489, mean_q: 0.242493
 47736/100000: episode: 887, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: -15.210, mean reward: -0.152 [-1.000, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.659, 10.133], loss: 0.003268, mae: 0.059943, mean_q: 0.250351
 47836/100000: episode: 888, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -16.556, mean reward: -0.166 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.463, 10.289], loss: 0.016386, mae: 0.067689, mean_q: 0.277961
 47936/100000: episode: 889, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -16.739, mean reward: -0.167 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.581, 10.132], loss: 0.002945, mae: 0.056172, mean_q: 0.212521
 48036/100000: episode: 890, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -10.526, mean reward: -0.105 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.724, 10.098], loss: 0.016849, mae: 0.066282, mean_q: 0.290399
 48136/100000: episode: 891, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -19.500, mean reward: -0.195 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.022, 10.098], loss: 0.003496, mae: 0.061799, mean_q: 0.221550
 48236/100000: episode: 892, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -20.540, mean reward: -0.205 [-1.000, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.524, 10.184], loss: 0.003213, mae: 0.059671, mean_q: 0.271332
 48336/100000: episode: 893, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -14.908, mean reward: -0.149 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.428, 10.098], loss: 0.002892, mae: 0.055616, mean_q: 0.271403
 48436/100000: episode: 894, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -19.136, mean reward: -0.191 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.721, 10.107], loss: 0.016290, mae: 0.066051, mean_q: 0.254211
 48536/100000: episode: 895, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.320, mean reward: -0.153 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.610, 10.229], loss: 0.030249, mae: 0.079674, mean_q: 0.290209
 48636/100000: episode: 896, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.208, mean reward: -0.192 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.904, 10.098], loss: 0.002885, mae: 0.055880, mean_q: 0.253246
 48736/100000: episode: 897, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: -16.058, mean reward: -0.161 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.820, 10.127], loss: 0.002700, mae: 0.054092, mean_q: 0.246421
 48836/100000: episode: 898, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -18.121, mean reward: -0.181 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.104, 10.098], loss: 0.002929, mae: 0.056174, mean_q: 0.232469
 48936/100000: episode: 899, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -19.884, mean reward: -0.199 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.757, 10.209], loss: 0.002954, mae: 0.056863, mean_q: 0.216833
 49036/100000: episode: 900, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -16.602, mean reward: -0.166 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.904, 10.175], loss: 0.003001, mae: 0.056411, mean_q: 0.197114
 49136/100000: episode: 901, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.465, mean reward: -0.175 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.434, 10.109], loss: 0.016497, mae: 0.069138, mean_q: 0.184001
 49236/100000: episode: 902, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.202, mean reward: -0.182 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.842, 10.242], loss: 0.002739, mae: 0.054460, mean_q: 0.171189
 49336/100000: episode: 903, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.663, mean reward: -0.167 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.852, 10.188], loss: 0.002822, mae: 0.054845, mean_q: 0.118685
 49436/100000: episode: 904, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -16.332, mean reward: -0.163 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.258, 10.098], loss: 0.002978, mae: 0.056533, mean_q: 0.121442
 49536/100000: episode: 905, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -18.322, mean reward: -0.183 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.590, 10.098], loss: 0.016450, mae: 0.067930, mean_q: 0.101118
 49636/100000: episode: 906, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.888, mean reward: -0.179 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.651, 10.182], loss: 0.002881, mae: 0.055422, mean_q: 0.084368
 49736/100000: episode: 907, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -15.211, mean reward: -0.152 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.274, 10.349], loss: 0.002740, mae: 0.054158, mean_q: 0.041119
 49836/100000: episode: 908, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -12.020, mean reward: -0.120 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.559, 10.098], loss: 0.002897, mae: 0.055109, mean_q: 0.034529
 49936/100000: episode: 909, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.325, mean reward: -0.183 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.989, 10.106], loss: 0.015990, mae: 0.062849, mean_q: -0.013829
 50036/100000: episode: 910, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -14.405, mean reward: -0.144 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.898, 10.130], loss: 0.016015, mae: 0.063946, mean_q: -0.011504
 50136/100000: episode: 911, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -19.585, mean reward: -0.196 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.019, 10.217], loss: 0.002893, mae: 0.056075, mean_q: -0.009122
 50236/100000: episode: 912, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -16.810, mean reward: -0.168 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.602, 10.289], loss: 0.002841, mae: 0.054318, mean_q: -0.060366
 50336/100000: episode: 913, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.169, mean reward: -0.182 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.507, 10.098], loss: 0.003578, mae: 0.060836, mean_q: -0.054243
 50436/100000: episode: 914, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -18.354, mean reward: -0.184 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.615, 10.098], loss: 0.003068, mae: 0.056868, mean_q: -0.080174
 50536/100000: episode: 915, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.000, mean reward: -0.190 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.795, 10.098], loss: 0.016422, mae: 0.065576, mean_q: -0.105534
 50636/100000: episode: 916, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -12.828, mean reward: -0.128 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.385, 10.098], loss: 0.015640, mae: 0.061744, mean_q: -0.091551
 50736/100000: episode: 917, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.796, mean reward: -0.178 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.261, 10.265], loss: 0.030106, mae: 0.078662, mean_q: -0.117568
 50836/100000: episode: 918, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -6.530, mean reward: -0.065 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.409, 10.497], loss: 0.028477, mae: 0.069884, mean_q: -0.140087
 50936/100000: episode: 919, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -15.187, mean reward: -0.152 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.625, 10.350], loss: 0.042693, mae: 0.086901, mean_q: -0.149368
 51036/100000: episode: 920, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -17.758, mean reward: -0.178 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.064, 10.098], loss: 0.003241, mae: 0.057744, mean_q: -0.194517
 51136/100000: episode: 921, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -16.812, mean reward: -0.168 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.786, 10.188], loss: 0.039896, mae: 0.066890, mean_q: -0.224922
 51236/100000: episode: 922, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -19.288, mean reward: -0.193 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.347, 10.098], loss: 0.015776, mae: 0.063552, mean_q: -0.274382
 51336/100000: episode: 923, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -19.246, mean reward: -0.192 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.651, 10.098], loss: 0.002619, mae: 0.050426, mean_q: -0.279196
 51436/100000: episode: 924, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -12.556, mean reward: -0.126 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.524, 10.353], loss: 0.002720, mae: 0.051884, mean_q: -0.314598
 51536/100000: episode: 925, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.783, mean reward: -0.168 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.040, 10.098], loss: 0.002518, mae: 0.049618, mean_q: -0.298585
 51636/100000: episode: 926, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -17.082, mean reward: -0.171 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.481, 10.127], loss: 0.002468, mae: 0.049209, mean_q: -0.345934
 51736/100000: episode: 927, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -19.183, mean reward: -0.192 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.775, 10.263], loss: 0.002884, mae: 0.054008, mean_q: -0.301503
 51836/100000: episode: 928, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.362, mean reward: -0.184 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.838, 10.130], loss: 0.002610, mae: 0.051427, mean_q: -0.306033
 51936/100000: episode: 929, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.362, mean reward: -0.164 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.902, 10.098], loss: 0.002651, mae: 0.052007, mean_q: -0.332046
 52036/100000: episode: 930, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.086, mean reward: -0.181 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.571, 10.098], loss: 0.003683, mae: 0.060278, mean_q: -0.319717
 52136/100000: episode: 931, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -14.729, mean reward: -0.147 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.590, 10.098], loss: 0.002781, mae: 0.052548, mean_q: -0.331712
 52236/100000: episode: 932, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -13.778, mean reward: -0.138 [-1.000, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.051, 10.342], loss: 0.002672, mae: 0.051847, mean_q: -0.301678
 52336/100000: episode: 933, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -16.904, mean reward: -0.169 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.922, 10.098], loss: 0.003049, mae: 0.055403, mean_q: -0.302616
 52436/100000: episode: 934, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -20.244, mean reward: -0.202 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.322, 10.099], loss: 0.002761, mae: 0.052041, mean_q: -0.302640
 52536/100000: episode: 935, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -15.855, mean reward: -0.159 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.774, 10.263], loss: 0.002698, mae: 0.052660, mean_q: -0.312777
 52636/100000: episode: 936, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -18.276, mean reward: -0.183 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.018, 10.175], loss: 0.002833, mae: 0.053660, mean_q: -0.308321
 52736/100000: episode: 937, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -18.247, mean reward: -0.182 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.769, 10.098], loss: 0.002658, mae: 0.050197, mean_q: -0.322094
 52836/100000: episode: 938, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -18.530, mean reward: -0.185 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.964, 10.121], loss: 0.002924, mae: 0.054221, mean_q: -0.323859
 52936/100000: episode: 939, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -13.901, mean reward: -0.139 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.313, 10.370], loss: 0.002916, mae: 0.054730, mean_q: -0.308143
 53036/100000: episode: 940, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -14.328, mean reward: -0.143 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.789, 10.098], loss: 0.002894, mae: 0.053853, mean_q: -0.297154
 53136/100000: episode: 941, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -18.050, mean reward: -0.181 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.165, 10.357], loss: 0.002768, mae: 0.052256, mean_q: -0.294635
 53236/100000: episode: 942, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -12.529, mean reward: -0.125 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.747, 10.191], loss: 0.002685, mae: 0.050766, mean_q: -0.327136
 53336/100000: episode: 943, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -14.270, mean reward: -0.143 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.668, 10.235], loss: 0.002719, mae: 0.051546, mean_q: -0.330885
 53436/100000: episode: 944, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.477, mean reward: -0.175 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.166, 10.098], loss: 0.002871, mae: 0.052663, mean_q: -0.314093
 53536/100000: episode: 945, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -18.834, mean reward: -0.188 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.416, 10.098], loss: 0.002641, mae: 0.051689, mean_q: -0.336824
 53636/100000: episode: 946, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -13.702, mean reward: -0.137 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.782, 10.259], loss: 0.002843, mae: 0.053878, mean_q: -0.310423
 53736/100000: episode: 947, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -15.889, mean reward: -0.159 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.936, 10.098], loss: 0.003013, mae: 0.056365, mean_q: -0.303710
 53836/100000: episode: 948, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -12.526, mean reward: -0.125 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.388, 10.098], loss: 0.002989, mae: 0.055870, mean_q: -0.304978
 53936/100000: episode: 949, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -12.316, mean reward: -0.123 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.839, 10.098], loss: 0.002859, mae: 0.053844, mean_q: -0.314747
 54036/100000: episode: 950, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.114, mean reward: -0.181 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.785, 10.198], loss: 0.002876, mae: 0.053678, mean_q: -0.307795
 54136/100000: episode: 951, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -18.518, mean reward: -0.185 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.955, 10.263], loss: 0.004250, mae: 0.064758, mean_q: -0.315692
 54236/100000: episode: 952, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -12.605, mean reward: -0.126 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.669, 10.465], loss: 0.002742, mae: 0.053100, mean_q: -0.289973
 54336/100000: episode: 953, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -19.464, mean reward: -0.195 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.476, 10.105], loss: 0.002689, mae: 0.051140, mean_q: -0.320668
 54436/100000: episode: 954, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.113, mean reward: -0.171 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.661, 10.237], loss: 0.002700, mae: 0.051869, mean_q: -0.323707
 54536/100000: episode: 955, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.680, mean reward: -0.187 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.679, 10.223], loss: 0.002921, mae: 0.055385, mean_q: -0.319039
 54636/100000: episode: 956, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.046, mean reward: -0.190 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.749, 10.306], loss: 0.002890, mae: 0.055185, mean_q: -0.290760
 54736/100000: episode: 957, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.478, mean reward: -0.175 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.179, 10.453], loss: 0.002834, mae: 0.053324, mean_q: -0.353781
 54836/100000: episode: 958, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.235, mean reward: -0.182 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.693, 10.098], loss: 0.002848, mae: 0.054029, mean_q: -0.327503
 54936/100000: episode: 959, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -16.849, mean reward: -0.168 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.254, 10.098], loss: 0.002730, mae: 0.052560, mean_q: -0.305283
 55036/100000: episode: 960, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -15.563, mean reward: -0.156 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.575, 10.248], loss: 0.002644, mae: 0.051000, mean_q: -0.353996
 55136/100000: episode: 961, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -15.595, mean reward: -0.156 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.018, 10.311], loss: 0.002689, mae: 0.053059, mean_q: -0.297821
 55236/100000: episode: 962, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -19.250, mean reward: -0.192 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.391, 10.121], loss: 0.002738, mae: 0.052491, mean_q: -0.322958
 55336/100000: episode: 963, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -15.142, mean reward: -0.151 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.067, 10.098], loss: 0.002733, mae: 0.052670, mean_q: -0.314765
 55436/100000: episode: 964, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -13.703, mean reward: -0.137 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.962, 10.098], loss: 0.002704, mae: 0.051975, mean_q: -0.320869
 55536/100000: episode: 965, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.497, mean reward: -0.165 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.759, 10.098], loss: 0.005118, mae: 0.065111, mean_q: -0.299353
 55636/100000: episode: 966, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -16.637, mean reward: -0.166 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.526, 10.098], loss: 0.002734, mae: 0.052597, mean_q: -0.319174
 55736/100000: episode: 967, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -19.022, mean reward: -0.190 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.322, 10.108], loss: 0.002810, mae: 0.054348, mean_q: -0.312732
 55836/100000: episode: 968, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -14.937, mean reward: -0.149 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.660, 10.098], loss: 0.002645, mae: 0.050582, mean_q: -0.337444
 55936/100000: episode: 969, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.003, mean reward: -0.180 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.257, 10.359], loss: 0.002628, mae: 0.050904, mean_q: -0.316618
 56036/100000: episode: 970, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.366, mean reward: -0.174 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.223, 10.098], loss: 0.002449, mae: 0.048741, mean_q: -0.327556
 56136/100000: episode: 971, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -17.925, mean reward: -0.179 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.064, 10.267], loss: 0.002880, mae: 0.053025, mean_q: -0.305906
 56236/100000: episode: 972, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -19.326, mean reward: -0.193 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.436, 10.098], loss: 0.002652, mae: 0.052203, mean_q: -0.303051
 56336/100000: episode: 973, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -11.019, mean reward: -0.110 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.674, 10.098], loss: 0.002672, mae: 0.051275, mean_q: -0.286559
 56436/100000: episode: 974, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.594, mean reward: -0.176 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.021, 10.098], loss: 0.002829, mae: 0.054387, mean_q: -0.304894
[Info] 100-TH LEVEL FOUND: 0.5286807417869568, Considering 10/90 traces
 56536/100000: episode: 975, duration: 4.291s, episode steps: 100, steps per second: 23, episode reward: -20.402, mean reward: -0.204 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.175, 10.182], loss: 0.002970, mae: 0.055740, mean_q: -0.329930
 56577/100000: episode: 976, duration: 0.202s, episode steps: 41, steps per second: 203, episode reward: 8.728, mean reward: 0.213 [0.032, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.763, 10.100], loss: 0.002774, mae: 0.053780, mean_q: -0.347394
 56584/100000: episode: 977, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 3.158, mean reward: 0.451 [0.409, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.393], loss: 0.002826, mae: 0.053683, mean_q: -0.331840
 56622/100000: episode: 978, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 12.975, mean reward: 0.341 [0.116, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.178, 10.100], loss: 0.002848, mae: 0.054095, mean_q: -0.294014
 56649/100000: episode: 979, duration: 0.131s, episode steps: 27, steps per second: 206, episode reward: 8.325, mean reward: 0.308 [0.217, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.291, 10.100], loss: 0.002705, mae: 0.052023, mean_q: -0.319702
 56676/100000: episode: 980, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 8.952, mean reward: 0.332 [0.211, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.281, 10.100], loss: 0.002790, mae: 0.053894, mean_q: -0.272277
 56694/100000: episode: 981, duration: 0.086s, episode steps: 18, steps per second: 210, episode reward: 4.728, mean reward: 0.263 [0.179, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.914, 10.100], loss: 0.002864, mae: 0.053120, mean_q: -0.273641
 56721/100000: episode: 982, duration: 0.131s, episode steps: 27, steps per second: 207, episode reward: 9.635, mean reward: 0.357 [0.276, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.989, 10.100], loss: 0.002841, mae: 0.054400, mean_q: -0.274496
 56739/100000: episode: 983, duration: 0.084s, episode steps: 18, steps per second: 214, episode reward: 3.521, mean reward: 0.196 [0.097, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.304, 10.100], loss: 0.002563, mae: 0.052539, mean_q: -0.259447
 56756/100000: episode: 984, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 5.186, mean reward: 0.305 [0.204, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.429], loss: 0.002323, mae: 0.047832, mean_q: -0.366852
 56767/100000: episode: 985, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 4.204, mean reward: 0.382 [0.268, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.257, 10.100], loss: 0.002263, mae: 0.048959, mean_q: -0.370547
 56808/100000: episode: 986, duration: 0.220s, episode steps: 41, steps per second: 187, episode reward: 10.332, mean reward: 0.252 [0.042, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.389, 10.121], loss: 0.002740, mae: 0.054381, mean_q: -0.272347
 56849/100000: episode: 987, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 11.214, mean reward: 0.274 [0.052, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.495, 10.220], loss: 0.002839, mae: 0.053505, mean_q: -0.267259
 56862/100000: episode: 988, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 4.912, mean reward: 0.378 [0.331, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.073, 10.433], loss: 0.002944, mae: 0.054829, mean_q: -0.208372
 56873/100000: episode: 989, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 4.085, mean reward: 0.371 [0.311, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.180, 10.100], loss: 0.003072, mae: 0.053709, mean_q: -0.313043
 56900/100000: episode: 990, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 6.268, mean reward: 0.232 [0.064, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.777, 10.100], loss: 0.002824, mae: 0.053356, mean_q: -0.231793
 56918/100000: episode: 991, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 3.454, mean reward: 0.192 [0.090, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.324, 10.100], loss: 0.002773, mae: 0.054299, mean_q: -0.201841
 56945/100000: episode: 992, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 7.844, mean reward: 0.291 [0.201, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.718, 10.100], loss: 0.002992, mae: 0.054808, mean_q: -0.211550
 56956/100000: episode: 993, duration: 0.073s, episode steps: 11, steps per second: 150, episode reward: 4.593, mean reward: 0.418 [0.340, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.312, 10.100], loss: 0.002455, mae: 0.049723, mean_q: -0.248402
 56983/100000: episode: 994, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 8.979, mean reward: 0.333 [0.226, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.435, 10.100], loss: 0.002791, mae: 0.053173, mean_q: -0.256034
 57003/100000: episode: 995, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 8.529, mean reward: 0.426 [0.333, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.751, 10.496], loss: 0.002502, mae: 0.052584, mean_q: -0.238631
 57021/100000: episode: 996, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 6.549, mean reward: 0.364 [0.216, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.264, 10.100], loss: 0.002464, mae: 0.051330, mean_q: -0.158998
 57038/100000: episode: 997, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 6.956, mean reward: 0.409 [0.336, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-1.276, 10.487], loss: 0.002695, mae: 0.052250, mean_q: -0.149120
 57079/100000: episode: 998, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 8.449, mean reward: 0.206 [0.026, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.600, 10.218], loss: 0.002512, mae: 0.050567, mean_q: -0.219261
 57111/100000: episode: 999, duration: 0.150s, episode steps: 32, steps per second: 213, episode reward: 7.123, mean reward: 0.223 [0.038, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.238, 10.100], loss: 0.002875, mae: 0.054960, mean_q: -0.152845
 57138/100000: episode: 1000, duration: 0.128s, episode steps: 27, steps per second: 212, episode reward: 10.132, mean reward: 0.375 [0.238, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.716, 10.100], loss: 0.002954, mae: 0.055070, mean_q: -0.214124
 57158/100000: episode: 1001, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 6.240, mean reward: 0.312 [0.247, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.956, 10.403], loss: 0.002563, mae: 0.050363, mean_q: -0.246645
 57171/100000: episode: 1002, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 4.090, mean reward: 0.315 [0.251, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.439], loss: 0.002659, mae: 0.053062, mean_q: -0.141141
 57209/100000: episode: 1003, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 9.801, mean reward: 0.258 [0.069, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.249, 10.100], loss: 0.002950, mae: 0.056377, mean_q: -0.154017
 57222/100000: episode: 1004, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 5.674, mean reward: 0.436 [0.396, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.556], loss: 0.002696, mae: 0.052401, mean_q: -0.199097
 57249/100000: episode: 1005, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 6.358, mean reward: 0.235 [0.132, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.490, 10.100], loss: 0.002587, mae: 0.052379, mean_q: -0.203943
 57256/100000: episode: 1006, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 2.525, mean reward: 0.361 [0.335, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.288, 10.444], loss: 0.003593, mae: 0.063522, mean_q: -0.173220
 57297/100000: episode: 1007, duration: 0.204s, episode steps: 41, steps per second: 201, episode reward: 10.559, mean reward: 0.258 [0.072, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.058, 10.100], loss: 0.002944, mae: 0.056446, mean_q: -0.163954
 57329/100000: episode: 1008, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 7.659, mean reward: 0.239 [0.047, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.035, 10.213], loss: 0.003319, mae: 0.058137, mean_q: -0.156749
 57347/100000: episode: 1009, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 4.883, mean reward: 0.271 [0.180, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.419, 10.100], loss: 0.003016, mae: 0.057216, mean_q: -0.101781
 57365/100000: episode: 1010, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 4.984, mean reward: 0.277 [0.212, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.208, 10.100], loss: 0.002833, mae: 0.054242, mean_q: -0.184588
 57403/100000: episode: 1011, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 13.749, mean reward: 0.362 [0.280, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.346, 10.100], loss: 0.002891, mae: 0.055296, mean_q: -0.140593
 57430/100000: episode: 1012, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 10.269, mean reward: 0.380 [0.215, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-1.393, 10.100], loss: 0.002720, mae: 0.053739, mean_q: -0.169879
 57441/100000: episode: 1013, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 2.995, mean reward: 0.272 [0.190, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.141, 10.100], loss: 0.003337, mae: 0.057532, mean_q: -0.086932
 57473/100000: episode: 1014, duration: 0.163s, episode steps: 32, steps per second: 197, episode reward: 10.225, mean reward: 0.320 [0.211, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.929, 10.100], loss: 0.002605, mae: 0.052212, mean_q: -0.165465
 57486/100000: episode: 1015, duration: 0.063s, episode steps: 13, steps per second: 207, episode reward: 4.277, mean reward: 0.329 [0.257, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.453], loss: 0.002863, mae: 0.053340, mean_q: -0.208294
 57499/100000: episode: 1016, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 3.782, mean reward: 0.291 [0.142, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.043, 10.430], loss: 0.002858, mae: 0.055091, mean_q: -0.158770
 57531/100000: episode: 1017, duration: 0.156s, episode steps: 32, steps per second: 205, episode reward: 13.032, mean reward: 0.407 [0.307, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.499, 10.100], loss: 0.002833, mae: 0.056045, mean_q: -0.193449
 57544/100000: episode: 1018, duration: 0.062s, episode steps: 13, steps per second: 211, episode reward: 4.881, mean reward: 0.375 [0.309, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.412, 10.353], loss: 0.002613, mae: 0.050874, mean_q: -0.115454
 57557/100000: episode: 1019, duration: 0.062s, episode steps: 13, steps per second: 209, episode reward: 4.702, mean reward: 0.362 [0.290, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.458], loss: 0.002852, mae: 0.055371, mean_q: -0.102888
 57568/100000: episode: 1020, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 3.683, mean reward: 0.335 [0.264, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.274, 10.100], loss: 0.002548, mae: 0.049629, mean_q: -0.116404
 57585/100000: episode: 1021, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 5.239, mean reward: 0.308 [0.173, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.317], loss: 0.002858, mae: 0.054158, mean_q: -0.181226
 57605/100000: episode: 1022, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 5.722, mean reward: 0.286 [0.210, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.404, 10.359], loss: 0.002663, mae: 0.053505, mean_q: -0.134458
 57646/100000: episode: 1023, duration: 0.196s, episode steps: 41, steps per second: 209, episode reward: 15.819, mean reward: 0.386 [0.223, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-1.176, 10.100], loss: 0.002741, mae: 0.053589, mean_q: -0.162866
 57653/100000: episode: 1024, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 2.284, mean reward: 0.326 [0.281, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.464], loss: 0.003019, mae: 0.055889, mean_q: -0.159277
 57680/100000: episode: 1025, duration: 0.131s, episode steps: 27, steps per second: 205, episode reward: 8.511, mean reward: 0.315 [0.156, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.700, 10.100], loss: 0.002816, mae: 0.054994, mean_q: -0.118213
 57700/100000: episode: 1026, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 8.153, mean reward: 0.408 [0.316, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.494, 10.448], loss: 0.002431, mae: 0.051523, mean_q: -0.104978
 57713/100000: episode: 1027, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 3.013, mean reward: 0.232 [0.145, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.432, 10.292], loss: 0.002584, mae: 0.053225, mean_q: -0.100708
 57751/100000: episode: 1028, duration: 0.188s, episode steps: 38, steps per second: 202, episode reward: 7.974, mean reward: 0.210 [0.027, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-1.825, 10.100], loss: 0.003239, mae: 0.059635, mean_q: -0.100558
 57771/100000: episode: 1029, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 7.174, mean reward: 0.359 [0.257, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.165, 10.482], loss: 0.002535, mae: 0.052070, mean_q: -0.131718
 57798/100000: episode: 1030, duration: 0.139s, episode steps: 27, steps per second: 195, episode reward: 8.928, mean reward: 0.331 [0.245, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.100, 10.100], loss: 0.002659, mae: 0.053258, mean_q: -0.130742
 57809/100000: episode: 1031, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 3.591, mean reward: 0.326 [0.254, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.438, 10.100], loss: 0.002677, mae: 0.054994, mean_q: -0.109126
 57827/100000: episode: 1032, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 7.253, mean reward: 0.403 [0.288, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.262, 10.100], loss: 0.003389, mae: 0.059418, mean_q: -0.048567
 57834/100000: episode: 1033, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 2.435, mean reward: 0.348 [0.327, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.461], loss: 0.003127, mae: 0.061354, mean_q: 0.005248
 57845/100000: episode: 1034, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 3.707, mean reward: 0.337 [0.279, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.661, 10.100], loss: 0.003648, mae: 0.062939, mean_q: -0.064049
 57886/100000: episode: 1035, duration: 0.214s, episode steps: 41, steps per second: 192, episode reward: 9.003, mean reward: 0.220 [0.020, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-1.194, 10.100], loss: 0.002659, mae: 0.053725, mean_q: -0.093750
 57903/100000: episode: 1036, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 5.969, mean reward: 0.351 [0.233, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.407], loss: 0.002844, mae: 0.054756, mean_q: -0.020560
 57910/100000: episode: 1037, duration: 0.051s, episode steps: 7, steps per second: 137, episode reward: 2.503, mean reward: 0.358 [0.272, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.871, 10.527], loss: 0.002736, mae: 0.055049, mean_q: 0.026209
 57928/100000: episode: 1038, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 3.893, mean reward: 0.216 [0.112, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.324, 10.100], loss: 0.002510, mae: 0.051145, mean_q: -0.104730
 57941/100000: episode: 1039, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 4.862, mean reward: 0.374 [0.306, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.302, 10.560], loss: 0.003732, mae: 0.064628, mean_q: -0.054991
 57968/100000: episode: 1040, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 8.477, mean reward: 0.314 [0.155, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.119, 10.100], loss: 0.002644, mae: 0.052880, mean_q: -0.042027
 57988/100000: episode: 1041, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 8.344, mean reward: 0.417 [0.336, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.153, 10.502], loss: 0.002977, mae: 0.056479, mean_q: -0.087857
 58015/100000: episode: 1042, duration: 0.128s, episode steps: 27, steps per second: 211, episode reward: 6.828, mean reward: 0.253 [0.093, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.206, 10.100], loss: 0.002812, mae: 0.053706, mean_q: -0.004493
 58047/100000: episode: 1043, duration: 0.156s, episode steps: 32, steps per second: 206, episode reward: 5.216, mean reward: 0.163 [0.020, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.526, 10.241], loss: 0.002731, mae: 0.052258, mean_q: -0.110350
 58088/100000: episode: 1044, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 15.148, mean reward: 0.369 [0.205, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.620, 10.100], loss: 0.003106, mae: 0.056493, mean_q: -0.041513
 58120/100000: episode: 1045, duration: 0.161s, episode steps: 32, steps per second: 198, episode reward: 8.205, mean reward: 0.256 [0.134, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.611, 10.100], loss: 0.002623, mae: 0.055073, mean_q: -0.021317
 58137/100000: episode: 1046, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 5.602, mean reward: 0.330 [0.256, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.927, 10.412], loss: 0.002642, mae: 0.054149, mean_q: 0.007336
 58169/100000: episode: 1047, duration: 0.163s, episode steps: 32, steps per second: 197, episode reward: 7.826, mean reward: 0.245 [0.040, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.124, 10.120], loss: 0.004319, mae: 0.064591, mean_q: 0.007150
 58182/100000: episode: 1048, duration: 0.062s, episode steps: 13, steps per second: 209, episode reward: 5.528, mean reward: 0.425 [0.340, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.115, 10.411], loss: 0.003943, mae: 0.066404, mean_q: -0.028457
 58200/100000: episode: 1049, duration: 0.088s, episode steps: 18, steps per second: 205, episode reward: 4.917, mean reward: 0.273 [0.189, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.270, 10.100], loss: 0.005567, mae: 0.070125, mean_q: 0.044496
 58220/100000: episode: 1050, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 6.280, mean reward: 0.314 [0.196, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.330], loss: 0.003168, mae: 0.062304, mean_q: -0.028553
 58261/100000: episode: 1051, duration: 0.231s, episode steps: 41, steps per second: 177, episode reward: 8.588, mean reward: 0.209 [0.029, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.592, 10.100], loss: 0.002833, mae: 0.055378, mean_q: -0.024904
 58272/100000: episode: 1052, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 3.450, mean reward: 0.314 [0.271, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.300, 10.100], loss: 0.002920, mae: 0.059382, mean_q: 0.049177
 58313/100000: episode: 1053, duration: 0.200s, episode steps: 41, steps per second: 205, episode reward: 12.933, mean reward: 0.315 [0.172, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.378, 10.100], loss: 0.003056, mae: 0.057427, mean_q: -0.000659
 58320/100000: episode: 1054, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 2.823, mean reward: 0.403 [0.324, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.586], loss: 0.002703, mae: 0.056083, mean_q: 0.001844
 58340/100000: episode: 1055, duration: 0.098s, episode steps: 20, steps per second: 205, episode reward: 6.482, mean reward: 0.324 [0.181, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.075, 10.290], loss: 0.002804, mae: 0.056421, mean_q: 0.001946
 58353/100000: episode: 1056, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 5.340, mean reward: 0.411 [0.369, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.463, 10.473], loss: 0.002274, mae: 0.049423, mean_q: -0.029730
 58360/100000: episode: 1057, duration: 0.036s, episode steps: 7, steps per second: 197, episode reward: 2.416, mean reward: 0.345 [0.243, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.925, 10.341], loss: 0.002275, mae: 0.051503, mean_q: -0.025834
 58398/100000: episode: 1058, duration: 0.181s, episode steps: 38, steps per second: 209, episode reward: 7.910, mean reward: 0.208 [0.075, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.594, 10.100], loss: 0.002785, mae: 0.056067, mean_q: 0.052205
 58415/100000: episode: 1059, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 5.919, mean reward: 0.348 [0.263, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.400], loss: 0.002927, mae: 0.056267, mean_q: 0.026472
 58435/100000: episode: 1060, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 7.267, mean reward: 0.363 [0.236, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.462, 10.369], loss: 0.003392, mae: 0.060640, mean_q: 0.071597
 58452/100000: episode: 1061, duration: 0.080s, episode steps: 17, steps per second: 213, episode reward: 7.092, mean reward: 0.417 [0.356, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.315, 10.555], loss: 0.002709, mae: 0.056024, mean_q: 0.035330
 58479/100000: episode: 1062, duration: 0.131s, episode steps: 27, steps per second: 207, episode reward: 7.605, mean reward: 0.282 [0.112, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.378, 10.100], loss: 0.002874, mae: 0.056293, mean_q: 0.016643
 58517/100000: episode: 1063, duration: 0.183s, episode steps: 38, steps per second: 208, episode reward: 11.465, mean reward: 0.302 [0.219, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.259, 10.100], loss: 0.002683, mae: 0.054255, mean_q: 0.006727
 58534/100000: episode: 1064, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 7.265, mean reward: 0.427 [0.375, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.499], loss: 0.002986, mae: 0.058450, mean_q: 0.066241
[Info] 200-TH LEVEL FOUND: 0.8656138777732849, Considering 10/90 traces
 58551/100000: episode: 1065, duration: 3.934s, episode steps: 17, steps per second: 4, episode reward: 5.771, mean reward: 0.339 [0.279, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.257, 10.407], loss: 0.002763, mae: 0.053471, mean_q: 0.075902
 58571/100000: episode: 1066, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 7.736, mean reward: 0.387 [0.294, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.452, 10.100], loss: 0.002423, mae: 0.052449, mean_q: 0.034017
 58581/100000: episode: 1067, duration: 0.048s, episode steps: 10, steps per second: 207, episode reward: 3.723, mean reward: 0.372 [0.290, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.503, 10.100], loss: 0.002206, mae: 0.047106, mean_q: -0.040343
 58601/100000: episode: 1068, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 9.159, mean reward: 0.458 [0.347, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.274, 10.100], loss: 0.002510, mae: 0.052956, mean_q: 0.080808
 58621/100000: episode: 1069, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 8.830, mean reward: 0.441 [0.387, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.372, 10.100], loss: 0.002485, mae: 0.053090, mean_q: 0.052021
 58624/100000: episode: 1070, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 1.460, mean reward: 0.487 [0.450, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.388], loss: 0.002475, mae: 0.049926, mean_q: 0.115249
 58628/100000: episode: 1071, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 1.725, mean reward: 0.431 [0.421, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.516], loss: 0.002341, mae: 0.050960, mean_q: 0.074688
 58638/100000: episode: 1072, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 4.741, mean reward: 0.474 [0.401, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.425, 10.100], loss: 0.003069, mae: 0.058986, mean_q: 0.129413
 58645/100000: episode: 1073, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 3.464, mean reward: 0.495 [0.416, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.613], loss: 0.002756, mae: 0.055260, mean_q: 0.169416
 58651/100000: episode: 1074, duration: 0.037s, episode steps: 6, steps per second: 160, episode reward: 2.793, mean reward: 0.465 [0.424, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.116, 10.530], loss: 0.003560, mae: 0.064514, mean_q: 0.068782
 58654/100000: episode: 1075, duration: 0.023s, episode steps: 3, steps per second: 130, episode reward: 1.397, mean reward: 0.466 [0.417, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.499], loss: 0.002662, mae: 0.053677, mean_q: -0.050077
 58658/100000: episode: 1076, duration: 0.024s, episode steps: 4, steps per second: 165, episode reward: 1.977, mean reward: 0.494 [0.433, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.459], loss: 0.002720, mae: 0.053090, mean_q: 0.062690
 58664/100000: episode: 1077, duration: 0.031s, episode steps: 6, steps per second: 196, episode reward: 2.677, mean reward: 0.446 [0.424, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.469], loss: 0.002370, mae: 0.051536, mean_q: -0.011794
 58674/100000: episode: 1078, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 4.360, mean reward: 0.436 [0.344, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.135, 10.628], loss: 0.001944, mae: 0.045926, mean_q: 0.108110
 58694/100000: episode: 1079, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 9.100, mean reward: 0.455 [0.357, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.154, 10.100], loss: 0.002606, mae: 0.051905, mean_q: 0.041699
 58703/100000: episode: 1080, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 4.371, mean reward: 0.486 [0.421, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.372, 10.100], loss: 0.002760, mae: 0.057292, mean_q: 0.153963
 58709/100000: episode: 1081, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 2.560, mean reward: 0.427 [0.390, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.475], loss: 0.002198, mae: 0.050946, mean_q: 0.038668
 58716/100000: episode: 1082, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 3.098, mean reward: 0.443 [0.378, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-1.368, 10.504], loss: 0.002984, mae: 0.057669, mean_q: 0.083777
 58725/100000: episode: 1083, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 3.985, mean reward: 0.443 [0.361, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.514, 10.100], loss: 0.002858, mae: 0.056962, mean_q: 0.089781
 58735/100000: episode: 1084, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 3.621, mean reward: 0.362 [0.271, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.293, 10.100], loss: 0.002753, mae: 0.054873, mean_q: 0.123260
 58738/100000: episode: 1085, duration: 0.017s, episode steps: 3, steps per second: 173, episode reward: 1.606, mean reward: 0.535 [0.506, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.554], loss: 0.002571, mae: 0.056059, mean_q: 0.111971
 58744/100000: episode: 1086, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 2.914, mean reward: 0.486 [0.416, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.640], loss: 0.002622, mae: 0.054268, mean_q: 0.146921
 58757/100000: episode: 1087, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 6.544, mean reward: 0.503 [0.365, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.529, 10.675], loss: 0.003662, mae: 0.065823, mean_q: 0.184413
 58764/100000: episode: 1088, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 3.248, mean reward: 0.464 [0.422, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.412, 10.561], loss: 0.003000, mae: 0.060451, mean_q: 0.136721
 58768/100000: episode: 1089, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 1.835, mean reward: 0.459 [0.425, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.562], loss: 0.002939, mae: 0.056294, mean_q: 0.141638
 58778/100000: episode: 1090, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 4.453, mean reward: 0.445 [0.381, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.111, 10.499], loss: 0.002665, mae: 0.054615, mean_q: 0.017489
 58788/100000: episode: 1091, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 3.307, mean reward: 0.331 [0.263, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.387, 10.100], loss: 0.002695, mae: 0.052515, mean_q: 0.096813
 58804/100000: episode: 1092, duration: 0.085s, episode steps: 16, steps per second: 187, episode reward: 6.787, mean reward: 0.424 [0.343, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.408], loss: 0.003191, mae: 0.060800, mean_q: 0.159355
 58811/100000: episode: 1093, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 2.764, mean reward: 0.395 [0.345, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.522], loss: 0.002648, mae: 0.053941, mean_q: 0.122381
 58815/100000: episode: 1094, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 1.863, mean reward: 0.466 [0.441, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.589], loss: 0.002985, mae: 0.058173, mean_q: 0.146481
 58825/100000: episode: 1095, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 3.616, mean reward: 0.362 [0.311, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.389, 10.100], loss: 0.002711, mae: 0.055322, mean_q: 0.169877
 58829/100000: episode: 1096, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 1.741, mean reward: 0.435 [0.416, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.363, 10.505], loss: 0.002122, mae: 0.048887, mean_q: -0.010902
 58838/100000: episode: 1097, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 3.425, mean reward: 0.381 [0.347, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.268, 10.100], loss: 0.003248, mae: 0.056298, mean_q: 0.034047
 58848/100000: episode: 1098, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 3.706, mean reward: 0.371 [0.286, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-1.113, 10.100], loss: 0.002726, mae: 0.055276, mean_q: 0.114822
 58864/100000: episode: 1099, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 6.718, mean reward: 0.420 [0.306, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.282, 10.461], loss: 0.002515, mae: 0.053314, mean_q: 0.157791
 58871/100000: episode: 1100, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 2.994, mean reward: 0.428 [0.360, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.485], loss: 0.002929, mae: 0.056193, mean_q: 0.191063
 58878/100000: episode: 1101, duration: 0.036s, episode steps: 7, steps per second: 197, episode reward: 2.957, mean reward: 0.422 [0.375, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.449], loss: 0.002920, mae: 0.058774, mean_q: 0.207040
 58894/100000: episode: 1102, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 7.753, mean reward: 0.485 [0.350, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-1.105, 10.424], loss: 0.002882, mae: 0.058087, mean_q: 0.133735
 58907/100000: episode: 1103, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 5.953, mean reward: 0.458 [0.379, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.242, 10.540], loss: 0.003024, mae: 0.058956, mean_q: 0.147538
 58923/100000: episode: 1104, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 8.787, mean reward: 0.549 [0.440, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.633], loss: 0.002775, mae: 0.056427, mean_q: 0.235720
 58933/100000: episode: 1105, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 4.055, mean reward: 0.406 [0.341, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.554], loss: 0.002764, mae: 0.056217, mean_q: 0.151036
 58942/100000: episode: 1106, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 3.760, mean reward: 0.418 [0.331, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.416, 10.100], loss: 0.003366, mae: 0.060715, mean_q: 0.095363
 58962/100000: episode: 1107, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 9.010, mean reward: 0.451 [0.352, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.259, 10.100], loss: 0.003080, mae: 0.058530, mean_q: 0.140298
 58972/100000: episode: 1108, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 4.373, mean reward: 0.437 [0.345, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.441], loss: 0.002802, mae: 0.056529, mean_q: 0.104213
 58985/100000: episode: 1109, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 5.928, mean reward: 0.456 [0.377, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.220, 10.664], loss: 0.002862, mae: 0.055035, mean_q: 0.170577
 58992/100000: episode: 1110, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 3.125, mean reward: 0.446 [0.432, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-1.444, 10.484], loss: 0.003130, mae: 0.060773, mean_q: 0.164842
 59002/100000: episode: 1111, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 4.024, mean reward: 0.402 [0.329, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.547, 10.100], loss: 0.003133, mae: 0.060411, mean_q: 0.132481
 59005/100000: episode: 1112, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 1.625, mean reward: 0.542 [0.514, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.460], loss: 0.002626, mae: 0.054903, mean_q: 0.216749
 59014/100000: episode: 1113, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 3.206, mean reward: 0.356 [0.261, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.248, 10.100], loss: 0.002889, mae: 0.056574, mean_q: 0.134251
 59024/100000: episode: 1114, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 4.206, mean reward: 0.421 [0.397, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.507], loss: 0.002915, mae: 0.057919, mean_q: 0.085714
 59040/100000: episode: 1115, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 8.088, mean reward: 0.506 [0.392, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-1.074, 10.601], loss: 0.002962, mae: 0.058176, mean_q: 0.189851
 59046/100000: episode: 1116, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 2.764, mean reward: 0.461 [0.434, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.572], loss: 0.003026, mae: 0.059037, mean_q: 0.215633
 59050/100000: episode: 1117, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 1.880, mean reward: 0.470 [0.448, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.488], loss: 0.003879, mae: 0.065838, mean_q: 0.160673
 59060/100000: episode: 1118, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 3.906, mean reward: 0.391 [0.327, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.318, 10.100], loss: 0.002840, mae: 0.055450, mean_q: 0.143091
 59070/100000: episode: 1119, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 3.601, mean reward: 0.360 [0.300, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.996, 10.100], loss: 0.003637, mae: 0.065019, mean_q: 0.285995
 59090/100000: episode: 1120, duration: 0.094s, episode steps: 20, steps per second: 212, episode reward: 9.375, mean reward: 0.469 [0.384, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.247, 10.100], loss: 0.002969, mae: 0.059441, mean_q: 0.182807
 59097/100000: episode: 1121, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 2.577, mean reward: 0.368 [0.287, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.455], loss: 0.003047, mae: 0.058602, mean_q: 0.236252
 59106/100000: episode: 1122, duration: 0.044s, episode steps: 9, steps per second: 205, episode reward: 3.748, mean reward: 0.416 [0.366, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.408, 10.100], loss: 0.002965, mae: 0.058289, mean_q: 0.271509
 59115/100000: episode: 1123, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 3.571, mean reward: 0.397 [0.368, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-1.163, 10.100], loss: 0.002694, mae: 0.053053, mean_q: 0.080389
 59122/100000: episode: 1124, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 2.679, mean reward: 0.383 [0.366, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.389], loss: 0.002932, mae: 0.056819, mean_q: 0.077934
 59126/100000: episode: 1125, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 1.767, mean reward: 0.442 [0.417, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.482], loss: 0.003512, mae: 0.063419, mean_q: 0.155560
 59136/100000: episode: 1126, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 4.402, mean reward: 0.440 [0.356, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.393], loss: 0.003967, mae: 0.062596, mean_q: 0.194595
 59145/100000: episode: 1127, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 3.768, mean reward: 0.419 [0.312, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.234, 10.100], loss: 0.005518, mae: 0.072692, mean_q: 0.131998
 59152/100000: episode: 1128, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 2.981, mean reward: 0.426 [0.348, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.505], loss: 0.005185, mae: 0.069978, mean_q: 0.120940
 59168/100000: episode: 1129, duration: 0.091s, episode steps: 16, steps per second: 177, episode reward: 7.443, mean reward: 0.465 [0.401, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.567], loss: 0.003298, mae: 0.063661, mean_q: 0.169523
 59181/100000: episode: 1130, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 5.053, mean reward: 0.389 [0.342, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.738, 10.460], loss: 0.002717, mae: 0.057302, mean_q: 0.189360
 59185/100000: episode: 1131, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 1.597, mean reward: 0.399 [0.363, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.453], loss: 0.001715, mae: 0.043707, mean_q: -0.061280
 59191/100000: episode: 1132, duration: 0.034s, episode steps: 6, steps per second: 176, episode reward: 2.467, mean reward: 0.411 [0.346, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.567], loss: 0.002806, mae: 0.058019, mean_q: 0.219854
 59197/100000: episode: 1133, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 2.622, mean reward: 0.437 [0.387, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.546], loss: 0.002991, mae: 0.058767, mean_q: 0.352539
 59210/100000: episode: 1134, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 5.901, mean reward: 0.454 [0.375, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.342, 10.597], loss: 0.002753, mae: 0.055682, mean_q: 0.202659
 59219/100000: episode: 1135, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 4.269, mean reward: 0.474 [0.403, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.413, 10.100], loss: 0.002784, mae: 0.056844, mean_q: 0.197030
 59228/100000: episode: 1136, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 3.430, mean reward: 0.381 [0.349, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.395, 10.100], loss: 0.002662, mae: 0.056344, mean_q: 0.220104
 59244/100000: episode: 1137, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 7.474, mean reward: 0.467 [0.398, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.502], loss: 0.002784, mae: 0.057160, mean_q: 0.291622
 59248/100000: episode: 1138, duration: 0.034s, episode steps: 4, steps per second: 119, episode reward: 1.802, mean reward: 0.450 [0.411, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.584], loss: 0.002530, mae: 0.054061, mean_q: 0.232425
 59251/100000: episode: 1139, duration: 0.021s, episode steps: 3, steps per second: 144, episode reward: 1.318, mean reward: 0.439 [0.424, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.035, 10.460], loss: 0.001960, mae: 0.049629, mean_q: 0.176296
 59271/100000: episode: 1140, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 7.608, mean reward: 0.380 [0.189, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.331, 10.100], loss: 0.003020, mae: 0.058237, mean_q: 0.138299
 59275/100000: episode: 1141, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 1.782, mean reward: 0.446 [0.421, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.352, 10.527], loss: 0.002890, mae: 0.057916, mean_q: 0.156834
 59295/100000: episode: 1142, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 10.786, mean reward: 0.539 [0.358, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.272, 10.100], loss: 0.002836, mae: 0.056281, mean_q: 0.220781
 59315/100000: episode: 1143, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 8.147, mean reward: 0.407 [0.279, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.192, 10.100], loss: 0.002782, mae: 0.057040, mean_q: 0.224442
 59325/100000: episode: 1144, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 4.414, mean reward: 0.441 [0.299, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.630, 10.100], loss: 0.002944, mae: 0.059711, mean_q: 0.276630
 59332/100000: episode: 1145, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 2.860, mean reward: 0.409 [0.382, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.203, 10.480], loss: 0.002211, mae: 0.050646, mean_q: 0.131924
 59336/100000: episode: 1146, duration: 0.029s, episode steps: 4, steps per second: 140, episode reward: 1.991, mean reward: 0.498 [0.457, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.174, 10.634], loss: 0.002351, mae: 0.050400, mean_q: 0.213985
 59346/100000: episode: 1147, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 4.603, mean reward: 0.460 [0.389, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.544], loss: 0.002817, mae: 0.056251, mean_q: 0.205600
 59355/100000: episode: 1148, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 3.358, mean reward: 0.373 [0.339, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.350, 10.100], loss: 0.002791, mae: 0.055615, mean_q: 0.184092
 59375/100000: episode: 1149, duration: 0.097s, episode steps: 20, steps per second: 206, episode reward: 9.750, mean reward: 0.488 [0.393, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.023, 10.100], loss: 0.002638, mae: 0.055313, mean_q: 0.175214
 59391/100000: episode: 1150, duration: 0.098s, episode steps: 16, steps per second: 164, episode reward: 6.628, mean reward: 0.414 [0.345, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.603, 10.383], loss: 0.003385, mae: 0.061252, mean_q: 0.244680
 59400/100000: episode: 1151, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 3.918, mean reward: 0.435 [0.384, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.416, 10.100], loss: 0.002882, mae: 0.059298, mean_q: 0.281643
 59410/100000: episode: 1152, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 2.584, mean reward: 0.258 [0.176, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.214, 10.100], loss: 0.003281, mae: 0.062791, mean_q: 0.254052
 59413/100000: episode: 1153, duration: 0.018s, episode steps: 3, steps per second: 171, episode reward: 1.480, mean reward: 0.493 [0.467, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.035, 10.619], loss: 0.002836, mae: 0.057282, mean_q: 0.249832
 59429/100000: episode: 1154, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 6.358, mean reward: 0.397 [0.349, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.453], loss: 0.002827, mae: 0.056868, mean_q: 0.274055
[Info] 300-TH LEVEL FOUND: 0.9235073328018188, Considering 10/90 traces
 59442/100000: episode: 1155, duration: 3.929s, episode steps: 13, steps per second: 3, episode reward: 5.693, mean reward: 0.438 [0.409, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.521], loss: 0.002839, mae: 0.058247, mean_q: 0.285763
 59453/100000: episode: 1156, duration: 0.053s, episode steps: 11, steps per second: 206, episode reward: 4.720, mean reward: 0.429 [0.358, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-1.169, 10.516], loss: 0.002593, mae: 0.053217, mean_q: 0.239416
 59460/100000: episode: 1157, duration: 0.035s, episode steps: 7, steps per second: 199, episode reward: 2.935, mean reward: 0.419 [0.383, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.537], loss: 0.002674, mae: 0.056582, mean_q: 0.283583
 59471/100000: episode: 1158, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 5.182, mean reward: 0.471 [0.375, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.903, 10.533], loss: 0.003810, mae: 0.065751, mean_q: 0.281156
 59478/100000: episode: 1159, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 3.265, mean reward: 0.466 [0.428, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.475, 10.100], loss: 0.003615, mae: 0.066023, mean_q: 0.265836
 59489/100000: episode: 1160, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 5.896, mean reward: 0.536 [0.500, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.492], loss: 0.003032, mae: 0.061189, mean_q: 0.317088
 59506/100000: episode: 1161, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 8.453, mean reward: 0.497 [0.447, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.272, 10.100], loss: 0.002783, mae: 0.056034, mean_q: 0.302656
 59523/100000: episode: 1162, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 9.054, mean reward: 0.533 [0.456, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.748, 10.100], loss: 0.003300, mae: 0.060107, mean_q: 0.340941
 59540/100000: episode: 1163, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 8.440, mean reward: 0.496 [0.400, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.667, 10.100], loss: 0.003433, mae: 0.062617, mean_q: 0.307409
 59551/100000: episode: 1164, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 5.371, mean reward: 0.488 [0.399, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.089, 10.502], loss: 0.002226, mae: 0.050745, mean_q: 0.255180
 59558/100000: episode: 1165, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 3.152, mean reward: 0.450 [0.394, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.469, 10.100], loss: 0.002545, mae: 0.054151, mean_q: 0.360372
 59575/100000: episode: 1166, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 8.079, mean reward: 0.475 [0.328, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.830, 10.100], loss: 0.002726, mae: 0.054683, mean_q: 0.232945
 59585/100000: episode: 1167, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 5.647, mean reward: 0.565 [0.451, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.082, 10.709], loss: 0.003748, mae: 0.063563, mean_q: 0.290867
 59603/100000: episode: 1168, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 7.290, mean reward: 0.405 [0.225, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.279, 10.100], loss: 0.002857, mae: 0.056265, mean_q: 0.367049
 59620/100000: episode: 1169, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 8.603, mean reward: 0.506 [0.443, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.359, 10.100], loss: 0.003168, mae: 0.061352, mean_q: 0.325924
 59638/100000: episode: 1170, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 7.002, mean reward: 0.389 [0.076, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.100], loss: 0.002824, mae: 0.056647, mean_q: 0.238837
 59645/100000: episode: 1171, duration: 0.049s, episode steps: 7, steps per second: 142, episode reward: 2.599, mean reward: 0.371 [0.341, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.410], loss: 0.003431, mae: 0.065467, mean_q: 0.341155
 59652/100000: episode: 1172, duration: 0.035s, episode steps: 7, steps per second: 199, episode reward: 3.461, mean reward: 0.494 [0.445, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.471, 10.100], loss: 0.002948, mae: 0.060012, mean_q: 0.364263
 59669/100000: episode: 1173, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 9.167, mean reward: 0.539 [0.464, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.544, 10.100], loss: 0.002839, mae: 0.056757, mean_q: 0.344458
[Info] FALSIFICATION!
 59676/100000: episode: 1174, duration: 0.036s, episode steps: 7, steps per second: 197, episode reward: 13.863, mean reward: 1.980 [0.586, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.483, 10.494], loss: 0.003273, mae: 0.057969, mean_q: 0.196612
 59776/100000: episode: 1175, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.193, mean reward: -0.182 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.551, 10.271], loss: 0.003101, mae: 0.059216, mean_q: 0.303639
 59876/100000: episode: 1176, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -8.647, mean reward: -0.086 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.071, 10.441], loss: 0.018297, mae: 0.077835, mean_q: 0.331870
 59976/100000: episode: 1177, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.015, mean reward: -0.160 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.262, 10.168], loss: 0.003574, mae: 0.064266, mean_q: 0.322377
 60076/100000: episode: 1178, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -19.234, mean reward: -0.192 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.583, 10.388], loss: 0.003373, mae: 0.061421, mean_q: 0.304150
 60176/100000: episode: 1179, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.923, mean reward: -0.179 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.744, 10.322], loss: 0.003025, mae: 0.059528, mean_q: 0.327177
 60276/100000: episode: 1180, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.628, mean reward: -0.176 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.542, 10.239], loss: 0.017666, mae: 0.073115, mean_q: 0.345873
 60376/100000: episode: 1181, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -19.132, mean reward: -0.191 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.593, 10.187], loss: 0.016582, mae: 0.067207, mean_q: 0.319990
 60476/100000: episode: 1182, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.919, mean reward: -0.169 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.815, 10.424], loss: 0.016886, mae: 0.070160, mean_q: 0.342783
 60576/100000: episode: 1183, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -14.981, mean reward: -0.150 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.369, 10.353], loss: 0.017002, mae: 0.068614, mean_q: 0.315603
 60676/100000: episode: 1184, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.453, mean reward: -0.175 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.278, 10.098], loss: 0.002950, mae: 0.057617, mean_q: 0.300559
 60776/100000: episode: 1185, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -14.491, mean reward: -0.145 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.589, 10.406], loss: 0.017837, mae: 0.070930, mean_q: 0.304833
 60876/100000: episode: 1186, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.831, mean reward: -0.188 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.312, 10.121], loss: 0.017323, mae: 0.074026, mean_q: 0.341528
 60976/100000: episode: 1187, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.117, mean reward: -0.181 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.178, 10.122], loss: 0.056031, mae: 0.084929, mean_q: 0.337114
 61076/100000: episode: 1188, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -15.377, mean reward: -0.154 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.991, 10.287], loss: 0.004075, mae: 0.066814, mean_q: 0.322900
 61176/100000: episode: 1189, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -17.144, mean reward: -0.171 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.445, 10.098], loss: 0.017849, mae: 0.070700, mean_q: 0.341193
 61276/100000: episode: 1190, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.961, mean reward: -0.190 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.629, 10.098], loss: 0.016853, mae: 0.069590, mean_q: 0.346463
 61376/100000: episode: 1191, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -13.257, mean reward: -0.133 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.159, 10.098], loss: 0.002911, mae: 0.057795, mean_q: 0.301042
 61476/100000: episode: 1192, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -16.036, mean reward: -0.160 [-1.000, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.196, 10.098], loss: 0.002933, mae: 0.057946, mean_q: 0.300970
 61576/100000: episode: 1193, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -19.251, mean reward: -0.193 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.679, 10.098], loss: 0.029812, mae: 0.072501, mean_q: 0.309021
 61676/100000: episode: 1194, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -15.004, mean reward: -0.150 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.689, 10.098], loss: 0.016724, mae: 0.069006, mean_q: 0.268655
 61776/100000: episode: 1195, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -18.240, mean reward: -0.182 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.304, 10.098], loss: 0.043131, mae: 0.080628, mean_q: 0.246268
 61876/100000: episode: 1196, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -18.547, mean reward: -0.185 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.422, 10.230], loss: 0.016758, mae: 0.069594, mean_q: 0.228424
 61976/100000: episode: 1197, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -17.293, mean reward: -0.173 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.438, 10.141], loss: 0.029465, mae: 0.072973, mean_q: 0.230167
 62076/100000: episode: 1198, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -18.930, mean reward: -0.189 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.557, 10.098], loss: 0.017288, mae: 0.070686, mean_q: 0.228395
 62176/100000: episode: 1199, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -14.729, mean reward: -0.147 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.947, 10.249], loss: 0.016348, mae: 0.065799, mean_q: 0.188485
 62276/100000: episode: 1200, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.513, mean reward: -0.165 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.910, 10.098], loss: 0.016873, mae: 0.066773, mean_q: 0.155042
 62376/100000: episode: 1201, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -16.055, mean reward: -0.161 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.671, 10.098], loss: 0.016335, mae: 0.063668, mean_q: 0.141335
 62476/100000: episode: 1202, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -17.994, mean reward: -0.180 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.786, 10.198], loss: 0.002934, mae: 0.056200, mean_q: 0.118807
 62576/100000: episode: 1203, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -14.093, mean reward: -0.141 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.272, 10.098], loss: 0.016691, mae: 0.065367, mean_q: 0.085536
 62676/100000: episode: 1204, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -12.451, mean reward: -0.125 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.845, 10.098], loss: 0.030097, mae: 0.073381, mean_q: 0.098738
 62776/100000: episode: 1205, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -18.453, mean reward: -0.185 [-1.000, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.999, 10.210], loss: 0.015948, mae: 0.060698, mean_q: 0.054055
 62876/100000: episode: 1206, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.398, mean reward: -0.184 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.164, 10.210], loss: 0.029223, mae: 0.070631, mean_q: 0.082543
 62976/100000: episode: 1207, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -16.572, mean reward: -0.166 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.511, 10.098], loss: 0.002942, mae: 0.056518, mean_q: 0.039230
 63076/100000: episode: 1208, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -18.463, mean reward: -0.185 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.950, 10.201], loss: 0.030604, mae: 0.076857, mean_q: 0.002306
 63176/100000: episode: 1209, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.039, mean reward: -0.170 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.080, 10.243], loss: 0.003007, mae: 0.057289, mean_q: 0.023749
 63276/100000: episode: 1210, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -16.454, mean reward: -0.165 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.897, 10.430], loss: 0.002810, mae: 0.054079, mean_q: -0.021674
 63376/100000: episode: 1211, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -16.011, mean reward: -0.160 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.328, 10.098], loss: 0.002976, mae: 0.056403, mean_q: -0.063423
 63476/100000: episode: 1212, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -18.027, mean reward: -0.180 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.338, 10.260], loss: 0.002854, mae: 0.054172, mean_q: -0.078920
 63576/100000: episode: 1213, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -19.739, mean reward: -0.197 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.938, 10.098], loss: 0.002974, mae: 0.056692, mean_q: -0.092550
 63676/100000: episode: 1214, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -16.952, mean reward: -0.170 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.725, 10.261], loss: 0.002624, mae: 0.051494, mean_q: -0.125196
 63776/100000: episode: 1215, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.742, mean reward: -0.177 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.108, 10.173], loss: 0.002902, mae: 0.055034, mean_q: -0.126395
 63876/100000: episode: 1216, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -19.770, mean reward: -0.198 [-1.000, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.419, 10.104], loss: 0.002605, mae: 0.052020, mean_q: -0.151949
 63976/100000: episode: 1217, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -14.846, mean reward: -0.148 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.728, 10.098], loss: 0.003107, mae: 0.055981, mean_q: -0.188300
 64076/100000: episode: 1218, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.666, mean reward: -0.167 [-1.000, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.089, 10.098], loss: 0.002509, mae: 0.051173, mean_q: -0.195434
 64176/100000: episode: 1219, duration: 0.460s, episode steps: 100, steps per second: 218, episode reward: -18.047, mean reward: -0.180 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.421, 10.098], loss: 0.016172, mae: 0.058419, mean_q: -0.202678
 64276/100000: episode: 1220, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -13.144, mean reward: -0.131 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.324, 10.098], loss: 0.003103, mae: 0.054585, mean_q: -0.224886
 64376/100000: episode: 1221, duration: 0.473s, episode steps: 100, steps per second: 212, episode reward: -18.106, mean reward: -0.181 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.710, 10.128], loss: 0.016199, mae: 0.059474, mean_q: -0.253452
 64476/100000: episode: 1222, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -13.207, mean reward: -0.132 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.354, 10.098], loss: 0.002657, mae: 0.051996, mean_q: -0.278504
 64576/100000: episode: 1223, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -16.382, mean reward: -0.164 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.251, 10.098], loss: 0.002569, mae: 0.050302, mean_q: -0.280672
 64676/100000: episode: 1224, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.329, mean reward: -0.173 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.930, 10.098], loss: 0.002583, mae: 0.049976, mean_q: -0.307923
 64776/100000: episode: 1225, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -18.508, mean reward: -0.185 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.918, 10.227], loss: 0.003105, mae: 0.055838, mean_q: -0.309475
 64876/100000: episode: 1226, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -16.390, mean reward: -0.164 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.979, 10.098], loss: 0.002454, mae: 0.049343, mean_q: -0.333013
 64976/100000: episode: 1227, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -11.943, mean reward: -0.119 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.095, 10.518], loss: 0.002771, mae: 0.053736, mean_q: -0.309947
 65076/100000: episode: 1228, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -17.941, mean reward: -0.179 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.219, 10.383], loss: 0.002902, mae: 0.054815, mean_q: -0.343651
 65176/100000: episode: 1229, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.729, mean reward: -0.167 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.353, 10.208], loss: 0.002426, mae: 0.049315, mean_q: -0.316494
 65276/100000: episode: 1230, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -15.796, mean reward: -0.158 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.591, 10.190], loss: 0.002504, mae: 0.050357, mean_q: -0.299362
 65376/100000: episode: 1231, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -19.361, mean reward: -0.194 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.622, 10.098], loss: 0.002890, mae: 0.053603, mean_q: -0.319230
 65476/100000: episode: 1232, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.579, mean reward: -0.156 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.820, 10.098], loss: 0.002676, mae: 0.052032, mean_q: -0.318558
 65576/100000: episode: 1233, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -20.362, mean reward: -0.204 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.910, 10.161], loss: 0.002487, mae: 0.049109, mean_q: -0.324558
 65676/100000: episode: 1234, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -19.294, mean reward: -0.193 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.903, 10.098], loss: 0.002612, mae: 0.051351, mean_q: -0.316945
 65776/100000: episode: 1235, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -10.162, mean reward: -0.102 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.895, 10.098], loss: 0.002779, mae: 0.052898, mean_q: -0.314977
 65876/100000: episode: 1236, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -15.665, mean reward: -0.157 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.611, 10.212], loss: 0.002594, mae: 0.050822, mean_q: -0.350326
 65976/100000: episode: 1237, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -20.531, mean reward: -0.205 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.857, 10.103], loss: 0.002742, mae: 0.052377, mean_q: -0.333121
 66076/100000: episode: 1238, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.060, mean reward: -0.171 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.722, 10.265], loss: 0.002880, mae: 0.054327, mean_q: -0.262744
 66176/100000: episode: 1239, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -12.537, mean reward: -0.125 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.716, 10.098], loss: 0.002749, mae: 0.052089, mean_q: -0.307926
 66276/100000: episode: 1240, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: -18.943, mean reward: -0.189 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.032, 10.176], loss: 0.002778, mae: 0.052959, mean_q: -0.304020
 66376/100000: episode: 1241, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.619, mean reward: -0.166 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.346, 10.161], loss: 0.002391, mae: 0.049355, mean_q: -0.323231
 66476/100000: episode: 1242, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: -16.806, mean reward: -0.168 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.598, 10.367], loss: 0.002835, mae: 0.053094, mean_q: -0.330449
 66576/100000: episode: 1243, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -17.840, mean reward: -0.178 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.795, 10.098], loss: 0.002359, mae: 0.047800, mean_q: -0.344325
 66676/100000: episode: 1244, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.258, mean reward: -0.173 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.720, 10.162], loss: 0.002879, mae: 0.053955, mean_q: -0.322362
 66776/100000: episode: 1245, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -18.504, mean reward: -0.185 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.407, 10.306], loss: 0.002664, mae: 0.051578, mean_q: -0.326735
 66876/100000: episode: 1246, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -16.513, mean reward: -0.165 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.346, 10.098], loss: 0.002783, mae: 0.051961, mean_q: -0.317048
 66976/100000: episode: 1247, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -18.004, mean reward: -0.180 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.909, 10.098], loss: 0.003096, mae: 0.054680, mean_q: -0.327960
 67076/100000: episode: 1248, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.200, mean reward: -0.182 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.937, 10.098], loss: 0.003541, mae: 0.059593, mean_q: -0.341635
 67176/100000: episode: 1249, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -20.264, mean reward: -0.203 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.152, 10.143], loss: 0.002923, mae: 0.054005, mean_q: -0.317227
 67276/100000: episode: 1250, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -15.535, mean reward: -0.155 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.229, 10.342], loss: 0.002645, mae: 0.051034, mean_q: -0.328299
 67376/100000: episode: 1251, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -16.636, mean reward: -0.166 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.157, 10.098], loss: 0.002651, mae: 0.050702, mean_q: -0.351966
 67476/100000: episode: 1252, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -15.498, mean reward: -0.155 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.945, 10.098], loss: 0.002650, mae: 0.051589, mean_q: -0.287049
 67576/100000: episode: 1253, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -18.751, mean reward: -0.188 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.842, 10.273], loss: 0.002747, mae: 0.051993, mean_q: -0.318302
 67676/100000: episode: 1254, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -16.202, mean reward: -0.162 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.769, 10.098], loss: 0.002877, mae: 0.053686, mean_q: -0.303354
 67776/100000: episode: 1255, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -12.835, mean reward: -0.128 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.834, 10.098], loss: 0.002623, mae: 0.051800, mean_q: -0.329086
 67876/100000: episode: 1256, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -16.106, mean reward: -0.161 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.587, 10.336], loss: 0.002704, mae: 0.052090, mean_q: -0.282000
 67976/100000: episode: 1257, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -16.325, mean reward: -0.163 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.468, 10.317], loss: 0.002822, mae: 0.052661, mean_q: -0.325580
 68076/100000: episode: 1258, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.863, mean reward: -0.189 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.184, 10.140], loss: 0.002779, mae: 0.052941, mean_q: -0.304838
 68176/100000: episode: 1259, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -18.369, mean reward: -0.184 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.816, 10.098], loss: 0.002469, mae: 0.049574, mean_q: -0.301658
 68276/100000: episode: 1260, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -15.543, mean reward: -0.155 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.337, 10.226], loss: 0.002811, mae: 0.053361, mean_q: -0.324861
 68376/100000: episode: 1261, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -18.096, mean reward: -0.181 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.471, 10.159], loss: 0.002790, mae: 0.053145, mean_q: -0.333479
 68476/100000: episode: 1262, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -19.594, mean reward: -0.196 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.767, 10.098], loss: 0.002690, mae: 0.052417, mean_q: -0.331199
 68576/100000: episode: 1263, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: -12.811, mean reward: -0.128 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.699, 10.098], loss: 0.002568, mae: 0.051185, mean_q: -0.316408
 68676/100000: episode: 1264, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -13.101, mean reward: -0.131 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.655, 10.098], loss: 0.002691, mae: 0.052100, mean_q: -0.342631
 68776/100000: episode: 1265, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -17.080, mean reward: -0.171 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.070, 10.314], loss: 0.002690, mae: 0.052060, mean_q: -0.358165
 68876/100000: episode: 1266, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -18.218, mean reward: -0.182 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.825, 10.323], loss: 0.002775, mae: 0.053304, mean_q: -0.320527
 68976/100000: episode: 1267, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: -16.787, mean reward: -0.168 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.512, 10.098], loss: 0.002874, mae: 0.054308, mean_q: -0.308080
 69076/100000: episode: 1268, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -11.147, mean reward: -0.111 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.403, 10.410], loss: 0.002823, mae: 0.053313, mean_q: -0.295759
 69176/100000: episode: 1269, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -15.545, mean reward: -0.155 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.350, 10.181], loss: 0.002605, mae: 0.051729, mean_q: -0.321943
 69276/100000: episode: 1270, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -18.783, mean reward: -0.188 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.378, 10.140], loss: 0.002558, mae: 0.049952, mean_q: -0.337798
 69376/100000: episode: 1271, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.900, mean reward: -0.189 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.851, 10.098], loss: 0.002719, mae: 0.052276, mean_q: -0.320983
 69476/100000: episode: 1272, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -15.058, mean reward: -0.151 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.966, 10.307], loss: 0.002899, mae: 0.054817, mean_q: -0.307312
 69576/100000: episode: 1273, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.135, mean reward: -0.181 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.506, 10.129], loss: 0.002800, mae: 0.053191, mean_q: -0.305443
[Info] 100-TH LEVEL FOUND: 0.5564086437225342, Considering 10/90 traces
 69676/100000: episode: 1274, duration: 4.296s, episode steps: 100, steps per second: 23, episode reward: -17.523, mean reward: -0.175 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.131, 10.098], loss: 0.002686, mae: 0.052292, mean_q: -0.327809
 69699/100000: episode: 1275, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 4.021, mean reward: 0.175 [0.042, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.803, 10.100], loss: 0.002582, mae: 0.050081, mean_q: -0.353275
 69735/100000: episode: 1276, duration: 0.186s, episode steps: 36, steps per second: 193, episode reward: 10.690, mean reward: 0.297 [0.163, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-1.719, 10.100], loss: 0.002806, mae: 0.053768, mean_q: -0.289319
 69755/100000: episode: 1277, duration: 0.094s, episode steps: 20, steps per second: 213, episode reward: 5.236, mean reward: 0.262 [0.190, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.269, 10.100], loss: 0.003001, mae: 0.054974, mean_q: -0.251182
 69771/100000: episode: 1278, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 6.096, mean reward: 0.381 [0.312, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.553, 10.100], loss: 0.002794, mae: 0.055914, mean_q: -0.313795
 69788/100000: episode: 1279, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 5.372, mean reward: 0.316 [0.229, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.329, 10.100], loss: 0.002843, mae: 0.053981, mean_q: -0.288162
 69814/100000: episode: 1280, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 5.792, mean reward: 0.223 [0.114, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.411, 10.100], loss: 0.002984, mae: 0.056101, mean_q: -0.309073
 69854/100000: episode: 1281, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 8.364, mean reward: 0.209 [0.004, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.549, 10.136], loss: 0.002623, mae: 0.053383, mean_q: -0.278437
 69873/100000: episode: 1282, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 7.470, mean reward: 0.393 [0.328, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.747, 10.100], loss: 0.002450, mae: 0.051013, mean_q: -0.347703
 69896/100000: episode: 1283, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 8.403, mean reward: 0.365 [0.267, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.665, 10.100], loss: 0.007260, mae: 0.073652, mean_q: -0.314216
 69932/100000: episode: 1284, duration: 0.180s, episode steps: 36, steps per second: 200, episode reward: 9.858, mean reward: 0.274 [0.114, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.129, 10.100], loss: 0.003853, mae: 0.063266, mean_q: -0.262810
 69949/100000: episode: 1285, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 3.963, mean reward: 0.233 [0.077, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.000, 10.100], loss: 0.002866, mae: 0.055753, mean_q: -0.164371
 69989/100000: episode: 1286, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 8.129, mean reward: 0.203 [0.043, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.605, 10.100], loss: 0.002482, mae: 0.052042, mean_q: -0.226485
 70040/100000: episode: 1287, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 10.994, mean reward: 0.216 [0.040, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.425, 10.100], loss: 0.002828, mae: 0.053446, mean_q: -0.226247
 70063/100000: episode: 1288, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 7.491, mean reward: 0.326 [0.202, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.345, 10.100], loss: 0.002670, mae: 0.054074, mean_q: -0.220020
 70079/100000: episode: 1289, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 4.711, mean reward: 0.294 [0.228, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.546, 10.100], loss: 0.002513, mae: 0.051890, mean_q: -0.215482
 70102/100000: episode: 1290, duration: 0.127s, episode steps: 23, steps per second: 182, episode reward: 7.337, mean reward: 0.319 [0.188, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.470, 10.100], loss: 0.002202, mae: 0.048196, mean_q: -0.285954
 70128/100000: episode: 1291, duration: 0.132s, episode steps: 26, steps per second: 196, episode reward: 8.433, mean reward: 0.324 [0.239, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.311, 10.100], loss: 0.002525, mae: 0.050928, mean_q: -0.239777
 70151/100000: episode: 1292, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 6.557, mean reward: 0.285 [0.141, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.760, 10.100], loss: 0.002663, mae: 0.052198, mean_q: -0.222630
 70167/100000: episode: 1293, duration: 0.077s, episode steps: 16, steps per second: 208, episode reward: 5.625, mean reward: 0.352 [0.238, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.040, 10.100], loss: 0.003003, mae: 0.055537, mean_q: -0.182894
 70190/100000: episode: 1294, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 4.162, mean reward: 0.181 [0.077, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.270, 10.100], loss: 0.002930, mae: 0.054718, mean_q: -0.194308
 70207/100000: episode: 1295, duration: 0.112s, episode steps: 17, steps per second: 151, episode reward: 5.471, mean reward: 0.322 [0.166, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.349, 10.100], loss: 0.002536, mae: 0.050643, mean_q: -0.221251
 70223/100000: episode: 1296, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 4.797, mean reward: 0.300 [0.226, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.297, 10.100], loss: 0.002500, mae: 0.051691, mean_q: -0.144195
 70243/100000: episode: 1297, duration: 0.100s, episode steps: 20, steps per second: 199, episode reward: 5.863, mean reward: 0.293 [0.121, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.065, 10.100], loss: 0.002881, mae: 0.054719, mean_q: -0.145121
 70283/100000: episode: 1298, duration: 0.193s, episode steps: 40, steps per second: 208, episode reward: 11.094, mean reward: 0.277 [0.165, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.600, 10.100], loss: 0.002482, mae: 0.051473, mean_q: -0.264101
 70334/100000: episode: 1299, duration: 0.276s, episode steps: 51, steps per second: 185, episode reward: 12.452, mean reward: 0.244 [0.060, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.471, 10.100], loss: 0.002547, mae: 0.050803, mean_q: -0.222661
 70370/100000: episode: 1300, duration: 0.186s, episode steps: 36, steps per second: 193, episode reward: 9.397, mean reward: 0.261 [0.076, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.183, 10.100], loss: 0.002625, mae: 0.051567, mean_q: -0.209188
 70410/100000: episode: 1301, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 9.148, mean reward: 0.229 [0.013, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.611, 10.332], loss: 0.002383, mae: 0.049195, mean_q: -0.231999
 70450/100000: episode: 1302, duration: 0.210s, episode steps: 40, steps per second: 191, episode reward: 8.777, mean reward: 0.219 [0.080, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.965, 10.100], loss: 0.002686, mae: 0.052328, mean_q: -0.178438
 70467/100000: episode: 1303, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 5.022, mean reward: 0.295 [0.165, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-1.187, 10.100], loss: 0.003370, mae: 0.060297, mean_q: -0.124411
 70486/100000: episode: 1304, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 5.815, mean reward: 0.306 [0.179, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.153, 10.100], loss: 0.002908, mae: 0.056198, mean_q: -0.124717
 70522/100000: episode: 1305, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 8.612, mean reward: 0.239 [0.126, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.324, 10.100], loss: 0.002824, mae: 0.054108, mean_q: -0.160351
 70541/100000: episode: 1306, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 7.572, mean reward: 0.399 [0.312, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.587, 10.100], loss: 0.002780, mae: 0.052241, mean_q: -0.274767
 70581/100000: episode: 1307, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 11.725, mean reward: 0.293 [0.204, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.578, 10.100], loss: 0.002695, mae: 0.053748, mean_q: -0.165181
 70597/100000: episode: 1308, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 5.026, mean reward: 0.314 [0.253, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.240, 10.100], loss: 0.003080, mae: 0.053558, mean_q: -0.267239
 70614/100000: episode: 1309, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 6.144, mean reward: 0.361 [0.206, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.189, 10.100], loss: 0.002758, mae: 0.053829, mean_q: -0.160645
 70631/100000: episode: 1310, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 4.962, mean reward: 0.292 [0.245, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.345, 10.100], loss: 0.002687, mae: 0.053263, mean_q: -0.127511
 70667/100000: episode: 1311, duration: 0.188s, episode steps: 36, steps per second: 191, episode reward: 10.699, mean reward: 0.297 [0.110, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.293, 10.100], loss: 0.002845, mae: 0.054164, mean_q: -0.042318
 70718/100000: episode: 1312, duration: 0.240s, episode steps: 51, steps per second: 213, episode reward: 13.422, mean reward: 0.263 [0.036, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.111, 10.100], loss: 0.002643, mae: 0.052187, mean_q: -0.129123
 70744/100000: episode: 1313, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 8.684, mean reward: 0.334 [0.245, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.216, 10.100], loss: 0.002962, mae: 0.056281, mean_q: -0.107968
 70760/100000: episode: 1314, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 3.420, mean reward: 0.214 [0.053, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.837, 10.100], loss: 0.003098, mae: 0.057085, mean_q: -0.085966
 70780/100000: episode: 1315, duration: 0.099s, episode steps: 20, steps per second: 201, episode reward: 4.573, mean reward: 0.229 [0.127, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.492, 10.100], loss: 0.003136, mae: 0.058177, mean_q: -0.135433
 70803/100000: episode: 1316, duration: 0.114s, episode steps: 23, steps per second: 203, episode reward: 7.592, mean reward: 0.330 [0.208, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.301, 10.100], loss: 0.002896, mae: 0.056853, mean_q: -0.065583
 70829/100000: episode: 1317, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 7.936, mean reward: 0.305 [0.225, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.715, 10.100], loss: 0.002770, mae: 0.054169, mean_q: -0.133728
 70865/100000: episode: 1318, duration: 0.172s, episode steps: 36, steps per second: 209, episode reward: 11.388, mean reward: 0.316 [0.201, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.401, 10.100], loss: 0.003344, mae: 0.062184, mean_q: -0.085713
 70891/100000: episode: 1319, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 7.564, mean reward: 0.291 [0.175, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.348, 10.100], loss: 0.002446, mae: 0.051851, mean_q: -0.077025
 70908/100000: episode: 1320, duration: 0.080s, episode steps: 17, steps per second: 213, episode reward: 3.689, mean reward: 0.217 [0.113, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.612, 10.100], loss: 0.002505, mae: 0.050849, mean_q: -0.066463
 70944/100000: episode: 1321, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 9.212, mean reward: 0.256 [0.184, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-1.466, 10.100], loss: 0.002489, mae: 0.051511, mean_q: -0.115493
 70967/100000: episode: 1322, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 5.925, mean reward: 0.258 [0.112, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.212, 10.100], loss: 0.002820, mae: 0.055960, mean_q: -0.076645
 70983/100000: episode: 1323, duration: 0.077s, episode steps: 16, steps per second: 209, episode reward: 3.811, mean reward: 0.238 [0.155, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.385, 10.100], loss: 0.002947, mae: 0.057253, mean_q: -0.075898
 71003/100000: episode: 1324, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 5.361, mean reward: 0.268 [0.227, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.129, 10.100], loss: 0.002772, mae: 0.055016, mean_q: -0.098727
 71022/100000: episode: 1325, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 5.028, mean reward: 0.265 [0.108, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.182, 10.100], loss: 0.002656, mae: 0.052426, mean_q: -0.135991
 71048/100000: episode: 1326, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 6.639, mean reward: 0.255 [0.147, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.360, 10.100], loss: 0.003178, mae: 0.059224, mean_q: -0.047610
 71064/100000: episode: 1327, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 4.444, mean reward: 0.278 [0.175, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.214, 10.100], loss: 0.002320, mae: 0.049048, mean_q: -0.160481
 71081/100000: episode: 1328, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 5.919, mean reward: 0.348 [0.255, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.155, 10.100], loss: 0.002720, mae: 0.052685, mean_q: -0.040147
 71107/100000: episode: 1329, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 3.901, mean reward: 0.150 [0.052, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.035, 10.146], loss: 0.002630, mae: 0.053792, mean_q: -0.078712
 71124/100000: episode: 1330, duration: 0.081s, episode steps: 17, steps per second: 211, episode reward: 5.084, mean reward: 0.299 [0.186, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.244, 10.100], loss: 0.002425, mae: 0.051245, mean_q: -0.003447
 71141/100000: episode: 1331, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 5.240, mean reward: 0.308 [0.224, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.745, 10.100], loss: 0.002644, mae: 0.053707, mean_q: -0.027940
 71164/100000: episode: 1332, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 5.939, mean reward: 0.258 [0.158, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.145, 10.100], loss: 0.002712, mae: 0.053856, mean_q: -0.054304
 71180/100000: episode: 1333, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 4.318, mean reward: 0.270 [0.169, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.281, 10.100], loss: 0.002827, mae: 0.053582, mean_q: -0.052603
 71216/100000: episode: 1334, duration: 0.168s, episode steps: 36, steps per second: 215, episode reward: 12.070, mean reward: 0.335 [0.215, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.528, 10.100], loss: 0.002699, mae: 0.053658, mean_q: -0.029609
 71236/100000: episode: 1335, duration: 0.102s, episode steps: 20, steps per second: 195, episode reward: 6.596, mean reward: 0.330 [0.195, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.574, 10.100], loss: 0.002430, mae: 0.051416, mean_q: 0.000703
 71287/100000: episode: 1336, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 9.198, mean reward: 0.180 [0.019, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.294, 10.297], loss: 0.003057, mae: 0.058057, mean_q: 0.010788
 71327/100000: episode: 1337, duration: 0.220s, episode steps: 40, steps per second: 182, episode reward: 10.652, mean reward: 0.266 [0.138, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.669, 10.100], loss: 0.002803, mae: 0.056096, mean_q: 0.003679
 71344/100000: episode: 1338, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 4.616, mean reward: 0.272 [0.207, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.534, 10.100], loss: 0.002528, mae: 0.052728, mean_q: 0.004407
 71364/100000: episode: 1339, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 6.024, mean reward: 0.301 [0.192, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.520, 10.100], loss: 0.002631, mae: 0.052731, mean_q: -0.048744
 71380/100000: episode: 1340, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 4.354, mean reward: 0.272 [0.139, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.100], loss: 0.002899, mae: 0.056234, mean_q: -0.052543
 71420/100000: episode: 1341, duration: 0.193s, episode steps: 40, steps per second: 207, episode reward: 11.847, mean reward: 0.296 [0.182, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-1.123, 10.100], loss: 0.002833, mae: 0.055883, mean_q: -0.018309
 71437/100000: episode: 1342, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 5.150, mean reward: 0.303 [0.235, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.907, 10.100], loss: 0.002689, mae: 0.055433, mean_q: 0.064037
 71488/100000: episode: 1343, duration: 0.246s, episode steps: 51, steps per second: 207, episode reward: 11.335, mean reward: 0.222 [0.061, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.845, 10.100], loss: 0.002850, mae: 0.056597, mean_q: -0.011792
 71507/100000: episode: 1344, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 7.741, mean reward: 0.407 [0.318, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.344, 10.100], loss: 0.002587, mae: 0.052493, mean_q: 0.059283
 71558/100000: episode: 1345, duration: 0.252s, episode steps: 51, steps per second: 202, episode reward: 9.294, mean reward: 0.182 [0.061, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.380, 10.191], loss: 0.002746, mae: 0.055683, mean_q: 0.029446
 71584/100000: episode: 1346, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 7.286, mean reward: 0.280 [0.048, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.021, 10.100], loss: 0.002783, mae: 0.055696, mean_q: 0.032404
 71603/100000: episode: 1347, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 5.562, mean reward: 0.293 [0.184, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.095, 10.100], loss: 0.002790, mae: 0.056371, mean_q: 0.025243
 71654/100000: episode: 1348, duration: 0.262s, episode steps: 51, steps per second: 195, episode reward: 12.399, mean reward: 0.243 [0.035, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.563, 10.247], loss: 0.002944, mae: 0.057045, mean_q: 0.041888
 71671/100000: episode: 1349, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 5.862, mean reward: 0.345 [0.277, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.033, 10.100], loss: 0.003117, mae: 0.059238, mean_q: 0.010694
 71687/100000: episode: 1350, duration: 0.076s, episode steps: 16, steps per second: 211, episode reward: 4.080, mean reward: 0.255 [0.094, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.587, 10.100], loss: 0.002758, mae: 0.056195, mean_q: -0.002919
 71738/100000: episode: 1351, duration: 0.249s, episode steps: 51, steps per second: 205, episode reward: 10.078, mean reward: 0.198 [0.020, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-1.705, 10.114], loss: 0.002730, mae: 0.054335, mean_q: 0.054070
 71754/100000: episode: 1352, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 6.579, mean reward: 0.411 [0.323, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.485, 10.100], loss: 0.002654, mae: 0.054346, mean_q: 0.006724
 71805/100000: episode: 1353, duration: 0.244s, episode steps: 51, steps per second: 209, episode reward: 16.219, mean reward: 0.318 [0.056, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.325, 10.100], loss: 0.002680, mae: 0.055272, mean_q: 0.090578
 71831/100000: episode: 1354, duration: 0.134s, episode steps: 26, steps per second: 195, episode reward: 8.472, mean reward: 0.326 [0.193, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.441, 10.100], loss: 0.002648, mae: 0.053908, mean_q: 0.097047
 71851/100000: episode: 1355, duration: 0.093s, episode steps: 20, steps per second: 215, episode reward: 5.921, mean reward: 0.296 [0.201, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.437, 10.100], loss: 0.002678, mae: 0.055306, mean_q: 0.058379
 71871/100000: episode: 1356, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 5.628, mean reward: 0.281 [0.205, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.291, 10.100], loss: 0.002784, mae: 0.054225, mean_q: 0.007696
 71897/100000: episode: 1357, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 6.553, mean reward: 0.252 [0.104, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.609, 10.100], loss: 0.002700, mae: 0.054342, mean_q: 0.024545
 71923/100000: episode: 1358, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 6.488, mean reward: 0.250 [0.115, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.050, 10.100], loss: 0.003085, mae: 0.058005, mean_q: 0.098732
 71949/100000: episode: 1359, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 5.121, mean reward: 0.197 [0.030, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.185, 10.100], loss: 0.002624, mae: 0.054132, mean_q: 0.123374
 71972/100000: episode: 1360, duration: 0.109s, episode steps: 23, steps per second: 212, episode reward: 4.647, mean reward: 0.202 [0.077, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.874, 10.100], loss: 0.002782, mae: 0.057313, mean_q: 0.103963
 71991/100000: episode: 1361, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 7.229, mean reward: 0.380 [0.257, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.509, 10.100], loss: 0.002959, mae: 0.058438, mean_q: 0.058087
 72010/100000: episode: 1362, duration: 0.093s, episode steps: 19, steps per second: 205, episode reward: 5.887, mean reward: 0.310 [0.143, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.642, 10.100], loss: 0.002692, mae: 0.054680, mean_q: 0.131250
 72029/100000: episode: 1363, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 5.410, mean reward: 0.285 [0.149, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.179, 10.100], loss: 0.002919, mae: 0.055377, mean_q: 0.093993
[Info] 200-TH LEVEL FOUND: 0.7003390789031982, Considering 10/90 traces
 72055/100000: episode: 1364, duration: 3.970s, episode steps: 26, steps per second: 7, episode reward: 10.098, mean reward: 0.388 [0.198, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.381, 10.100], loss: 0.005370, mae: 0.068530, mean_q: 0.115321
 72062/100000: episode: 1365, duration: 0.036s, episode steps: 7, steps per second: 197, episode reward: 2.567, mean reward: 0.367 [0.326, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.365, 10.100], loss: 0.006247, mae: 0.087215, mean_q: 0.118059
 72073/100000: episode: 1366, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 3.641, mean reward: 0.331 [0.197, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.407, 10.100], loss: 0.004935, mae: 0.072708, mean_q: 0.039239
 72084/100000: episode: 1367, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 3.756, mean reward: 0.341 [0.300, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.563, 10.100], loss: 0.003065, mae: 0.061601, mean_q: 0.148228
 72096/100000: episode: 1368, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 4.652, mean reward: 0.388 [0.309, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.365, 10.100], loss: 0.003823, mae: 0.062336, mean_q: 0.107818
 72107/100000: episode: 1369, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 5.263, mean reward: 0.478 [0.439, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.396, 10.100], loss: 0.003704, mae: 0.061048, mean_q: 0.186413
 72118/100000: episode: 1370, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 3.827, mean reward: 0.348 [0.317, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.282, 10.100], loss: 0.002734, mae: 0.057535, mean_q: 0.108705
 72131/100000: episode: 1371, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 4.674, mean reward: 0.360 [0.265, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.380, 10.100], loss: 0.003331, mae: 0.060595, mean_q: 0.143740
 72140/100000: episode: 1372, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 3.095, mean reward: 0.344 [0.314, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.337, 10.100], loss: 0.002878, mae: 0.060544, mean_q: 0.116693
 72151/100000: episode: 1373, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 4.245, mean reward: 0.386 [0.296, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.349, 10.100], loss: 0.002595, mae: 0.053240, mean_q: 0.037311
 72162/100000: episode: 1374, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 3.717, mean reward: 0.338 [0.297, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.828, 10.100], loss: 0.002560, mae: 0.053623, mean_q: 0.155547
 72172/100000: episode: 1375, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 4.072, mean reward: 0.407 [0.267, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.350, 10.100], loss: 0.003158, mae: 0.059561, mean_q: 0.086026
 72183/100000: episode: 1376, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 3.976, mean reward: 0.361 [0.320, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.430, 10.100], loss: 0.002730, mae: 0.056652, mean_q: 0.097941
 72190/100000: episode: 1377, duration: 0.035s, episode steps: 7, steps per second: 200, episode reward: 2.389, mean reward: 0.341 [0.283, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.268, 10.100], loss: 0.002502, mae: 0.054685, mean_q: 0.078101
 72201/100000: episode: 1378, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 4.604, mean reward: 0.419 [0.376, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.309, 10.100], loss: 0.003250, mae: 0.058632, mean_q: 0.107304
 72212/100000: episode: 1379, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 3.988, mean reward: 0.363 [0.285, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-1.970, 10.100], loss: 0.002973, mae: 0.055569, mean_q: 0.060596
 72221/100000: episode: 1380, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 3.840, mean reward: 0.427 [0.362, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.519, 10.100], loss: 0.002965, mae: 0.058217, mean_q: 0.210122
 72232/100000: episode: 1381, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 4.571, mean reward: 0.416 [0.343, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.481, 10.100], loss: 0.002606, mae: 0.054255, mean_q: 0.131800
 72245/100000: episode: 1382, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 5.228, mean reward: 0.402 [0.318, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.441, 10.100], loss: 0.002407, mae: 0.051717, mean_q: 0.080465
 72256/100000: episode: 1383, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 3.998, mean reward: 0.363 [0.314, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.350, 10.100], loss: 0.002648, mae: 0.055749, mean_q: 0.172584
 72267/100000: episode: 1384, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 5.147, mean reward: 0.468 [0.335, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.319, 10.100], loss: 0.002458, mae: 0.053086, mean_q: 0.127597
 72274/100000: episode: 1385, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 2.540, mean reward: 0.363 [0.290, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.427, 10.100], loss: 0.002890, mae: 0.056359, mean_q: 0.215602
 72284/100000: episode: 1386, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 3.604, mean reward: 0.360 [0.305, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.492, 10.100], loss: 0.002639, mae: 0.054138, mean_q: 0.151698
 72295/100000: episode: 1387, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 4.612, mean reward: 0.419 [0.362, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.761, 10.100], loss: 0.002442, mae: 0.051202, mean_q: 0.103000
 72305/100000: episode: 1388, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 3.866, mean reward: 0.387 [0.326, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.423, 10.100], loss: 0.002868, mae: 0.057210, mean_q: 0.152653
 72317/100000: episode: 1389, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 4.843, mean reward: 0.404 [0.334, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.296, 10.100], loss: 0.002954, mae: 0.056901, mean_q: 0.153598
 72330/100000: episode: 1390, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 4.128, mean reward: 0.318 [0.271, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.381, 10.100], loss: 0.002827, mae: 0.056816, mean_q: 0.198586
 72341/100000: episode: 1391, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 4.025, mean reward: 0.366 [0.322, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.693, 10.100], loss: 0.003094, mae: 0.057167, mean_q: 0.176301
 72353/100000: episode: 1392, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 3.508, mean reward: 0.292 [0.202, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.171, 10.100], loss: 0.002828, mae: 0.056087, mean_q: 0.199727
 72362/100000: episode: 1393, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 2.891, mean reward: 0.321 [0.267, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.394, 10.100], loss: 0.003149, mae: 0.058651, mean_q: 0.065907
 72373/100000: episode: 1394, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 5.281, mean reward: 0.480 [0.462, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.435, 10.100], loss: 0.002607, mae: 0.056248, mean_q: 0.213500
 72385/100000: episode: 1395, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 3.309, mean reward: 0.276 [0.142, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.617, 10.100], loss: 0.002532, mae: 0.054498, mean_q: 0.141288
 72396/100000: episode: 1396, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 3.556, mean reward: 0.323 [0.265, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.263, 10.100], loss: 0.002601, mae: 0.055357, mean_q: 0.206051
 72407/100000: episode: 1397, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 4.728, mean reward: 0.430 [0.375, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.528, 10.100], loss: 0.002661, mae: 0.056055, mean_q: 0.195266
 72418/100000: episode: 1398, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 4.509, mean reward: 0.410 [0.327, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.464, 10.100], loss: 0.002481, mae: 0.053601, mean_q: 0.142754
 72429/100000: episode: 1399, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 4.078, mean reward: 0.371 [0.278, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.342, 10.100], loss: 0.002825, mae: 0.055850, mean_q: 0.243701
 72440/100000: episode: 1400, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 3.123, mean reward: 0.284 [0.235, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.295, 10.100], loss: 0.002570, mae: 0.054411, mean_q: 0.223605
 72453/100000: episode: 1401, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 4.458, mean reward: 0.343 [0.292, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.430, 10.100], loss: 0.002376, mae: 0.053165, mean_q: 0.214641
 72464/100000: episode: 1402, duration: 0.062s, episode steps: 11, steps per second: 179, episode reward: 5.798, mean reward: 0.527 [0.479, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.489, 10.100], loss: 0.002893, mae: 0.055741, mean_q: 0.151222
 72473/100000: episode: 1403, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 2.494, mean reward: 0.277 [0.231, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.588, 10.100], loss: 0.003154, mae: 0.058231, mean_q: 0.226815
 72486/100000: episode: 1404, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 5.250, mean reward: 0.404 [0.340, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.274, 10.100], loss: 0.002769, mae: 0.056777, mean_q: 0.195487
 72497/100000: episode: 1405, duration: 0.053s, episode steps: 11, steps per second: 206, episode reward: 4.769, mean reward: 0.434 [0.330, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.232, 10.100], loss: 0.002997, mae: 0.060399, mean_q: 0.124480
 72508/100000: episode: 1406, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 4.188, mean reward: 0.381 [0.338, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.360, 10.100], loss: 0.003136, mae: 0.059802, mean_q: 0.181010
 72515/100000: episode: 1407, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 2.807, mean reward: 0.401 [0.345, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.216, 10.100], loss: 0.002958, mae: 0.060080, mean_q: 0.188584
 72522/100000: episode: 1408, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 2.719, mean reward: 0.388 [0.235, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.273, 10.100], loss: 0.002579, mae: 0.053838, mean_q: 0.164223
 72529/100000: episode: 1409, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 2.455, mean reward: 0.351 [0.291, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.314, 10.100], loss: 0.003133, mae: 0.061926, mean_q: 0.265714
 72540/100000: episode: 1410, duration: 0.053s, episode steps: 11, steps per second: 207, episode reward: 4.313, mean reward: 0.392 [0.297, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.898, 10.100], loss: 0.002544, mae: 0.052872, mean_q: 0.194823
 72551/100000: episode: 1411, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 4.044, mean reward: 0.368 [0.341, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.481, 10.100], loss: 0.003129, mae: 0.058540, mean_q: 0.199974
 72558/100000: episode: 1412, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 2.835, mean reward: 0.405 [0.374, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.451, 10.100], loss: 0.002822, mae: 0.057195, mean_q: 0.141405
 72568/100000: episode: 1413, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 3.949, mean reward: 0.395 [0.299, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.496, 10.100], loss: 0.003376, mae: 0.060798, mean_q: 0.198599
 72575/100000: episode: 1414, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 2.474, mean reward: 0.353 [0.321, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.246, 10.100], loss: 0.002126, mae: 0.050406, mean_q: 0.218829
 72585/100000: episode: 1415, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 4.406, mean reward: 0.441 [0.369, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.719, 10.100], loss: 0.002797, mae: 0.055217, mean_q: 0.178865
 72596/100000: episode: 1416, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 5.170, mean reward: 0.470 [0.426, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.282, 10.100], loss: 0.002643, mae: 0.056195, mean_q: 0.278953
 72607/100000: episode: 1417, duration: 0.054s, episode steps: 11, steps per second: 202, episode reward: 3.868, mean reward: 0.352 [0.278, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.403, 10.100], loss: 0.002636, mae: 0.053570, mean_q: 0.185289
 72614/100000: episode: 1418, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 2.441, mean reward: 0.349 [0.293, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.376, 10.100], loss: 0.002591, mae: 0.053059, mean_q: 0.155470
 72625/100000: episode: 1419, duration: 0.053s, episode steps: 11, steps per second: 207, episode reward: 4.222, mean reward: 0.384 [0.332, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.730, 10.100], loss: 0.003210, mae: 0.058832, mean_q: 0.135905
 72634/100000: episode: 1420, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 4.190, mean reward: 0.466 [0.371, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.470, 10.100], loss: 0.003235, mae: 0.058156, mean_q: 0.204787
 72645/100000: episode: 1421, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 5.891, mean reward: 0.536 [0.492, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.697, 10.100], loss: 0.004213, mae: 0.071188, mean_q: 0.198325
 72658/100000: episode: 1422, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 5.082, mean reward: 0.391 [0.323, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.253, 10.100], loss: 0.002963, mae: 0.059790, mean_q: 0.215445
 72667/100000: episode: 1423, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 3.116, mean reward: 0.346 [0.309, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.331, 10.100], loss: 0.003107, mae: 0.058356, mean_q: 0.233384
 72678/100000: episode: 1424, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 4.329, mean reward: 0.394 [0.269, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-1.449, 10.100], loss: 0.002977, mae: 0.058752, mean_q: 0.308663
 72689/100000: episode: 1425, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 5.001, mean reward: 0.455 [0.357, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.403, 10.100], loss: 0.002871, mae: 0.055457, mean_q: 0.237402
 72696/100000: episode: 1426, duration: 0.035s, episode steps: 7, steps per second: 199, episode reward: 3.047, mean reward: 0.435 [0.346, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.876, 10.100], loss: 0.002698, mae: 0.055376, mean_q: 0.362828
 72707/100000: episode: 1427, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 4.238, mean reward: 0.385 [0.269, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.316, 10.100], loss: 0.002965, mae: 0.058068, mean_q: 0.237245
 72714/100000: episode: 1428, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 3.122, mean reward: 0.446 [0.353, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.189, 10.100], loss: 0.002980, mae: 0.060492, mean_q: 0.274393
 72725/100000: episode: 1429, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 5.233, mean reward: 0.476 [0.397, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.473, 10.100], loss: 0.002881, mae: 0.055939, mean_q: 0.171420
 72737/100000: episode: 1430, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 4.716, mean reward: 0.393 [0.351, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.291, 10.100], loss: 0.003033, mae: 0.059694, mean_q: 0.207207
 72750/100000: episode: 1431, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 5.376, mean reward: 0.414 [0.320, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.218, 10.100], loss: 0.003262, mae: 0.061186, mean_q: 0.210994
 72757/100000: episode: 1432, duration: 0.045s, episode steps: 7, steps per second: 154, episode reward: 2.369, mean reward: 0.338 [0.248, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.260, 10.100], loss: 0.003360, mae: 0.058227, mean_q: 0.193884
 72766/100000: episode: 1433, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 3.367, mean reward: 0.374 [0.312, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.517, 10.100], loss: 0.002611, mae: 0.053097, mean_q: 0.164050
 72775/100000: episode: 1434, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 4.051, mean reward: 0.450 [0.359, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.704, 10.100], loss: 0.003016, mae: 0.057667, mean_q: 0.287792
 72784/100000: episode: 1435, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 3.395, mean reward: 0.377 [0.302, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.405, 10.100], loss: 0.002906, mae: 0.057760, mean_q: 0.264636
 72791/100000: episode: 1436, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 2.378, mean reward: 0.340 [0.303, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.285, 10.100], loss: 0.003200, mae: 0.060929, mean_q: 0.265943
 72798/100000: episode: 1437, duration: 0.039s, episode steps: 7, steps per second: 182, episode reward: 2.824, mean reward: 0.403 [0.358, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-1.079, 10.100], loss: 0.003059, mae: 0.059516, mean_q: 0.249247
 72809/100000: episode: 1438, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 4.465, mean reward: 0.406 [0.340, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.309, 10.100], loss: 0.002970, mae: 0.059862, mean_q: 0.282838
 72818/100000: episode: 1439, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 3.653, mean reward: 0.406 [0.319, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.553, 10.100], loss: 0.002839, mae: 0.057253, mean_q: 0.279386
 72829/100000: episode: 1440, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 4.245, mean reward: 0.386 [0.321, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.320, 10.100], loss: 0.003007, mae: 0.060043, mean_q: 0.264453
 72840/100000: episode: 1441, duration: 0.053s, episode steps: 11, steps per second: 207, episode reward: 4.850, mean reward: 0.441 [0.363, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-1.344, 10.100], loss: 0.003394, mae: 0.061836, mean_q: 0.322878
 72847/100000: episode: 1442, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 2.750, mean reward: 0.393 [0.329, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.401, 10.100], loss: 0.007074, mae: 0.063836, mean_q: 0.301936
 72856/100000: episode: 1443, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 3.425, mean reward: 0.381 [0.354, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.265, 10.100], loss: 0.004108, mae: 0.066927, mean_q: 0.293810
 72868/100000: episode: 1444, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 4.502, mean reward: 0.375 [0.334, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.500, 10.100], loss: 0.004978, mae: 0.069809, mean_q: 0.189168
 72880/100000: episode: 1445, duration: 0.065s, episode steps: 12, steps per second: 183, episode reward: 4.986, mean reward: 0.416 [0.333, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.617, 10.100], loss: 0.004010, mae: 0.070108, mean_q: 0.304799
 72889/100000: episode: 1446, duration: 0.044s, episode steps: 9, steps per second: 204, episode reward: 3.180, mean reward: 0.353 [0.319, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.260, 10.100], loss: 0.004915, mae: 0.077344, mean_q: 0.276718
 72896/100000: episode: 1447, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 2.755, mean reward: 0.394 [0.319, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.270, 10.100], loss: 0.003698, mae: 0.068399, mean_q: 0.294383
 72907/100000: episode: 1448, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 4.516, mean reward: 0.411 [0.340, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.333, 10.100], loss: 0.002975, mae: 0.056962, mean_q: 0.295966
 72916/100000: episode: 1449, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 3.887, mean reward: 0.432 [0.337, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.530, 10.100], loss: 0.003243, mae: 0.059755, mean_q: 0.314913
 72927/100000: episode: 1450, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 5.440, mean reward: 0.495 [0.432, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.417, 10.100], loss: 0.003237, mae: 0.062546, mean_q: 0.284896
 72939/100000: episode: 1451, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 4.228, mean reward: 0.352 [0.236, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.579, 10.100], loss: 0.003396, mae: 0.063272, mean_q: 0.316807
 72951/100000: episode: 1452, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 4.279, mean reward: 0.357 [0.279, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.205, 10.100], loss: 0.002848, mae: 0.057253, mean_q: 0.218363
 72962/100000: episode: 1453, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 4.514, mean reward: 0.410 [0.371, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.512, 10.100], loss: 0.002713, mae: 0.058076, mean_q: 0.323692
[Info] 300-TH LEVEL FOUND: 0.8368968367576599, Considering 10/90 traces
 72971/100000: episode: 1454, duration: 3.858s, episode steps: 9, steps per second: 2, episode reward: 2.965, mean reward: 0.329 [0.183, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.293, 10.100], loss: 0.003309, mae: 0.064141, mean_q: 0.312362
 72979/100000: episode: 1455, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 4.012, mean reward: 0.502 [0.460, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.478, 10.100], loss: 0.003002, mae: 0.061550, mean_q: 0.383227
 72988/100000: episode: 1456, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 4.735, mean reward: 0.526 [0.500, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.870, 10.100], loss: 0.002324, mae: 0.051203, mean_q: 0.323942
 72997/100000: episode: 1457, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 4.112, mean reward: 0.457 [0.420, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.455, 10.100], loss: 0.002628, mae: 0.055243, mean_q: 0.310884
 73005/100000: episode: 1458, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 3.436, mean reward: 0.429 [0.381, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.261, 10.100], loss: 0.002763, mae: 0.056982, mean_q: 0.267472
 73012/100000: episode: 1459, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 3.374, mean reward: 0.482 [0.430, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.548, 10.100], loss: 0.003154, mae: 0.059580, mean_q: 0.275433
 73022/100000: episode: 1460, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 5.052, mean reward: 0.505 [0.463, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.533, 10.100], loss: 0.002638, mae: 0.054215, mean_q: 0.266257
 73030/100000: episode: 1461, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 3.945, mean reward: 0.493 [0.455, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.905, 10.100], loss: 0.003312, mae: 0.060594, mean_q: 0.291261
 73038/100000: episode: 1462, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 3.531, mean reward: 0.441 [0.400, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.321, 10.100], loss: 0.002857, mae: 0.057682, mean_q: 0.292288
 73047/100000: episode: 1463, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 4.274, mean reward: 0.475 [0.422, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.363, 10.100], loss: 0.003144, mae: 0.062757, mean_q: 0.369801
 73056/100000: episode: 1464, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 3.726, mean reward: 0.414 [0.293, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.409, 10.100], loss: 0.002694, mae: 0.058399, mean_q: 0.328008
 73063/100000: episode: 1465, duration: 0.045s, episode steps: 7, steps per second: 154, episode reward: 2.902, mean reward: 0.415 [0.366, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.521, 10.100], loss: 0.002822, mae: 0.058254, mean_q: 0.328542
 73073/100000: episode: 1466, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 4.977, mean reward: 0.498 [0.425, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-1.283, 10.100], loss: 0.002689, mae: 0.057093, mean_q: 0.278180
 73082/100000: episode: 1467, duration: 0.044s, episode steps: 9, steps per second: 202, episode reward: 4.549, mean reward: 0.505 [0.419, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.682, 10.100], loss: 0.003462, mae: 0.061968, mean_q: 0.302254
 73090/100000: episode: 1468, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 3.919, mean reward: 0.490 [0.459, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.702, 10.100], loss: 0.002323, mae: 0.051324, mean_q: 0.328204
 73099/100000: episode: 1469, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 4.524, mean reward: 0.503 [0.475, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.750, 10.100], loss: 0.003027, mae: 0.059831, mean_q: 0.330286
 73107/100000: episode: 1470, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 4.100, mean reward: 0.512 [0.467, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.538, 10.100], loss: 0.002557, mae: 0.053878, mean_q: 0.347015
 73116/100000: episode: 1471, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 4.241, mean reward: 0.471 [0.391, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.316, 10.100], loss: 0.003000, mae: 0.059206, mean_q: 0.277827
 73124/100000: episode: 1472, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 3.836, mean reward: 0.479 [0.406, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.347, 10.100], loss: 0.002735, mae: 0.056192, mean_q: 0.361698
 73133/100000: episode: 1473, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 4.122, mean reward: 0.458 [0.397, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.271, 10.100], loss: 0.003038, mae: 0.057699, mean_q: 0.286095
 73142/100000: episode: 1474, duration: 0.044s, episode steps: 9, steps per second: 204, episode reward: 4.055, mean reward: 0.451 [0.364, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.287, 10.100], loss: 0.003006, mae: 0.060004, mean_q: 0.368149
 73151/100000: episode: 1475, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 3.769, mean reward: 0.419 [0.377, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.624, 10.100], loss: 0.002677, mae: 0.056789, mean_q: 0.344474
 73157/100000: episode: 1476, duration: 0.031s, episode steps: 6, steps per second: 194, episode reward: 2.039, mean reward: 0.340 [0.304, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.534, 10.100], loss: 0.002576, mae: 0.058265, mean_q: 0.391992
 73164/100000: episode: 1477, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 3.135, mean reward: 0.448 [0.417, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.419, 10.100], loss: 0.002752, mae: 0.058226, mean_q: 0.412116
 73171/100000: episode: 1478, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 3.247, mean reward: 0.464 [0.436, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.541, 10.100], loss: 0.002552, mae: 0.055937, mean_q: 0.395545
 73180/100000: episode: 1479, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 4.142, mean reward: 0.460 [0.412, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.313, 10.100], loss: 0.002861, mae: 0.058195, mean_q: 0.320283
 73190/100000: episode: 1480, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 5.051, mean reward: 0.505 [0.389, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.363, 10.100], loss: 0.002534, mae: 0.056309, mean_q: 0.368073
 73196/100000: episode: 1481, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 2.898, mean reward: 0.483 [0.434, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.381, 10.100], loss: 0.002581, mae: 0.056977, mean_q: 0.360677
 73204/100000: episode: 1482, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 3.618, mean reward: 0.452 [0.405, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.877, 10.100], loss: 0.002822, mae: 0.053871, mean_q: 0.230957
 73210/100000: episode: 1483, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 3.378, mean reward: 0.563 [0.420, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.336, 10.100], loss: 0.003737, mae: 0.061027, mean_q: 0.312616
 73220/100000: episode: 1484, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 3.849, mean reward: 0.385 [0.246, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.229, 10.100], loss: 0.002673, mae: 0.057863, mean_q: 0.420628
 73230/100000: episode: 1485, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 4.884, mean reward: 0.488 [0.400, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.335, 10.100], loss: 0.002536, mae: 0.052304, mean_q: 0.331809
 73239/100000: episode: 1486, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 4.226, mean reward: 0.470 [0.380, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.420, 10.100], loss: 0.002365, mae: 0.053006, mean_q: 0.399155
 73246/100000: episode: 1487, duration: 0.035s, episode steps: 7, steps per second: 197, episode reward: 2.534, mean reward: 0.362 [0.334, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.408, 10.100], loss: 0.002359, mae: 0.051848, mean_q: 0.333212
 73252/100000: episode: 1488, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 2.920, mean reward: 0.487 [0.465, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.381, 10.100], loss: 0.002408, mae: 0.055462, mean_q: 0.399308
 73261/100000: episode: 1489, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 4.087, mean reward: 0.454 [0.357, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.184, 10.100], loss: 0.003129, mae: 0.060847, mean_q: 0.381468
 73271/100000: episode: 1490, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 3.865, mean reward: 0.387 [0.311, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.399, 10.100], loss: 0.002553, mae: 0.056696, mean_q: 0.371069
 73280/100000: episode: 1491, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 4.356, mean reward: 0.484 [0.438, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.342, 10.100], loss: 0.002789, mae: 0.057600, mean_q: 0.370349
 73288/100000: episode: 1492, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 4.174, mean reward: 0.522 [0.483, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.896, 10.100], loss: 0.002531, mae: 0.054812, mean_q: 0.393948
 73294/100000: episode: 1493, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 2.821, mean reward: 0.470 [0.426, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.420, 10.100], loss: 0.004354, mae: 0.069800, mean_q: 0.412741
 73300/100000: episode: 1494, duration: 0.031s, episode steps: 6, steps per second: 193, episode reward: 2.888, mean reward: 0.481 [0.450, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.423, 10.100], loss: 0.003169, mae: 0.062049, mean_q: 0.430673
 73308/100000: episode: 1495, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 3.255, mean reward: 0.407 [0.331, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.399, 10.100], loss: 0.004119, mae: 0.067035, mean_q: 0.401663
 73315/100000: episode: 1496, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 3.203, mean reward: 0.458 [0.386, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.408, 10.100], loss: 0.003053, mae: 0.060621, mean_q: 0.398584
 73321/100000: episode: 1497, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 2.822, mean reward: 0.470 [0.420, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.293, 10.100], loss: 0.003082, mae: 0.060317, mean_q: 0.405044
 73329/100000: episode: 1498, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 3.383, mean reward: 0.423 [0.381, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.303, 10.100], loss: 0.003095, mae: 0.061654, mean_q: 0.372721
 73336/100000: episode: 1499, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 2.454, mean reward: 0.351 [0.299, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.449, 10.100], loss: 0.002848, mae: 0.058792, mean_q: 0.418757
 73345/100000: episode: 1500, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 4.558, mean reward: 0.506 [0.461, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.466, 10.100], loss: 0.003173, mae: 0.062457, mean_q: 0.329017
 73354/100000: episode: 1501, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 3.922, mean reward: 0.436 [0.360, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.227, 10.100], loss: 0.002682, mae: 0.057637, mean_q: 0.427467
 73360/100000: episode: 1502, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 2.522, mean reward: 0.420 [0.394, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.354, 10.100], loss: 0.002978, mae: 0.061404, mean_q: 0.400707
 73370/100000: episode: 1503, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 4.751, mean reward: 0.475 [0.415, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.211, 10.100], loss: 0.003280, mae: 0.060875, mean_q: 0.375348
 73378/100000: episode: 1504, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 3.577, mean reward: 0.447 [0.359, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.498, 10.100], loss: 0.002761, mae: 0.058623, mean_q: 0.393456
 73387/100000: episode: 1505, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 3.847, mean reward: 0.427 [0.360, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.252, 10.100], loss: 0.003340, mae: 0.061226, mean_q: 0.452289
 73396/100000: episode: 1506, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 4.863, mean reward: 0.540 [0.455, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.477, 10.100], loss: 0.002831, mae: 0.059720, mean_q: 0.397370
 73405/100000: episode: 1507, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 4.318, mean reward: 0.480 [0.453, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.382, 10.100], loss: 0.002843, mae: 0.058947, mean_q: 0.386455
 73413/100000: episode: 1508, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 4.120, mean reward: 0.515 [0.474, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.473, 10.100], loss: 0.003032, mae: 0.060518, mean_q: 0.431407
 73422/100000: episode: 1509, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 3.398, mean reward: 0.378 [0.294, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.382, 10.100], loss: 0.003010, mae: 0.057779, mean_q: 0.398599
 73431/100000: episode: 1510, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 4.208, mean reward: 0.468 [0.416, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.606, 10.100], loss: 0.002635, mae: 0.055929, mean_q: 0.388592
 73439/100000: episode: 1511, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 4.393, mean reward: 0.549 [0.515, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.448, 10.100], loss: 0.002791, mae: 0.056572, mean_q: 0.293963
 73448/100000: episode: 1512, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 4.375, mean reward: 0.486 [0.396, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.235, 10.100], loss: 0.002841, mae: 0.057495, mean_q: 0.404312
 73457/100000: episode: 1513, duration: 0.044s, episode steps: 9, steps per second: 203, episode reward: 4.288, mean reward: 0.476 [0.414, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.343, 10.100], loss: 0.003057, mae: 0.061272, mean_q: 0.447398
 73464/100000: episode: 1514, duration: 0.035s, episode steps: 7, steps per second: 199, episode reward: 3.460, mean reward: 0.494 [0.438, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.229, 10.100], loss: 0.002178, mae: 0.052786, mean_q: 0.507177
 73471/100000: episode: 1515, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 3.436, mean reward: 0.491 [0.461, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.533, 10.100], loss: 0.002850, mae: 0.059063, mean_q: 0.461811
 73481/100000: episode: 1516, duration: 0.049s, episode steps: 10, steps per second: 205, episode reward: 5.091, mean reward: 0.509 [0.417, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.443, 10.100], loss: 0.003213, mae: 0.062659, mean_q: 0.442154
 73490/100000: episode: 1517, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 3.612, mean reward: 0.401 [0.363, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.271, 10.100], loss: 0.002901, mae: 0.057969, mean_q: 0.477920
 73496/100000: episode: 1518, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 2.758, mean reward: 0.460 [0.361, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.360, 10.100], loss: 0.002466, mae: 0.054561, mean_q: 0.494038
 73505/100000: episode: 1519, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 4.698, mean reward: 0.522 [0.469, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.568, 10.100], loss: 0.002702, mae: 0.059117, mean_q: 0.450131
 73511/100000: episode: 1520, duration: 0.031s, episode steps: 6, steps per second: 195, episode reward: 2.975, mean reward: 0.496 [0.470, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.391, 10.100], loss: 0.002774, mae: 0.058200, mean_q: 0.403395
 73520/100000: episode: 1521, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 4.243, mean reward: 0.471 [0.390, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.334, 10.100], loss: 0.002896, mae: 0.059718, mean_q: 0.456348
 73529/100000: episode: 1522, duration: 0.044s, episode steps: 9, steps per second: 203, episode reward: 4.405, mean reward: 0.489 [0.419, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.176, 10.100], loss: 0.002556, mae: 0.057283, mean_q: 0.462195
 73536/100000: episode: 1523, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 2.915, mean reward: 0.416 [0.348, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.455, 10.100], loss: 0.003769, mae: 0.069240, mean_q: 0.446496
 73542/100000: episode: 1524, duration: 0.040s, episode steps: 6, steps per second: 149, episode reward: 3.072, mean reward: 0.512 [0.471, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.507, 10.100], loss: 0.002779, mae: 0.058469, mean_q: 0.424678
 73548/100000: episode: 1525, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 2.454, mean reward: 0.409 [0.385, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.437, 10.100], loss: 0.003681, mae: 0.064317, mean_q: 0.436053
 73555/100000: episode: 1526, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 3.127, mean reward: 0.447 [0.405, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.427, 10.100], loss: 0.003238, mae: 0.063164, mean_q: 0.492223
 73561/100000: episode: 1527, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 2.592, mean reward: 0.432 [0.339, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.536, 10.100], loss: 0.002513, mae: 0.056925, mean_q: 0.456682
 73567/100000: episode: 1528, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 2.684, mean reward: 0.447 [0.396, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.421, 10.100], loss: 0.002783, mae: 0.061601, mean_q: 0.452915
 73576/100000: episode: 1529, duration: 0.052s, episode steps: 9, steps per second: 171, episode reward: 3.341, mean reward: 0.371 [0.294, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.230, 10.100], loss: 0.002711, mae: 0.057067, mean_q: 0.425260
 73585/100000: episode: 1530, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 4.075, mean reward: 0.453 [0.387, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.492, 10.100], loss: 0.002893, mae: 0.058714, mean_q: 0.426684
 73594/100000: episode: 1531, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 3.504, mean reward: 0.389 [0.307, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.357, 10.100], loss: 0.003258, mae: 0.062272, mean_q: 0.427404
 73603/100000: episode: 1532, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 3.745, mean reward: 0.416 [0.313, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.332, 10.100], loss: 0.002926, mae: 0.059531, mean_q: 0.458597
 73609/100000: episode: 1533, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 2.930, mean reward: 0.488 [0.463, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.369, 10.100], loss: 0.002532, mae: 0.054232, mean_q: 0.437909
 73615/100000: episode: 1534, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 2.216, mean reward: 0.369 [0.307, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.281, 10.100], loss: 0.002602, mae: 0.055305, mean_q: 0.401296
 73624/100000: episode: 1535, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 4.078, mean reward: 0.453 [0.402, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.367, 10.100], loss: 0.002281, mae: 0.053433, mean_q: 0.503294
 73634/100000: episode: 1536, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 4.488, mean reward: 0.449 [0.276, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.208, 10.100], loss: 0.002916, mae: 0.059166, mean_q: 0.496647
 73643/100000: episode: 1537, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 4.702, mean reward: 0.522 [0.484, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.498, 10.100], loss: 0.002836, mae: 0.056567, mean_q: 0.475048
 73650/100000: episode: 1538, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 3.515, mean reward: 0.502 [0.421, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.956, 10.100], loss: 0.003009, mae: 0.060053, mean_q: 0.537600
 73657/100000: episode: 1539, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 2.824, mean reward: 0.403 [0.362, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.530, 10.100], loss: 0.004206, mae: 0.069457, mean_q: 0.448468
 73665/100000: episode: 1540, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 3.019, mean reward: 0.377 [0.293, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.366, 10.100], loss: 0.004046, mae: 0.072297, mean_q: 0.533471
 73671/100000: episode: 1541, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 3.278, mean reward: 0.546 [0.515, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.522, 10.100], loss: 0.003000, mae: 0.060739, mean_q: 0.484800
 73677/100000: episode: 1542, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 3.050, mean reward: 0.508 [0.463, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.243, 10.100], loss: 0.003658, mae: 0.066679, mean_q: 0.399812
 73685/100000: episode: 1543, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 3.353, mean reward: 0.419 [0.340, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.896, 10.100], loss: 0.003123, mae: 0.062584, mean_q: 0.519870
[Info] 400-TH LEVEL FOUND: 0.9880768060684204, Considering 10/90 traces
 73692/100000: episode: 1544, duration: 3.867s, episode steps: 7, steps per second: 2, episode reward: 3.315, mean reward: 0.474 [0.383, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.538, 10.100], loss: 0.003445, mae: 0.062675, mean_q: 0.448992
 73697/100000: episode: 1545, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 2.467, mean reward: 0.493 [0.415, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.523, 10.100], loss: 0.003422, mae: 0.063253, mean_q: 0.537117
 73704/100000: episode: 1546, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 2.877, mean reward: 0.411 [0.341, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.374, 10.100], loss: 0.002955, mae: 0.060379, mean_q: 0.437826
 73709/100000: episode: 1547, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 2.659, mean reward: 0.532 [0.501, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.628, 10.100], loss: 0.003344, mae: 0.064543, mean_q: 0.508536
 73715/100000: episode: 1548, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 3.077, mean reward: 0.513 [0.485, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.561, 10.100], loss: 0.002992, mae: 0.061467, mean_q: 0.516624
[Info] FALSIFICATION!
 73718/100000: episode: 1549, duration: 0.023s, episode steps: 3, steps per second: 129, episode reward: 11.184, mean reward: 3.728 [0.585, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.339, 10.082], loss: 0.002378, mae: 0.054308, mean_q: 0.474103
 73818/100000: episode: 1550, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -20.574, mean reward: -0.206 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.311, 10.098], loss: 0.031545, mae: 0.084117, mean_q: 0.499843
 73918/100000: episode: 1551, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -14.622, mean reward: -0.146 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.591, 10.098], loss: 0.005998, mae: 0.074578, mean_q: 0.476970
 74018/100000: episode: 1552, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -17.541, mean reward: -0.175 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.903, 10.239], loss: 0.019293, mae: 0.084823, mean_q: 0.509009
 74118/100000: episode: 1553, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.828, mean reward: -0.178 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.537, 10.194], loss: 0.004695, mae: 0.071361, mean_q: 0.494710
 74218/100000: episode: 1554, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.611, mean reward: -0.166 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.339, 10.098], loss: 0.031831, mae: 0.083570, mean_q: 0.500000
 74318/100000: episode: 1555, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -14.005, mean reward: -0.140 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.611, 10.218], loss: 0.004452, mae: 0.069491, mean_q: 0.484560
 74418/100000: episode: 1556, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -15.010, mean reward: -0.150 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.487, 10.098], loss: 0.004439, mae: 0.069781, mean_q: 0.475524
 74518/100000: episode: 1557, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: -12.107, mean reward: -0.121 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.571, 10.098], loss: 0.004409, mae: 0.070042, mean_q: 0.465284
 74618/100000: episode: 1558, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -9.337, mean reward: -0.093 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.814, 10.488], loss: 0.004239, mae: 0.068370, mean_q: 0.452986
 74718/100000: episode: 1559, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -18.552, mean reward: -0.186 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.319, 10.280], loss: 0.044961, mae: 0.093587, mean_q: 0.448702
 74818/100000: episode: 1560, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -9.641, mean reward: -0.096 [-1.000, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.901, 10.098], loss: 0.018735, mae: 0.083105, mean_q: 0.435724
 74918/100000: episode: 1561, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.617, mean reward: -0.176 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.658, 10.125], loss: 0.018168, mae: 0.078706, mean_q: 0.406462
 75018/100000: episode: 1562, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.073, mean reward: -0.181 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.646, 10.115], loss: 0.031619, mae: 0.083153, mean_q: 0.409278
 75118/100000: episode: 1563, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -15.104, mean reward: -0.151 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.515, 10.129], loss: 0.005235, mae: 0.074136, mean_q: 0.361312
 75218/100000: episode: 1564, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -16.686, mean reward: -0.167 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.312, 10.200], loss: 0.004399, mae: 0.068123, mean_q: 0.336656
 75318/100000: episode: 1565, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.852, mean reward: -0.189 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.130, 10.363], loss: 0.004052, mae: 0.066102, mean_q: 0.329129
 75418/100000: episode: 1566, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -14.334, mean reward: -0.143 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.850, 10.216], loss: 0.020352, mae: 0.087953, mean_q: 0.330508
 75518/100000: episode: 1567, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -15.343, mean reward: -0.153 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.765, 10.390], loss: 0.005667, mae: 0.076291, mean_q: 0.302430
 75618/100000: episode: 1568, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -21.115, mean reward: -0.211 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.666, 10.102], loss: 0.018593, mae: 0.080302, mean_q: 0.266807
 75718/100000: episode: 1569, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -10.649, mean reward: -0.106 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.834, 10.413], loss: 0.004625, mae: 0.070219, mean_q: 0.294767
 75818/100000: episode: 1570, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -16.468, mean reward: -0.165 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.049, 10.123], loss: 0.004297, mae: 0.066326, mean_q: 0.223779
 75918/100000: episode: 1571, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -17.079, mean reward: -0.171 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.651, 10.192], loss: 0.017515, mae: 0.073121, mean_q: 0.230047
 76018/100000: episode: 1572, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -17.696, mean reward: -0.177 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.828, 10.098], loss: 0.031227, mae: 0.080120, mean_q: 0.199471
 76118/100000: episode: 1573, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -19.522, mean reward: -0.195 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.748, 10.178], loss: 0.004209, mae: 0.066416, mean_q: 0.213895
 76218/100000: episode: 1574, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -20.733, mean reward: -0.207 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.939, 10.098], loss: 0.018299, mae: 0.078524, mean_q: 0.194601
 76318/100000: episode: 1575, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.744, mean reward: -0.197 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.269, 10.175], loss: 0.017402, mae: 0.071678, mean_q: 0.165920
 76418/100000: episode: 1576, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.532, mean reward: -0.155 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.556, 10.370], loss: 0.004340, mae: 0.067718, mean_q: 0.154397
 76518/100000: episode: 1577, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -15.494, mean reward: -0.155 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.183, 10.098], loss: 0.017338, mae: 0.071621, mean_q: 0.129841
 76618/100000: episode: 1578, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -20.716, mean reward: -0.207 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.674, 10.119], loss: 0.005096, mae: 0.069106, mean_q: 0.110555
 76718/100000: episode: 1579, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -19.594, mean reward: -0.196 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.168, 10.340], loss: 0.031994, mae: 0.085414, mean_q: 0.137795
 76818/100000: episode: 1580, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.233, mean reward: -0.182 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.777, 10.183], loss: 0.043306, mae: 0.078566, mean_q: 0.058762
 76918/100000: episode: 1581, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.993, mean reward: -0.190 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.671, 10.154], loss: 0.004547, mae: 0.065403, mean_q: 0.066029
 77018/100000: episode: 1582, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.666, mean reward: -0.177 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.038, 10.098], loss: 0.017661, mae: 0.072447, mean_q: 0.048876
 77118/100000: episode: 1583, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -16.367, mean reward: -0.164 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.626, 10.128], loss: 0.017682, mae: 0.072290, mean_q: 0.013057
 77218/100000: episode: 1584, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -17.480, mean reward: -0.175 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.159, 10.108], loss: 0.016808, mae: 0.064377, mean_q: -0.037108
 77318/100000: episode: 1585, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.326, mean reward: -0.173 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.804, 10.164], loss: 0.016633, mae: 0.065048, mean_q: -0.033548
 77418/100000: episode: 1586, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.130, mean reward: -0.181 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.786, 10.231], loss: 0.017405, mae: 0.069390, mean_q: -0.071214
 77518/100000: episode: 1587, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -16.122, mean reward: -0.161 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.486, 10.331], loss: 0.003852, mae: 0.060990, mean_q: -0.101176
 77618/100000: episode: 1588, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.300, mean reward: -0.173 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.428, 10.220], loss: 0.017522, mae: 0.070253, mean_q: -0.107195
 77718/100000: episode: 1589, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -16.021, mean reward: -0.160 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.031, 10.134], loss: 0.016766, mae: 0.064954, mean_q: -0.127591
 77818/100000: episode: 1590, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -20.563, mean reward: -0.206 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.194, 10.098], loss: 0.003585, mae: 0.059754, mean_q: -0.145522
 77918/100000: episode: 1591, duration: 0.467s, episode steps: 100, steps per second: 214, episode reward: -18.634, mean reward: -0.186 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.591, 10.164], loss: 0.003431, mae: 0.059693, mean_q: -0.149579
 78018/100000: episode: 1592, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -14.721, mean reward: -0.147 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.310, 10.098], loss: 0.003348, mae: 0.058234, mean_q: -0.162607
 78118/100000: episode: 1593, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -21.373, mean reward: -0.214 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.507, 10.144], loss: 0.003475, mae: 0.058650, mean_q: -0.192416
 78218/100000: episode: 1594, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -19.189, mean reward: -0.192 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.550, 10.098], loss: 0.031794, mae: 0.078443, mean_q: -0.259833
 78318/100000: episode: 1595, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -18.916, mean reward: -0.189 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.298, 10.207], loss: 0.016698, mae: 0.062475, mean_q: -0.247563
 78418/100000: episode: 1596, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -15.274, mean reward: -0.153 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.128, 10.131], loss: 0.004012, mae: 0.060864, mean_q: -0.307552
 78518/100000: episode: 1597, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.314, mean reward: -0.183 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.648, 10.185], loss: 0.016971, mae: 0.065416, mean_q: -0.264160
 78618/100000: episode: 1598, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.644, mean reward: -0.186 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.534, 10.098], loss: 0.003076, mae: 0.054811, mean_q: -0.293575
 78718/100000: episode: 1599, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.742, mean reward: -0.187 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.190, 10.337], loss: 0.003000, mae: 0.053706, mean_q: -0.318735
 78818/100000: episode: 1600, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -12.610, mean reward: -0.126 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.954, 10.468], loss: 0.002942, mae: 0.053653, mean_q: -0.312602
 78918/100000: episode: 1601, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -20.873, mean reward: -0.209 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.141, 10.195], loss: 0.002881, mae: 0.053699, mean_q: -0.312368
 79018/100000: episode: 1602, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.198, mean reward: -0.182 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.303, 10.252], loss: 0.002978, mae: 0.053821, mean_q: -0.299297
 79118/100000: episode: 1603, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -17.189, mean reward: -0.172 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.757, 10.169], loss: 0.002911, mae: 0.053304, mean_q: -0.288417
 79218/100000: episode: 1604, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.967, mean reward: -0.180 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.480, 10.139], loss: 0.002938, mae: 0.053757, mean_q: -0.309725
 79318/100000: episode: 1605, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -14.459, mean reward: -0.145 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.899, 10.098], loss: 0.002893, mae: 0.053181, mean_q: -0.309680
 79418/100000: episode: 1606, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -17.327, mean reward: -0.173 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.951, 10.098], loss: 0.002897, mae: 0.053300, mean_q: -0.292684
 79518/100000: episode: 1607, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: -16.788, mean reward: -0.168 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.521, 10.098], loss: 0.002889, mae: 0.053239, mean_q: -0.316010
 79618/100000: episode: 1608, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.732, mean reward: -0.177 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.792, 10.098], loss: 0.002719, mae: 0.050870, mean_q: -0.360322
 79718/100000: episode: 1609, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.803, mean reward: -0.168 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.668, 10.166], loss: 0.002885, mae: 0.052017, mean_q: -0.362665
 79818/100000: episode: 1610, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -17.142, mean reward: -0.171 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.526, 10.098], loss: 0.002988, mae: 0.054018, mean_q: -0.314563
 79918/100000: episode: 1611, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -17.442, mean reward: -0.174 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.740, 10.098], loss: 0.002637, mae: 0.050513, mean_q: -0.333382
 80018/100000: episode: 1612, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.756, mean reward: -0.158 [-1.000, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.493, 10.213], loss: 0.002808, mae: 0.052371, mean_q: -0.340816
 80118/100000: episode: 1613, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -17.068, mean reward: -0.171 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.255, 10.334], loss: 0.002614, mae: 0.050236, mean_q: -0.346391
 80218/100000: episode: 1614, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -18.566, mean reward: -0.186 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.119, 10.098], loss: 0.002853, mae: 0.053922, mean_q: -0.310080
 80318/100000: episode: 1615, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -20.046, mean reward: -0.200 [-1.000, 0.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.767, 10.098], loss: 0.002601, mae: 0.050309, mean_q: -0.339883
 80418/100000: episode: 1616, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.993, mean reward: -0.180 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.736, 10.216], loss: 0.002575, mae: 0.050045, mean_q: -0.337385
 80518/100000: episode: 1617, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -17.802, mean reward: -0.178 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.505, 10.098], loss: 0.002740, mae: 0.051119, mean_q: -0.336435
 80618/100000: episode: 1618, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -11.804, mean reward: -0.118 [-1.000, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.708, 10.098], loss: 0.002565, mae: 0.049925, mean_q: -0.345082
 80718/100000: episode: 1619, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.016, mean reward: -0.170 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.733, 10.101], loss: 0.002451, mae: 0.049083, mean_q: -0.327734
 80818/100000: episode: 1620, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -17.146, mean reward: -0.171 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.452, 10.403], loss: 0.002663, mae: 0.051182, mean_q: -0.313898
 80918/100000: episode: 1621, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.867, mean reward: -0.179 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.592, 10.187], loss: 0.002437, mae: 0.048292, mean_q: -0.345375
 81018/100000: episode: 1622, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.228, mean reward: -0.172 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.844, 10.224], loss: 0.002556, mae: 0.049824, mean_q: -0.335015
 81118/100000: episode: 1623, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: -18.725, mean reward: -0.187 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.811, 10.362], loss: 0.002563, mae: 0.050642, mean_q: -0.309845
 81218/100000: episode: 1624, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.559, mean reward: -0.176 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.727, 10.318], loss: 0.002688, mae: 0.050519, mean_q: -0.306820
 81318/100000: episode: 1625, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -9.767, mean reward: -0.098 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.864, 10.098], loss: 0.002571, mae: 0.050687, mean_q: -0.344865
 81418/100000: episode: 1626, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -17.043, mean reward: -0.170 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.981, 10.098], loss: 0.002469, mae: 0.049384, mean_q: -0.290043
 81518/100000: episode: 1627, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -14.844, mean reward: -0.148 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.015, 10.194], loss: 0.003725, mae: 0.057583, mean_q: -0.314770
 81618/100000: episode: 1628, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.000, mean reward: -0.180 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.274, 10.316], loss: 0.004430, mae: 0.065391, mean_q: -0.303393
 81718/100000: episode: 1629, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -18.110, mean reward: -0.181 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.733, 10.229], loss: 0.002574, mae: 0.051411, mean_q: -0.317150
 81818/100000: episode: 1630, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.931, mean reward: -0.189 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.393, 10.130], loss: 0.002648, mae: 0.050435, mean_q: -0.337756
 81918/100000: episode: 1631, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -18.985, mean reward: -0.190 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.375, 10.127], loss: 0.002420, mae: 0.048415, mean_q: -0.353288
 82018/100000: episode: 1632, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -14.548, mean reward: -0.145 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.824, 10.139], loss: 0.002508, mae: 0.048622, mean_q: -0.364980
 82118/100000: episode: 1633, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -17.711, mean reward: -0.177 [-1.000, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.958, 10.155], loss: 0.002653, mae: 0.050316, mean_q: -0.340945
 82218/100000: episode: 1634, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -18.873, mean reward: -0.189 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.129, 10.134], loss: 0.002479, mae: 0.048785, mean_q: -0.324270
 82318/100000: episode: 1635, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.370, mean reward: -0.164 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.298, 10.251], loss: 0.002425, mae: 0.048697, mean_q: -0.331714
 82418/100000: episode: 1636, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -18.494, mean reward: -0.185 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.922, 10.201], loss: 0.002771, mae: 0.053097, mean_q: -0.323780
 82518/100000: episode: 1637, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.452, mean reward: -0.195 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.876, 10.098], loss: 0.002633, mae: 0.050929, mean_q: -0.325379
 82618/100000: episode: 1638, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -16.177, mean reward: -0.162 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.887, 10.098], loss: 0.002801, mae: 0.052981, mean_q: -0.321278
 82718/100000: episode: 1639, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.330, mean reward: -0.153 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.421, 10.289], loss: 0.003098, mae: 0.054654, mean_q: -0.339489
 82818/100000: episode: 1640, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -10.322, mean reward: -0.103 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.553, 10.281], loss: 0.002724, mae: 0.051409, mean_q: -0.320172
 82918/100000: episode: 1641, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -13.669, mean reward: -0.137 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.688, 10.226], loss: 0.002845, mae: 0.052821, mean_q: -0.327319
 83018/100000: episode: 1642, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.928, mean reward: -0.189 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.221, 10.180], loss: 0.002584, mae: 0.049882, mean_q: -0.334643
 83118/100000: episode: 1643, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -18.184, mean reward: -0.182 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.636, 10.188], loss: 0.002519, mae: 0.049555, mean_q: -0.332612
 83218/100000: episode: 1644, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -18.782, mean reward: -0.188 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.806, 10.317], loss: 0.002669, mae: 0.051173, mean_q: -0.333013
 83318/100000: episode: 1645, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: -15.042, mean reward: -0.150 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.716, 10.098], loss: 0.002782, mae: 0.052773, mean_q: -0.314988
 83418/100000: episode: 1646, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.241, mean reward: -0.182 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.317, 10.165], loss: 0.002780, mae: 0.052319, mean_q: -0.327292
 83518/100000: episode: 1647, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -14.545, mean reward: -0.145 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.135, 10.408], loss: 0.002480, mae: 0.048802, mean_q: -0.344605
 83618/100000: episode: 1648, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: -13.597, mean reward: -0.136 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.185, 10.383], loss: 0.002571, mae: 0.051042, mean_q: -0.305331
[Info] 100-TH LEVEL FOUND: 0.6175137162208557, Considering 10/90 traces
 83718/100000: episode: 1649, duration: 4.291s, episode steps: 100, steps per second: 23, episode reward: -16.960, mean reward: -0.170 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.800, 10.098], loss: 0.002667, mae: 0.051998, mean_q: -0.319611
 83740/100000: episode: 1650, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 7.476, mean reward: 0.340 [0.183, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.659, 10.100], loss: 0.002559, mae: 0.050442, mean_q: -0.272257
 83753/100000: episode: 1651, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 5.753, mean reward: 0.443 [0.266, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.071, 10.547], loss: 0.002791, mae: 0.052579, mean_q: -0.247844
 83784/100000: episode: 1652, duration: 0.173s, episode steps: 31, steps per second: 180, episode reward: 9.422, mean reward: 0.304 [0.212, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-1.091, 10.100], loss: 0.002804, mae: 0.053629, mean_q: -0.239560
 83815/100000: episode: 1653, duration: 0.150s, episode steps: 31, steps per second: 206, episode reward: 8.931, mean reward: 0.288 [0.205, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.336, 10.100], loss: 0.002576, mae: 0.050890, mean_q: -0.306739
 83828/100000: episode: 1654, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 4.538, mean reward: 0.349 [0.266, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.784, 10.512], loss: 0.002548, mae: 0.050377, mean_q: -0.234006
 83864/100000: episode: 1655, duration: 0.187s, episode steps: 36, steps per second: 192, episode reward: 9.621, mean reward: 0.267 [0.171, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.720, 10.100], loss: 0.002799, mae: 0.052636, mean_q: -0.266764
 83877/100000: episode: 1656, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 3.562, mean reward: 0.274 [0.228, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.430], loss: 0.002981, mae: 0.054818, mean_q: -0.226181
 83913/100000: episode: 1657, duration: 0.176s, episode steps: 36, steps per second: 204, episode reward: 11.260, mean reward: 0.313 [0.218, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.433, 10.100], loss: 0.002674, mae: 0.052375, mean_q: -0.242777
 83941/100000: episode: 1658, duration: 0.145s, episode steps: 28, steps per second: 194, episode reward: 11.267, mean reward: 0.402 [0.293, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.064, 10.429], loss: 0.002793, mae: 0.053431, mean_q: -0.276916
 83966/100000: episode: 1659, duration: 0.120s, episode steps: 25, steps per second: 209, episode reward: 7.289, mean reward: 0.292 [0.186, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.120, 10.100], loss: 0.002828, mae: 0.052495, mean_q: -0.294521
 84002/100000: episode: 1660, duration: 0.178s, episode steps: 36, steps per second: 203, episode reward: 11.049, mean reward: 0.307 [0.168, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.446, 10.100], loss: 0.002428, mae: 0.049132, mean_q: -0.316276
 84030/100000: episode: 1661, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 8.793, mean reward: 0.314 [0.222, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.521, 10.430], loss: 0.002575, mae: 0.050621, mean_q: -0.286876
 84061/100000: episode: 1662, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 8.979, mean reward: 0.290 [0.188, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.744, 10.100], loss: 0.002660, mae: 0.051503, mean_q: -0.273347
 84089/100000: episode: 1663, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 10.195, mean reward: 0.364 [0.229, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.191, 10.434], loss: 0.003109, mae: 0.056883, mean_q: -0.206766
 84102/100000: episode: 1664, duration: 0.063s, episode steps: 13, steps per second: 208, episode reward: 3.386, mean reward: 0.260 [0.180, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.715, 10.325], loss: 0.002894, mae: 0.055338, mean_q: -0.272328
 84127/100000: episode: 1665, duration: 0.123s, episode steps: 25, steps per second: 204, episode reward: 4.720, mean reward: 0.189 [0.021, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.035, 10.119], loss: 0.002188, mae: 0.048032, mean_q: -0.270774
 84149/100000: episode: 1666, duration: 0.103s, episode steps: 22, steps per second: 213, episode reward: 6.043, mean reward: 0.275 [0.150, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.077, 10.100], loss: 0.003098, mae: 0.056596, mean_q: -0.193926
 84174/100000: episode: 1667, duration: 0.122s, episode steps: 25, steps per second: 205, episode reward: 7.140, mean reward: 0.286 [0.172, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.841, 10.100], loss: 0.002960, mae: 0.054910, mean_q: -0.314663
 84205/100000: episode: 1668, duration: 0.147s, episode steps: 31, steps per second: 211, episode reward: 8.918, mean reward: 0.288 [0.153, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-1.445, 10.100], loss: 0.002766, mae: 0.054409, mean_q: -0.173997
 84231/100000: episode: 1669, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 8.858, mean reward: 0.341 [0.230, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.407, 10.377], loss: 0.002901, mae: 0.053954, mean_q: -0.223058
 84267/100000: episode: 1670, duration: 0.172s, episode steps: 36, steps per second: 209, episode reward: 9.444, mean reward: 0.262 [0.034, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.449, 10.100], loss: 0.002806, mae: 0.056759, mean_q: -0.166150
 84298/100000: episode: 1671, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 10.200, mean reward: 0.329 [0.238, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.343, 10.100], loss: 0.002779, mae: 0.053216, mean_q: -0.219898
 84334/100000: episode: 1672, duration: 0.180s, episode steps: 36, steps per second: 200, episode reward: 6.127, mean reward: 0.170 [0.022, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.094, 10.177], loss: 0.002853, mae: 0.052743, mean_q: -0.225734
 84370/100000: episode: 1673, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 11.315, mean reward: 0.314 [0.192, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-2.399, 10.100], loss: 0.002914, mae: 0.054541, mean_q: -0.130314
 84401/100000: episode: 1674, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 7.136, mean reward: 0.230 [0.086, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.596, 10.100], loss: 0.002781, mae: 0.054831, mean_q: -0.196226
 84432/100000: episode: 1675, duration: 0.149s, episode steps: 31, steps per second: 208, episode reward: 9.475, mean reward: 0.306 [0.242, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.308, 10.100], loss: 0.002984, mae: 0.054242, mean_q: -0.204964
 84460/100000: episode: 1676, duration: 0.137s, episode steps: 28, steps per second: 205, episode reward: 8.598, mean reward: 0.307 [0.228, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.035, 10.353], loss: 0.003269, mae: 0.058771, mean_q: -0.199383
 84488/100000: episode: 1677, duration: 0.129s, episode steps: 28, steps per second: 216, episode reward: 9.000, mean reward: 0.321 [0.203, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.481], loss: 0.002952, mae: 0.057089, mean_q: -0.135521
 84519/100000: episode: 1678, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 10.399, mean reward: 0.335 [0.211, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.438, 10.100], loss: 0.002848, mae: 0.053314, mean_q: -0.139415
 84541/100000: episode: 1679, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 4.551, mean reward: 0.207 [0.074, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.614, 10.100], loss: 0.002457, mae: 0.049149, mean_q: -0.182362
 84561/100000: episode: 1680, duration: 0.100s, episode steps: 20, steps per second: 199, episode reward: 5.170, mean reward: 0.259 [0.138, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.620, 10.250], loss: 0.002895, mae: 0.053051, mean_q: -0.242987
 84587/100000: episode: 1681, duration: 0.128s, episode steps: 26, steps per second: 204, episode reward: 8.839, mean reward: 0.340 [0.234, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.455, 10.471], loss: 0.002519, mae: 0.052434, mean_q: -0.165722
 84612/100000: episode: 1682, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 7.188, mean reward: 0.288 [0.136, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.083, 10.100], loss: 0.003309, mae: 0.061684, mean_q: -0.169560
 84648/100000: episode: 1683, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 11.247, mean reward: 0.312 [0.097, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.849, 10.100], loss: 0.002959, mae: 0.056294, mean_q: -0.179930
 84670/100000: episode: 1684, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 4.800, mean reward: 0.218 [0.019, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.262, 10.100], loss: 0.002786, mae: 0.053201, mean_q: -0.098208
 84704/100000: episode: 1685, duration: 0.163s, episode steps: 34, steps per second: 208, episode reward: 10.568, mean reward: 0.311 [0.196, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.658, 10.100], loss: 0.002767, mae: 0.053415, mean_q: -0.180111
 84724/100000: episode: 1686, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 8.340, mean reward: 0.417 [0.255, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.509], loss: 0.002609, mae: 0.051861, mean_q: -0.166154
 84760/100000: episode: 1687, duration: 0.190s, episode steps: 36, steps per second: 189, episode reward: 8.588, mean reward: 0.239 [0.050, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.828, 10.100], loss: 0.003064, mae: 0.056279, mean_q: -0.080729
 84796/100000: episode: 1688, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 9.115, mean reward: 0.253 [0.143, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.453, 10.100], loss: 0.003066, mae: 0.055975, mean_q: -0.110089
 84818/100000: episode: 1689, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 5.618, mean reward: 0.255 [0.029, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.035, 10.100], loss: 0.002956, mae: 0.056396, mean_q: -0.051665
 84843/100000: episode: 1690, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 9.183, mean reward: 0.367 [0.307, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.413, 10.100], loss: 0.002612, mae: 0.050732, mean_q: -0.196520
 84863/100000: episode: 1691, duration: 0.096s, episode steps: 20, steps per second: 209, episode reward: 6.096, mean reward: 0.305 [0.164, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-1.014, 10.306], loss: 0.003257, mae: 0.056379, mean_q: -0.114475
 84889/100000: episode: 1692, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 7.306, mean reward: 0.281 [0.155, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.349], loss: 0.002903, mae: 0.056276, mean_q: -0.089489
 84925/100000: episode: 1693, duration: 0.182s, episode steps: 36, steps per second: 197, episode reward: 12.244, mean reward: 0.340 [0.137, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.195, 10.100], loss: 0.002798, mae: 0.054554, mean_q: -0.102235
 84961/100000: episode: 1694, duration: 0.180s, episode steps: 36, steps per second: 200, episode reward: 9.503, mean reward: 0.264 [0.072, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.040, 10.100], loss: 0.002798, mae: 0.054666, mean_q: -0.074572
 84992/100000: episode: 1695, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 9.256, mean reward: 0.299 [0.186, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.585, 10.100], loss: 0.002711, mae: 0.051540, mean_q: -0.094760
 85005/100000: episode: 1696, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 3.902, mean reward: 0.300 [0.249, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.184, 10.380], loss: 0.002358, mae: 0.051978, mean_q: -0.018059
 85039/100000: episode: 1697, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 11.812, mean reward: 0.347 [0.267, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-1.249, 10.100], loss: 0.002938, mae: 0.054943, mean_q: -0.076640
 85064/100000: episode: 1698, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 6.202, mean reward: 0.248 [0.127, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.671, 10.100], loss: 0.003070, mae: 0.056169, mean_q: -0.085933
 85077/100000: episode: 1699, duration: 0.063s, episode steps: 13, steps per second: 208, episode reward: 3.825, mean reward: 0.294 [0.252, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.340], loss: 0.002842, mae: 0.054325, mean_q: -0.035163
 85102/100000: episode: 1700, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 5.813, mean reward: 0.233 [0.149, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.266, 10.100], loss: 0.003084, mae: 0.056500, mean_q: -0.060258
 85130/100000: episode: 1701, duration: 0.140s, episode steps: 28, steps per second: 201, episode reward: 10.898, mean reward: 0.389 [0.271, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.712, 10.494], loss: 0.002809, mae: 0.054179, mean_q: -0.075332
 85166/100000: episode: 1702, duration: 0.180s, episode steps: 36, steps per second: 200, episode reward: 16.399, mean reward: 0.456 [0.299, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.540, 10.100], loss: 0.008431, mae: 0.079624, mean_q: -0.025385
 85202/100000: episode: 1703, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 14.830, mean reward: 0.412 [0.267, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.545, 10.100], loss: 0.006809, mae: 0.072756, mean_q: -0.112992
 85215/100000: episode: 1704, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 3.504, mean reward: 0.270 [0.161, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-1.311, 10.287], loss: 0.003095, mae: 0.061097, mean_q: -0.035792
 85240/100000: episode: 1705, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 7.942, mean reward: 0.318 [0.236, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.004, 10.100], loss: 0.003100, mae: 0.058474, mean_q: -0.040188
 85262/100000: episode: 1706, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 7.903, mean reward: 0.359 [0.241, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.185, 10.100], loss: 0.003254, mae: 0.059869, mean_q: -0.039275
 85298/100000: episode: 1707, duration: 0.185s, episode steps: 36, steps per second: 195, episode reward: 7.947, mean reward: 0.221 [0.025, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.806, 10.100], loss: 0.002557, mae: 0.051713, mean_q: -0.073067
 85323/100000: episode: 1708, duration: 0.116s, episode steps: 25, steps per second: 216, episode reward: 7.638, mean reward: 0.306 [0.121, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.172, 10.100], loss: 0.003146, mae: 0.057907, mean_q: -0.016521
 85359/100000: episode: 1709, duration: 0.190s, episode steps: 36, steps per second: 189, episode reward: 6.855, mean reward: 0.190 [0.022, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.145, 10.100], loss: 0.002703, mae: 0.054312, mean_q: -0.022588
 85384/100000: episode: 1710, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 7.621, mean reward: 0.305 [0.233, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.282, 10.100], loss: 0.002796, mae: 0.053187, mean_q: 0.041544
 85406/100000: episode: 1711, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 4.972, mean reward: 0.226 [0.123, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.709, 10.100], loss: 0.002836, mae: 0.053662, mean_q: -0.043993
 85434/100000: episode: 1712, duration: 0.153s, episode steps: 28, steps per second: 184, episode reward: 10.368, mean reward: 0.370 [0.274, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.733, 10.440], loss: 0.002700, mae: 0.052960, mean_q: 0.033697
 85470/100000: episode: 1713, duration: 0.177s, episode steps: 36, steps per second: 204, episode reward: 7.832, mean reward: 0.218 [0.070, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.735, 10.100], loss: 0.002770, mae: 0.054081, mean_q: 0.037832
 85506/100000: episode: 1714, duration: 0.171s, episode steps: 36, steps per second: 210, episode reward: 6.793, mean reward: 0.189 [0.011, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.160, 10.100], loss: 0.002584, mae: 0.052080, mean_q: 0.004491
 85519/100000: episode: 1715, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 3.065, mean reward: 0.236 [0.023, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.214], loss: 0.003600, mae: 0.060842, mean_q: 0.023481
 85545/100000: episode: 1716, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 8.117, mean reward: 0.312 [0.169, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.334], loss: 0.003081, mae: 0.054845, mean_q: -0.010535
 85565/100000: episode: 1717, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 5.205, mean reward: 0.260 [0.161, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.057, 10.429], loss: 0.003133, mae: 0.058570, mean_q: 0.062678
 85591/100000: episode: 1718, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 10.026, mean reward: 0.386 [0.285, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.453, 10.453], loss: 0.002692, mae: 0.053384, mean_q: -0.000402
 85616/100000: episode: 1719, duration: 0.120s, episode steps: 25, steps per second: 208, episode reward: 8.568, mean reward: 0.343 [0.286, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.396, 10.100], loss: 0.002924, mae: 0.055869, mean_q: 0.014664
 85638/100000: episode: 1720, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 7.012, mean reward: 0.319 [0.206, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.329, 10.100], loss: 0.003293, mae: 0.058573, mean_q: 0.043816
 85663/100000: episode: 1721, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 4.642, mean reward: 0.186 [0.069, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.164, 10.100], loss: 0.003016, mae: 0.055975, mean_q: 0.059689
 85699/100000: episode: 1722, duration: 0.176s, episode steps: 36, steps per second: 205, episode reward: 12.851, mean reward: 0.357 [0.293, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.373, 10.100], loss: 0.002882, mae: 0.055356, mean_q: 0.063500
 85735/100000: episode: 1723, duration: 0.169s, episode steps: 36, steps per second: 213, episode reward: 8.550, mean reward: 0.238 [0.014, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.339, 10.123], loss: 0.003227, mae: 0.059850, mean_q: 0.071084
 85757/100000: episode: 1724, duration: 0.110s, episode steps: 22, steps per second: 199, episode reward: 5.234, mean reward: 0.238 [0.134, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.100], loss: 0.003255, mae: 0.057709, mean_q: -0.005062
 85783/100000: episode: 1725, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 7.957, mean reward: 0.306 [0.162, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.699, 10.317], loss: 0.002876, mae: 0.056251, mean_q: 0.077171
 85808/100000: episode: 1726, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 6.943, mean reward: 0.278 [0.204, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.466, 10.100], loss: 0.003282, mae: 0.059396, mean_q: 0.118191
 85836/100000: episode: 1727, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 7.793, mean reward: 0.278 [0.203, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.412, 10.376], loss: 0.002954, mae: 0.056914, mean_q: 0.067636
 85849/100000: episode: 1728, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 4.742, mean reward: 0.365 [0.209, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.300], loss: 0.003022, mae: 0.057011, mean_q: 0.161893
 85869/100000: episode: 1729, duration: 0.097s, episode steps: 20, steps per second: 205, episode reward: 5.636, mean reward: 0.282 [0.216, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.458], loss: 0.003623, mae: 0.062329, mean_q: 0.118762
 85882/100000: episode: 1730, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 4.276, mean reward: 0.329 [0.207, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.424], loss: 0.004102, mae: 0.067885, mean_q: 0.208218
 85904/100000: episode: 1731, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 4.945, mean reward: 0.225 [0.053, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.006, 10.114], loss: 0.003038, mae: 0.058420, mean_q: 0.104796
 85924/100000: episode: 1732, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 6.772, mean reward: 0.339 [0.247, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.676, 10.527], loss: 0.002666, mae: 0.053551, mean_q: 0.086082
 85958/100000: episode: 1733, duration: 0.189s, episode steps: 34, steps per second: 180, episode reward: 10.301, mean reward: 0.303 [0.082, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.298, 10.100], loss: 0.003071, mae: 0.058722, mean_q: 0.082615
 85986/100000: episode: 1734, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 5.511, mean reward: 0.197 [0.087, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.424, 10.190], loss: 0.002604, mae: 0.052112, mean_q: 0.070092
 85999/100000: episode: 1735, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 4.449, mean reward: 0.342 [0.296, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.512, 10.453], loss: 0.002344, mae: 0.049576, mean_q: 0.031525
 86035/100000: episode: 1736, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 12.314, mean reward: 0.342 [0.187, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-1.023, 10.100], loss: 0.002817, mae: 0.054960, mean_q: 0.084295
 86066/100000: episode: 1737, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 7.369, mean reward: 0.238 [0.095, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-1.232, 10.100], loss: 0.002893, mae: 0.057372, mean_q: 0.122449
 86079/100000: episode: 1738, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 4.160, mean reward: 0.320 [0.219, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.149, 10.295], loss: 0.003716, mae: 0.064329, mean_q: 0.135114
[Info] 200-TH LEVEL FOUND: 0.7739085555076599, Considering 10/90 traces
 86110/100000: episode: 1739, duration: 4.008s, episode steps: 31, steps per second: 8, episode reward: 8.682, mean reward: 0.280 [0.131, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.311, 10.100], loss: 0.002656, mae: 0.055080, mean_q: 0.114210
 86123/100000: episode: 1740, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 6.836, mean reward: 0.526 [0.421, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.528, 10.589], loss: 0.003157, mae: 0.059788, mean_q: 0.200455
 86136/100000: episode: 1741, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 6.360, mean reward: 0.489 [0.376, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.569], loss: 0.002885, mae: 0.054530, mean_q: 0.104591
 86157/100000: episode: 1742, duration: 0.110s, episode steps: 21, steps per second: 190, episode reward: 8.435, mean reward: 0.402 [0.336, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.681, 10.100], loss: 0.003161, mae: 0.058358, mean_q: 0.128061
 86178/100000: episode: 1743, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 6.336, mean reward: 0.302 [0.137, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.475, 10.296], loss: 0.002671, mae: 0.055437, mean_q: 0.124379
 86199/100000: episode: 1744, duration: 0.102s, episode steps: 21, steps per second: 205, episode reward: 9.505, mean reward: 0.453 [0.354, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.688, 10.500], loss: 0.003106, mae: 0.056316, mean_q: 0.138708
 86211/100000: episode: 1745, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 5.306, mean reward: 0.442 [0.387, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.498, 10.100], loss: 0.003184, mae: 0.060146, mean_q: 0.126876
 86231/100000: episode: 1746, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 9.153, mean reward: 0.458 [0.396, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.479, 10.576], loss: 0.003168, mae: 0.058741, mean_q: 0.146262
 86254/100000: episode: 1747, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 9.009, mean reward: 0.392 [0.254, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.476, 10.100], loss: 0.003018, mae: 0.057198, mean_q: 0.161859
 86274/100000: episode: 1748, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 7.516, mean reward: 0.376 [0.269, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.449, 10.378], loss: 0.003044, mae: 0.056615, mean_q: 0.121520
 86295/100000: episode: 1749, duration: 0.111s, episode steps: 21, steps per second: 188, episode reward: 8.195, mean reward: 0.390 [0.270, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.445, 10.100], loss: 0.003101, mae: 0.058400, mean_q: 0.164928
 86308/100000: episode: 1750, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 5.643, mean reward: 0.434 [0.302, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.281, 10.419], loss: 0.003219, mae: 0.060408, mean_q: 0.180862
 86320/100000: episode: 1751, duration: 0.058s, episode steps: 12, steps per second: 205, episode reward: 4.883, mean reward: 0.407 [0.341, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.223, 10.100], loss: 0.002847, mae: 0.054696, mean_q: 0.181130
 86341/100000: episode: 1752, duration: 0.111s, episode steps: 21, steps per second: 188, episode reward: 6.952, mean reward: 0.331 [0.212, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.428, 10.383], loss: 0.003099, mae: 0.057287, mean_q: 0.181543
 86354/100000: episode: 1753, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 6.647, mean reward: 0.511 [0.450, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.634], loss: 0.003460, mae: 0.060955, mean_q: 0.132518
 86381/100000: episode: 1754, duration: 0.129s, episode steps: 27, steps per second: 210, episode reward: 9.407, mean reward: 0.348 [0.161, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.688, 10.100], loss: 0.003277, mae: 0.060580, mean_q: 0.170859
 86393/100000: episode: 1755, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 4.498, mean reward: 0.375 [0.293, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.449, 10.100], loss: 0.002955, mae: 0.057648, mean_q: 0.171240
 86411/100000: episode: 1756, duration: 0.085s, episode steps: 18, steps per second: 212, episode reward: 8.613, mean reward: 0.479 [0.334, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.438, 10.100], loss: 0.003025, mae: 0.058871, mean_q: 0.267463
 86431/100000: episode: 1757, duration: 0.103s, episode steps: 20, steps per second: 193, episode reward: 7.401, mean reward: 0.370 [0.223, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.341, 10.351], loss: 0.003453, mae: 0.062370, mean_q: 0.196190
 86444/100000: episode: 1758, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 4.390, mean reward: 0.338 [0.257, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.541], loss: 0.003304, mae: 0.060639, mean_q: 0.169172
 86464/100000: episode: 1759, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 7.021, mean reward: 0.351 [0.221, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.469, 10.444], loss: 0.003209, mae: 0.060033, mean_q: 0.228063
 86487/100000: episode: 1760, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 7.895, mean reward: 0.343 [0.225, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.241, 10.100], loss: 0.003981, mae: 0.066222, mean_q: 0.173283
 86508/100000: episode: 1761, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 9.063, mean reward: 0.432 [0.319, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.332, 10.527], loss: 0.003309, mae: 0.061121, mean_q: 0.191463
 86531/100000: episode: 1762, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 8.661, mean reward: 0.377 [0.184, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.750, 10.100], loss: 0.003108, mae: 0.059144, mean_q: 0.232507
 86552/100000: episode: 1763, duration: 0.102s, episode steps: 21, steps per second: 206, episode reward: 10.173, mean reward: 0.484 [0.323, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.344, 10.100], loss: 0.003831, mae: 0.065473, mean_q: 0.198003
 86565/100000: episode: 1764, duration: 0.062s, episode steps: 13, steps per second: 211, episode reward: 4.881, mean reward: 0.375 [0.217, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.322, 10.498], loss: 0.003369, mae: 0.060981, mean_q: 0.257331
 86586/100000: episode: 1765, duration: 0.098s, episode steps: 21, steps per second: 215, episode reward: 6.849, mean reward: 0.326 [0.111, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.194, 10.237], loss: 0.003169, mae: 0.058304, mean_q: 0.199652
 86606/100000: episode: 1766, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 7.427, mean reward: 0.371 [0.218, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.393, 10.390], loss: 0.003212, mae: 0.058378, mean_q: 0.196518
 86619/100000: episode: 1767, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 3.655, mean reward: 0.281 [0.229, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.570, 10.366], loss: 0.002928, mae: 0.055142, mean_q: 0.153334
 86642/100000: episode: 1768, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 8.397, mean reward: 0.365 [0.225, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.670, 10.100], loss: 0.003561, mae: 0.062462, mean_q: 0.231113
 86663/100000: episode: 1769, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 6.981, mean reward: 0.332 [0.220, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.339, 10.100], loss: 0.003421, mae: 0.061024, mean_q: 0.230253
 86684/100000: episode: 1770, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 7.909, mean reward: 0.377 [0.211, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.513, 10.408], loss: 0.003724, mae: 0.065294, mean_q: 0.247161
 86702/100000: episode: 1771, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 6.778, mean reward: 0.377 [0.271, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.377, 10.100], loss: 0.003514, mae: 0.062225, mean_q: 0.267862
 86725/100000: episode: 1772, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 9.317, mean reward: 0.405 [0.353, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.391, 10.100], loss: 0.003358, mae: 0.061539, mean_q: 0.201073
 86752/100000: episode: 1773, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 7.852, mean reward: 0.291 [0.146, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.295, 10.100], loss: 0.003285, mae: 0.059696, mean_q: 0.246629
 86779/100000: episode: 1774, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 11.207, mean reward: 0.415 [0.322, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.328, 10.100], loss: 0.003010, mae: 0.058188, mean_q: 0.237522
 86797/100000: episode: 1775, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 7.575, mean reward: 0.421 [0.351, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.343, 10.100], loss: 0.003070, mae: 0.059241, mean_q: 0.257253
 86818/100000: episode: 1776, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 9.576, mean reward: 0.456 [0.373, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-1.404, 10.485], loss: 0.003806, mae: 0.064332, mean_q: 0.271002
 86839/100000: episode: 1777, duration: 0.110s, episode steps: 21, steps per second: 192, episode reward: 7.939, mean reward: 0.378 [0.305, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-1.146, 10.100], loss: 0.003996, mae: 0.065423, mean_q: 0.294386
 86851/100000: episode: 1778, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 5.746, mean reward: 0.479 [0.408, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.548], loss: 0.003588, mae: 0.063956, mean_q: 0.240213
 86872/100000: episode: 1779, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 8.305, mean reward: 0.395 [0.291, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.672, 10.100], loss: 0.014920, mae: 0.087362, mean_q: 0.245953
 86892/100000: episode: 1780, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 8.283, mean reward: 0.414 [0.342, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.438], loss: 0.004753, mae: 0.075492, mean_q: 0.243215
 86910/100000: episode: 1781, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 9.145, mean reward: 0.508 [0.374, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.511, 10.100], loss: 0.003540, mae: 0.065131, mean_q: 0.307263
 86922/100000: episode: 1782, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 4.586, mean reward: 0.382 [0.271, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.489], loss: 0.003919, mae: 0.066607, mean_q: 0.232509
 86934/100000: episode: 1783, duration: 0.058s, episode steps: 12, steps per second: 207, episode reward: 5.473, mean reward: 0.456 [0.416, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.579], loss: 0.003736, mae: 0.066104, mean_q: 0.259034
 86961/100000: episode: 1784, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 12.149, mean reward: 0.450 [0.361, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.342, 10.100], loss: 0.002941, mae: 0.059690, mean_q: 0.285114
 86982/100000: episode: 1785, duration: 0.111s, episode steps: 21, steps per second: 188, episode reward: 8.160, mean reward: 0.389 [0.309, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.480, 10.100], loss: 0.003037, mae: 0.058453, mean_q: 0.313408
 87003/100000: episode: 1786, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 8.146, mean reward: 0.388 [0.294, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.333], loss: 0.003142, mae: 0.059562, mean_q: 0.314316
 87026/100000: episode: 1787, duration: 0.111s, episode steps: 23, steps per second: 207, episode reward: 8.826, mean reward: 0.384 [0.302, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.857, 10.100], loss: 0.003578, mae: 0.061433, mean_q: 0.307350
 87039/100000: episode: 1788, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 6.564, mean reward: 0.505 [0.356, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.582], loss: 0.002852, mae: 0.057560, mean_q: 0.380009
 87062/100000: episode: 1789, duration: 0.111s, episode steps: 23, steps per second: 207, episode reward: 10.366, mean reward: 0.451 [0.236, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.983, 10.100], loss: 0.003504, mae: 0.061869, mean_q: 0.349319
 87080/100000: episode: 1790, duration: 0.085s, episode steps: 18, steps per second: 211, episode reward: 7.854, mean reward: 0.436 [0.337, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.655, 10.100], loss: 0.003086, mae: 0.057226, mean_q: 0.310526
 87093/100000: episode: 1791, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 5.494, mean reward: 0.423 [0.303, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.248, 10.640], loss: 0.003097, mae: 0.059657, mean_q: 0.321867
 87114/100000: episode: 1792, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 6.211, mean reward: 0.296 [0.203, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.114, 10.239], loss: 0.002873, mae: 0.055724, mean_q: 0.363459
 87135/100000: episode: 1793, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 6.693, mean reward: 0.319 [0.175, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.413, 10.100], loss: 0.003384, mae: 0.062138, mean_q: 0.334335
 87148/100000: episode: 1794, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 4.831, mean reward: 0.372 [0.320, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.194, 10.309], loss: 0.003066, mae: 0.057878, mean_q: 0.344385
 87161/100000: episode: 1795, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 4.108, mean reward: 0.316 [0.263, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.809, 10.449], loss: 0.002644, mae: 0.055774, mean_q: 0.408397
 87173/100000: episode: 1796, duration: 0.057s, episode steps: 12, steps per second: 209, episode reward: 5.010, mean reward: 0.417 [0.351, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.310, 10.390], loss: 0.003415, mae: 0.061338, mean_q: 0.346335
 87191/100000: episode: 1797, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 6.300, mean reward: 0.350 [0.175, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.334, 10.100], loss: 0.003727, mae: 0.063317, mean_q: 0.339711
 87204/100000: episode: 1798, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 5.327, mean reward: 0.410 [0.335, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-1.231, 10.403], loss: 0.003960, mae: 0.066372, mean_q: 0.379356
 87225/100000: episode: 1799, duration: 0.113s, episode steps: 21, steps per second: 187, episode reward: 8.529, mean reward: 0.406 [0.306, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.277, 10.100], loss: 0.003144, mae: 0.059568, mean_q: 0.384352
 87248/100000: episode: 1800, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 9.487, mean reward: 0.412 [0.367, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.348, 10.100], loss: 0.002716, mae: 0.054832, mean_q: 0.330806
 87266/100000: episode: 1801, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 6.830, mean reward: 0.379 [0.280, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.433, 10.100], loss: 0.002903, mae: 0.056019, mean_q: 0.353326
 87289/100000: episode: 1802, duration: 0.118s, episode steps: 23, steps per second: 196, episode reward: 9.259, mean reward: 0.403 [0.265, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.548, 10.100], loss: 0.003298, mae: 0.059448, mean_q: 0.394561
 87307/100000: episode: 1803, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 7.134, mean reward: 0.396 [0.231, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.010, 10.100], loss: 0.003990, mae: 0.068004, mean_q: 0.412979
 87330/100000: episode: 1804, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 7.720, mean reward: 0.336 [0.237, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.822, 10.100], loss: 0.003376, mae: 0.061782, mean_q: 0.388377
 87353/100000: episode: 1805, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 6.302, mean reward: 0.274 [0.153, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.388, 10.100], loss: 0.003377, mae: 0.062609, mean_q: 0.391660
 87371/100000: episode: 1806, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 6.649, mean reward: 0.369 [0.302, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.367, 10.100], loss: 0.003233, mae: 0.061269, mean_q: 0.387081
 87389/100000: episode: 1807, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 8.228, mean reward: 0.457 [0.396, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.474, 10.100], loss: 0.003899, mae: 0.066739, mean_q: 0.396924
 87402/100000: episode: 1808, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 4.703, mean reward: 0.362 [0.169, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.634, 10.419], loss: 0.003432, mae: 0.064022, mean_q: 0.408642
 87420/100000: episode: 1809, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 5.426, mean reward: 0.301 [0.166, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.204, 10.100], loss: 0.003462, mae: 0.063097, mean_q: 0.413238
 87440/100000: episode: 1810, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 8.383, mean reward: 0.419 [0.362, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.680, 10.435], loss: 0.003497, mae: 0.062601, mean_q: 0.404444
 87461/100000: episode: 1811, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 8.295, mean reward: 0.395 [0.333, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.309, 10.501], loss: 0.003317, mae: 0.061389, mean_q: 0.413025
 87474/100000: episode: 1812, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 5.016, mean reward: 0.386 [0.308, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.586], loss: 0.003172, mae: 0.061185, mean_q: 0.362230
 87492/100000: episode: 1813, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 9.533, mean reward: 0.530 [0.420, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.391, 10.100], loss: 0.003684, mae: 0.063539, mean_q: 0.431698
 87505/100000: episode: 1814, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 5.843, mean reward: 0.449 [0.396, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.666, 10.539], loss: 0.003910, mae: 0.067859, mean_q: 0.427576
 87518/100000: episode: 1815, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 5.123, mean reward: 0.394 [0.309, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.815, 10.473], loss: 0.003764, mae: 0.065615, mean_q: 0.422784
 87539/100000: episode: 1816, duration: 0.111s, episode steps: 21, steps per second: 190, episode reward: 5.820, mean reward: 0.277 [0.189, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.347, 10.100], loss: 0.004129, mae: 0.071486, mean_q: 0.457729
 87560/100000: episode: 1817, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 10.105, mean reward: 0.481 [0.333, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.805, 10.100], loss: 0.003727, mae: 0.066353, mean_q: 0.446647
 87581/100000: episode: 1818, duration: 0.111s, episode steps: 21, steps per second: 190, episode reward: 7.245, mean reward: 0.345 [0.211, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.691, 10.100], loss: 0.003101, mae: 0.059663, mean_q: 0.422531
 87593/100000: episode: 1819, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 3.930, mean reward: 0.327 [0.257, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.503, 10.100], loss: 0.003509, mae: 0.062661, mean_q: 0.428729
 87620/100000: episode: 1820, duration: 0.130s, episode steps: 27, steps per second: 208, episode reward: 11.095, mean reward: 0.411 [0.349, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.457, 10.100], loss: 0.003370, mae: 0.062065, mean_q: 0.408182
 87640/100000: episode: 1821, duration: 0.102s, episode steps: 20, steps per second: 195, episode reward: 5.822, mean reward: 0.291 [0.133, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.938, 10.269], loss: 0.003452, mae: 0.062577, mean_q: 0.488630
 87652/100000: episode: 1822, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 4.964, mean reward: 0.414 [0.351, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.455], loss: 0.003445, mae: 0.061394, mean_q: 0.460339
 87664/100000: episode: 1823, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 5.214, mean reward: 0.434 [0.327, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.779, 10.100], loss: 0.003061, mae: 0.059008, mean_q: 0.480661
 87691/100000: episode: 1824, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 9.313, mean reward: 0.345 [0.162, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.707, 10.100], loss: 0.003337, mae: 0.061793, mean_q: 0.482851
 87711/100000: episode: 1825, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 7.898, mean reward: 0.395 [0.226, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.238, 10.406], loss: 0.003523, mae: 0.064774, mean_q: 0.495398
 87724/100000: episode: 1826, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 5.879, mean reward: 0.452 [0.415, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.525], loss: 0.003223, mae: 0.062116, mean_q: 0.551575
 87747/100000: episode: 1827, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 7.099, mean reward: 0.309 [0.198, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.519, 10.100], loss: 0.003899, mae: 0.067592, mean_q: 0.519749
 87774/100000: episode: 1828, duration: 0.133s, episode steps: 27, steps per second: 202, episode reward: 13.361, mean reward: 0.495 [0.353, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.390, 10.100], loss: 0.003278, mae: 0.061967, mean_q: 0.517644
[Info] 300-TH LEVEL FOUND: 0.9405039548873901, Considering 10/90 traces
 87792/100000: episode: 1829, duration: 3.919s, episode steps: 18, steps per second: 5, episode reward: 5.939, mean reward: 0.330 [0.217, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.326, 10.100], loss: 0.003761, mae: 0.066544, mean_q: 0.497240
 87804/100000: episode: 1830, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 5.594, mean reward: 0.466 [0.380, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.545], loss: 0.003285, mae: 0.060913, mean_q: 0.496887
 87817/100000: episode: 1831, duration: 0.063s, episode steps: 13, steps per second: 208, episode reward: 7.445, mean reward: 0.573 [0.518, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.555, 10.100], loss: 0.003287, mae: 0.062707, mean_q: 0.500037
 87827/100000: episode: 1832, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 4.800, mean reward: 0.480 [0.419, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.659, 10.534], loss: 0.003613, mae: 0.064805, mean_q: 0.525532
 87837/100000: episode: 1833, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 4.736, mean reward: 0.474 [0.411, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.540, 10.508], loss: 0.003496, mae: 0.063807, mean_q: 0.466786
 87850/100000: episode: 1834, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 5.510, mean reward: 0.424 [0.316, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.166, 10.429], loss: 0.003939, mae: 0.067027, mean_q: 0.527506
 87865/100000: episode: 1835, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 7.535, mean reward: 0.502 [0.439, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.575, 10.100], loss: 0.002968, mae: 0.059739, mean_q: 0.543756
 87876/100000: episode: 1836, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 5.688, mean reward: 0.517 [0.452, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.617], loss: 0.003235, mae: 0.060987, mean_q: 0.497430
 87888/100000: episode: 1837, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 5.758, mean reward: 0.480 [0.387, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.568, 10.498], loss: 0.003127, mae: 0.059589, mean_q: 0.535682
 87903/100000: episode: 1838, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 7.526, mean reward: 0.502 [0.450, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.606, 10.100], loss: 0.003388, mae: 0.064308, mean_q: 0.507672
 87913/100000: episode: 1839, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 4.750, mean reward: 0.475 [0.421, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.655, 10.573], loss: 0.003500, mae: 0.061788, mean_q: 0.535539
 87924/100000: episode: 1840, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 5.084, mean reward: 0.462 [0.384, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.316, 10.640], loss: 0.003456, mae: 0.062064, mean_q: 0.480032
 87937/100000: episode: 1841, duration: 0.062s, episode steps: 13, steps per second: 208, episode reward: 6.213, mean reward: 0.478 [0.408, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.105, 10.584], loss: 0.003296, mae: 0.061176, mean_q: 0.570557
 87949/100000: episode: 1842, duration: 0.058s, episode steps: 12, steps per second: 208, episode reward: 6.647, mean reward: 0.554 [0.452, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.331, 10.100], loss: 0.003261, mae: 0.059866, mean_q: 0.536070
 87961/100000: episode: 1843, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 5.805, mean reward: 0.484 [0.420, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.600], loss: 0.003752, mae: 0.067821, mean_q: 0.557677
[Info] FALSIFICATION!
 87971/100000: episode: 1844, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 15.579, mean reward: 1.558 [0.542, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.563, 10.093], loss: 0.003739, mae: 0.064838, mean_q: 0.533880
 88071/100000: episode: 1845, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -6.433, mean reward: -0.064 [-1.000, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.323, 10.098], loss: 0.043950, mae: 0.090019, mean_q: 0.559562
 88171/100000: episode: 1846, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -16.548, mean reward: -0.165 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.470, 10.221], loss: 0.003507, mae: 0.063373, mean_q: 0.549387
 88271/100000: episode: 1847, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -16.279, mean reward: -0.163 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.765, 10.255], loss: 0.003537, mae: 0.063000, mean_q: 0.542291
 88371/100000: episode: 1848, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -12.824, mean reward: -0.128 [-1.000, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.215, 10.098], loss: 0.003288, mae: 0.060127, mean_q: 0.545188
 88471/100000: episode: 1849, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -18.947, mean reward: -0.189 [-1.000, 0.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.941, 10.203], loss: 0.017604, mae: 0.076130, mean_q: 0.557156
 88571/100000: episode: 1850, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -16.076, mean reward: -0.161 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.084, 10.315], loss: 0.003738, mae: 0.063670, mean_q: 0.536858
 88671/100000: episode: 1851, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -17.755, mean reward: -0.178 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.904, 10.229], loss: 0.003518, mae: 0.062231, mean_q: 0.509491
 88771/100000: episode: 1852, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -18.853, mean reward: -0.189 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.288, 10.129], loss: 0.003423, mae: 0.062063, mean_q: 0.509802
 88871/100000: episode: 1853, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: -16.835, mean reward: -0.168 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.988, 10.098], loss: 0.003496, mae: 0.061847, mean_q: 0.485633
 88971/100000: episode: 1854, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.690, mean reward: -0.177 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.009, 10.098], loss: 0.003355, mae: 0.060982, mean_q: 0.448152
 89071/100000: episode: 1855, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -9.878, mean reward: -0.099 [-1.000, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.842, 10.098], loss: 0.017638, mae: 0.075197, mean_q: 0.439757
 89171/100000: episode: 1856, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -18.326, mean reward: -0.183 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.821, 10.098], loss: 0.003473, mae: 0.062792, mean_q: 0.436475
 89271/100000: episode: 1857, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: -15.215, mean reward: -0.152 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.062, 10.391], loss: 0.016644, mae: 0.068394, mean_q: 0.406853
 89371/100000: episode: 1858, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -17.107, mean reward: -0.171 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.332, 10.098], loss: 0.016656, mae: 0.067858, mean_q: 0.404882
 89471/100000: episode: 1859, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -16.345, mean reward: -0.163 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.367, 10.194], loss: 0.003334, mae: 0.060239, mean_q: 0.368323
 89571/100000: episode: 1860, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.525, mean reward: -0.165 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.403, 10.367], loss: 0.003395, mae: 0.060492, mean_q: 0.360992
 89671/100000: episode: 1861, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -18.741, mean reward: -0.187 [-1.000, 0.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.623, 10.098], loss: 0.003373, mae: 0.060817, mean_q: 0.354847
 89771/100000: episode: 1862, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -16.275, mean reward: -0.163 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.687, 10.140], loss: 0.030205, mae: 0.080549, mean_q: 0.320811
 89871/100000: episode: 1863, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -17.732, mean reward: -0.177 [-1.000, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.742, 10.098], loss: 0.003188, mae: 0.059388, mean_q: 0.284349
 89971/100000: episode: 1864, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.297, mean reward: -0.173 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.605, 10.142], loss: 0.003345, mae: 0.060753, mean_q: 0.284601
 90071/100000: episode: 1865, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -19.255, mean reward: -0.193 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.144, 10.098], loss: 0.003697, mae: 0.063908, mean_q: 0.252363
 90171/100000: episode: 1866, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -18.014, mean reward: -0.180 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.537, 10.098], loss: 0.029606, mae: 0.074411, mean_q: 0.276635
 90271/100000: episode: 1867, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -18.881, mean reward: -0.189 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.745, 10.236], loss: 0.016412, mae: 0.064997, mean_q: 0.223371
 90371/100000: episode: 1868, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: -19.379, mean reward: -0.194 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.417, 10.098], loss: 0.005151, mae: 0.066775, mean_q: 0.222886
 90471/100000: episode: 1869, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: -15.456, mean reward: -0.155 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.383, 10.098], loss: 0.017165, mae: 0.071887, mean_q: 0.199066
 90571/100000: episode: 1870, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.700, mean reward: -0.167 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.268, 10.098], loss: 0.029578, mae: 0.074276, mean_q: 0.178223
 90671/100000: episode: 1871, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -20.085, mean reward: -0.201 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.750, 10.102], loss: 0.004049, mae: 0.062236, mean_q: 0.160786
 90771/100000: episode: 1872, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -19.190, mean reward: -0.192 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.828, 10.194], loss: 0.031416, mae: 0.086858, mean_q: 0.159803
 90871/100000: episode: 1873, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.478, mean reward: -0.165 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.761, 10.098], loss: 0.029338, mae: 0.072187, mean_q: 0.134314
 90971/100000: episode: 1874, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -16.496, mean reward: -0.165 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.442, 10.145], loss: 0.003252, mae: 0.057224, mean_q: 0.100134
 91071/100000: episode: 1875, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -14.912, mean reward: -0.149 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.238, 10.291], loss: 0.003294, mae: 0.057144, mean_q: 0.102182
 91171/100000: episode: 1876, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.869, mean reward: -0.179 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.368, 10.360], loss: 0.016409, mae: 0.065493, mean_q: 0.047007
 91271/100000: episode: 1877, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.560, mean reward: -0.186 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.444, 10.127], loss: 0.003272, mae: 0.057345, mean_q: 0.064005
 91371/100000: episode: 1878, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.881, mean reward: -0.179 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.350, 10.315], loss: 0.016152, mae: 0.062539, mean_q: 0.017122
 91471/100000: episode: 1879, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -19.018, mean reward: -0.190 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.728, 10.098], loss: 0.016544, mae: 0.067662, mean_q: -0.000857
 91571/100000: episode: 1880, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: -14.313, mean reward: -0.143 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.021, 10.098], loss: 0.029821, mae: 0.075452, mean_q: -0.004879
 91671/100000: episode: 1881, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.683, mean reward: -0.187 [-1.000, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.435, 10.180], loss: 0.015648, mae: 0.059815, mean_q: -0.047499
 91771/100000: episode: 1882, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.630, mean reward: -0.166 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.390, 10.307], loss: 0.002952, mae: 0.053004, mean_q: -0.080014
 91871/100000: episode: 1883, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: -19.669, mean reward: -0.197 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.644, 10.288], loss: 0.015843, mae: 0.060498, mean_q: -0.076565
 91971/100000: episode: 1884, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: -18.749, mean reward: -0.187 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.832, 10.098], loss: 0.003011, mae: 0.053922, mean_q: -0.134837
 92071/100000: episode: 1885, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -15.875, mean reward: -0.159 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.305, 10.244], loss: 0.002868, mae: 0.053006, mean_q: -0.135308
 92171/100000: episode: 1886, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -17.308, mean reward: -0.173 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.001, 10.209], loss: 0.002897, mae: 0.053766, mean_q: -0.165797
 92271/100000: episode: 1887, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.983, mean reward: -0.190 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.529, 10.108], loss: 0.002769, mae: 0.051346, mean_q: -0.185654
 92371/100000: episode: 1888, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: -18.003, mean reward: -0.180 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.547, 10.320], loss: 0.016230, mae: 0.061194, mean_q: -0.203760
 92471/100000: episode: 1889, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -14.354, mean reward: -0.144 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.918, 10.362], loss: 0.016109, mae: 0.061678, mean_q: -0.236298
 92571/100000: episode: 1890, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: -18.104, mean reward: -0.181 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.892, 10.188], loss: 0.002939, mae: 0.051915, mean_q: -0.247815
 92671/100000: episode: 1891, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -16.629, mean reward: -0.166 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.051, 10.098], loss: 0.015171, mae: 0.055214, mean_q: -0.278994
 92771/100000: episode: 1892, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -15.707, mean reward: -0.157 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.518, 10.151], loss: 0.002899, mae: 0.052655, mean_q: -0.283684
 92871/100000: episode: 1893, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.739, mean reward: -0.177 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.653, 10.380], loss: 0.002580, mae: 0.049638, mean_q: -0.310974
 92971/100000: episode: 1894, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.013, mean reward: -0.160 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.751, 10.232], loss: 0.002645, mae: 0.049731, mean_q: -0.330174
 93071/100000: episode: 1895, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -18.526, mean reward: -0.185 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.611, 10.098], loss: 0.003107, mae: 0.056192, mean_q: -0.298166
 93171/100000: episode: 1896, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -18.160, mean reward: -0.182 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.825, 10.098], loss: 0.003337, mae: 0.058242, mean_q: -0.310909
 93271/100000: episode: 1897, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -20.805, mean reward: -0.208 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.535, 10.098], loss: 0.002515, mae: 0.048585, mean_q: -0.353282
 93371/100000: episode: 1898, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -17.290, mean reward: -0.173 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.494, 10.098], loss: 0.002573, mae: 0.049742, mean_q: -0.326428
 93471/100000: episode: 1899, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -17.912, mean reward: -0.179 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.456, 10.098], loss: 0.002500, mae: 0.047905, mean_q: -0.332520
 93571/100000: episode: 1900, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.952, mean reward: -0.160 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.318, 10.177], loss: 0.002727, mae: 0.050633, mean_q: -0.324246
 93671/100000: episode: 1901, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -20.642, mean reward: -0.206 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.336, 10.098], loss: 0.002463, mae: 0.048356, mean_q: -0.327382
 93771/100000: episode: 1902, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: -17.050, mean reward: -0.170 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.182, 10.098], loss: 0.002587, mae: 0.049477, mean_q: -0.319886
 93871/100000: episode: 1903, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.710, mean reward: -0.187 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.896, 10.233], loss: 0.002420, mae: 0.048533, mean_q: -0.355793
 93971/100000: episode: 1904, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -17.809, mean reward: -0.178 [-1.000, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.787, 10.189], loss: 0.002602, mae: 0.050156, mean_q: -0.290341
 94071/100000: episode: 1905, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -9.651, mean reward: -0.097 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.336, 10.355], loss: 0.002622, mae: 0.049715, mean_q: -0.347787
 94171/100000: episode: 1906, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: -18.394, mean reward: -0.184 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.860, 10.098], loss: 0.002638, mae: 0.051436, mean_q: -0.290212
 94271/100000: episode: 1907, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -17.864, mean reward: -0.179 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.729, 10.144], loss: 0.002487, mae: 0.049030, mean_q: -0.313302
 94371/100000: episode: 1908, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -16.188, mean reward: -0.162 [-1.000, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.587, 10.098], loss: 0.002482, mae: 0.049014, mean_q: -0.328719
 94471/100000: episode: 1909, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: -16.861, mean reward: -0.169 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.291, 10.098], loss: 0.002480, mae: 0.049778, mean_q: -0.326191
 94571/100000: episode: 1910, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -8.521, mean reward: -0.085 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.518, 10.275], loss: 0.002517, mae: 0.048984, mean_q: -0.305532
 94671/100000: episode: 1911, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.421, mean reward: -0.164 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.480, 10.294], loss: 0.002600, mae: 0.049748, mean_q: -0.321489
 94771/100000: episode: 1912, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: -17.244, mean reward: -0.172 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.348, 10.180], loss: 0.002474, mae: 0.049566, mean_q: -0.323257
 94871/100000: episode: 1913, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -19.405, mean reward: -0.194 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.290, 10.106], loss: 0.002380, mae: 0.047844, mean_q: -0.356652
 94971/100000: episode: 1914, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -17.186, mean reward: -0.172 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.712, 10.241], loss: 0.002468, mae: 0.049349, mean_q: -0.333219
 95071/100000: episode: 1915, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -19.161, mean reward: -0.192 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.996, 10.098], loss: 0.002362, mae: 0.047850, mean_q: -0.343752
 95171/100000: episode: 1916, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -15.377, mean reward: -0.154 [-1.000, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.543, 10.098], loss: 0.002413, mae: 0.048689, mean_q: -0.335722
 95271/100000: episode: 1917, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.231, mean reward: -0.162 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.197, 10.098], loss: 0.003026, mae: 0.056390, mean_q: -0.302091
 95371/100000: episode: 1918, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -15.053, mean reward: -0.151 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.387, 10.098], loss: 0.002516, mae: 0.049396, mean_q: -0.323695
 95471/100000: episode: 1919, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.451, mean reward: -0.175 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.772, 10.098], loss: 0.002350, mae: 0.047672, mean_q: -0.315316
 95571/100000: episode: 1920, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: -15.428, mean reward: -0.154 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.778, 10.339], loss: 0.002616, mae: 0.049816, mean_q: -0.308115
 95671/100000: episode: 1921, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: -20.412, mean reward: -0.204 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.775, 10.098], loss: 0.002940, mae: 0.053401, mean_q: -0.346280
 95771/100000: episode: 1922, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -8.571, mean reward: -0.086 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.329, 10.491], loss: 0.004852, mae: 0.063043, mean_q: -0.337504
 95871/100000: episode: 1923, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -11.021, mean reward: -0.110 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.713, 10.098], loss: 0.002512, mae: 0.049592, mean_q: -0.307707
 95971/100000: episode: 1924, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.711, mean reward: -0.187 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.895, 10.098], loss: 0.002323, mae: 0.047025, mean_q: -0.302731
 96071/100000: episode: 1925, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: -13.890, mean reward: -0.139 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.796, 10.098], loss: 0.002367, mae: 0.047505, mean_q: -0.297602
 96171/100000: episode: 1926, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -13.311, mean reward: -0.133 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.979, 10.098], loss: 0.002458, mae: 0.048087, mean_q: -0.302913
 96271/100000: episode: 1927, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: -16.214, mean reward: -0.162 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.932, 10.176], loss: 0.002302, mae: 0.046942, mean_q: -0.301482
 96371/100000: episode: 1928, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: -19.651, mean reward: -0.197 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.620, 10.116], loss: 0.002335, mae: 0.046832, mean_q: -0.325683
 96471/100000: episode: 1929, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -18.748, mean reward: -0.187 [-1.000, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.502, 10.177], loss: 0.002491, mae: 0.048918, mean_q: -0.322811
 96571/100000: episode: 1930, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: -16.894, mean reward: -0.169 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.444, 10.272], loss: 0.002462, mae: 0.047579, mean_q: -0.355563
 96671/100000: episode: 1931, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: -18.018, mean reward: -0.180 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.338, 10.098], loss: 0.002306, mae: 0.047254, mean_q: -0.309505
 96771/100000: episode: 1932, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -17.530, mean reward: -0.175 [-1.000, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.661, 10.224], loss: 0.002396, mae: 0.048295, mean_q: -0.322560
 96871/100000: episode: 1933, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.348, mean reward: -0.193 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.556, 10.103], loss: 0.002371, mae: 0.047953, mean_q: -0.316456
 96971/100000: episode: 1934, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.661, mean reward: -0.187 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.323, 10.227], loss: 0.002474, mae: 0.048374, mean_q: -0.325788
 97071/100000: episode: 1935, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: -18.448, mean reward: -0.184 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.852, 10.232], loss: 0.002402, mae: 0.048034, mean_q: -0.296352
 97171/100000: episode: 1936, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: -14.735, mean reward: -0.147 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.287, 10.328], loss: 0.002476, mae: 0.048010, mean_q: -0.299756
 97271/100000: episode: 1937, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: -19.712, mean reward: -0.197 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.269, 10.098], loss: 0.002324, mae: 0.047027, mean_q: -0.338476
 97371/100000: episode: 1938, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -14.328, mean reward: -0.143 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.219, 10.231], loss: 0.002368, mae: 0.047446, mean_q: -0.326087
 97471/100000: episode: 1939, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.997, mean reward: -0.190 [-1.000, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.785, 10.098], loss: 0.002716, mae: 0.052107, mean_q: -0.264217
 97571/100000: episode: 1940, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.288, mean reward: -0.173 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.471, 10.132], loss: 0.003146, mae: 0.052554, mean_q: -0.329539
 97671/100000: episode: 1941, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: -15.695, mean reward: -0.157 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.872, 10.108], loss: 0.002468, mae: 0.048895, mean_q: -0.287457
 97771/100000: episode: 1942, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -17.069, mean reward: -0.171 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.176, 10.105], loss: 0.002448, mae: 0.048294, mean_q: -0.332792
 97871/100000: episode: 1943, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: -16.734, mean reward: -0.167 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.897, 10.098], loss: 0.002523, mae: 0.048778, mean_q: -0.311628
[Info] 100-TH LEVEL FOUND: 0.653340220451355, Considering 10/90 traces
 97971/100000: episode: 1944, duration: 4.281s, episode steps: 100, steps per second: 23, episode reward: -15.331, mean reward: -0.153 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.403, 10.216], loss: 0.002399, mae: 0.048037, mean_q: -0.306529
 97995/100000: episode: 1945, duration: 0.115s, episode steps: 24, steps per second: 208, episode reward: 6.251, mean reward: 0.260 [0.057, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.258, 10.159], loss: 0.002353, mae: 0.048037, mean_q: -0.295974
 98014/100000: episode: 1946, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 5.990, mean reward: 0.315 [0.175, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.793, 10.100], loss: 0.002403, mae: 0.048663, mean_q: -0.376555
 98033/100000: episode: 1947, duration: 0.089s, episode steps: 19, steps per second: 213, episode reward: 5.898, mean reward: 0.310 [0.210, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.261, 10.100], loss: 0.002397, mae: 0.047884, mean_q: -0.290319
 98060/100000: episode: 1948, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 10.675, mean reward: 0.395 [0.253, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-1.586, 10.100], loss: 0.002312, mae: 0.048016, mean_q: -0.255293
 98080/100000: episode: 1949, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 6.779, mean reward: 0.339 [0.271, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.466, 10.100], loss: 0.002587, mae: 0.049330, mean_q: -0.365470
 98107/100000: episode: 1950, duration: 0.140s, episode steps: 27, steps per second: 192, episode reward: 6.626, mean reward: 0.245 [0.103, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.051, 10.106], loss: 0.002630, mae: 0.049792, mean_q: -0.293637
 98135/100000: episode: 1951, duration: 0.141s, episode steps: 28, steps per second: 198, episode reward: 8.979, mean reward: 0.321 [0.183, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.407, 10.100], loss: 0.002611, mae: 0.049378, mean_q: -0.318225
 98163/100000: episode: 1952, duration: 0.142s, episode steps: 28, steps per second: 198, episode reward: 6.664, mean reward: 0.238 [0.162, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.313, 10.100], loss: 0.002501, mae: 0.049857, mean_q: -0.232731
 98219/100000: episode: 1953, duration: 0.269s, episode steps: 56, steps per second: 208, episode reward: 16.887, mean reward: 0.302 [0.079, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-0.548, 10.187], loss: 0.002299, mae: 0.047224, mean_q: -0.306590
 98239/100000: episode: 1954, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 4.724, mean reward: 0.236 [0.130, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.207, 10.100], loss: 0.002306, mae: 0.047543, mean_q: -0.276412
 98266/100000: episode: 1955, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 9.563, mean reward: 0.354 [0.244, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.790, 10.100], loss: 0.002808, mae: 0.052098, mean_q: -0.242059
 98290/100000: episode: 1956, duration: 0.115s, episode steps: 24, steps per second: 209, episode reward: 8.622, mean reward: 0.359 [0.286, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.477], loss: 0.002650, mae: 0.050312, mean_q: -0.229707
 98317/100000: episode: 1957, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 4.894, mean reward: 0.181 [0.023, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.190, 10.100], loss: 0.002642, mae: 0.050367, mean_q: -0.247570
 98341/100000: episode: 1958, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 9.558, mean reward: 0.398 [0.295, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.123, 10.467], loss: 0.002881, mae: 0.053719, mean_q: -0.187604
 98360/100000: episode: 1959, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 5.052, mean reward: 0.266 [0.199, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.259, 10.100], loss: 0.003157, mae: 0.054742, mean_q: -0.192547
 98381/100000: episode: 1960, duration: 0.109s, episode steps: 21, steps per second: 192, episode reward: 5.994, mean reward: 0.285 [0.107, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.161, 10.368], loss: 0.002612, mae: 0.051686, mean_q: -0.173533
 98437/100000: episode: 1961, duration: 0.282s, episode steps: 56, steps per second: 199, episode reward: 23.527, mean reward: 0.420 [0.318, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.834 [-0.384, 10.403], loss: 0.002331, mae: 0.047527, mean_q: -0.222517
 98458/100000: episode: 1962, duration: 0.105s, episode steps: 21, steps per second: 201, episode reward: 6.270, mean reward: 0.299 [0.089, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.349, 10.270], loss: 0.002836, mae: 0.050953, mean_q: -0.225203
 98478/100000: episode: 1963, duration: 0.095s, episode steps: 20, steps per second: 210, episode reward: 5.057, mean reward: 0.253 [0.104, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.092, 10.100], loss: 0.002584, mae: 0.050537, mean_q: -0.213365
 98534/100000: episode: 1964, duration: 0.278s, episode steps: 56, steps per second: 202, episode reward: 15.296, mean reward: 0.273 [0.027, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.835 [-1.046, 10.100], loss: 0.002674, mae: 0.052904, mean_q: -0.202757
 98553/100000: episode: 1965, duration: 0.089s, episode steps: 19, steps per second: 214, episode reward: 7.625, mean reward: 0.401 [0.309, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.255, 10.100], loss: 0.002931, mae: 0.054257, mean_q: -0.224871
 98572/100000: episode: 1966, duration: 0.094s, episode steps: 19, steps per second: 202, episode reward: 5.779, mean reward: 0.304 [0.193, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.310, 10.100], loss: 0.003075, mae: 0.056867, mean_q: -0.191908
 98591/100000: episode: 1967, duration: 0.098s, episode steps: 19, steps per second: 195, episode reward: 4.608, mean reward: 0.243 [0.065, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.226, 10.100], loss: 0.002494, mae: 0.049072, mean_q: -0.214594
 98610/100000: episode: 1968, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 5.429, mean reward: 0.286 [0.173, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.155, 10.100], loss: 0.002595, mae: 0.050317, mean_q: -0.194584
 98637/100000: episode: 1969, duration: 0.124s, episode steps: 27, steps per second: 218, episode reward: 6.699, mean reward: 0.248 [0.042, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.072, 10.130], loss: 0.002402, mae: 0.049733, mean_q: -0.217093
 98654/100000: episode: 1970, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 5.207, mean reward: 0.306 [0.184, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.133, 10.349], loss: 0.002360, mae: 0.047997, mean_q: -0.168453
 98673/100000: episode: 1971, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 6.492, mean reward: 0.342 [0.211, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.398, 10.100], loss: 0.003009, mae: 0.056241, mean_q: -0.172885
 98692/100000: episode: 1972, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 5.293, mean reward: 0.279 [0.210, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.278, 10.100], loss: 0.002802, mae: 0.053426, mean_q: -0.148741
 98709/100000: episode: 1973, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 6.421, mean reward: 0.378 [0.273, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.522], loss: 0.002823, mae: 0.053333, mean_q: -0.190846
 98765/100000: episode: 1974, duration: 0.268s, episode steps: 56, steps per second: 209, episode reward: 18.174, mean reward: 0.325 [0.081, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-1.145, 10.409], loss: 0.003555, mae: 0.058426, mean_q: -0.149068
 98786/100000: episode: 1975, duration: 0.098s, episode steps: 21, steps per second: 215, episode reward: 4.380, mean reward: 0.209 [0.048, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.767, 10.152], loss: 0.002939, mae: 0.055492, mean_q: -0.131481
 98813/100000: episode: 1976, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 10.006, mean reward: 0.371 [0.265, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.432, 10.100], loss: 0.002731, mae: 0.052286, mean_q: -0.193794
 98833/100000: episode: 1977, duration: 0.102s, episode steps: 20, steps per second: 195, episode reward: 6.818, mean reward: 0.341 [0.235, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.628, 10.100], loss: 0.003200, mae: 0.057746, mean_q: -0.072108
 98861/100000: episode: 1978, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 10.196, mean reward: 0.364 [0.196, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.324, 10.100], loss: 0.002600, mae: 0.051241, mean_q: -0.171388
 98888/100000: episode: 1979, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 8.196, mean reward: 0.304 [0.091, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.670, 10.100], loss: 0.002800, mae: 0.052263, mean_q: -0.141641
 98915/100000: episode: 1980, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 7.667, mean reward: 0.284 [0.152, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.812, 10.100], loss: 0.002867, mae: 0.053015, mean_q: -0.110537
 98934/100000: episode: 1981, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 5.085, mean reward: 0.268 [0.155, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.402, 10.100], loss: 0.002359, mae: 0.047246, mean_q: -0.195786
 98990/100000: episode: 1982, duration: 0.303s, episode steps: 56, steps per second: 185, episode reward: 15.642, mean reward: 0.279 [0.073, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-0.759, 10.208], loss: 0.002616, mae: 0.051184, mean_q: -0.161367
 99014/100000: episode: 1983, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 7.867, mean reward: 0.328 [0.265, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.440], loss: 0.002783, mae: 0.052115, mean_q: -0.088113
 99033/100000: episode: 1984, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 4.373, mean reward: 0.230 [0.123, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.363, 10.100], loss: 0.002650, mae: 0.051346, mean_q: -0.085793
 99089/100000: episode: 1985, duration: 0.278s, episode steps: 56, steps per second: 201, episode reward: 17.808, mean reward: 0.318 [0.076, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-0.623, 10.163], loss: 0.002706, mae: 0.052481, mean_q: -0.105623
 99117/100000: episode: 1986, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 7.621, mean reward: 0.272 [0.068, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.974, 10.100], loss: 0.002837, mae: 0.052686, mean_q: -0.055029
 99138/100000: episode: 1987, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 7.044, mean reward: 0.335 [0.252, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.853, 10.329], loss: 0.002896, mae: 0.052410, mean_q: -0.098077
 99166/100000: episode: 1988, duration: 0.139s, episode steps: 28, steps per second: 202, episode reward: 8.913, mean reward: 0.318 [0.215, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.349, 10.100], loss: 0.003462, mae: 0.062750, mean_q: -0.065735
 99193/100000: episode: 1989, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 8.974, mean reward: 0.332 [0.164, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.215, 10.100], loss: 0.003733, mae: 0.058576, mean_q: -0.114022
 99249/100000: episode: 1990, duration: 0.266s, episode steps: 56, steps per second: 211, episode reward: 12.068, mean reward: 0.215 [0.098, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-1.076, 10.254], loss: 0.003295, mae: 0.059562, mean_q: -0.071916
 99305/100000: episode: 1991, duration: 0.287s, episode steps: 56, steps per second: 195, episode reward: 18.245, mean reward: 0.326 [0.104, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.845 [-0.756, 10.207], loss: 0.002795, mae: 0.053573, mean_q: -0.054386
 99333/100000: episode: 1992, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 7.667, mean reward: 0.274 [0.179, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.302, 10.100], loss: 0.002911, mae: 0.054771, mean_q: -0.037313
 99360/100000: episode: 1993, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 6.121, mean reward: 0.227 [0.024, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.454, 10.158], loss: 0.002716, mae: 0.053658, mean_q: -0.060713
 99381/100000: episode: 1994, duration: 0.111s, episode steps: 21, steps per second: 190, episode reward: 5.981, mean reward: 0.285 [0.148, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.735, 10.273], loss: 0.002863, mae: 0.052806, mean_q: -0.052918
 99398/100000: episode: 1995, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 6.278, mean reward: 0.369 [0.273, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.518], loss: 0.002960, mae: 0.055113, mean_q: -0.012669
 99425/100000: episode: 1996, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 6.411, mean reward: 0.237 [0.031, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.518, 10.100], loss: 0.003154, mae: 0.055862, mean_q: -0.064737
 99481/100000: episode: 1997, duration: 0.287s, episode steps: 56, steps per second: 195, episode reward: 14.315, mean reward: 0.256 [0.025, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.846 [-1.434, 10.206], loss: 0.002541, mae: 0.050780, mean_q: -0.102329
 99498/100000: episode: 1998, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 5.656, mean reward: 0.333 [0.285, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.727, 10.415], loss: 0.002589, mae: 0.051301, mean_q: -0.036662
 99554/100000: episode: 1999, duration: 0.281s, episode steps: 56, steps per second: 199, episode reward: 14.047, mean reward: 0.251 [0.106, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-1.550, 10.260], loss: 0.002988, mae: 0.054575, mean_q: -0.037301
 99581/100000: episode: 2000, duration: 0.133s, episode steps: 27, steps per second: 204, episode reward: 9.607, mean reward: 0.356 [0.254, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.251, 10.100], loss: 0.003176, mae: 0.056688, mean_q: -0.028604
 99637/100000: episode: 2001, duration: 0.283s, episode steps: 56, steps per second: 198, episode reward: 18.634, mean reward: 0.333 [0.209, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-0.380, 10.326], loss: 0.003422, mae: 0.059111, mean_q: -0.035302
 99664/100000: episode: 2002, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 9.089, mean reward: 0.337 [0.200, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.257, 10.100], loss: 0.002992, mae: 0.053499, mean_q: -0.013023
 99691/100000: episode: 2003, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 9.074, mean reward: 0.336 [0.228, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.150, 10.100], loss: 0.002747, mae: 0.053155, mean_q: 0.041712
 99710/100000: episode: 2004, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 7.679, mean reward: 0.404 [0.216, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.876, 10.100], loss: 0.003178, mae: 0.058341, mean_q: 0.004931
 99730/100000: episode: 2005, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 5.894, mean reward: 0.295 [0.182, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.321, 10.100], loss: 0.003112, mae: 0.057987, mean_q: 0.065636
 99749/100000: episode: 2006, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 8.266, mean reward: 0.435 [0.310, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.387, 10.100], loss: 0.003037, mae: 0.055283, mean_q: -0.007123
 99769/100000: episode: 2007, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 6.975, mean reward: 0.349 [0.276, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.213, 10.100], loss: 0.002949, mae: 0.054080, mean_q: 0.023571
 99789/100000: episode: 2008, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 5.776, mean reward: 0.289 [0.168, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.189, 10.100], loss: 0.002770, mae: 0.052806, mean_q: -0.012347
 99845/100000: episode: 2009, duration: 0.279s, episode steps: 56, steps per second: 200, episode reward: 15.350, mean reward: 0.274 [0.067, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.583, 10.183], loss: 0.002909, mae: 0.054801, mean_q: 0.034310
 99873/100000: episode: 2010, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 6.774, mean reward: 0.242 [0.080, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.721, 10.100], loss: 0.003044, mae: 0.056433, mean_q: 0.043331
 99929/100000: episode: 2011, duration: 0.267s, episode steps: 56, steps per second: 210, episode reward: 13.782, mean reward: 0.246 [0.046, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.834 [-1.270, 10.166], loss: 0.003505, mae: 0.060550, mean_q: 0.072655
 99949/100000: episode: 2012, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 8.593, mean reward: 0.430 [0.296, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.324, 10.100], loss: 0.002773, mae: 0.053673, mean_q: 0.044886
 99977/100000: episode: 2013, duration: 0.136s, episode steps: 28, steps per second: 206, episode reward: 8.745, mean reward: 0.312 [0.194, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.308, 10.100], loss: 0.003263, mae: 0.058831, mean_q: 0.018110
 99996/100000: episode: 2014, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 6.087, mean reward: 0.320 [0.140, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.881, 10.100], loss: 0.003244, mae: 0.057971, mean_q: 0.090117
done, took 574.550 seconds
[Info] End Importance Splitting. Falsification occurred 6 times.
