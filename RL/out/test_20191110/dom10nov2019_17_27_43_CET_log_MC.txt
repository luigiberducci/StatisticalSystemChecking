Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.174s, episode steps: 100, steps per second: 573, episode reward: -18.931, mean reward: -0.189 [-1.000, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.344, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.063s, episode steps: 100, steps per second: 1588, episode reward: -16.810, mean reward: -0.168 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.952, 10.164], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.082s, episode steps: 100, steps per second: 1225, episode reward: -14.101, mean reward: -0.141 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.519, 10.342], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.071s, episode steps: 100, steps per second: 1410, episode reward: -14.299, mean reward: -0.143 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.687, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.064s, episode steps: 100, steps per second: 1559, episode reward: -18.656, mean reward: -0.187 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.445, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.254s, episode steps: 100, steps per second: 80, episode reward: -18.793, mean reward: -0.188 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.873, 10.104], loss: 0.074552, mae: 0.271916, mean_q: -0.598528
   700/100000: episode: 7, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.322, mean reward: -0.183 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.853, 10.098], loss: 0.015666, mae: 0.121369, mean_q: -0.464735
   800/100000: episode: 8, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.398, mean reward: -0.164 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.371, 10.098], loss: 0.011802, mae: 0.102943, mean_q: -0.374733
   900/100000: episode: 9, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.693, mean reward: -0.177 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.852, 10.184], loss: 0.010285, mae: 0.097145, mean_q: -0.377825
  1000/100000: episode: 10, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.760, mean reward: -0.188 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.581, 10.098], loss: 0.008895, mae: 0.089336, mean_q: -0.357795
  1100/100000: episode: 11, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.843, mean reward: -0.178 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.664, 10.229], loss: 0.007623, mae: 0.081049, mean_q: -0.333351
  1200/100000: episode: 12, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -15.771, mean reward: -0.158 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.125, 10.132], loss: 0.007952, mae: 0.085058, mean_q: -0.331827
  1300/100000: episode: 13, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.644, mean reward: -0.176 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.691, 10.098], loss: 0.007908, mae: 0.084508, mean_q: -0.343136
  1400/100000: episode: 14, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.887, mean reward: -0.189 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.502, 10.098], loss: 0.007728, mae: 0.083096, mean_q: -0.358921
  1500/100000: episode: 15, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -13.994, mean reward: -0.140 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.916, 10.452], loss: 0.007138, mae: 0.081719, mean_q: -0.305935
  1600/100000: episode: 16, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -16.568, mean reward: -0.166 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.690, 10.098], loss: 0.007494, mae: 0.080570, mean_q: -0.342279
  1700/100000: episode: 17, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.259, mean reward: -0.173 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.756, 10.098], loss: 0.007359, mae: 0.081946, mean_q: -0.294457
  1800/100000: episode: 18, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -13.456, mean reward: -0.135 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.957, 10.098], loss: 0.007354, mae: 0.083269, mean_q: -0.336407
  1900/100000: episode: 19, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -15.912, mean reward: -0.159 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.855, 10.098], loss: 0.006449, mae: 0.077575, mean_q: -0.337599
  2000/100000: episode: 20, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.844, mean reward: -0.178 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.723, 10.160], loss: 0.005953, mae: 0.075301, mean_q: -0.323930
  2100/100000: episode: 21, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.889, mean reward: -0.149 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.335, 10.098], loss: 0.005598, mae: 0.072444, mean_q: -0.303244
  2200/100000: episode: 22, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.750, mean reward: -0.148 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.420, 10.226], loss: 0.007175, mae: 0.078360, mean_q: -0.315221
  2300/100000: episode: 23, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -19.574, mean reward: -0.196 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.994, 10.203], loss: 0.005536, mae: 0.072440, mean_q: -0.320389
  2400/100000: episode: 24, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -17.907, mean reward: -0.179 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.175, 10.317], loss: 0.005940, mae: 0.074339, mean_q: -0.329624
  2500/100000: episode: 25, duration: 0.593s, episode steps: 100, steps per second: 168, episode reward: -14.983, mean reward: -0.150 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.256, 10.098], loss: 0.005100, mae: 0.069458, mean_q: -0.332046
  2600/100000: episode: 26, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -14.871, mean reward: -0.149 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.586, 10.273], loss: 0.005924, mae: 0.073791, mean_q: -0.330767
  2700/100000: episode: 27, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.483, mean reward: -0.175 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.204, 10.232], loss: 0.005743, mae: 0.073903, mean_q: -0.317932
  2800/100000: episode: 28, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -15.570, mean reward: -0.156 [-1.000, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.167, 10.098], loss: 0.006106, mae: 0.072995, mean_q: -0.323204
  2900/100000: episode: 29, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -14.689, mean reward: -0.147 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.726, 10.448], loss: 0.005126, mae: 0.069221, mean_q: -0.317046
  3000/100000: episode: 30, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.431, mean reward: -0.174 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.661, 10.098], loss: 0.006453, mae: 0.079343, mean_q: -0.319708
  3100/100000: episode: 31, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -16.576, mean reward: -0.166 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.880, 10.098], loss: 0.005135, mae: 0.071400, mean_q: -0.293731
  3200/100000: episode: 32, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.970, mean reward: -0.190 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.251, 10.187], loss: 0.005159, mae: 0.071149, mean_q: -0.336474
  3300/100000: episode: 33, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.559, mean reward: -0.166 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.790, 10.098], loss: 0.004477, mae: 0.066509, mean_q: -0.339892
  3400/100000: episode: 34, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -16.982, mean reward: -0.170 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.897, 10.098], loss: 0.004552, mae: 0.065906, mean_q: -0.314160
  3500/100000: episode: 35, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.986, mean reward: -0.190 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.528, 10.277], loss: 0.004952, mae: 0.070600, mean_q: -0.327960
  3600/100000: episode: 36, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -14.152, mean reward: -0.142 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.035, 10.098], loss: 0.004744, mae: 0.068713, mean_q: -0.336656
  3700/100000: episode: 37, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -15.892, mean reward: -0.159 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.892, 10.144], loss: 0.006102, mae: 0.076257, mean_q: -0.302352
  3800/100000: episode: 38, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -14.987, mean reward: -0.150 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.447, 10.098], loss: 0.004836, mae: 0.068399, mean_q: -0.344727
  3900/100000: episode: 39, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.703, mean reward: -0.187 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.630, 10.311], loss: 0.005365, mae: 0.071163, mean_q: -0.324592
  4000/100000: episode: 40, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -13.388, mean reward: -0.134 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.652, 10.098], loss: 0.004888, mae: 0.070750, mean_q: -0.325777
  4100/100000: episode: 41, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.513, mean reward: -0.195 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.831, 10.166], loss: 0.004550, mae: 0.066865, mean_q: -0.330600
  4200/100000: episode: 42, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.614, mean reward: -0.186 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.299, 10.098], loss: 0.004680, mae: 0.068320, mean_q: -0.291428
  4300/100000: episode: 43, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -19.138, mean reward: -0.191 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.321, 10.098], loss: 0.004506, mae: 0.065825, mean_q: -0.349916
  4400/100000: episode: 44, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.244, mean reward: -0.162 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.858, 10.098], loss: 0.004191, mae: 0.064335, mean_q: -0.309485
  4500/100000: episode: 45, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -20.224, mean reward: -0.202 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.795, 10.098], loss: 0.004216, mae: 0.064619, mean_q: -0.289002
  4600/100000: episode: 46, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.058, mean reward: -0.191 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.577, 10.160], loss: 0.003942, mae: 0.065464, mean_q: -0.314961
  4700/100000: episode: 47, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.444, mean reward: -0.174 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.840, 10.098], loss: 0.004330, mae: 0.065978, mean_q: -0.287631
  4800/100000: episode: 48, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.123, mean reward: -0.191 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.776, 10.151], loss: 0.004654, mae: 0.069963, mean_q: -0.320922
  4900/100000: episode: 49, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -16.605, mean reward: -0.166 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.009, 10.098], loss: 0.004528, mae: 0.067514, mean_q: -0.301166
  5000/100000: episode: 50, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -17.296, mean reward: -0.173 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.226, 10.098], loss: 0.003975, mae: 0.061100, mean_q: -0.323436
  5100/100000: episode: 51, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -14.655, mean reward: -0.147 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.863, 10.441], loss: 0.003787, mae: 0.063122, mean_q: -0.317125
  5200/100000: episode: 52, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -14.212, mean reward: -0.142 [-1.000, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.585, 10.098], loss: 0.004609, mae: 0.068092, mean_q: -0.320749
  5300/100000: episode: 53, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.022, mean reward: -0.180 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.486, 10.098], loss: 0.004700, mae: 0.067320, mean_q: -0.317387
  5400/100000: episode: 54, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -17.521, mean reward: -0.175 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.011, 10.098], loss: 0.004029, mae: 0.063366, mean_q: -0.332037
  5500/100000: episode: 55, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.498, mean reward: -0.175 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.433, 10.098], loss: 0.004307, mae: 0.065185, mean_q: -0.313244
  5600/100000: episode: 56, duration: 0.623s, episode steps: 100, steps per second: 161, episode reward: -11.814, mean reward: -0.118 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.408, 10.487], loss: 0.004983, mae: 0.068806, mean_q: -0.336993
  5700/100000: episode: 57, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.401, mean reward: -0.164 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.340, 10.103], loss: 0.003702, mae: 0.061640, mean_q: -0.315780
  5800/100000: episode: 58, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -15.758, mean reward: -0.158 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.609, 10.098], loss: 0.003558, mae: 0.057618, mean_q: -0.325637
  5900/100000: episode: 59, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.950, mean reward: -0.190 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.719, 10.098], loss: 0.004373, mae: 0.064888, mean_q: -0.316652
  6000/100000: episode: 60, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.547, mean reward: -0.165 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.157, 10.098], loss: 0.004929, mae: 0.067803, mean_q: -0.315750
  6100/100000: episode: 61, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -16.404, mean reward: -0.164 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.447, 10.098], loss: 0.003761, mae: 0.062700, mean_q: -0.313785
  6200/100000: episode: 62, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -10.139, mean reward: -0.101 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.348, 10.222], loss: 0.004203, mae: 0.065409, mean_q: -0.308516
  6300/100000: episode: 63, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -17.220, mean reward: -0.172 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.476, 10.098], loss: 0.004261, mae: 0.065493, mean_q: -0.314559
  6400/100000: episode: 64, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -16.108, mean reward: -0.161 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.639, 10.098], loss: 0.003675, mae: 0.060907, mean_q: -0.316738
  6500/100000: episode: 65, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -19.440, mean reward: -0.194 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.741, 10.110], loss: 0.003951, mae: 0.064908, mean_q: -0.302702
  6600/100000: episode: 66, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -13.661, mean reward: -0.137 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.372, 10.140], loss: 0.004394, mae: 0.066120, mean_q: -0.320861
  6700/100000: episode: 67, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.829, mean reward: -0.178 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.165, 10.238], loss: 0.004598, mae: 0.066306, mean_q: -0.328146
  6800/100000: episode: 68, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.703, mean reward: -0.187 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.647, 10.098], loss: 0.003942, mae: 0.063023, mean_q: -0.319701
  6900/100000: episode: 69, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.362, mean reward: -0.184 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.979, 10.098], loss: 0.003626, mae: 0.060855, mean_q: -0.347082
  7000/100000: episode: 70, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.720, mean reward: -0.147 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.333, 10.243], loss: 0.003828, mae: 0.062921, mean_q: -0.320390
  7100/100000: episode: 71, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.630, mean reward: -0.156 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.651, 10.098], loss: 0.004040, mae: 0.065281, mean_q: -0.330772
  7200/100000: episode: 72, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.192, mean reward: -0.182 [-1.000, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.391, 10.347], loss: 0.004222, mae: 0.066454, mean_q: -0.305597
  7300/100000: episode: 73, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -6.793, mean reward: -0.068 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.328, 10.331], loss: 0.003892, mae: 0.063667, mean_q: -0.300020
  7400/100000: episode: 74, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.136, mean reward: -0.191 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.276, 10.134], loss: 0.003871, mae: 0.063787, mean_q: -0.322763
  7500/100000: episode: 75, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.296, mean reward: -0.163 [-1.000, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.883, 10.098], loss: 0.003634, mae: 0.060488, mean_q: -0.312126
  7600/100000: episode: 76, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.668, mean reward: -0.167 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.772, 10.132], loss: 0.004177, mae: 0.065515, mean_q: -0.281055
  7700/100000: episode: 77, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.481, mean reward: -0.155 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.516, 10.098], loss: 0.004450, mae: 0.066198, mean_q: -0.295362
  7800/100000: episode: 78, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.403, mean reward: -0.184 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.391, 10.098], loss: 0.003852, mae: 0.063652, mean_q: -0.293203
  7900/100000: episode: 79, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.584, mean reward: -0.196 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.171, 10.139], loss: 0.003934, mae: 0.064191, mean_q: -0.307944
  8000/100000: episode: 80, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.475, mean reward: -0.185 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.216, 10.098], loss: 0.004375, mae: 0.066442, mean_q: -0.310543
  8100/100000: episode: 81, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.297, mean reward: -0.163 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.905, 10.098], loss: 0.004067, mae: 0.064341, mean_q: -0.297522
  8200/100000: episode: 82, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.009, mean reward: -0.190 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.488, 10.098], loss: 0.003951, mae: 0.063328, mean_q: -0.358735
  8300/100000: episode: 83, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -19.049, mean reward: -0.190 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.300, 10.098], loss: 0.003750, mae: 0.063458, mean_q: -0.321448
  8400/100000: episode: 84, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.655, mean reward: -0.187 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.646, 10.098], loss: 0.004092, mae: 0.066547, mean_q: -0.282459
  8500/100000: episode: 85, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.552, mean reward: -0.156 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.905, 10.115], loss: 0.004112, mae: 0.066141, mean_q: -0.315060
  8600/100000: episode: 86, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.859, mean reward: -0.179 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.575, 10.098], loss: 0.004842, mae: 0.069970, mean_q: -0.318648
  8700/100000: episode: 87, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -14.252, mean reward: -0.143 [-1.000, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.292, 10.348], loss: 0.004186, mae: 0.066988, mean_q: -0.321244
  8800/100000: episode: 88, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -16.603, mean reward: -0.166 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.447, 10.108], loss: 0.004219, mae: 0.067225, mean_q: -0.311741
  8900/100000: episode: 89, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -12.266, mean reward: -0.123 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.772, 10.205], loss: 0.003391, mae: 0.059502, mean_q: -0.328759
  9000/100000: episode: 90, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -8.736, mean reward: -0.087 [-1.000, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.702, 10.152], loss: 0.003593, mae: 0.061542, mean_q: -0.338836
  9100/100000: episode: 91, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.286, mean reward: -0.193 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.883, 10.098], loss: 0.004201, mae: 0.066328, mean_q: -0.293369
  9200/100000: episode: 92, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.626, mean reward: -0.156 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.094, 10.098], loss: 0.003822, mae: 0.063448, mean_q: -0.325146
  9300/100000: episode: 93, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.065, mean reward: -0.161 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.406, 10.098], loss: 0.006330, mae: 0.076342, mean_q: -0.319644
  9400/100000: episode: 94, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.455, mean reward: -0.155 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.961, 10.252], loss: 0.003796, mae: 0.062982, mean_q: -0.307228
  9500/100000: episode: 95, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.294, mean reward: -0.173 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.699, 10.349], loss: 0.004540, mae: 0.066148, mean_q: -0.303596
  9600/100000: episode: 96, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -13.861, mean reward: -0.139 [-1.000, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.138, 10.223], loss: 0.004466, mae: 0.069020, mean_q: -0.323636
  9700/100000: episode: 97, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -18.929, mean reward: -0.189 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.679, 10.265], loss: 0.003492, mae: 0.060757, mean_q: -0.331694
  9800/100000: episode: 98, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.754, mean reward: -0.148 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.778, 10.261], loss: 0.003915, mae: 0.063901, mean_q: -0.281748
  9900/100000: episode: 99, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -15.927, mean reward: -0.159 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.117, 10.098], loss: 0.004154, mae: 0.063995, mean_q: -0.323133
 10000/100000: episode: 100, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.641, mean reward: -0.196 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.395, 10.098], loss: 0.003882, mae: 0.063972, mean_q: -0.286515
 10100/100000: episode: 101, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.000, mean reward: -0.150 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.490, 10.520], loss: 0.003664, mae: 0.062069, mean_q: -0.333964
 10200/100000: episode: 102, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.361, mean reward: -0.184 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.260, 10.101], loss: 0.003920, mae: 0.065190, mean_q: -0.309455
 10300/100000: episode: 103, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.598, mean reward: -0.186 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.420, 10.222], loss: 0.004234, mae: 0.066895, mean_q: -0.296281
 10400/100000: episode: 104, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.829, mean reward: -0.168 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.997, 10.357], loss: 0.003416, mae: 0.060109, mean_q: -0.344790
 10500/100000: episode: 105, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.857, mean reward: -0.189 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.040, 10.340], loss: 0.004351, mae: 0.066999, mean_q: -0.284685
 10600/100000: episode: 106, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -19.730, mean reward: -0.197 [-1.000, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.357, 10.329], loss: 0.003845, mae: 0.062334, mean_q: -0.284861
 10700/100000: episode: 107, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.500, mean reward: -0.155 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.908, 10.098], loss: 0.004282, mae: 0.065486, mean_q: -0.298347
 10800/100000: episode: 108, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -9.315, mean reward: -0.093 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.199, 10.098], loss: 0.004460, mae: 0.066723, mean_q: -0.331501
 10900/100000: episode: 109, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -20.114, mean reward: -0.201 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.008, 10.179], loss: 0.003928, mae: 0.063893, mean_q: -0.301663
 11000/100000: episode: 110, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.679, mean reward: -0.187 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.982, 10.098], loss: 0.004068, mae: 0.065238, mean_q: -0.280626
 11100/100000: episode: 111, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.561, mean reward: -0.166 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.133, 10.241], loss: 0.003451, mae: 0.058967, mean_q: -0.344872
 11200/100000: episode: 112, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.035, mean reward: -0.130 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.305, 10.098], loss: 0.004288, mae: 0.064332, mean_q: -0.330644
 11300/100000: episode: 113, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.332, mean reward: -0.163 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.881, 10.135], loss: 0.005028, mae: 0.071748, mean_q: -0.324326
 11400/100000: episode: 114, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.925, mean reward: -0.189 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.253, 10.098], loss: 0.003509, mae: 0.059592, mean_q: -0.292957
 11500/100000: episode: 115, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.992, mean reward: -0.190 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.074, 10.098], loss: 0.005140, mae: 0.070561, mean_q: -0.292667
 11600/100000: episode: 116, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.913, mean reward: -0.169 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.692, 10.098], loss: 0.005383, mae: 0.070228, mean_q: -0.329729
 11700/100000: episode: 117, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -20.211, mean reward: -0.202 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.513, 10.275], loss: 0.003615, mae: 0.060718, mean_q: -0.325920
 11800/100000: episode: 118, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -12.178, mean reward: -0.122 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.390, 10.098], loss: 0.003867, mae: 0.061924, mean_q: -0.302491
 11900/100000: episode: 119, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -14.357, mean reward: -0.144 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.264, 10.239], loss: 0.003671, mae: 0.061608, mean_q: -0.277783
 12000/100000: episode: 120, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -8.647, mean reward: -0.086 [-1.000, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.836, 10.165], loss: 0.003498, mae: 0.060000, mean_q: -0.307328
 12100/100000: episode: 121, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -17.194, mean reward: -0.172 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.926, 10.157], loss: 0.003756, mae: 0.061554, mean_q: -0.302413
 12200/100000: episode: 122, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.871, mean reward: -0.169 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.498, 10.141], loss: 0.003418, mae: 0.058971, mean_q: -0.313263
 12300/100000: episode: 123, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.804, mean reward: -0.158 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.568, 10.098], loss: 0.003588, mae: 0.060463, mean_q: -0.290651
 12400/100000: episode: 124, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: -17.822, mean reward: -0.178 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.661, 10.207], loss: 0.003770, mae: 0.063231, mean_q: -0.312837
 12500/100000: episode: 125, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -17.999, mean reward: -0.180 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.923, 10.098], loss: 0.003791, mae: 0.063489, mean_q: -0.309386
 12600/100000: episode: 126, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -14.510, mean reward: -0.145 [-1.000, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.720, 10.470], loss: 0.003754, mae: 0.062720, mean_q: -0.273559
 12700/100000: episode: 127, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.617, mean reward: -0.166 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.543, 10.098], loss: 0.006914, mae: 0.078129, mean_q: -0.317666
 12800/100000: episode: 128, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.571, mean reward: -0.186 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.775, 10.169], loss: 0.004401, mae: 0.066615, mean_q: -0.309313
 12900/100000: episode: 129, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.097, mean reward: -0.171 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.505, 10.311], loss: 0.003677, mae: 0.060843, mean_q: -0.292625
 13000/100000: episode: 130, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.174, mean reward: -0.162 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.821, 10.148], loss: 0.004045, mae: 0.064390, mean_q: -0.303213
 13100/100000: episode: 131, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -17.846, mean reward: -0.178 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.873, 10.098], loss: 0.003392, mae: 0.058862, mean_q: -0.332155
 13200/100000: episode: 132, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.485, mean reward: -0.185 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.775, 10.108], loss: 0.003447, mae: 0.061182, mean_q: -0.266632
 13300/100000: episode: 133, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.096, mean reward: -0.161 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.371, 10.418], loss: 0.003303, mae: 0.057831, mean_q: -0.304780
 13400/100000: episode: 134, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.697, mean reward: -0.167 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.846, 10.115], loss: 0.003786, mae: 0.061653, mean_q: -0.318916
 13500/100000: episode: 135, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -20.882, mean reward: -0.209 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.324, 10.142], loss: 0.004308, mae: 0.064485, mean_q: -0.309463
 13600/100000: episode: 136, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.314, mean reward: -0.153 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.337, 10.244], loss: 0.004342, mae: 0.064562, mean_q: -0.307812
 13700/100000: episode: 137, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.334, mean reward: -0.173 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.588, 10.235], loss: 0.007012, mae: 0.077511, mean_q: -0.284484
 13800/100000: episode: 138, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.979, mean reward: -0.180 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.280, 10.101], loss: 0.003502, mae: 0.059865, mean_q: -0.304926
 13900/100000: episode: 139, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.631, mean reward: -0.176 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.721, 10.127], loss: 0.003239, mae: 0.057910, mean_q: -0.300893
 14000/100000: episode: 140, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.501, mean reward: -0.195 [-1.000, 0.254], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.611, 10.098], loss: 0.003199, mae: 0.057225, mean_q: -0.306985
 14100/100000: episode: 141, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.551, mean reward: -0.176 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.554, 10.285], loss: 0.003355, mae: 0.058545, mean_q: -0.323025
 14200/100000: episode: 142, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.143, mean reward: -0.151 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.500, 10.334], loss: 0.003677, mae: 0.062503, mean_q: -0.315484
 14300/100000: episode: 143, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -19.469, mean reward: -0.195 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.264, 10.166], loss: 0.003690, mae: 0.062269, mean_q: -0.342008
 14400/100000: episode: 144, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -19.527, mean reward: -0.195 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.025, 10.232], loss: 0.003244, mae: 0.057900, mean_q: -0.272242
 14500/100000: episode: 145, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.234, mean reward: -0.162 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.685, 10.267], loss: 0.002782, mae: 0.053447, mean_q: -0.349244
 14600/100000: episode: 146, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.748, mean reward: -0.197 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.056, 10.098], loss: 0.002845, mae: 0.053736, mean_q: -0.303154
 14700/100000: episode: 147, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -18.683, mean reward: -0.187 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.133, 10.190], loss: 0.003029, mae: 0.055809, mean_q: -0.301272
 14800/100000: episode: 148, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.188, mean reward: -0.172 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.109, 10.098], loss: 0.002885, mae: 0.054116, mean_q: -0.320834
 14900/100000: episode: 149, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.152, mean reward: -0.142 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.893, 10.468], loss: 0.003060, mae: 0.055851, mean_q: -0.311094
 15000/100000: episode: 150, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.675, mean reward: -0.167 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.525, 10.098], loss: 0.005106, mae: 0.070523, mean_q: -0.322268
 15100/100000: episode: 151, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -16.219, mean reward: -0.162 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.746, 10.308], loss: 0.003148, mae: 0.057814, mean_q: -0.314094
 15200/100000: episode: 152, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.222, mean reward: -0.172 [-1.000, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.586, 10.139], loss: 0.003019, mae: 0.054627, mean_q: -0.335947
 15300/100000: episode: 153, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.130, mean reward: -0.181 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.826, 10.246], loss: 0.003206, mae: 0.059384, mean_q: -0.292700
 15400/100000: episode: 154, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.606, mean reward: -0.176 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.791, 10.240], loss: 0.003226, mae: 0.058336, mean_q: -0.313340
 15500/100000: episode: 155, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -19.954, mean reward: -0.200 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.301, 10.098], loss: 0.003033, mae: 0.055246, mean_q: -0.355964
 15600/100000: episode: 156, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.568, mean reward: -0.146 [-1.000, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.371, 10.098], loss: 0.003303, mae: 0.060796, mean_q: -0.317131
 15700/100000: episode: 157, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.897, mean reward: -0.169 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.905, 10.237], loss: 0.003470, mae: 0.059339, mean_q: -0.301506
 15800/100000: episode: 158, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -21.600, mean reward: -0.216 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.843, 10.104], loss: 0.002882, mae: 0.054233, mean_q: -0.303251
 15900/100000: episode: 159, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.892, mean reward: -0.169 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.735, 10.098], loss: 0.002830, mae: 0.053722, mean_q: -0.294304
 16000/100000: episode: 160, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.706, mean reward: -0.177 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.528, 10.302], loss: 0.002974, mae: 0.054819, mean_q: -0.330607
 16100/100000: episode: 161, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -13.543, mean reward: -0.135 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.826, 10.389], loss: 0.003006, mae: 0.055664, mean_q: -0.278358
 16200/100000: episode: 162, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -20.901, mean reward: -0.209 [-1.000, 0.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.182, 10.098], loss: 0.002905, mae: 0.054178, mean_q: -0.328208
 16300/100000: episode: 163, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.317, mean reward: -0.193 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.210, 10.175], loss: 0.002909, mae: 0.054443, mean_q: -0.303554
 16400/100000: episode: 164, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -14.180, mean reward: -0.142 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.208, 10.098], loss: 0.003138, mae: 0.056984, mean_q: -0.315380
 16500/100000: episode: 165, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.608, mean reward: -0.186 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.735, 10.159], loss: 0.002808, mae: 0.053176, mean_q: -0.329570
 16600/100000: episode: 166, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -12.506, mean reward: -0.125 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.210, 10.098], loss: 0.002863, mae: 0.054454, mean_q: -0.336619
 16700/100000: episode: 167, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.266, mean reward: -0.173 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.199, 10.098], loss: 0.002901, mae: 0.054811, mean_q: -0.333353
 16800/100000: episode: 168, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.389, mean reward: -0.194 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.641, 10.103], loss: 0.002926, mae: 0.054599, mean_q: -0.304285
 16900/100000: episode: 169, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -15.275, mean reward: -0.153 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.651, 10.289], loss: 0.003113, mae: 0.057431, mean_q: -0.333497
 17000/100000: episode: 170, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -8.314, mean reward: -0.083 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.147, 10.467], loss: 0.003029, mae: 0.056589, mean_q: -0.333954
 17100/100000: episode: 171, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -19.110, mean reward: -0.191 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.688, 10.229], loss: 0.004877, mae: 0.068010, mean_q: -0.331329
 17200/100000: episode: 172, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -11.393, mean reward: -0.114 [-1.000, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.591, 10.369], loss: 0.005598, mae: 0.070451, mean_q: -0.327064
 17300/100000: episode: 173, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.668, mean reward: -0.167 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.701, 10.098], loss: 0.003617, mae: 0.062126, mean_q: -0.340819
 17400/100000: episode: 174, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -9.925, mean reward: -0.099 [-1.000, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.369, 10.412], loss: 0.003362, mae: 0.059950, mean_q: -0.282526
 17500/100000: episode: 175, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.540, mean reward: -0.155 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.001, 10.329], loss: 0.003418, mae: 0.058328, mean_q: -0.321532
 17600/100000: episode: 176, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -18.970, mean reward: -0.190 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.034, 10.159], loss: 0.002992, mae: 0.054900, mean_q: -0.302340
 17700/100000: episode: 177, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -17.909, mean reward: -0.179 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.251, 10.098], loss: 0.003053, mae: 0.055970, mean_q: -0.318994
 17800/100000: episode: 178, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.574, mean reward: -0.156 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.782, 10.344], loss: 0.003521, mae: 0.062286, mean_q: -0.312877
 17900/100000: episode: 179, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.225, mean reward: -0.192 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.199, 10.098], loss: 0.002989, mae: 0.054906, mean_q: -0.338635
 18000/100000: episode: 180, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.929, mean reward: -0.169 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.507, 10.098], loss: 0.002833, mae: 0.053855, mean_q: -0.317415
 18100/100000: episode: 181, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.595, mean reward: -0.186 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.504, 10.098], loss: 0.003250, mae: 0.059226, mean_q: -0.283987
 18200/100000: episode: 182, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -18.554, mean reward: -0.186 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.462, 10.251], loss: 0.003116, mae: 0.056364, mean_q: -0.313653
 18300/100000: episode: 183, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.511, mean reward: -0.175 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.177, 10.117], loss: 0.002793, mae: 0.053493, mean_q: -0.309964
 18400/100000: episode: 184, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.000, mean reward: -0.160 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.840, 10.384], loss: 0.003692, mae: 0.061393, mean_q: -0.314739
 18500/100000: episode: 185, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -10.826, mean reward: -0.108 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.481, 10.098], loss: 0.003196, mae: 0.057715, mean_q: -0.338201
 18600/100000: episode: 186, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.878, mean reward: -0.189 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.146, 10.098], loss: 0.003325, mae: 0.060163, mean_q: -0.305226
 18700/100000: episode: 187, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.578, mean reward: -0.166 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.494, 10.274], loss: 0.003034, mae: 0.055443, mean_q: -0.308659
 18800/100000: episode: 188, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -16.743, mean reward: -0.167 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.708, 10.339], loss: 0.003105, mae: 0.055320, mean_q: -0.332291
 18900/100000: episode: 189, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.138, mean reward: -0.141 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.862, 10.340], loss: 0.003032, mae: 0.056532, mean_q: -0.311688
 19000/100000: episode: 190, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.878, mean reward: -0.189 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.476, 10.141], loss: 0.003126, mae: 0.056616, mean_q: -0.280835
 19100/100000: episode: 191, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.571, mean reward: -0.166 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.827, 10.098], loss: 0.002967, mae: 0.054637, mean_q: -0.314983
 19200/100000: episode: 192, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -18.440, mean reward: -0.184 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.613, 10.098], loss: 0.002900, mae: 0.054248, mean_q: -0.329555
 19300/100000: episode: 193, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -20.248, mean reward: -0.202 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.171, 10.217], loss: 0.003228, mae: 0.057187, mean_q: -0.330045
 19400/100000: episode: 194, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.477, mean reward: -0.155 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.197, 10.241], loss: 0.002940, mae: 0.053960, mean_q: -0.354445
 19500/100000: episode: 195, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.092, mean reward: -0.171 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.848, 10.098], loss: 0.003262, mae: 0.058182, mean_q: -0.275537
 19600/100000: episode: 196, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.482, mean reward: -0.185 [-1.000, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.145, 10.098], loss: 0.003196, mae: 0.057654, mean_q: -0.282580
 19700/100000: episode: 197, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.238, mean reward: -0.182 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.897, 10.098], loss: 0.004749, mae: 0.069705, mean_q: -0.314843
 19800/100000: episode: 198, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -19.126, mean reward: -0.191 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.326, 10.212], loss: 0.002885, mae: 0.054286, mean_q: -0.327474
 19900/100000: episode: 199, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -16.685, mean reward: -0.167 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.970, 10.130], loss: 0.003286, mae: 0.057157, mean_q: -0.323838
 20000/100000: episode: 200, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -12.088, mean reward: -0.121 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.716, 10.098], loss: 0.002948, mae: 0.054087, mean_q: -0.344887
 20100/100000: episode: 201, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.400, mean reward: -0.194 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.184, 10.111], loss: 0.004907, mae: 0.067212, mean_q: -0.299602
 20200/100000: episode: 202, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.710, mean reward: -0.197 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.424, 10.114], loss: 0.006578, mae: 0.076240, mean_q: -0.303516
 20300/100000: episode: 203, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.259, mean reward: -0.183 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.766, 10.121], loss: 0.003429, mae: 0.062798, mean_q: -0.303637
 20400/100000: episode: 204, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -18.732, mean reward: -0.187 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.537, 10.098], loss: 0.003093, mae: 0.057117, mean_q: -0.317070
 20500/100000: episode: 205, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.951, mean reward: -0.170 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.796, 10.253], loss: 0.002942, mae: 0.054483, mean_q: -0.331210
 20600/100000: episode: 206, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -15.153, mean reward: -0.152 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.007, 10.098], loss: 0.003086, mae: 0.055611, mean_q: -0.351110
 20700/100000: episode: 207, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.627, mean reward: -0.156 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.991, 10.397], loss: 0.003107, mae: 0.056273, mean_q: -0.338053
 20800/100000: episode: 208, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.803, mean reward: -0.148 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.580, 10.098], loss: 0.003132, mae: 0.056551, mean_q: -0.292432
 20900/100000: episode: 209, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -14.589, mean reward: -0.146 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.153, 10.277], loss: 0.003314, mae: 0.059265, mean_q: -0.310940
 21000/100000: episode: 210, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.796, mean reward: -0.178 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.735, 10.098], loss: 0.003047, mae: 0.055162, mean_q: -0.330123
 21100/100000: episode: 211, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -9.522, mean reward: -0.095 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.888, 10.453], loss: 0.003492, mae: 0.061210, mean_q: -0.322568
 21200/100000: episode: 212, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.014, mean reward: -0.160 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.538, 10.320], loss: 0.003273, mae: 0.059590, mean_q: -0.274887
 21300/100000: episode: 213, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -21.045, mean reward: -0.210 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.937, 10.207], loss: 0.003266, mae: 0.058816, mean_q: -0.286280
 21400/100000: episode: 214, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.058, mean reward: -0.181 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.770, 10.098], loss: 0.003064, mae: 0.055600, mean_q: -0.296433
 21500/100000: episode: 215, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.177, mean reward: -0.172 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.703, 10.098], loss: 0.002975, mae: 0.054176, mean_q: -0.308011
 21600/100000: episode: 216, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -6.892, mean reward: -0.069 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.616, 10.220], loss: 0.002955, mae: 0.053914, mean_q: -0.320910
 21700/100000: episode: 217, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.282, mean reward: -0.163 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.474, 10.098], loss: 0.003670, mae: 0.061442, mean_q: -0.305767
 21800/100000: episode: 218, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.205, mean reward: -0.172 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.087, 10.133], loss: 0.004119, mae: 0.066053, mean_q: -0.293487
 21900/100000: episode: 219, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.929, mean reward: -0.159 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.233, 10.098], loss: 0.003308, mae: 0.060858, mean_q: -0.276211
 22000/100000: episode: 220, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.102, mean reward: -0.141 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.047, 10.252], loss: 0.002951, mae: 0.055354, mean_q: -0.324270
 22100/100000: episode: 221, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -19.200, mean reward: -0.192 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.825, 10.191], loss: 0.002664, mae: 0.051839, mean_q: -0.314135
 22200/100000: episode: 222, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -15.785, mean reward: -0.158 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.579, 10.098], loss: 0.002840, mae: 0.053147, mean_q: -0.323736
 22300/100000: episode: 223, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.784, mean reward: -0.178 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.069, 10.098], loss: 0.002882, mae: 0.054010, mean_q: -0.300842
 22400/100000: episode: 224, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.193, mean reward: -0.172 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.561, 10.182], loss: 0.002835, mae: 0.053866, mean_q: -0.323397
 22500/100000: episode: 225, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.206, mean reward: -0.182 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.662, 10.098], loss: 0.002968, mae: 0.055678, mean_q: -0.312079
 22600/100000: episode: 226, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.029, mean reward: -0.180 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.312, 10.098], loss: 0.003848, mae: 0.061199, mean_q: -0.317558
 22700/100000: episode: 227, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.683, mean reward: -0.177 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.518, 10.098], loss: 0.003276, mae: 0.059044, mean_q: -0.349038
 22800/100000: episode: 228, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.037, mean reward: -0.170 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.410, 10.098], loss: 0.002852, mae: 0.053457, mean_q: -0.339705
 22900/100000: episode: 229, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.151, mean reward: -0.182 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.147, 10.158], loss: 0.005393, mae: 0.071249, mean_q: -0.299594
 23000/100000: episode: 230, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.174, mean reward: -0.142 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.567, 10.238], loss: 0.002990, mae: 0.057238, mean_q: -0.332098
 23100/100000: episode: 231, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -19.829, mean reward: -0.198 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.584, 10.184], loss: 0.004941, mae: 0.064393, mean_q: -0.314142
 23200/100000: episode: 232, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.185, mean reward: -0.182 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.101, 10.098], loss: 0.004074, mae: 0.064315, mean_q: -0.306177
 23300/100000: episode: 233, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -20.197, mean reward: -0.202 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.656, 10.098], loss: 0.002924, mae: 0.055496, mean_q: -0.337585
 23400/100000: episode: 234, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -13.015, mean reward: -0.130 [-1.000, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.457, 10.198], loss: 0.002757, mae: 0.053443, mean_q: -0.305723
 23500/100000: episode: 235, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.382, mean reward: -0.184 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.364, 10.111], loss: 0.003141, mae: 0.057011, mean_q: -0.308387
 23600/100000: episode: 236, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.694, mean reward: -0.177 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.839, 10.098], loss: 0.003162, mae: 0.057339, mean_q: -0.314432
 23700/100000: episode: 237, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -14.506, mean reward: -0.145 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.400, 10.098], loss: 0.003992, mae: 0.064417, mean_q: -0.322647
 23800/100000: episode: 238, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -14.243, mean reward: -0.142 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.523, 10.266], loss: 0.002945, mae: 0.057075, mean_q: -0.310780
 23900/100000: episode: 239, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -17.958, mean reward: -0.180 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.864, 10.098], loss: 0.003051, mae: 0.057247, mean_q: -0.341846
 24000/100000: episode: 240, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -17.226, mean reward: -0.172 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.380, 10.098], loss: 0.003149, mae: 0.056982, mean_q: -0.321700
 24100/100000: episode: 241, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -12.589, mean reward: -0.126 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.753, 10.098], loss: 0.003048, mae: 0.056752, mean_q: -0.297876
 24200/100000: episode: 242, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -19.679, mean reward: -0.197 [-1.000, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.316, 10.205], loss: 0.002975, mae: 0.055618, mean_q: -0.312119
 24300/100000: episode: 243, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.060, mean reward: -0.191 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.294, 10.098], loss: 0.003156, mae: 0.057780, mean_q: -0.336528
 24400/100000: episode: 244, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -19.563, mean reward: -0.196 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.807, 10.151], loss: 0.003617, mae: 0.062153, mean_q: -0.327432
 24500/100000: episode: 245, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.304, mean reward: -0.173 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.907, 10.274], loss: 0.003067, mae: 0.056418, mean_q: -0.305682
 24600/100000: episode: 246, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.991, mean reward: -0.180 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.655, 10.157], loss: 0.002951, mae: 0.056070, mean_q: -0.329648
 24700/100000: episode: 247, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.947, mean reward: -0.189 [-1.000, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.115, 10.098], loss: 0.003054, mae: 0.055809, mean_q: -0.330789
 24800/100000: episode: 248, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.190, mean reward: -0.172 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.746, 10.264], loss: 0.002846, mae: 0.054085, mean_q: -0.302937
 24900/100000: episode: 249, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.619, mean reward: -0.146 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.311, 10.098], loss: 0.003056, mae: 0.056710, mean_q: -0.308798
 25000/100000: episode: 250, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.498, mean reward: -0.195 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.372, 10.115], loss: 0.004491, mae: 0.064632, mean_q: -0.315566
 25100/100000: episode: 251, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.994, mean reward: -0.160 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.208, 10.243], loss: 0.003025, mae: 0.056744, mean_q: -0.282032
 25200/100000: episode: 252, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.452, mean reward: -0.165 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.544, 10.157], loss: 0.003094, mae: 0.057139, mean_q: -0.341392
 25300/100000: episode: 253, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -16.639, mean reward: -0.166 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.264, 10.141], loss: 0.003077, mae: 0.056642, mean_q: -0.298805
 25400/100000: episode: 254, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.549, mean reward: -0.175 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.752, 10.296], loss: 0.002995, mae: 0.056118, mean_q: -0.345560
 25500/100000: episode: 255, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -7.976, mean reward: -0.080 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.913, 10.540], loss: 0.003028, mae: 0.056439, mean_q: -0.307109
 25600/100000: episode: 256, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -14.465, mean reward: -0.145 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.115, 10.098], loss: 0.002868, mae: 0.054900, mean_q: -0.309124
 25700/100000: episode: 257, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.939, mean reward: -0.129 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.023, 10.098], loss: 0.003435, mae: 0.061117, mean_q: -0.289695
 25800/100000: episode: 258, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -18.710, mean reward: -0.187 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.909, 10.153], loss: 0.005281, mae: 0.069207, mean_q: -0.307654
 25900/100000: episode: 259, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.958, mean reward: -0.190 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.899, 10.271], loss: 0.003008, mae: 0.057127, mean_q: -0.338051
 26000/100000: episode: 260, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -16.830, mean reward: -0.168 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.524, 10.098], loss: 0.003993, mae: 0.064301, mean_q: -0.293857
 26100/100000: episode: 261, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -16.671, mean reward: -0.167 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.874, 10.098], loss: 0.003510, mae: 0.059198, mean_q: -0.311131
 26200/100000: episode: 262, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -19.423, mean reward: -0.194 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.398, 10.184], loss: 0.002902, mae: 0.054747, mean_q: -0.365491
 26300/100000: episode: 263, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -15.444, mean reward: -0.154 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.051, 10.195], loss: 0.003839, mae: 0.064564, mean_q: -0.255220
 26400/100000: episode: 264, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.052, mean reward: -0.171 [-1.000, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.264, 10.098], loss: 0.003419, mae: 0.061554, mean_q: -0.363200
 26500/100000: episode: 265, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.646, mean reward: -0.186 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.574, 10.111], loss: 0.003066, mae: 0.056334, mean_q: -0.308002
 26600/100000: episode: 266, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.670, mean reward: -0.147 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.792, 10.324], loss: 0.002865, mae: 0.054331, mean_q: -0.295548
 26700/100000: episode: 267, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -15.550, mean reward: -0.156 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.313, 10.135], loss: 0.002777, mae: 0.054166, mean_q: -0.320246
 26800/100000: episode: 268, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -14.364, mean reward: -0.144 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.811, 10.316], loss: 0.002772, mae: 0.054070, mean_q: -0.315400
 26900/100000: episode: 269, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -19.463, mean reward: -0.195 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.885, 10.207], loss: 0.002789, mae: 0.053242, mean_q: -0.318394
 27000/100000: episode: 270, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.164, mean reward: -0.182 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.534, 10.114], loss: 0.003039, mae: 0.056094, mean_q: -0.311492
 27100/100000: episode: 271, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.717, mean reward: -0.187 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.852, 10.138], loss: 0.002844, mae: 0.054050, mean_q: -0.300382
 27200/100000: episode: 272, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.967, mean reward: -0.170 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.661, 10.201], loss: 0.002985, mae: 0.055254, mean_q: -0.325194
 27300/100000: episode: 273, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.137, mean reward: -0.171 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.466, 10.098], loss: 0.002829, mae: 0.054815, mean_q: -0.301411
 27400/100000: episode: 274, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -13.757, mean reward: -0.138 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.992, 10.135], loss: 0.003012, mae: 0.055425, mean_q: -0.342895
 27500/100000: episode: 275, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -16.016, mean reward: -0.160 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.765, 10.354], loss: 0.002715, mae: 0.053060, mean_q: -0.292967
 27600/100000: episode: 276, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.578, mean reward: -0.166 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.837, 10.129], loss: 0.003447, mae: 0.058270, mean_q: -0.369468
 27700/100000: episode: 277, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.525, mean reward: -0.185 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.937, 10.098], loss: 0.002921, mae: 0.054623, mean_q: -0.326772
 27800/100000: episode: 278, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -10.401, mean reward: -0.104 [-1.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.795, 10.374], loss: 0.003078, mae: 0.057271, mean_q: -0.316360
 27900/100000: episode: 279, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -17.432, mean reward: -0.174 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.830, 10.098], loss: 0.002980, mae: 0.055176, mean_q: -0.280746
 28000/100000: episode: 280, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -18.573, mean reward: -0.186 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.208, 10.098], loss: 0.003314, mae: 0.058602, mean_q: -0.324219
 28100/100000: episode: 281, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -19.876, mean reward: -0.199 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.108, 10.098], loss: 0.002834, mae: 0.054613, mean_q: -0.312417
 28200/100000: episode: 282, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -13.152, mean reward: -0.132 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.251, 10.113], loss: 0.002864, mae: 0.053801, mean_q: -0.311711
 28300/100000: episode: 283, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -20.237, mean reward: -0.202 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.648, 10.098], loss: 0.002880, mae: 0.053892, mean_q: -0.320922
 28400/100000: episode: 284, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.490, mean reward: -0.195 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.759, 10.220], loss: 0.002916, mae: 0.055051, mean_q: -0.302063
 28500/100000: episode: 285, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -6.325, mean reward: -0.063 [-1.000, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.058, 10.098], loss: 0.002725, mae: 0.053097, mean_q: -0.291933
 28600/100000: episode: 286, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.153, mean reward: -0.162 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.189, 10.262], loss: 0.002952, mae: 0.055388, mean_q: -0.319337
 28700/100000: episode: 287, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.551, mean reward: -0.186 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.707, 10.160], loss: 0.002889, mae: 0.053377, mean_q: -0.320678
 28800/100000: episode: 288, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.168, mean reward: -0.182 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.988, 10.277], loss: 0.003051, mae: 0.055767, mean_q: -0.296959
 28900/100000: episode: 289, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -16.914, mean reward: -0.169 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.749, 10.103], loss: 0.003969, mae: 0.064466, mean_q: -0.307727
 29000/100000: episode: 290, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.844, mean reward: -0.188 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.914, 10.098], loss: 0.003134, mae: 0.058041, mean_q: -0.314188
 29100/100000: episode: 291, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.486, mean reward: -0.195 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.618, 10.098], loss: 0.002971, mae: 0.055571, mean_q: -0.308736
 29200/100000: episode: 292, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -15.194, mean reward: -0.152 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.009, 10.207], loss: 0.002983, mae: 0.055265, mean_q: -0.309611
 29300/100000: episode: 293, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -14.426, mean reward: -0.144 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.654, 10.309], loss: 0.002980, mae: 0.054293, mean_q: -0.286285
 29400/100000: episode: 294, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.249, mean reward: -0.182 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.658, 10.299], loss: 0.002863, mae: 0.054298, mean_q: -0.294691
 29500/100000: episode: 295, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.610, mean reward: -0.166 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.982, 10.168], loss: 0.003971, mae: 0.059346, mean_q: -0.342062
 29600/100000: episode: 296, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -15.377, mean reward: -0.154 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.636, 10.098], loss: 0.004191, mae: 0.061936, mean_q: -0.317518
 29700/100000: episode: 297, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.689, mean reward: -0.167 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.401, 10.098], loss: 0.002910, mae: 0.054842, mean_q: -0.342791
 29800/100000: episode: 298, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.922, mean reward: -0.159 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.150, 10.098], loss: 0.002516, mae: 0.050256, mean_q: -0.319557
 29900/100000: episode: 299, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -18.705, mean reward: -0.187 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.615, 10.244], loss: 0.002446, mae: 0.048837, mean_q: -0.338725
 30000/100000: episode: 300, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.800, mean reward: -0.168 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.879, 10.393], loss: 0.002697, mae: 0.051469, mean_q: -0.345801
 30100/100000: episode: 301, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.500, mean reward: -0.145 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.481, 10.106], loss: 0.002781, mae: 0.053081, mean_q: -0.309561
 30200/100000: episode: 302, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: -17.892, mean reward: -0.179 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.842, 10.236], loss: 0.002612, mae: 0.050913, mean_q: -0.336881
 30300/100000: episode: 303, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.540, mean reward: -0.145 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.322, 10.356], loss: 0.002749, mae: 0.051998, mean_q: -0.323729
 30400/100000: episode: 304, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -7.499, mean reward: -0.075 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.489, 10.423], loss: 0.002664, mae: 0.051436, mean_q: -0.315887
 30500/100000: episode: 305, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.506, mean reward: -0.185 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.024, 10.114], loss: 0.003025, mae: 0.054642, mean_q: -0.303925
 30600/100000: episode: 306, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -10.274, mean reward: -0.103 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.797, 10.447], loss: 0.003053, mae: 0.055906, mean_q: -0.319247
 30700/100000: episode: 307, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.145, mean reward: -0.191 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.781, 10.098], loss: 0.002896, mae: 0.053116, mean_q: -0.313029
 30800/100000: episode: 308, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.093, mean reward: -0.151 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.713, 10.098], loss: 0.002824, mae: 0.052409, mean_q: -0.307987
 30900/100000: episode: 309, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.429, mean reward: -0.174 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.845, 10.137], loss: 0.002866, mae: 0.053520, mean_q: -0.329114
 31000/100000: episode: 310, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -15.539, mean reward: -0.155 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.410, 10.098], loss: 0.003188, mae: 0.059225, mean_q: -0.287054
 31100/100000: episode: 311, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -15.905, mean reward: -0.159 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.754, 10.172], loss: 0.002626, mae: 0.052244, mean_q: -0.328648
 31200/100000: episode: 312, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.476, mean reward: -0.185 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.498, 10.119], loss: 0.006527, mae: 0.073162, mean_q: -0.306365
 31300/100000: episode: 313, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.102, mean reward: -0.161 [-1.000, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.569, 10.098], loss: 0.004000, mae: 0.061809, mean_q: -0.332255
 31400/100000: episode: 314, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -15.649, mean reward: -0.156 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.902, 10.407], loss: 0.002942, mae: 0.054622, mean_q: -0.291113
 31500/100000: episode: 315, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -19.622, mean reward: -0.196 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.440, 10.133], loss: 0.002986, mae: 0.054992, mean_q: -0.289312
 31600/100000: episode: 316, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.958, mean reward: -0.180 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.605, 10.131], loss: 0.002939, mae: 0.053290, mean_q: -0.328943
 31700/100000: episode: 317, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -19.710, mean reward: -0.197 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.535, 10.098], loss: 0.002923, mae: 0.053364, mean_q: -0.313480
 31800/100000: episode: 318, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: -14.741, mean reward: -0.147 [-1.000, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.997, 10.376], loss: 0.002781, mae: 0.051736, mean_q: -0.329922
 31900/100000: episode: 319, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.847, mean reward: -0.178 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.869, 10.152], loss: 0.002627, mae: 0.050994, mean_q: -0.318739
 32000/100000: episode: 320, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.028, mean reward: -0.160 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.638, 10.176], loss: 0.002749, mae: 0.052058, mean_q: -0.306295
 32100/100000: episode: 321, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.151, mean reward: -0.182 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.535, 10.098], loss: 0.002682, mae: 0.051380, mean_q: -0.332993
 32200/100000: episode: 322, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.940, mean reward: -0.189 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.587, 10.098], loss: 0.002788, mae: 0.053056, mean_q: -0.320695
 32300/100000: episode: 323, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -9.682, mean reward: -0.097 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.697, 10.098], loss: 0.002688, mae: 0.051110, mean_q: -0.332770
 32400/100000: episode: 324, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.181, mean reward: -0.182 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.851, 10.098], loss: 0.002668, mae: 0.051413, mean_q: -0.331914
 32500/100000: episode: 325, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -14.176, mean reward: -0.142 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.037, 10.107], loss: 0.003057, mae: 0.055803, mean_q: -0.296023
 32600/100000: episode: 326, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -14.417, mean reward: -0.144 [-1.000, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.717, 10.098], loss: 0.002789, mae: 0.053620, mean_q: -0.315084
 32700/100000: episode: 327, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.487, mean reward: -0.195 [-1.000, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.483, 10.132], loss: 0.003073, mae: 0.056080, mean_q: -0.300090
 32800/100000: episode: 328, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.166, mean reward: -0.162 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.851, 10.098], loss: 0.003092, mae: 0.057927, mean_q: -0.318138
 32900/100000: episode: 329, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.476, mean reward: -0.165 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.740, 10.098], loss: 0.002845, mae: 0.054385, mean_q: -0.324984
 33000/100000: episode: 330, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.963, mean reward: -0.170 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.886, 10.140], loss: 0.003329, mae: 0.059491, mean_q: -0.287914
 33100/100000: episode: 331, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -12.820, mean reward: -0.128 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.803, 10.098], loss: 0.002954, mae: 0.055026, mean_q: -0.295216
 33200/100000: episode: 332, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -11.286, mean reward: -0.113 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.621, 10.336], loss: 0.002758, mae: 0.053404, mean_q: -0.299937
 33300/100000: episode: 333, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.980, mean reward: -0.170 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.890, 10.098], loss: 0.002803, mae: 0.053365, mean_q: -0.320827
 33400/100000: episode: 334, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.192, mean reward: -0.182 [-1.000, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.313, 10.098], loss: 0.004367, mae: 0.062273, mean_q: -0.309803
 33500/100000: episode: 335, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: -17.850, mean reward: -0.178 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.523, 10.190], loss: 0.003822, mae: 0.060645, mean_q: -0.299881
 33600/100000: episode: 336, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -19.031, mean reward: -0.190 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.673, 10.098], loss: 0.002705, mae: 0.052681, mean_q: -0.301487
 33700/100000: episode: 337, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -12.598, mean reward: -0.126 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.227, 10.518], loss: 0.002937, mae: 0.055804, mean_q: -0.301625
 33800/100000: episode: 338, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -15.437, mean reward: -0.154 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.151, 10.267], loss: 0.002830, mae: 0.053411, mean_q: -0.314193
 33900/100000: episode: 339, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.403, mean reward: -0.184 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.132, 10.098], loss: 0.002452, mae: 0.049752, mean_q: -0.305934
 34000/100000: episode: 340, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -5.554, mean reward: -0.056 [-1.000, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.682, 10.098], loss: 0.002485, mae: 0.049627, mean_q: -0.323038
 34100/100000: episode: 341, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -14.865, mean reward: -0.149 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.405, 10.098], loss: 0.002546, mae: 0.050229, mean_q: -0.310991
 34200/100000: episode: 342, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -16.451, mean reward: -0.165 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.800, 10.201], loss: 0.002757, mae: 0.051924, mean_q: -0.307766
 34300/100000: episode: 343, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.632, mean reward: -0.186 [-1.000, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.274, 10.098], loss: 0.002851, mae: 0.054113, mean_q: -0.288691
 34400/100000: episode: 344, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.007, mean reward: -0.150 [-1.000, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.057, 10.145], loss: 0.002625, mae: 0.051638, mean_q: -0.334658
 34500/100000: episode: 345, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.902, mean reward: -0.169 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.204, 10.098], loss: 0.002981, mae: 0.054550, mean_q: -0.316755
 34600/100000: episode: 346, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.140, mean reward: -0.161 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.860, 10.098], loss: 0.003813, mae: 0.060369, mean_q: -0.300361
 34700/100000: episode: 347, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -15.240, mean reward: -0.152 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.586, 10.194], loss: 0.003821, mae: 0.062119, mean_q: -0.313153
 34800/100000: episode: 348, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.531, mean reward: -0.165 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.649, 10.385], loss: 0.002992, mae: 0.055700, mean_q: -0.250436
 34900/100000: episode: 349, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.470, mean reward: -0.185 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.388, 10.098], loss: 0.002685, mae: 0.051762, mean_q: -0.283375
 35000/100000: episode: 350, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.020, mean reward: -0.180 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.620, 10.098], loss: 0.002660, mae: 0.051602, mean_q: -0.301870
 35100/100000: episode: 351, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -16.359, mean reward: -0.164 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.835, 10.280], loss: 0.002706, mae: 0.051814, mean_q: -0.326871
 35200/100000: episode: 352, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.420, mean reward: -0.194 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.901, 10.199], loss: 0.002704, mae: 0.052179, mean_q: -0.289384
 35300/100000: episode: 353, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -19.193, mean reward: -0.192 [-1.000, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.775, 10.098], loss: 0.002687, mae: 0.053259, mean_q: -0.307116
 35400/100000: episode: 354, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -13.205, mean reward: -0.132 [-1.000, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.650, 10.098], loss: 0.006796, mae: 0.066793, mean_q: -0.322760
 35500/100000: episode: 355, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.092, mean reward: -0.191 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.589, 10.159], loss: 0.003193, mae: 0.058132, mean_q: -0.326566
 35600/100000: episode: 356, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.436, mean reward: -0.174 [-1.000, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.717, 10.121], loss: 0.002619, mae: 0.052278, mean_q: -0.338933
 35700/100000: episode: 357, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.218, mean reward: -0.172 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.401, 10.205], loss: 0.002637, mae: 0.051708, mean_q: -0.287430
 35800/100000: episode: 358, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.696, mean reward: -0.167 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.874, 10.098], loss: 0.002745, mae: 0.051647, mean_q: -0.323214
 35900/100000: episode: 359, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -11.269, mean reward: -0.113 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.561, 10.098], loss: 0.002693, mae: 0.051301, mean_q: -0.350331
 36000/100000: episode: 360, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -21.045, mean reward: -0.210 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.921, 10.098], loss: 0.003040, mae: 0.055262, mean_q: -0.318370
 36100/100000: episode: 361, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -18.628, mean reward: -0.186 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.128, 10.098], loss: 0.002897, mae: 0.054778, mean_q: -0.278988
 36200/100000: episode: 362, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.649, mean reward: -0.176 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.166, 10.098], loss: 0.002571, mae: 0.050489, mean_q: -0.326157
 36300/100000: episode: 363, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.336, mean reward: -0.183 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.201, 10.207], loss: 0.002718, mae: 0.052170, mean_q: -0.322953
 36400/100000: episode: 364, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.645, mean reward: -0.186 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.544, 10.201], loss: 0.002883, mae: 0.052839, mean_q: -0.329358
 36500/100000: episode: 365, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.950, mean reward: -0.170 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.186, 10.164], loss: 0.002763, mae: 0.052684, mean_q: -0.313035
 36600/100000: episode: 366, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.882, mean reward: -0.159 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.160, 10.243], loss: 0.002891, mae: 0.053360, mean_q: -0.317674
 36700/100000: episode: 367, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -17.117, mean reward: -0.171 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.709, 10.098], loss: 0.002828, mae: 0.053005, mean_q: -0.317641
 36800/100000: episode: 368, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -13.732, mean reward: -0.137 [-1.000, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.297, 10.224], loss: 0.002584, mae: 0.050668, mean_q: -0.333449
 36900/100000: episode: 369, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.736, mean reward: -0.187 [-1.000, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.949, 10.183], loss: 0.002732, mae: 0.052514, mean_q: -0.313956
 37000/100000: episode: 370, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -12.732, mean reward: -0.127 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.281, 10.268], loss: 0.002868, mae: 0.052684, mean_q: -0.304092
 37100/100000: episode: 371, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.228, mean reward: -0.182 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.359, 10.136], loss: 0.002349, mae: 0.049014, mean_q: -0.325074
 37200/100000: episode: 372, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.503, mean reward: -0.175 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.830, 10.120], loss: 0.003007, mae: 0.055304, mean_q: -0.282878
 37300/100000: episode: 373, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -12.491, mean reward: -0.125 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.526, 10.098], loss: 0.002675, mae: 0.051986, mean_q: -0.336203
 37400/100000: episode: 374, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.690, mean reward: -0.187 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.561, 10.179], loss: 0.003610, mae: 0.056972, mean_q: -0.320708
 37500/100000: episode: 375, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.418, mean reward: -0.154 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.624, 10.243], loss: 0.003105, mae: 0.054877, mean_q: -0.334775
 37600/100000: episode: 376, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.311, mean reward: -0.193 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.446, 10.098], loss: 0.002606, mae: 0.051100, mean_q: -0.298161
 37700/100000: episode: 377, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -11.960, mean reward: -0.120 [-1.000, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.296, 10.153], loss: 0.002990, mae: 0.054893, mean_q: -0.328485
 37800/100000: episode: 378, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -20.328, mean reward: -0.203 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.868, 10.144], loss: 0.002860, mae: 0.052806, mean_q: -0.298641
 37900/100000: episode: 379, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -19.981, mean reward: -0.200 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.992, 10.100], loss: 0.002922, mae: 0.054138, mean_q: -0.336789
 38000/100000: episode: 380, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.609, mean reward: -0.196 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.410, 10.098], loss: 0.002673, mae: 0.051983, mean_q: -0.293581
 38100/100000: episode: 381, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: -16.958, mean reward: -0.170 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.987, 10.098], loss: 0.002710, mae: 0.051999, mean_q: -0.300233
 38200/100000: episode: 382, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -14.202, mean reward: -0.142 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.844, 10.187], loss: 0.002617, mae: 0.051073, mean_q: -0.327527
 38300/100000: episode: 383, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: -18.171, mean reward: -0.182 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.965, 10.137], loss: 0.002454, mae: 0.049457, mean_q: -0.311389
 38400/100000: episode: 384, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -14.776, mean reward: -0.148 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.211, 10.144], loss: 0.002518, mae: 0.050285, mean_q: -0.322497
 38500/100000: episode: 385, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -17.478, mean reward: -0.175 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.891, 10.179], loss: 0.002527, mae: 0.050051, mean_q: -0.338847
 38600/100000: episode: 386, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -18.293, mean reward: -0.183 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.824, 10.105], loss: 0.002788, mae: 0.053775, mean_q: -0.306440
 38700/100000: episode: 387, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.189, mean reward: -0.182 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.630, 10.098], loss: 0.002872, mae: 0.053347, mean_q: -0.322061
 38800/100000: episode: 388, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.068, mean reward: -0.181 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.962, 10.157], loss: 0.002966, mae: 0.054782, mean_q: -0.343356
 38900/100000: episode: 389, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: -16.658, mean reward: -0.167 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.410, 10.098], loss: 0.007289, mae: 0.079139, mean_q: -0.306013
 39000/100000: episode: 390, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.594, mean reward: -0.166 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.594, 10.285], loss: 0.003009, mae: 0.055510, mean_q: -0.369440
 39100/100000: episode: 391, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.679, mean reward: -0.187 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.229, 10.154], loss: 0.002717, mae: 0.051574, mean_q: -0.353365
 39200/100000: episode: 392, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -8.687, mean reward: -0.087 [-1.000, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.936, 10.270], loss: 0.002713, mae: 0.051492, mean_q: -0.336059
 39300/100000: episode: 393, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.250, mean reward: -0.163 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.251, 10.196], loss: 0.002864, mae: 0.052854, mean_q: -0.330763
 39400/100000: episode: 394, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -16.397, mean reward: -0.164 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.323, 10.098], loss: 0.002897, mae: 0.053506, mean_q: -0.295182
 39500/100000: episode: 395, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.789, mean reward: -0.168 [-1.000, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.180, 10.098], loss: 0.002823, mae: 0.053856, mean_q: -0.305530
 39600/100000: episode: 396, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -19.257, mean reward: -0.193 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.675, 10.152], loss: 0.002804, mae: 0.052312, mean_q: -0.326802
 39700/100000: episode: 397, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: -17.019, mean reward: -0.170 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.068, 10.098], loss: 0.002770, mae: 0.053023, mean_q: -0.318840
 39800/100000: episode: 398, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -20.574, mean reward: -0.206 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.027, 10.168], loss: 0.002677, mae: 0.051664, mean_q: -0.340177
 39900/100000: episode: 399, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.230, mean reward: -0.172 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.581, 10.110], loss: 0.002860, mae: 0.052542, mean_q: -0.310733
 40000/100000: episode: 400, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.964, mean reward: -0.170 [-1.000, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.913, 10.098], loss: 0.002688, mae: 0.051875, mean_q: -0.288587
 40100/100000: episode: 401, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.345, mean reward: -0.173 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.780, 10.275], loss: 0.002918, mae: 0.053526, mean_q: -0.328321
 40200/100000: episode: 402, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -12.726, mean reward: -0.127 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.678, 10.098], loss: 0.002804, mae: 0.051957, mean_q: -0.353100
 40300/100000: episode: 403, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.964, mean reward: -0.170 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.912, 10.098], loss: 0.002703, mae: 0.050758, mean_q: -0.333288
 40400/100000: episode: 404, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.480, mean reward: -0.155 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.953, 10.177], loss: 0.003939, mae: 0.060191, mean_q: -0.323635
 40500/100000: episode: 405, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -14.083, mean reward: -0.141 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.722, 10.271], loss: 0.002732, mae: 0.052311, mean_q: -0.286423
 40600/100000: episode: 406, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -16.931, mean reward: -0.169 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.239, 10.098], loss: 0.002791, mae: 0.053102, mean_q: -0.287422
 40700/100000: episode: 407, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -12.658, mean reward: -0.127 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.487, 10.098], loss: 0.002596, mae: 0.050439, mean_q: -0.327591
 40800/100000: episode: 408, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.080, mean reward: -0.161 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.708, 10.132], loss: 0.002849, mae: 0.053083, mean_q: -0.304525
 40900/100000: episode: 409, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -13.396, mean reward: -0.134 [-1.000, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.281, 10.319], loss: 0.003122, mae: 0.054495, mean_q: -0.325964
 41000/100000: episode: 410, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -19.241, mean reward: -0.192 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.128, 10.245], loss: 0.002786, mae: 0.050917, mean_q: -0.342498
 41100/100000: episode: 411, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -6.264, mean reward: -0.063 [-1.000, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.974, 10.098], loss: 0.002608, mae: 0.050143, mean_q: -0.320235
 41200/100000: episode: 412, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -18.703, mean reward: -0.187 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.542, 10.098], loss: 0.004184, mae: 0.064559, mean_q: -0.304237
 41300/100000: episode: 413, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -16.977, mean reward: -0.170 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.208, 10.162], loss: 0.003118, mae: 0.055421, mean_q: -0.312600
 41400/100000: episode: 414, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -15.369, mean reward: -0.154 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.002, 10.135], loss: 0.002731, mae: 0.051434, mean_q: -0.311967
 41500/100000: episode: 415, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: -15.950, mean reward: -0.160 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.256, 10.098], loss: 0.002987, mae: 0.054801, mean_q: -0.287767
 41600/100000: episode: 416, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -13.547, mean reward: -0.135 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.503, 10.098], loss: 0.003086, mae: 0.054476, mean_q: -0.308560
 41700/100000: episode: 417, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -15.852, mean reward: -0.159 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.653, 10.098], loss: 0.004659, mae: 0.066448, mean_q: -0.277806
 41800/100000: episode: 418, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.825, mean reward: -0.178 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.333, 10.098], loss: 0.002921, mae: 0.055517, mean_q: -0.338552
 41900/100000: episode: 419, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.845, mean reward: -0.168 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.399, 10.098], loss: 0.002666, mae: 0.052145, mean_q: -0.319023
 42000/100000: episode: 420, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.727, mean reward: -0.187 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.634, 10.098], loss: 0.002748, mae: 0.052198, mean_q: -0.315775
 42100/100000: episode: 421, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -17.893, mean reward: -0.179 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.621, 10.119], loss: 0.002720, mae: 0.052062, mean_q: -0.319189
 42200/100000: episode: 422, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -14.423, mean reward: -0.144 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.201, 10.380], loss: 0.002639, mae: 0.051216, mean_q: -0.325758
 42300/100000: episode: 423, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -17.143, mean reward: -0.171 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.858, 10.098], loss: 0.002731, mae: 0.052575, mean_q: -0.320730
 42400/100000: episode: 424, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.231, mean reward: -0.182 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.235, 10.098], loss: 0.003775, mae: 0.059440, mean_q: -0.313596
 42500/100000: episode: 425, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -14.553, mean reward: -0.146 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.273, 10.098], loss: 0.003124, mae: 0.057693, mean_q: -0.321546
 42600/100000: episode: 426, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -21.011, mean reward: -0.210 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.834, 10.098], loss: 0.002799, mae: 0.052146, mean_q: -0.289302
 42700/100000: episode: 427, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.120, mean reward: -0.181 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.930, 10.098], loss: 0.002703, mae: 0.051042, mean_q: -0.298826
 42800/100000: episode: 428, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -13.710, mean reward: -0.137 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.745, 10.098], loss: 0.002849, mae: 0.053085, mean_q: -0.307409
 42900/100000: episode: 429, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -18.469, mean reward: -0.185 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.757, 10.098], loss: 0.002757, mae: 0.051402, mean_q: -0.306182
 43000/100000: episode: 430, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.429, mean reward: -0.184 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.906, 10.098], loss: 0.002793, mae: 0.052108, mean_q: -0.305611
 43100/100000: episode: 431, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.566, mean reward: -0.156 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.546, 10.098], loss: 0.002720, mae: 0.051841, mean_q: -0.319100
 43200/100000: episode: 432, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.075, mean reward: -0.181 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.638, 10.337], loss: 0.002738, mae: 0.051271, mean_q: -0.319505
 43300/100000: episode: 433, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.083, mean reward: -0.181 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.235, 10.098], loss: 0.002768, mae: 0.051640, mean_q: -0.304236
 43400/100000: episode: 434, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -20.391, mean reward: -0.204 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.549, 10.098], loss: 0.002738, mae: 0.051503, mean_q: -0.303278
 43500/100000: episode: 435, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.297, mean reward: -0.183 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.332, 10.318], loss: 0.002980, mae: 0.052858, mean_q: -0.300039
 43600/100000: episode: 436, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.623, mean reward: -0.186 [-1.000, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.469, 10.178], loss: 0.003740, mae: 0.060082, mean_q: -0.280732
 43700/100000: episode: 437, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -19.414, mean reward: -0.194 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.381, 10.204], loss: 0.002920, mae: 0.054829, mean_q: -0.301530
 43800/100000: episode: 438, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -16.543, mean reward: -0.165 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.752, 10.098], loss: 0.002665, mae: 0.050415, mean_q: -0.301047
 43900/100000: episode: 439, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.230, mean reward: -0.172 [-1.000, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.505, 10.177], loss: 0.002903, mae: 0.053070, mean_q: -0.303080
 44000/100000: episode: 440, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.800, mean reward: -0.198 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.099, 10.098], loss: 0.002820, mae: 0.052563, mean_q: -0.321107
 44100/100000: episode: 441, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -15.524, mean reward: -0.155 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.477, 10.339], loss: 0.005093, mae: 0.067915, mean_q: -0.330169
 44200/100000: episode: 442, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.257, mean reward: -0.183 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.253, 10.098], loss: 0.002730, mae: 0.052549, mean_q: -0.317551
 44300/100000: episode: 443, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.937, mean reward: -0.179 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.825, 10.098], loss: 0.002573, mae: 0.049449, mean_q: -0.323839
 44400/100000: episode: 444, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -15.939, mean reward: -0.159 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.391, 10.287], loss: 0.002726, mae: 0.050826, mean_q: -0.317351
 44500/100000: episode: 445, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -13.216, mean reward: -0.132 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.126, 10.098], loss: 0.002699, mae: 0.050277, mean_q: -0.311218
 44600/100000: episode: 446, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.077, mean reward: -0.151 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.600, 10.113], loss: 0.002641, mae: 0.050905, mean_q: -0.299658
 44700/100000: episode: 447, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -20.583, mean reward: -0.206 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.642, 10.255], loss: 0.002720, mae: 0.051488, mean_q: -0.347788
 44800/100000: episode: 448, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -16.375, mean reward: -0.164 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.963, 10.098], loss: 0.002577, mae: 0.049917, mean_q: -0.329721
 44900/100000: episode: 449, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.269, mean reward: -0.183 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.354, 10.098], loss: 0.002778, mae: 0.051122, mean_q: -0.289823
 45000/100000: episode: 450, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -19.433, mean reward: -0.194 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.581, 10.110], loss: 0.002760, mae: 0.051123, mean_q: -0.341098
 45100/100000: episode: 451, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.547, mean reward: -0.155 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.339, 10.098], loss: 0.002702, mae: 0.051783, mean_q: -0.318589
 45200/100000: episode: 452, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.139, mean reward: -0.141 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.584, 10.098], loss: 0.002858, mae: 0.052349, mean_q: -0.307799
 45300/100000: episode: 453, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.522, mean reward: -0.175 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.503, 10.098], loss: 0.002776, mae: 0.051950, mean_q: -0.331596
 45400/100000: episode: 454, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -15.395, mean reward: -0.154 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.452, 10.098], loss: 0.002682, mae: 0.050935, mean_q: -0.325882
 45500/100000: episode: 455, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.780, mean reward: -0.148 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.484, 10.098], loss: 0.002642, mae: 0.050249, mean_q: -0.305849
 45600/100000: episode: 456, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -15.656, mean reward: -0.157 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.833, 10.503], loss: 0.002868, mae: 0.052786, mean_q: -0.309127
 45700/100000: episode: 457, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.252, mean reward: -0.173 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.476, 10.213], loss: 0.002826, mae: 0.051924, mean_q: -0.297883
 45800/100000: episode: 458, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -17.110, mean reward: -0.171 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.996, 10.098], loss: 0.002627, mae: 0.050236, mean_q: -0.297495
 45900/100000: episode: 459, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.945, mean reward: -0.169 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.398, 10.098], loss: 0.002615, mae: 0.050546, mean_q: -0.308058
 46000/100000: episode: 460, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -18.632, mean reward: -0.186 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.378, 10.098], loss: 0.002743, mae: 0.053157, mean_q: -0.330922
 46100/100000: episode: 461, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.084, mean reward: -0.191 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.083, 10.098], loss: 0.003039, mae: 0.054430, mean_q: -0.333476
 46200/100000: episode: 462, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -17.732, mean reward: -0.177 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.782, 10.243], loss: 0.002477, mae: 0.048569, mean_q: -0.287327
 46300/100000: episode: 463, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -17.059, mean reward: -0.171 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.962, 10.098], loss: 0.005064, mae: 0.059996, mean_q: -0.318800
 46400/100000: episode: 464, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -18.167, mean reward: -0.182 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.722, 10.445], loss: 0.006608, mae: 0.074807, mean_q: -0.311816
 46500/100000: episode: 465, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: -15.940, mean reward: -0.159 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.348, 10.262], loss: 0.002606, mae: 0.050714, mean_q: -0.285151
 46600/100000: episode: 466, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -13.582, mean reward: -0.136 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.062, 10.318], loss: 0.002512, mae: 0.048898, mean_q: -0.359533
 46700/100000: episode: 467, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.076, mean reward: -0.181 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.869, 10.186], loss: 0.002543, mae: 0.048933, mean_q: -0.303986
 46800/100000: episode: 468, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.081, mean reward: -0.181 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.014, 10.287], loss: 0.002516, mae: 0.049628, mean_q: -0.305757
 46900/100000: episode: 469, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.447, mean reward: -0.184 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.583, 10.228], loss: 0.002489, mae: 0.048748, mean_q: -0.324397
 47000/100000: episode: 470, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.121, mean reward: -0.171 [-1.000, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.019, 10.222], loss: 0.002350, mae: 0.047078, mean_q: -0.313245
 47100/100000: episode: 471, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: -19.564, mean reward: -0.196 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.847, 10.286], loss: 0.002592, mae: 0.049761, mean_q: -0.293076
 47200/100000: episode: 472, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.215, mean reward: -0.182 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.941, 10.167], loss: 0.002651, mae: 0.049851, mean_q: -0.347381
 47300/100000: episode: 473, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -15.009, mean reward: -0.150 [-1.000, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.706, 10.098], loss: 0.002637, mae: 0.049596, mean_q: -0.324120
 47400/100000: episode: 474, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.058, mean reward: -0.171 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.371, 10.158], loss: 0.002510, mae: 0.049375, mean_q: -0.297210
 47500/100000: episode: 475, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -14.785, mean reward: -0.148 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.906, 10.313], loss: 0.002650, mae: 0.049609, mean_q: -0.306385
 47600/100000: episode: 476, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.198, mean reward: -0.192 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.052, 10.098], loss: 0.002744, mae: 0.050333, mean_q: -0.319040
 47700/100000: episode: 477, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.915, mean reward: -0.159 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.447, 10.098], loss: 0.002617, mae: 0.049398, mean_q: -0.348954
 47800/100000: episode: 478, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -14.150, mean reward: -0.141 [-1.000, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.315, 10.113], loss: 0.002563, mae: 0.049982, mean_q: -0.290962
 47900/100000: episode: 479, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.251, mean reward: -0.183 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.872, 10.234], loss: 0.002783, mae: 0.051610, mean_q: -0.303468
 48000/100000: episode: 480, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.275, mean reward: -0.173 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.699, 10.180], loss: 0.002616, mae: 0.050500, mean_q: -0.305712
 48100/100000: episode: 481, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.835, mean reward: -0.168 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.701, 10.217], loss: 0.002557, mae: 0.048834, mean_q: -0.356173
 48200/100000: episode: 482, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.231, mean reward: -0.162 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.550, 10.264], loss: 0.002554, mae: 0.049149, mean_q: -0.344747
 48300/100000: episode: 483, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.076, mean reward: -0.171 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.766, 10.244], loss: 0.002733, mae: 0.050927, mean_q: -0.378555
 48400/100000: episode: 484, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.594, mean reward: -0.146 [-1.000, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.258, 10.098], loss: 0.002810, mae: 0.052529, mean_q: -0.300439
 48500/100000: episode: 485, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.944, mean reward: -0.149 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.531, 10.098], loss: 0.002628, mae: 0.051109, mean_q: -0.305241
 48600/100000: episode: 486, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.722, mean reward: -0.157 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.568, 10.419], loss: 0.002838, mae: 0.052755, mean_q: -0.327327
 48700/100000: episode: 487, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.824, mean reward: -0.198 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.563, 10.114], loss: 0.004355, mae: 0.061542, mean_q: -0.328046
 48800/100000: episode: 488, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -12.341, mean reward: -0.123 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.919, 10.098], loss: 0.003175, mae: 0.056756, mean_q: -0.308639
 48900/100000: episode: 489, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.270, mean reward: -0.143 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.112, 10.098], loss: 0.002699, mae: 0.050696, mean_q: -0.338000
 49000/100000: episode: 490, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.697, mean reward: -0.197 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.434, 10.098], loss: 0.002555, mae: 0.049403, mean_q: -0.338707
 49100/100000: episode: 491, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.682, mean reward: -0.187 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.957, 10.241], loss: 0.002556, mae: 0.049826, mean_q: -0.326877
 49200/100000: episode: 492, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -13.115, mean reward: -0.131 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.790, 10.098], loss: 0.002807, mae: 0.052609, mean_q: -0.333593
 49300/100000: episode: 493, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -13.035, mean reward: -0.130 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.957, 10.098], loss: 0.002615, mae: 0.050854, mean_q: -0.333840
 49400/100000: episode: 494, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -9.358, mean reward: -0.094 [-1.000, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.370, 10.336], loss: 0.003593, mae: 0.060382, mean_q: -0.307243
 49500/100000: episode: 495, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.942, mean reward: -0.179 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.096, 10.312], loss: 0.002946, mae: 0.056067, mean_q: -0.291947
 49600/100000: episode: 496, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -12.496, mean reward: -0.125 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.712, 10.315], loss: 0.002857, mae: 0.053473, mean_q: -0.310520
 49700/100000: episode: 497, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -12.866, mean reward: -0.129 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.979, 10.098], loss: 0.002416, mae: 0.048409, mean_q: -0.314678
 49800/100000: episode: 498, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -10.139, mean reward: -0.101 [-1.000, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.170, 10.445], loss: 0.002712, mae: 0.051974, mean_q: -0.286719
 49900/100000: episode: 499, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: -21.022, mean reward: -0.210 [-1.000, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.765, 10.133], loss: 0.002789, mae: 0.052025, mean_q: -0.287833
 50000/100000: episode: 500, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -20.718, mean reward: -0.207 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.483, 10.110], loss: 0.002731, mae: 0.051340, mean_q: -0.284984
 50100/100000: episode: 501, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -15.954, mean reward: -0.160 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.064, 10.229], loss: 0.002831, mae: 0.051986, mean_q: -0.316838
 50200/100000: episode: 502, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: -11.327, mean reward: -0.113 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.592, 10.340], loss: 0.002561, mae: 0.049354, mean_q: -0.311189
 50300/100000: episode: 503, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -14.035, mean reward: -0.140 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.077, 10.098], loss: 0.002905, mae: 0.053555, mean_q: -0.269003
 50400/100000: episode: 504, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.229, mean reward: -0.182 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.811, 10.144], loss: 0.002837, mae: 0.053385, mean_q: -0.312894
 50500/100000: episode: 505, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -13.292, mean reward: -0.133 [-1.000, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.133, 10.098], loss: 0.002920, mae: 0.053616, mean_q: -0.331057
 50600/100000: episode: 506, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.316, mean reward: -0.183 [-1.000, 0.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.651, 10.126], loss: 0.002690, mae: 0.050726, mean_q: -0.308790
 50700/100000: episode: 507, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.422, mean reward: -0.174 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.616, 10.098], loss: 0.002709, mae: 0.051981, mean_q: -0.302756
 50800/100000: episode: 508, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -14.708, mean reward: -0.147 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.698, 10.098], loss: 0.002752, mae: 0.052573, mean_q: -0.299524
 50900/100000: episode: 509, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.616, mean reward: -0.166 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.503, 10.327], loss: 0.005278, mae: 0.068219, mean_q: -0.315801
 51000/100000: episode: 510, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.733, mean reward: -0.177 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.603, 10.263], loss: 0.002945, mae: 0.054571, mean_q: -0.315746
 51100/100000: episode: 511, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -15.471, mean reward: -0.155 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.351, 10.098], loss: 0.002667, mae: 0.052082, mean_q: -0.310996
 51200/100000: episode: 512, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -20.107, mean reward: -0.201 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.470, 10.098], loss: 0.002775, mae: 0.051907, mean_q: -0.311234
 51300/100000: episode: 513, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -15.405, mean reward: -0.154 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.686, 10.098], loss: 0.002771, mae: 0.051990, mean_q: -0.300472
 51400/100000: episode: 514, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -13.461, mean reward: -0.135 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.065, 10.212], loss: 0.002857, mae: 0.052469, mean_q: -0.314456
 51500/100000: episode: 515, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -19.445, mean reward: -0.194 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.516, 10.098], loss: 0.002803, mae: 0.051345, mean_q: -0.324649
 51600/100000: episode: 516, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.951, mean reward: -0.180 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.251, 10.098], loss: 0.002632, mae: 0.050279, mean_q: -0.318466
 51700/100000: episode: 517, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -14.552, mean reward: -0.146 [-1.000, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.757, 10.098], loss: 0.002974, mae: 0.053843, mean_q: -0.305108
 51800/100000: episode: 518, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.727, mean reward: -0.167 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.601, 10.098], loss: 0.002684, mae: 0.050744, mean_q: -0.278041
 51900/100000: episode: 519, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.226, mean reward: -0.172 [-1.000, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.367, 10.098], loss: 0.002899, mae: 0.053180, mean_q: -0.322895
 52000/100000: episode: 520, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -9.954, mean reward: -0.100 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.882, 10.098], loss: 0.002602, mae: 0.050063, mean_q: -0.305811
 52100/100000: episode: 521, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.043, mean reward: -0.160 [-1.000, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.002, 10.287], loss: 0.002658, mae: 0.050322, mean_q: -0.308937
 52200/100000: episode: 522, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.608, mean reward: -0.166 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.229, 10.098], loss: 0.002830, mae: 0.051939, mean_q: -0.301968
 52300/100000: episode: 523, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.459, mean reward: -0.195 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.771, 10.098], loss: 0.002673, mae: 0.051281, mean_q: -0.288529
 52400/100000: episode: 524, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.130, mean reward: -0.171 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.011, 10.098], loss: 0.002638, mae: 0.050599, mean_q: -0.307319
 52500/100000: episode: 525, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.704, mean reward: -0.177 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.037, 10.098], loss: 0.002550, mae: 0.049000, mean_q: -0.304608
 52600/100000: episode: 526, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: -11.600, mean reward: -0.116 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.489, 10.263], loss: 0.002378, mae: 0.047758, mean_q: -0.311884
 52700/100000: episode: 527, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: -16.033, mean reward: -0.160 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.484, 10.098], loss: 0.002522, mae: 0.049164, mean_q: -0.339616
 52800/100000: episode: 528, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -11.536, mean reward: -0.115 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.398, 10.098], loss: 0.002843, mae: 0.052884, mean_q: -0.310437
 52900/100000: episode: 529, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.930, mean reward: -0.189 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.659, 10.141], loss: 0.002613, mae: 0.051103, mean_q: -0.311699
 53000/100000: episode: 530, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -18.208, mean reward: -0.182 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.819, 10.098], loss: 0.002455, mae: 0.049927, mean_q: -0.331387
 53100/100000: episode: 531, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -18.739, mean reward: -0.187 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.933, 10.170], loss: 0.003679, mae: 0.060582, mean_q: -0.283107
 53200/100000: episode: 532, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.329, mean reward: -0.163 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.225, 10.098], loss: 0.002559, mae: 0.050895, mean_q: -0.285062
 53300/100000: episode: 533, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.577, mean reward: -0.176 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.175, 10.117], loss: 0.002564, mae: 0.049770, mean_q: -0.317937
 53400/100000: episode: 534, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.411, mean reward: -0.184 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.511, 10.120], loss: 0.002478, mae: 0.048389, mean_q: -0.300481
 53500/100000: episode: 535, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.250, mean reward: -0.192 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.941, 10.098], loss: 0.002616, mae: 0.050121, mean_q: -0.296273
 53600/100000: episode: 536, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: -7.165, mean reward: -0.072 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.340, 10.292], loss: 0.002502, mae: 0.049371, mean_q: -0.291846
 53700/100000: episode: 537, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.225, mean reward: -0.182 [-1.000, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.332, 10.098], loss: 0.002387, mae: 0.048192, mean_q: -0.281018
 53800/100000: episode: 538, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -19.114, mean reward: -0.191 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.332, 10.259], loss: 0.002908, mae: 0.055297, mean_q: -0.269853
 53900/100000: episode: 539, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -16.492, mean reward: -0.165 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.820, 10.271], loss: 0.002463, mae: 0.049185, mean_q: -0.334317
 54000/100000: episode: 540, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -13.850, mean reward: -0.139 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.011, 10.098], loss: 0.002670, mae: 0.050236, mean_q: -0.283174
 54100/100000: episode: 541, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.894, mean reward: -0.189 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.696, 10.098], loss: 0.002383, mae: 0.047540, mean_q: -0.317795
 54200/100000: episode: 542, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -16.701, mean reward: -0.167 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.046, 10.344], loss: 0.002259, mae: 0.047551, mean_q: -0.329129
 54300/100000: episode: 543, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.982, mean reward: -0.190 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.871, 10.157], loss: 0.002446, mae: 0.049161, mean_q: -0.294602
 54400/100000: episode: 544, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -13.864, mean reward: -0.139 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.711, 10.236], loss: 0.002416, mae: 0.048440, mean_q: -0.327030
 54500/100000: episode: 545, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -12.876, mean reward: -0.129 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.728, 10.344], loss: 0.002734, mae: 0.052836, mean_q: -0.299074
 54600/100000: episode: 546, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.499, mean reward: -0.175 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.985, 10.098], loss: 0.002574, mae: 0.052342, mean_q: -0.282099
 54700/100000: episode: 547, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.948, mean reward: -0.189 [-1.000, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.971, 10.098], loss: 0.002535, mae: 0.049843, mean_q: -0.294807
 54800/100000: episode: 548, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -13.596, mean reward: -0.136 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.003, 10.150], loss: 0.002461, mae: 0.048437, mean_q: -0.321663
 54900/100000: episode: 549, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.578, mean reward: -0.176 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.443, 10.098], loss: 0.002579, mae: 0.050197, mean_q: -0.299965
 55000/100000: episode: 550, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.906, mean reward: -0.159 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.450, 10.098], loss: 0.002613, mae: 0.050390, mean_q: -0.309919
 55100/100000: episode: 551, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.832, mean reward: -0.198 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.942, 10.180], loss: 0.002618, mae: 0.050911, mean_q: -0.305929
 55200/100000: episode: 552, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.364, mean reward: -0.184 [-1.000, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.398, 10.098], loss: 0.002767, mae: 0.052510, mean_q: -0.325396
 55300/100000: episode: 553, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.997, mean reward: -0.180 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.186, 10.145], loss: 0.002516, mae: 0.049942, mean_q: -0.285391
 55400/100000: episode: 554, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: -14.663, mean reward: -0.147 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.371, 10.098], loss: 0.002663, mae: 0.050913, mean_q: -0.278509
 55500/100000: episode: 555, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.324, mean reward: -0.183 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.004, 10.291], loss: 0.003243, mae: 0.058370, mean_q: -0.334884
 55600/100000: episode: 556, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.998, mean reward: -0.150 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.598, 10.098], loss: 0.002743, mae: 0.053493, mean_q: -0.329571
 55700/100000: episode: 557, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -14.270, mean reward: -0.143 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.687, 10.098], loss: 0.002499, mae: 0.049460, mean_q: -0.293086
 55800/100000: episode: 558, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.537, mean reward: -0.175 [-1.000, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.346, 10.098], loss: 0.002505, mae: 0.049412, mean_q: -0.275993
 55900/100000: episode: 559, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.522, mean reward: -0.185 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.984, 10.098], loss: 0.002527, mae: 0.050369, mean_q: -0.309989
 56000/100000: episode: 560, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -15.791, mean reward: -0.158 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.269, 10.098], loss: 0.002588, mae: 0.050498, mean_q: -0.323102
 56100/100000: episode: 561, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -14.129, mean reward: -0.141 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.327, 10.098], loss: 0.002503, mae: 0.050178, mean_q: -0.306620
 56200/100000: episode: 562, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -9.398, mean reward: -0.094 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.121, 10.098], loss: 0.002665, mae: 0.051772, mean_q: -0.318791
 56300/100000: episode: 563, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -11.852, mean reward: -0.119 [-1.000, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.665, 10.098], loss: 0.002572, mae: 0.050211, mean_q: -0.337850
 56400/100000: episode: 564, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: -11.695, mean reward: -0.117 [-1.000, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.853, 10.405], loss: 0.002700, mae: 0.051350, mean_q: -0.296397
 56500/100000: episode: 565, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -18.480, mean reward: -0.185 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.304, 10.205], loss: 0.002702, mae: 0.052166, mean_q: -0.290946
 56600/100000: episode: 566, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -18.547, mean reward: -0.185 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.281, 10.098], loss: 0.003173, mae: 0.058941, mean_q: -0.274607
 56700/100000: episode: 567, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.092, mean reward: -0.171 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.834, 10.098], loss: 0.002747, mae: 0.051899, mean_q: -0.319506
 56800/100000: episode: 568, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.156, mean reward: -0.182 [-1.000, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.846, 10.228], loss: 0.002597, mae: 0.050424, mean_q: -0.321549
 56900/100000: episode: 569, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.958, mean reward: -0.180 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.207, 10.205], loss: 0.002717, mae: 0.051077, mean_q: -0.321046
 57000/100000: episode: 570, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.787, mean reward: -0.178 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.338, 10.098], loss: 0.002591, mae: 0.048899, mean_q: -0.331757
 57100/100000: episode: 571, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.129, mean reward: -0.161 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.304, 10.098], loss: 0.002563, mae: 0.049848, mean_q: -0.309987
 57200/100000: episode: 572, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.882, mean reward: -0.169 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.804, 10.349], loss: 0.002451, mae: 0.048089, mean_q: -0.320028
 57300/100000: episode: 573, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -17.943, mean reward: -0.179 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.111, 10.256], loss: 0.002401, mae: 0.047973, mean_q: -0.330387
 57400/100000: episode: 574, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -15.695, mean reward: -0.157 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.477, 10.098], loss: 0.002666, mae: 0.052000, mean_q: -0.358318
 57500/100000: episode: 575, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -14.962, mean reward: -0.150 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.926, 10.178], loss: 0.002647, mae: 0.050898, mean_q: -0.301794
 57600/100000: episode: 576, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -15.790, mean reward: -0.158 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.849, 10.098], loss: 0.010098, mae: 0.089835, mean_q: -0.315852
 57700/100000: episode: 577, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -10.933, mean reward: -0.109 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.222, 10.396], loss: 0.002971, mae: 0.055798, mean_q: -0.306801
 57800/100000: episode: 578, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -11.498, mean reward: -0.115 [-1.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.001, 10.459], loss: 0.002677, mae: 0.052515, mean_q: -0.293695
 57900/100000: episode: 579, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.065, mean reward: -0.181 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.125, 10.098], loss: 0.002660, mae: 0.052478, mean_q: -0.278599
 58000/100000: episode: 580, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.150, mean reward: -0.192 [-1.000, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.402, 10.098], loss: 0.002630, mae: 0.050757, mean_q: -0.329760
 58100/100000: episode: 581, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -11.265, mean reward: -0.113 [-1.000, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.268, 10.383], loss: 0.002506, mae: 0.050806, mean_q: -0.315408
 58200/100000: episode: 582, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -15.916, mean reward: -0.159 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.339, 10.098], loss: 0.002853, mae: 0.054727, mean_q: -0.328794
 58300/100000: episode: 583, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.019, mean reward: -0.180 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.349, 10.098], loss: 0.003099, mae: 0.056901, mean_q: -0.293804
 58400/100000: episode: 584, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.700, mean reward: -0.177 [-1.000, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.591, 10.346], loss: 0.002613, mae: 0.051160, mean_q: -0.314823
 58500/100000: episode: 585, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.220, mean reward: -0.172 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.408, 10.151], loss: 0.002528, mae: 0.049896, mean_q: -0.320580
 58600/100000: episode: 586, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -11.428, mean reward: -0.114 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.523, 10.098], loss: 0.002664, mae: 0.051776, mean_q: -0.302331
 58700/100000: episode: 587, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -13.327, mean reward: -0.133 [-1.000, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.570, 10.098], loss: 0.003004, mae: 0.056165, mean_q: -0.313676
 58800/100000: episode: 588, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -13.635, mean reward: -0.136 [-1.000, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.105, 10.098], loss: 0.003043, mae: 0.056513, mean_q: -0.273296
 58900/100000: episode: 589, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.544, mean reward: -0.145 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.637, 10.098], loss: 0.002620, mae: 0.051569, mean_q: -0.292099
 59000/100000: episode: 590, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -7.871, mean reward: -0.079 [-1.000, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.725, 10.420], loss: 0.002489, mae: 0.049565, mean_q: -0.333148
 59100/100000: episode: 591, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.273, mean reward: -0.163 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.830, 10.098], loss: 0.002642, mae: 0.050862, mean_q: -0.314039
 59200/100000: episode: 592, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -20.157, mean reward: -0.202 [-1.000, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.566, 10.098], loss: 0.002872, mae: 0.053862, mean_q: -0.287903
 59300/100000: episode: 593, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -13.066, mean reward: -0.131 [-1.000, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.899, 10.531], loss: 0.002809, mae: 0.052791, mean_q: -0.304779
 59400/100000: episode: 594, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -15.501, mean reward: -0.155 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.747, 10.098], loss: 0.002909, mae: 0.054413, mean_q: -0.289032
 59500/100000: episode: 595, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -16.963, mean reward: -0.170 [-1.000, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.263, 10.206], loss: 0.002797, mae: 0.054106, mean_q: -0.307164
 59600/100000: episode: 596, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -9.220, mean reward: -0.092 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.973, 10.098], loss: 0.002663, mae: 0.051023, mean_q: -0.301298
 59700/100000: episode: 597, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.902, mean reward: -0.169 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.490, 10.253], loss: 0.002759, mae: 0.052318, mean_q: -0.320147
 59800/100000: episode: 598, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -17.636, mean reward: -0.176 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.660, 10.119], loss: 0.002665, mae: 0.050956, mean_q: -0.303220
 59900/100000: episode: 599, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -16.923, mean reward: -0.169 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.318, 10.227], loss: 0.002751, mae: 0.051339, mean_q: -0.319899
 60000/100000: episode: 600, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.615, mean reward: -0.186 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.438, 10.177], loss: 0.002697, mae: 0.051074, mean_q: -0.285861
 60100/100000: episode: 601, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -14.938, mean reward: -0.149 [-1.000, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.243, 10.098], loss: 0.002401, mae: 0.047972, mean_q: -0.278112
 60200/100000: episode: 602, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -16.327, mean reward: -0.163 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.624, 10.098], loss: 0.002396, mae: 0.048582, mean_q: -0.306392
 60300/100000: episode: 603, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -20.169, mean reward: -0.202 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.550, 10.193], loss: 0.002562, mae: 0.049016, mean_q: -0.326338
 60400/100000: episode: 604, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -15.750, mean reward: -0.157 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.032, 10.347], loss: 0.002546, mae: 0.049585, mean_q: -0.287371
 60500/100000: episode: 605, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -16.659, mean reward: -0.167 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.576, 10.217], loss: 0.002461, mae: 0.048092, mean_q: -0.353726
 60600/100000: episode: 606, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -14.507, mean reward: -0.145 [-1.000, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.748, 10.098], loss: 0.002532, mae: 0.050904, mean_q: -0.290048
 60700/100000: episode: 607, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.930, mean reward: -0.189 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.406, 10.098], loss: 0.002697, mae: 0.051238, mean_q: -0.302846
 60800/100000: episode: 608, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -14.805, mean reward: -0.148 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.560, 10.253], loss: 0.002715, mae: 0.052772, mean_q: -0.287937
 60900/100000: episode: 609, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -14.722, mean reward: -0.147 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-2.552, 10.098], loss: 0.004725, mae: 0.067590, mean_q: -0.289252
 61000/100000: episode: 610, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -16.733, mean reward: -0.167 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.709, 10.204], loss: 0.002659, mae: 0.051332, mean_q: -0.254362
 61100/100000: episode: 611, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -16.159, mean reward: -0.162 [-1.000, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.615, 10.098], loss: 0.002606, mae: 0.049716, mean_q: -0.307864
 61200/100000: episode: 612, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.879, mean reward: -0.179 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.967, 10.126], loss: 0.002579, mae: 0.049899, mean_q: -0.269182
 61300/100000: episode: 613, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.170, mean reward: -0.162 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.472, 10.123], loss: 0.002432, mae: 0.048687, mean_q: -0.280173
 61400/100000: episode: 614, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.054, mean reward: -0.171 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.956, 10.098], loss: 0.002458, mae: 0.048544, mean_q: -0.308018
 61500/100000: episode: 615, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.003, mean reward: -0.170 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.985, 10.098], loss: 0.002550, mae: 0.049214, mean_q: -0.287579
 61600/100000: episode: 616, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -19.182, mean reward: -0.192 [-1.000, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.696, 10.115], loss: 0.002414, mae: 0.048357, mean_q: -0.293782
 61700/100000: episode: 617, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.320, mean reward: -0.173 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.020, 10.098], loss: 0.002384, mae: 0.047707, mean_q: -0.310687
 61800/100000: episode: 618, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.786, mean reward: -0.188 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.300, 10.139], loss: 0.002499, mae: 0.049106, mean_q: -0.297655
 61900/100000: episode: 619, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.982, mean reward: -0.190 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.142, 10.098], loss: 0.002846, mae: 0.053574, mean_q: -0.320056
 62000/100000: episode: 620, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -19.196, mean reward: -0.192 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.380, 10.154], loss: 0.004187, mae: 0.064526, mean_q: -0.280368
 62100/100000: episode: 621, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.850, mean reward: -0.189 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.394, 10.098], loss: 0.002716, mae: 0.052518, mean_q: -0.291769
 62200/100000: episode: 622, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -17.999, mean reward: -0.180 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.621, 10.098], loss: 0.002847, mae: 0.053806, mean_q: -0.295401
 62300/100000: episode: 623, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.465, mean reward: -0.155 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.709, 10.098], loss: 0.002451, mae: 0.048322, mean_q: -0.330353
 62400/100000: episode: 624, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.175, mean reward: -0.192 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.131, 10.098], loss: 0.002489, mae: 0.049146, mean_q: -0.302230
 62500/100000: episode: 625, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.179, mean reward: -0.172 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.588, 10.165], loss: 0.002525, mae: 0.048921, mean_q: -0.338884
 62600/100000: episode: 626, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -15.816, mean reward: -0.158 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.945, 10.358], loss: 0.002661, mae: 0.050697, mean_q: -0.283677
 62700/100000: episode: 627, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -18.089, mean reward: -0.181 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.349, 10.098], loss: 0.002791, mae: 0.052476, mean_q: -0.322712
 62800/100000: episode: 628, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.895, mean reward: -0.179 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.681, 10.098], loss: 0.002729, mae: 0.051606, mean_q: -0.315314
 62900/100000: episode: 629, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -18.798, mean reward: -0.188 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.657, 10.098], loss: 0.002625, mae: 0.049450, mean_q: -0.301922
 63000/100000: episode: 630, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.991, mean reward: -0.200 [-1.000, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.778, 10.098], loss: 0.002401, mae: 0.048078, mean_q: -0.326696
 63100/100000: episode: 631, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -19.588, mean reward: -0.196 [-1.000, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.145, 10.098], loss: 0.002549, mae: 0.049014, mean_q: -0.316677
 63200/100000: episode: 632, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -19.252, mean reward: -0.193 [-1.000, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.504, 10.154], loss: 0.002645, mae: 0.049843, mean_q: -0.289953
 63300/100000: episode: 633, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.121, mean reward: -0.151 [-1.000, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.116, 10.098], loss: 0.002735, mae: 0.050967, mean_q: -0.317213
 63400/100000: episode: 634, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.148, mean reward: -0.161 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.816, 10.098], loss: 0.002546, mae: 0.050472, mean_q: -0.309205
 63500/100000: episode: 635, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -5.371, mean reward: -0.054 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.120, 10.399], loss: 0.002671, mae: 0.051548, mean_q: -0.313083
 63600/100000: episode: 636, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -12.702, mean reward: -0.127 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.417, 10.098], loss: 0.002300, mae: 0.046972, mean_q: -0.313755
 63700/100000: episode: 637, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.815, mean reward: -0.178 [-1.000, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.132, 10.098], loss: 0.002833, mae: 0.053103, mean_q: -0.310936
 63800/100000: episode: 638, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.200, mean reward: -0.182 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.654, 10.146], loss: 0.002577, mae: 0.049114, mean_q: -0.333882
 63900/100000: episode: 639, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -14.194, mean reward: -0.142 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.477, 10.098], loss: 0.002487, mae: 0.048971, mean_q: -0.346010
 64000/100000: episode: 640, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.980, mean reward: -0.160 [-1.000, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.643, 10.345], loss: 0.002694, mae: 0.050574, mean_q: -0.344862
 64100/100000: episode: 641, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.001, mean reward: -0.190 [-1.000, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.554, 10.168], loss: 0.002644, mae: 0.051103, mean_q: -0.315643
 64200/100000: episode: 642, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -17.919, mean reward: -0.179 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.851, 10.105], loss: 0.002491, mae: 0.048674, mean_q: -0.323378
 64300/100000: episode: 643, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -14.531, mean reward: -0.145 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.913, 10.118], loss: 0.002405, mae: 0.047778, mean_q: -0.364734
 64400/100000: episode: 644, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -12.281, mean reward: -0.123 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.653, 10.098], loss: 0.002582, mae: 0.050523, mean_q: -0.314657
 64500/100000: episode: 645, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -12.028, mean reward: -0.120 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.604, 10.098], loss: 0.002639, mae: 0.050168, mean_q: -0.353192
 64600/100000: episode: 646, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: -11.261, mean reward: -0.113 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.957, 10.195], loss: 0.002432, mae: 0.048091, mean_q: -0.326649
 64700/100000: episode: 647, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.703, mean reward: -0.167 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.908, 10.098], loss: 0.002574, mae: 0.049906, mean_q: -0.282041
 64800/100000: episode: 648, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -15.183, mean reward: -0.152 [-1.000, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.316, 10.098], loss: 0.002457, mae: 0.048311, mean_q: -0.338377
 64900/100000: episode: 649, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.594, mean reward: -0.176 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.637, 10.098], loss: 0.002554, mae: 0.049244, mean_q: -0.350235
 65000/100000: episode: 650, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -13.510, mean reward: -0.135 [-1.000, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.817, 10.098], loss: 0.002955, mae: 0.055822, mean_q: -0.318449
 65100/100000: episode: 651, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.404, mean reward: -0.184 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.083, 10.098], loss: 0.002556, mae: 0.049953, mean_q: -0.332374
 65200/100000: episode: 652, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.143, mean reward: -0.191 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.524, 10.229], loss: 0.002705, mae: 0.050731, mean_q: -0.317902
 65300/100000: episode: 653, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -18.731, mean reward: -0.187 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.137, 10.139], loss: 0.002585, mae: 0.050292, mean_q: -0.310561
 65400/100000: episode: 654, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -18.491, mean reward: -0.185 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.019, 10.161], loss: 0.002664, mae: 0.051162, mean_q: -0.346982
 65500/100000: episode: 655, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: -18.362, mean reward: -0.184 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.486, 10.244], loss: 0.002506, mae: 0.048719, mean_q: -0.313785
 65600/100000: episode: 656, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -18.287, mean reward: -0.183 [-1.000, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.335, 10.098], loss: 0.002534, mae: 0.048667, mean_q: -0.303624
 65700/100000: episode: 657, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.374, mean reward: -0.154 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.417, 10.098], loss: 0.003072, mae: 0.054417, mean_q: -0.302595
 65800/100000: episode: 658, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.229, mean reward: -0.172 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.775, 10.098], loss: 0.002410, mae: 0.048034, mean_q: -0.349919
 65900/100000: episode: 659, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.108, mean reward: -0.151 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.338, 10.098], loss: 0.002819, mae: 0.052280, mean_q: -0.305987
 66000/100000: episode: 660, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.646, mean reward: -0.146 [-1.000, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.950, 10.098], loss: 0.002684, mae: 0.050689, mean_q: -0.321505
 66100/100000: episode: 661, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.874, mean reward: -0.189 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.248, 10.120], loss: 0.002702, mae: 0.050846, mean_q: -0.334513
 66200/100000: episode: 662, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -12.073, mean reward: -0.121 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.449, 10.424], loss: 0.002774, mae: 0.052041, mean_q: -0.316420
 66300/100000: episode: 663, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.917, mean reward: -0.169 [-1.000, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.127, 10.272], loss: 0.003011, mae: 0.052689, mean_q: -0.357104
 66400/100000: episode: 664, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -15.851, mean reward: -0.159 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.574, 10.223], loss: 0.007159, mae: 0.076984, mean_q: -0.287487
 66500/100000: episode: 665, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.629, mean reward: -0.176 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.965, 10.131], loss: 0.002923, mae: 0.054776, mean_q: -0.307721
 66600/100000: episode: 666, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -16.981, mean reward: -0.170 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.655, 10.139], loss: 0.002838, mae: 0.052676, mean_q: -0.321116
 66700/100000: episode: 667, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: -15.944, mean reward: -0.159 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.378, 10.098], loss: 0.002885, mae: 0.052252, mean_q: -0.318314
 66800/100000: episode: 668, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.582, mean reward: -0.186 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.803, 10.269], loss: 0.002672, mae: 0.050720, mean_q: -0.293821
 66900/100000: episode: 669, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -12.737, mean reward: -0.127 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.758, 10.098], loss: 0.002652, mae: 0.050546, mean_q: -0.313493
 67000/100000: episode: 670, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.834, mean reward: -0.168 [-1.000, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.139, 10.098], loss: 0.002586, mae: 0.049233, mean_q: -0.314715
 67100/100000: episode: 671, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -13.756, mean reward: -0.138 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.101, 10.098], loss: 0.002553, mae: 0.049571, mean_q: -0.330391
 67200/100000: episode: 672, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -20.283, mean reward: -0.203 [-1.000, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.110, 10.098], loss: 0.002714, mae: 0.050422, mean_q: -0.339887
 67300/100000: episode: 673, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -17.695, mean reward: -0.177 [-1.000, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.651, 10.098], loss: 0.002625, mae: 0.050406, mean_q: -0.306567
 67400/100000: episode: 674, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.466, mean reward: -0.175 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.046, 10.138], loss: 0.002590, mae: 0.049761, mean_q: -0.286490
 67500/100000: episode: 675, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -12.826, mean reward: -0.128 [-1.000, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.480, 10.098], loss: 0.002701, mae: 0.051094, mean_q: -0.289127
 67600/100000: episode: 676, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.792, mean reward: -0.188 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.413, 10.098], loss: 0.002721, mae: 0.052375, mean_q: -0.299031
 67700/100000: episode: 677, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -15.767, mean reward: -0.158 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.334, 10.098], loss: 0.002558, mae: 0.049634, mean_q: -0.322481
 67800/100000: episode: 678, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -20.874, mean reward: -0.209 [-1.000, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.981, 10.188], loss: 0.002730, mae: 0.051883, mean_q: -0.280854
 67900/100000: episode: 679, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -12.396, mean reward: -0.124 [-1.000, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.150, 10.485], loss: 0.002457, mae: 0.048592, mean_q: -0.293964
 68000/100000: episode: 680, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.004, mean reward: -0.160 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.530, 10.212], loss: 0.002700, mae: 0.051436, mean_q: -0.302676
 68100/100000: episode: 681, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.124, mean reward: -0.191 [-1.000, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.663, 10.098], loss: 0.008736, mae: 0.081194, mean_q: -0.271262
 68200/100000: episode: 682, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -13.136, mean reward: -0.131 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.914, 10.098], loss: 0.002925, mae: 0.054461, mean_q: -0.332845
 68300/100000: episode: 683, duration: 0.519s, episode steps: 100, steps per second: 192, episode reward: -17.907, mean reward: -0.179 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.236, 10.103], loss: 0.002843, mae: 0.054351, mean_q: -0.272060
 68400/100000: episode: 684, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -15.614, mean reward: -0.156 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.602, 10.342], loss: 0.002681, mae: 0.051946, mean_q: -0.314066
 68500/100000: episode: 685, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.162, mean reward: -0.182 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.212, 10.282], loss: 0.002531, mae: 0.050296, mean_q: -0.341764
 68600/100000: episode: 686, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.719, mean reward: -0.177 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.018, 10.098], loss: 0.002781, mae: 0.052736, mean_q: -0.296715
 68700/100000: episode: 687, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -18.527, mean reward: -0.185 [-1.000, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.592, 10.187], loss: 0.002609, mae: 0.050397, mean_q: -0.326966
 68800/100000: episode: 688, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.752, mean reward: -0.198 [-1.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.151, 10.125], loss: 0.002772, mae: 0.053064, mean_q: -0.308235
 68900/100000: episode: 689, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.416, mean reward: -0.134 [-1.000, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.351, 10.098], loss: 0.002532, mae: 0.049528, mean_q: -0.311780
 69000/100000: episode: 690, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.771, mean reward: -0.188 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.219, 10.098], loss: 0.003011, mae: 0.054681, mean_q: -0.287049
 69100/100000: episode: 691, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -13.841, mean reward: -0.138 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.925, 10.098], loss: 0.002596, mae: 0.050314, mean_q: -0.323884
 69200/100000: episode: 692, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.551, mean reward: -0.166 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.625, 10.213], loss: 0.002812, mae: 0.052895, mean_q: -0.293529
 69300/100000: episode: 693, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -20.717, mean reward: -0.207 [-1.000, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.352, 10.210], loss: 0.005175, mae: 0.067032, mean_q: -0.313600
 69400/100000: episode: 694, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -15.517, mean reward: -0.155 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.888, 10.202], loss: 0.002356, mae: 0.048069, mean_q: -0.325380
 69500/100000: episode: 695, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.182, mean reward: -0.172 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.674, 10.098], loss: 0.002450, mae: 0.048690, mean_q: -0.314617
 69600/100000: episode: 696, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -15.846, mean reward: -0.158 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.770, 10.152], loss: 0.003223, mae: 0.058202, mean_q: -0.344519
 69700/100000: episode: 697, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.611, mean reward: -0.176 [-1.000, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.720, 10.098], loss: 0.002674, mae: 0.051170, mean_q: -0.312513
 69800/100000: episode: 698, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.817, mean reward: -0.168 [-1.000, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.601, 10.098], loss: 0.002713, mae: 0.051007, mean_q: -0.313682
 69900/100000: episode: 699, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.971, mean reward: -0.160 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.023, 10.127], loss: 0.002512, mae: 0.049571, mean_q: -0.311072
 70000/100000: episode: 700, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -18.055, mean reward: -0.181 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.389, 10.250], loss: 0.002792, mae: 0.052342, mean_q: -0.304741
 70100/100000: episode: 701, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.759, mean reward: -0.168 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.101, 10.098], loss: 0.002399, mae: 0.047537, mean_q: -0.369092
 70200/100000: episode: 702, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.596, mean reward: -0.196 [-1.000, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.904, 10.098], loss: 0.002477, mae: 0.049988, mean_q: -0.281965
 70300/100000: episode: 703, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -11.830, mean reward: -0.118 [-1.000, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.639, 10.625], loss: 0.002530, mae: 0.049219, mean_q: -0.330030
 70400/100000: episode: 704, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.033, mean reward: -0.180 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.712, 10.098], loss: 0.002750, mae: 0.051111, mean_q: -0.324856
 70500/100000: episode: 705, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.022, mean reward: -0.170 [-1.000, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.861, 10.100], loss: 0.002761, mae: 0.051739, mean_q: -0.316762
 70600/100000: episode: 706, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -19.337, mean reward: -0.193 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.328, 10.245], loss: 0.002606, mae: 0.049911, mean_q: -0.317934
 70700/100000: episode: 707, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -13.090, mean reward: -0.131 [-1.000, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.692, 10.098], loss: 0.002504, mae: 0.049752, mean_q: -0.310012
 70800/100000: episode: 708, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -17.980, mean reward: -0.180 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.195, 10.098], loss: 0.002657, mae: 0.049270, mean_q: -0.381842
 70900/100000: episode: 709, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -14.757, mean reward: -0.148 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.394, 10.260], loss: 0.002632, mae: 0.049941, mean_q: -0.332896
 71000/100000: episode: 710, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.523, mean reward: -0.165 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.485, 10.148], loss: 0.002324, mae: 0.046884, mean_q: -0.331315
 71100/100000: episode: 711, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -11.958, mean reward: -0.120 [-1.000, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.928, 10.098], loss: 0.002531, mae: 0.049128, mean_q: -0.322358
 71200/100000: episode: 712, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.306, mean reward: -0.173 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.395, 10.098], loss: 0.002638, mae: 0.049209, mean_q: -0.337936
 71300/100000: episode: 713, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -16.516, mean reward: -0.165 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.693, 10.098], loss: 0.003488, mae: 0.056772, mean_q: -0.295352
 71400/100000: episode: 714, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -16.797, mean reward: -0.168 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.863, 10.098], loss: 0.002749, mae: 0.051910, mean_q: -0.298959
 71500/100000: episode: 715, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -20.145, mean reward: -0.201 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.642, 10.098], loss: 0.002396, mae: 0.048167, mean_q: -0.285396
 71600/100000: episode: 716, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.782, mean reward: -0.178 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.119, 10.151], loss: 0.002627, mae: 0.051986, mean_q: -0.325938
 71700/100000: episode: 717, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.590, mean reward: -0.186 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.320, 10.098], loss: 0.002563, mae: 0.048731, mean_q: -0.322863
 71800/100000: episode: 718, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -13.018, mean reward: -0.130 [-1.000, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.867, 10.130], loss: 0.002694, mae: 0.050541, mean_q: -0.322878
 71900/100000: episode: 719, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -10.095, mean reward: -0.101 [-1.000, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.306, 10.098], loss: 0.002621, mae: 0.050143, mean_q: -0.309951
 72000/100000: episode: 720, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: -17.751, mean reward: -0.178 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.825, 10.267], loss: 0.002457, mae: 0.048586, mean_q: -0.317092
 72100/100000: episode: 721, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.996, mean reward: -0.190 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.210, 10.098], loss: 0.002665, mae: 0.051401, mean_q: -0.285475
 72200/100000: episode: 722, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -17.556, mean reward: -0.176 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.609, 10.192], loss: 0.002654, mae: 0.050607, mean_q: -0.296540
 72300/100000: episode: 723, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.428, mean reward: -0.184 [-1.000, 0.254], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.708, 10.103], loss: 0.002600, mae: 0.050223, mean_q: -0.315771
 72400/100000: episode: 724, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -11.236, mean reward: -0.112 [-1.000, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.859, 10.098], loss: 0.002448, mae: 0.048163, mean_q: -0.313091
 72500/100000: episode: 725, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.146, mean reward: -0.161 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.424, 10.098], loss: 0.002416, mae: 0.047764, mean_q: -0.312940
 72600/100000: episode: 726, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -15.794, mean reward: -0.158 [-1.000, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.602, 10.119], loss: 0.002463, mae: 0.048307, mean_q: -0.289012
 72700/100000: episode: 727, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -19.406, mean reward: -0.194 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.012, 10.380], loss: 0.002381, mae: 0.048215, mean_q: -0.298294
 72800/100000: episode: 728, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -16.886, mean reward: -0.169 [-1.000, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.568, 10.098], loss: 0.002447, mae: 0.048451, mean_q: -0.324394
 72900/100000: episode: 729, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.161, mean reward: -0.182 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.797, 10.183], loss: 0.004862, mae: 0.067880, mean_q: -0.317990
 73000/100000: episode: 730, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -19.675, mean reward: -0.197 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.237, 10.098], loss: 0.002734, mae: 0.051543, mean_q: -0.334904
 73100/100000: episode: 731, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.761, mean reward: -0.158 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.692, 10.098], loss: 0.002572, mae: 0.050177, mean_q: -0.296768
 73200/100000: episode: 732, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -13.727, mean reward: -0.137 [-1.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.654, 10.370], loss: 0.002559, mae: 0.048146, mean_q: -0.334409
 73300/100000: episode: 733, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -20.481, mean reward: -0.205 [-1.000, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.407, 10.098], loss: 0.002484, mae: 0.048675, mean_q: -0.307831
 73400/100000: episode: 734, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -18.206, mean reward: -0.182 [-1.000, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.086, 10.104], loss: 0.002558, mae: 0.049280, mean_q: -0.316283
 73500/100000: episode: 735, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -15.388, mean reward: -0.154 [-1.000, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.943, 10.357], loss: 0.002556, mae: 0.049848, mean_q: -0.297524
 73600/100000: episode: 736, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.886, mean reward: -0.179 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.224, 10.232], loss: 0.002503, mae: 0.047660, mean_q: -0.321890
 73700/100000: episode: 737, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: -14.667, mean reward: -0.147 [-1.000, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.250, 10.138], loss: 0.002502, mae: 0.047890, mean_q: -0.350019
 73800/100000: episode: 738, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.512, mean reward: -0.165 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.119, 10.098], loss: 0.002609, mae: 0.049634, mean_q: -0.318001
 73900/100000: episode: 739, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.211, mean reward: -0.182 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.821, 10.160], loss: 0.002579, mae: 0.048991, mean_q: -0.302406
 74000/100000: episode: 740, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.397, mean reward: -0.184 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.878, 10.266], loss: 0.002600, mae: 0.049629, mean_q: -0.296396
 74100/100000: episode: 741, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -7.291, mean reward: -0.073 [-1.000, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.571, 10.328], loss: 0.002445, mae: 0.047610, mean_q: -0.310391
 74200/100000: episode: 742, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -19.920, mean reward: -0.199 [-1.000, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.684, 10.126], loss: 0.002773, mae: 0.051647, mean_q: -0.295696
 74300/100000: episode: 743, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -17.946, mean reward: -0.179 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.221, 10.221], loss: 0.002496, mae: 0.049326, mean_q: -0.320049
 74400/100000: episode: 744, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -19.277, mean reward: -0.193 [-1.000, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.073, 10.183], loss: 0.002430, mae: 0.047537, mean_q: -0.339034
 74500/100000: episode: 745, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -10.572, mean reward: -0.106 [-1.000, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.888, 10.328], loss: 0.002635, mae: 0.050864, mean_q: -0.297422
 74600/100000: episode: 746, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.433, mean reward: -0.164 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.616, 10.299], loss: 0.002351, mae: 0.047019, mean_q: -0.305232
 74700/100000: episode: 747, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.573, mean reward: -0.166 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.611, 10.098], loss: 0.002685, mae: 0.050607, mean_q: -0.320705
 74800/100000: episode: 748, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -19.299, mean reward: -0.193 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.866, 10.224], loss: 0.002610, mae: 0.049893, mean_q: -0.297516
 74900/100000: episode: 749, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -15.892, mean reward: -0.159 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.531, 10.098], loss: 0.002583, mae: 0.049281, mean_q: -0.300280
 75000/100000: episode: 750, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.818, mean reward: -0.188 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.477, 10.189], loss: 0.002465, mae: 0.047573, mean_q: -0.328357
 75100/100000: episode: 751, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -17.780, mean reward: -0.178 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.058, 10.098], loss: 0.002633, mae: 0.051677, mean_q: -0.325950
 75200/100000: episode: 752, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -12.734, mean reward: -0.127 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.679, 10.191], loss: 0.002443, mae: 0.048254, mean_q: -0.300976
 75300/100000: episode: 753, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -13.235, mean reward: -0.132 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.729, 10.098], loss: 0.002520, mae: 0.048228, mean_q: -0.305489
 75400/100000: episode: 754, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -16.758, mean reward: -0.168 [-1.000, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.882, 10.098], loss: 0.002582, mae: 0.049027, mean_q: -0.312453
 75500/100000: episode: 755, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.247, mean reward: -0.182 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.477, 10.098], loss: 0.002464, mae: 0.049251, mean_q: -0.299140
 75600/100000: episode: 756, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.257, mean reward: -0.163 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.430, 10.308], loss: 0.002493, mae: 0.048231, mean_q: -0.303910
 75700/100000: episode: 757, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -19.229, mean reward: -0.192 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.390, 10.161], loss: 0.002799, mae: 0.051422, mean_q: -0.313707
 75800/100000: episode: 758, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.193, mean reward: -0.152 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.392, 10.214], loss: 0.002639, mae: 0.050570, mean_q: -0.299484
 75900/100000: episode: 759, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.792, mean reward: -0.188 [-1.000, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.736, 10.348], loss: 0.002713, mae: 0.050768, mean_q: -0.291825
 76000/100000: episode: 760, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.821, mean reward: -0.188 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.564, 10.098], loss: 0.002639, mae: 0.049717, mean_q: -0.323044
 76100/100000: episode: 761, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.662, mean reward: -0.177 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.499, 10.211], loss: 0.002425, mae: 0.048041, mean_q: -0.304489
 76200/100000: episode: 762, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -14.157, mean reward: -0.142 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.849, 10.098], loss: 0.002630, mae: 0.051251, mean_q: -0.321387
 76300/100000: episode: 763, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.401, mean reward: -0.184 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.534, 10.177], loss: 0.003703, mae: 0.056116, mean_q: -0.306347
 76400/100000: episode: 764, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -13.386, mean reward: -0.134 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.900, 10.098], loss: 0.005506, mae: 0.067428, mean_q: -0.286720
 76500/100000: episode: 765, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.300, mean reward: -0.183 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.914, 10.154], loss: 0.002612, mae: 0.050679, mean_q: -0.320847
 76600/100000: episode: 766, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.129, mean reward: -0.181 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.485, 10.098], loss: 0.002503, mae: 0.049109, mean_q: -0.309336
 76700/100000: episode: 767, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.640, mean reward: -0.186 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.708, 10.231], loss: 0.002497, mae: 0.048030, mean_q: -0.353726
 76800/100000: episode: 768, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: -16.293, mean reward: -0.163 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.589, 10.098], loss: 0.002564, mae: 0.050305, mean_q: -0.306528
 76900/100000: episode: 769, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -14.911, mean reward: -0.149 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.430, 10.098], loss: 0.002367, mae: 0.047842, mean_q: -0.305207
 77000/100000: episode: 770, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.016, mean reward: -0.180 [-1.000, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.901, 10.156], loss: 0.002380, mae: 0.046970, mean_q: -0.361207
 77100/100000: episode: 771, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.802, mean reward: -0.168 [-1.000, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.069, 10.232], loss: 0.002501, mae: 0.049219, mean_q: -0.290529
 77200/100000: episode: 772, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.204, mean reward: -0.162 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.050, 10.206], loss: 0.002358, mae: 0.047013, mean_q: -0.320996
 77300/100000: episode: 773, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -19.854, mean reward: -0.199 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.395, 10.179], loss: 0.002430, mae: 0.047832, mean_q: -0.332477
 77400/100000: episode: 774, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -16.061, mean reward: -0.161 [-1.000, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.419, 10.225], loss: 0.002345, mae: 0.047267, mean_q: -0.344518
 77500/100000: episode: 775, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.342, mean reward: -0.163 [-1.000, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.766, 10.157], loss: 0.002552, mae: 0.048410, mean_q: -0.342506
 77600/100000: episode: 776, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -14.554, mean reward: -0.146 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.150, 10.098], loss: 0.002529, mae: 0.049894, mean_q: -0.314256
 77700/100000: episode: 777, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -19.222, mean reward: -0.192 [-1.000, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.514, 10.188], loss: 0.002513, mae: 0.049002, mean_q: -0.309409
 77800/100000: episode: 778, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -20.300, mean reward: -0.203 [-1.000, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.197, 10.168], loss: 0.002507, mae: 0.049389, mean_q: -0.329435
 77900/100000: episode: 779, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -14.196, mean reward: -0.142 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.065, 10.098], loss: 0.002381, mae: 0.048494, mean_q: -0.329201
 78000/100000: episode: 780, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -16.863, mean reward: -0.169 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.035, 10.098], loss: 0.002479, mae: 0.048462, mean_q: -0.317897
 78100/100000: episode: 781, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -21.043, mean reward: -0.210 [-1.000, 0.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.608, 10.166], loss: 0.002605, mae: 0.051125, mean_q: -0.294604
 78200/100000: episode: 782, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -14.266, mean reward: -0.143 [-1.000, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.661, 10.211], loss: 0.002611, mae: 0.050687, mean_q: -0.317725
 78300/100000: episode: 783, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -16.181, mean reward: -0.162 [-1.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.427, 10.203], loss: 0.002754, mae: 0.051554, mean_q: -0.294893
 78400/100000: episode: 784, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -20.496, mean reward: -0.205 [-1.000, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.011, 10.174], loss: 0.002519, mae: 0.050203, mean_q: -0.339039
 78500/100000: episode: 785, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -14.469, mean reward: -0.145 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.461, 10.098], loss: 0.002590, mae: 0.050122, mean_q: -0.330199
 78600/100000: episode: 786, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.164, mean reward: -0.152 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.954, 10.305], loss: 0.002599, mae: 0.051042, mean_q: -0.326993
 78700/100000: episode: 787, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.366, mean reward: -0.174 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.715, 10.098], loss: 0.002539, mae: 0.049985, mean_q: -0.311634
 78800/100000: episode: 788, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.211, mean reward: -0.152 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.202, 10.110], loss: 0.004578, mae: 0.063008, mean_q: -0.305402
 78900/100000: episode: 789, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.086, mean reward: -0.191 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.865, 10.109], loss: 0.002367, mae: 0.048341, mean_q: -0.314342
 79000/100000: episode: 790, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -19.058, mean reward: -0.191 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.610, 10.104], loss: 0.002461, mae: 0.048693, mean_q: -0.319575
 79100/100000: episode: 791, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -13.980, mean reward: -0.140 [-1.000, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.582, 10.110], loss: 0.002485, mae: 0.048774, mean_q: -0.311586
 79200/100000: episode: 792, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -11.084, mean reward: -0.111 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.714, 10.098], loss: 0.002387, mae: 0.047390, mean_q: -0.337179
 79300/100000: episode: 793, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: -20.487, mean reward: -0.205 [-1.000, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.237, 10.098], loss: 0.002372, mae: 0.048418, mean_q: -0.299459
 79400/100000: episode: 794, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.820, mean reward: -0.178 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.753, 10.132], loss: 0.002437, mae: 0.048249, mean_q: -0.362500
 79500/100000: episode: 795, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -16.688, mean reward: -0.167 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.305, 10.110], loss: 0.002487, mae: 0.048985, mean_q: -0.307332
 79600/100000: episode: 796, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: -20.378, mean reward: -0.204 [-1.000, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.064, 10.129], loss: 0.002654, mae: 0.051096, mean_q: -0.324692
 79700/100000: episode: 797, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -16.505, mean reward: -0.165 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.032, 10.272], loss: 0.002806, mae: 0.051809, mean_q: -0.320859
 79800/100000: episode: 798, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -17.491, mean reward: -0.175 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.806, 10.206], loss: 0.002413, mae: 0.048562, mean_q: -0.318180
 79900/100000: episode: 799, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -16.121, mean reward: -0.161 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.457, 10.240], loss: 0.002330, mae: 0.047166, mean_q: -0.351604
 80000/100000: episode: 800, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -17.200, mean reward: -0.172 [-1.000, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.060, 10.206], loss: 0.002413, mae: 0.048074, mean_q: -0.293879
 80100/100000: episode: 801, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -15.163, mean reward: -0.152 [-1.000, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.919, 10.273], loss: 0.002438, mae: 0.048752, mean_q: -0.331939
 80200/100000: episode: 802, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: -16.761, mean reward: -0.168 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.758, 10.098], loss: 0.002382, mae: 0.047737, mean_q: -0.350208
 80300/100000: episode: 803, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -19.151, mean reward: -0.192 [-1.000, 0.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.954, 10.217], loss: 0.002431, mae: 0.049082, mean_q: -0.296980
 80400/100000: episode: 804, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -15.084, mean reward: -0.151 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.256, 10.098], loss: 0.002552, mae: 0.050200, mean_q: -0.317964
 80500/100000: episode: 805, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -17.446, mean reward: -0.174 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.964, 10.230], loss: 0.002416, mae: 0.047814, mean_q: -0.358943
 80600/100000: episode: 806, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -15.631, mean reward: -0.156 [-1.000, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.333, 10.098], loss: 0.002507, mae: 0.049082, mean_q: -0.293572
 80700/100000: episode: 807, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -11.007, mean reward: -0.110 [-1.000, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.490, 10.098], loss: 0.002380, mae: 0.048193, mean_q: -0.332677
 80800/100000: episode: 808, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: -19.647, mean reward: -0.196 [-1.000, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.274, 10.187], loss: 0.002445, mae: 0.047432, mean_q: -0.297189
 80900/100000: episode: 809, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -17.319, mean reward: -0.173 [-1.000, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.866, 10.332], loss: 0.002302, mae: 0.046523, mean_q: -0.311307
 81000/100000: episode: 810, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.390, mean reward: -0.184 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.217, 10.105], loss: 0.002388, mae: 0.047313, mean_q: -0.328897
 81100/100000: episode: 811, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -13.015, mean reward: -0.130 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.608, 10.098], loss: 0.002508, mae: 0.049049, mean_q: -0.315127
 81200/100000: episode: 812, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -9.964, mean reward: -0.100 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.455, 10.098], loss: 0.002414, mae: 0.047619, mean_q: -0.334048
 81300/100000: episode: 813, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -19.262, mean reward: -0.193 [-1.000, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.251, 10.098], loss: 0.002278, mae: 0.046651, mean_q: -0.315550
 81400/100000: episode: 814, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -15.571, mean reward: -0.156 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.716, 10.227], loss: 0.002513, mae: 0.050350, mean_q: -0.318040
 81500/100000: episode: 815, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.075, mean reward: -0.181 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.238, 10.179], loss: 0.002394, mae: 0.047258, mean_q: -0.339532
 81600/100000: episode: 816, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.997, mean reward: -0.190 [-1.000, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.393, 10.111], loss: 0.002648, mae: 0.050812, mean_q: -0.277152
 81700/100000: episode: 817, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.538, mean reward: -0.145 [-1.000, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.707, 10.341], loss: 0.002408, mae: 0.048529, mean_q: -0.304056
 81800/100000: episode: 818, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.500, mean reward: -0.145 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.426, 10.098], loss: 0.002222, mae: 0.045770, mean_q: -0.354275
 81900/100000: episode: 819, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.575, mean reward: -0.176 [-1.000, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.866, 10.129], loss: 0.002381, mae: 0.047097, mean_q: -0.317947
 82000/100000: episode: 820, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: -19.089, mean reward: -0.191 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.231, 10.098], loss: 0.005147, mae: 0.060938, mean_q: -0.279657
 82100/100000: episode: 821, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.390, mean reward: -0.184 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.927, 10.350], loss: 0.002947, mae: 0.052512, mean_q: -0.327782
 82200/100000: episode: 822, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: -18.423, mean reward: -0.184 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.434, 10.215], loss: 0.003409, mae: 0.056037, mean_q: -0.352979
 82300/100000: episode: 823, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -19.670, mean reward: -0.197 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.841, 10.156], loss: 0.002526, mae: 0.050278, mean_q: -0.301765
 82400/100000: episode: 824, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -14.704, mean reward: -0.147 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.053, 10.143], loss: 0.002084, mae: 0.044187, mean_q: -0.322659
 82500/100000: episode: 825, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.659, mean reward: -0.167 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.676, 10.098], loss: 0.002350, mae: 0.047574, mean_q: -0.310656
 82600/100000: episode: 826, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.765, mean reward: -0.198 [-1.000, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.009, 10.282], loss: 0.002331, mae: 0.046315, mean_q: -0.318314
 82700/100000: episode: 827, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -20.400, mean reward: -0.204 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.402, 10.098], loss: 0.002156, mae: 0.044534, mean_q: -0.329926
 82800/100000: episode: 828, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -18.631, mean reward: -0.186 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.887, 10.247], loss: 0.002156, mae: 0.043925, mean_q: -0.338000
 82900/100000: episode: 829, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -20.584, mean reward: -0.206 [-1.000, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.026, 10.206], loss: 0.002443, mae: 0.047546, mean_q: -0.311018
 83000/100000: episode: 830, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.836, mean reward: -0.198 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.145, 10.098], loss: 0.002088, mae: 0.044478, mean_q: -0.316395
 83100/100000: episode: 831, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -18.388, mean reward: -0.184 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.992, 10.098], loss: 0.002436, mae: 0.047373, mean_q: -0.299866
 83200/100000: episode: 832, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -18.166, mean reward: -0.182 [-1.000, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.101, 10.200], loss: 0.002343, mae: 0.046662, mean_q: -0.337887
 83300/100000: episode: 833, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -14.566, mean reward: -0.146 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.657, 10.419], loss: 0.002352, mae: 0.047184, mean_q: -0.334935
 83400/100000: episode: 834, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -17.086, mean reward: -0.171 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.267, 10.142], loss: 0.002350, mae: 0.046141, mean_q: -0.330636
 83500/100000: episode: 835, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -12.759, mean reward: -0.128 [-1.000, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.750, 10.098], loss: 0.002208, mae: 0.045200, mean_q: -0.360592
 83600/100000: episode: 836, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.086, mean reward: -0.161 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.761, 10.206], loss: 0.002271, mae: 0.046106, mean_q: -0.326280
 83700/100000: episode: 837, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -18.824, mean reward: -0.188 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.406, 10.222], loss: 0.002250, mae: 0.046192, mean_q: -0.320654
 83800/100000: episode: 838, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -16.123, mean reward: -0.161 [-1.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.669, 10.098], loss: 0.002208, mae: 0.045115, mean_q: -0.319575
 83900/100000: episode: 839, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -9.645, mean reward: -0.096 [-1.000, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.097, 10.351], loss: 0.002282, mae: 0.045968, mean_q: -0.348389
 84000/100000: episode: 840, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -16.975, mean reward: -0.170 [-1.000, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.046, 10.098], loss: 0.002343, mae: 0.046230, mean_q: -0.287068
 84100/100000: episode: 841, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -18.582, mean reward: -0.186 [-1.000, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.838, 10.210], loss: 0.002336, mae: 0.047681, mean_q: -0.289869
 84200/100000: episode: 842, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -18.996, mean reward: -0.190 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.133, 10.098], loss: 0.002437, mae: 0.047422, mean_q: -0.325451
 84300/100000: episode: 843, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -16.386, mean reward: -0.164 [-1.000, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.923, 10.098], loss: 0.002322, mae: 0.047255, mean_q: -0.331029
 84400/100000: episode: 844, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -17.191, mean reward: -0.172 [-1.000, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.146, 10.320], loss: 0.005281, mae: 0.066891, mean_q: -0.326031
 84500/100000: episode: 845, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.943, mean reward: -0.159 [-1.000, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.888, 10.346], loss: 0.002458, mae: 0.049929, mean_q: -0.327750
 84600/100000: episode: 846, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -16.487, mean reward: -0.165 [-1.000, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.863, 10.098], loss: 0.002246, mae: 0.045970, mean_q: -0.343034
 84700/100000: episode: 847, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: -16.643, mean reward: -0.166 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.441, 10.265], loss: 0.002317, mae: 0.046735, mean_q: -0.298973
 84800/100000: episode: 848, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -18.110, mean reward: -0.181 [-1.000, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.704, 10.288], loss: 0.002228, mae: 0.045838, mean_q: -0.298143
 84900/100000: episode: 849, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -18.381, mean reward: -0.184 [-1.000, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.371, 10.098], loss: 0.002218, mae: 0.044876, mean_q: -0.350056
 85000/100000: episode: 850, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -14.301, mean reward: -0.143 [-1.000, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.549, 10.098], loss: 0.002376, mae: 0.047402, mean_q: -0.288343
 85100/100000: episode: 851, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: -17.166, mean reward: -0.172 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.103, 10.098], loss: 0.002372, mae: 0.047557, mean_q: -0.288089
 85200/100000: episode: 852, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: -20.859, mean reward: -0.209 [-1.000, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.109, 10.104], loss: 0.002113, mae: 0.044351, mean_q: -0.315304
 85300/100000: episode: 853, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -13.779, mean reward: -0.138 [-1.000, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.461, 10.297], loss: 0.002410, mae: 0.047292, mean_q: -0.337369
 85400/100000: episode: 854, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: -16.353, mean reward: -0.164 [-1.000, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.429, 10.216], loss: 0.002506, mae: 0.047582, mean_q: -0.335396
 85500/100000: episode: 855, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: -17.563, mean reward: -0.176 [-1.000, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.824, 10.098], loss: 0.002145, mae: 0.044207, mean_q: -0.348497
 85600/100000: episode: 856, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.991, mean reward: -0.190 [-1.000, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.446, 10.098], loss: 0.002334, mae: 0.046194, mean_q: -0.319591
 85700/100000: episode: 857, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -18.624, mean reward: -0.186 [-1.000, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.479, 10.357], loss: 0.002358, mae: 0.046551, mean_q: -0.314094
 85800/100000: episode: 858, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: -16.417, mean reward: -0.164 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.078, 10.098], loss: 0.002382, mae: 0.047193, mean_q: -0.302946
 85900/100000: episode: 859, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: -20.406, mean reward: -0.204 [-1.000, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.465, 10.130], loss: 0.002540, mae: 0.048196, mean_q: -0.356667
 86000/100000: episode: 860, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -20.505, mean reward: -0.205 [-1.000, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.285, 10.123], loss: 0.002348, mae: 0.047170, mean_q: -0.309149
 86100/100000: episode: 861, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -18.635, mean reward: -0.186 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.571, 10.098], loss: 0.002439, mae: 0.048703, mean_q: -0.282318
 86200/100000: episode: 862, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -17.943, mean reward: -0.179 [-1.000, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.011, 10.098], loss: 0.002278, mae: 0.046312, mean_q: -0.339001
 86300/100000: episode: 863, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: -14.845, mean reward: -0.148 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.258, 10.327], loss: 0.002324, mae: 0.046958, mean_q: -0.349378
 86400/100000: episode: 864, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: -19.225, mean reward: -0.192 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.917, 10.195], loss: 0.002383, mae: 0.047760, mean_q: -0.330188
 86500/100000: episode: 865, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -16.261, mean reward: -0.163 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.937, 10.244], loss: 0.002576, mae: 0.049335, mean_q: -0.351905
 86600/100000: episode: 866, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -9.667, mean reward: -0.097 [-1.000, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.809, 10.427], loss: 0.002245, mae: 0.046531, mean_q: -0.328479
 86700/100000: episode: 867, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -19.183, mean reward: -0.192 [-1.000, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.450, 10.098], loss: 0.002305, mae: 0.046607, mean_q: -0.321649
 86800/100000: episode: 868, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.231, mean reward: -0.182 [-1.000, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.368, 10.161], loss: 0.002511, mae: 0.048044, mean_q: -0.367929
 86900/100000: episode: 869, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -15.005, mean reward: -0.150 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.009, 10.098], loss: 0.002407, mae: 0.048024, mean_q: -0.351378
 87000/100000: episode: 870, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -13.547, mean reward: -0.135 [-1.000, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.883, 10.098], loss: 0.002256, mae: 0.046299, mean_q: -0.341368
 87100/100000: episode: 871, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -16.764, mean reward: -0.168 [-1.000, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.799, 10.098], loss: 0.003076, mae: 0.051584, mean_q: -0.308698
 87200/100000: episode: 872, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: -17.448, mean reward: -0.174 [-1.000, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.969, 10.221], loss: 0.004432, mae: 0.061630, mean_q: -0.312536
 87300/100000: episode: 873, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -10.685, mean reward: -0.107 [-1.000, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.078, 10.098], loss: 0.002337, mae: 0.047507, mean_q: -0.368550
 87400/100000: episode: 874, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.386, mean reward: -0.174 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.805, 10.098], loss: 0.002494, mae: 0.049015, mean_q: -0.363550
 87500/100000: episode: 875, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -15.234, mean reward: -0.152 [-1.000, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.333, 10.135], loss: 0.002552, mae: 0.049538, mean_q: -0.347691
 87600/100000: episode: 876, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -20.203, mean reward: -0.202 [-1.000, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.999, 10.130], loss: 0.002377, mae: 0.047569, mean_q: -0.312087
 87700/100000: episode: 877, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.911, mean reward: -0.169 [-1.000, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.525, 10.098], loss: 0.002251, mae: 0.046342, mean_q: -0.353345
 87800/100000: episode: 878, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: -16.659, mean reward: -0.167 [-1.000, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.793, 10.098], loss: 0.002715, mae: 0.050990, mean_q: -0.311770
 87900/100000: episode: 879, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -18.972, mean reward: -0.190 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.306, 10.146], loss: 0.002222, mae: 0.045924, mean_q: -0.315358
 88000/100000: episode: 880, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -12.402, mean reward: -0.124 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.326, 10.305], loss: 0.002271, mae: 0.045654, mean_q: -0.355577
 88100/100000: episode: 881, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -20.017, mean reward: -0.200 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.809, 10.098], loss: 0.002426, mae: 0.048821, mean_q: -0.296946
 88200/100000: episode: 882, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.261, mean reward: -0.173 [-1.000, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.352, 10.098], loss: 0.002575, mae: 0.050373, mean_q: -0.274910
 88300/100000: episode: 883, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -17.421, mean reward: -0.174 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.327, 10.105], loss: 0.002203, mae: 0.046176, mean_q: -0.293239
 88400/100000: episode: 884, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -18.929, mean reward: -0.189 [-1.000, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.782, 10.098], loss: 0.002159, mae: 0.045935, mean_q: -0.313958
 88500/100000: episode: 885, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -19.253, mean reward: -0.193 [-1.000, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.668, 10.200], loss: 0.002145, mae: 0.045869, mean_q: -0.311682
 88600/100000: episode: 886, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -18.961, mean reward: -0.190 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.958, 10.183], loss: 0.002209, mae: 0.045575, mean_q: -0.333830
 88700/100000: episode: 887, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -14.489, mean reward: -0.145 [-1.000, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.158, 10.098], loss: 0.002205, mae: 0.045837, mean_q: -0.314684
 88800/100000: episode: 888, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -8.184, mean reward: -0.082 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.793, 10.454], loss: 0.002137, mae: 0.044627, mean_q: -0.344022
 88900/100000: episode: 889, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -19.727, mean reward: -0.197 [-1.000, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.658, 10.198], loss: 0.002372, mae: 0.047356, mean_q: -0.340336
 89000/100000: episode: 890, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -8.034, mean reward: -0.080 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.613, 10.189], loss: 0.002144, mae: 0.045142, mean_q: -0.318404
 89100/100000: episode: 891, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -12.976, mean reward: -0.130 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.596, 10.454], loss: 0.002239, mae: 0.046935, mean_q: -0.324372
 89200/100000: episode: 892, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: -18.773, mean reward: -0.188 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.604, 10.236], loss: 0.002198, mae: 0.046533, mean_q: -0.321274
 89300/100000: episode: 893, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -20.181, mean reward: -0.202 [-1.000, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.579, 10.098], loss: 0.002136, mae: 0.044587, mean_q: -0.329299
 89400/100000: episode: 894, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: -17.569, mean reward: -0.176 [-1.000, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.650, 10.164], loss: 0.002462, mae: 0.050057, mean_q: -0.311172
 89500/100000: episode: 895, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.546, mean reward: -0.185 [-1.000, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.345, 10.111], loss: 0.002197, mae: 0.046659, mean_q: -0.320240
 89600/100000: episode: 896, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -19.111, mean reward: -0.191 [-1.000, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.628, 10.098], loss: 0.002122, mae: 0.046264, mean_q: -0.331120
 89700/100000: episode: 897, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: -17.837, mean reward: -0.178 [-1.000, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.506, 10.098], loss: 0.002160, mae: 0.045014, mean_q: -0.330412
 89800/100000: episode: 898, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -16.732, mean reward: -0.167 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.364, 10.098], loss: 0.002466, mae: 0.049138, mean_q: -0.322138
 89900/100000: episode: 899, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -19.462, mean reward: -0.195 [-1.000, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.388, 10.235], loss: 0.002176, mae: 0.045844, mean_q: -0.333177
 90000/100000: episode: 900, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -17.043, mean reward: -0.170 [-1.000, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.480, 10.098], loss: 0.002314, mae: 0.047743, mean_q: -0.322219
 90100/100000: episode: 901, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -14.691, mean reward: -0.147 [-1.000, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.078, 10.098], loss: 0.002438, mae: 0.048663, mean_q: -0.287134
 90200/100000: episode: 902, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -19.634, mean reward: -0.196 [-1.000, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.256, 10.134], loss: 0.002219, mae: 0.046579, mean_q: -0.344907
 90300/100000: episode: 903, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -20.488, mean reward: -0.205 [-1.000, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.396, 10.098], loss: 0.002404, mae: 0.048503, mean_q: -0.338102
 90400/100000: episode: 904, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -13.873, mean reward: -0.139 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.209, 10.273], loss: 0.002407, mae: 0.049247, mean_q: -0.325342
 90500/100000: episode: 905, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -14.405, mean reward: -0.144 [-1.000, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.692, 10.180], loss: 0.002394, mae: 0.048794, mean_q: -0.301609
 90600/100000: episode: 906, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -19.399, mean reward: -0.194 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.543, 10.204], loss: 0.002347, mae: 0.048629, mean_q: -0.325537
 90700/100000: episode: 907, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -20.273, mean reward: -0.203 [-1.000, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.188, 10.184], loss: 0.002315, mae: 0.048911, mean_q: -0.326164
 90800/100000: episode: 908, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -14.306, mean reward: -0.143 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.809, 10.098], loss: 0.002296, mae: 0.048396, mean_q: -0.327175
 90900/100000: episode: 909, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -15.512, mean reward: -0.155 [-1.000, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.198, 10.274], loss: 0.002751, mae: 0.051741, mean_q: -0.335387
 91000/100000: episode: 910, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -14.933, mean reward: -0.149 [-1.000, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.825, 10.098], loss: 0.002525, mae: 0.049544, mean_q: -0.338756
 91100/100000: episode: 911, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -18.159, mean reward: -0.182 [-1.000, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.222, 10.098], loss: 0.002538, mae: 0.049914, mean_q: -0.283685
 91200/100000: episode: 912, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -19.623, mean reward: -0.196 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.460, 10.155], loss: 0.002328, mae: 0.047464, mean_q: -0.322419
 91300/100000: episode: 913, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -20.102, mean reward: -0.201 [-1.000, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.290, 10.179], loss: 0.002565, mae: 0.049287, mean_q: -0.323665
 91400/100000: episode: 914, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.205, mean reward: -0.172 [-1.000, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.813, 10.098], loss: 0.002526, mae: 0.050321, mean_q: -0.306222
 91500/100000: episode: 915, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -14.019, mean reward: -0.140 [-1.000, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.204, 10.396], loss: 0.002541, mae: 0.049581, mean_q: -0.318988
 91600/100000: episode: 916, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -18.968, mean reward: -0.190 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.592, 10.163], loss: 0.002823, mae: 0.052311, mean_q: -0.316133
 91700/100000: episode: 917, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: -17.784, mean reward: -0.178 [-1.000, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.278, 10.256], loss: 0.002483, mae: 0.048794, mean_q: -0.301416
 91800/100000: episode: 918, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -18.823, mean reward: -0.188 [-1.000, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.651, 10.185], loss: 0.002504, mae: 0.049443, mean_q: -0.360045
 91900/100000: episode: 919, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.789, mean reward: -0.148 [-1.000, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.222, 10.133], loss: 0.002436, mae: 0.048905, mean_q: -0.300432
 92000/100000: episode: 920, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: -15.923, mean reward: -0.159 [-1.000, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.447, 10.207], loss: 0.002538, mae: 0.049511, mean_q: -0.342482
 92100/100000: episode: 921, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -17.702, mean reward: -0.177 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.326, 10.153], loss: 0.002539, mae: 0.050252, mean_q: -0.338396
 92200/100000: episode: 922, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.071, mean reward: -0.141 [-1.000, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.562, 10.098], loss: 0.002519, mae: 0.049934, mean_q: -0.310845
 92300/100000: episode: 923, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -16.543, mean reward: -0.165 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.638, 10.369], loss: 0.002464, mae: 0.048405, mean_q: -0.343846
 92400/100000: episode: 924, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: -13.708, mean reward: -0.137 [-1.000, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.247, 10.339], loss: 0.002442, mae: 0.049627, mean_q: -0.339867
 92500/100000: episode: 925, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.747, mean reward: -0.177 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.385, 10.098], loss: 0.002433, mae: 0.049433, mean_q: -0.324281
 92600/100000: episode: 926, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: -18.154, mean reward: -0.182 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.952, 10.118], loss: 0.002394, mae: 0.048325, mean_q: -0.327153
 92700/100000: episode: 927, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: -16.900, mean reward: -0.169 [-1.000, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.258, 10.254], loss: 0.002859, mae: 0.055659, mean_q: -0.299937
 92800/100000: episode: 928, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: -18.344, mean reward: -0.183 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.465, 10.172], loss: 0.002633, mae: 0.051279, mean_q: -0.318584
 92900/100000: episode: 929, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -18.518, mean reward: -0.185 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.006, 10.106], loss: 0.002640, mae: 0.049780, mean_q: -0.316083
 93000/100000: episode: 930, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: -18.382, mean reward: -0.184 [-1.000, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.959, 10.167], loss: 0.002576, mae: 0.050188, mean_q: -0.301144
 93100/100000: episode: 931, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.497, mean reward: -0.175 [-1.000, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.119, 10.098], loss: 0.002341, mae: 0.047294, mean_q: -0.329881
 93200/100000: episode: 932, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -14.490, mean reward: -0.145 [-1.000, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.014, 10.098], loss: 0.002400, mae: 0.048152, mean_q: -0.317231
 93300/100000: episode: 933, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: -18.614, mean reward: -0.186 [-1.000, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.771, 10.218], loss: 0.002504, mae: 0.049435, mean_q: -0.330139
 93400/100000: episode: 934, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: -16.126, mean reward: -0.161 [-1.000, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.886, 10.255], loss: 0.002595, mae: 0.049998, mean_q: -0.337856
 93500/100000: episode: 935, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.654, mean reward: -0.187 [-1.000, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.940, 10.179], loss: 0.002591, mae: 0.049878, mean_q: -0.351997
 93600/100000: episode: 936, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -21.115, mean reward: -0.211 [-1.000, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.370, 10.116], loss: 0.002635, mae: 0.050670, mean_q: -0.306297
 93700/100000: episode: 937, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -18.739, mean reward: -0.187 [-1.000, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.173, 10.287], loss: 0.002846, mae: 0.050885, mean_q: -0.307476
 93800/100000: episode: 938, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -19.377, mean reward: -0.194 [-1.000, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.906, 10.102], loss: 0.003988, mae: 0.058792, mean_q: -0.323320
 93900/100000: episode: 939, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -17.027, mean reward: -0.170 [-1.000, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.280, 10.098], loss: 0.002731, mae: 0.052516, mean_q: -0.322536
 94000/100000: episode: 940, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: -19.161, mean reward: -0.192 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.218, 10.226], loss: 0.002495, mae: 0.048334, mean_q: -0.323546
 94100/100000: episode: 941, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -18.321, mean reward: -0.183 [-1.000, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.992, 10.299], loss: 0.002486, mae: 0.048863, mean_q: -0.324508
 94200/100000: episode: 942, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.065, mean reward: -0.171 [-1.000, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.924, 10.098], loss: 0.002531, mae: 0.048158, mean_q: -0.345176
 94300/100000: episode: 943, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -17.908, mean reward: -0.179 [-1.000, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.406, 10.111], loss: 0.002388, mae: 0.047661, mean_q: -0.331553
 94400/100000: episode: 944, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -13.501, mean reward: -0.135 [-1.000, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.873, 10.098], loss: 0.002644, mae: 0.049953, mean_q: -0.292875
 94500/100000: episode: 945, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -17.779, mean reward: -0.178 [-1.000, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.761, 10.098], loss: 0.002458, mae: 0.048017, mean_q: -0.350892
 94600/100000: episode: 946, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: -17.740, mean reward: -0.177 [-1.000, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.727, 10.098], loss: 0.002426, mae: 0.047392, mean_q: -0.324156
 94700/100000: episode: 947, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: -19.288, mean reward: -0.193 [-1.000, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.584, 10.121], loss: 0.002502, mae: 0.047953, mean_q: -0.349979
 94800/100000: episode: 948, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -17.352, mean reward: -0.174 [-1.000, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.277, 10.098], loss: 0.002415, mae: 0.048423, mean_q: -0.356849
 94900/100000: episode: 949, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: -18.226, mean reward: -0.182 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.977, 10.324], loss: 0.002604, mae: 0.049037, mean_q: -0.324298
 95000/100000: episode: 950, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -15.507, mean reward: -0.155 [-1.000, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.559, 10.098], loss: 0.002286, mae: 0.046724, mean_q: -0.316745
 95100/100000: episode: 951, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.199, mean reward: -0.172 [-1.000, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.703, 10.098], loss: 0.002550, mae: 0.049637, mean_q: -0.313549
 95200/100000: episode: 952, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -17.870, mean reward: -0.179 [-1.000, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.134, 10.269], loss: 0.002684, mae: 0.050680, mean_q: -0.338699
 95300/100000: episode: 953, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -18.275, mean reward: -0.183 [-1.000, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.431, 10.176], loss: 0.002584, mae: 0.049299, mean_q: -0.352603
 95400/100000: episode: 954, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: -14.214, mean reward: -0.142 [-1.000, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.173, 10.263], loss: 0.002497, mae: 0.049029, mean_q: -0.321855
 95500/100000: episode: 955, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: -16.369, mean reward: -0.164 [-1.000, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.440, 10.340], loss: 0.002524, mae: 0.048830, mean_q: -0.318893
 95600/100000: episode: 956, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -19.035, mean reward: -0.190 [-1.000, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.094, 10.176], loss: 0.002543, mae: 0.048305, mean_q: -0.337673
 95700/100000: episode: 957, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -16.845, mean reward: -0.168 [-1.000, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.256, 10.098], loss: 0.002733, mae: 0.050487, mean_q: -0.347520
 95800/100000: episode: 958, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: -12.228, mean reward: -0.122 [-1.000, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.966, 10.098], loss: 0.003623, mae: 0.057042, mean_q: -0.318975
 95900/100000: episode: 959, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: -18.233, mean reward: -0.182 [-1.000, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.577, 10.098], loss: 0.003564, mae: 0.060058, mean_q: -0.318541
 96000/100000: episode: 960, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: -18.032, mean reward: -0.180 [-1.000, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.315, 10.283], loss: 0.002605, mae: 0.049146, mean_q: -0.325921
 96100/100000: episode: 961, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -17.994, mean reward: -0.180 [-1.000, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.211, 10.137], loss: 0.002642, mae: 0.050270, mean_q: -0.301678
 96200/100000: episode: 962, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -19.521, mean reward: -0.195 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.322, 10.098], loss: 0.002595, mae: 0.049966, mean_q: -0.329746
 96300/100000: episode: 963, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -17.861, mean reward: -0.179 [-1.000, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.405, 10.098], loss: 0.002353, mae: 0.047765, mean_q: -0.328659
 96400/100000: episode: 964, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -18.252, mean reward: -0.183 [-1.000, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.625, 10.126], loss: 0.002326, mae: 0.045905, mean_q: -0.377243
 96500/100000: episode: 965, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: -15.198, mean reward: -0.152 [-1.000, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.008, 10.154], loss: 0.002595, mae: 0.049590, mean_q: -0.298548
 96600/100000: episode: 966, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: -15.233, mean reward: -0.152 [-1.000, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.724, 10.359], loss: 0.002607, mae: 0.048350, mean_q: -0.354838
 96700/100000: episode: 967, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: -17.451, mean reward: -0.175 [-1.000, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.392, 10.288], loss: 0.002283, mae: 0.046280, mean_q: -0.343719
 96800/100000: episode: 968, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -19.110, mean reward: -0.191 [-1.000, 0.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.837, 10.280], loss: 0.002694, mae: 0.049652, mean_q: -0.361442
 96900/100000: episode: 969, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: -18.256, mean reward: -0.183 [-1.000, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.421, 10.228], loss: 0.005618, mae: 0.062080, mean_q: -0.339954
 97000/100000: episode: 970, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: -17.364, mean reward: -0.174 [-1.000, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.277, 10.318], loss: 0.003503, mae: 0.059879, mean_q: -0.355255
 97100/100000: episode: 971, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.671, mean reward: -0.177 [-1.000, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.230, 10.098], loss: 0.002609, mae: 0.050952, mean_q: -0.311253
 97200/100000: episode: 972, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: -16.088, mean reward: -0.161 [-1.000, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.587, 10.107], loss: 0.002389, mae: 0.047685, mean_q: -0.346139
 97300/100000: episode: 973, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: -12.431, mean reward: -0.124 [-1.000, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.270, 10.098], loss: 0.002459, mae: 0.047637, mean_q: -0.334611
 97400/100000: episode: 974, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: -16.115, mean reward: -0.161 [-1.000, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.799, 10.167], loss: 0.002156, mae: 0.045928, mean_q: -0.327199
 97500/100000: episode: 975, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.373, mean reward: -0.174 [-1.000, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.300, 10.098], loss: 0.002564, mae: 0.048589, mean_q: -0.351826
 97600/100000: episode: 976, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: -17.025, mean reward: -0.170 [-1.000, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.796, 10.174], loss: 0.002513, mae: 0.049073, mean_q: -0.324929
 97700/100000: episode: 977, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -18.231, mean reward: -0.182 [-1.000, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.349, 10.166], loss: 0.002725, mae: 0.050567, mean_q: -0.307940
 97800/100000: episode: 978, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: -16.008, mean reward: -0.160 [-1.000, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.168, 10.145], loss: 0.002323, mae: 0.046322, mean_q: -0.344050
 97900/100000: episode: 979, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -15.063, mean reward: -0.151 [-1.000, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.494, 10.130], loss: 0.002412, mae: 0.047715, mean_q: -0.303248
 98000/100000: episode: 980, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: -12.182, mean reward: -0.122 [-1.000, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.893, 10.098], loss: 0.002579, mae: 0.050703, mean_q: -0.322914
 98100/100000: episode: 981, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -17.865, mean reward: -0.179 [-1.000, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.257, 10.175], loss: 0.002215, mae: 0.045725, mean_q: -0.345622
 98200/100000: episode: 982, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -17.267, mean reward: -0.173 [-1.000, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.669, 10.098], loss: 0.002247, mae: 0.045351, mean_q: -0.363165
 98300/100000: episode: 983, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: -16.595, mean reward: -0.166 [-1.000, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.816, 10.098], loss: 0.002623, mae: 0.049484, mean_q: -0.328876
 98400/100000: episode: 984, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: -17.784, mean reward: -0.178 [-1.000, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.842, 10.331], loss: 0.002396, mae: 0.047141, mean_q: -0.302952
 98500/100000: episode: 985, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: -16.454, mean reward: -0.165 [-1.000, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.949, 10.317], loss: 0.002285, mae: 0.046560, mean_q: -0.307892
 98600/100000: episode: 986, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: -19.192, mean reward: -0.192 [-1.000, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.296, 10.167], loss: 0.002579, mae: 0.049065, mean_q: -0.312934
 98700/100000: episode: 987, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -19.717, mean reward: -0.197 [-1.000, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.936, 10.098], loss: 0.003457, mae: 0.054243, mean_q: -0.360102
 98800/100000: episode: 988, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: -10.316, mean reward: -0.103 [-1.000, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.748, 10.172], loss: 0.002979, mae: 0.057489, mean_q: -0.275765
 98900/100000: episode: 989, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: -18.663, mean reward: -0.187 [-1.000, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.054, 10.168], loss: 0.002386, mae: 0.048403, mean_q: -0.298175
 99000/100000: episode: 990, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -16.445, mean reward: -0.164 [-1.000, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.673, 10.126], loss: 0.002335, mae: 0.047290, mean_q: -0.317181
 99100/100000: episode: 991, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: -17.050, mean reward: -0.170 [-1.000, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.894, 10.106], loss: 0.002549, mae: 0.049680, mean_q: -0.284528
 99200/100000: episode: 992, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -19.161, mean reward: -0.192 [-1.000, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.105, 10.098], loss: 0.002558, mae: 0.049617, mean_q: -0.310572
 99300/100000: episode: 993, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: -13.030, mean reward: -0.130 [-1.000, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.742, 10.098], loss: 0.002437, mae: 0.048433, mean_q: -0.299728
 99400/100000: episode: 994, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: -10.131, mean reward: -0.101 [-1.000, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.967, 10.300], loss: 0.002454, mae: 0.048039, mean_q: -0.347029
 99500/100000: episode: 995, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: -14.987, mean reward: -0.150 [-1.000, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.786, 10.098], loss: 0.002306, mae: 0.046361, mean_q: -0.328935
 99600/100000: episode: 996, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: -17.514, mean reward: -0.175 [-1.000, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.594, 10.229], loss: 0.002352, mae: 0.046963, mean_q: -0.302099
 99700/100000: episode: 997, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: -16.614, mean reward: -0.166 [-1.000, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.259, 10.098], loss: 0.002462, mae: 0.047710, mean_q: -0.328984
 99800/100000: episode: 998, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: -7.130, mean reward: -0.071 [-1.000, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.715, 10.412], loss: 0.002522, mae: 0.048626, mean_q: -0.300212
 99900/100000: episode: 999, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: -17.822, mean reward: -0.178 [-1.000, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.337, 10.130], loss: 0.002361, mae: 0.047072, mean_q: -0.310927
 100000/100000: episode: 1000, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: -18.898, mean reward: -0.189 [-1.000, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.737, 10.099], loss: 0.002343, mae: 0.046963, mean_q: -0.308421
done, took 533.211 seconds
[Info] End Uniform Random Simulation. Falsification occurred 0 times.
